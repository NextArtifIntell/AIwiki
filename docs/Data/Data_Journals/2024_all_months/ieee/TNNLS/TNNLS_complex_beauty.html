<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TNNLS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tnnls---1448">TNNLS - 1448</h2>
<ul>
<li><details>
<summary>
(2024). Effect of time-varying multiplicative noise on DNN-kWTA
model. <em>TNNLS</em>, <em>35</em>(12), 18922–18930. (<a
href="https://doi.org/10.1109/TNNLS.2023.3317135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among many $k$ -winners-take-all ( $k$ WTA) models, the dual-neural network (DNN- $k$ WTA) model is with significantly less number of connections. However, for analog realization, noise is inevitable and affects the operational correctness of the $k$ WTA process. Most existing results focus on the effect of additive noise. This brief studies the effect of time-varying multiplicative input noise. Two scenarios are considered. The first one is the bounded noise case, in which only the noise range is known. Another one is for the general noise distribution case, in which we either know the noise distribution or have noise samples. For each scenario, we first prove the convergence property of the DNN- $k$ WTA model under multiplicative input noise and then provide an efficient method to determine whether a noise-affected DNN- $k$ WTA network performs the correct $k$ WTA process for a given set of inputs. With the two methods, we can efficiently measure the probability of the network performing the correct $k$ WTA process. In addition, for the case of the inputs being uniformly distributed, we derive two closed-form expressions, one for each scenario, for estimating the probability of the model having correct operation. Finally, we conduct simulations to verify our theoretical results.},
  archive      = {J_TNNLS},
  author       = {Wenhao Lu and Yuanjin Zheng and Chi-Sing Leung},
  doi          = {10.1109/TNNLS.2023.3317135},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18922-18930},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effect of time-varying multiplicative noise on DNN-kWTA model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A gradient-aware search algorithm for constrained markov
decision processes. <em>TNNLS</em>, <em>35</em>(12), 18914–18921. (<a
href="https://doi.org/10.1109/TNNLS.2023.3315598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The canonical solution methodology for finite constrained Markov decision processes (CMDPs), where the objective is to maximize the expected infinite-horizon discounted rewards subject to the expected infinite-horizon discounted costs’ constraints, is based on convex linear programming (LP). In this brief, we first prove that the optimization objective in the dual linear program of a finite CMDP is a piecewise linear convex (PWLC) function with respect to the Lagrange penalty multipliers. Next, we propose a novel, provably optimal, two-level gradient-aware search (GAS) algorithm which exploits the PWLC structure to find the optimal state-value function and Lagrange penalty multipliers of a finite CMDP. The proposed algorithm is applied in two stochastic control problems with constraints for performance comparison with binary search (BS), Lagrangian primal–dual optimization (PDO), and LP. Compared with the benchmark algorithms, it is shown that the proposed GAS algorithm converges to the optimal solution quickly without any hyperparameter tuning. In addition, the convergence speed of the proposed algorithm is not sensitive to the initialization of the Lagrange multipliers.},
  archive      = {J_TNNLS},
  author       = {Sami Khairy and Prasanna Balaprakash and Lin X. Cai},
  doi          = {10.1109/TNNLS.2023.3315598},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18914-18921},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A gradient-aware search algorithm for constrained markov decision processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Limit cycle of a single-neuron system with smooth continuous
and binary-value activation functions and its circuitry design.
<em>TNNLS</em>, <em>35</em>(12), 18907–18913. (<a
href="https://doi.org/10.1109/TNNLS.2023.3314675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we investigate the limit cycle of a single-neuron system with smooth continuous and binary-value activation functions and its circuit design. By transforming the system into Liénard-type and using Poincaré–Bendixson theorem as well as the symmetry of these systems, we obtain the existence conditions of limit cycle of the system. Then, by comparing the integral value of the differential of positive definite function along two assumed limit cycles, we prove that the system cannot produce two coexisting limit cycles, which means that the system has at most one limit cycle. In addition, according to the two specific functions, i.e., smooth continuous and binary-value activation functions of the system, we give the numerical simulation and realize the circuit design of the single-neuron system by using Multisim modeling, respectively. The waveform diagram and phase diagram of the numerical simulation and circuit simulation are obtained. By comparing the results of numerical and circuit simulation, the effectiveness of our mathematical analysis and the feasibility of circuit design are better illustrated.},
  archive      = {J_TNNLS},
  author       = {Jintao Huang and Xiaofeng Liao and Yunhang Zhu},
  doi          = {10.1109/TNNLS.2023.3314675},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18907-18913},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Limit cycle of a single-neuron system with smooth continuous and binary-value activation functions and its circuitry design},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronization of time-delay coupled neural networks with
stabilizing delayed impulsive control. <em>TNNLS</em>, <em>35</em>(12),
18899–18906. (<a
href="https://doi.org/10.1109/TNNLS.2023.3320651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief studies the distributed synchronization of time-delay coupled neural networks (NNs) with impulsive pinning control involving stabilizing delays. A novel differential inequality is proposed, where the state’s past information at impulsive time is effectively extracted and used to handle the synchronization of coupled NNs. Based on this inequality, the restriction that the size of impulsive delay is always limited by the system delay is removed, and the upper bound on the impulsive delay is relaxed, which is improved the existing related results. By using the methods of average impulsive interval (AII) and impulsive delay, some relaxed criteria for distributed synchronization of time-delay coupled NNs are obtained. The proposed synchronization conditions do not impose on the upper bound of two consecutive impulsive signals, and the lower bound is more flexible. Moreover, our results reveal that the impulsive delays may contribute to the synchronization of time-delay systems. Finally, typical networks are presented to illustrate the advantage of our delayed impulsive control method.},
  archive      = {J_TNNLS},
  author       = {Lingzhong Zhang and Jianquan Lu and Fengyi Liu and Jungang Lou},
  doi          = {10.1109/TNNLS.2023.3320651},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18899-18906},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of time-delay coupled neural networks with stabilizing delayed impulsive control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spike attention coding for spiking neural networks.
<em>TNNLS</em>, <em>35</em>(12), 18892–18898. (<a
href="https://doi.org/10.1109/TNNLS.2023.3310263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs), an important family of neuroscience-oriented intelligent models, play an essential role in the neuromorphic computing community. Spike rate coding and temporal coding are the mainstream coding schemes in the current modeling of SNNs. However, rate coding usually suffers from limited representation resolution and long latency, while temporal coding usually suffers from under-utilization of spike activities. To this end, we propose spike attention coding (SAC) for SNNs. By introducing learnable attention coefficients for each time step, our coding scheme can naturally unify rate coding and temporal coding, and then flexibly learn optimal coefficients for better performance. Several normalization and regularization techniques are further incorporated to control the range and distribution of the learned attention coefficients. Extensive experiments on classification, generation, and regression tasks are conducted and demonstrate the superiority of the proposed coding scheme. This work provides a flexible coding scheme to enhance the representation power of SNNs and extends their application scope beyond the mainstream classification scenario.},
  archive      = {J_TNNLS},
  author       = {Jiawen Liu and Yifan Hu and Guoqi Li and Jing Pei and Lei Deng},
  doi          = {10.1109/TNNLS.2023.3310263},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18892-18898},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spike attention coding for spiking neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotic stability of delayed boolean networks with random
data dropouts. <em>TNNLS</em>, <em>35</em>(12), 18886–18891. (<a
href="https://doi.org/10.1109/TNNLS.2023.3301220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real networks, communication constraints often prevent the full exchange of information between nodes, which is inevitable. This brief investigates the problem of time delay and randomly missing data in Boolean networks (BNs). A Bernoulli random variable is assigned to each node to characterize the probability of data packet dropout. Time delay and missing data are modeled by independent random variables. A novel data-sending rule that incorporates both communication constraints is proposed. An augmented system, comprising current states, delayed information, and successfully transmitted data, is established for theoretical analysis. Using the semitensor product (STP), the necessary and sufficient condition for asymptotic stability of delayed BNs with random data dropouts is derived. The convergence rate is also obtained.},
  archive      = {J_TNNLS},
  author       = {Chi Huang and Wenjun Xiong and Jianquan Lu and Darong Huang},
  doi          = {10.1109/TNNLS.2023.3301220},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18886-18891},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asymptotic stability of delayed boolean networks with random data dropouts},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic forecasting with modified n-BEATS networks.
<em>TNNLS</em>, <em>35</em>(12), 18872–18885. (<a
href="https://doi.org/10.1109/TNNLS.2024.3450832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a modification to the state-of-the-art N-BEATS deep learning architecture for the univariate time series point forecasting problem for generating parametric probabilistic forecasts. Next, we propose an extension to this probabilistic N-BEATS architecture to allow optimizing probabilistic forecasts from both a traditional forecast accuracy perspective as well as a forecast stability perspective, where the latter is defined in terms of a change in the forecast distribution for a specific time period caused by updating the probabilistic forecast for this time period when new observations become available (i.e., as time passes). We empirically show that this extension leads to more stable forecast distributions without causing considerable losses in forecast accuracy for the M4 monthly dataset. Finally, we present a second extension to the probabilistic N-BEATS network which makes it possible to jointly optimize single-period marginal and multiperiod cumulative (i.e., aggregated over multiple time periods) probabilistic forecasts. Empirical results are reported for the M4 monthly dataset and indicate that improvements in accuracy can be obtained over basic but well-established methods to produce probabilistic cumulative forecasts. The proposed probabilistic N-BEATS network and the extensions are all useful in a supply chain planning context.},
  archive      = {J_TNNLS},
  author       = {Jente Van Belle and Ruben Crevits and Daan Caljon and Wouter Verbeke},
  doi          = {10.1109/TNNLS.2024.3450832},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18872-18885},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic forecasting with modified N-BEATS networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bypassing stationary points in training deep learning
models. <em>TNNLS</em>, <em>35</em>(12), 18859–18871. (<a
href="https://doi.org/10.1109/TNNLS.2024.3411020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient-descent-based optimizers are prone to slowdowns in training deep learning models, as stationary points are ubiquitous in the loss landscape of most neural networks. We present an intuitive concept of bypassing the stationary points and realize the concept into a novel method designed to actively rescue optimizers from slowdowns encountered in neural network training. The method, bypass pipeline, revitalizes the optimizer by extending the model space and later contracts the model back to its original space with function-preserving algebraic constraints. We implement the method into the bypass algorithm, verify that the algorithm shows theoretically expected behaviors of bypassing, and demonstrate its empirical benefit in regression and classification benchmarks. Bypass algorithm is highly practical, as it is computationally efficient and compatible with other improvements of first-order optimizers. In addition, bypassing for neural networks leads to new theoretical research such as model-specific bypassing and neural architecture search (NAS).},
  archive      = {J_TNNLS},
  author       = {Jaeheun Jung and Donghun Lee},
  doi          = {10.1109/TNNLS.2024.3411020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18859-18871},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bypassing stationary points in training deep learning models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The deep promotion time cure model. <em>TNNLS</em>,
<em>35</em>(12), 18848–18858. (<a
href="https://doi.org/10.1109/TNNLS.2024.3398559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method for predicting time-to-event data in the presence of cure fractions based on flexible survival models integrated into a deep neural network (DNN) framework. Our approach allows for nonlinear relationships and high-dimensional interactions between covariates and survival and is suitable for large-scale applications. To ensure the identifiability of the overall predictor formed of an additive decomposition of interpretable linear and nonlinear effects and potential higher-dimensional interactions captured through a DNN, we employ an orthogonalization layer. We demonstrate the usefulness and computational efficiency of our method via simulations and apply it to a large portfolio of U.S. mortgage loans. Here, we find not only a better predictive performance of our framework but also a more realistic picture of covariate effects.},
  archive      = {J_TNNLS},
  author       = {Victor Medina-Olivares and Stefan Lessmann and Nadja Klein},
  doi          = {10.1109/TNNLS.2024.3398559},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18848-18858},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The deep promotion time cure model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding double descent using VC-theoretical framework.
<em>TNNLS</em>, <em>35</em>(12), 18838–18847. (<a
href="https://doi.org/10.1109/TNNLS.2024.3388873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spite of many successful applications of deep learning (DL) networks, theoretical understanding of their generalization capabilities and limitations remains limited. We present analysis of generalization performance of DL networks for classification under VC-theoretical framework. In particular, we analyze the so-called “double descent” phenomenon, when large overparameterized networks can generalize well, even when they perfectly memorize all available training data. This appears to contradict conventional statistical view that optimal model complexity should reflect an optimal balance between underfitting and overfitting, i.e., the bias–variance trade-off. We present VC-theoretical explanation of double descent phenomenon, under classification setting. Our theoretical explanation is supported by empirical modeling of double descent curves, using analytic VC-bounds, for several learning methods, such as support vector machine (SVM), least squares (LS), and multilayer perceptron classifiers. The proposed VC-theoretical approach enables better understanding of overparameterized estimators during second descent.},
  archive      = {J_TNNLS},
  author       = {Eng Hock Lee and Vladimir Cherkassky},
  doi          = {10.1109/TNNLS.2024.3388873},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18838-18847},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding double descent using VC-theoretical framework},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-based semi-supervised deep image clustering with
adaptive adjacency matrix. <em>TNNLS</em>, <em>35</em>(12), 18828–18837.
(<a href="https://doi.org/10.1109/TNNLS.2024.3367322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image clustering is a research hotspot in machine learning and computer vision. Existing graph-based semi-supervised deep clustering methods suffer from three problems: 1) because clustering uses only high-level features, the detailed information contained in shallow-level features is ignored; 2) most feature extraction networks employ the step odd convolutional kernel, which results in an uneven distribution of receptive field intensity; and 3) because the adjacency matrix is precomputed and fixed, it cannot adapt to changes in the relationship between samples. To solve the above problems, we propose a novel graph-based semi-supervised deep clustering method for image clustering. First, the parity cross-convolutional feature extraction and fusion module is used to extract high-quality image features. Then, the clustering constraint layer is designed to improve the clustering efficiency. And, the output layer is customized to achieve unsupervised regularization training. Finally, the adjacency matrix is inferred by actual network prediction. A graph-based regularization method is adopted for unsupervised training networks. Experimental results show that our method significantly outperforms state-of-the-art methods on USPS, MNIST, street view house numbers (SVHN), and fashion MNIST (FMNIST) datasets in terms of ACC, normalized mutual information (NMI), and ARI.},
  archive      = {J_TNNLS},
  author       = {Shifei Ding and Haiwei Hou and Xiao Xu and Jian Zhang and Lili Guo and Ling Ding},
  doi          = {10.1109/TNNLS.2024.3367322},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18828-18837},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-based semi-supervised deep image clustering with adaptive adjacency matrix},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Merge loss calculation method for highly imbalanced data
multiclass classification. <em>TNNLS</em>, <em>35</em>(12), 18814–18827.
(<a href="https://doi.org/10.1109/TNNLS.2023.3321753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real classification scenarios, the number distribution of modeling samples is usually out of proportion. Most of the existing classification methods still face challenges in comprehensive model performance for imbalanced data. In this article, a novel theoretical framework is proposed that establishes a proportion coefficient independent of the number distribution of modeling samples and a general merge loss calculation method independent of class distribution. The loss calculation method of the imbalanced problem focuses on both the global and batch sample levels. Specifically, the loss function calculation introduces the true-positive rate (TPR) and the false-positive rate (FPR) to ensure the independence and balance of loss calculation for each class. Based on this, global and local loss weight coefficients are generated from the entire dataset and batch dataset for the multiclass classification problem, and a merge weight loss function is calculated after unifying the weight coefficient scale. Furthermore, the designed loss function is applied to different neural network models and datasets. The method shows better performance on imbalanced datasets than state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zehua Du and Hao Zhang and Zhiqiang Wei and Yuanyuan Zhu and Jiali Xu and Xianqing Huang and Bo Yin},
  doi          = {10.1109/TNNLS.2023.3321753},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18814-18827},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Merge loss calculation method for highly imbalanced data multiclass classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regression-based hyperparameter learning for support vector
machines. <em>TNNLS</em>, <em>35</em>(12), 18799–18813. (<a
href="https://doi.org/10.1109/TNNLS.2023.3321685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unification of classification and regression is a major challenge in machine learning and has attracted increasing attentions from researchers. In this article, we present a new idea for this challenge, where we convert the classification problem into a regression problem, and then use the methods in regression to solve the problem in classification. To this end, we leverage the widely used maximum margin classification algorithm and its typical representative, support vector machine (SVM). More specifically, we convert SVM into a piecewise linear regression task and propose a regression-based SVM (RBSVM) hyperparameter learning algorithm, where regression methods are used to solve several key problems in classification, such as learning of hyperparameters, calculation of prediction probabilities, and measurement of model uncertainty. To analyze the uncertainty of the model, we propose a new concept of model entropy, where the leave-one-out prediction probability of each sample is converted into entropy, and then used to quantify the uncertainty of the model. The model entropy is different from the classification margin, in the sense that it considers the distribution of all samples, not just the support vectors. Therefore, it can assess the uncertainty of the model more accurately than the classification margin. In the case of the same classification margin, the farther the sample distribution is from the classification hyperplane, the lower the model entropy. Experiments show that our algorithm (RBSVM) provides higher prediction accuracy and lower model uncertainty, when compared with state-of-the-art algorithms, such as Bayesian hyperparameter search and gradient-based hyperparameter learning algorithms.},
  archive      = {J_TNNLS},
  author       = {Shili Peng and Wenwu Wang and Yinli Chen and Xueling Zhong and Qinghua Hu},
  doi          = {10.1109/TNNLS.2023.3321685},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18799-18813},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Regression-based hyperparameter learning for support vector machines},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Toward robust discriminative projections learning against
adversarial patch attacks. <em>TNNLS</em>, <em>35</em>(12), 18784–18798.
(<a href="https://doi.org/10.1109/TNNLS.2023.3321606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most popular supervised dimensionality reduction methods, linear discriminant analysis (LDA) has been widely studied in machine learning community and applied to many scientific applications. Traditional LDA minimizes the ratio of squared $\ell _{2}$ norms, which is vulnerable to the adversarial examples. In recent studies, many $\ell _{1}$ -norm-based robust dimensionality reduction methods are proposed to improve the robustness of model. However, due to the difficulty of $\ell _{1}$ -norm ratio optimization and weakness on defending a large number of adversarial examples, so far, scarce works have been proposed to utilize sparsity-inducing norms for LDA objective. In this article, we propose a novel robust discriminative projections learning (rDPL) method based on the $\ell _{1,2}$ -norm trace-ratio minimization optimization algorithm. Minimizing the $\ell _{1,2}$ -norm ratio problem directly is a much more challenging problem than the traditional methods, and there is no existing optimization algorithm to solve such nonsmooth terms ratio problem. We derive a new efficient algorithm to solve this challenging problem and provide a theoretical analysis on the convergence of our algorithm. The proposed algorithm is easy to implement and converges fast in practice. Extensive experiments on both synthetic data and several real benchmark datasets show the effectiveness of the proposed method on defending the adversarial patch attack by comparison with many state-of-the-art robust dimensionality reduction methods.},
  archive      = {J_TNNLS},
  author       = {Zheng Wang and Feiping Nie and Hua Wang and Heng Huang and Fei Wang},
  doi          = {10.1109/TNNLS.2023.3321606},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18784-18798},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward robust discriminative projections learning against adversarial patch attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive neural control of constrained MIMO nonlinear
systems with asymmetric input saturation and dead zone. <em>TNNLS</em>,
<em>35</em>(12), 18771–18783. (<a
href="https://doi.org/10.1109/TNNLS.2023.3321596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the adaptive neural control is studied for multiple-input–multiple-output (MIMO) nonlinear systems with asymmetric input saturation, dead zone, and full state-function constraints. A suitable transformation is introduced to overcome the dead zone and saturation nonlinearity, and radial basis function (RBF) neural networks (NNs) are used to approximate the unknown nonlinear functions. What is more, we apply the Nussbaum function and time-varying barrier Lyapunov function (BLF) to deal with the unknown control gains and full state-function constraints, respectively. Based on the backstepping method, a universal adaptive neural control scheme is presented such that not only the state-function constraints of the closed-loop system cannot be violated and all signals of the closed-loop systems are bounded, but also the tracking error converges to a small neighborhood containing the origin. The effectiveness of the proposed control scheme is verified by an application to the mass-spring-damper system and a numerical example.},
  archive      = {J_TNNLS},
  author       = {Zhibao Song and Lihong Gao and Zhen Wang and Ping Li},
  doi          = {10.1109/TNNLS.2023.3321596},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18771-18783},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural control of constrained MIMO nonlinear systems with asymmetric input saturation and dead zone},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Conditional goal-oriented trajectory prediction for
interacting vehicles. <em>TNNLS</em>, <em>35</em>(12), 18758–18770. (<a
href="https://doi.org/10.1109/TNNLS.2023.3321564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting future trajectories of pairwise traffic agents in highly interactive scenarios, such as cut-in, yielding, and merging, is challenging for autonomous driving. The existing works either treat such a problem as a marginal prediction task or perform single-axis factorized joint prediction, where the former strategy produces individual predictions without considering future interaction, while the latter strategy conducts conditional trajectory-oriented prediction via agentwise interaction or achieves conditional rollout-oriented prediction via timewise interaction. In this article, we propose a novel double-axis factorized joint prediction pipeline, namely, conditional goal-oriented trajectory prediction (CGTP) framework, which models future interaction both along the agent and time axes to achieve goal and trajectory interactive prediction. First, a goals-of-interest network (GoINet) is designed to extract fine-grained features of goal candidates via hierarchical vectorized representation. Furthermore, we propose a conditional goal prediction network (CGPNet) to produce multimodal goal pairs in an agentwise conditional manner, along with a newly designed goal interactive loss to better learn the joint distribution of the intermediate interpretable modes. Explicitly guided by the goal-pair predictions, we propose a goal-oriented trajectory rollout network (GTRNet) to predict scene-compliant trajectory pairs via timewise interactive rollouts. Extensive experimental results confirm that the proposed CGTP outperforms the state-of-the-art (SOTA) prediction models on the Waymo open motion dataset (WOMD), Argoverse motion forecasting dataset, and In-house cut-in dataset. Code is available at https://github.com/LiDinga/CGTP/ .},
  archive      = {J_TNNLS},
  author       = {Ding Li and Qichao Zhang and Shuai Lu and Yifeng Pan and Dongbin Zhao},
  doi          = {10.1109/TNNLS.2023.3321564},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18758-18770},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Conditional goal-oriented trajectory prediction for interacting vehicles},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multistability and robustness of competitive neural networks
with time-varying delays. <em>TNNLS</em>, <em>35</em>(12), 18746–18757.
(<a href="https://doi.org/10.1109/TNNLS.2023.3321434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is devoted to analyzing the multistability and robustness of competitive neural networks (NNs) with time-varying delays. Based on the geometrical structure of activation functions, some sufficient conditions are proposed to ascertain the coexistence of $\prod _{i=1}^{n}(2R_{i}+1)$ equilibrium points, $\prod _{i=1}^{n}(R_{i}+1)$ of them are locally exponentially stable, where $n$ represents a dimension of system and $R_{i}$ is the parameter related to activation functions. The derived stability results not only involve exponential stability but also include power stability and logarithmical stability. In addition, the robustness of $\prod _{i=1}^{n}(R_{i}+1)$ stable equilibrium points is discussed in the presence of perturbations. Compared with previous papers, the conclusions proposed in this article are easy to verify and enrich the existing stability theories of competitive NNs. Finally, numerical examples are provided to support theoretical results.},
  archive      = {J_TNNLS},
  author       = {Song Zhu and Jiahui Zhang and Xiaoyang Liu and Mouquan Shen and Shiping Wen and Chaoxu Mu},
  doi          = {10.1109/TNNLS.2023.3321434},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18746-18757},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multistability and robustness of competitive neural networks with time-varying delays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMS-net: Modeling adaptive multi-granularity spatio-temporal
cues for video action recognition. <em>TNNLS</em>, <em>35</em>(12),
18731–18745. (<a
href="https://doi.org/10.1109/TNNLS.2023.3321141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective spatio-temporal modeling as a core of video representation learning is challenged by complex scale variations in spatio-temporal cues in videos, especially different visual tempos of actions and varying spatial sizes of moving objects. Most of the existing works handle complex spatio-temporal scale variations based on input-level or feature-level pyramid mechanisms, which, however, rely on expensive multistream architectures or explore multiscale spatio-temporal features in a fixed manner. To effectively capture complex scale dynamics of spatio-temporal cues in an efficient way, this article proposes a single-stream architecture (SS-Arch.) with single-input [namely, adaptive multi-granularity spatio-temporal network (AMS-Net)] to model adaptive multi-granularity (Multi-Gran.) Spatio-temporal cues for video action recognition. To this end, our AMS-Net proposes two core components, namely, competitive progressive temporal modeling (CPTM) block and collaborative spatio-temporal pyramid (CSTP) module. They, respectively, capture fine-grained temporal cues and fuse coarse-level spatio-temporal features in an adaptive manner. It admits that AMS-Net can handle subtle variations in visual tempos and fair-sized spatio-temporal dynamics in a unified architecture. Note that our AMS-Net can be flexibly instantiated based on existing deep convolutional neural networks (CNNs) with the proposed CPTM block and CSTP module. The experiments are conducted on eight video benchmarks, and the results show our AMS-Net establishes state-of-the-art (SOTA) performance on fine-grained action recognition (i.e., Diving48 and FineGym), while performing very competitively on widely used Something-Something and Kinetics.},
  archive      = {J_TNNLS},
  author       = {Qilong Wang and Qiyao Hu and Zilin Gao and Peihua Li and Qinghua Hu},
  doi          = {10.1109/TNNLS.2023.3321141},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18731-18745},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AMS-net: Modeling adaptive multi-granularity spatio-temporal cues for video action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learnable central similarity quantization for efficient
image and video retrieval. <em>TNNLS</em>, <em>35</em>(12), 18717–18730.
(<a href="https://doi.org/10.1109/TNNLS.2023.3321148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-dependent hashing methods aim to learn hash functions from the pairwise or triplet relationships among the data, which often lead to low efficiency and low collision rate by only capturing the local distribution of the data. To solve the limitation, we propose central similarity, in which the hash codes of similar data pairs are encouraged to approach a common center and those of dissimilar pairs to converge to different centers. As a new global similarity metric, central similarity can improve the efficiency and retrieval accuracy of hash learning. By introducing a new concept, hash centers, we principally formulate the computation of the proposed central similarity metric, in which the hash centers refer to a set of points scattered in the Hamming space with a sufficient mutual distance between each other. To construct well-separated hash centers, we provide two efficient methods: 1) leveraging the Hadamard matrix and Bernoulli distributions to generate data-independent hash centers and 2) learning data-dependent hash centers from data representations. Based on the proposed similarity metric and hash centers, we propose central similarity quantization (CSQ) that optimizes the central similarity between data points with respect to their hash centers instead of optimizing the local similarity to generate a high-quality deep hash function. We also further improve the CSQ with data-dependent hash centers, dubbed as CSQ with learnable center (CSQLC). The proposed CSQ and CSQLC are generic and applicable to image and video hashing scenarios. We conduct extensive experiments on large-scale image and video retrieval tasks, and the proposed CSQ yields noticeably boosted retrieval performance, i.e., 3%–20% in mean average precision (mAP) over the previous state-of-the-art methods, which also demonstrates that our methods can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs.},
  archive      = {J_TNNLS},
  author       = {Li Yuan and Tao Wang and Xiaopeng Zhang and Francis Eng Hock Tay and Zequn Jie and Yonghong Tian and Wei Liu and Jiashi Feng},
  doi          = {10.1109/TNNLS.2023.3321148},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18717-18730},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learnable central similarity quantization for efficient image and video retrieval},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Event-triggered learning-based fault accommodation for a
class of nonlinear interconnected systems. <em>TNNLS</em>,
<em>35</em>(12), 18702–18716. (<a
href="https://doi.org/10.1109/TNNLS.2023.3320227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a distributed learning-based fault accommodation scheme is proposed for a class of nonlinear interconnected systems under event-triggered communication of control and measurement signals. Process faults occurring in the local dynamics and/or propagated from interconnected neighboring subsystems are considered. An event-triggered nominal control law is used for each subsystem before detecting any fault occurrence in its dynamics. After fault detection, the corresponding event-triggered fault accommodation law is utilized to reconfigure the nominal control law with a neural-network-based adaptive learning scheme employed to estimate an ideal fault-tolerant control function online. Under the asynchronous controller reconfiguration mechanism for each subsystem, the closed-loop stability of the interconnected systems in different operating modes with the proposed event-triggered learning-based fault accommodation scheme is rigorously analyzed with the explicit stabilization condition and state upper bound derived in terms of event-triggering parameters, and the Zeno behavior is shown to be excluded. An interconnected inverted pendulum system is used to illustrate the proposed fault accommodation scheme.},
  archive      = {J_TNNLS},
  author       = {Dong Zhao and Xiaodong Zhang and Marios M. Polycarpou},
  doi          = {10.1109/TNNLS.2023.3320227},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18702-18716},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered learning-based fault accommodation for a class of nonlinear interconnected systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convergence analysis of online gradient method for
high-order neural networks and their sparse optimization.
<em>TNNLS</em>, <em>35</em>(12), 18687–18701. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the boundedness and convergence of the online gradient method with the smoothing group $L_{1/2}$ regularization for the sigma-pi-sigma neural network (SPSNN). This enhances the sparseness of the network and improves its generalization ability. For the original group $L_{1/2}$ regularization, the error function is nonconvex and nonsmooth, which can cause oscillation of the error function. To ameliorate this drawback, we propose a simple and effective smoothing technique, which can effectively eliminate the deficiency of the original group $L_{1/2}$ regularization. The group $L_{1/2}$ regularization effectively optimizes the network structure from two aspects redundant hidden nodes tending to zero and redundant weights of surviving hidden nodes in the network tending to zero. This article shows the strong and weak convergence results for the proposed method and proves the boundedness of weights. Experiment results clearly demonstrate the capability of the proposed method and the effectiveness of redundancy control. The simulation results are observed to support the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Qinwei Fan and Qian Kang and Jacek M. Zurada and Tingwen Huang and Dongpo Xu},
  doi          = {10.1109/TNNLS.2023.3319989},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18687-18701},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convergence analysis of online gradient method for high-order neural networks and their sparse optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable gain impulsive synchronization for discrete-time
delayed neural networks and its application in digital secure
communication. <em>TNNLS</em>, <em>35</em>(12), 18674–18686. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article revisits the problems of impulsive stabilization and impulsive synchronization of discrete-time delayed neural networks (DDNNs) in the presence of disturbance in the input channel. A new Lyapunov approach based on double Lyapunov functionals is introduced for analyzing exponential input-to-state stability (EISS) of discrete impulsive delayed systems. In the framework of double Lyapunov functionals, a pair of timer-dependent Lyapunov functionals are constructed for impulsive DDNNs. The pair of Lyapunov functionals can introduce more degrees of freedom that not only can be exploited to reduce the conservatism of the previous methods, but also make it possible to design variable gain impulsive controllers. New design criteria for impulsive stabilization and impulsive synchronization are derived in terms of linear matrix inequalities. Numerical results show that compared with the constant gain design technique, the proposed variable gain design technique can accept larger impulse intervals and equip the impulsive controllers with a stronger disturbance attenuation ability. Applications to digital signal encryption and image encryption are provided which validate the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Wu-Hua Chen and Yufan Chen and Wei Xing Zheng},
  doi          = {10.1109/TNNLS.2023.3319974},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18674-18686},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Variable gain impulsive synchronization for discrete-time delayed neural networks and its application in digital secure communication},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiview spectral clustering based on consensus neighbor
strategy. <em>TNNLS</em>, <em>35</em>(12), 18661–18673. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview spectral clustering, renowned for its spatial learning capability, has garnered significant attention in the data mining field. However, existing methods assume that the optimal consensus adjacency matrix is confined within the space spanned by each view’s adjacency matrix. This constraint restricts the feasible domain of the algorithm and hinders the exploration of the optimal consensus adjacency matrix. To address this limitation, we propose a novel and convex strategy, termed the consensus neighbor strategy, for learning the optimal consensus adjacency matrix. This approach constructs the optimal consensus adjacency matrix by capturing the consensus local structure of each sample across all views, thereby expanding the search space and facilitating the discovery of the optimal consensus adjacency matrix. Furthermore, we introduce the concept of a correlation measuring matrix to prevent trivial solution. We develop an efficient iterative algorithm to solve the resulting optimization problem, benefitting from the convex nature of our model, which ensures convergence to a global optimum. Experimental results on 16 multiview datasets demonstrate that our proposed algorithm surpasses state-of-the-art methods in terms of its robust consensus representation learning capability. The code of this article is uploaded to https://github.com/PhdJiayiTang/Consensus-Neighbor-Strategy.git .},
  archive      = {J_TNNLS},
  author       = {Jiayi Tang and Yuping Lai and Xinwang Liu},
  doi          = {10.1109/TNNLS.2023.3319823},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18661-18673},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview spectral clustering based on consensus neighbor strategy},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Propagation structure fusion for rumor detection based on
node-level contrastive learning. <em>TNNLS</em>, <em>35</em>(12),
18649–18660. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of social media, the rapid spread of rumors online has resulted in numerous negative effects on society and the economy. The methods for rumor detection have attracted great interest from both academia and industry. Given the widespread effectiveness of contrastive learning, many graph contrastive learning models for rumor detection have been proposed by using the event propagation structure as graph data. However, the existing contrastive models usually treat the propagation structure of other events similar to the anchor events as negative samples. While this design choice allows for discriminative learning, on the other hand, it also inevitably pushes apart semantically similar samples and, thus, degrades model performance. In this article, we propose a novel propagation fusion model called propagation structure fusion model based on node-level contrastive learning (PFNC) for rumor detection based on node-level contrastive learning. PFNC first obtains three augmented propagation structures by masking the text of each node in the propagation structure randomly and perturbing some edges in the propagation structure based on the importance of edges. Then, PFNC applies the node-level contrastive learning method between every two augmented propagation structures to prevent the samples with similar propagation structure from far away. Finally, a convolutional neural network (CNN)-based model is proposed to capture the relevant information that is consistent and supplementary among three augmented propagation structures by regarding the propagation structure of the event as a color picture, three augmented propagation structures as color channels, and each node as a pixel. The experimental results on real datasets show that the PFNC significantly outperforms the state-of-the-art models for rumor detection.},
  archive      = {J_TNNLS},
  author       = {Jiachen Ma and Yong Liu and Meng Han and Chunqiang Hu and Zhaojie Ju},
  doi          = {10.1109/TNNLS.2023.3319661},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18649-18660},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Propagation structure fusion for rumor detection based on node-level contrastive learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian hierarchical graph neural networks with uncertainty
feedback for trustworthy fault diagnosis of industrial processes.
<em>TNNLS</em>, <em>35</em>(12), 18635–18648. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) methods have been widely applied to intelligent fault diagnosis of industrial processes and achieved state-of-the-art performance. However, fault diagnosis with point estimate may provide untrustworthy decisions. Recently, Bayesian inference shows to be a promising approach to trustworthy fault diagnosis by quantifying the uncertainty of the decisions with a DL model. The uncertainty information is not involved in the training process, which does not help the learning of highly uncertain samples and has little effect on improving the fault diagnosis performance. To address this challenge, we propose a Bayesian hierarchical graph neural network (BHGNN) with an uncertainty feedback mechanism, which formulates a trustworthy fault diagnosis on the Bayesian DL (BDL) framework. Specifically, BHGNN captures the epistemic uncertainty and aleatoric uncertainty via a variational dropout approach and utilizes the uncertainty information of each sample to adjust the strength of the temporal consistency (TC) constraint for robust feature learning. Meanwhile, the BHGNN method models the process data as a hierarchical graph (HG) by leveraging the interaction-aware module and physical topology knowledge of the industrial process, which integrates data with domain knowledge to learn fault representation. Moreover, the experiments on a three-phase flow facility (TFF) and secure water treatment (SWaT) show superior and competitive performance in fault diagnosis and verify the trustworthiness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Dongyue Chen and Zongxia Xie and Ruonan Liu and Wenlong Yu and Qinghua Hu and Xianling Li and Steven X. Ding},
  doi          = {10.1109/TNNLS.2023.3319468},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18635-18648},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bayesian hierarchical graph neural networks with uncertainty feedback for trustworthy fault diagnosis of industrial processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SOUL-net: A sparse and low-rank unrolling network for
spectral CT image reconstruction. <em>TNNLS</em>, <em>35</em>(12),
18620–18634. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral computed tomography (CT) is an emerging technology, that generates a multienergy attenuation map for the interior of an object and extends the traditional image volume into a 4-D form. Compared with traditional CT based on energy-integrating detectors, spectral CT can make full use of spectral information, resulting in high resolution and providing accurate material quantification. Numerous model-based iterative reconstruction methods have been proposed for spectral CT reconstruction. However, these methods usually suffer from difficulties such as laborious parameter selection and expensive computational costs. In addition, due to the image similarity of different energy bins, spectral CT usually implies a strong low-rank prior, which has been widely adopted in current iterative reconstruction models. Singular value thresholding (SVT) is an effective algorithm to solve the low-rank constrained model. However, the SVT method requires a manual selection of thresholds, which may lead to suboptimal results. To relieve these problems, in this article, we propose a sparse and low-rank unrolling network (SOUL-Net) for spectral CT image reconstruction, that learns the parameters and thresholds in a data-driven manner. Furthermore, a Taylor expansion-based neural network backpropagation method is introduced to improve the numerical stability. The qualitative and quantitative results demonstrate that the proposed method outperforms several representative state-of-the-art algorithms in terms of detail preservation and artifact reduction.},
  archive      = {J_TNNLS},
  author       = {Xiang Chen and Wenjun Xia and Ziyuan Yang and Hu Chen and Yan Liu and Jiliu Zhou and Zhe Wang and Yang Chen and Bihan Wen and Yi Zhang},
  doi          = {10.1109/TNNLS.2023.3319408},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18620-18634},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SOUL-net: A sparse and low-rank unrolling network for spectral CT image reconstruction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pseudo-label guided structural discriminative subspace
learning for unsupervised feature selection. <em>TNNLS</em>,
<em>35</em>(12), 18605–18619. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new unsupervised feature selection method named pseudo-label guided structural discriminative subspace learning (PSDSL). Unlike the previous methods that perform the two stages independently, it introduces the construction of probability graph into the feature selection learning process as a unified general framework, and therefore the probability graph can be learned adaptively. Moreover, we design a pseudo-label guided learning mechanism, and combine the graph-based method and the idea of maximizing the between-class scatter matrix with the trace ratio to construct an objective function that can improve the discrimination of the selected features. Besides, the main existing strategies of selecting features are to employ $\ell _{2,1}$ -norm for feature selection, but this faces the challenges of sparsity limitations and parameter tuning. For addressing this issue, we employ the $\ell _{2,0} $ -norm constraint on the learned subspace to ensure the row sparsity of the model and make the selected feature more stable. Effective optimization strategy is given to solve such NP-hard problem with the determination of parameters and complexity analysis in theory. Ultimately, extensive experiments conducted on nine real-world datasets and three biological ScRNA-seq genes datasets verify the effectiveness of the proposed method on the data clustering downstream task.},
  archive      = {J_TNNLS},
  author       = {Zheng Wang and Yongjin Yuan and Rong Wang and Feiping Nie and Qinghua Huang and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3319372},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18605-18619},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pseudo-label guided structural discriminative subspace learning for unsupervised feature selection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Contextual learning in fourier complex field for VHR remote
sensing images. <em>TNNLS</em>, <em>35</em>(12), 18590–18604. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Very high-resolution (VHR) remote sensing (RS) image classification is the fundamental task for RS image analysis and understanding. Recently, Transformer-based models demonstrated outstanding potential for learning high-order contextual relationships from natural images with general resolution ( $\approx 224\times224$ pixels) and achieved remarkable results on general image classification tasks. However, the complexity of the naive Transformer grows quadratically with the increase in image size, which prevents Transformer-based models from VHR RS image ( $\geq 500\times500$ pixels) classification and other computationally expensive downstream tasks. To this end, we propose to decompose the expensive self-attention (SA) into real and imaginary parts via discrete Fourier transform (DFT) and, therefore, propose an efficient complex SA (CSA) mechanism. Benefiting from the conjugated symmetric property of DFT, CSA is capable to model the high-order contextual information with less than half computations of naive SA. To overcome the gradient explosion in Fourier complex field, we replace the Softmax function with the carefully designed Logmax function to normalize the attention map of CSA and stabilize the gradient propagation. By stacking various layers of CSA blocks, we propose the Fourier complex Transformer (FCT) model to learn global contextual information from VHR aerial images following the hierarchical manners. Universal experiments conducted on commonly used RS classification datasets demonstrate the effectiveness and efficiency of FCT, especially on VHR RS images. The source code of FCT will be available at https://github.com/Gao-xiyuan/FCT .},
  archive      = {J_TNNLS},
  author       = {Yan Zhang and Xiyuan Gao and Qingyan Duan and Jiaxu Leng and Xiao Pu and Xinbo Gao},
  doi          = {10.1109/TNNLS.2023.3319363},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18590-18604},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Contextual learning in fourier complex field for VHR remote sensing images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pixel-centric context perception network for camouflaged
object detection. <em>TNNLS</em>, <em>35</em>(12), 18576–18589. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to identify object pixels visually embedded in the background environment. Existing deep learning methods fail to utilize the context information around different pixels adequately and efficiently. In order to solve this problem, a novel pixel-centric context perception network (PCPNet) is proposed, the core of which is to customize the personalized context of each pixel based on the automatic estimation of its surroundings. Specifically, PCPNet first employs an elegant encoder equipped with the designed vital component generation (VCG) module to obtain a set of compact features rich in low-level spatial and high-level semantic information across multiple subspaces. Then, we present a parameter-free pixel importance estimation (PIE) function based on multiwindow information fusion. Object pixels with complex backgrounds will be assigned with higher PIE values. Subsequently, PIE is utilized to regularize the optimization loss. In this way, the network can pay more attention to those pixels with higher PIE values in the decoding stage. Finally, a local continuity refinement module (LCRM) is used to refine the detection results. Extensive experiments on four COD benchmarks, five salient object detection (SOD) benchmarks, and five polyp segmentation benchmarks demonstrate the superiority of PCPNet with respect to other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Ze Song and Xudong Kang and Xiaohui Wei and Shutao Li},
  doi          = {10.1109/TNNLS.2023.3319323},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18576-18589},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pixel-centric context perception network for camouflaged object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid network using dynamic graph convolution and temporal
self-attention for EEG-based emotion recognition. <em>TNNLS</em>,
<em>35</em>(12), 18565–18575. (<a
href="https://doi.org/10.1109/TNNLS.2023.3319315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electroencephalogram (EEG) signal has become a highly effective decoding target for emotion recognition and has garnered significant attention from researchers. Its spatial topological and time-dependent characteristics make it crucial to explore both spatial information and temporal information for accurate emotion recognition. However, existing studies often focus on either spatial or temporal aspects of EEG signals, neglecting the joint consideration of both perspectives. To this end, this article proposes a hybrid network consisting of a dynamic graph convolution (DGC) module and temporal self-attention representation (TSAR) module, which concurrently incorporates the representative knowledge of spatial topology and temporal context into the EEG emotion recognition task. Specifically, the DGC module is designed to capture the spatial functional relationships within the brain by dynamically updating the adjacency matrix during the model training process. Simultaneously, the TSAR module is introduced to emphasize more valuable time segments and extract global temporal features from EEG signals. To fully exploit the interactivity between spatial and temporal information, the hierarchical cross-attention fusion (H-CAF) module is incorporated to fuse the complementary information from spatial and temporal features. Extensive experimental results on the DEAP, SEED, and SEED-IV datasets demonstrate that the proposed method outperforms other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Cheng Cheng and Zikang Yu and Yong Zhang and Lin Feng},
  doi          = {10.1109/TNNLS.2023.3319315},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18565-18575},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hybrid network using dynamic graph convolution and temporal self-attention for EEG-based emotion recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Sampling efficient deep reinforcement learning through
preference-guided stochastic exploration. <em>TNNLS</em>,
<em>35</em>(12), 18553–18564. (<a
href="https://doi.org/10.1109/TNNLS.2023.3317628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic exploration is the key to the success of the deep $Q$ -network (DQN) algorithm. However, most existing stochastic exploration approaches either explore actions heuristically regardless of their $Q$ values or couple the sampling with $Q$ values, which inevitably introduce bias into the learning process. In this article, we propose a novel preference-guided $\epsilon $ -greedy exploration algorithm that can efficiently facilitate exploration for DQN without introducing additional bias. Specifically, we design a dual architecture consisting of two branches, one of which is a copy of DQN, namely, the $Q$ branch. The other branch, which we call the preference branch, learns the action preference that the DQN implicitly follows. We theoretically prove that the policy improvement theorem holds for the preference-guided $\epsilon $ -greedy policy and experimentally show that the inferred action preference distribution aligns with the landscape of corresponding $Q$ values. Intuitively, the preference-guided $\epsilon $ -greedy exploration motivates the DQN agent to take diverse actions, so that actions with larger $Q$ values can be sampled more frequently, and those with smaller $Q$ values still have a chance to be explored, thus encouraging the exploration. We comprehensively evaluate the proposed method by benchmarking it with well-known DQN variants in nine different environments. Extensive results confirm the superiority of our proposed method in terms of performance and convergence speed.},
  archive      = {J_TNNLS},
  author       = {Wenhui Huang and Cong Zhang and Jingda Wu and Xiangkun He and Jie Zhang and Chen Lv},
  doi          = {10.1109/TNNLS.2023.3317628},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18553-18564},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sampling efficient deep reinforcement learning through preference-guided stochastic exploration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic recognition of speakers for consent management by
contrastive embedding replay. <em>TNNLS</em>, <em>35</em>(12),
18538–18552. (<a
href="https://doi.org/10.1109/TNNLS.2023.3317493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voice assistants overhear conversations, and a consent management mechanism is required. Consent management can be implemented using speaker recognition. Users that do not give consent enroll their voice, and all their further recordings are discarded. Building speaker recognition-based consent management is challenging as dynamic registration, removal, and reregistration of speakers must be efficiently handled. This work proposes a consent management system addressing the aforementioned challenges. A contrastive-based training is applied to learn the underlying speaker equivariance inductive bias. The contrastive features for buckets of speakers are trained a few steps into each iteration and act as replay buffers. These features are progressively selected using a multi-strided random sampler for classification. Moreover, new methods for dynamic registration using a portion of old utterances, removal, and reregistration of speakers are proposed. The results verify memory efficiency and dynamic capabilities of the proposed methods and outperform the existing approaches from the literature in terms of convergence rate and number of required parameters.},
  archive      = {J_TNNLS},
  author       = {Arash Shahmansoori and Utz Roedig},
  doi          = {10.1109/TNNLS.2023.3317493},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18538-18552},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic recognition of speakers for consent management by contrastive embedding replay},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). SMONAC: Supervised multiobjective negative actor–critic for
sequential recommendation. <em>TNNLS</em>, <em>35</em>(12), 18525–18537.
(<a href="https://doi.org/10.1109/TNNLS.2023.3317353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research shows that the sole accuracy metric may lead to the homogeneous and repetitive recommendations for users and affect the long-term user engagement. Multiobjective reinforcement learning (RL) is a promising method to achieve a good balance in multiple objectives, including accuracy, diversity, and novelty. However, it has two deficiencies: neglecting the updating of negative action $Q$ values and limited regulation from the RL Q-networks to the (self-)supervised learning recommendation network. To address these disadvantages, we develop the supervised multiobjective negative actor–critic (SMONAC) algorithm, which includes a negative action update mechanism and multiobjective actor–critic mechanism. For the negative action update mechanism, several negative actions are randomly sampled during each time updating, and then, the offline RL approach is utilized to learn their $Q$ values. For the multiobjective actor–critic mechanism, accuracy, diversity, and novelty $Q$ values are integrated into the scalarized $Q$ value, which is used to criticize the supervised learning recommendation network. The comparative experiments are conducted on two real-world datasets, and the results demonstrate that the developed SMONAC achieves tremendous performance promotion, especially for the metrics of diversity and novelty.},
  archive      = {J_TNNLS},
  author       = {Fei Zhou and Biao Luo and Zhengke Wu and Tingwen Huang},
  doi          = {10.1109/TNNLS.2023.3317353},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18525-18537},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SMONAC: Supervised multiobjective negative Actor–Critic for sequential recommendation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). When broad learning system meets label noise learning: A
reweighting learning framework. <em>TNNLS</em>, <em>35</em>(12),
18512–18524. (<a
href="https://doi.org/10.1109/TNNLS.2023.3317255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broad learning system (BLS) is a novel neural network with efficient learning and expansion capacity, but it is sensitive to noise. Accordingly, the existing robust broad models try to suppress noise by assigning each sample an appropriate scalar weight to tune down the contribution of noisy samples in network training. However, they disregard the useful information of the noncorrupted elements hidden in the noisy samples, leading to unsatisfactory performance. To this end, a novel BLS with adaptive reweighting (BLS-AR) strategy is proposed in this article for the classification of data with label noise. Different from the previous works, the BLS-AR learns for each sample a weight vector rather than a scalar weight to indicate the noise degree of each element in the sample, which extends the reweighting strategy from sample level to element level. This enables the proposed network to precisely identify noisy elements and thus highlight the contribution of informative ones to train a more accurate representation model. Thanks to the separability of the model, the proposed network can be divided into several subnetworks, each of which can be trained efficiently. In addition, three corresponding incremental learning algorithms of the BLS-AR are developed for adding new samples or expanding the network. Substantial experiments are conducted to explicate the effectiveness and robustness of the proposed BLS-AR model.},
  archive      = {J_TNNLS},
  author       = {Licheng Liu and Junhao Chen and Bin Yang and Qiying Feng and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2023.3317255},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18512-18524},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {When broad learning system meets label noise learning: A reweighting learning framework},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mind reasoning manners: Enhancing type perception for
generalized zero-shot logical reasoning over text. <em>TNNLS</em>,
<em>35</em>(12), 18499–18511. (<a
href="https://doi.org/10.1109/TNNLS.2023.3317254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logical reasoning task involves diverse types of complex reasoning over text, based on the form of multiple-choice question answering (MCQA). Given the context, question and a set of options as the input, previous methods achieve superior performances on the full-data setting. However, the current benchmark dataset has the ideal assumption that the reasoning type distribution on the train split is close to the test split, which is inconsistent with many real application scenarios. To address it, there remain two problems to be studied: 1) how is the zero-shot capability of the models (train on seen types and test on unseen types)? and 2) how to enhance the perception of reasoning types for the models? For problem 1, we propose a new benchmark for generalized zero-shot logical reasoning, named ZsLR. It includes six splits based on the three type sampling strategies. For problem 2, a type-aware model TaCo is proposed. It utilizes the heuristic input reconstruction and builds a text graph with a global node. Incorporating graph reasoning and contrastive learning, TaCo can improve the type perception in the global representation. Extensive experiments on both the zero-shot and full-data settings prove the superiority of TaCo over the state-of-the-art (SOTA) methods. Also, we experiment and verify the generalization capability of TaCo on other logical reasoning dataset.},
  archive      = {J_TNNLS},
  author       = {Fangzhi Xu and Jun Liu and Qika Lin and Tianzhe Zhao and Jian Zhang and Lingling Zhang},
  doi          = {10.1109/TNNLS.2023.3317254},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18499-18511},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mind reasoning manners: Enhancing type perception for generalized zero-shot logical reasoning over text},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RAP vol: Robust adversary populations with volume diversity
measure. <em>TNNLS</em>, <em>35</em>(12), 18485–18498. (<a
href="https://doi.org/10.1109/TNNLS.2023.3317145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) algorithms have made remarkable achievements in various fields, but they are vulnerable to changes in environment dynamics. This vulnerability easily leads to poor generalization, low performance, and catastrophic failures in unseen environments, which severely hinders the application of DRL in real-world scenarios. The robustness via adversary populations (RAP) algorithm addresses this issue by introducing a population of adversaries that perturb the protagonist. However, the low data utilization efficiency and lack of population diversity greatly limit the generalization performance. This article proposes robust adversary populations with volume diversity measure (RAP Vol) to address these drawbacks. In the proposed joint adversarial training framework, we use the training data to update all adversaries rather than only a single adversary, leading to a higher data utilization efficiency and a fast convergence speed. In the proposed population diversity iterative improvement mechanism, the vectors representing adversaries span a high-dimensional region. The volume of this region is utilized to measure and enhance population diversity via its square. The ablation experiments have verified the effectiveness of our proposed method in improving the robustness against variations in environment dynamics. Also, the influence of various factors (such as adversary population size and diversity weight) on the robustness has been investigated.},
  archive      = {J_TNNLS},
  author       = {Jiachen Yang and Jipeng Zhang},
  doi          = {10.1109/TNNLS.2023.3317145},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18485-18498},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RAP vol: Robust adversary populations with volume diversity measure},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative camouflaged object detection: A large-scale
dataset and benchmark. <em>TNNLS</em>, <em>35</em>(12), 18470–18484. (<a
href="https://doi.org/10.1109/TNNLS.2023.3317091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we provide a comprehensive study of a new task called collaborative camouflaged object detection (CoCOD), which aims to simultaneously detect camouflaged objects with the same properties from a group of relevant images. To this end, we meticulously construct the first large-scale dataset, termed CoCOD8K, which consists of 8528 high-quality and elaborately selected images with object mask annotations, covering five superclasses and 70 subclasses. The dataset spans a wide range of natural and artificial camouflage scenes with diverse object appearances and backgrounds, making it a very challenging dataset for CoCOD. Besides, we propose the first baseline model for CoCOD, named bilateral-branch network (BBNet), which explores and aggregates co-camouflaged cues within a single image and between images within a group, respectively, for accurate camouflaged object detection (COD) in given images. This is implemented by an interimage collaborative feature exploration (CFE) module, an intraimage object feature search (OFS) module, and a local-global refinement (LGR) module. We benchmark 18 state-of-the-art (SOTA) models, including 12 COD algorithms and six CoSOD algorithms, on the proposed CoCOD8K dataset under five widely used evaluation metrics. Extensive experiments demonstrate the effectiveness of the proposed method and the significantly superior performance compared to other competitors. We hope that our proposed dataset and model will boost growth in the COD community. The dataset, model, and results will be available at: https://github.com/zc199823/BBNet-CoCOD .},
  archive      = {J_TNNLS},
  author       = {Cong Zhang and Hongbo Bi and Tian-Zhu Xiang and Ranwan Wu and Jinghui Tong and Xiufang Wang},
  doi          = {10.1109/TNNLS.2023.3317091},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18470-18484},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collaborative camouflaged object detection: A large-scale dataset and benchmark},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). INNAbstract: An INN-based abstraction method for large-scale
neural network verification. <em>TNNLS</em>, <em>35</em>(12),
18455–18469. (<a
href="https://doi.org/10.1109/TNNLS.2023.3316551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) have witnessed widespread deployment across various domains, including some safetycritical applications. In this regard, the demand for verifying means of such artificial intelligence techniques is more and more pressing. Nowadays, the development of evaluation approaches for NNs is a hot topic that is attracting considerable interest, and a number of verification methods have been proposed. Yet, a challenging issue for NN verification is pertaining to the scalability when some NNs of practical interest have to be evaluated. This work aims to present INNAbstract, an abstraction method to reduce the size of NNs, which leads to improving the scalability of NN verification and reachability analysis methods. This is achieved by merging neurons while ensuring that the obtained model (i.e., abstract model) overapproximates the original one. INNAbstract supports networks with numerous activation functions. In addition, we propose a heuristic for nodes’ selection to build more precise abstract models, in the sense that the outputs are closer to those of the original network. The experimental results illustrate the efficiency of the proposed approach compared to the existing relevant abstraction techniques. Furthermore, they demonstrate that INNAbstract can help the existing verification tools to be applied on larger networks while considering various activation functions.},
  archive      = {J_TNNLS},
  author       = {Fateh Boudardara and Abderraouf Boussif and Pierre-Jean Meyer and Mohamed Ghazel},
  doi          = {10.1109/TNNLS.2023.3316551},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18455-18469},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {INNAbstract: An INN-based abstraction method for large-scale neural network verification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deformable dynamic sampling and dynamic predictable mask
mining for image inpainting. <em>TNNLS</em>, <em>35</em>(12),
18445–18454. (<a
href="https://doi.org/10.1109/TNNLS.2023.3316123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image inpainting methods often produce artifacts that are caused by using vanilla convolution layers as building blocks that treat all image regions equally and generate holes at random locations with equal probability. This design does not differentiate the missing regions and valid regions in inference and does not consider the predictability of missing regions in training. To address these issues, we propose a deformable dynamic sampling (DDS) mechanism which is built on deformable convolutions (DCs), and a constraint is proposed to avoid the deformably sampled elements falling into the corrupted regions. Furthermore, to select both valid sample locations and suitable kernels dynamically, we equip DCs with content-aware dynamic kernel selection (DKS). In addition, to further encourage the DDS mechanism to find meaningful sampling locations, we propose to train the inpainting model with mined predictable regions as holes. During training, we jointly train a mask generator with the inpainting network to generate hole masks dynamically for each training sample. Thus, the mask generator can find large yet predictable missing regions as a better alternative to random masks. Extensive experiments demonstrate the advantages of our method over state-of-the-art methods qualitatively and quantitatively.},
  archive      = {J_TNNLS},
  author       = {Cai Cai and Yu Zeng and Shu Yang and Xu Jia and Huchuan Lu and You He},
  doi          = {10.1109/TNNLS.2023.3316123},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18445-18454},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deformable dynamic sampling and dynamic predictable mask mining for image inpainting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InitialGAN: A language GAN with completely random
initialization. <em>TNNLS</em>, <em>35</em>(12), 18431–18444. (<a
href="https://doi.org/10.1109/TNNLS.2023.3315778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text generative models trained via maximum likelihood estimation (MLE) suffer from the notorious exposure bias problem, and generative adversarial networks (GANs) are shown to have potential to tackle this problem. The existing language GANs adopt estimators, such as REINFORCE or continuous relaxations to model word probabilities. The inherent limitations of such estimators lead current models to rely on pretraining techniques (MLE pretraining or pretrained embeddings). Representation modeling methods (RMMs), which are free from those limitations, however, are seldomly explored because of their poor performance in previous attempts. Our analyses reveal that invalid sampling methods and unhealthy gradients are the main contributors to such unsatisfactory performance. In this work, we present two techniques to tackle these problems: dropout sampling and fully normalized long short-term memory network (LSTM). Based on these two techniques, we propose InitialGAN whose parameters are randomly initialized in full. Besides, we introduce a new evaluation metric, least coverage rate (LCR), to better evaluate the quality of generated samples. The experimental results demonstrate that the InitialGAN outperforms both MLE and other compared models. To the best of our knowledge, it is the first time a language GAN can outperform MLE without using any pretraining techniques.},
  archive      = {J_TNNLS},
  author       = {Da Ren and Qing Li},
  doi          = {10.1109/TNNLS.2023.3315778},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18431-18444},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {InitialGAN: A language GAN with completely random initialization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient correction for white-box adversarial attacks.
<em>TNNLS</em>, <em>35</em>(12), 18419–18430. (<a
href="https://doi.org/10.1109/TNNLS.2023.3315414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) play key roles in various artificial intelligence applications such as image classification and object recognition. However, a growing number of studies have shown that there exist adversarial examples in DNNs, which are almost imperceptibly different from the original samples but can greatly change the output of DNNs. Recently, many white-box attack algorithms have been proposed, and most of the algorithms concentrate on how to make the best use of gradients per iteration to improve adversarial performance. In this article, we focus on the properties of the widely used activation function, rectified linear unit (ReLU), and find that there exist two phenomena (i.e., wrong blocking and over transmission) misguiding the calculation of gradients for ReLU during backpropagation. Both issues enlarge the difference between the predicted changes of the loss function from gradients and corresponding actual changes and misguide the optimized direction, which results in larger perturbations. Therefore, we propose a universal gradient correction adversarial example generation method, called ADV-ReLU, to enhance the performance of gradient-based white-box attack algorithms such as fast gradient signed method (FGSM), iterative FGSM (I-FGSM), momentum I-FGSM (MI-FGSM), and variance tuning MI-FGSM (VMI-FGSM). Through backpropagation, our approach calculates the gradient of the loss function with respect to the network input, maps the values to scores, and selects a part of them to update the misguided gradients. Comprehensive experimental results on ImageNet and CIFAR10 demonstrate that our ADV-ReLU can be easily integrated into many state-of-the-art gradient-based white-box attack algorithms, as well as transferred to black-box attacks, to further decrease perturbations measured in the ${\ell _{2}}$ -norm.},
  archive      = {J_TNNLS},
  author       = {Hongying Liu and Zhijin Ge and Zhenyu Zhou and Fanhua Shang and Yuanyuan Liu and Licheng Jiao},
  doi          = {10.1109/TNNLS.2023.3315414},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18419-18430},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gradient correction for white-box adversarial attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HQG-net: Unpaired medical image enhancement with
high-quality guidance. <em>TNNLS</em>, <em>35</em>(12), 18404–18418. (<a
href="https://doi.org/10.1109/TNNLS.2023.3315307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unpaired medical image enhancement (UMIE) aims to transform a low-quality (LQ) medical image into a high-quality (HQ) one without relying on paired images for training. While most existing approaches are based on Pix2Pix/CycleGAN and are effective to some extent, they fail to explicitly use HQ information to guide the enhancement process, which can lead to undesired artifacts and structural distortions. In this article, we propose a novel UMIE approach that avoids the above limitation of existing methods by directly encoding HQ cues into the LQ enhancement process in a variational fashion and thus model the UMIE task under the joint distribution between the LQ and HQ domains. Specifically, we extract features from an HQ image and explicitly insert the features, which are expected to encode HQ cues, into the enhancement network to guide the LQ enhancement with the variational normalization module. We train the enhancement network adversarially with a discriminator to ensure the generated HQ image falls into the HQ domain. We further propose a content-aware loss to guide the enhancement process with wavelet-based pixel-level and multiencoder-based feature-level constraints. Additionally, as a key motivation for performing image enhancement is to make the enhanced images serve better for downstream tasks, we propose a bi-level learning scheme to optimize the UMIE task and downstream tasks cooperatively, helping generate HQ images both visually appealing and favorable for downstream tasks. Experiments on three medical datasets verify that our method outperforms existing techniques in terms of both enhancement quality and downstream task performance. The code and the newly collected datasets are publicly available at https://github.com/ChunmingHe/HQG-Net .},
  archive      = {J_TNNLS},
  author       = {Chunming He and Kai Li and Guoxia Xu and Jiangpeng Yan and Longxiang Tang and Yulun Zhang and Yaowei Wang and Xiu Li},
  doi          = {10.1109/TNNLS.2023.3315307},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18404-18418},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HQG-net: Unpaired medical image enhancement with high-quality guidance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic gain fixed-time robust ZNN model for time-variant
equality constrained quaternion least squares problem with applications
to multiagent systems. <em>TNNLS</em>, <em>35</em>(12), 18394–18403. (<a
href="https://doi.org/10.1109/TNNLS.2023.3315332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dynamic gain fixed-time (FXT) robust zeroing neural network (DFTRZNN) model is proposed to effectively solve time-variant equality constrained quaternion least squares problem (TV-EQLS). The proposed approach surmounts the shortcomings of conventional numerical algorithms which fail to address time-variant problems. The DFTRZNN model is constructed with a novel dynamic gain parameter and a novel activation function (NAF), which differs from previous zeroing neural network (ZNN) models. Moreover, the comprehensive theoretical derivation of the FXT stability and robustness of the DFTRZNN model is presented in detail. Simulation results further confirm the availability and superiority of the DFTRZNN model for solving TV-EQLS. Finally, the consensus protocols of multiagent systems are presented by utilizing the design scheme of the DFTRZNN model, which further demonstrates its practical application value.},
  archive      = {J_TNNLS},
  author       = {Penglin Cao and Lin Xiao and Yongjun He and Jichun Li},
  doi          = {10.1109/TNNLS.2023.3315332},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18394-18403},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A dynamic gain fixed-time robust ZNN model for time-variant equality constrained quaternion least squares problem with applications to multiagent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Accurate lung nodule segmentation with detailed
representation transfer and soft mask supervision. <em>TNNLS</em>,
<em>35</em>(12), 18381–18393. (<a
href="https://doi.org/10.1109/TNNLS.2023.3315271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate lung lesion segmentation from computed tomography (CT) images is crucial to the analysis and diagnosis of lung diseases, such as COVID-19 and lung cancer. However, the smallness and variety of lung nodules and the lack of high-quality labeling make the accurate lung nodule segmentation difficult. To address these issues, we first introduce a novel segmentation mask named “ soft mask,” which has richer and more accurate edge details description and better visualization, and develop a universal automatic soft mask annotation pipeline to deal with different datasets correspondingly. Then, a novel network with detailed representation transfer and soft mask supervision (DSNet) is proposed to process the input low-resolution images of lung nodules into high-quality segmentation results. Our DSNet contains a special detailed representation transfer module (DRTM) for reconstructing the detailed representation to alleviate the small size of lung nodules images and an adversarial training framework with soft mask for further improving the accuracy of segmentation. Extensive experiments validate that our DSNet outperforms other state-of-the-art methods for accurate lung nodule segmentation, and has strong generalization ability in other accurate medical segmentation tasks with competitive results. Besides, we provide a new challenging lung nodules segmentation dataset for further studies ( https://drive.google.com/file/d/15NNkvDTb_0Ku0IoPsNMHezJRTH1Oi1wm/view?usp=sharing ).},
  archive      = {J_TNNLS},
  author       = {Changwei Wang and Rongtao Xu and Shibiao Xu and Weiliang Meng and Jun Xiao and Xiaopeng Zhang},
  doi          = {10.1109/TNNLS.2023.3315271},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18381-18393},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accurate lung nodule segmentation with detailed representation transfer and soft mask supervision},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Structure-aware graph attention diffusion network for
protein–ligand binding affinity prediction. <em>TNNLS</em>,
<em>35</em>(12), 18370–18380. (<a
href="https://doi.org/10.1109/TNNLS.2023.3314928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of protein–ligand binding affinities can significantly advance the development of drug discovery. Several graph neural network (GNN)-based methods learn representations of protein–ligand complexes via modeling intermolecule interactions and spatial structures (e.g., distances and angles) of complexes. However, these methods fail to emphasize the importance of bonds and learn hierarchical structures of complexes, which are significant for binding affinity prediction. In this article, we propose the structure-aware graph attention diffusion network (SGADN) to incorporate both distance and angle information for efficient spatial structure learning. We model complexes as line graphs with distance and angle information, focusing on bonds as nodes. Then we perform line graph attention diffusion layers (LGADLs) on line graphs to explore long-range bond node interactions and enhance spatial structure learning. Furthermore, we propose an attentive pooling layer (APL) to refine the hierarchical structures in complexes. Extensive experimental studies on two benchmarks demonstrate the superiority of SGADN for binding affinity prediction.},
  archive      = {J_TNNLS},
  author       = {Mei Li and Ye Cao and Xiaoguang Liu and Hua Ji},
  doi          = {10.1109/TNNLS.2023.3314928},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18370-18380},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Structure-aware graph attention diffusion network for Protein–Ligand binding affinity prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Template-free prompting for few-shot named entity
recognition via semantic-enhanced contrastive learning. <em>TNNLS</em>,
<em>35</em>(12), 18357–18369. (<a
href="https://doi.org/10.1109/TNNLS.2023.3314807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning has achieved great success in various sentence-level classification tasks by using elaborated label word mappings and prompt templates. However, for solving token-level classification tasks, e.g., named entity recognition (NER), previous research, which utilizes N-gram traversal for prompting all spans with all possible entity types, is time-consuming. To this end, we propose a novel prompt-based contrastive learning method for few-shot NER without template construction and label word mappings. First, we leverage external knowledge to initialize semantic anchors for each entity type. These anchors are simply appended with input sentence embeddings as template-free prompts (TFPs). Then, the prompts and sentence embeddings are in-context optimized with our proposed semantic-enhanced contrastive loss. Our proposed loss function enables contrastive learning in few-shot scenarios without requiring a significant number of negative samples. Moreover, it effectively addresses the issue of conventional contrastive learning, where negative instances with similar semantics are erroneously pushed apart in natural language processing (NLP)-related tasks. We examine our method in label extension (LE), domain-adaption (DA), and low-resource generalization evaluation tasks with six public datasets and different settings, achieving state-of-the-art (SOTA) results in most cases.},
  archive      = {J_TNNLS},
  author       = {Kai He and Rui Mao and Yucheng Huang and Tieliang Gong and Chen Li and Erik Cambria},
  doi          = {10.1109/TNNLS.2023.3314807},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18357-18369},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Template-free prompting for few-shot named entity recognition via semantic-enhanced contrastive learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D2CFR: Minimize counterfactual regret with deep dueling
neural network. <em>TNNLS</em>, <em>35</em>(12), 18343–18356. (<a
href="https://doi.org/10.1109/TNNLS.2023.3314638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual regret minimization (CFR) is a popular method for finding approximate Nash equilibrium in two-player zero-sum games with imperfect information. Solving large-scale games with CFR needs a combination of abstraction techniques and certain expert knowledge, which constrains its scalability. Recent neural-based CFR methods mitigate the need for abstraction and expert knowledge by training an efficient network to directly obtain counterfactual regret without abstraction. However, these methods only consider estimating regret values for individual actions, neglecting the evaluation of state values, which are significant for decision-making. In this article, we introduce deep dueling CFR (D2CFR), which emphasizes the state value estimation by employing a novel value network with a dueling structure. Moreover, a rectification module based on a time-shifted Monte Carlo simulation is designed to rectify the inaccurate state value estimation. Extensive experimental results are conducted to show that D2CFR converges faster and outperforms comparison methods on test games.},
  archive      = {J_TNNLS},
  author       = {Huale Li and Xuan Wang and Zengyue Guo and Jiajia Zhang and Shuhan Qi},
  doi          = {10.1109/TNNLS.2023.3314638},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18343-18356},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {D2CFR: Minimize counterfactual regret with deep dueling neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the real-world adversarial robustness of real-time
semantic segmentation models for autonomous driving. <em>TNNLS</em>,
<em>35</em>(12), 18328–18342. (<a
href="https://doi.org/10.1109/TNNLS.2023.3314512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of real-world adversarial examples (RWAEs) (commonly in the form of patches) poses a serious threat for the use of deep learning models in safety-critical computer vision tasks such as visual perception in autonomous driving. This article presents an extensive evaluation of the robustness of semantic segmentation (SS) models when attacked with different types of adversarial patches, including digital, simulated, and physical ones. A novel loss function is proposed to improve the capabilities of attackers in inducing a misclassification of pixels. Also, a novel attack strategy is presented to improve the expectation over transformation (EOT) method for placing a patch in the scene. Finally, a state-of-the-art method for detecting adversarial patch is first extended to cope with SS models, then improved to obtain real-time performance, and eventually evaluated in real-world scenarios. Experimental results reveal that even though the adversarial effect is visible with both digital and real-world attacks, its impact is often spatially confined to areas of the image around the patch. This opens to further questions about the spatial robustness of real-time SS models.},
  archive      = {J_TNNLS},
  author       = {Giulio Rossolini and Federico Nesti and Gianluca D’Amico and Saasha Nair and Alessandro Biondi and Giorgio Buttazzo},
  doi          = {10.1109/TNNLS.2023.3314512},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18328-18342},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the real-world adversarial robustness of real-time semantic segmentation models for autonomous driving},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redundancy-free self-supervised relational learning for
graph clustering. <em>TNNLS</em>, <em>35</em>(12), 18313–18327. (<a
href="https://doi.org/10.1109/TNNLS.2023.3314451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering, which learns the node representations for effective cluster assignments, is a fundamental yet challenging task in data analysis and has received considerable attention accompanied by graph neural networks (GNNs) in recent years. However, most existing methods overlook the inherent relational information among the nonindependent and nonidentically distributed nodes in a graph. Due to the lack of exploration of relational attributes, the semantic information of the graph-structured data fails to be fully exploited which leads to poor clustering performance. In this article, we propose a novel self-supervised deep graph clustering method named relational redundancy-free graph clustering (R2FGC) to tackle the problem. It extracts the attribute- and structure-level relational information from both global and local views based on an autoencoder (AE) and a graph AE (GAE). To obtain effective representations of the semantic information, we preserve the consistent relationship among augmented nodes, whereas the redundant relationship is further reduced for learning discriminative embeddings. In addition, a simple yet valid strategy is used to alleviate the oversmoothing issue. Extensive experiments are performed on widely used benchmark datasets to validate the superiority of our R2FGC over state-of-the-art baselines. Our codes are available at https://github.com/yisiyu95/R2FGC .},
  archive      = {J_TNNLS},
  author       = {Siyu Yi and Wei Ju and Yifang Qin and Xiao Luo and Luchen Liu and Yongdao Zhou and Ming Zhang},
  doi          = {10.1109/TNNLS.2023.3314451},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18313-18327},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Redundancy-free self-supervised relational learning for graph clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural improvement heuristics for graph combinatorial
optimization problems. <em>TNNLS</em>, <em>35</em>(12), 18300–18312. (<a
href="https://doi.org/10.1109/TNNLS.2023.3314375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in graph neural network (GNN) architectures and increased computation power have revolutionized the field of combinatorial optimization (CO). Among the proposed models for CO problems, neural improvement (NI) models have been particularly successful. However, the existing NI approaches are limited in their applicability to problems where crucial information is encoded in the edges, as they only consider node features and nodewise positional encodings (PEs). To overcome this limitation, we introduce a novel NI model capable of handling graph-based problems where information is encoded in the nodes, edges, or both. The presented model serves as a fundamental component for hill-climbing-based algorithms that guide the selection of neighborhood operations for each iteration. Conducted experiments demonstrate that the proposed model can recommend neighborhood operations that outperform conventional versions for the preference ranking problem (PRP) with a performance in the 99th percentile. We also extend the proposal to two well-known problems: the traveling salesman problem and the graph partitioning problem (GPP), recommending operations in the 98th and 97th percentile, respectively.},
  archive      = {J_TNNLS},
  author       = {Andoni I. Garmendia and Josu Ceberio and Alexander Mendiburu},
  doi          = {10.1109/TNNLS.2023.3314375},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18300-18312},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural improvement heuristics for graph combinatorial optimization problems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RadarFormer: End-to-end human perception with through-wall
radar and transformers. <em>TNNLS</em>, <em>35</em>(12), 18285–18299.
(<a href="https://doi.org/10.1109/TNNLS.2023.3314031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For fine-grained human perception tasks such as pose estimation and activity recognition, radar-based sensors show advantages over optical cameras in low-visibility, privacy-aware, and wall-occlusive environments. Radar transmits radio frequency signals to irradiate the target of interest and store the target information in the echo signals. One common approach is to transform the echoes into radar images and extract the features with convolutional neural networks. This article introduces RadarFormer, the first method that introduces the self-attention (SA) mechanism to perform human perception tasks directly from radar echoes. It bypasses the imaging algorithm and realizes end-to-end signal processing. Specifically, we give constructive proof that processing radar echoes using the SA mechanism is at least as expressive as processing radar images using the convolutional layer. On this foundation, we design RadarFormer, which is a Transformer-like model to process radar signals. It benefits from the fast-/slow-time SA mechanism considering the physical characteristics of radar signals. RadarFormer extracts human representations from radar echoes and handles various downstream human perception tasks. The experimental results demonstrate that our method outperforms the state-of-the-art radar-based methods both in performance and computational cost and obtains accurate human perception results even in dark and occlusive environments.},
  archive      = {J_TNNLS},
  author       = {Zhijie Zheng and Diankun Zhang and Xiao Liang and Xiaojun Liu and Guangyou Fang},
  doi          = {10.1109/TNNLS.2023.3314031},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18285-18299},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RadarFormer: End-to-end human perception with through-wall radar and transformers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative deep structural graph contrast clustering for
multiview raw data. <em>TNNLS</em>, <em>35</em>(12), 18272–18284. (<a
href="https://doi.org/10.1109/TNNLS.2023.3313692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering has attracted increasing attention to automatically divide instances into various groups without manual annotations. Traditional shadow methods discover the internal structure of data, while deep multiview clustering (DMVC) utilizes neural networks with clustering-friendly data embeddings. Although both of them achieve impressive performance in practical applications, we find that the former heavily relies on the quality of raw features, while the latter ignores the structure information of data. To address the above issue, we propose a novel method termed iterative deep structural graph contrast clustering (IDSGCC) for multiview raw data consisting of topology learning (TL), representation learning (RL), and graph structure contrastive learning to achieve better performance. The TL module aims to obtain a structured global graph with constraint structural information and then guides the RL to preserve the structural information. In the RL module, graph convolutional network (GCN) takes the global structural graph and raw features as inputs to aggregate the samples of the same cluster and keep the samples of different clusters away. Unlike previous methods performing contrastive learning at the representation level of the samples, in the graph contrastive learning module, we conduct contrastive learning at the graph structure level by imposing a regularization term on the similarity matrix. The credible neighbors of the samples are constructed as positive pairs through the credible graph, and other samples are constructed as negative pairs. The three modules promote each other and finally obtain clustering-friendly embedding. Also, we set up an iterative update mechanism to update the topology to obtain a more credible topology. Impressive clustering results are obtained through the iterative mechanism. Comparative experiments on eight multiview datasets show that our model outperforms the state-of-the-art traditional and deep clustering competitors.},
  archive      = {J_TNNLS},
  author       = {Zhibin Dong and Jiaqi Jin and Yuyang Xiao and Siwei Wang and Xinzhong Zhu and Xinwang Liu and En Zhu},
  doi          = {10.1109/TNNLS.2023.3313692},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18272-18284},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative deep structural graph contrast clustering for multiview raw data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Attention-based multimodal tCNN for classification of
steady-state visual evoked potentials and its application to gripper
control. <em>TNNLS</em>, <em>35</em>(12), 18263–18271. (<a
href="https://doi.org/10.1109/TNNLS.2023.3313691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification problem for short time-window steady-state visual evoked potentials (SSVEPs) is important in practical applications because shorter time-window often means faster response speed. By combining the advantages of the local feature learning ability of convolutional neural network (CNN) and the feature importance distinguishing ability of attention mechanism, a novel network called AttentCNN is proposed to further improve the classification performance for short time-window SSVEP. Considering the frequency-domain features extracted from short time-window signals are not obvious, this network starts with the time-domain feature extraction module based on the filter bank (FB). The FB consists of four sixth-order Butterworth filters with different bandpass ranges. Then extracted multimodal features are aggregated together. The second major module is a set of residual squeeze and excitation blocks (RSEs) that has the ability to improve the quality of extracted features by learning the interdependence between features. The final major module is time-domain CNN (tCNN) that consists of four CNNs for further feature extraction and followed by a fully connected (FC) layer for output. Our designed networks are validated over two large public datasets, and necessary comparisons are given to verify the effectiveness and superiority of the proposed network. In the end, in order to demonstrate the application potential of the proposed strategy in the medical rehabilitation field, we design a novel five-finger bionic hand and connect it to our trained network to achieve the control of bionic hand by human brain signals directly. Our source codes are available on Github: https://github.com/JiannanChen/AggtCNN.git .},
  archive      = {J_TNNLS},
  author       = {Jiannan Chen and Fuchun Sun and Wenjun Zhang and Shubin Zhang and Kai Liu and Chunpeng Qi},
  doi          = {10.1109/TNNLS.2023.3313691},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18263-18271},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention-based multimodal tCNN for classification of steady-state visual evoked potentials and its application to gripper control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distribution fitting for combating mode collapse in
generative adversarial networks. <em>TNNLS</em>, <em>35</em>(12),
18251–18262. (<a
href="https://doi.org/10.1109/TNNLS.2023.3313600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mode collapse is a significant unsolved issue of generative adversarial networks (GANs). In this work, we examine the causes of mode collapse from a novel perspective. Due to the nonuniform sampling in the training process, some subdistributions may be missed when sampling data. As a result, even when the generated distribution differs from the real one, the GAN objective can still achieve the minimum. To address the issue, we propose a global distribution fitting (GDF) method with a penalty term to confine the generated data distribution. When the generated distribution differs from the real one, GDF will make the objective harder to reach the minimal value, while the original global minimum is not changed. To deal with the circumstance when the overall real data is unreachable, we also propose a local distribution fitting (LDF) method. Experiments on several benchmarks demonstrate the effectiveness and competitive performance of GDF and LDF.},
  archive      = {J_TNNLS},
  author       = {Yanxiang Gong and Zhiwei Xie and Guozhen Duan and Zheng Ma and Mei Xie},
  doi          = {10.1109/TNNLS.2023.3313600},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18251-18262},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distribution fitting for combating mode collapse in generative adversarial networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Path following control for unmanned surface vehicles: A
reinforcement learning-based method with experimental validation.
<em>TNNLS</em>, <em>35</em>(12), 18237–18250. (<a
href="https://doi.org/10.1109/TNNLS.2023.3313312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a reinforcement learning (RL)-based strategy for unmanned surface vehicle (USV) path following control is developed. The proposed method learns integrated guidance and heading control policy, which directly maps the USV’s navigation states to motor control commands. By introducing a twin-critic design and an integral compensator to the conventional deep deterministic policy gradient (DDPG) algorithm, the tracking accuracy and robustness of the controller can be significantly improved. Moreover, a pretrained neural network-based USV model is built to help the learning algorithm efficiently deal with unknown nonlinear dynamics. The self-learning and path following capabilities of the proposed method were validated in both simulations and real sea experiments. The results show that our control policy can achieve better performance than a traditional cascade control policy and a DDPG-based control policy.},
  archive      = {J_TNNLS},
  author       = {Yuanda Wang and Jingyu Cao and Jia Sun and Xuesong Zou and Changyin Sun},
  doi          = {10.1109/TNNLS.2023.3313312},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18237-18250},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Path following control for unmanned surface vehicles: A reinforcement learning-based method with experimental validation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSTF-unet: Spatial–spectral transformer-based u-net for
high-resolution hyperspectral image acquisition. <em>TNNLS</em>,
<em>35</em>(12), 18222–18236. (<a
href="https://doi.org/10.1109/TNNLS.2023.3313202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To obtain a high-resolution hyperspectral image (HR-HSI), fusing a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI) is a prominent approach. Numerous approaches based on convolutional neural networks (CNNs) have been presented for hyperspectral image (HSI) and multispectral image (MSI) fusion. Nevertheless, these CNN-based methods may ignore the global relevant features from the input image due to the geometric limitations of convolutional kernels. To obtain more accurate fusion results, we provide a spatial–spectral transformer-based U-net (SSTF-Unet). Our SSTF-Unet can capture the association between distant features and explore the intrinsic information of images. More specifically, we use the spatial transformer block (SATB) and spectral transformer block (SETB) to calculate the spatial and spectral self-attention, respectively. Then, SATB and SETB are connected in parallel to form the spatial–spectral fusion block (SSFB). Inspired by the U-net architecture, we build up our SSTF-Unet through stacking several SSFBs for multiscale spatial–spectral feature fusion. Experimental results on public HSI datasets demonstrate that the designed SSTF-Unet achieves better performance than other existing HSI and MSI fusion approaches.},
  archive      = {J_TNNLS},
  author       = {Haibo Liu and Chenguo Feng and Renwei Dian and Shutao Li},
  doi          = {10.1109/TNNLS.2023.3313202},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18222-18236},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SSTF-unet: Spatial–Spectral transformer-based U-net for high-resolution hyperspectral image acquisition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge removal and q-learning for stabilizability of boolean
networks. <em>TNNLS</em>, <em>35</em>(12), 18212–18221. (<a
href="https://doi.org/10.1109/TNNLS.2023.3312942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a new edge removal mechanism for the global stabilizability of Boolean networks (BNs). In order to achieve the edge removal control, several control variables are properly placed into the dynamics of BNs based on the fundamental logical operators. On the basis of the new edge removal mechanism, several necessary and sufficient conditions are obtained for the global stabilizability and set stabilizability of BNs. Furthermore, a kind of stable edge removal control is proposed and achieved via the $Q$ -learning algorithm to optimize the edge removal mechanism. As an application, the edge removal control is used to verify whether or not the mammalian cortical area development model can be made stabilizable to the expected stable states.},
  archive      = {J_TNNLS},
  author       = {Wenrong Li and Haitao Li},
  doi          = {10.1109/TNNLS.2023.3312942},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18212-18221},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Edge removal and Q-learning for stabilizability of boolean networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic temperature parameter tuning for reinforcement
learning using path integral policy improvement. <em>TNNLS</em>,
<em>35</em>(12), 18200–18211. (<a
href="https://doi.org/10.1109/TNNLS.2023.3312857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel variant of path integral policy improvement with covariance matrix adaptation (PI2-CMA), which is a reinforcement learning (RL) algorithm that aims to optimize a parameterized policy for the continuous behavior of robots. PI2-CMA has a hyperparameter called the temperature parameter, and its value is critical for performance; however, little research has been conducted on it and the existing method still contains a tunable parameter, which can be critical to performance. Therefore, tuning by trial and error is necessary in the existing method. Moreover, we show that there is a problem setting that cannot be learned by the existing method. The proposed method solves both problems by automatically adjusting the temperature parameter for each update. We confirmed the effectiveness of the proposed method using numerical tests.},
  archive      = {J_TNNLS},
  author       = {Hiroyasu Nakano and Ryo Ariizumi and Toru Asai and Shun-Ichi Azuma},
  doi          = {10.1109/TNNLS.2023.3312857},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18200-18211},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic temperature parameter tuning for reinforcement learning using path integral policy improvement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond pattern variance: Unsupervised 3-d action
representation learning with point cloud sequence. <em>TNNLS</em>,
<em>35</em>(12), 18186–18199. (<a
href="https://doi.org/10.1109/TNNLS.2023.3312673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work pays the first research effort to address unsupervised 3-D action representation learning with point cloud sequence, which is different from existing unsupervised methods that rely on 3-D skeleton information. Our proposition is built on the state-of-the-art 3-D action descriptor 3-D dynamic voxel (3DV) with contrastive learning (CL). The 3DV can compress the point cloud sequence into a compact point cloud of 3-D motion information. Spatiotemporal data augmentations are conducted on it to drive CL. However, we find that existing CL methods (e.g., SimCLR or MoCo v2) often suffer from high pattern variance toward the augmented 3DV samples from the same action instance, that is, the augmented 3DV samples are still of high feature complementarity after CL, while the complementary discriminative clues within them have not been well exploited yet. To address this, a feature augmentation adapted CL (FACL) approach is proposed, which facilitates 3-D action representation via concerning the features from all augmented 3DV samples jointly, in spirit of feature augmentation. FACL runs in a global–local way: one branch learns global feature that involves the discriminative clues from the raw and augmented 3DV samples, and the other focuses on enhancing the discriminative power of local feature learned from each augmented 3DV sample. The global and local features are fused to characterize 3-D action jointly via concatenation. To fit FACL, a series of spatiotemporal data augmentation approaches is also studied on 3DV. Wide-range experiments verify the superiority of our unsupervised learning method for 3-D action feature learning. It outperforms the state-of-the-art skeleton-based counterparts by 6.4% and 3.6% with the cross-setup and cross-subject test settings on NTU RGB+D 120, respectively. The source code is available at https://github.com/tangent-T/FACL .},
  archive      = {J_TNNLS},
  author       = {Bo Tan and Yang Xiao and Yancheng Wang and Shuai Li and Jianyu Yang and Zhiguo Cao and Joey Tianyi Zhou and Junsong Yuan},
  doi          = {10.1109/TNNLS.2023.3312673},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18186-18199},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Beyond pattern variance: Unsupervised 3-D action representation learning with point cloud sequence},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARISE: Graph anomaly detection on attributed networks via
substructure awareness. <em>TNNLS</em>, <em>35</em>(12), 18172–18185.
(<a href="https://doi.org/10.1109/TNNLS.2023.3312655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph anomaly detection on attributed networks has attracted growing attention in data mining and machine learning communities. Apart from attribute anomalies, graph anomaly detection also aims at suspicious topological-abnormal nodes that exhibit collective anomalous behavior. Closely connected uncorrelated node groups form uncommonly dense substructures in the network. However, existing methods overlook that the topology anomaly detection performance can be improved by recognizing such a collective pattern. To this end, we propose a new graph anomaly detection framework on attributed networks via substructure awareness (ARISE). Unlike previous algorithms, we focus on the substructures in the graph to discern abnormalities. Specifically, we establish a region proposal module to discover high-density substructures in the network as suspicious regions. The average node-pair similarity can be regarded as the topology anomaly degree of nodes within substructures. Generally, the lower the similarity, the higher the probability that internal nodes are topology anomalies. To distill better embeddings of node attributes, we further introduce a graph contrastive learning scheme, which observes attribute anomalies in the meantime. In this way, ARISE can detect both topology and attribute anomalies. Ultimately, extensive experiments on benchmark datasets show that ARISE greatly improves detection performance (up to 7.30% AUC and 17.46% AUPRC gains) compared to state-of-the-art attributed networks anomaly detection (ANAD) algorithms.},
  archive      = {J_TNNLS},
  author       = {Jingcan Duan and Bin Xiao and Siwei Wang and Haifang Zhou and Xinwang Liu},
  doi          = {10.1109/TNNLS.2023.3312655},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18172-18185},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ARISE: Graph anomaly detection on attributed networks via substructure awareness},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph convolutional neural networks with diverse negative
samples via decomposed determinant point processes. <em>TNNLS</em>,
<em>35</em>(12), 18160–18171. (<a
href="https://doi.org/10.1109/TNNLS.2023.3312307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional neural networks (GCNs) have achieved great success in graph representation learning by extracting high-level features from nodes and their topology. Since GCNs generally follow a message-passing mechanism, each node aggregates information from its first-order neighbor to update its representation. As a result, the representations of nodes with edges between them should be positively correlated and thus can be considered positive samples. However, there are more non-neighbor nodes in the whole graph, which provide diverse and useful information for the representation update. Two non-adjacent nodes usually have different representations, which can be seen as negative samples. Besides the node representations, the structural information of the graph is also crucial for learning. In this article, we used quality-diversity decomposition in determinant point processes (DPPs) to obtain diverse negative samples. When defining a distribution on diverse subsets of all non-neighboring nodes, we incorporate both graph structure information and node representations. Since the DPP sampling process requires matrix eigenvalue decomposition, we propose a new shortest-path-base method to improve computational efficiency. Finally, we incorporate the obtained negative samples into the graph convolution operation. The ideas are evaluated empirically in experiments on node classification tasks. These experiments show that the newly proposed methods not only improve the overall performance of standard representation learning but also significantly alleviate over-smoothing problems.},
  archive      = {J_TNNLS},
  author       = {Wei Duan and Junyu Xuan and Maoying Qiao and Jie Lu},
  doi          = {10.1109/TNNLS.2023.3312307},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18160-18171},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph convolutional neural networks with diverse negative samples via decomposed determinant point processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid quantum-classical convolutional neural network model
for image classification. <em>TNNLS</em>, <em>35</em>(12), 18145–18159.
(<a href="https://doi.org/10.1109/TNNLS.2023.3312170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification plays an important role in remote sensing. Earth observation (EO) has inevitably arrived in the big data era, but the high requirement on computation power has already become a bottleneck for analyzing large amounts of remote sensing data with sophisticated machine learning models. Exploiting quantum computing might contribute to a solution to tackle this challenge by leveraging quantum properties. This article introduces a hybrid quantum-classical convolutional neural network (QC-CNN) that applies quantum computing to effectively extract high-level critical features from EO data for classification purposes. Besides that, the adoption of the amplitude encoding technique reduces the required quantum bit resources. The complexity analysis indicates that the proposed model can accelerate the convolutional operation in comparison with its classical counterpart. The model’s performance is evaluated with different EO benchmarks, including Overhead-MNIST, So2Sat LCZ42, PatternNet, RSI-CB256, and NaSC-TG2, through the TensorFlow Quantum platform, and it can achieve better performance than its classical counterpart and have higher generalizability, which verifies the validity of the QC-CNN model on EO data classification tasks.},
  archive      = {J_TNNLS},
  author       = {Fan Fan and Yilei Shi and Tobias Guggemos and Xiao Xiang Zhu},
  doi          = {10.1109/TNNLS.2023.3312170},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18145-18159},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hybrid quantum-classical convolutional neural network model for image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Concurrent learning-based adaptive control of underactuated
robotic systems with guaranteed transient performance for both actuated
and unactuated motions. <em>TNNLS</em>, <em>35</em>(12), 18133–18144.
(<a href="https://doi.org/10.1109/TNNLS.2023.3311927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide applications of underactuated robotic systems, more complex tasks and higher safety demands are put forward. However, it is still an open issue to utilize “fewer” control inputs to satisfy control accuracy and transient performance with theoretical and practical guarantee, especially for unactuated variables. To this end, for underactuated robotic systems, this article designs an adaptive tracking controller to realize exponential convergence results, rather than only asymptotic stability or boundedness; meanwhile, unactuated states exponentially converge to a small enough bound, which is adjustable by control gains. The maximum motion ranges and convergence speed of all variables both exhibit satisfactory performance with higher safety and efficiency. Here, a data-driven concurrent learning (CL) method is proposed to compensate for unknown dynamics/disturbances and improve the estimate accuracy of parameters/weights, without the need for persistency of excitation or linear parametrization (LP) conditions. Then, a disturbance judgment mechanism is utilized to eliminate the detrimental impacts of external disturbances. As far as we know, for general underactuated systems with uncertainties/disturbances, it is the first time to theoretically and practically ensure transient performance and exponential convergence speed for unactuated states, and simultaneously obtain the exponential tracking result of actuated motions. Both theoretical analysis and hardware experiment results illustrate the effectiveness of the designed controller.},
  archive      = {J_TNNLS},
  author       = {Tong Yang and Ning Sun and Zhuoqing Liu and Yongchun Fang},
  doi          = {10.1109/TNNLS.2023.3311927},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18133-18144},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Concurrent learning-based adaptive control of underactuated robotic systems with guaranteed transient performance for both actuated and unactuated motions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A lightweight pixel-level unified image fusion network.
<em>TNNLS</em>, <em>35</em>(12), 18120–18132. (<a
href="https://doi.org/10.1109/TNNLS.2023.3311820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep-learning-based pixel-level unified image fusion methods have received more and more attention due to their practicality and robustness. However, they usually require a complex network to achieve more effective fusion, leading to high computational cost. To achieve more efficient and accurate image fusion, a lightweight pixel-level unified image fusion (L-PUIF) network is proposed. Specifically, the information refinement and measurement process are used to extract the gradient and intensity information and enhance the feature extraction capability of the network. In addition, these information are converted into weights to guide the loss function adaptively. Thus, more effective image fusion can be achieved while ensuring the lightweight of the network. Extensive experiments have been conducted on four public image fusion datasets across multimodal fusion, multifocus fusion, and multiexposure fusion. Experimental results show that L-PUIF can achieve better fusion efficiency and has a greater visual effect compared with state-of-the-art methods. In addition, the practicability of L-PUIF in high-level computer vision tasks, i.e., object detection and image segmentation, has been verified.},
  archive      = {J_TNNLS},
  author       = {Jinyang Liu and Shutao Li and Haibo Liu and Renwei Dian and Xiaohui Wei},
  doi          = {10.1109/TNNLS.2023.3311820},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18120-18132},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A lightweight pixel-level unified image fusion network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semisupervised subspace learning with adaptive pairwise
graph embedding. <em>TNNLS</em>, <em>35</em>(12), 18105–18119. (<a
href="https://doi.org/10.1109/TNNLS.2023.3311789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semisupervised learning can explore the graph topology information behind the samples, becoming one of the most attractive research areas in machine learning in recent years. Nevertheless, existing graph-based methods also suffer from two shortcomings. On the one hand, the existing methods generate graphs in the original high-dimensional space, which are easily disturbed by noisy and redundancy features, resulting in low-quality constructed graphs that cannot accurately portray the relationships between data. On the other hand, most of the existing models are based on the Gaussian assumption, which cannot capture the local submanifold structure information of the data, thus reducing the discriminativeness of the learned low-dimensional representations. This article proposes a semisupervised subspace learning with adaptive pairwise graph embedding (APGE), which first builds a $k_{1}$ -nearest neighbor graph on the labeled data to learn local discriminant embeddings for exploring the intrinsic structure of the non-Gaussian labeled data, i.e., the submanifold structure. Then, a $k_{2}$ -nearest neighbor graph is constructed on all samples and mapped to GE learning to adaptively explore the global structure of all samples. Clustering unlabeled data and its corresponding labeled neighbors into the same submanifold, sharing the same label information, improves embedded data’s discriminative ability. And the adaptive neighborhood learning method is used to learn the graph structure in the continuously optimized subspace to ensure that the optimal graph matrix and projection matrix are finally learned, which has strong robustness. Meanwhile, the rank constraint is added to the Laplacian matrix of the similarity matrix of all samples so that the connected components in the obtained similarity matrix are precisely equal to the number of classes in the sample, which makes the structure of the graph clearer and the relationship between the near-neighbor sample points more explicit. Finally, multiple experiments on several synthetic and real-world datasets show that the method performs well in exploring local structure and classification tasks.},
  archive      = {J_TNNLS},
  author       = {Hebing Nie and Qi Li and Zheng Wang and Haifeng Zhao and Feiping Nie},
  doi          = {10.1109/TNNLS.2023.3311789},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18105-18119},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised subspace learning with adaptive pairwise graph embedding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DW-GAN: Toward high-fidelity color-tones of GAN-generated
images with dynamic weights. <em>TNNLS</em>, <em>35</em>(12),
18090–18104. (<a
href="https://doi.org/10.1109/TNNLS.2023.3311545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color-tone represents the prominent color of an image, and training generative adversarial nets (GAN) to change color-tones of generated images is desirable in many applications. Advances such as HistoGAN can manipulate color-tones of generated images with a target image. Yet, there are challenges. Kullback–Leibler (KL) divergence adopted by HistoGAN might bring the color-tone mismatching, because it is possible to provide infinite score to a generator. Moreover, only relying on distribution estimation also produces images with lower fidelity in HistoGAN. To address these issues, we propose a new approach, named dynamic weights GAN (DW-GAN). We use two discriminators to estimate the distribution matching degree and details’ similarity, with Laplacian operator and Hinge loss. Laplacian operator can help capture more image details, while Hinge loss is deduced from mean difference (MD) that could avoid the case of infinite score. To synthesize desired images, we combine the loss of the two discriminators with generator loss and set the weights of the two estimated scores to be dynamic through the previous discriminators’ outputs, given that the training signal of a generator is from a discriminator. Besides, we innovatively integrate the dynamic weights into other GAN variants (e.g., HistoGAN and StyleGAN) to show the improved performance. Finally, we conduct extensive experiments on one industrial Fabric and seven public datasets to demonstrate the significant performance of DW-GAN in producing higher fidelity images and achieving the lowest Frechet inception distance (FID) scores over SOTA baselines.},
  archive      = {J_TNNLS},
  author       = {Wei Li and Chengchun Gu and Jinlin Chen and Chao Ma and Xiaowu Zhang and Bin Chen and Ping Chen},
  doi          = {10.1109/TNNLS.2023.3311545},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18090-18104},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DW-GAN: Toward high-fidelity color-tones of GAN-generated images with dynamic weights},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid data preprocessing-based hierarchical attention
BiLSTM network for remaining useful life prediction of spacecraft
lithium-ion batteries. <em>TNNLS</em>, <em>35</em>(12), 18076–18089. (<a
href="https://doi.org/10.1109/TNNLS.2023.3311443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial energy storage for the spacecraft power system, lithium-ion batteries degradation mechanisms are complex and involved with external environmental perturbations. Hence, effective remaining useful life (RUL) prediction and model reliability assessment confronts considerable obstacles. This article develops a new RUL prediction method for spacecraft lithium-ion batteries, where a hybrid data preprocessing-based deep learning model is proposed. First, to improve the correlation between battery capacity and features, the empirically selected high-dimensional features are linearized by using the Box-Cox transformation and then denoised via the complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) method. Second, the principal component analysis (PCA) algorithm is employed to perform feature dimensionality reduction, and the output of PCA is further processed by the sliding window technique. Third, a multiscale hierarchical attention bi-directional long short-term memory (MHA-BiLSTM) model is constructed to estimate the capacity in future cycles. Specifically, the MHA-BiLSTM model can predict the RUL of lithium-ion batteries by considering the correlation and significance of each cycle’s information during the degradation process on different scales. Finally, the proposed method is validated based on multiple types of experiments under two lithium-ion battery datasets, demonstrating its superior performance in terms of feature extraction and multidimensional time series prediction.},
  archive      = {J_TNNLS},
  author       = {Tianyi Luo and Ming Liu and Peng Shi and Guangren Duan and Xibin Cao},
  doi          = {10.1109/TNNLS.2023.3311443},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18076-18089},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid data preprocessing-based hierarchical attention BiLSTM network for remaining useful life prediction of spacecraft lithium-ion batteries},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Site-invariant meta-modulation learning for multisite autism
spectrum disorders diagnosis. <em>TNNLS</em>, <em>35</em>(12),
18062–18075. (<a
href="https://doi.org/10.1109/TNNLS.2023.3311195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large amounts of fMRI data are essential to building generalized predictive models for brain disease diagnosis. In order to conduct extensive data analysis, it is often necessary to gather data from multiple organizations. However, the site variation inherent in multisite resting-state functional magnetic resonance imaging (rs-fMRI) leads to unfavorable heterogeneity in data distribution, negatively impacting the identification of biomarkers and the diagnostic decision. Several existing methods have alleviated this shift of domain distribution (i.e., multisite problem). Statistical tuning schemes directly regress out site disparity factors from the data prior to model training. Such methods have a limitation in processing data each time through variance estimation according to the added site. In the model adjustment approaches, domain adaptation (DA) methods adjust the features or models of the source domain according to the target domain during model training. Thus, it is inevitable that it needs updating model parameters according to the samples of a target site, causing great limitations in practical applicability. Meanwhile, the approach of domain generalization (DG) aims to create a universal model that can be quickly adapted to multiple domains. In this study, we propose a novel framework for disease diagnosis that alleviates the multisite problem by adaptively calibrating site-specific features into site-invariant features. Specifically, it applies directly to samples from unseen sites without the need for fine-tuning. With a learning-to-learn strategy that learns how to calibrate the features under the various domain shift environments, our novel modulation mechanism extracts site-invariant features. In our experiments over the Autism Brain Imaging Data Exchange (ABIDE I and II) dataset, we validated the generalization ability of the proposed network by improving diagnostic accuracy in both seen and unseen multisite samples.},
  archive      = {J_TNNLS},
  author       = {Jaein Lee and Eunsong Kang and Da-Woon Heo and Heung-Il Suk},
  doi          = {10.1109/TNNLS.2023.3311195},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18062-18075},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Site-invariant meta-modulation learning for multisite autism spectrum disorders diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural networks for portfolio analysis in high-frequency
trading. <em>TNNLS</em>, <em>35</em>(12), 18052–18061. (<a
href="https://doi.org/10.1109/TNNLS.2023.3311169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-frequency trading proposes new challenges to classical portfolio selection problems. Especially, the timely and accurate solution of portfolios is highly demanded in financial market nowadays. This article makes progress along this direction by proposing novel neural networks with softmax equalization to address the problem. To the best of our knowledge, this is the first time that softmax technique is used to deal with equation constraints in portfolio selections. Theoretical analysis shows that the proposed method is globally convergent to the optimum of the optimization formulation of portfolio selection. Experiments based on real stock data verify the effectiveness of the proposed solution. It is worth mentioning that the two proposed models achieve 5.50% and 5.47% less cost, respectively, than the solution obtained by using MATLAB dedicated solvers, which demonstrates the superiority of the proposed strategies.},
  archive      = {J_TNNLS},
  author       = {Xinwei Cao and Chen Peng and Yuhua Zheng and Shuai Li and Tran Thu Ha and Victor Shutyaev and Vasilios Katsikis and Predrag Stanimirovic},
  doi          = {10.1109/TNNLS.2023.3311169},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18052-18061},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural networks for portfolio analysis in high-frequency trading},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confusion region mining for crowd counting. <em>TNNLS</em>,
<em>35</em>(12), 18039–18051. (<a
href="https://doi.org/10.1109/TNNLS.2023.3311020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing works mainly focus on crowd and ignore the confusion regions which contain extremely similar appearance to crowd in the background, while crowd counting needs to face these two sides at the same time. To address this issue, we propose a novel end-to-end trainable confusion region discriminating and erasing network called CDENet. Specifically, CDENet is composed of two modules of confusion region mining module (CRM) and guided erasing module (GEM). CRM consists of basic density estimation (BDE) network, confusion region aware bridge and confusion region discriminating network. The BDE network first generates a primary density map, and then the confusion region aware bridge excavates the confusion regions by comparing the primary prediction result with the ground-truth density map. Finally, the confusion region discriminating network learns the difference of feature representations in confusion regions and crowds. Furthermore, GEM gives the refined density map by erasing the confusion regions. We evaluate the proposed method on four crowd counting benchmarks, including ShanghaiTech Part_A, ShanghaiTech Part_B, UCF_CC_50, and UCF-QNRF, and our CDENet achieves superior performance compared with the state-of-the-arts.},
  archive      = {J_TNNLS},
  author       = {Jiawen Zhu and Wenda Zhao and Libo Yao and You He and Maodi Hu and Xiaoxing Zhang and Shuo Wang and Tao Li and Huchuan Lu},
  doi          = {10.1109/TNNLS.2023.3311020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18039-18051},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Confusion region mining for crowd counting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predefined-time convergent kinematic control of robotic
manipulators with unknown models based on hybrid neural dynamics and
human behaviors. <em>TNNLS</em>, <em>35</em>(12), 18026–18038. (<a
href="https://doi.org/10.1109/TNNLS.2023.3310744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a model-free kinematic control method with predefined-time convergence for robotic manipulators with unknown models. The predefined-time convergence property guarantees that the regulation task can be finished by robotic manipulators in a preset time, in spite of the initial state of manipulators. This feature will facilitate the scheduling of a series of tasks in industrial applications. To this end, a varying-parameter predefined-time convergent zeroing neural dynamics (ZND) model is first proposed and employed to solve the regulation problem. As well as the primary task, a conventional ZND model is utilized to achieve the avoidance of obstacle. The stability of the proposed controller is analyzed based on the Lyapunov stability theory. For the sake of dealing with the unknown kinematic model of robotic manipulators, gradient neural dynamics (GND) models are exploited to adapt the Jacobian matrices just relying on the control signal and sensory output, which enables us to control robotic manipulators in a model-free manner. Finally, the efficacy and merits of the proposed control method are verified by simulations and experiments, including a comparison with the existing method.},
  archive      = {J_TNNLS},
  author       = {Ning Tan and Peng Yu},
  doi          = {10.1109/TNNLS.2023.3310744},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18026-18038},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Predefined-time convergent kinematic control of robotic manipulators with unknown models based on hybrid neural dynamics and human behaviors},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rectify ViT shortcut learning by visual saliency.
<em>TNNLS</em>, <em>35</em>(12), 18013–18025. (<a
href="https://doi.org/10.1109/TNNLS.2023.3310531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortcut learning in deep learning models occurs when unintended features are prioritized, resulting in degenerated feature representations and reduced generalizability and interpretability. However, shortcut learning in the widely used vision transformer (ViT) framework is largely unknown. Meanwhile, introducing domain-specific knowledge is a major approach to rectifying the shortcuts that are predominated by background-related factors. For example, eye-gaze data from radiologists are effective human visual prior knowledge that has the great potential to guide the deep learning models to focus on meaningful foreground regions. However, obtaining eye-gaze data can still sometimes be time-consuming, labor-intensive, and even impractical. In this work, we propose a novel and effective saliency-guided ViT (SGT) model to rectify shortcut learning in ViT with the absence of eye-gaze data. Specifically, a computational visual saliency model (either pretrained or fine-tuned) is adopted to predict saliency maps for input image samples. Then, the saliency maps are used to filter the most informative image patches. Considering that this filter operation may lead to global information loss, we further introduce a residual connection that calculates the self-attention across all the image patches. The experiment results on natural and medical image datasets show that our SGT framework can effectively learn and leverage human prior knowledge without eye-gaze data and achieves much better performance than baselines. Meanwhile, it successfully rectifies the harmful shortcut learning and significantly improves the interpretability of the ViT model, demonstrating the promise of transferring human prior knowledge derived visual saliency in rectifying shortcut learning.},
  archive      = {J_TNNLS},
  author       = {Chong Ma and Lin Zhao and Yuzhong Chen and Lei Guo and Tuo Zhang and Xintao Hu and Dinggang Shen and Xi Jiang and Tianming Liu},
  doi          = {10.1109/TNNLS.2023.3310531},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18013-18025},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rectify ViT shortcut learning by visual saliency},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive prognosis framework between deep learning and a
stochastic process model for remaining useful life prediction.
<em>TNNLS</em>, <em>35</em>(12), 18000–18012. (<a
href="https://doi.org/10.1109/TNNLS.2023.3310482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification of the remaining useful life (RUL) for degraded systems under the big data era has been a hot topic in recent years. A general idea is to execute two separate steps: deep-learning-based health indicator (HI) construction and stochastic process-based degradation modeling. However, there exists a critical matching defect between the constructed HI and a degradation model, which seriously affects the RUL prediction accuracy. Toward this end, this article proposes an interactive prognosis framework between deep learning and a stochastic process model for the RUL prediction. First, we resort to stacked contractive autoencoders to fuse multiple sensor information of historical systems for constructing the HI in a typical unsupervised manner. Then, considering the nonlinear characteristic of the constructed HI, an exponential-like degradation model is introduced to construct its degradation evolving model, and theoretical expressions of the prediction results are derived under the concept of the first hitting time. Furthermore, we design an optimization objective function by integrating the HI construction and degradation modeling for the RUL prediction. To minimize the designed objective function of the proposed interactive prognosis framework, a gradient descent algorithm is employed to update the model parameters. Based on the well-trained interactive prognosis model, we can obtain the HI of a field system from stacked contractive autoencoders with sensor data and the probability density function (pdf) of the predicted RUL on the basis of the estimated parameters. Finally, the effectiveness and superiority of the proposed interactive prognosis method are verified by two case studies associated with turbofan engines.},
  archive      = {J_TNNLS},
  author       = {Hong Pei and Xiaosheng Si and Tianmei Li and Zhengxin Zhang and Yaguo Lei},
  doi          = {10.1109/TNNLS.2023.3310482},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {18000-18012},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interactive prognosis framework between deep learning and a stochastic process model for remaining useful life prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QC-ODKLA: Quantized and communication- censored online
decentralized kernel learning via linearized ADMM. <em>TNNLS</em>,
<em>35</em>(12), 17987–17999. (<a
href="https://doi.org/10.1109/TNNLS.2023.3310499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on online kernel learning over a decentralized network. Each agent in the network receives online streaming data and collaboratively learns a globally optimal nonlinear prediction function in the reproducing kernel Hilbert space (RKHS). To overcome the curse of dimensionality issue in traditional online kernel learning, we utilize random feature (RF) mapping to convert the nonparametric kernel learning problem into a fixed-length parametric one in the RF space. We then propose a novel learning framework, named online decentralized kernel learning via linearized ADMM (ODKLA), to efficiently solve the online decentralized kernel learning problem. To enhance communication efficiency, we introduce quantization and censoring strategies in the communication stage, resulting in the quantized and communication-censored ODKLA (QC-ODKLA) algorithm. We theoretically prove that both ODKLA and QC-ODKLA can achieve the optimal sublinear regret $\mathcal {O}(\sqrt {T})$ over $T$ time slots. Through numerical experiments, we evaluate the learning effectiveness, communication efficiency, and computation efficiency of the proposed methods.},
  archive      = {J_TNNLS},
  author       = {Ping Xu and Yue Wang and Xiang Chen and Zhi Tian},
  doi          = {10.1109/TNNLS.2023.3310499},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17987-17999},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {QC-ODKLA: Quantized and communication- censored online decentralized kernel learning via linearized ADMM},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image-specific information suppression and implicit local
alignment for text-based person search. <em>TNNLS</em>, <em>35</em>(12),
17973–17986. (<a
href="https://doi.org/10.1109/TNNLS.2023.3310118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search (TBPS) is a challenging task that aims to search pedestrian images with the same identity from an image gallery given a query text. In recent years, TBPS has made remarkable progress, and state-of-the-art (SOTA) methods achieve superior performance by learning local fine-grained correspondence between images and texts. However, most existing methods rely on explicitly generated local parts to model fine-grained correspondence between modalities, which is unreliable due to the lack of contextual information or the potential introduction of noise. Moreover, the existing methods seldom consider the information inequality problem between modalities caused by image-specific information. To address these limitations, we propose an efficient joint multilevel alignment network (MANet) for TBPS, which can learn aligned image/text feature representations between modalities at multiple levels, and realize fast and effective person search. Specifically, we first design an image-specific information suppression (ISS) module, which suppresses image background and environmental factors by relation-guided localization (RGL) and channel attention filtration (CAF), respectively. This module effectively alleviates the information inequality problem and realizes the alignment of information volume between images and texts. Second, we propose an implicit local alignment (ILA) module to adaptively aggregate all pixel/word features of image/text to a set of modality-shared semantic topic centers and implicitly learn the local fine-grained correspondence between modalities without additional supervision and cross-modal interactions. Also, a global alignment (GA) is introduced as a supplement to the local perspective. The cooperation of global and local alignment modules enables better semantic alignment between modalities. Extensive experiments on multiple databases demonstrate the effectiveness and superiority of our MANet.},
  archive      = {J_TNNLS},
  author       = {Shuanglin Yan and Hao Tang and Liyan Zhang and Jinhui Tang},
  doi          = {10.1109/TNNLS.2023.3310118},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17973-17986},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Image-specific information suppression and implicit local alignment for text-based person search},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discretize relaxed solution of spectral clustering via a
nonheuristic algorithm. <em>TNNLS</em>, <em>35</em>(12), 17965–17972.
(<a href="https://doi.org/10.1109/TNNLS.2023.3309871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering and its extensions usually consist of two steps: 1) constructing a graph and computing the relaxed solution and 2) discretizing relaxed solutions. Although the former has been extensively investigated, the discretization techniques are mainly heuristic methods, e.g., $k$ -means (KM), spectral rotation (SR). Unfortunately, the goal of the existing methods is not to find a discrete solution that minimizes the original objective. In other words, the primary drawback is the neglect of the original objective when computing the discrete solution. Inspired by the first-order optimization algorithms, we propose to develop a first-order term to bridge the original problem and discretization algorithm, which is the first nonheuristic to the best of our knowledge. Since the nonheuristic method is aware of the original graph cut problem, the final discrete solution is more reliable and achieves the preferable loss value. We also theoretically show that the continuous optimum is beneficial to discretization algorithms though simply finding its closest discrete solution is an existing heuristic algorithm which is also unreliable. Sufficient experiments significantly show the superiority of our method.},
  archive      = {J_TNNLS},
  author       = {Hongyuan Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3309871},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17965-17972},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discretize relaxed solution of spectral clustering via a nonheuristic algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mild policy evaluation for offline actor–critic.
<em>TNNLS</em>, <em>35</em>(12), 17950–17964. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In offline actor–critic (AC) algorithms, the distributional shift between the training data and target policy causes optimistic $Q$ value estimates for out-of-distribution (OOD) actions. This leads to learned policies skewed toward OOD actions with falsely high $Q$ values. The existing value-regularized offline AC algorithms address this issue by learning a conservative value function, leading to a performance drop. In this article, we propose a mild policy evaluation (MPE) by constraining the difference between the $Q$ values of actions supported by the target policy and those of actions contained within the offline dataset. The convergence of the proposed MPE, the gap between the learned value function and the true one, and the suboptimality of the offline AC with MPE are analyzed, respectively. A mild offline AC (MOAC) algorithm is developed by integrating MPE into off-policy AC. Compared with existing offline AC algorithms, the value function gap of MOAC is bounded by the existence of sampling errors. Moreover, in the absence of sampling errors, the true state value function can be obtained. Experimental results on the D4RL benchmark dataset demonstrate the effectiveness of MPE and the performance superiority of MOAC compared to the state-of-the-art offline reinforcement learning (RL) algorithms.},
  archive      = {J_TNNLS},
  author       = {Longyang Huang and Botao Dong and Jinhui Lu and Weidong Zhang},
  doi          = {10.1109/TNNLS.2023.3309906},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17950-17964},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mild policy evaluation for offline Actor–Critic},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Localizing from classification: Self-directed weakly
supervised object localization for remote sensing images.
<em>TNNLS</em>, <em>35</em>(12), 17935–17949. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, object localization and detection methods in remote sensing images (RSIs) have received increasing attention due to their broad applications. However, most previous fully supervised methods require a large number of time-consuming and labor-intensive instance-level annotations. Compared with those fully supervised methods, weakly supervised object localization (WSOL) aims to recognize object instances using only image-level labels, which greatly saves the labeling costs of RSIs. In this article, we propose a self-directed weakly supervised strategy (SD-WSS) to perform WSOL in RSIs. To specify, we fully exploit and enhance the spatial feature extraction capability of the RSIs’ classification model to accurately localize the objects of interest. To alleviate the serious discriminative region problem exhibited by previous WSOL methods, the spatial location information implicit in the classification model is carefully extracted by GradCAM++ to guide the learning procedure. Furthermore, to eliminate the interference from complex backgrounds of RSIs, we design a novel self-directed loss to make the model optimize itself and explicitly tell it where to look. Finally, we review and annotate the existing remote sensing scene classification dataset and create two new WSOL benchmarks in RSIs, named C45V2 and PN2. We conduct extensive experiments to evaluate the proposed method and six mainstream WSOL methods with three backbones on C45V2 and PN2. The results demonstrate that our proposed method achieves better performance when compared with state-of-the-arts.},
  archive      = {J_TNNLS},
  author       = {Jing Bai and Junjie Ren and Zhu Xiao and Zheng Chen and Chengxi Gao and Talal Ahmed Ali Ali and Licheng Jiao},
  doi          = {10.1109/TNNLS.2023.3309889},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17935-17949},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Localizing from classification: Self-directed weakly supervised object localization for remote sensing images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Curved geometric networks for visual anomaly recognition.
<em>TNNLS</em>, <em>35</em>(12), 17921–17934. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a latent embedding to understand the underlying nature of data distribution is often formulated in Euclidean spaces with zero curvature. However, the success of the geometry constraints, posed in the embedding space, indicates that curved spaces might encode more structural information, leading to better discriminative power and hence richer representations. In this work, we investigate the benefits of the curved space for analyzing anomalous, open-set, or out-of-distribution (OOD) objects in data. This is achieved by considering embeddings via three geometry constraints, namely, spherical geometry (with positive curvature), hyperbolic geometry (with negative curvature), or mixed geometry (with both positive and negative curvatures). Three geometric constraints can be chosen interchangeably in a unified design, given the task at hand. Tailored for the embeddings in the curved space, we also formulate functions to compute the anomaly score. Two types of geometric modules (i.e., geometric-in-one (GiO) and geometric-in-two (GiT) models) are proposed to plug in the original Euclidean classifier, and anomaly scores are computed from the curved embeddings. We evaluate the resulting designs under a diverse set of visual recognition scenarios, including image detection (multiclass OOD detection and one-class anomaly detection) and segmentation (multiclass anomaly segmentation and one-class anomaly segmentation). The empirical results show the effectiveness of our proposal through consistent improvement over various scenarios. The code is made available at https://github.com/JHome1/GiO-GiT .},
  archive      = {J_TNNLS},
  author       = {Jie Hong and Pengfei Fang and Weihao Li and Junlin Han and Lars Petersson and Mehrtash Harandi},
  doi          = {10.1109/TNNLS.2023.3309846},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17921-17934},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Curved geometric networks for visual anomaly recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GTR: An SQL generator with transition representation in
cross-domain database systems. <em>TNNLS</em>, <em>35</em>(12),
17908–17920. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have focused on using natural language (NL) to automatically retrieve useful data from database (DB) systems. As an important component of autonomous DB systems, the NL-to-SQL technique can assist DB administrators in writing high-quality SQL statements and make persons with no SQL background knowledge learn complex SQL languages. However, existing studies cannot deal with the issue that the expression of NL inevitably mismatches the implementation details of SQLs, and the large number of out-of-domain (OOD) words makes it difficult to predict table columns. In particular, it is difficult to accurately convert NL into SQL in an end-to-end fashion. Intuitively, it facilitates the model to understand the relations if a “bridge” [transition representation (TR)] is employed to make it compatible with both NL and SQL in the phase of conversion. In this article, we propose an automatic SQL generator with TR called GTR in cross-domain DB systems. Specifically, GTR contains three SQL generation steps: 1) GTR learns the relation between questions and DB schemas; 2) GTR uses a grammar-based model to synthesize a TR; and 3) GTR predicts SQL from TR based on the rules. We conduct extensive experiments on two commonly used datasets, that is, WikiSQL and Spider. On the testing set of the Spider and WikiSQL datasets, the results show that GTR achieves 58.32% and 71.29% exact matching accuracy which outperforms the state-of-the-art methods, respectively.},
  archive      = {J_TNNLS},
  author       = {Shaojie Qiao and Chenxu Liu and Guoping Yang and Nan Han and Yuhan Peng and Lingchun Wu and He Li and Guan Yuan},
  doi          = {10.1109/TNNLS.2023.3309824},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17908-17920},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GTR: An SQL generator with transition representation in cross-domain database systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing distributed neural network training through
node-based communications. <em>TNNLS</em>, <em>35</em>(12), 17893–17907.
(<a href="https://doi.org/10.1109/TNNLS.2023.3309735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of data needed to effectively train modern deep neural architectures has grown significantly, leading to increased computational requirements. These intensive computations are tackled by the combination of last generation computing resources, such as accelerators, or classic processing units. Nevertheless, gradient communication remains as the major bottleneck, hindering the efficiency notwithstanding the improvements in runtimes obtained through data parallelism strategies. Data parallelism involves all processes in a global exchange of potentially high amount of data, which may impede the achievement of the desired speedup and the elimination of noticeable delays or bottlenecks. As a result, communication latency issues pose a significant challenge that profoundly impacts the performance on distributed platforms. This research presents node-based optimization steps to significantly reduce the gradient exchange between model replicas whilst ensuring model convergence. The proposal serves as a versatile communication scheme, suitable for integration into a wide range of general-purpose deep neural network (DNN) algorithms. The optimization takes into consideration the specific location of each replica within the platform. To demonstrate the effectiveness, different neural network approaches and datasets with disjoint properties are used. In addition, multiple types of applications are considered to demonstrate the robustness and versatility of our proposal. The experimental results show a global training time reduction whilst slightly improving accuracy. Code: https://github.com/mhaut/eDNNcomm .},
  archive      = {J_TNNLS},
  author       = {Sergio Moreno-Álvarez and Mercedes E. Paoletti and Gabriele Cavallaro and Juan M. Haut},
  doi          = {10.1109/TNNLS.2023.3309735},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17893-17907},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhancing distributed neural network training through node-based communications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible vertical federated learning with heterogeneous
parties. <em>TNNLS</em>, <em>35</em>(12), 17878–17892. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose flexible vertical federated learning (Flex-VFL), a distributed machine algorithm that trains a smooth, nonconvex function in a distributed system with vertically partitioned data. We consider a system with several parties that wish to collaboratively learn a global function. Each party holds a local dataset; the datasets have different features but share the same sample ID space. The parties are heterogeneous in nature: the parties’ operating speeds, local model architectures, and optimizers may be different from one another and, further, they may change over time. To train a global model in such a system, Flex-VFL utilizes a form of parallel block coordinate descent (P-BCD), where parties train a partition of the global model via stochastic coordinate descent. We provide theoretical convergence analysis for Flex-VFL and show that the convergence rate is constrained by the party speeds and local optimizer parameters. We apply this analysis and extend our algorithm to adapt party learning rates in response to changing speeds and local optimizer parameters. Finally, we compare the convergence time of Flex-VFL against synchronous and asynchronous VFL algorithms, as well as illustrate the effectiveness of our adaptive extension.},
  archive      = {J_TNNLS},
  author       = {Timothy Castiglia and Shiqiang Wang and Stacy Patterson},
  doi          = {10.1109/TNNLS.2023.3309701},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17878-17892},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Flexible vertical federated learning with heterogeneous parties},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable-MADDPG-based cooperative target invasion for a
multi-USV system. <em>TNNLS</em>, <em>35</em>(12), 17867–17877. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concentrates on proposing a scalable deep reinforcement learning (DRL) method for a multiple unmanned surface vehicle (multi-USV) system to operate cooperative target invasion. The multi-USV system, which is made up of multiple invaders, needs to invade target areas in a specified time. A novel scalable reinforcement learning (RL) method called Scalable-MADDPG is proposed for the first time. In this method, the scale of the multi-USV system can be changed at any time without interrupting the training process. Then, to mitigate the policy oscillation after applying Scalable-MADDPG, a bi-directional long–short-term memory (Bi-LSTM) network is constructed. Moreover, an improved $\epsilon $ -greedy strategy is proposed to help balance the exploration and exploitation in RL. Furthermore, to enhance the robustness of the optimal policy, Ornstein–Uhlenbeck (OU) noise is added in this improved $\epsilon $ -greedy strategy during the training process. Finally, the scalable RL method is used to help the multi-USV system perform cooperative target invasion under complex marine environments. The effectiveness of Scalable-MADDPG is demonstrated through three experiments.},
  archive      = {J_TNNLS},
  author       = {Cheng-Cheng Wang and Yu-Long Wang and Peng Shi and Fei Wang},
  doi          = {10.1109/TNNLS.2023.3309689},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17867-17877},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Scalable-MADDPG-based cooperative target invasion for a multi-USV system},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TECO: A unified feature map compression framework based on
transform and entropy. <em>TNNLS</em>, <em>35</em>(12), 17856–17866. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive memory accesses of feature maps (FMs) in deep neural network (DNN) processors lead to huge power consumption, which becomes a major energy bottleneck of DNN accelerators. In this article, we propose a unified framework named Transform and Entropy-based COmpression (TECO) scheme to efficiently compress FMs with various attributes in DNN inference. We explore, for the first time, the intrinsic unimodal distribution characteristic that widely exists in the frequency domain of various FMs. In addition, a well-optimized hardware-friendly coding scheme is designed, which fully utilizes this remarkable data distribution characteristic to encode and compress the frequency spectrum of different FMs. Furthermore, the information entropy theory is leveraged to develop a novel loss function for improving the compression ratio and to make a fast comparison among different compressors. Extensive experiments are performed on multiple tasks and demonstrate that the proposed TECO achieves compression ratios of $2.31\times $ in ResNet-50 on image classification, $3.47\times $ in UNet on dark image enhancement, and $3.18\times $ in Yolo-v4 on object detection while keeping the accuracy of these models. Compared with the upper limit of the compression ratio for original FMs, the proposed framework achieves the compression ratio improvement of 21%, 157%, and 152% on the above models.},
  archive      = {J_TNNLS},
  author       = {Yubo Shi and Meiqi Wang and Tianyu Cao and Jun Lin and Zhongfeng Wang},
  doi          = {10.1109/TNNLS.2023.3309667},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17856-17866},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TECO: A unified feature map compression framework based on transform and entropy},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain-adaptive graph attention-supervised network for
cross-network edge classification. <em>TNNLS</em>, <em>35</em>(12),
17842–17855. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have shown great ability in modeling graphs; however, their performance would significantly degrade when there are noisy edges connecting nodes from different classes. To alleviate negative effect of noisy edges on neighborhood aggregation, some recent GNNs propose to predict the label agreement between node pairs within a single network. However, predicting the label agreement of edges across different networks has not been investigated yet. Our work makes the pioneering attempt to study a novel problem of cross-network homophilous and heterophilous edge classification (CNHHEC) and proposes a novel domain-adaptive graph attention-supervised network (DGASN) to effectively tackle the CNHHEC problem. First, DGASN adopts multihead graph attention network (GAT) as the GNN encoder, which jointly trains node embeddings and edge embeddings via the node classification and edge classification losses. As a result, label-discriminative embeddings can be obtained to distinguish homophilous edges from heterophilous edges. In addition, DGASN applies direct supervision on graph attention learning based on the observed edge labels from the source network, thus lowering the negative effects of heterophilous edges while enlarging the positive effects of homophilous edges during neighborhood aggregation. To facilitate knowledge transfer across networks, DGASN employs adversarial domain adaptation to mitigate domain divergence. Extensive experiments on real-world benchmark datasets demonstrate that the proposed DGASN achieves the state-of-the-art performance in CNHHEC.},
  archive      = {J_TNNLS},
  author       = {Xiao Shen and Mengqiu Shao and Shirui Pan and Laurence T. Yang and Xi Zhou},
  doi          = {10.1109/TNNLS.2023.3309632},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17842-17855},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain-adaptive graph attention-supervised network for cross-network edge classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NVIF: Neighboring variational information flow for
cooperative large-scale multiagent reinforcement learning.
<em>TNNLS</em>, <em>35</em>(12), 17829–17841. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication-based multiagent reinforcement learning (MARL) has shown promising results in promoting cooperation by enabling agents to exchange information. However, the existing methods have limitations in large-scale multiagent systems due to high information redundancy, and they tend to overlook the unstable training process caused by the online-trained communication protocol. In this work, we propose a novel method called neighboring variational information flow (NVIF), which enhances communication among neighboring agents by providing them with the maximum information set (MIS) containing more information than the existing methods. NVIF compresses the MIS into a compact latent state while adopting neighboring communication. To stabilize the overall training process, we introduce a two-stage training mechanism. We first pretrain the NVIF module using a randomly sampled offline dataset to create a task-agnostic and stable communication protocol, and then use the pretrained protocol to perform online policy training with RL algorithms. Our theoretical analysis indicates that NVIF-proximal policy optimization (PPO), which combines NVIF with PPO, has the potential to promote cooperation with agent-specific rewards. Experiment results demonstrate the superiority of our method in both heterogeneous and homogeneous settings. Additional experiment results also demonstrate the potential of our method for multitask learning.},
  archive      = {J_TNNLS},
  author       = {Jiajun Chai and Yuanheng Zhu and Dongbin Zhao},
  doi          = {10.1109/TNNLS.2023.3309608},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17829-17841},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {NVIF: Neighboring variational information flow for cooperative large-scale multiagent reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive event-triggered bipartite formation for multiagent
systems via reinforcement learning. <em>TNNLS</em>, <em>35</em>(12),
17817–17828. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the online learning and energy-efficient control issues for nonlinear discrete-time multiagent systems (MASs) with unknown dynamics models and antagonistic interactions. First, a distributed combined measurement error function is formulated using the signed graph theory to transfer the bipartite formation issue into a consensus issue. Then, an enhanced linearization controller model for the controlled MASs is developed by employing dynamic linearization technology. After that, an online learning adaptive event-triggered (ET) actor-critic neural network (AC-NN) framework for the MASs to implement bipartite formation control tasks is proposed by employing the optimized NNs and designed adaptive ET mechanism. Moreover, the convergence of the designed formation control framework is strictly proved by the constructed Lyapunov functions. Finally, simulation and experimental studies further demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Huarong Zhao and Jinjun Shan and Li Peng and Hongnian Yu},
  doi          = {10.1109/TNNLS.2023.3309326},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17817-17828},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive event-triggered bipartite formation for multiagent systems via reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero-shot human–object interaction detection via similarity
propagation. <em>TNNLS</em>, <em>35</em>(12), 17805–17816. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–object interaction (HOI) detection involves identifying interactions represented as $\langle \text {human, action, object} \rangle $ , requiring the localization of human–object pairs and interaction classification within an image. This work focuses on the challenge of detecting HOIs with unseen objects using the prevalent Transformer architecture. Our empirical analysis reveals that the performance degradation of novel HOI instances primarily arises from misclassifying unseen objects as confusable seen objects. To address this issue, we propose a similarity propagation (SP) scheme that leverages cosine similarity distance to regulate the prediction margin between seen and unseen objects. In addition, we introduce pseudo-supervision for unseen objects based on class semantic similarities during training. Furthermore, we incorporate semantic-aware instance-level and interaction-level contrastive losses with Transformer to enhance intraclass compactness and interclass separability, resulting in improved visual representations. Extensive experiments on two challenging benchmarks, V-COCO and HICO-DET, demonstrate the effectiveness of our model, outperforming current state-of-the-art methods under various zero-shot settings.},
  archive      = {J_TNNLS},
  author       = {Daoming Zong and Shiliang Sun},
  doi          = {10.1109/TNNLS.2023.3309104},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17805-17816},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Zero-shot Human–Object interaction detection via similarity propagation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online active continual learning for robotic lifelong object
recognition. <em>TNNLS</em>, <em>35</em>(12), 17790–17804. (<a
href="https://doi.org/10.1109/TNNLS.2023.3308900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, robotic systems collect vast amounts of new data from ever-changing environments over time. They need to continually interact and learn new knowledge from the external world to adapt to the environment. Particularly, lifelong object recognition in an online and interactive manner is a crucial and fundamental capability for robotic systems. To meet this realistic demand, in this article, we propose an online active continual learning (OACL) framework for robotic lifelong object recognition, in the scenario of both classes and domains changing with dynamic environments. First, to reduce the labeling cost as much as possible while maximizing the performance, a new online active learning (OAL) strategy is designed by taking both the uncertainty and diversity of samples into account to protect the information volume and distribution of data. In addition, to prevent catastrophic forgetting and reduce memory costs, a novel online continual learning (OCL) algorithm is proposed based on the deep feature semantic augmentation and a new loss-based deep model and replay buffer update, which can mitigate the class imbalance between the old and new classes and alleviate confusion between two similar classes. Moreover, the mistake bound of the proposed method is analyzed in theory. OACL allows robots to select the most representative new samples to query labels and continually learn new objects and new variants of previously learned objects from a nonindependent and identically distributed (i.i.d.) data stream without catastrophic forgetting. Extensive experiments conducted on real lifelong robotic vision datasets demonstrate that our algorithm, even trained with fewer labeled samples and replay exemplars, can achieve state-of-the-art performance on OCL tasks.},
  archive      = {J_TNNLS},
  author       = {Xiangli Nie and Zhiguang Deng and Mingdong He and Mingyu Fan and Zheng Tang},
  doi          = {10.1109/TNNLS.2023.3308900},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17790-17804},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online active continual learning for robotic lifelong object recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medical transformer: Universal encoder for 3-d brain MRI
analysis. <em>TNNLS</em>, <em>35</em>(12), 17779–17789. (<a
href="https://doi.org/10.1109/TNNLS.2023.3308712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning has attracted considerable attention in medical image analysis because of the limited number of annotated 3-D medical datasets available for training data-driven deep learning models in the real world. We propose Medical Transformer, a novel transfer learning framework that effectively models 3-D volumetric images as a sequence of 2-D image slices. To improve the high-level representation in 3-D-form empowering spatial relations, we use a multiview approach that leverages information from three planes of the 3-D volume, while providing parameter-efficient training. For building a source model generally applicable to various tasks, we pretrain the model using self-supervised learning (SSL) for masked encoding vector prediction as a proxy task, using a large-scale normal, healthy brain magnetic resonance imaging (MRI) dataset. Our pretrained model is evaluated on three downstream tasks: 1) brain disease diagnosis; 2) brain age prediction; and 3) brain tumor segmentation, which are widely studied in brain MRI research. Experimental results demonstrate that our Medical Transformer outperforms the state-of-the-art (SOTA) transfer learning methods, efficiently reducing the number of parameters by up to approximately 92% for classification and regression tasks and 97% for segmentation task, and it also achieves good performance in scenarios where only partial training samples are used.},
  archive      = {J_TNNLS},
  author       = {Eunji Jun and Seungwoo Jeong and Da-Woon Heo and Heung-Il Suk},
  doi          = {10.1109/TNNLS.2023.3308712},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17779-17789},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Medical transformer: Universal encoder for 3-D brain MRI analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing the pretrained models of source code by
re-pretraining under a unified setup. <em>TNNLS</em>, <em>35</em>(12),
17768–17778. (<a
href="https://doi.org/10.1109/TNNLS.2023.3308595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen the successful application of large pretrained models of source code (CodePTMs) to code representation learning, which have taken the field of software engineering (SE) from task-specific solutions to task-agnostic generic models. By the remarkable results, CodePTMs are seen as a promising direction in both academia and industry. While a number of CodePTMs have been proposed, they are often not directly comparable because they differ in experimental setups such as pretraining dataset, model size, evaluation tasks, and datasets. In this article, we first review the experimental setup used in previous work and propose a standardized setup to facilitate fair comparisons among CodePTMs to explore the impacts of their pretraining tasks. Then, under the standardized setup, we re-pretrain CodePTMs using the same model architecture, input modalities, and pretraining tasks, as they declared and fine-tune each model on each evaluation SE task for evaluating. Finally, we present the experimental results and make a comprehensive discussion on the relative strength and weakness of different pretraining tasks with respect to each SE task. We hope our view can inspire and advance the future study of more powerful CodePTMs.},
  archive      = {J_TNNLS},
  author       = {Changan Niu and Chuanyi Li and Vincent Ng and Bin Luo},
  doi          = {10.1109/TNNLS.2023.3308595},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17768-17778},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Comparing the pretrained models of source code by re-pretraining under a unified setup},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global and local interactive perception network for
referring image segmentation. <em>TNNLS</em>, <em>35</em>(12),
17754–17767. (<a
href="https://doi.org/10.1109/TNNLS.2023.3308550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective modal fusion and perception between the language and the image are necessary for inferring the reference instance in the referring image segmentation (RIS) task. In this article, we propose a novel RIS network, the global and local interactive perception network (GLIPN), to enhance the quality of modal fusion between the language and the image from the local and global perspectives. The core of GLIPN is the global and local interactive perception (GLIP) scheme. Specifically, the GLIP scheme contains the local perception module (LPM) and the global perception module (GPM). The LPM is designed to enhance the local modal fusion by the correspondence between word and image local semantics. The GPM is designed to inject the global structured semantics of images into the modal fusion process, which can better guide the word embedding to perceive the whole image’s global structure. Combined with the local–global context semantics fusion, extensive experiments on several benchmark datasets demonstrate the advantage of the proposed GLIPN over most state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Jing Liu and Hongchen Tan and Yongli Hu and Yanfeng Sun and Huasheng Wang and Baocai Yin},
  doi          = {10.1109/TNNLS.2023.3308550},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17754-17767},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global and local interactive perception network for referring image segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized distributed filtering for saturated systems with
amplify-and-forward relays over sensor networks: A dynamic
event-triggered approach. <em>TNNLS</em>, <em>35</em>(12), 17742–17753.
(<a href="https://doi.org/10.1109/TNNLS.2023.3308192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the optimized distributed filtering problem is studied for a class of saturated systems with amplify-and-forward (AF) relays via a dynamic event-triggered mechanism (DETM). The AF relays are located in the channels between sensors and filters to prolong the transmission distance of signals, where the transmission powers of sensors and relays can be described by a sequence of random variables with a known probability distribution. With the purpose of alleviating the communication burden and preventing data collision, the DETM is used to schedule the transmission cases of nodes by dynamically adjusting the triggered threshold according to the practical requirements. An upper bound matrix (UBM) of the filtering error (FE) covariance is first provided under the sense of variance constraint and the proper filter gain is further constructed via minimizing the proposed UBM. In addition, the boundedness evaluation regarding the trace of the UBM is provided. Finally, simulation experiments are used to illustrate the usefulness of the developed distributed recursive filtering scheme.},
  archive      = {J_TNNLS},
  author       = {Jun Hu and Jiaxing Li and Huaicheng Yan and Hongjian Liu},
  doi          = {10.1109/TNNLS.2023.3308192},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17742-17753},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimized distributed filtering for saturated systems with amplify-and-forward relays over sensor networks: A dynamic event-triggered approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning generative models using denoising density
estimators. <em>TNNLS</em>, <em>35</em>(12), 17730–17741. (<a
href="https://doi.org/10.1109/TNNLS.2023.3308191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning probabilistic models that can estimate the density of a given set of samples, and generate samples from that density, is one of the fundamental challenges in unsupervised machine learning. We introduce a new generative model based on denoising density estimators (DDEs), which are scalar functions parametrized by neural networks, that are efficiently trained to represent kernel density estimators of the data. Leveraging DDEs, our main contribution is a novel technique to obtain generative models by minimizing the Kullback–Leibler (KL)-divergence directly. We prove that our algorithm for obtaining generative models is guaranteed to converge consistently to the correct solution. Our approach does not require specific network architecture as in normalizing flows (NFs), nor use ordinary differential equation (ODE) solvers as in continuous NFs. Experimental results demonstrate substantial improvement in density estimation and competitive performance in generative model training.},
  archive      = {J_TNNLS},
  author       = {Siavash A. Bigdeli and Geng Lin and L. Andrea Dunbar and Tiziano Portenier and Matthias Zwicker},
  doi          = {10.1109/TNNLS.2023.3308191},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17730-17741},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning generative models using denoising density estimators},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiresolution interpretable contourlet graph network for
image classification. <em>TNNLS</em>, <em>35</em>(12), 17716–17729. (<a
href="https://doi.org/10.1109/TNNLS.2023.3307721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling contextual relationships in images as graph inference is an interesting and promising research topic. However, existing approaches only perform graph modeling of entities, ignoring the intrinsic geometric features of images. To overcome this problem, a novel multiresolution interpretable contourlet graph network (MICGNet) is proposed in this article. MICGNet delicately balances graph representation learning with the multiscale and multidirectional features of images, where contourlet is used to capture the hyperplanar directional singularities of images and multilevel sparse contourlet coefficients are encoded into graph for further graph representation learning. This process provides interpretable theoretical support for optimizing the model structure. Specifically, first, the superpixel-based region graph is constructed. Then, the region graph is applied to code the nonsubsampled contourlet transform (NSCT) coefficients of the image, which are considered as node features. Considering the statistical properties of the NSCT coefficients, we calculate the node similarity, i.e., the adjacency matrix, using Mahalanobis distance. Next, graph convolutional networks (GCNs) are employed to further learn more abstract multilevel NSCT-enhanced graph representations. Finally, the learnable graph assignment matrix is designed to get the geometric association representations, which accomplish the assignment of graph representations to grid feature maps. We conduct comparative experiments on six publicly available datasets, and the experimental analysis shows that MICGNet is significantly more effective and efficient than other algorithms of recent years.},
  archive      = {J_TNNLS},
  author       = {Jie Chen and Licheng Jiao and Xu Liu and Fang Liu and Lingling Li and Shuyuan Yang},
  doi          = {10.1109/TNNLS.2023.3307721},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17716-17729},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiresolution interpretable contourlet graph network for image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural networks on SPD manifolds for motor imagery
classification: A perspective from the time–frequency analysis.
<em>TNNLS</em>, <em>35</em>(12), 17701–17715. (<a
href="https://doi.org/10.1109/TNNLS.2023.3307470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motor imagery (MI) classification has been a prominent research topic in brain–computer interfaces (BCIs) based on electroencephalography (EEG). Over the past few decades, the performance of MI-EEG classifiers has seen gradual enhancement. In this study, we amplify the geometric deep-learning-based MI-EEG classifiers from the perspective of time–frequency analysis, introducing a new architecture called Graph-CSPNet. We refer to this category of classifiers as Geometric Classifiers, highlighting their foundation in differential geometry stemming from EEG spatial covariance matrices. Graph-CSPNet utilizes novel manifold-valued graph convolutional techniques to capture the EEG features in the time–frequency domain, offering heightened flexibility in signal segmentation for capturing localized fluctuations. To evaluate the effectiveness of Graph-CSPNet, we employ five commonly used publicly available MI-EEG datasets, achieving near-optimal classification accuracies in nine out of 11 scenarios. The Python repository can be found at https://github.com/GeometricBCI/Tensor-CSPNet-and-Graph-CSPNet .},
  archive      = {J_TNNLS},
  author       = {Ce Ju and Cuntai Guan},
  doi          = {10.1109/TNNLS.2023.3307470},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17701-17715},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph neural networks on SPD manifolds for motor imagery classification: A perspective from the Time–Frequency analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered adaptive antidisturbance switching control
for switched systems with dynamic neural network disturbance modeling.
<em>TNNLS</em>, <em>35</em>(12), 17688–17700. (<a
href="https://doi.org/10.1109/TNNLS.2023.3307389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a dynamic event-triggered adaptive antidisturbance (ETAAD) switching control strategy is proposed for switched systems subject to multisource disturbances. The disturbances are divided into two categories: the available unmodeled disturbance and the unavailable dynamic neural network modeled disturbance. First, a dynamic ET criterion is set based on the system state. Then, a novel dynamic ETA disturbance estimator is introduced to observe the modeled disturbance. Furthermore, according to the ET rule and adaptive disturbance observer, a switched controller is designed. Next, under the controller and switching criterion with the average dwell time limitation, sufficient conditions are given to force the switched systems to realize multisource disturbance suppression (DS), trajectory tracking, and communication resource (CR) saving simultaneously. Meanwhile, the Zeno phenomenon may be caused by the ET rule being excluded. In addition, the presented ETAAD approach is also applicable to the nonswitched systems case. Finally, a simulation case is given to validate the effectiveness of the dynamic ETAAD switching control method.},
  archive      = {J_TNNLS},
  author       = {Ying Zhao and Yuxuan Gao and Hong Sang and Jun Fu and Yuzhe Li},
  doi          = {10.1109/TNNLS.2023.3307389},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17688-17700},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive antidisturbance switching control for switched systems with dynamic neural network disturbance modeling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural networks for portfolio analysis with cardinality
constraints. <em>TNNLS</em>, <em>35</em>(12), 17674–17687. (<a
href="https://doi.org/10.1109/TNNLS.2023.3307192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio analysis is a crucial subject within modern finance. However, the classical Markowitz model, which was awarded the Nobel Prize in Economics in 1991, faces new challenges in contemporary financial environments. Specifically, it fails to consider transaction costs and cardinality constraints, which have become increasingly critical factors, particularly in the era of high-frequency trading. To address these limitations, this research is motivated by the successful application of machine learning tools in various engineering disciplines. In this work, three novel dynamic neural networks are proposed to tackle nonconvex portfolio optimization under the presence of transaction costs and cardinality constraints. The neural dynamics are intentionally designed to exploit the structural characteristics of the problem, and the proposed models are rigorously proven to achieve global convergence. To validate their effectiveness, experimental analysis is conducted using real stock market data of companies listed in the Dow Jones Index (DJI), covering the period from November 8, 2021 to November 8, 2022, encompassing an entire year. The results demonstrate the efficacy of the proposed methods. Notably, the proposed model achieves a substantial reduction in costs (which combines investment risk and reward) by as much as 56.71% compared with portfolios that are averagely selected.},
  archive      = {J_TNNLS},
  author       = {Xinwei Cao and Shuai Li},
  doi          = {10.1109/TNNLS.2023.3307192},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17674-17687},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural networks for portfolio analysis with cardinality constraints},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). K-relations-based consensus clustering with entropy-norm
regularizers. <em>TNNLS</em>, <em>35</em>(12), 17662–17673. (<a
href="https://doi.org/10.1109/TNNLS.2023.3307158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus clustering is to find a high quality and robust partition that is in agreement with multiple existing base clusterings. However, its computational cost is often very expensive and the quality of the final clustering is easily affected by uncertain consensus relations between clusters. In order to solve these problems, we develop a new $k$ -type algorithm, called $k$ -relations-based consensus clustering with double entropy-norm regularizers (KRCC-DE). In this algorithm, we build an optimization model to learn a consensus-relation matrix between final and base clusters and employ double entropy-norm regularizers to control the distribution of these consensus relations, which can reduce the impact of the uncertain consensus relations. The proposed algorithm uses an iterative strategy with strict updating formulas to get the optimal solution. Since its computation complexity is linear with the number of objects, base clusters, or final clusters, it can take low computational costs to effectively solve the consensus clustering problem. In experimental analysis, we compared the proposed algorithm with other $k$ -type-based and global-search consensus clustering algorithms on benchmark datasets. The experimental results illustrate that the proposed algorithm can balance the quality of the final clustering and its computational cost well.},
  archive      = {J_TNNLS},
  author       = {Liang Bai and Jiye Liang},
  doi          = {10.1109/TNNLS.2023.3307158},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17662-17673},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {K-relations-based consensus clustering with entropy-norm regularizers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph joint representation clustering via penalized graph
contrastive learning. <em>TNNLS</em>, <em>35</em>(12), 17650–17661. (<a
href="https://doi.org/10.1109/TNNLS.2023.3307149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering based on graph contrastive learning (GCL) is one of the dominant paradigms in the current graph clustering research field. However, those GCL-based methods often yield false negative samples, which can distort the learned representations and limit clustering performance. In order to alleviate this issue, we propose the idea of maintaining mutual information (MI) between the representations and the inputs to mitigate the loss of semantic information of false negative samples. We demonstrate the validity of this proposal through relevant experiments. Since maximizing MI can be approximately replaced by minimizing reconstruction error, we further propose a graph clustering method based on GCL penalized by reconstruction error, in which our carefully designed reconstruction decoder, as well as reconstruction error term, improve the clustering performance. In addition, we use a pseudo-label-guided strategy to improve the GCL process and further alleviate the problem of false negative samples. Our experiment results demonstrate the superiority and great potential of our proposed graph clustering method compared with state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Zihua Zhao and Rong Wang and Zheng Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3307149},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17650-17661},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph joint representation clustering via penalized graph contrastive learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multitask network for joint multispectral pansharpening on
diverse satellite data. <em>TNNLS</em>, <em>35</em>(12), 17635–17649.
(<a href="https://doi.org/10.1109/TNNLS.2023.3306896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the rapid advance in multispectral (MS) pansharpening, existing convolutional neural network (CNN)-based methods require training on separate CNNs for different satellite datasets. However, such a single-task learning (STL) paradigm often leads to overlooking any underlying correlations between datasets. Aiming at this challenging problem, a multitask network (MTNet) is presented to accomplish joint MS pansharpening in a unified framework for images acquired by different satellites. Particularly, the pansharpening process of each satellite is treated as a specific task, while MTNet simultaneously learns from all data obtained from these satellites following the multitask learning (MTL) paradigm. MTNet shares the generic knowledge between datasets via task-agnostic subnetwork (TASNet), utilizing task-specific subnetworks (TSSNets) to facilitate the adaptation of such knowledge to a certain satellite. To tackle the limitation of the local connectivity property of the CNN, TASNet incorporates Transformer modules to derive global information. In addition, band-aware dynamic convolutions (BDConvs) are proposed that can accommodate various ground scenes and bands by adjusting their respective receptive field (RF) size. Systematic experimental results over different datasets demonstrate that the proposed approach outperforms the existing state-of-the-art (SOTA) techniques.},
  archive      = {J_TNNLS},
  author       = {Dong Wang and Chanyue Wu and Yunpeng Bai and Ying Li and Changjing Shang and Qiang Shen},
  doi          = {10.1109/TNNLS.2023.3306896},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17635-17649},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A multitask network for joint multispectral pansharpening on diverse satellite data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated data quality assessment approach: Robust learning
with mixed label noise. <em>TNNLS</em>, <em>35</em>(12), 17620–17634.
(<a href="https://doi.org/10.1109/TNNLS.2023.3306874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has been an effective way to train a machine learning model distributedly, holding local data without exchanging them. However, due to the inaccessibility of local data, FL with label noise would be more challenging. Most existing methods assume only open-set or closed-set noise and correspondingly propose filtering or correction solutions, ignoring that label noise can be mixed in real-world scenarios. In this article, we propose a novel FL method to discriminate the type of noise and make the FL mixed noise-robust, named FedMIN. FedMIN employs a composite framework that captures local-global differences in multiparticipant distributions to model generalized noise patterns. By determining adaptive thresholds for identifying mixed label noise in each client and assigning appropriate weights during model aggregation, FedMIN enhances the performance of the global model. Furthermore, FedMIN incorporates a loss alignment mechanism using local and global Gaussian mixture models (GMMs) to mitigate the risk of revealing samplewise loss. Extensive experiments are conducted on several public datasets, which include the simulated FL testbeds, i.e., CIFAR-10, CIFAR-100, and SVHN, and the real-world ones, i.e., Camelyon17 and multiorgan nuclei challenge (MoNuSAC). Compared to FL benchmarks, FedMIN improves model accuracy by up to 9.9% due to its superior noise estimation capabilities.},
  archive      = {J_TNNLS},
  author       = {Bixiao Zeng and Xiaodong Yang and Yiqiang Chen and Hanchao Yu and Chunyu Hu and Yingwei Zhang},
  doi          = {10.1109/TNNLS.2023.3306874},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17620-17634},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Federated data quality assessment approach: Robust learning with mixed label noise},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Master–slave deep architecture for top-k multiarmed bandits
with nonlinear bandit feedback and diversity constraints.
<em>TNNLS</em>, <em>35</em>(12), 17608–17619. (<a
href="https://doi.org/10.1109/TNNLS.2023.3306801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel master–slave architecture to solve the top- $K$ combinatorial multiarmed bandits (CMABs) problem with nonlinear bandit feedback and diversity constraints, which, to the best of our knowledge, is the first combinatorial bandits setting considering diversity constraints under bandit feedback. Specifically, to efficiently explore the combinatorial and constrained action space, we introduce six slave models with distinguished merits to generate diversified samples well balancing rewards and constraints as well as efficiency. Moreover, we propose teacher learning-based optimization and the policy cotraining technique to boost the performance of the multiple slave models. The master model then collects the elite samples provided by the slave models and selects the best sample estimated by a neural contextual UCB-based network (NeuralUCB) to decide on a tradeoff between exploration and exploitation. Thanks to the elaborate design of slave models, the cotraining mechanism among slave models, and the novel interactions between the master and slave models, our approach significantly surpasses existing state-of-the-art algorithms in both synthetic and real datasets for recommendation tasks. The code is available at https://github.com/huanghanchi/Master-slave-Algorithm-for-Top-K-Bandits .},
  archive      = {J_TNNLS},
  author       = {Hanchi Huang and Li Shen and Deheng Ye and Wei Liu},
  doi          = {10.1109/TNNLS.2023.3306801},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17608-17619},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Master–Slave deep architecture for top-K multiarmed bandits with nonlinear bandit feedback and diversity constraints},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-driven spiking learning algorithm using aggregated
labels. <em>TNNLS</em>, <em>35</em>(12), 17596–17607. (<a
href="https://doi.org/10.1109/TNNLS.2023.3306749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional spiking learning algorithm aims to train neurons to spike at a specific time or on a particular frequency, which requires precise time and frequency labels in the training process. While in reality, usually only aggregated labels of sequential patterns are provided. The aggregate-label (AL) learning is proposed to discover these predictive features in distracting background streams only by aggregated spikes. It has achieved much success recently, but it is still computationally intensive and has limited use in deep networks. To address these issues, we propose an event-driven spiking aggregate learning algorithm (SALA) in this article. Specifically, to reduce the computational complexity, we improve the conventional spike-threshold-surface (STS) calculation in AL learning by analytical calculating voltage peak values in spiking neurons. Then we derive the algorithm to multilayers by event-driven strategy using aggregated spikes. We conduct comprehensive experiments on various tasks including temporal clue recognition, segmented and continuous speech recognition, and neuromorphic image classification. The experimental results demonstrate that the new STS method improves the efficiency of AL learning significantly, and the proposed algorithm outperforms the conventional spiking algorithm in various temporal clue recognition tasks.},
  archive      = {J_TNNLS},
  author       = {Xiurui Xie and Yansong Chua and Guisong Liu and Malu Zhang and Guangchun Luo and Huajin Tang},
  doi          = {10.1109/TNNLS.2023.3306749},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17596-17607},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-driven spiking learning algorithm using aggregated labels},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Block-wise partner learning for model compression.
<em>TNNLS</em>, <em>35</em>(12), 17582–17595. (<a
href="https://doi.org/10.1109/TNNLS.2023.3306512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great potential of convolutional neural networks (CNNs) in various tasks, the resource-hungry nature greatly hinders their wide deployment in cost-sensitive and low-powered scenarios, especially applications in remote sensing. Existing model pruning approaches, implemented by a “subtraction” operation, impose a performance ceiling on the slimmed model. Self-knowledge distillation (Self-KD) resorts to auxiliary networks that are only active in the training phase for performance improvement. However, the knowledge is holistic and crude, and the learning-based knowledge transfer is mediate and lossy. Here, we propose a novel model-compression method, termed block-wise partner learning (BPL), which comprises “extension” and “fusion” operations and liberates the compressed model from the bondage of baseline. Different from the Self-KD, the proposed BPL creates a partner for each block for performance enhancement in training. For the model to absorb more diverse information, a diversity loss (DL) is designed to evaluate the difference between the original block and the partner. Besides, the partner is fused equivalently instead of being discarded directly. After training, we can simply adopt the fused compressed model that contains the enhancement information of partners but with fewer parameters and less inference cost. As validated using the UC Merced land-use, NWPU-RESISC45, and RSD46-WHU datasets, the BPL demonstrates superiority over other compared model-compression approaches. For example, it attains a substantial floating-point operations (FLOPs) reduction of 73.97% with only 0.24 accuracy (ACC.) loss for ResNet-50 on the UC Merced land-use dataset. The code is available at https://github.com/zhangxin-xd/BPL .},
  archive      = {J_TNNLS},
  author       = {Xin Zhang and Weiying Xie and Yunsong Li and Jie Lei and Kai Jiang and Leyuan Fang and Qian Du},
  doi          = {10.1109/TNNLS.2023.3306512},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17582-17595},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Block-wise partner learning for model compression},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coevolutionary neural solution for nonconvex optimization
with noise tolerance. <em>TNNLS</em>, <em>35</em>(12), 17571–17581. (<a
href="https://doi.org/10.1109/TNNLS.2023.3306374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing solutions for nonconvex optimization problems show satisfactory performance in noise-free scenarios. However, they are prone to yield inaccurate results in the presence of noise in real-world problems, which may lead to failures in optimizing nonconvex problems. To this end, in this article, we propose a coevolutionary neural solution (CNS) by combining a simplified neurodynamics (SND) model with the particle swarm optimization (PSO) algorithm. Specifically, the proposed SND model does not leverage the time-derivative information, exhibiting greater stability compared to existing models. Furthermore, due to the noise tolerance capacity and rapid convergence property exhibited by the SND model, the CNS can rapidly achieve the optimal solution even in the presence of various perturbations. Theoretical analyses ensure that the proposed CNS is globally convergent with robustness and probability. In addition, the effectiveness of the CNS is compared with those of the existing solutions by a class of illustrative examples. We further apply the proposed solution to design a finite impulse response (FIR) filter and a pressure vessel to demonstrate its performance.},
  archive      = {J_TNNLS},
  author       = {Long Jin and Zeyu Su and Dongyang Fu and Xiuchun Xiao},
  doi          = {10.1109/TNNLS.2023.3306374},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17571-17581},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coevolutionary neural solution for nonconvex optimization with noise tolerance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint projection learning and tensor decomposition-based
incomplete multiview clustering. <em>TNNLS</em>, <em>35</em>(12),
17559–17570. (<a
href="https://doi.org/10.1109/TNNLS.2023.3306006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multiview clustering (IMVC) has received increasing attention since it is often that some views of samples are incomplete in reality. Most existing methods learn similarity subgraphs from original incomplete multiview data and seek complete graphs by exploring the incomplete subgraphs of each view for spectral clustering. However, the graphs constructed on the original high-dimensional data may be suboptimal due to feature redundancy and noise. Besides, previous methods generally ignored the graph noise caused by the interclass and intraclass structure variation during the transformation of incomplete graphs and complete graphs. To address these problems, we propose a novel joint projection learning and tensor decomposition (JPLTD)-based method for IMVC. Specifically, to alleviate the influence of redundant features and noise in high-dimensional data, JPLTD introduces an orthogonal projection matrix to project the high-dimensional features into a lower-dimensional space for compact feature learning. Meanwhile, based on the lower-dimensional space, the similarity graphs corresponding to instances of different views are learned, and JPLTD stacks these graphs into a third-order low-rank tensor to explore the high-order correlations across different views. We further consider the graph noise of projected data caused by missing samples and use a tensor-decomposition-based graph filter for robust clustering. JPLTD decomposes the original tensor into an intrinsic tensor and a sparse tensor. The intrinsic tensor models the true data similarities. An effective optimization algorithm is adopted to solve the JPLTD model. Comprehensive experiments on several benchmark datasets demonstrate that JPLTD outperforms the state-of-the-art methods. The code of JPLTD is available at https://github.com/weilvNJU/JPLTD .},
  archive      = {J_TNNLS},
  author       = {Wei Lv and Chao Zhang and Huaxiong Li and Xiuyi Jia and Chunlin Chen},
  doi          = {10.1109/TNNLS.2023.3306006},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17559-17570},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint projection learning and tensor decomposition-based incomplete multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical adversarial inverse reinforcement learning.
<em>TNNLS</em>, <em>35</em>(12), 17549–17558. (<a
href="https://doi.org/10.1109/TNNLS.2023.3305983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imitation learning (IL) has been proposed to recover the expert policy from demonstrations. However, it would be difficult to learn a single monolithic policy for highly complex long-horizon tasks of which the expert policy usually contains subtask hierarchies. Therefore, hierarchical IL (HIL) has been developed to learn a hierarchical policy from expert demonstrations through explicitly modeling the activity structure in a task with the option framework. Existing HIL methods either overlook the causal relationship between the subtask structure and the learned policy, or fail to learn the high-level and low-level policy in the hierarchical framework in conjuncture, which leads to suboptimality. In this work, we propose a novel HIL algorithm—hierarchical adversarial inverse reinforcement learning (H-AIRL), which extends a state-of-the-art (SOTA) IL algorithm—AIRL, with the one-step option framework. Specifically, we redefine the AIRL objectives on the extended state and action spaces, and further introduce a directed information term to the objective function to enhance the causality between the low-level policy and its corresponding subtask. Moreover, we propose an expectation-maximization (EM) adaption of our algorithm so that it can be applied to expert demonstrations without the subtask annotations which are more accessible in practice. Theoretical justifications of our algorithm design and evaluations on challenging robotic control tasks are provided to show the superiority of our algorithm compared with SOTA HIL baselines. The codes are available at https://github.com/LucasCJYSDL/HierAIRL .},
  archive      = {J_TNNLS},
  author       = {Jiayu Chen and Tian Lan and Vaneet Aggarwal},
  doi          = {10.1109/TNNLS.2023.3305983},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17549-17558},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical adversarial inverse reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A human–machine joint learning framework to boost endogenous
BCI training. <em>TNNLS</em>, <em>35</em>(12), 17534–17548. (<a
href="https://doi.org/10.1109/TNNLS.2023.3305621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–computer interfaces (BCIs) provide a direct pathway from the brain to external devices and have demonstrated great potential for assistive and rehabilitation technologies. Endogenous BCIs based on electroencephalogram (EEG) signals, such as motor imagery (MI) BCIs, can provide some level of control. However, mastering spontaneous BCI control requires the users to generate discriminative and stable brain signal patterns by imagery, which is challenging and is usually achieved over a very long training time (weeks/months). Here, we propose a human–machine joint learning framework to boost the learning process in endogenous BCIs, by guiding the user to generate brain signals toward an optimal distribution estimated by the decoder, given the historical brain signals of the user. To this end, we first model the human–machine joint learning process in a uniform formulation. Then a human–machine joint learning framework is proposed: 1) for the human side, we model the learning process in a sequential trial-and-error scenario and propose a novel “copy/new” feedback paradigm to help shape the signal generation of the subject toward the optimal distribution and 2) for the machine side, we propose a novel adaptive learning algorithm to learn an optimal signal distribution along with the subject’s learning process. Specifically, the decoder reweighs the brain signals generated by the subject to focus more on “good” samples to cope with the learning process of the subject. Online and psuedo-online BCI experiments with 18 healthy subjects demonstrated the advantages of the proposed joint learning process over coadaptive approaches in both learning efficiency and effectiveness.},
  archive      = {J_TNNLS},
  author       = {Hanwen Wang and Yu Qi and Lin Yao and Yueming Wang and Dario Farina and Gang Pan},
  doi          = {10.1109/TNNLS.2023.3305621},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17534-17548},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A Human–Machine joint learning framework to boost endogenous BCI training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). An adaptation-aware interactive learning approach for
multiple operational condition-based degradation modeling.
<em>TNNLS</em>, <em>35</em>(12), 17519–17533. (<a
href="https://doi.org/10.1109/TNNLS.2023.3305601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although degradation modeling has been widely applied to use multiple sensor signals to monitor the degradation process and predict the remaining useful lifetime (RUL) of operating machinery units, three challenging issues remain. One challenge is that units in engineering cases usually work under multiple operational conditions, causing the distribution of sensor signals to vary over conditions. It remains unexplored to characterize time-varying conditions as a distribution shift problem. The second challenge is that sensor signal fusion and degradation status modeling are separated into two independent steps in most of the existing methods, which ignores the intrinsic correlation between the two parts. The last challenge is how to find an accurate health index (HI) of units using previous knowledge of degradation. To tackle these issues, this article proposes an adaptation-aware interactive learning (AAIL) approach for degradation modeling. First, a condition-invariant HI is developed to handle time-varying operation conditions. Second, an interactive framework based on the fusion and degradation model is constructed, which naturally integrates a supervised learner and an unsupervised learner. To estimate the model parameters of AAIL, we propose an interactive training algorithm that shares learned degradation and fusion information during the model training process. A case study that uses the degradation data set of aircraft engines demonstrates that the proposed AAIL outperforms related benchmark methods.},
  archive      = {J_TNNLS},
  author       = {Di Wang and Ying Wang and Xiaochen Xian and Bin Cheng},
  doi          = {10.1109/TNNLS.2023.3305601},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17519-17533},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An adaptation-aware interactive learning approach for multiple operational condition-based degradation modeling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). Toward learning joint inference tasks for IASS-MTS using
dual attention memory with stochastic generative imputation.
<em>TNNLS</em>, <em>35</em>(12), 17504–17518. (<a
href="https://doi.org/10.1109/TNNLS.2023.3305542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Irregularly, asynchronously and sparsely sampled multivariate time series (IASS-MTS) are characterized by sparse and uneven time intervals and nonsynchronous sampling rates, posing significant challenges for machine learning models to learn complex relationships within and beyond IASS-MTS to support various inference tasks. The existing methods typically either focus solely on single-task forecasting or simply concatenate them through a separate preprocessing imputation procedure for the subsequent classification application. However, these methods often ignore valuable annotated labels or fail to discover meaningful patterns from unlabeled data. Moreover, the approach of separate prefilling may introduce errors due to the noise in raw records, and thus degrade the downstream prediction performance. To overcome these challenges, we propose the time-aware dual attention and memory-augmented network (DAMA) with stochastic generative imputation (SGI). Our model constructs a joint task learning architecture that unifies imputation and classification tasks collaboratively. First, we design a new time-aware DAMA that accounts for irregular sampling rates, inherent data nonalignment, and sparse values in IASS-MTS data. The proposed network integrates both attention and memory to effectively analyze complex interactions within and across IASS-MTS for the classification task. Second, we develop the stochastic generative imputation (SGI) network that uses auxiliary information from sequence data for inferring the time series missing observations. By balancing joint tasks, our model facilitates interaction between them, leading to improved performance on both classification and imputation tasks. Third, we evaluate our model on real-world datasets and demonstrate its superior performance in terms of imputation accuracy and classification results, outperforming the baselines.},
  archive      = {J_TNNLS},
  author       = {Zhen Wang and Yang Zhang and Yan Pang and Nannan Wang and Mohamed Jaward Bah and Ke Li and Ji Zhang},
  doi          = {10.1109/TNNLS.2023.3305542},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17504-17518},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward learning joint inference tasks for IASS-MTS using dual attention memory with stochastic generative imputation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite-horizon h∞ state estimation for complex networks with
uncertain couplings and packet losses: Handling amplify-and-forward
relays. <em>TNNLS</em>, <em>35</em>(12), 17493–17503. (<a
href="https://doi.org/10.1109/TNNLS.2023.3304515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the state estimation problem for a class of complex networks (CNs) with uncertain inner couplings and packet losses over communication networks. The inner couplings are allowed to be uncertain and varying in a specific interval. The amplify-and-forward (AaF) relay protocols are introduced to improve the communication quality and enhance the propagation distance. The Bernoulli random variables are used to characterize the randomly occurring packet losses encountered in communication channels. The focus of this article is on the design of a state estimator for each node of CNs such that a prescribed $H_{\infty }$ performance constraint is satisfied for the dynamical error system over a finite horizon. A sufficient condition is first provided to verify the existence of the desired $H_{\infty }$ state estimator, and the estimator gain is then determined by solving two coupled backward Riccati difference equations (RDEs). Subsequently, a recursive state estimation algorithm is put forward that is suitable for online computation. Finally, a numerical example is given to demonstrate the effectiveness of the proposed estimation method.},
  archive      = {J_TNNLS},
  author       = {Xueyang Meng and Zidong Wang and Fan Wang and Yun Chen},
  doi          = {10.1109/TNNLS.2023.3304515},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17493-17503},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-horizon h∞ state estimation for complex networks with uncertain couplings and packet losses: Handling amplify-and-forward relays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedGAMMA: Federated learning with global sharpness-aware
minimization. <em>TNNLS</em>, <em>35</em>(12), 17479–17492. (<a
href="https://doi.org/10.1109/TNNLS.2023.3304453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a promising framework for privacy-preserving and distributed training with decentralized clients. However, there exists a large divergence between the collected local updates and the expected global update, which is known as the client drift and mainly caused by heterogeneous data distribution among clients, multiple local training steps, and partial client participation training. Most existing works tackle this challenge based on the empirical risk minimization (ERM) rule, while less attention has been paid to the relationship between the global loss landscape and the generalization ability. In this work, we propose FedGAMMA, a novel FL algorithm with Global sharpness-Aware MiniMizAtion to seek a global flat landscape with high performance. Specifically, in contrast to FedSAM which only seeks the local flatness and still suffers from performance degradation when facing the client-drift issue, we adopt a local varieties control technique to better align each client’s local updates to alleviate the client drift and make each client heading toward the global flatness together. Finally, extensive experiments demonstrate that FedGAMMA can substantially outperform several existing FL baselines on various datasets, and it can well address the client-drift issue and simultaneously seek a smoother and flatter global landscape.},
  archive      = {J_TNNLS},
  author       = {Rong Dai and Xun Yang and Yan Sun and Li Shen and Xinmei Tian and Meng Wang and Yongdong Zhang},
  doi          = {10.1109/TNNLS.2023.3304453},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17479-17492},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FedGAMMA: Federated learning with global sharpness-aware minimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Crowd counting based on multiscale spatial guided
perception aggregation network. <em>TNNLS</em>, <em>35</em>(12),
17465–17478. (<a
href="https://doi.org/10.1109/TNNLS.2023.3304348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting has received extensive attention in the field of computer vision, and methods based on deep convolutional neural networks (CNNs) have made great progress in this task. However, challenges such as scale variation, nonuniform distribution, complex background, and occlusion in crowded scenes hinder the performance of these networks in crowd counting. In order to overcome these challenges, this article proposes a multiscale spatial guidance perception aggregation network (MGANet) to achieve efficient and accurate crowd counting. MGANet consists of three parts: multiscale feature extraction network (MFEN), spatial guidance network (SGN), and attention fusion network (AFN). Specifically, to alleviate the scale variation problem in crowded scenes, MFEN is introduced to enhance the scale adaptability and effectively capture multiscale features in scenes with drastic scale variation. To address the challenges of nonuniform distribution and complex background in population, an SGN is proposed. The SGN includes two parts: the spatial context network (SCN) and the guidance perception network (GPN). SCN is used to capture the detailed semantic information between the multiscale feature positions extracted by MFEN, and improve the ability of deep structured information exploration. At the same time, the dependence relationship between the spatial remote context is established to enhance the receptive field. GPN is used to enhance the information exchange between channels and guide the network to select appropriate multiscale features and spatial context semantic features. AFN is used to adaptively measure the importance of the above different features, and obtain accurate and effective feature representations from them. In addition, this article proposes a novel region-adaptive loss function, which optimizes the regions with large recognition errors in the image, and alleviates the inconsistency between the training target and the evaluation metric. In order to evaluate the performance of the proposed method, extensive experiments were carried out on challenging benchmarks including ShanghaiTech Part A and Part B, UCF-CC-50, UCF-QNRF, and JHU-CROWD++. Experimental results show that the proposed method has good performance on all four datasets. Especially on ShanghaiTech Part A and Part B, CUCF-QNRF, and JHU-CROWD++ datasets, compared with the state-of-the-art methods, our proposed method achieves superior recognition performance and better robustness.},
  archive      = {J_TNNLS},
  author       = {Zhangping Chen and Shuo Zhang and Xiaoqing Zheng and Xiaodong Zhao and Yaguang Kong},
  doi          = {10.1109/TNNLS.2023.3304348},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17465-17478},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Crowd counting based on multiscale spatial guided perception aggregation network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast evolutionary knowledge transfer search for multiscale
deep neural architecture. <em>TNNLS</em>, <em>35</em>(12), 17450–17464.
(<a href="https://doi.org/10.1109/TNNLS.2023.3304291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of neural architecture search (NAS) algorithms has removed the constraints on manually designed neural network architectures, so that neural network development no longer requires extensive professional knowledge, trial and error. However, the extremely high computational cost limits the development of NAS algorithms. In this article, in order to reduce computational costs and to improve the efficiency and effectiveness of evolutionary NAS (ENAS) is investigated. In this article, we present a fast ENAS framework for multiscale convolutional networks based on evolutionary knowledge transfer search (EKTS). This framework is novel, in that it combines global optimization methods with local optimization methods for search, and searches a multiscale network architecture. In this article, evolutionary computation is used as a global optimization algorithm with high robustness and wide applicability for searching neural architectures. At the same time, for fast search, we combine knowledge transfer and local fast learning to improve the search speed. In addition, we explore a multiscale gray-box structure. This gray box structure combines the Bandelet transform with convolution to improve network approximation, learning, and generalization. Finally, we compare the architectures with more than 40 different neural architectures, and the results confirmed its effectiveness.},
  archive      = {J_TNNLS},
  author       = {Ruohan Zhang and Licheng Jiao and Dan Wang and Fang Liu and Xu Liu and Shuyuan Yang},
  doi          = {10.1109/TNNLS.2023.3304291},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17450-17464},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A fast evolutionary knowledge transfer search for multiscale deep neural architecture},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive dynamic event-triggered distributed output observer
for leader–follower multiagent systems under directed graphs.
<em>TNNLS</em>, <em>35</em>(12), 17440–17449. (<a
href="https://doi.org/10.1109/TNNLS.2023.3303863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leader–follower consensus problem for multiagent systems (MASs) is an important research hotspot. However, the existing methods take the leader system matrix as a priori knowledge for each agent to design the controller and use the leader’s state information. In fact, only the output information may be available in some practical applications. On this basis, this article first designs a novel adaptive distributed dynamic event-triggered observer for each follower to learn the minimum polynomial coefficients of the leader system matrix instead of the leader system matrix. The proposed method is scalable and suitable for large-scale MASs and can reduce the information transmission dimension in observer design. Then, an adaptive dynamic event-triggered compensator based on the observer and leader output information is designed for each follower, thereby solving the leader–follower consensus problem. Finally, several simulation examples are given to verify the effectiveness of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Juan Zhang and Huaguang Zhang and Wei Wang and Yanhong Luo and Gang Wang},
  doi          = {10.1109/TNNLS.2023.3303863},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17440-17449},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive dynamic event-triggered distributed output observer for Leader–Follower multiagent systems under directed graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online and robust intermittent motion planning in dynamic
and changing environments. <em>TNNLS</em>, <em>35</em>(12), 17425–17439.
(<a href="https://doi.org/10.1109/TNNLS.2023.3303811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose RRT- $\text{Q}^{{\,\textrm {X}}}_{\infty }$ , an online and intermittent kinodynamic motion planning framework for dynamic environments with unknown robot dynamics and unknown disturbances. We leverage RRT $^{\text {X}}$ for global path planning and rapid replanning to produce waypoints as a sequence of boundary-value problems (BVPs). For each BVP, we formulate a finite-horizon, continuous-time zero-sum game, where the control input is the minimizer, and the worst case disturbance is the maximizer. We propose a robust intermittent Q-learning controller for waypoint navigation with completely unknown system dynamics, external disturbances, and intermittent control updates. We execute a relaxed persistence of excitation technique to guarantee that the Q-learning controller converges to the optimal controller. We provide rigorous Lyapunov-based proofs to guarantee the closed-loop stability of the equilibrium point. The effectiveness of the proposed RRT- $\text{Q}^{{\,\textrm {X}}}_{\infty }$ is illustrated with Monte Carlo numerical experiments in numerous dynamic and changing environments.},
  archive      = {J_TNNLS},
  author       = {Zirui Xu and George P. Kontoudis and Kyriakos G. Vamvoudakis},
  doi          = {10.1109/TNNLS.2023.3303811},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17425-17439},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online and robust intermittent motion planning in dynamic and changing environments},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When to switch: Planning and learning for partially
observable multi-agent pathfinding. <em>TNNLS</em>, <em>35</em>(12),
17411–17424. (<a
href="https://doi.org/10.1109/TNNLS.2023.3303502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent pathfinding (MAPF) is a problem that involves finding a set of non-conflicting paths for a set of agents confined to a graph. In this work, we study a MAPF setting, where the environment is only partially observable for each agent, i.e., an agent observes the obstacles and other agents only within a limited field-of-view. Moreover, we assume that the agents do not communicate and do not share knowledge on their goals, intended actions, etc. The task is to construct a policy that maps the agent’s observations to actions. Our contribution is multifold. First, we propose two novel policies for solving partially observable MAPF (PO-MAPF): one based on heuristic search and another one based on reinforcement learning (RL). Next, we introduce a mixed policy that is based on switching between the two. We suggest three different switch scenarios: the heuristic, the deterministic, and the learnable one. A thorough empirical evaluation of all the proposed policies in a variety of setups shows that the mixing policy demonstrates the best performance is able to generalize well to the unseen maps and problem instances, and, additionally, outperforms the state-of-the-art counterparts (PRIMAL2 and PICO). The source-code is available at https://github.com/AIRI-Institute/when-to-switch .},
  archive      = {J_TNNLS},
  author       = {Alexey Skrynnik and Anton Andreychuk and Konstantin Yakovlev and Aleksandr I. Panov},
  doi          = {10.1109/TNNLS.2023.3303502},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17411-17424},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {When to switch: Planning and learning for partially observable multi-agent pathfinding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ricci curvature-based graph sparsification for continual
graph representation learning. <em>TNNLS</em>, <em>35</em>(12),
17398–17410. (<a
href="https://doi.org/10.1109/TNNLS.2023.3303454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory replay, which stores a subset of historical data from previous tasks to replay while learning new tasks, exhibits state-of-the-art performance for various continual learning applications on the Euclidean data. While topological information plays a critical role in characterizing graph data, existing memory replay-based graph learning techniques only store individual nodes for replay and do not consider their associated edge information. To this end, based on the message-passing mechanism in graph neural networks (GNNs), we present the Ricci curvature-based graph sparsification technique to perform continual graph representation learning. Specifically, we first develop the subgraph episodic memory (SEM) to store the topological information in the form of computation subgraphs. Next, we sparsify the subgraphs such that they only contain the most informative structures (nodes and edges). The informativeness is evaluated with the Ricci curvature, a theoretically justified metric to estimate the contribution of neighbors to represent a target node. In this way, we can reduce the memory consumption of a computation subgraph from $\mathcal {O}(d^{L})$ to $\mathcal {O}(1)$ and enable GNNs to fully utilize the most informative topological information for memory replay. Besides, to ensure the applicability on large graphs, we also provide the theoretically justified surrogate for the Ricci curvature in the sparsification process, which can greatly facilitate the computation. Finally, our empirical studies show that SEM outperforms state-of-the-art approaches significantly on four different public datasets. Unlike existing methods, which mainly focus on task incremental learning (task-IL) setting, SEM also succeeds in the challenging class incremental learning (class-IL) setting in which the model is required to distinguish all learned classes without task indicators and even achieves comparable performance to joint training, which is the performance upper bound for continual learning.},
  archive      = {J_TNNLS},
  author       = {Xikun Zhang and Dongjin Song and Dacheng Tao},
  doi          = {10.1109/TNNLS.2023.3303454},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17398-17410},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ricci curvature-based graph sparsification for continual graph representation learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bio-inspired spiking attentional neural network for
attentional selection in the listening brain. <em>TNNLS</em>,
<em>35</em>(12), 17387–17397. (<a
href="https://doi.org/10.1109/TNNLS.2023.3303308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans show a remarkable ability in solving the cocktail party problem. Decoding auditory attention from the brain signals is a major step toward the development of bionic ears emulating human capabilities. Electroencephalography (EEG)-based auditory attention detection (AAD) has attracted considerable interest recently. Despite much progress, the performance of traditional AAD decoders remains to be improved, especially in low-latency settings. State-of-the-art AAD decoders based on deep neural networks generally lack the intrinsic temporal coding ability in biological networks. In this study, we first propose a bio-inspired spiking attentional neural network, denoted as BSAnet, for decoding auditory attention. BSAnet is capable of exploiting the temporal dynamics of EEG signals using biologically plausible neurons and an attentional mechanism. Experiments on two publicly available datasets confirm the superior performance of BSAnet over other state-of-the-art systems across various evaluation conditions. Moreover, BSAnet imitates realistic brain-like information processing, through which we show the advantage of brain-inspired computational models.},
  archive      = {J_TNNLS},
  author       = {Siqi Cai and Peiwen Li and Haizhou Li},
  doi          = {10.1109/TNNLS.2023.3303308},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17387-17397},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A bio-inspired spiking attentional neural network for attentional selection in the listening brain},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph evolution-based vertex extraction for hyperspectral
anomaly detection. <em>TNNLS</em>, <em>35</em>(12), 17372–17386. (<a
href="https://doi.org/10.1109/TNNLS.2023.3303273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a fundamental task in hyperspectral image (HSI) processing. However, most existing methods rely on pixel feature vectors and overlook the relational structure information between pixels, limiting the detection performance. In this article, we propose a novel approach to hyperspectral anomaly detection that characterizes the HSI data using a vertex- and edge-weighted graph with the pixels as vertices. The constructed graph encodes rich structural information in an affinity matrix. A crucial innovation of our method is the ability to obtain internal relations between pixels at multiple topological scales by processing different powers of the affinity matrix. This power processing is viewed as a graph evolution, which enables anomaly detection using vertex extraction formulated as a quadratic programming problem on graphs of varying topological scales. We also design a hierarchical guided filtering architecture to fuse multiscale detection results derived from graph evolution, which significantly reduces the false alarm rate. Our approach effectively characterizes the topological properties of HSIs, leveraging the structural information between pixels to improve anomaly detection accuracy. Experimental results on four real HSIs demonstrate the superior detection performance of our proposed approach compared to some state-of-the-art hyperspectral anomaly detection methods.},
  archive      = {J_TNNLS},
  author       = {Xianchang Yang and Bing Tu and Qianming Li and Jun Li and Antonio Plaza},
  doi          = {10.1109/TNNLS.2023.3303273},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17372-17386},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph evolution-based vertex extraction for hyperspectral anomaly detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A memory-efficient federated kernel support vector machine
for edge devices. <em>TNNLS</em>, <em>35</em>(12), 17359–17371. (<a
href="https://doi.org/10.1109/TNNLS.2023.3302802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A federated learning (FL) scheme (denoted as Fed-KSVM) is designed to train kernel support vector machines (SVMs) over multiple edge devices with low memory consumption. To decompose the training process of kernel SVM, each edge device first constructs high-dimensional random feature vectors of its local data, and then trains a local SVM model over the random feature vectors. To reduce the memory consumption on each edge device, the optimization problem of the local model is divided into several subproblems. Each subproblem only optimizes a subset of the model parameters over a block of random feature vectors with a low dimension. To achieve the same optimal solution to the original optimization problem, an incremental learning algorithm called block boosting is designed to solve these subproblems sequentially. After training of the local models, the central server constructs a global SVM model by averaging the model parameters of these local models. Fed-KSVM only increases the iterations of training the local SVM models to save the memory consumption, while the communication rounds between the edge devices and the central server are not affected. Theoretical analysis shows that the kernel SVM model trained by Fed-KSVM converges to the optimal model with a linear convergence rate. Because of such a fast convergence rate, Fed-KSVM reduces the communication cost during training by up to 99% compared with the centralized training method. The experimental results also show that Fed-KSVM reduces the memory consumption on the edge devices by nearly 90% while achieving the highest test accuracy, compared with the state-of-the-art schemes.},
  archive      = {J_TNNLS},
  author       = {Xiaochen Zhou and Xudong Wang},
  doi          = {10.1109/TNNLS.2023.3302802},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17359-17371},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A memory-efficient federated kernel support vector machine for edge devices},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A complex-former tracker with dynamic polar spatio-temporal
encoding. <em>TNNLS</em>, <em>35</em>(12), 17344–17358. (<a
href="https://doi.org/10.1109/TNNLS.2023.3302368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the excellent performance of transformer has attracted the attention of the visual community. Visual transformer models usually reshape images into sequence format and encode them sequentially. However, it is difficult to explicitly represent the relative relationship in distance and direction of visual data with typical 2-D spatial structures. Also, the temporal motion properties of consecutive frames are hardly exploited when it comes to dynamic video tasks like tracking. Therefore, we propose a novel dynamic polar spatio-temporal encoding for video scenes. We use spiral functions in polar space to fully exploit the spatial dependences of distance and direction in real scenes. We then design a dynamic relative encoding mode for continuous frames to capture the continuous spatio-temporal motion characteristics among video frames. Finally, we construct a complex-former framework with the proposed encoding applied to video-tracking tasks, where the complex fusion mode (CFM) realizes the effective fusion of scenes and positions for consecutive frames. The theoretical analysis demonstrates the feasibility and effectiveness of our proposed method. The experimental results on multiple datasets validate that our method can improve tracker performance in various video scenarios.},
  archive      = {J_TNNLS},
  author       = {Xiaotong Li and Licheng Jiao and Hao Zhu and Zhongjian Huang and Fang Liu and Lingling Li and Puhua Chen and Shuyuan Yang},
  doi          = {10.1109/TNNLS.2023.3302368},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17344-17358},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A complex-former tracker with dynamic polar spatio-temporal encoding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximum correntropy filtering for complex networks with
uncertain dynamical bias: Enabling componentwise event-triggered
transmission. <em>TNNLS</em>, <em>35</em>(12), 17330–17343. (<a
href="https://doi.org/10.1109/TNNLS.2023.3302190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the maximum correntropy filtering (MCF) problem for a class of nonlinear complex networks subject to non-Gaussian noises and uncertain dynamical bias. With aim to utilize the constrained network bandwidth and energy resources in an efficient way, a componentwise dynamic event-triggered transmission (DETT) protocol is adopted to ensure that each sensor component independently determines the time instant for transmitting data according to the individual triggering condition. The principal purpose of the addressed problem is to put forward a dynamic event-triggered recursive filtering scheme under the maximum correntropy criterion, such that the effects of the non-Gaussian noises can be attenuated. In doing so, a novel correntropy-based performance index (CBPI) is first proposed to reflect the impacts from the componentwise DETT mechanism, the system nonlinearity, and the uncertain dynamical bias. The CBPI is parameterized by deriving upper bounds on the one-step prediction error covariance and the equivalent noise covariance. Subsequently, the filter gain matrix is designed by means of maximizing the proposed CBPI. Finally, an illustrative example is provided to substantiate the feasibility and effectiveness of the developed MCF scheme.},
  archive      = {J_TNNLS},
  author       = {Weihao Song and Zidong Wang and Zhongkui Li and Qing-Long Han and Dong Yue},
  doi          = {10.1109/TNNLS.2023.3302190},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17330-17343},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Maximum correntropy filtering for complex networks with uncertain dynamical bias: Enabling componentwise event-triggered transmission},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust reward-free actor–critic for cooperative multiagent
reinforcement learning. <em>TNNLS</em>, <em>35</em>(12), 17318–17329.
(<a href="https://doi.org/10.1109/TNNLS.2023.3302131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider centralized training and decentralized execution (CTDE) with diverse and private reward functions in cooperative multiagent reinforcement learning (MARL). The main challenge is that an unknown number of agents, whose identities are also unknown, can deliberately generate malicious messages and transmit them to the central controller. We term these malicious actions as Byzantine attacks. First, without Byzantine attacks, we propose a reward-free deep deterministic policy gradient (RF-DDPG) algorithm, in which gradients of agents’ critics rather than rewards are sent to the central controller for preserving privacy. Second, to cope with Byzantine attacks, we develop a robust extension of RF-DDPG termed R2F-DDPG, which replaces the vulnerable average aggregation rule with robust ones. We propose a novel class of RL-specific Byzantine attacks that fail conventional robust aggregation rules, motivating the projection-boosted robust aggregation rules for R2F-DDPG. Numerical experiments show that RF-DDPG successfully trains agents to work cooperatively and that R2F-DDPG demonstrates robustness to Byzantine attacks.},
  archive      = {J_TNNLS},
  author       = {Qifeng Lin and Qing Ling},
  doi          = {10.1109/TNNLS.2023.3302131},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17318-17329},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust reward-free Actor–Critic for cooperative multiagent reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AAformer: Auto-aligned transformer for person
re-identification. <em>TNNLS</em>, <em>35</em>(12), 17307–17317. (<a
href="https://doi.org/10.1109/TNNLS.2023.3301856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In person re-identification (re-ID), extracting part-level features from person images has been verified to be crucial to offer fine-grained information. Most of the existing CNN-based methods only locate the human parts coarsely, or rely on pretrained human parsing models and fail in locating the identifiable nonhuman parts (e.g., knapsack). In this article, we introduce an alignment scheme in transformer architecture for the first time and propose the auto-aligned transformer (AAformer) to automatically locate both the human parts and nonhuman ones at patch level. We introduce the “Part tokens ([PART]s),” which are learnable vectors, to extract part features in the transformer. A [PART] only interacts with a local subset of patches in self-attention and learns to be the part representation. To adaptively group the image patches into different subsets, we design the auto-alignment. Auto-alignment employs a fast variant of optimal transport (OT) algorithm to online cluster the patch embeddings into several groups with the [PART]s as their prototypes. AAformer integrates the part alignment into the self-attention and the output [PART]s can be directly used as part features for retrieval. Extensive experiments validate the effectiveness of [PART]s and the superiority of AAformer over various state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Kuan Zhu and Haiyun Guo and Shiliang Zhang and Yaowei Wang and Jing Liu and Jinqiao Wang and Ming Tang},
  doi          = {10.1109/TNNLS.2023.3301856},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17307-17317},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AAformer: Auto-aligned transformer for person re-identification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised monocular depth estimation with
self-perceptual anomaly handling. <em>TNNLS</em>, <em>35</em>(12),
17292–17306. (<a
href="https://doi.org/10.1109/TNNLS.2023.3301711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is attractive to extract plausible 3-D information from a single 2-D image, and self-supervised learning has shown impressive potential in this field. However, when only monocular videos are available as training data, moving objects at similar speeds to the camera can disturb the reprojection process during training. Existing methods filter out some moving pixels by comparing pixelwise photometric error, but the illumination inconsistency between frames leads to incomplete filtering. In addition, existing methods calculate photometric error within local windows, which leads to the fact that even if an anomalous pixel is masked out, it can still implicitly disturb the reprojection process, as long as it is in the local neighborhood of a nonanomalous pixel. Moreover, the ill-posed nature of monocular depth estimation makes the same scene correspond to multiple plausible depth maps, which damages the robustness of the model. In order to alleviate the above problems, we propose: 1) a self-reprojection mask to further filter out moving objects while avoiding illumination inconsistency; 2) a self-statistical mask method to prevent the filtered anomalous pixels from implicitly disturbing the reprojection; and 3) a self-distillation augmentation consistency loss to reduce the impact of ill-posed nature of monocular depth estimation. Our method shows superior performance on the KITTI dataset, especially when evaluating only the depth of potential moving objects.},
  archive      = {J_TNNLS},
  author       = {Yourun Zhang and Maoguo Gong and Mingyang Zhang and Jianzhao Li},
  doi          = {10.1109/TNNLS.2023.3301711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17292-17306},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised monocular depth estimation with self-perceptual anomaly handling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observer-based consensus control for MASs with prescribed
constraints via reinforcement learning algorithm. <em>TNNLS</em>,
<em>35</em>(12), 17281–17291. (<a
href="https://doi.org/10.1109/TNNLS.2023.3301538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive optimal consensus control problem is studied for multiagent systems (MASs) with external disturbances, unmeasurable states, and prescribed constraints. First, by using neural networks (NNs), a composite observer is constructed to estimate the unmeasurable states and disturbances simultaneously. Then, the consensus error is guaranteed within a prescribed boundary by presenting an improved prescribed performance control (PPC) technique, and the initial conditions for the error are eliminated. In addition, the updating laws of actor-critic NNs are established by using a simplified reinforcement learning (RL) algorithm based on the uniqueness of optimal solution, and the asymmetric input saturation is resolved by designing auxiliary system instead of using nonquadratic cost functions in other optimal control methods. Finally, the boundedness of all signals in the closed-loop system is proved by using Lyapunov stability theory. The effectiveness of the proposed control method is verified by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Ao Luo and Qi Zhou and Hui Ma and Hongyi Li},
  doi          = {10.1109/TNNLS.2023.3301538},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17281-17291},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based consensus control for MASs with prescribed constraints via reinforcement learning algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain network classification for accurate detection of
alzheimer’s disease via manifold harmonic discriminant analysis.
<em>TNNLS</em>, <em>35</em>(12), 17266–17280. (<a
href="https://doi.org/10.1109/TNNLS.2023.3301456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mounting evidence shows that Alzheimer’s disease (AD) manifests the dysfunction of the brain network much earlier before the onset of clinical symptoms, making its early diagnosis possible. Current brain network analyses treat high-dimensional network data as a regular matrix or vector, which destroys the essential network topology, thereby seriously affecting diagnosis accuracy. In this context, harmonic waves provide a solid theoretical background for exploring brain network topology. However, the harmonic waves are originally intended to discover neurological disease propagation patterns in the brain, which makes it difficult to accommodate brain disease diagnosis with high heterogeneity. To address this challenge, this article proposes a network manifold harmonic discriminant analysis (MHDA) method for accurately detecting AD. Each brain network is regarded as an instance drawn on a Stiefel manifold. Every instance is represented by a set of orthonormal eigenvectors (i.e., harmonic waves) derived from its Laplacian matrix, which fully respects the topological structure of the brain network. An MHDA method within the Stiefel space is proposed to identify the group-dependent common harmonic waves, which can be used as group-specific references for downstream analyses. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method in stratifying cognitively normal (CN) controls, mild cognitive impairment (MCI), and AD.},
  archive      = {J_TNNLS},
  author       = {Hongmin Cai and Xiaoqi Sheng and Guorong Wu and Bin Hu and Yiu-Ming Cheung and Jiazhou Chen},
  doi          = {10.1109/TNNLS.2023.3301456},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17266-17280},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Brain network classification for accurate detection of alzheimer’s disease via manifold harmonic discriminant analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal adaptive tracking control of partially uncertain
nonlinear discrete-time systems using lifelong hybrid learning.
<em>TNNLS</em>, <em>35</em>(12), 17254–17265. (<a
href="https://doi.org/10.1109/TNNLS.2023.3301383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses a multilayer neural network (MNN)-based optimal adaptive tracking of partially uncertain nonlinear discrete-time (DT) systems in affine form. By employing an actor–critic neural network (NN) to approximate the value function and optimal control policy, the critic NN is updated via a novel hybrid learning scheme, where its weights are adjusted once at a sampling instant and also in a finite iterative manner within the instants to enhance the convergence rate. Moreover, to deal with the persistency of excitation (PE) condition, a replay buffer is incorporated into the critic update law through concurrent learning. To address the vanishing gradient issue, the actor and critic MNN weights are tuned using control input and temporal difference errors (TDEs), respectively. In addition, a weight consolidation scheme is incorporated into the critic MNN update law to attain lifelong learning and overcome catastrophic forgetting, thus lowering the cumulative cost. The tracking error, and the actor and critic weight estimation errors are shown to be bounded using the Lyapunov analysis. Simulation results using the proposed approach on a two-link robot manipulator show a significant reduction in tracking error by 44% and cumulative cost by 31% in a multitask environment.},
  archive      = {J_TNNLS},
  author       = {Behzad Farzanegan and Rohollah Moghadam and Sarangapani Jagannathan and Pappa Natarajan},
  doi          = {10.1109/TNNLS.2023.3301383},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17254-17265},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal adaptive tracking control of partially uncertain nonlinear discrete-time systems using lifelong hybrid learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial multiview representation learning with cross-view
generation. <em>TNNLS</em>, <em>35</em>(12), 17239–17253. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview learning has made significant progress in recent years. However, an implicit assumption is that multiview data are complete, which is often contrary to practical applications. Due to human or data acquisition equipment errors, what we actually get is partial multiview data, which existing multiview algorithms are limited to processing. Modeling complex dependencies between views in terms of consistency and complementarity remains challenging, especially in partial multiview data scenarios. To address the above issues, this article proposes a deep Gaussian cross-view generation model (named PMvCG), which aims to model views according to the principles of consistency and complementarity and eventually learn the comprehensive representation of partial multiview data. PMvCG can discover cross-view associations by learning view-sharing and view-specific features of different views in the representation space. The missing views can be reconstructed and are applied in turn to further optimize the model. The estimated uncertainty in the model is also considered and integrated into the representation to improve the performance. We design a variational inference and iterative optimization algorithm to solve PMvCG effectively. We conduct comprehensive experiments on multiple real-world datasets to validate the performance of PMvCG. We compare the PMvCG with various methods by applying the learned representation to clustering and classification. We also provide more insightful analysis to explore the PMvCG, such as convergence analysis, parameter sensitivity analysis, and the effect of uncertainty in the representation. The experimental results indicate that PMvCG obtains promising results and surpasses other comparative methods under different experimental settings.},
  archive      = {J_TNNLS},
  author       = {Wenbo Dong and Shiliang Sun},
  doi          = {10.1109/TNNLS.2023.3300977},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17239-17253},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Partial multiview representation learning with cross-view generation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). PSAQ-ViT v2: Toward accurate and general data-free
quantization for vision transformers. <em>TNNLS</em>, <em>35</em>(12),
17227–17238. (<a
href="https://doi.org/10.1109/TNNLS.2023.3301007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-free quantization can potentially address data privacy and security concerns in model compression and thus has been widely investigated. Recently, patch similarity aware data-free quantization for vision transformers (PSAQ-ViT) designs a relative value metric, patch similarity, to generate data from pretrained vision transformers (ViTs), achieving the first attempt at data-free quantization for ViTs. In this article, we propose PSAQ-ViT V2, a more accurate and general data-free quantization framework for ViTs, built on top of PSAQ-ViT. More specifically, following the patch similarity metric in PSAQ-ViT, we introduce an adaptive teacher–student strategy, which facilitates the constant cyclic evolution of the generated samples and the quantized model (student) in a competitive and interactive fashion under the supervision of the full-precision (FP) model (teacher), thus significantly improving the accuracy of the quantized model. Moreover, without the auxiliary category guidance, we employ the task- and model-independent prior information, making the general-purpose scheme compatible with a broad range of vision tasks and models. Extensive experiments are conducted on various models on image classification, object detection, and semantic segmentation tasks, and PSAQ-ViT V2, with the naive quantization strategy and without access to real-world data, consistently achieves competitive results, showing potential as a powerful baseline on data-free quantization for ViTs. For instance, with Swin-S as the (backbone) model, 8-bit quantization reaches 82.13 top-1 accuracy on ImageNet, 50.9 box AP and 44.1 mask AP on COCO, and 47.2 mean Intersection over Union (mIoU) on ADE20K. We hope that accurate and general PSAQ-ViT V2 can serve as a potential and practice solution in real-world applications involving sensitive data. Code is released and merged at: https://github.com/zkkli/PSAQ-ViT .},
  archive      = {J_TNNLS},
  author       = {Zhikai Li and Mengjuan Chen and Junrui Xiao and Qingyi Gu},
  doi          = {10.1109/TNNLS.2023.3301007},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17227-17238},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PSAQ-ViT v2: Toward accurate and general data-free quantization for vision transformers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Informative trajectory planning using reinforcement learning
for minimum-time exploration of spatiotemporal fields. <em>TNNLS</em>,
<em>35</em>(12), 17216–17226. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the informative trajectory planning problem of an autonomous vehicle for field exploration. In contrast to existing works concerned with maximizing the amount of information about spatial fields, this work considers efficient exploration of spatiotemporal fields with unknown distributions and seeks minimum-time trajectories of the vehicle while respecting a cumulative information constraint. In this work, upon adopting the observability constant as an information measure for expressing the cumulative information constraint, the existence of a minimum-time trajectory is proven under mild conditions. Given the spatiotemporal nature, the problem is modeled as a Markov decision process (MDP), for which a reinforcement learning (RL) algorithm is proposed to learn a continuous planning policy. To accelerate the policy learning, we design a new reward function by leveraging field approximations, which is demonstrated to yield dense rewards. Simulations show that the learned policy can steer the vehicle to achieve an efficient exploration, and it outperforms the commonly-used coverage planning method in terms of exploration time for sufficient cumulative information.},
  archive      = {J_TNNLS},
  author       = {Zhuo Li and Keyou You and Jian Sun and Gang Wang},
  doi          = {10.1109/TNNLS.2023.3300926},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17216-17226},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Informative trajectory planning using reinforcement learning for minimum-time exploration of spatiotemporal fields},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast granular-ball-based density peaks clustering
algorithm for large-scale data. <em>TNNLS</em>, <em>35</em>(12),
17202–17215. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering algorithm (DP) has difficulty in clustering large-scale data, because it requires the distance matrix to compute the density and $\delta $ -distance for each object, which has $O(n^{2})$ time complexity. Granular ball (GB) is a coarse-grained representation of data. It is based on the fact that an object and its local neighbors have similar distribution and they have high possibility of belonging to the same class. It has been introduced into supervised learning by Xia et al. to improve the efficiency of supervised learning, such as support vector machine, $k$ -nearest neighbor classification, rough set, etc. Inspired by the idea of GB, we introduce it into unsupervised learning for the first time and propose a GB-based DP algorithm, called GB-DP. First, it generates GBs from the original data with an unsupervised partitioning method. Then, it defines the density of GBs, instead of the density of objects, according to the centers, radius, and distances between its members and centers, without setting any parameters. After that, it computes the distance between the centers of GBs as the distance between GBs and defines the $\delta $ -distance of GBs. Finally, it uses GBs’ density and $\delta $ -distance to plot the decision graph, employs DP algorithm to cluster them, and expands the clustering result to the original data. Since there is no need to calculate the distance between any two objects and the number of GBs is far less than the scale of a data, it greatly reduces the running time of DP algorithm. By comparing with $k$ -means, ball $k$ -means, DP, DPC-KNN-PCA, FastDPeak, and DLORE-DP, GB-DP can get similar or even better clustering results in much less running time without setting any parameters. The source code is available at https://github.com/DongdongCheng/GB-DP .},
  archive      = {J_TNNLS},
  author       = {Dongdong Cheng and Ya Li and Shuyin Xia and Guoyin Wang and Jinlong Huang and Sulan Zhang},
  doi          = {10.1109/TNNLS.2023.3300916},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17202-17215},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A fast granular-ball-based density peaks clustering algorithm for large-scale data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multistage information complementary fusion network based
on flexible-mixup for HSI-x image classification. <em>TNNLS</em>,
<em>35</em>(12), 17189–17201. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixup-based data augmentation has been proven to be beneficial to the regularization of models during training, especially in the remote-sensing field where the training data is scarce. However, in the process of data augmentation, the Mixup-based methods ignore the target proportion in different inputs and keep the linear insertion ratio consistent, which leads to the response of label space even if no effective objects are introduced in the mixed image due to the randomness of the augmentation process. Moreover, although some previous works have attempted to utilize different multimodal interaction strategies, they could not be well extended to various remote-sensing data combinations. To this end, a multistage information complementary fusion network based on flexible-mixup (Flex-MCFNet) is proposed for hyperspectral-X image classification. First, to bridge the gap between the mixed image and the label, a flexible-mixup (FlexMix) data augmentation strategy is designed, where the weight of the label increases with the ratio of the input image to prevent the negative impact on the label space because of the introduction of invalid information. More importantly, to summarize diverse remote-sensing data inputs including various modal supplements and uncertainties, a multistage information complementary fusion network (MCFNet) is developed. After extracting the features of hyperspectral and complementary modalities [X-modal, including multispectral, synthetic aperture radar (SAR), and light detection and ranging (LiDAR)] separately, the information between complementary modalities is fully interacted and enhanced through multiple stages of information complement and fusion, which is used for the final image classification. Extensive experimental results have demonstrated that Flex-MCFNet can not only effectively expand the training data, but also adequately regularize different data combinations to achieve state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Junjie Wang and Mengmeng Zhang and Wei Li and Ran Tao},
  doi          = {10.1109/TNNLS.2023.3300903},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17189-17201},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A multistage information complementary fusion network based on flexible-mixup for HSI-X image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low latency and sparse computing spiking neural networks
with self-driven adaptive threshold plasticity. <em>TNNLS</em>,
<em>35</em>(12), 17177–17188. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have captivated the attention worldwide owing to their compelling advantages in low power consumption, high biological plausibility, and strong robustness. However, the intrinsic latency associated with SNNs during inference poses a significant challenge, impeding their further development and application. This latency is caused by the need for spiking neurons to collect electrical stimuli and generate spikes only when their membrane potential exceeds a firing threshold. Considering the firing threshold plays a crucial role in SNN performance, this article proposes a self-driven adaptive threshold plasticity (SATP) mechanism, wherein neurons autonomously adjust the firing thresholds based on their individual state information using unsupervised learning rules, of which the adjustment is triggered by their own firing events. SATP is based on the principle of maximizing the information contained in the output spike rate distribution of each neuron. This article derives the mathematical expression of SATP and provides extensive experimental results, demonstrating that SATP effectively reduces SNN inference latency, further reduces the computation density while improving computational accuracy, so that SATP facilitates SNN models to be with low latency, sparse computing, and high accuracy.},
  archive      = {J_TNNLS},
  author       = {Anguo Zhang and Jieming Shi and Junyi Wu and Yongcheng Zhou and Wei Yu},
  doi          = {10.1109/TNNLS.2023.3300514},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17177-17188},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low latency and sparse computing spiking neural networks with self-driven adaptive threshold plasticity},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantization via distillation and contrastive learning.
<em>TNNLS</em>, <em>35</em>(12), 17164–17176. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization is a critical technique employed across various research fields for compressing deep neural networks (DNNs) to facilitate deployment within resource-limited environments. This process necessitates a delicate balance between model size and performance. In this work, we explore knowledge distillation (KD) as a promising approach for improving quantization performance by transferring knowledge from high-precision networks to low-precision counterparts. We specifically investigate feature-level information loss during distillation and emphasize the importance of feature-level network quantization perception. We propose a novel quantization method that combines feature-level distillation and contrastive learning to extract and preserve more valuable information during the quantization process. Furthermore, we utilize the hyperbolic tangent function to estimate gradients with respect to the rounding function, which smoothens the training procedure. Our extensive experimental results demonstrate that the proposed approach achieves competitive model performance with the quantized network compared to its full-precision counterpart, thus validating its efficacy and potential for real-world applications.},
  archive      = {J_TNNLS},
  author       = {Zehua Pei and Xufeng Yao and Wenqian Zhao and Bei Yu},
  doi          = {10.1109/TNNLS.2023.3300309},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17164-17176},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quantization via distillation and contrastive learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Protocol-based synchronization of stochastic jumping
inertial neural networks under image encryption application.
<em>TNNLS</em>, <em>35</em>(12), 17151–17163. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates the protocol-based synchronization of inertial neural networks (INNs) with stochastic semi-Markovian jumping parameters and image encryption application. The semi-Markovian jumping process is adopted to characterize INNs under sudden complex changes. To conserve the limited available network bandwidth, an adaptive event-driven protocol (AEDP) is developed in the corresponding semi-Markovian jumping INNs (S-MJINNs), which not only reduces the amount of data transmission but also avoids the Zeno phenomenon. The objective is to construct an adaptive event-driven controller so that the drive and response systems maintain synchronous relationships. Based on the appropriate Lyapunov functional, integral inequality, and free weighting matrix, novel criteria are derived to realize the synchronization. Moreover, the desired adaptive event-driven controller is designed under a semi-Markovian jumping process. The proposed method is demonstrated through a numerical example and an image encryption process.},
  archive      = {J_TNNLS},
  author       = {Wenhai Qi and Yongbo Yang and Ju H. Park and Huaicheng Yan and Zheng-Guang Wu},
  doi          = {10.1109/TNNLS.2023.3300270},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17151-17163},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Protocol-based synchronization of stochastic jumping inertial neural networks under image encryption application},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HSGAN: Hyperspectral reconstruction from RGB images with
generative adversarial network. <em>TNNLS</em>, <em>35</em>(12),
17137–17150. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral (HS) reconstruction from RGB images denotes the recovery of whole-scene HS information, which has attracted much attention recently. State-of-the-art approaches often adopt convolutional neural networks to learn the mapping for HS reconstruction from RGB images. However, they often do not achieve high HS reconstruction performance across different scenes consistently. In addition, their performance in recovering HS images from clean and real-world noisy RGB images is not consistent. To improve the HS reconstruction accuracy and robustness across different scenes and from different input images, we present an effective HSGAN framework with a two-stage adversarial training strategy. The generator is a four-level top-down architecture that extracts and combines features on multiple scales. To generalize well to real-world noisy images, we further propose a spatial–spectral attention block (SSAB) to learn both spatial-wise and channel-wise relations. We conduct the HS reconstruction experiments from both clean and real-world noisy RGB images on five well-known HS datasets. The results demonstrate that HSGAN achieves superior performance to existing methods. Please visit https://github.com/zhaoyuzhi/HSGAN to try our codes.},
  archive      = {J_TNNLS},
  author       = {Yuzhi Zhao and Lai-Man Po and Tingyu Lin and Qiong Yan and Wei Liu and Pengfei Xian},
  doi          = {10.1109/TNNLS.2023.3300099},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17137-17150},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HSGAN: Hyperspectral reconstruction from RGB images with generative adversarial network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CIPL: Counterfactual interactive policy learning to
eliminate popularity bias for online recommendation. <em>TNNLS</em>,
<em>35</em>(12), 17123–17136. (<a
href="https://doi.org/10.1109/TNNLS.2023.3299929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popularity bias, as a long-standing problem in recommender systems (RSs), has been fully considered and explored for offline recommendation systems in most existing relevant researches, but very few studies have paid attention to eliminate such bias in online interactive recommendation scenarios. Bias amplification will become increasingly serious over time due to the existence of feedback loop between the user and the interactive system. However, existing methods have only investigated the causal relations among different factors statically without considering temporal dependencies inherent in the online interactive recommendation system, making them difficult to be adapted to online settings. To address these problems, we propose a novel counterfactual interactive policy learning (CIPL) method to eliminate popularity bias for online recommendation. It first scrutinizes the causal relations in the interactive recommender models and formulates a novel temporal causal graph (TCG) to guide the training and counterfactual inference of the causal interactive recommendation system. Concretely, TCG is used to estimate the causal relations of item popularity on prediction score when the user interacts with the system at each time during model training. Besides, it is also used to remove the negative effect of popularity bias in the test stage. To train the causal interactive recommendation system, we formulated our CIPL by the actor–critic framework with an online interactive environment simulator. We conduct extensive experiments on three public benchmarks and the experimental results demonstrate that our proposed method can achieve the new state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Yongsen Zheng and Jinghui Qin and Pengxu Wei and Ziliang Chen and Liang Lin},
  doi          = {10.1109/TNNLS.2023.3299929},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17123-17136},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CIPL: Counterfactual interactive policy learning to eliminate popularity bias for online recommendation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SIIR: Symmetrical information interaction modeling for news
recommendation. <em>TNNLS</em>, <em>35</em>(12), 17111–17122. (<a
href="https://doi.org/10.1109/TNNLS.2023.3299790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate matching between user and candidate news plays a fundamental role in news recommendation. Most existing studies capture fine-grained user interests through effective user modeling. Nevertheless, user interest representations are often extracted from multiple history news items, while candidate news representations are learned from specific news items. The asymmetry of information density causes invalid matching of user interests and candidate news, which severely affects the click-through rate prediction for specific candidate news. To resolve the problems mentioned above, we propose a symmetrical information interaction modeling for news recommendation (SIIR) in this article. We first design a light interactive attention network for user (LIAU) modeling to extract user interests related to the candidate news and reduce interference of noise effectively. LIAU overcomes the shortcomings of complex structure and high training costs of conventional interaction-based models and makes full use of domain-specific interest tendencies of users. We then propose a novel heterogeneous graph neural network (HGNN) to enhance candidate news representation through the potential relations among news. HGNN builds a candidate news enhancement scheme without user interaction to further facilitate accurate matching with user interests, which mitigates the cold-start problem effectively. Experiments on two realistic news datasets, i.e., MIND and Adressa, demonstrate that SIIR outperforms the state-of-the-art (SOTA) single-model methods by a large margin.},
  archive      = {J_TNNLS},
  author       = {Zhonghong Ou and Zongzhi Han and Peihang Liu and Shengyu Teng and Meina Song},
  doi          = {10.1109/TNNLS.2023.3299790},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17111-17122},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SIIR: Symmetrical information interaction modeling for news recommendation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New scalable and efficient online pairwise learning
algorithm. <em>TNNLS</em>, <em>35</em>(12), 17099–17110. (<a
href="https://doi.org/10.1109/TNNLS.2023.3299756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise learning is an important machine-learning topic with many practical applications. An online algorithm is the first choice for processing streaming data and is preferred for handling large-scale pairwise learning problems. However, existing online pairwise learning algorithms are not scalable and efficient enough for large-scale high-dimensional data, because they were designed based on singly stochastic gradients. To address this challenging problem, in this article, we propose a dynamic doubly stochastic gradient algorithm (D2SG) for online pairwise learning. Especially, only the time and space complexities of $\mathcal {O} (d)$ are needed for incorporating a new sample, where $d$ is the dimensionality of data. This means that our D2SG is much faster and more scalable than the existing online pairwise learning algorithms while the statistical accuracy can be guaranteed through our rigorous theoretical analysis under standard assumptions. The experimental results on a variety of real-world datasets not only confirm the theoretical result of our new D2SG algorithm, but also show that D2SG has better efficiency and scalability than the existing online pairwise learning algorithms.},
  archive      = {J_TNNLS},
  author       = {Bin Gu and Runxue Bao and Chenkang Zhang and Heng Huang},
  doi          = {10.1109/TNNLS.2023.3299756},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17099-17110},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {New scalable and efficient online pairwise learning algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Attention-driven memory network for online visual tracking.
<em>TNNLS</em>, <em>35</em>(12), 17085–17098. (<a
href="https://doi.org/10.1109/TNNLS.2023.3299412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A memory mechanism has attracted growing popularity in tracking tasks due to the ability of learning long-term-dependent information. However, it is very challenging for existing memory modules to provide the intrinsic attribute information of the target to the tracker in complex scenes. In this article, by considering the biological visual memory mechanisms, we propose the novel online tracking method via an attention-driven memory network, which can mine discriminative memory information and enhance the robustness and reliability of the tracker. First, to reinforce effectiveness of memory content, we design a novel attention-driven memory network. In the network, the long memory module gains property-level memory information by focusing on the state of the target at both the channel and spatial levels. Meanwhile, in reciprocity, we add a short-term memory module to maintain good adaptability when confronting drastic deformation of the target. The attention-driven memory network can adaptively adjust the contribution of short-term and long-term memories to tracking results under the weighted gradient harmonized loss. On this basis, to avoid model performance degradation, an online memory updater (MU) is further proposed. It is designed to mining for target information in tracking results through the Mixer layer and the online head network together. By evaluating the confidence of the tracking results, the memory updater can accurately judge the time of updating the model, which guarantees the effectiveness of online memory updates. Finally, the proposed method performs favorably and has been extensively validated on several benchmark datasets, including object tracking benchmark-50/100 (OTB-50/100), temple color-128 (TC-128), unmanned aerial vehicles-123 (UAV-123), generic object tracking -10k (GOT-10k), visual object tracking-2016 (VOT-2016), and VOT-2018 against several advanced methods.},
  archive      = {J_TNNLS},
  author       = {Huanlong Zhang and Jiamei Liang and Jiapeng Zhang and Tianzhu Zhang and Yingzi Lin and Yanfeng Wang},
  doi          = {10.1109/TNNLS.2023.3299412},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17085-17098},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention-driven memory network for online visual tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Generating semantic adversarial examples via feature
manipulation in latent space. <em>TNNLS</em>, <em>35</em>(12),
17070–17084. (<a
href="https://doi.org/10.1109/TNNLS.2023.3299408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The susceptibility of deep neural networks (DNNs) to adversarial intrusions, exemplified by adversarial examples, is well-documented. Conventional attacks implement unstructured, pixel-wise perturbations to mislead classifiers, which often results in a noticeable departure from natural samples and lacks human-perceptible interpretability. In this work, we present an adversarial attack strategy that implements fine-granularity, semantic-meaning-oriented structural perturbations. Our proposed methodology manipulates the semantic attributes of images through the use of disentangled latent codes. We engineer adversarial perturbations by manipulating either a single latent code or a combination thereof. To this end, we propose two unsupervised semantic manipulation strategies: one based on vector-disentangled representation and the other on feature map-disentangled representation, taking into consideration the complexity of the latent codes and the smoothness of the reconstructed images. Our empirical evaluations, conducted extensively on real-world image data, showcase the potency of our attacks, particularly against black-box classifiers. Furthermore, we establish the existence of a universal semantic adversarial example that is agnostic to specific images.},
  archive      = {J_TNNLS},
  author       = {Shuo Wang and Shangyu Chen and Tianle Chen and Surya Nepal and Carsten Rudolph and Marthie Grobler},
  doi          = {10.1109/TNNLS.2023.3299408},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17070-17084},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generating semantic adversarial examples via feature manipulation in latent space},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-adaptive graph with nonlocal attention network for
skeleton-based action recognition. <em>TNNLS</em>, <em>35</em>(12),
17057–17069. (<a
href="https://doi.org/10.1109/TNNLS.2023.3298950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have achieved encouraging progress in modeling human body skeletons as spatial–temporal graphs. However, existing methods still suffer from two inherent drawbacks. Firstly, these models process the input data based on the physical structure of the human body, which leads to some latent correlations among joints being ignored. Furthermore, the key temporal relationships between nonadjacent frames are overlooked, preventing to fully learn the changes of the body joints along the temporal dimension. To address these issues, we propose an innovative spatial–temporal model by introducing a self-adaptive GCN (SAGCN) with global attention network, collectively termed SAGGAN. Specifically, the SAGCN module is proposed to construct two additional dynamic topological graphs to learn the common characteristics of all data and represent a unique pattern for each sample, respectively. Meanwhile, the global attention module (spatial attention (SA) and temporal attention (TA) modules) is designed to extract the global connections between different joints in a single frame and model temporal relationships between adjacent and nonadjacent frames in temporal sequences. In this manner, our network can capture richer features of actions for accurate action recognition and overcome the defect of the standard graph convolution. Extensive experiments on three benchmark datasets (NTU-60, NTU-120, and Kinetics) have demonstrated the superiority of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Chen Pang and Xingyu Gao and Zhenyu Chen and Lei Lyu},
  doi          = {10.1109/TNNLS.2023.3298950},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17057-17069},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-adaptive graph with nonlocal attention network for skeleton-based action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physical adversarial attacks for surveillance: A survey.
<em>TNNLS</em>, <em>35</em>(12), 17036–17056. (<a
href="https://doi.org/10.1109/TNNLS.2023.3321432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern automated surveillance techniques are heavily reliant on deep learning methods. Despite the superior performance, these learning systems are inherently vulnerable to adversarial attacks—maliciously crafted inputs that are designed to mislead, or trick, models into making incorrect predictions. An adversary can physically change their appearance by wearing adversarial t-shirts, glasses, or hats or by specific behavior, to potentially avoid various forms of detection, tracking, and recognition of surveillance systems; and obtain unauthorized access to secure properties and assets. This poses a severe threat to the security and safety of modern surveillance systems. This article reviews recent attempts and findings in learning and designing physical adversarial attacks for surveillance applications. In particular, we propose a framework to analyze physical adversarial attacks and provide a comprehensive survey of physical adversarial attacks on four key surveillance tasks: detection, identification, tracking, and action recognition under this framework. Furthermore, we review and analyze strategies to defend against physical adversarial attacks and the methods for evaluating the strengths of the defense. The insights in this article present an important step in building resilience within surveillance systems to physical adversarial attacks.},
  archive      = {J_TNNLS},
  author       = {Kien Nguyen and Tharindu Fernando and Clinton Fookes and Sridha Sridharan},
  doi          = {10.1109/TNNLS.2023.3321432},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17036-17056},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Physical adversarial attacks for surveillance: A survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking the robustness of instance segmentation models.
<em>TNNLS</em>, <em>35</em>(12), 17021–17035. (<a
href="https://doi.org/10.1109/TNNLS.2023.3310985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive evaluation of instance segmentation models with respect to real-world image corruptions as well as out-of-domain image collections, e.g., images captured by a different set-up than the training dataset. The out-of-domain image evaluation shows the generalization capability of models, an essential aspect of real-world applications, and an extensively studied topic of domain adaptation. These presented robustness and generalization evaluations are important when designing instance segmentation models for real-world applications and picking an off-the-shelf pretrained model to directly use for the task at hand. Specifically, this benchmark study includes state-of-the-art network architectures, network backbones, normalization layers, models trained starting from scratch versus pretrained networks, and the effect of multitask training on robustness and generalization. Through this study, we gain several insights. For example, we find that group normalization (GN) enhances the robustness of networks across corruptions where the image contents stay the same but corruptions are added on top. On the other hand, batch normalization (BN) improves the generalization of the models across different datasets where statistics of image features change. We also find that single-stage detectors do not generalize well to larger image resolutions than their training size. On the other hand, multistage detectors can easily be used on images of different sizes. We hope that our comprehensive study will motivate the development of more robust and reliable instance segmentation models.},
  archive      = {J_TNNLS},
  author       = {Yusuf Dalva and Hamza Pehlivan and Said Fahri Altındiş and Aysegul Dundar},
  doi          = {10.1109/TNNLS.2023.3310985},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17021-17035},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Benchmarking the robustness of instance segmentation models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Deep learning for visual localization and mapping: A
survey. <em>TNNLS</em>, <em>35</em>(12), 17000–17020. (<a
href="https://doi.org/10.1109/TNNLS.2023.3309809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning-based localization and mapping approaches have recently emerged as a new research direction and receive significant attention from both industry and academia. Instead of creating hand-designed algorithms based on physical models or geometric theories, deep learning solutions provide an alternative to solve the problem in a data-driven way. Benefiting from the ever-increasing volumes of data and computational power on devices, these learning methods are fast evolving into a new area that shows potential to track self-motion and estimate environmental models accurately and robustly for mobile agents. In this work, we provide a comprehensive survey and propose a taxonomy for the localization and mapping methods using deep learning. This survey aims to discuss two basic questions: whether deep learning is promising for localization and mapping, and how deep learning should be applied to solve this problem. To this end, a series of localization and mapping topics are investigated, from the learning-based visual odometry and global relocalization to mapping, and simultaneous localization and mapping (SLAM). It is our hope that this survey organically weaves together the recent works in this vein from robotics, computer vision, and machine learning communities and serves as a guideline for future researchers to apply deep learning to tackle the problem of visual localization and mapping.},
  archive      = {J_TNNLS},
  author       = {Changhao Chen and Bing Wang and Chris Xiaoxuan Lu and Niki Trigoni and Andrew Markham},
  doi          = {10.1109/TNNLS.2023.3309809},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {17000-17020},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning for visual localization and mapping: A survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on scenario theory, complexity, and
compression-based learning and generalization. <em>TNNLS</em>,
<em>35</em>(12), 16985–16999. (<a
href="https://doi.org/10.1109/TNNLS.2023.3308828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates formal generalization error bounds that apply to support vector machines (SVMs) in realizable and agnostic learning problems. We focus on recently observed parallels between probably approximately correct (PAC)-learning bounds, such as compression and complexity-based bounds, and novel error guarantees derived within scenario theory. Scenario theory provides nonasymptotic and distributional-free error bounds for models trained by solving data-driven decision-making problems. Relevant theorems and assumptions are reviewed and discussed. We propose a numerical comparison of the tightness and effectiveness of theoretical error bounds for support vector classifiers trained on several randomized experiments from 13 real-life problems. This analysis allows for a fair comparison of different approaches from both conceptual and experimental standpoints. Based on the numerical results, we argue that the error guarantees derived from scenario theory are often tighter for realizable problems and always yield informative results, i.e., probability bounds tighter than a vacuous [0, 1] interval. This work promotes scenario theory as an alternative tool for model selection, structural-risk minimization, and generalization error analysis of SVMs. In this way, we hope to bring the communities of scenario and statistical learning theory closer, so that they can benefit from each other’s insights.},
  archive      = {J_TNNLS},
  author       = {Roberto Rocchetta and Alexander Mey and Frans A. Oliehoek},
  doi          = {10.1109/TNNLS.2023.3308828},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {12},
  number       = {12},
  pages        = {16985-16999},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on scenario theory, complexity, and compression-based learning and generalization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GDCNet: Graph enrichment learning via graph dropping
convolutional networks. <em>TNNLS</em>, <em>35</em>(11), 16975–16980.
(<a href="https://doi.org/10.1109/TNNLS.2023.3296760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have been widely studied to address graph data representation and learning. In contrast to traditional convolutional neural networks (CNNs) that employ many various (spatial) convolution filters to obtain rich feature descriptors to encode complex patterns of image data, GCNs, however, are defined on the input observed graph $G(\mathbf {X},\mathbf {A})$ and usually adopt the single fixed spatial convolution filter for graph data feature extraction. This limits the capacity of the existing GCNs to encode the complex patterns of graph data. To overcome this issue, inspired by depthwise separable convolution and DropEdge operation, we first propose to generate various graph convolution filters by randomly dropping out some edges from the input graph $\mathbf {A}$ . Then, we propose a novel graph-dropping convolution layer (GDCLayer) to produce rich feature descriptors for graph data. Using GDCLayer, we finally design a new end-to-end network architecture, that is, a graph-dropping convolutional network (GDCNet), for graph data learning. Experiments on several datasets demonstrate the effectiveness of the proposed GDCNet.},
  archive      = {J_TNNLS},
  author       = {Bo Jiang and Yong Chen and Beibei Wang and Haiyun Xu and Jin Tang},
  doi          = {10.1109/TNNLS.2023.3296760},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16975-16980},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GDCNet: Graph enrichment learning via graph dropping convolutional networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AL-SAR: Active learning for skeleton-based action
recognition. <em>TNNLS</em>, <em>35</em>(11), 16966–16974. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition from temporal multivariate sequences of features, such as identifying human actions, is typically approached by supervised training as it requires many ground truth annotations to reach high recognition accuracy. Unsupervised methods for the organization of sequences into clusters have been introduced, however, such methods continue to require annotations to associate clusters with actions. The challenges in annotation necessitate an effective classification methodology that minimizes the required number of labels. Active learning (AL) approaches have been proposed to address these challenges and were able to establish robust results on image classification. Such approaches are not directly applicable to sequences, since for sequences, the variations are in both spatial and temporal domains. In this brief, we introduce a novel method for AL for sequences, called “AL-SAR,” which combines unsupervised training with sparsely supervised annotation. In particular, AL-SAR employs a multi-head mechanism for robust uncertainty evaluation of the latent space learned by an encoder-decoder framework. It aims to iteratively select a sparse set of samples, which annotation contributes the most to the disentanglement of the latent space. We evaluate our system on common benchmark datasets with multiple sequences and actions, such as NW-UCLA, NTU RGB+D 60, and UWA3D. Our results indicate that AL-SAR coupled with encoder-decoder network outperforms other AL methods coupled with the same network structure.},
  archive      = {J_TNNLS},
  author       = {Jingyuan Li and Trung Le and Eli Shlizerman},
  doi          = {10.1109/TNNLS.2023.3297853},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16966-16974},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AL-SAR: Active learning for skeleton-based action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global stability of fractional nonlinear differential
systems with state-dependent delayed impulses. <em>TNNLS</em>,
<em>35</em>(11), 16960–16965. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fractional-order (FO) nonlinear differential system with state-dependent (SD) delayed impulses (DI) is considered in this brief. The considered impulses are related to the delayed state of the system and the delays are SD. A novel lemma for the monotonicity of the solution of Caputo’s FO derivative equation is given. By means of linear matrix inequality (LMI) and several comparative arguments, criteria of uniform stability, uniform asymptotical stability, and Mittag–Leffler stability are obtained. Compared with other works on integer-order (IO) impulsive delayed systems with SD delays or fixed delays, how to impose constraints on parameters and impulses is explored, without imposing the boundedness on the state delays. Two examples are implemented to examine the practicality and sharpness of our theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Yonggui Kao and Hui Li and Yunlong Liu and Hongwei Xia and Changhong Wang},
  doi          = {10.1109/TNNLS.2023.3291210},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16960-16965},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global stability of fractional nonlinear differential systems with state-dependent delayed impulses},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biologically plausible sparse temporal word representations.
<em>TNNLS</em>, <em>35</em>(11), 16952–16959. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word representations, usually derived from a large corpus and endowed with rich semantic information, have been widely applied to natural language tasks. Traditional deep language models, on the basis of dense word representations, requires large memory space and computing resource. The brain-inspired neuromorphic computing systems, with the advantages of better biological interpretability and less energy consumption, still have major difficulties in the representation of words in terms of neuronal activities, which has restricted their further application in more complicated downstream language tasks. Comprehensively exploring the diverse neuronal dynamics of both integration and resonance, we probe into three spiking neuron models to post-process the original dense word embeddings, and test the generated sparse temporal codes on several tasks concerning both word-level and sentence-level semantics. The experimental results show that our sparse binary word representations could perform on par with or even better than original word embeddings in capturing semantic information, while requiring less storage. Our methods provide a robust representation foundation of language in terms of neuronal activities, which could potentially be applied to future downstream natural language tasks under neuromorphic computing systems.},
  archive      = {J_TNNLS},
  author       = {Yuguo Liu and Wenyu Chen and Hanwen Liu and Yun Zhang and Malu Zhang and Hong Qu},
  doi          = {10.1109/TNNLS.2023.3290004},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16952-16959},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Biologically plausible sparse temporal word representations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved stability criteria for delayed neural networks via
time-varying free-weighting matrices and s-procedure. <em>TNNLS</em>,
<em>35</em>(11), 16945–16951. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief investigates the stability of neural networks with time-varying delays. Novel stability conditions are derived by employing free-matrix-based inequality and introducing the variable-augmented-based free-weighting matrices in the estimation of the derivative of the Lyapunov–Krasovskii functionals (LKFs). Both techniques avoid the appearance of the nonlinear terms of the time-varying delay. Especially, the time-varying free-weighting matrices associated with the derivative of the delay and the time-varying S-Procedure related to the delay and its derivative are combined to improve the presented criteria. Finally, numerical examples are given to illustrate the benefits of the presented methods.},
  archive      = {J_TNNLS},
  author       = {Xi-Zi Zhou and Jianqi An and Yong He and Jianhua Shen},
  doi          = {10.1109/TNNLS.2023.3289208},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16945-16951},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improved stability criteria for delayed neural networks via time-varying free-weighting matrices and S-procedure},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A compact constraint incremental method for random weight
networks and its application. <em>TNNLS</em>, <em>35</em>(11),
16936–16944. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental random weight networks (IRWNs) face the issues of weak generalization and complicated network structure. There is an important reason: the learning parameters of IRWNs are determined in a random fashion without guidance, which may increase many redundant hidden nodes, and thereby leading to inferior performance. To resolve this issue, a novel IRWN with compact constraint that guides the assignment of random learning parameters (CCIRWN) is developed in this brief. Using the iteration method of Greville, a compact constraint that simultaneously assures the quality of generated hidden nodes and the convergence of the CCIRWN is built to perform learning parameter configuration. Meanwhile, the output weights of the CCIRWN are assessed analytically. Two types of learning methods for constructing the CCIRWN are proposed. Finally, the performance evaluation of the proposed CCIRWN is undertaken on the 1-D nonlinear function approximation, several real-world datasets, and data-driven estimation based on the industrial data. Numerical and industrial examples indicate that the proposed CCIRWN with compact structure can achieve favorable generalization ability.},
  archive      = {J_TNNLS},
  author       = {Qianjin Wang and Wei Dai and Chunfu Zhang and Jiaji Zhu and Xiaoping Ma},
  doi          = {10.1109/TNNLS.2023.3289798},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16936-16944},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A compact constraint incremental method for random weight networks and its application},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving low-latency predictions in multi-exit neural
networks via block-dependent losses. <em>TNNLS</em>, <em>35</em>(11),
16927–16935. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the size of a model increases, making predictions using deep neural networks (DNNs) is becoming more computationally expensive. Multi-exit neural network is one promising solution that can flexibly make anytime predictions via early exits, depending on the current test-time budget which may vary over time in practice (e.g., self-driving cars with dynamically changing speeds). However, the prediction performance at the earlier exits is generally much lower than the final exit, which becomes a critical issue in low-latency applications having a tight test-time budget. Compared to the previous works where each block is optimized to minimize the losses of all exits simultaneously, in this work, we propose a new method for training multi-exit neural networks by strategically imposing different objectives on individual blocks. The proposed idea based on grouping and overlapping strategies improves the prediction performance at the earlier exits while not degrading the performance of later ones, making our scheme to be more suitable for low-latency applications. Extensive experimental results on both image classification and semantic segmentation confirm the advantage of our approach. The proposed idea does not require any modifications in the model architecture and can be easily combined with existing strategies aiming to improve the performance of multi-exit neural networks.},
  archive      = {J_TNNLS},
  author       = {Dong-Jun Han and Jungwuk Park and Seokil Ham and Namjin Lee and Jaekyun Moon},
  doi          = {10.1109/TNNLS.2023.3282249},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16927-16935},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving low-latency predictions in multi-exit neural networks via block-dependent losses},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Address the unseen relationships: Attribute correlations in
text attribute person search. <em>TNNLS</em>, <em>35</em>(11),
16916–16926. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text attribute person search aims to identify the particular pedestrian by textual attribute information. Compared to person re- identification tasks which requires imagery samples as its query, text attribute person search is more useful under the circumstance where only witness is available. Most existing text attribute person search methods focus on improving the matching correlation and alignments by learning better representations of person–attribute instance pairs, with few consideration of the latent correlations between attributes. In this work, we propose a graph convolutional network (GCN) and pseudo-label-based text attribute person search method. Concretely, the model directly constructs the attribute correlations by label co- occurrence probability, in which the nodes are represented by attribute embedding and edges are by the filtered correlation matrix of attribute labels. In order to obtain better representations, we combine the cross-attention module (CAM) and the GCN. Furthermore, to address the unseen attribute relationships, we update the edge information through the instances through testing set with high predicted probability thus to better adapt the attribute distribution. Extensive experiments illustrate that our model outperforms the existing state-of-the-art methods on publicly available person search benchmarks: Market-1501 and PETA.},
  archive      = {J_TNNLS},
  author       = {Xi Yang and Xiaoqi Wang and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2023.3300582},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16916-16926},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Address the unseen relationships: Attribute correlations in text attribute person search},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning heterogeneous relation graph and value
regularization policy for visual navigation. <em>TNNLS</em>,
<em>35</em>(11), 16901–16915. (<a
href="https://doi.org/10.1109/TNNLS.2023.3300888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of visual navigation is steering an agent to find a given target object with current observation. It is crucial to learn an informative visual representation and robust navigation policy in this task. Aiming to promote these two parts, we propose three complementary techniques, heterogeneous relation graph (HRG), a value regularized navigation policy (VRP), and gradient-based meta learning (ML). HRG integrates object relationships, including object semantic closeness and spatial directions, e.g., a knife is usually co-occurrence with bowl semantically or located at the left of the fork spatially. It improves visual representation learning. Both VRP and gradient-based ML improve robust navigation policy, regulating this process of the agent to escape from the deadlock states such as being stuck or looping. Specifically, gradient-based ML is a type of supervision method used in policy network training, which eliminates the gap between the seen and unseen environment distributions. In this process, VRP maximizes the transformation of the mutual information between visual observation and navigation policy, thus improving more informed navigation decisions. Our framework shows superior performance over the current state-of-the-art (SOTA) in terms of success rate and success weighted by length (SPL). Our HRG outperforms the Visual Genome knowledge graph on cross-scene generalization with $\approx 56\%$ and $\approx 39\%$ improvement on Hits@ $5^{*}$ (proportion of correct entities ranked in top 5) and MRR $^{*}$ (mean reciprocal rank), respectively. Our code and HRG datasets will be made publicly available in the scientific community.},
  archive      = {J_TNNLS},
  author       = {Kang Zhou and Chi Guo and Wenfei Guo and Huyin Zhang},
  doi          = {10.1109/TNNLS.2023.3300888},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16901-16915},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning heterogeneous relation graph and value regularization policy for visual navigation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long-term tracking of evasive urban target based on
intention inference and deep reinforcement learning. <em>TNNLS</em>,
<em>35</em>(11), 16886–16900. (<a
href="https://doi.org/10.1109/TNNLS.2023.3298944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have been widely used in urban target-tracking tasks, where long-term tracking of evasive targets is of great significance for public safety. However, the tracked targets are easily lost due to the evasive behavior of the targets and the unstructured characteristics of the urban environment. To address this issue, this article proposes a hybrid target-tracking approach based on target intention inference and deep reinforcement learning (DRL). First, a target intention inference model based on convolution neural networks (CNNs) is built to infer target intentions by fusing urban environment information and observed target trajectory. Then, the prediction of the target trajectory can be inspired by the inferred target intentions, which can further provide effective guidance to the target search process. In order to fully explore the policy space, the target search policy is developed under a DRL framework, where the search policy is modeled as a deep neural network (DNN) and trained by interacting with the task environment. The simulation results show that the inference of the target intentions can effectively guide the UAV to search for the target and significantly improve the target-tracking performance. Meanwhile, the generalization results indicate that the proposed DRL-based search policy has high robustness to the uncertainty of the target behavior.},
  archive      = {J_TNNLS},
  author       = {Peng Yan and Jifeng Guo and Xiaojie Su and Chengchao Bai},
  doi          = {10.1109/TNNLS.2023.3298944},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16886-16900},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Long-term tracking of evasive urban target based on intention inference and deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain adaptation networks with parameter-free adaptively
rectified linear units for fault diagnosis under variable operating
conditions. <em>TNNLS</em>, <em>35</em>(11), 16872–16885. (<a
href="https://doi.org/10.1109/TNNLS.2023.3298648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important component of the rotating machinery, rolling bearings usually work under the condition of variable speed and load, and vibration signals in the same health state are significantly different due to the change in operating conditions. To address the problem that the existing deep learning (DL) methods have fixed nonlinear transformations for all input signals in cross-domain fault diagnosis, we propose a new activation function, i.e., parameter-free adaptively rectified linear units (PfAReLU). The proposed activation function performs adaptive nonlinear transformations according to the input data and can better capture the fault features of vibration signals in the same fault state under different operating conditions. Furthermore, the number of PfAReLU parameters is zero, so that the risk of network overfitting is reduced. At the same time, deep parameter-free reconstruction-classification networks with PfAReLU (DPRCN-PfAReLU) are also constructed for cross-domain fault diagnosis. Specifically, DPRCN-PfAReLU consists of a shared encoder, a target domain decoder, and a source domain classifier. The shared encoder adds a parameter-free attention module at the output to enhance the weight of domain-invariant features without increasing network parameters. The shared encoded representation of source domain and target domain is learned by target domain decoder and source domain classifier. Compared with other methods under nine different operating conditions via real experiment studies, the proposed method shows superiority for cross-domain fault diagnosis.},
  archive      = {J_TNNLS},
  author       = {Yongyi Chen and Dan Zhang and Ruqiang Yan},
  doi          = {10.1109/TNNLS.2023.3298648},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16872-16885},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain adaptation networks with parameter-free adaptively rectified linear units for fault diagnosis under variable operating conditions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning visual affordance grounding from demonstration
videos. <em>TNNLS</em>, <em>35</em>(11), 16857–16871. (<a
href="https://doi.org/10.1109/TNNLS.2023.3298638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual affordance grounding aims to segment all possible interaction regions between people and objects from an image/video, which benefits many applications, such as robot grasping and action recognition. Prevailing methods predominantly depend on the appearance feature of the objects to segment each region of the image, which encounters the following two problems: 1) there are multiple possible regions in an object that people interact with and 2) there are multiple possible human interactions in the same object region. To address these problems, we propose a hand-aided affordance grounding network (HAG-Net) that leverages the aided clues provided by the position and action of the hand in demonstration videos to eliminate the multiple possibilities and better locate the interaction regions in the object. Specifically, HAG-Net adopts a dual-branch structure to process the demonstration video and object image data. For the video branch, we introduce hand-aided attention to enhance the region around the hand in each video frame and then use the long short-term memory (LSTM) network to aggregate the action features. For the object branch, we introduce a semantic enhancement module (SEM) to make the network focus on different parts of the object according to the action classes and utilize a distillation loss to align the output features of the object branch with that of the video branch and transfer the knowledge in the video branch to the object branch. Quantitative and qualitative evaluations on two challenging datasets show that our method has achieved state-of-the-art results for affordance grounding. The source code is available at: https://github.com/lhc1224/HAG-Net .},
  archive      = {J_TNNLS},
  author       = {Hongchen Luo and Wei Zhai and Jing Zhang and Yang Cao and Dacheng Tao},
  doi          = {10.1109/TNNLS.2023.3298638},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16857-16871},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning visual affordance grounding from demonstration videos},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistent arbitrary style transfer using consistency
training and self-attention module. <em>TNNLS</em>, <em>35</em>(11),
16845–16856. (<a
href="https://doi.org/10.1109/TNNLS.2023.3298383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary style transfer (AST) has garnered considerable attention for its ability to transfer styles infinitely. Although existing methods have achieved impressive results, they may overlook style consistencies and fail to capture crucial style patterns, leading to inconsistent style transfer (ST) caused by minor disturbances. To tackle this issue, we conduct a mathematical analysis of inconsistent ST and develop a style inconsistency measure (SIM) to quantify the inconsistencies between generated images. Moreover, we propose a consistent AST (CAST) framework that effectively captures and transfers essential style features into content images. The proposed CAST framework incorporates an intersection-of-union-preserving crop (IoUPC) module to obtain style pairs with minor disturbance, a self-attention (SA) module to learn the crucial style features, and a style inconsistency loss regularization (SILR) to facilitate consistent feature learning for consistent stylization. Our proposed framework not only provides an optimal solution for consistent ST but also outperforms existing methods when embedded into the CAST framework. Extensive experiments demonstrate that the proposed CAST framework can effectively transfer style patterns while preserving consistency and achieve the state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Zheng Zhou and Yue Wu and Yicong Zhou},
  doi          = {10.1109/TNNLS.2023.3298383},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16845-16856},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consistent arbitrary style transfer using consistency training and self-attention module},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Manipulating identical filter redundancy for efficient
pruning on deep and complicated CNN. <em>TNNLS</em>, <em>35</em>(11),
16831–16844. (<a
href="https://doi.org/10.1109/TNNLS.2023.3298263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of redundancy in convolutional neural networks (CNNs) enables us to remove some filters/channels with acceptable performance drops. However, the training objective of CNNs usually tends to minimize an accuracy-related loss function without any attention paid to the redundancy, making the redundancy distribute randomly on all the filters, such that removing any of them may trigger information loss and accuracy drop, necessitating a fine-tuning step for recovery. In this article, we propose to manipulate the redundancy during training to facilitate network pruning. To this end, we propose a novel centripetal SGD (C-SGD) to make some filters identical, resulting in ideal redundancy patterns, as such filters become purely redundant due to their duplicates, hence removing them does not harm the network. As shown on CIFAR and ImageNet, C-SGD delivers better performance because the redundancy is better organized, compared to the existing methods. The efficiency also characterizes C-SGD because it is as fast as regular SGD, requires no fine-tuning, and can be conducted simultaneously on all the layers even in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by first training a model with the same architecture but wider layers and then squeezing it into the original width.},
  archive      = {J_TNNLS},
  author       = {Tianxiang Hao and Xiaohan Ding and Jungong Han and Yuchen Guo and Guiguang Ding},
  doi          = {10.1109/TNNLS.2023.3298263},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16831-16844},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Manipulating identical filter redundancy for efficient pruning on deep and complicated CNN},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HTD-TS3: Weakly supervised hyperspectral target detection
based on transformer via spectral–spatial similarity. <em>TNNLS</em>,
<em>35</em>(11), 16816–16830. (<a
href="https://doi.org/10.1109/TNNLS.2023.3298145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an advanced technique in remote sensing, hyperspectral target detection (HTD) is widely concerned in civilian and military applications. However, the limitation of prior and heterogeneous backgrounds makes HTD models sensitive to data corruption under various interference from the environment. In this article, a novel united HTD framework based on the concept of transformer is proposed to extract [HTD based on transformer via spectral-spatial similarity (HTD-TS3)] under weak supervision, which opens up more flexible ways to study HTD. For the first time, the transformer mechanism is introduced into the HTD task to extract spectral and spatial features in a unified optimization procedure. By modeling long-range dependence among spectra, it realizes spectral–spatial joint inference based on long-range context, which addresses the issues of insufficient utilization of spatial information. To provide samples for weakly supervised learning (WSL), the coarse sample selection and spectral sequence construction in an efficient way are proposed, which makes full use of limited prior information. Finally, an exponential constrained nonlinear function is adopted to acquire pixel-level prediction via combining discriminative spectral–spatial features and coarse spatial information. Experiments on real hyperspectral images (HSIs) captured by different sensors at various scenes verify the effectiveness and efficiency of HTD-TS3.},
  archive      = {J_TNNLS},
  author       = {Haonan Qin and Weiying Xie and Yunsong Li and Qian Du},
  doi          = {10.1109/TNNLS.2023.3298145},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16816-16830},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HTD-TS3: Weakly supervised hyperspectral target detection based on transformer via Spectral–Spatial similarity},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonessentiality of reservoir’s fading memory for
universality of reservoir computing. <em>TNNLS</em>, <em>35</em>(11),
16801–16815. (<a
href="https://doi.org/10.1109/TNNLS.2023.3298013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes a novel sufficient condition concerning approximations with reservoir computing (RC). Recently, RC using a physical system as the reservoir has attracted attention. Because many physical systems are modeled as state-space systems, it is necessary to guarantee the approximations given by reservoirs represented as nonlinear state-space systems. There are two problems with existing approaches: a reservoir must have a property called fading memory and must be represented as a set of maps between input and output signals on the bi-infinite-time (BIT) interval. These two conditions are too strict for reservoirs represented as nonlinear state-space systems as they require the reservoir to have a unique equilibrium state for the zero input. This article proposes an approach that employs operators from right-infinite-time (RIT) inputs to RIT outputs. Furthermore, we develop a novel extension of the Stone–Weierstrass theorem to handle discontinuous functions. To apply the extended theorem, we define functionals corresponding to operators and introduce a metric on the domain of the functionals. The resulting sufficient condition does not require the reservoir to have fading memory or continuity with respect to inputs and time. Therefore, our result guarantees the approximations with very common reservoirs and provides a rationale for physical RC. We present an example of a physical reservoir without fading memory. With the example reservoir, the RC model successfully approximates NARMA10, a benchmark task for time series predictions.},
  archive      = {J_TNNLS},
  author       = {Shuhei Sugiura and Ryo Ariizumi and Toru Asai and Shun-Ichi Azuma},
  doi          = {10.1109/TNNLS.2023.3298013},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16801-16815},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonessentiality of reservoir’s fading memory for universality of reservoir computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust principal component analysis: A median of means
approach. <em>TNNLS</em>, <em>35</em>(11), 16788–16800. (<a
href="https://doi.org/10.1109/TNNLS.2023.3298011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is a fundamental tool for data visualization, denoising, and dimensionality reduction. It is widely popular in statistics, machine learning, computer vision, and related fields. However, PCA is well-known to fall prey to outliers and often fails to detect the true underlying low-dimensional structure within the dataset. Following the Median of Means (MoM) philosophy, recent supervised learning methods have shown great success in dealing with outlying observations without much compromise to their large sample theoretical properties. This article proposes a PCA procedure based on the MoM principle. Called the MoMPCA, the proposed method is not only computationally appealing but also achieves optimal convergence rates under minimal assumptions. In particular, we explore the nonasymptotic error bounds of the obtained solution via the aid of the Rademacher complexities while granting absolutely no assumption on the outlying observations. The derived concentration results are not dependent on the dimension because the analysis is conducted in a separable Hilbert space, and the results only depend on the fourth moment of the underlying distribution in the corresponding norm. The proposal’s efficacy is also thoroughly showcased through simulations and real data applications.},
  archive      = {J_TNNLS},
  author       = {Debolina Paul and Saptarshi Chakraborty and Swagatam Das},
  doi          = {10.1109/TNNLS.2023.3298011},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16788-16800},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust principal component analysis: A median of means approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning and symbolic regression for discovering
parametric equations. <em>TNNLS</em>, <em>35</em>(11), 16775–16787. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic regression is a machine learning technique that can learn the equations governing data and thus has the potential to transform scientific discovery. However, symbolic regression is still limited in the complexity and dimensionality of the systems that it can analyze. Deep learning, on the other hand, has transformed machine learning in its ability to analyze extremely complex and high-dimensional datasets. We propose a neural network architecture to extend symbolic regression to parametric systems where some coefficient may vary, but the structure of the underlying governing equation remains constant. We demonstrate our method on various analytic expressions and partial differential equations (PDEs) with varying coefficients and show that it extrapolates well outside of the training domain. The proposed neural-network-based architecture can also be enhanced by integrating with other deep learning architectures such that it can analyze high-dimensional data while being trained end-to-end. To this end, we demonstrate the scalability of our architecture by incorporating a convolutional encoder to analyze 1-D images of varying spring systems.},
  archive      = {J_TNNLS},
  author       = {Michael Zhang and Samuel Kim and Peter Y. Lu and Marin Soljačić},
  doi          = {10.1109/TNNLS.2023.3297978},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16775-16787},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning and symbolic regression for discovering parametric equations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronization-inspired interpretable neural networks.
<em>TNNLS</em>, <em>35</em>(11), 16762–16774. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synchronization is a ubiquitous phenomenon in nature that enables the orderly presentation of information. In the human brain, for instance, functional modules such as the visual, motor, and language cortices form through neuronal synchronization. Inspired by biological brains and previous neuroscience studies, we propose an interpretable neural network incorporating a synchronization mechanism. The basic idea is to constrain each neuron, such as a convolution filter, to capture a single semantic pattern while synchronizing similar neurons to facilitate the formation of interpretable functional modules. Specifically, we regularize the activation map of a neuron to surround its focus position of the activated pattern in a sample. Moreover, neurons locally interact with each other, and similar ones are synchronized together during the training phase adaptively. Such local aggregation preserves the globally distributed representation nature of the neural network model, enabling a reasonably interpretable representation. To analyze the neuron interpretability comprehensively, we introduce a series of novel evaluation metrics from multiple aspects. Qualitative and quantitative experiments demonstrate that the proposed method outperforms many state-of-the-art algorithms in terms of interpretability. The resulting synchronized functional modules show module consistency across data and semantic specificity within modules.},
  archive      = {J_TNNLS},
  author       = {Wei Han and Zhili Qin and Jiaming Liu and Christian Böhm and Junming Shao},
  doi          = {10.1109/TNNLS.2023.3297672},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16762-16774},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization-inspired interpretable neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchically contrastive hard sample mining for graph
self-supervised pretraining. <em>TNNLS</em>, <em>35</em>(11),
16748–16761. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has recently emerged as a powerful technique for graph self-supervised pretraining (GSP). By maximizing the mutual information (MI) between a positive sample pair, the network is forced to extract discriminative information from graphs to generate high-quality sample representations. However, we observe that, in the process of MI maximization (Infomax), the existing contrastive GSP algorithms suffer from at least one of the following problems: 1) treat all samples equally during optimization and 2) fall into a single contrasting pattern within the graph. Consequently, the vast number of well-categorized samples overwhelms the representation learning process, and limited information is accumulated, thus deteriorating the learning capability of the network. To solve these issues, in this article, by fusing the information from different views and conducting hard sample mining in a hierarchically contrastive manner, we propose a novel GSP algorithm called hierarchically contrastive hard sample mining (HCHSM). The hierarchical property of this algorithm is manifested in two aspects. First, according to the results of multilevel MI estimation in different views, the MI-based hard sample selection (MHSS) module keeps filtering the easy nodes and drives the network to focus more on hard nodes. Second, to collect more comprehensive information for hard sample learning, we introduce a hierarchically contrastive scheme to sequentially force the learned node representations to involve multilevel intrinsic graph features. In this way, as the contrastive granularity goes finer, the complementary information from different levels can be uniformly encoded to boost the discrimination of hard samples and enhance the quality of the learned graph embedding. Extensive experiments on seven benchmark datasets indicate that the HCHSM performs better than other competitors on node classification and node clustering tasks. The source code of HCHSM is available at https://github.com/WxTu/HCHSM .},
  archive      = {J_TNNLS},
  author       = {Wenxuan Tu and Sihang Zhou and Xinwang Liu and Chunpeng Ge and Zhiping Cai and Yue Liu},
  doi          = {10.1109/TNNLS.2023.3297607},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16748-16761},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchically contrastive hard sample mining for graph self-supervised pretraining},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral–spatial transformer for hyperspectral image
sharpening. <em>TNNLS</em>, <em>35</em>(11), 16733–16747. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have recently achieved outstanding performance for hyperspectral (HS) and multispectral (MS) image fusion. However, CNNs cannot explore the long-range dependence for HS and MS image fusion because of their local receptive fields. To overcome this limitation, a transformer is proposed to leverage the long-range dependence from the network inputs. Because of the ability of long-range modeling, the transformer overcomes the sole CNN on many tasks, whereas its use for HS and MS image fusion is still unexplored. In this article, we propose a spectral–spatial transformer (SST) to show the potentiality of transformers for HS and MS image fusion. We devise first two branches to extract spectral and spatial features in the HS and MS images by SST blocks, which can explore the spectral and spatial long-range dependence, respectively. Afterward, spectral and spatial features are fused feeding the result back to spectral and spatial branches for information interaction. Finally, the high-resolution (HR) HS image is reconstructed by dense links from all the fused features to make full use of them. The experimental analysis demonstrates the high performance of the proposed approach compared with some state-of-the-art (SOTA) methods.},
  archive      = {J_TNNLS},
  author       = {Lihui Chen and Gemine Vivone and Jiayi Qin and Jocelyn Chanussot and Xiaomin Yang},
  doi          = {10.1109/TNNLS.2023.3297319},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16733-16747},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spectral–Spatial transformer for hyperspectral image sharpening},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large margin weighted k-nearest neighbors label distribution
learning for classification. <em>TNNLS</em>, <em>35</em>(11),
16720–16732. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution learning (LDL) helps solve label ambiguity and has found wide applications. However, it may suffer from the challenge of objective inconsistency when adopted to classification problems because the learning objective of LDL is inconsistent with that of classification. Some LDL algorithms have been proposed to solve this issue, but they presume that label distribution can be represented by the maximum entropy model, which may not hold in many real-world problems. In this article, we design two novel LDL methods based on the $ k $ -nearest neighbors ( $ k $ NNs) approach without assuming any form of label distribution. First, we propose the large margin weighted $ k $ NN LDL (LW- $ k $ NNLDL). It learns a weight vector for the $ k $ NN algorithm to learn label distribution and implement a large margin to address the objective inconsistency. Second, we put forward the large margin distance-weighted $ k $ NN LDL (LD $ k $ NN-LDL) that learns distance-dependent weight vectors to consider the difference in the neighborhoods of different instances. Theoretical results show that our methods can learn any general-form label distribution. Moreover, extensive experimental studies validate that our methods significantly outperform state-of-the-art LDL approaches.},
  archive      = {J_TNNLS},
  author       = {Jing Wang and Xin Geng},
  doi          = {10.1109/TNNLS.2023.3297261},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16720-16732},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Large margin weighted k-nearest neighbors label distribution learning for classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting zero-shot learning via contrastive optimization of
attribute representations. <em>TNNLS</em>, <em>35</em>(11), 16706–16719.
(<a href="https://doi.org/10.1109/TNNLS.2023.3297134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize classes that do not have samples in the training set. One representative solution is to directly learn an embedding function associating visual features with corresponding class semantics for recognizing new classes. Many methods extend upon this solution, and recent ones are especially keen on extracting rich features from images, e.g., attribute features. These attribute features are normally extracted within each individual image; however, the common traits for features across images yet belonging to the same attribute are not emphasized. In this article, we propose a new framework to boost ZSL by explicitly learning attribute prototypes beyond images and contrastively optimizing them with attribute-level features within images. Besides the novel architecture, two elements are highlighted for attribute representations: a new prototype generation module (PM) is designed to generate attribute prototypes from attribute semantics; a hard-example-based contrastive optimization scheme is introduced to reinforce attribute-level features in the embedding space. We explore two alternative backbones, CNN-based and transformer-based, to build our framework and conduct experiments on three standard benchmarks, Caltech-UCSD Birds-200-2011 (CUB), SUN attribute database (SUN), and animals with attributes 2 (AwA2). Results on these benchmarks demonstrate that our method improves the state of the art by a considerable margin. Our codes will be available at https://github.com/dyabel/CoAR-ZSL.git .},
  archive      = {J_TNNLS},
  author       = {Yu Du and Miaojing Shi and Fangyun Wei and Guoqi Li},
  doi          = {10.1109/TNNLS.2023.3297134},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16706-16719},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Boosting zero-shot learning via contrastive optimization of attribute representations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary dynamics optimal research-oriented tumor
immunity architecture. <em>TNNLS</em>, <em>35</em>(11), 16696–16705. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article is devoted to evolutionary dynamics optimal control-oriented tumor immune differential game system. First, the mathematical model covering immune cells and tumor cells considering the effects of chemotherapy drugs and immune agents. Second, the bounded optimal control problem covering is transformed into solving Hamilton–Jacobi–Bellman (HJB) equation considering the actual constraints and infinite-horizon performance index based on minimizing the amount of medication administered. Finally, approximate optimal control strategy is acquired through iterative-dual heuristic dynamic programming (I-DHP) algorithm avoiding dimensional disaster effectively and providing optimal treatment scheme for clinical applications.},
  archive      = {J_TNNLS},
  author       = {Jiayue Sun and Ying Yan and Fangxiao Cheng and Jiajun Wang and Yuxue Dang},
  doi          = {10.1109/TNNLS.2023.3297121},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16696-16705},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Evolutionary dynamics optimal research-oriented tumor immunity architecture},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Going deeper, generalizing better: An information-theoretic
view for deep learning. <em>TNNLS</em>, <em>35</em>(11), 16683–16695.
(<a href="https://doi.org/10.1109/TNNLS.2023.3297113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has transformed computer vision, natural language processing, and speech recognition. However, two critical questions remain obscure: 1) why do deep neural networks (DNNs) generalize better than shallow networks and 2) does it always hold that a deeper network leads to better performance? In this article, we first show that the expected generalization error of neural networks (NNs) can be upper bounded by the mutual information between the learned features in the last hidden layer and the parameters of the output layer. This bound further implies that as the number of layers increases in the network, the expected generalization error will decrease under mild conditions. Layers with strict information loss, such as the convolutional or pooling layers, reduce the generalization error for the whole network; this answers the first question. However, algorithms with zero expected generalization error do not imply a small test error. This is because the expected training error is large when the information for fitting the data is lost as the number of layers increases. This suggests that the claim “the deeper the better” is conditioned on a small training error. Finally, we show that deep learning satisfies a weak notion of stability and provides some generalization error bounds for noisy stochastic gradient decent (SGD) and binary classification in DNNs.},
  archive      = {J_TNNLS},
  author       = {Jingwei Zhang and Tongliang Liu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2023.3297113},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16683-16695},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Going deeper, generalizing better: An information-theoretic view for deep learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MetaFed: Federated learning among federations with cyclic
knowledge distillation for personalized healthcare. <em>TNNLS</em>,
<em>35</em>(11), 16671–16682. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has attracted increasing attention to building models without accessing raw user data, especially in healthcare. In real applications, different federations can seldom work together due to possible reasons such as data heterogeneity and distrust/inexistence of the central server. In this article, we propose a novel framework called MetaFed to facilitate trustworthy FL between different federations. MetaFed obtains a personalized model for each federation without a central server via the proposed cyclic knowledge distillation. Specifically, MetaFed treats each federation as a meta distribution and aggregates knowledge of each federation in a cyclic manner. The training is split into two parts: common knowledge accumulation and personalization. Comprehensive experiments on seven benchmarks demonstrate that MetaFed without a server achieves better accuracy compared with state-of-the-art methods [e.g., 10%+ accuracy improvement compared with the baseline for physical activity monitoring dataset (PAMAP2)] with fewer communication costs. More importantly, MetaFed shows remarkable performance in real-healthcare-related applications.},
  archive      = {J_TNNLS},
  author       = {Yiqiang Chen and Wang Lu and Xin Qin and Jindong Wang and Xing Xie},
  doi          = {10.1109/TNNLS.2023.3297103},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16671-16682},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MetaFed: Federated learning among federations with cyclic knowledge distillation for personalized healthcare},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Toward generalized artificial intelligence by assessment
aggregation with applications to standard and extreme classifications.
<em>TNNLS</em>, <em>35</em>(11), 16659–16670. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article proposes a plural learning framework combining the ingredients found in a tribunal for the derivation of a more generalized artificial intelligence (GAI) when starting from a specialized set of convolutional neural networks (CNNs). This framework involves at least two different training stages called, respectively, specialization and generalization. In the specialization stage, any CNN considered in a given set learns to predict independently of other elements of the set. In the second stage called generalization, an integration network learns to predict from assessment measures fed by downstream specialized CNNs. The assessment measures considered are categorical softmax probabilities and learning to judge from these assessments relies on independent CNNs. Generalization proof of concepts is provided in terms of multimodel, multimodal, and distributed schemes. The multimodel framework is such that different CNN models operating on the same modality cooperate for decision purpose. The multimodal framework implies specializations of CNN with respect to different input modalities. The distributed framework proposed is associated with assessment exchanges: it such that the aggregation aims at determining relevant joint assessments for mapping a given input to a single or a multiple output category. The performance of these aggregation frameworks is shown to be outstanding for both standard and extreme classification issues.},
  archive      = {J_TNNLS},
  author       = {Abdourrahmane M. Atto},
  doi          = {10.1109/TNNLS.2023.3297079},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16659-16670},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward generalized artificial intelligence by assessment aggregation with applications to standard and extreme classifications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A latent representation generalizing network for domain
generalization in cross-scenario monitoring. <em>TNNLS</em>,
<em>35</em>(11), 16644–16658. (<a
href="https://doi.org/10.1109/TNNLS.2023.3296942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-scenario monitoring requires domain generalization (DG) for changed knowledge when auxiliary information is unavailable and only one source scenario is involved. In this article, a latent representation generalizing network (LRGN) is proposed to learn transferable knowledge through generalizing the latent representations for cross-scenario monitoring in perimeter security. LRGN is composed of a sequential-variational generative adversarial network (SVGAN), a coupled SVGAN (Co-SVGAN), and a knowledge-aggregated SVGAN. First, the Co-SVGAN can learn domain-invariant latent representations to model dual-domain joint distribution of background data, which is usually sufficient in the source and target scenarios. Deceptive domain shifts are generated based on the domain-invariant latent representations without auxiliary information. Then, SVGAN models the changing knowledge by estimating the distribution of domain shifts. Furthermore, the knowledge-aggregated SVGAN can transfer the learned domain-invariant knowledge from Co-SVGAN for generalizing the latent representations through approximating the distribution of domain shifts. Accordingly, LRGN is trained by a four-phase optimization strategy for DG through generating target-scenario samples of concerned events based on the generalized latent representations. The feasibility and effectiveness of the proposed method are validated through real-field experiments of perimeter security applications in two scenarios.},
  archive      = {J_TNNLS},
  author       = {Sudao He and Fuyang Chen and Hongtian Chen},
  doi          = {10.1109/TNNLS.2023.3296942},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16644-16658},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A latent representation generalizing network for domain generalization in cross-scenario monitoring},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online multikernel learning method via online biconvex
optimization. <em>TNNLS</em>, <em>35</em>(11), 16630–16643. (<a
href="https://doi.org/10.1109/TNNLS.2023.3296895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random feature-based online multikernel learning (RF-OMKL) is a promising low-complexity framework for machine learning optimization from continuous streaming data. Nonetheless, it is still an open problem to find an efficient algorithm with an analytical performance guarantee due to the challenge of an underlying online biconvex optimization (OBO). The state-of-the-art method [named expert-based online multikernel learning (EoKle)] tackled this problem approximately with the lens of expert-based online learning, in which multiple kernels (or experts) optimize their own kernel functions separately and the best sole one is determined via Hedge algorithm. It is asymptotically optimal as to the best sole kernel function in hindsight. We propose collaborative expert-based online multikernel learning (CoKle) by devising a collaborative Hedge (CoHedge) algorithm, in which kernel functions separately optimized as in EoKle are combined in an asymptotically optimal way. It is proved that CoKle is asymptotically optimal as to the best combination of each optimal kernel function in hindsight. Remarkably, this is the first method with a theoretical performance guarantee for expert-based RF-OMKL. Despite its effectiveness, CoKle is inherently suboptimal due to the individual optimization of kernel functions. We address this by presenting an OBO-based method (named BoKle) and partially prove its asymptotic optimality for RF-OMKL. Thus, BoKle can outperform the suboptimal expert-based methods such as CoKle and EoKle. Finally, we demonstrate the superiority of BoKle via experiments with real datasets.},
  archive      = {J_TNNLS},
  author       = {Songnam Hong},
  doi          = {10.1109/TNNLS.2023.3296895},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16630-16643},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online multikernel learning method via online biconvex optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fixed-time control and estimation of discontinuous fuzzy
neural networks: Novel lyapunov method of fixed-time stability.
<em>TNNLS</em>, <em>35</em>(11), 16616–16629. (<a
href="https://doi.org/10.1109/TNNLS.2023.3296881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the fixed-time stability (FXTS) issue for discontinuous system described by differential equation (DE) with time-varying parameters. Using the tool of differential inclusion (DI), several improved FXTS criteria and estimation formulas of settling time (SET) are derived by employing a relaxed Lyapunov method. The special novelty of the developed Lyapunov method is that the derivative of Lyapunov function possesses indefiniteness. As an important application, the established FXTS theorems are utilized to deal with fixed-time (FXT) synchronization issue for fuzzy neural networks (FNNs) possessing state discontinuity, where the fuzzy operation is used in the synaptic law computing, and one typical time-varying switching control protocol is designed. Moreover, a series of estimations of SET for FXT synchronization are given out. Finally, the simulation examples are present to substantiate the validity of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Zuowei Cai and Lihong Huang and Zengyun Wang},
  doi          = {10.1109/TNNLS.2023.3296881},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16616-16629},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fixed-time control and estimation of discontinuous fuzzy neural networks: Novel lyapunov method of fixed-time stability},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AHEGC: Adaptive hindsight experience replay with
goal-amended curiosity module for robot control. <em>TNNLS</em>,
<em>35</em>(11), 16602–16615. (<a
href="https://doi.org/10.1109/TNNLS.2023.3296765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With shaped reward functions, reinforcement learning (RL) has recently been successfully applied to several robot control tasks. However, designing a task-relevant and well-performing reward function takes time and effort. Still, if RL can train an agent to complete a task in a sparse reward environment, it is an effective way to address the difficulty of reward function design, but it is still a significant challenge. To address this issue, the pioneering hindsight experience replay (HER) method dramatically enhances the probability of acquiring skills in sparse reward environments by transforming unsuccessful experiences into helpful training samples. However, HER still requires a lengthy training period. In this article, we propose a new technique based on HER termed adaptive HER with goal-amended curiosity module (AHEGC) for further enhancing sample and exploration efficiency. Specifically, an adaptive adjustment strategy of hindsight experience (HE) sampling rate and reward weights is developed to enhance sample efficiency. Furthermore, we introduce a curiosity mechanism to encourage more efficient exploration of the environment and propose a goal-amended (GA) curiosity module as a solution to the problem of over-seeking novelty caused by the curiosity introduced. We conducted experiments on six demanding robot control tasks with binary rewards, including Fetch and Hand environments. The results show that the proposed method outperforms existing methods regarding learning ability and convergence speed.},
  archive      = {J_TNNLS},
  author       = {Hongliang Zeng and Ping Zhang and Fang Li and Chubin Lin and Junkang Zhou},
  doi          = {10.1109/TNNLS.2023.3296765},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16602-16615},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AHEGC: Adaptive hindsight experience replay with goal-amended curiosity module for robot control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring feature representation learning for
semi-supervised medical image segmentation. <em>TNNLS</em>,
<em>35</em>(11), 16589–16601. (<a
href="https://doi.org/10.1109/TNNLS.2023.3296652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a simple yet effective two-stage framework for semi-supervised medical image segmentation. Unlike prior state-of-the-art semi-supervised segmentation methods that predominantly rely on pseudo supervision directly on predictions, such as consistency regularization and pseudo labeling, our key insight is to explore the feature representation learning with labeled and unlabeled (i.e., pseudo labeled) images to regularize a more compact and better-separated feature space, which paves the way for low-density decision boundary learning and therefore enhances the segmentation performance. A stage-adaptive contrastive learning method is proposed, containing a boundary-aware contrastive loss that takes advantage of the labeled images in the first stage, as well as a prototype-aware contrastive loss to optimize both labeled and pseudo labeled images in the second stage. To obtain more accurate prototype estimation, which plays a critical role in prototype-aware contrastive learning, we present an aleatoric uncertainty-aware method to generate higher quality pseudo labels. Aleatoric-uncertainty adaptive (AUA) adaptively regularizes prediction consistency by taking advantage of image ambiguity, which, given its significance, is underexplored by existing works. Our method achieves the best results on three public medical image segmentation benchmarks.},
  archive      = {J_TNNLS},
  author       = {Huimin Wu and Xiaomeng Li and Kwang-Ting Cheng},
  doi          = {10.1109/TNNLS.2023.3296652},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16589-16601},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploring feature representation learning for semi-supervised medical image segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample efficient deep reinforcement learning with online
state abstraction and causal transformer model prediction.
<em>TNNLS</em>, <em>35</em>(11), 16574–16588. (<a
href="https://doi.org/10.1109/TNNLS.2023.3296642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (RL) typically requires a tremendous number of training samples, which are not practical in many applications. State abstraction and world models are two promising approaches for improving sample efficiency in deep RL. However, both state abstraction and world models may degrade the learning performance. In this article, we propose an abstracted model-based policy learning (AMPL) algorithm, which improves the sample efficiency of deep RL. In AMPL, a novel state abstraction method via multistep bisimulation is first developed to learn task-related latent state spaces. Hence, the original Markov decision processes (MDPs) are compressed into abstracted MDPs. Then, a causal transformer model predictor (CTMP) is designed to approximate the abstracted MDPs and generate long-horizon simulated trajectories with a smaller multistep prediction error. Policies are efficiently learned through these trajectories within the abstracted MDPs via a modified multistep soft actor-critic algorithm with a $\lambda $ -target. Moreover, theoretical analysis shows that the AMPL algorithm can improve sample efficiency during the training process. On Atari games and the DeepMind Control (DMControl) suite, AMPL surpasses current state-of-the-art deep RL algorithms in terms of sample efficiency. Furthermore, DMControl tasks with moving noises are conducted, and the results demonstrate that AMPL is robust to task-irrelevant observational distractors and significantly outperforms the existing approaches.},
  archive      = {J_TNNLS},
  author       = {Yixing Lan and Xin Xu and Qiang Fang and Jianye Hao},
  doi          = {10.1109/TNNLS.2023.3296642},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16574-16588},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sample efficient deep reinforcement learning with online state abstraction and causal transformer model prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network-based nonconservative predefined-time
backstepping control for uncertain strict-feedback nonlinear systems.
<em>TNNLS</em>, <em>35</em>(11), 16562–16573. (<a
href="https://doi.org/10.1109/TNNLS.2023.3296194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To handle the tracking problem of uncertain strict-feedback nonlinear systems with matched and mismatched composite disturbances, this article studies a predefined-time backstepping controller by resorting to a Lyapunov-based predefined-time dynamic paradigm, a regulation function, and neural networks (NNs). Moreover, an adding-absolute-value (ADV) technique is adopted in the design process to remove the control singularity. Theoretical analyses prove the boundedness of all closed-loop system signals and the predefined-time convergence of the tracking error into an arbitrarily small vicinity of the origin. The proposed controller exhibits four advantages: 1) the actual convergence time is precisely predefined by only one design parameter irrespective of the initial conditions, and the control energy is economized; 2) no unbounded terms are adopted for predefining the actual convergence time, thus avoiding numerical overflow problem under limited memory space and gaining strong noise-tolerant ability; 3) the peaking tracking error and control input magnitude can be effectively reduced by appropriately setting parameters of the regulation function; and 4) the controller is continuous and nonsingular everywhere. Finally, a practical example of a single-link manipulator is presented to validate the efficacy and superiority of our predefined-time controller.},
  archive      = {J_TNNLS},
  author       = {Jixing Lv and Xiaozhe Ju and Changhong Wang},
  doi          = {10.1109/TNNLS.2023.3296194},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16562-16573},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based nonconservative predefined-time backstepping control for uncertain strict-feedback nonlinear systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Who should i engage with at what time? A missing event-aware
temporal graph neural network. <em>TNNLS</em>, <em>35</em>(11),
16548–16561. (<a
href="https://doi.org/10.1109/TNNLS.2023.3295592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graph neural network (GNN) has recently received significant attention due to its wide application scenarios, such as bioinformatics, knowledge graphs, and social networks. There are some temporal GNNs that achieve remarkable results. However, these works focus on future event prediction and are performed under the assumption that all historical events are observable. In real-world applications, events are not always observable, and estimating event time is as important as predicting future events. In this article, we propose MTGN, a missing event-aware temporal GNN, which uniformly models evolving graph structure and timing of events to support predicting what will happen in the future and when it will happen. MTGN models the dynamic of both observed and missing events as two coupled temporal point processes (TPPs), thereby incorporating the effects of missing events into the network. Experimental results on several real-world temporal graphs demonstrate that MTGN significantly outperforms the existing methods with up to 89% and 112% more accurate time and link prediction. Code can be found on https://github.com/HIT-ICES/TNNLS-MTGN .},
  archive      = {J_TNNLS},
  author       = {Mingyi Liu and Zhiying Tu and Xiaofei Xu and Zhongjie Wang},
  doi          = {10.1109/TNNLS.2023.3295592},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16548-16561},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Who should i engage with at what time? a missing event-aware temporal graph neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel dynamic multiobjective optimization algorithm with
non-inductive transfer learning based on multi-strategy adaptive
selection. <em>TNNLS</em>, <em>35</em>(11), 16533–16547. (<a
href="https://doi.org/10.1109/TNNLS.2023.3295461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel multi-strategy adaptive selection-based dynamic multiobjective optimization algorithm (MSAS-DMOA) is proposed, which adopts the non-inductive transfer learning (TL) paradigm to solve dynamic multiobjective optimization problems (DMOPs). In particular, based on a scoring system that evaluates environmental changes, the source domain is adaptively constructed with several optional groups to enrich the knowledge. Along with a group of guide solutions, the importance of historical experiences is estimated via the kernel mean matching (KMM) method, which avoids designing strategies to label individuals. The proposed MSAS-DMOA is comprehensively evaluated on 14 DMOPs, and the results show an overwhelming performance improvement in terms of both convergence and diversity as compared with other four popular DMOAs. In addition, ablation studies are also conducted to validate the superiority of the applied strategies in MSAS-DMOA, which can effectively alleviate the negative transfer phenomenon. Without the conventional labeling procedure, the proposed method also yields satisfactory results, which can provide valuable reference for designing other evolutionary transfer optimization (ETO) algorithms.},
  archive      = {J_TNNLS},
  author       = {Han Li and Zidong Wang and Chengbo Lan and Peishu Wu and Nianyin Zeng},
  doi          = {10.1109/TNNLS.2023.3295461},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16533-16547},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel dynamic multiobjective optimization algorithm with non-inductive transfer learning based on multi-strategy adaptive selection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust PACm: Training ensemble models under misspecification
and outliers. <em>TNNLS</em>, <em>35</em>(11), 16518–16532. (<a
href="https://doi.org/10.1109/TNNLS.2023.3295168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard Bayesian learning is known to have suboptimal generalization capabilities under misspecification and in the presence of outliers. Probably approximately correct (PAC)-Bayes theory demonstrates that the free energy criterion minimized by Bayesian learning is a bound on the generalization error for Gibbs predictors (i.e., for single models drawn at random from the posterior) under the assumption of sampling distributions uncontaminated by outliers. This viewpoint provides a justification for the limitations of Bayesian learning when the model is misspecified, requiring ensembling, and when data are affected by outliers. In recent work, PAC-Bayes bounds—referred to as PACm—were derived to introduce free energy metrics that account for the performance of ensemble predictors, obtaining enhanced performance under misspecification. This work presents a novel robust free energy criterion that combines the generalized logarithm score function with PACm ensemble bounds. The proposed free energy training criterion produces predictive distributions that are able to concurrently counteract the detrimental effects of misspecification—with respect to both likelihood and prior distribution—and outliers.},
  archive      = {J_TNNLS},
  author       = {Matteo Zecchin and Sangwoo Park and Osvaldo Simeone and Marios Kountouris and David Gesbert},
  doi          = {10.1109/TNNLS.2023.3295168},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16518-16532},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust PACm: Training ensemble models under misspecification and outliers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STGlow: A flow-based generative framework with
dual-graphormer for pedestrian trajectory prediction. <em>TNNLS</em>,
<em>35</em>(11), 16504–16517. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pedestrian trajectory prediction task is an essential component of intelligent systems. Its applications include but are not limited to autonomous driving, robot navigation, and anomaly detection of monitoring systems. Due to the diversity of motion behaviors and the complex social interactions among pedestrians, accurately forecasting their future trajectory is challenging. Existing approaches commonly adopt generative adversarial networks (GANs) or conditional variational autoencoders (CVAEs) to generate diverse trajectories. However, GAN-based methods do not directly model data in a latent space, which may make them fail to have full support over the underlying data distribution. CVAE-based methods optimize a lower bound on the log-likelihood of observations, which may cause the learned distribution to deviate from the underlying distribution. The above limitations make existing approaches often generate highly biased or inaccurate trajectories. In this article, we propose a novel generative flow-based framework with a dual-graphormer for pedestrian trajectory prediction (STGlow). Different from previous approaches, our method can more precisely model the underlying data distribution by optimizing the exact log-likelihood of motion behaviors. Besides, our method has clear physical meanings for simulating the evolution of human motion behaviors. The forward process of the flow gradually degrades complex motion behavior into simple behavior, while its reverse process represents the evolution of simple behavior into complex motion behavior. Furthermore, we introduce a dual-graphormer combined with the graph structure to more adequately model the temporal dependencies and the mutual spatial interactions. Experimental results on several benchmarks demonstrate that our method achieves much better performance compared to previous state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Rongqin Liang and Yuanman Li and Jiantao Zhou and Xia Li},
  doi          = {10.1109/TNNLS.2023.3294998},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16504-16517},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {STGlow: A flow-based generative framework with dual-graphormer for pedestrian trajectory prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). XTQA: Span-level explanations for textbook question
answering. <em>TNNLS</em>, <em>35</em>(11), 16493–16503. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textbook question answering (TQA) is the task of correctly answering diagram or nondiagram (ND) questions given large multimodal contexts consisting of abundant essays and diagrams. In real-world scenarios, an explainable TQA system plays a key role in deepening humans’ understanding of learned knowledge. However, there is no work to investigate how to provide explanations currently. To address this issue, we devise a novel architecture toward span-level eXplanations for TQA (XTQA). In this article, spans are the combinations of sentences within a paragraph. The key idea is to consider the entire textual context of a lesson as candidate evidence and then use our proposed coarse-to-fine grained explanation extracting (EE) algorithm to narrow down the evidence scope and extract the span-level explanations with varying lengths for answering different questions. The EE algorithm can also be integrated into other TQA methods to make them explainable and improve the TQA performance. Experimental results show that XTQA obtains the best overall explanation result [mean intersection over union (mIoU)] of 52.38% on the first 300 questions of CK12-QA test splits, demonstrating the explainability of our method (ND: 150 and diagram: 150). The results also show that XTQA achieves the best TQA performance of 36.46% and 36.95% on the aforementioned splits. We have released our code in https://github.com/dr-majie/opentqa .},
  archive      = {J_TNNLS},
  author       = {Jie Ma and Qi Chai and Jun Liu and Qingyu Yin and Pinghui Wang and Qinghua Zheng},
  doi          = {10.1109/TNNLS.2023.3294991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16493-16503},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {XTQA: Span-level explanations for textbook question answering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperspectral blind unmixing using a double deep image
prior. <em>TNNLS</em>, <em>35</em>(11), 16478–16492. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of machine learning, hyperspectral image (HSI) unmixing problems have been tackled using learning-based methods. However, physically meaningful unmixing results are not guaranteed without proper guidance. In this work, we propose an unsupervised framework inspired by deep image prior (DIP) that can be used for both linear and nonlinear blind unmixing models. The framework consists of three modules: 1) an Endmember estimation module using DIP (EDIP); 2) an Abundance estimation module using DIP (ADIP); and 3) a mixing module (MM). EDIP and ADIP modules generate endmembers and abundances, respectively, while MM produces a reconstruction of the HSI observations based on the postulated unmixing model. We introduce a composite loss function that applies to both linear and nonlinear unmixing models to generate meaningful unmixing results. In addition, we propose an adaptive loss weight strategy for better unmixing results in nonlinear mixing scenarios. The proposed methods outperform state-of-the-art unmixing algorithms in extensive experiments conducted on both synthetic and real datasets.},
  archive      = {J_TNNLS},
  author       = {Chao Zhou and Miguel R. D. Rodrigues},
  doi          = {10.1109/TNNLS.2023.3294714},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16478-16492},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral blind unmixing using a double deep image prior},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Emotion-semantic-aware dual contrastive learning for
epistemic emotion identification of learner-generated reviews in MOOCs.
<em>TNNLS</em>, <em>35</em>(11), 16464–16477. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the epistemic emotions of learner-generated reviews in massive open online courses (MOOCs) can help instructors provide adaptive guidance and interventions for learners. The epistemic emotion identification task is a fine-grained identification task that contains multiple categories of emotions arising during the learning process. Previous studies only consider emotional or semantic information within the review texts alone, which leads to insufficient feature representation. In addition, some categories of epistemic emotions are ambiguously distributed in feature space, making them hard to be distinguished. In this article, we present an emotion-semantic-aware dual contrastive learning (ES-DCL) approach to tackle these issues. In order to learn sufficient feature representation, implicit semantic features and human-interpretable emotional features are, respectively, extracted from two different views to form complementary emotional-semantic features. On this basis, by leveraging the experience of domain experts and the input emotional-semantic features, two types of contrastive losses (label contrastive loss and feature contrastive loss) are formulated. They are designed to train the discriminative distribution of emotional-semantic features in the sample space and to solve the anisotropy problem between different categories of epistemic emotions. The proposed ES-DCL is compared with 11 other baseline models on four different disciplinary MOOCs review datasets. Extensive experimental results show that our approach improves the performance of epistemic emotion identification, and significantly outperforms state-of-the-art deep learning-based methods in learning more discriminative sentence representations.},
  archive      = {J_TNNLS},
  author       = {Zhi Liu and Chaodong Wen and Zhu Su and Sannyuya Liu and Jianwen Sun and Weizheng Kong and Zongkai Yang},
  doi          = {10.1109/TNNLS.2023.3294636},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16464-16477},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Emotion-semantic-aware dual contrastive learning for epistemic emotion identification of learner-generated reviews in MOOCs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clinical prompt learning with frozen language models.
<em>TNNLS</em>, <em>35</em>(11), 16453–16463. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the first transformer-based language models were published in the late 2010s, pretraining with general text and then fine-tuning the model on a task-specific dataset often achieved the state-of-the-art performance. However, more recent work suggests that for some tasks, directly prompting the pretrained model matches or surpasses fine-tuning in performance with few or no model parameter updates required. The use of prompts with language models for natural language processing (NLP) tasks is known as prompt learning. We investigated the viability of prompt learning on clinically meaningful decision tasks and directly compared this with more traditional fine-tuning methods. Results show that prompt learning methods were able to match or surpass the performance of traditional fine-tuning with up to 1000 times fewer trainable parameters, less training time, less training data, and lower computation resource requirements. We argue that these characteristics make prompt learning a very desirable alternative to traditional fine-tuning for clinical tasks, where the computational resources of public health providers are limited, and where data can often not be made available or not be used for fine-tuning due to patient privacy concerns. The complementary code to reproduce the experiments presented in this work can be found at https://github.com/NtaylorOX/Public_Clinical_Prompt .},
  archive      = {J_TNNLS},
  author       = {Niall Taylor and Yi Zhang and Dan W. Joyce and Ziming Gao and Andrey Kormilitzin and Alejo Nevado-Holgado},
  doi          = {10.1109/TNNLS.2023.3294633},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16453-16463},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Clinical prompt learning with frozen language models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lifelong learning with cycle memory networks.
<em>TNNLS</em>, <em>35</em>(11), 16439–16452. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from a sequence of tasks for a lifetime is essential for an agent toward artificial general intelligence. Despite the explosion of this research field in recent years, most work focuses on the well-known catastrophic forgetting issue. In contrast, this work aims to explore knowledge-transferable lifelong learning without storing historical data and significant additional computational overhead. We demonstrate that existing data-free frameworks, including regularization-based single-network and structure-based multinetwork frameworks, face a fundamental issue of lifelong learning, named anterograde forgetting, i.e., preserving and transferring memory may inhibit the learning of new knowledge. We attribute it to the fact that the learning network capacity decreases while memorizing historical knowledge and conceptual confusion between the irrelevant old knowledge and the current task. Inspired by the complementary learning theory in neuroscience, we endow artificial neural networks with the ability to continuously learn without forgetting while recalling historical knowledge to facilitate learning new knowledge. Specifically, this work proposes a general framework named cycle memory networks (CMNs). The CMN consists of two individual memory networks to store short- and long-term memories separately to avoid capacity shrinkage and a transfer cell between them. It enables knowledge transfer from the long-term to the short-term memory network to mitigate conceptual confusion. In addition, the memory consolidation mechanism integrates short-term knowledge into the long-term memory network for knowledge accumulation. We demonstrate that the CMN can effectively address the anterograde forgetting on several task-related, task-conflict, class-incremental, and cross-domain benchmarks. Furthermore, we provide extensive ablation studies to verify each framework component. The source codes are available at: https://github.com/GeoX-Lab/CMN .},
  archive      = {J_TNNLS},
  author       = {Jian Peng and Dingqi Ye and Bo Tang and Yinjie Lei and Yu Liu and Haifeng Li},
  doi          = {10.1109/TNNLS.2023.3294495},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16439-16452},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Lifelong learning with cycle memory networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active client selection for clustered federated learning.
<em>TNNLS</em>, <em>35</em>(11), 16424–16438. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging distributed machine learning (ML) framework that operates under privacy and communication constraints. To mitigate the data heterogeneity underlying FL, clustered FL (CFL) was proposed to learn customized models for different client groups. However, due to the lack of effective client selection strategies, the CFL process is relatively slow, and the model performance is also limited in the presence of nonindependent and identically distributed (non-IID) client data. In this work, for the first time, we propose selecting participating clients for each cluster with active learning (AL) and call our method active client selection for CFL (ACFL). More specifically, in each ACFL round, each cluster filters out a small set of clients, which are the most informative clients according to some AL metrics [e.g., uncertainty sampling, query-by-committee (QBC), loss], and aggregates only its model updates to update the cluster-specific model. We empirically evaluate our ACFL approach on the public MNIST, CIFAR-10, and LEAF synthetic datasets with class-imbalanced settings. Compared with several FL and CFL baselines, the results reveal that ACFL can dramatically speed up the learning process while requiring less client participation and significantly improving model accuracy with a relatively low communication overhead.},
  archive      = {J_TNNLS},
  author       = {Honglan Huang and Wei Shi and Yanghe Feng and Chaoyue Niu and Guangquan Cheng and Jincai Huang and Zhong Liu},
  doi          = {10.1109/TNNLS.2023.3294295},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16424-16438},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Active client selection for clustered federated learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive region-based transformer for nonrigid medical
image registration with a self-constructing latent graph.
<em>TNNLS</em>, <em>35</em>(11), 16409–16423. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonrigid registration of medical images is formulated usually as an optimization problem with the aim of seeking out the deformation field between a referential–moving image pair. During the past several years, advances have been achieved in the convolutional neural network (CNN)-based registration of images, whose performance was superior to most conventional methods. More lately, the long-range spatial correlations in images have been learned by incorporating an attention-based model into the transformer network. However, medical images often contain plural regions with structures that vary in size. The majority of the CNN- and transformer-based approaches adopt embedding of patches that are identical in size, disallowing representation of the inter-regional structural disparities within an image. Besides, it probably leads to the structural and semantical inconsistencies of objects as well. To address this issue, we put forward an innovative module called region-based structural relevance embedding (RSRE), which allows adaptive embedding of an image into unequally-sized structural regions based on the similarity of self-constructing latent graph instead of utilizing patches that are identical in size. Additionally, a transformer is integrated with the proposed module to serve as an adaptive region-based transformer (ART) for registering medical images nonrigidly. As demonstrated by the experimental outcomes, our ART is superior to the advanced nonrigid registration approaches in performance, whose Dice score is 0.734 on the LPBA40 dataset with 0.318% foldings for deformation field, and is 0.873 on the ADNI dataset with 0.331% foldings.},
  archive      = {J_TNNLS},
  author       = {Sheng Lan and Xiu Li and Zhenhua Guo},
  doi          = {10.1109/TNNLS.2023.3294290},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16409-16423},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An adaptive region-based transformer for nonrigid medical image registration with a self-constructing latent graph},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining low-rank and deep plug-and-play priors for
snapshot compressive imaging. <em>TNNLS</em>, <em>35</em>(11),
16396–16408. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snapshot compressive imaging (SCI) is a promising technique that captures a 3-D hyperspectral image (HSI) by a 2-D detector in a compressed manner. The ill-posed inverse process of reconstructing the HSI from their corresponding 2-D measurements is challenging. However, current approaches either neglect the underlying characteristics, such as high spectral correlation, or demand abundant training datasets, resulting in an inadequate balance among performance, generalizability, and interpretability. To address these challenges, in this article, we propose a novel approach called LR2DP that integrates the model-driven low-rank prior and data-driven deep priors for SCI reconstruction. This approach not only captures the spectral correlation and deep spatial features of HSI but also takes advantage of both model-based and learning-based methods without requiring any extra training datasets. Specifically, to preserve the strong spectral correlation of the HSI effectively, we propose that the HSI lies in a low-rank subspace, thereby transforming the problem of reconstructing the HSI into estimating the spectral basis and spatial representation coefficient. Inspired by the mutual promotion of unsupervised deep image prior (DIP) and trained deep denoising prior (DDP), we integrate the unsupervised network and pre-trained deep denoiser into the plug-and-play (PnP) regime to estimate the representation coefficient together, aiming to explore the internal target image prior (learned by DIP) and the external training image prior (depicted by pre-trained DDP) of the HSI. An effective half-quadratic splitting (HQS) technique is employed to optimize the proposed HSI reconstruction model. Extensive experiments on both simulated and real datasets demonstrate the superiority of the proposed method over the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Yong Chen and Xinfeng Gui and Jinshan Zeng and Xi-Le Zhao and Wei He},
  doi          = {10.1109/TNNLS.2023.3294262},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16396-16408},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Combining low-rank and deep plug-and-play priors for snapshot compressive imaging},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive sparse gaussian process. <em>TNNLS</em>,
<em>35</em>(11), 16383–16395. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive learning is necessary for nonstationary environments where the learning machine needs to forget past data distribution. Efficient algorithms require a compact model update to not grow in computational burden with the incoming data and with the lowest possible computational cost for online parameter updating. Existing solutions only partially cover these needs. Here, we propose the first adaptive sparse Gaussian process (GP) able to address all these issues. We first reformulate a variational sparse GP (VSGP) algorithm to make it adaptive through a forgetting factor. Next, to make the model inference as simple as possible, we propose updating a single inducing point of the SGP model together with the remaining model parameters every time a new sample arrives. As a result, the algorithm presents a fast convergence of the inference process, which allows an efficient model update (with a single inference iteration) even in highly nonstationary environments. Experimental results demonstrate the capabilities of the proposed algorithm and its good performance in modeling the predictive posterior in mean and confidence interval estimation compared to state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Vanessa Gómez-Verdejo and Emilio Parrado-Hernández and Manel Martínez-Ramón},
  doi          = {10.1109/TNNLS.2023.3294089},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16383-16395},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive sparse gaussian process},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). CTFNet: Long-sequence time-series forecasting based on
convolution and time–frequency analysis. <em>TNNLS</em>,
<em>35</em>(11), 16368–16382. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although current time-series forecasting methods have significantly improved the state-of-the-art (SOTA) results for long-sequence time-series forecasting (LSTF), they still have difficulty in capturing and extracting the features and dependencies of long-term sequences and suffer from information utilization bottlenecks and high-computational complexity. To address these issues, a lightweight single-hidden layer feedforward neural network (SLFN) combining convolution mapping and time–frequency decomposition called CTFNet is proposed with three distinctive characteristics. First, time-domain (TD) feature mining—in this article, a method for extracting the long-term correlation of horizontal TD features based on matrix factorization is proposed, which can effectively capture the interdependence among different sample points of a long time series. Second, multitask frequency-domain (FD) feature mining—this can effectively extract different frequency feature information of time-series data from the FD and minimize the loss of data features. Integrating multiscale dilated convolutions, simultaneously focusing on both global and local context feature dependencies at the sequence level, and mining the long-term dependencies of the multiscale frequency information and the spatial dependencies among the different scale frequency information, break the bottleneck of data utilization, and ensure the integrity of feature extraction. Third, highly efficient—the CTFNet model has a short training time and fast inference speed. Our empirical studies with nine benchmark datasets show that compared with state-of-the-art methods, CTFNet can reduce prediction error by 64.7% and 53.7% for multivariate and univariate time series, respectively.},
  archive      = {J_TNNLS},
  author       = {Zhiqiang Zhang and Yuxuan Chen and Dandan Zhang and Yining Qian and Hongbing Wang},
  doi          = {10.1109/TNNLS.2023.3294064},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16368-16382},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CTFNet: Long-sequence time-series forecasting based on convolution and Time–Frequency analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fractal few-shot learning. <em>TNNLS</em>, <em>35</em>(11),
16353–16367. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forming deep feature embeddings is an effective method for few-shot learning (FSL). However, in the case of insufficient samples, overcoming the task complexity while improving the accuracy is still a major challenge. To address this problem, this article considers the consistency between similar data from the fractal perspective, introduces a priori knowledge, and proposes a fractal embedding model by combining FSL with fractal dimension theory for the first time. We improve the original fractal dimension algorithm used to describe image texture roughness to suit a neural network. Moreover, in accordance with the improved algorithm, prior knowledge of the quantized image is integrated into the features to reduce the impact of the data distribution on the model. Experimental results obtained on multiple image benchmark datasets show that the performance of the proposed model exceeds or matches that of previous state-of-the-art models. In addition, the proposed model achieves the best performance in cross-domain scenarios, further illustrating its robustness.},
  archive      = {J_TNNLS},
  author       = {Fobao Zhou and Wenkai Huang},
  doi          = {10.1109/TNNLS.2023.3293995},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16353-16367},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fractal few-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Support matrix machine via joint ℓ2,1 and nuclear norm
minimization under matrix completion framework for classification of
corrupted data. <em>TNNLS</em>, <em>35</em>(11), 16341–16352. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional support vector machines (SVMs) are fragile in the presence of outliers; even a single corrupt data point can arbitrarily alter the quality of the approximation. If even a small fraction of columns is corrupted, then classification performance will inevitably deteriorate. This article considers the problem of high-dimensional data classification, where a number of the columns are arbitrarily corrupted. An efficient Support Matrix Machine that simultaneously performs matrix Recovery (SSMRe) is proposed, i.e. feature selection and classification through joint minimization of $\ell _{2,1}$ (the nuclear norm of $L$ ). The data are assumed to consist of a low-rank clean matrix plus a sparse noisy matrix. SSMRe works under incoherence and ambiguity conditions and is able to recover an intrinsic matrix of higher rank in the presence of data densely corrupted. The objective function is a spectral extension of the conventional elastic net; it combines the property of matrix recovery along with low rank and joint sparsity to deal with complex high-dimensional noisy data. Furthermore, SSMRe leverages structural information, as well as the intrinsic structure of data, avoiding the inevitable upper bound. Experimental results on different real-time applications, supported by the theoretical analysis and statistical testing, show significant gain for BCI, face recognition, and person identification datasets, especially in the presence of outliers, while preserving a reasonable number of support vectors.},
  archive      = {J_TNNLS},
  author       = {Imran Razzak and Mohamed Reda Bouadjenek and Raghib Abu Saris and Weiping Ding},
  doi          = {10.1109/TNNLS.2023.3293888},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16341-16352},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Support matrix machine via joint ℓ2,1 and nuclear norm minimization under matrix completion framework for classification of corrupted data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoupling long- and short-term patterns in spatiotemporal
inference. <em>TNNLS</em>, <em>35</em>(11), 16328–16340. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensors are the key to environmental monitoring, which impart benefits to smart cities in many aspects, such as providing real-time air quality information to assist human decision-making. However, it is impractical to deploy massive sensors due to the expensive costs, resulting in sparse data collection. Therefore, how to get fine-grained data measurement has long been a pressing issue. In this article, we aim to infer values at nonsensor locations based on observations from available sensors (termed spatiotemporal inference), where capturing spatiotemporal relationships among the data plays a critical role. Our investigations reveal two significant insights that have not been explored by previous works. First, data exhibit distinct patterns at both long- and short-term temporal scales, which should be analyzed separately. Second, short-term patterns contain more delicate relations, including those across spatial and temporal dimensions simultaneously, while long-term patterns involve high-level temporal trends. Based on these observations, we propose to decouple the modeling of short- and long-term patterns. Specifically, we introduce a joint spatiotemporal graph attention network to learn the relations across space and time for short-term patterns. Furthermore, we propose a graph recurrent network with a time skip strategy to alleviate the gradient vanishing problem and model the long-term dependencies. Experimental results on four public real-world datasets demonstrate that our method effectively captures both long- and short-term relations, achieving state-of-the-art performance against existing methods.},
  archive      = {J_TNNLS},
  author       = {Junfeng Hu and Yuxuan Liang and Zhencheng Fan and Li Liu and Yifang Yin and Roger Zimmermann},
  doi          = {10.1109/TNNLS.2023.3293814},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16328-16340},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decoupling long- and short-term patterns in spatiotemporal inference},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRASS: Learning spatial–temporal properties from chainlike
cascade data for microscopic diffusion prediction. <em>TNNLS</em>,
<em>35</em>(11), 16313–16327. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information diffusion prediction captures diffusion dynamics of online messages in social networks. Thus, it is the basis of many essential tasks such as popularity prediction and viral marketing. However, there are two thorny problems caused by the loss of spatial–temporal properties of cascade data: “position-hopping” and “branch-independency.” The former means no exact propagation relationship between any two consecutive infected users. The latter indicates that not all previously infected users contribute to the prediction of the next infected user. This article proposes the GR U-like A ttention Unit and S tructural S preading (GRASS) model for microscopic cascade prediction to overcome the above two problems. First, we introduce the attention mechanism into the gated recurrent unit (GRU) component to expand the restricted receptive field of the recurrent neural network (RNN)-type module, thus addressing the “position-hopping” problem. Second, the structural spreading (SS) mechanism leverages structural features to filter out related users and controls the generation of cascade hidden states, thereby solving the “branch-independency” problem. Experiments on multiple real-world datasets show that our model significantly outperforms state-of-the-art baseline models on both ${\text{ hits@}}\kappa $ and ${\text{ map@}}\kappa $ metrics. Furthermore, the visualization of latent representations by t-distributed stochastic neighbor embedding (t-SNE) indicates that our model makes different cascades more discriminative during the encoding process.},
  archive      = {J_TNNLS},
  author       = {Huacheng Li and Chunhe Xia and Tianbo Wang and Zhao Wang and Peng Cui and Xiaojian Li},
  doi          = {10.1109/TNNLS.2023.3293689},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16313-16327},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GRASS: Learning Spatial–Temporal properties from chainlike cascade data for microscopic diffusion prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised spatiotemporal clustering of vehicle
emissions with graph convolutional network. <em>TNNLS</em>,
<em>35</em>(11), 16301–16312. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal clustering of vehicle emissions, which reveals the evolution pattern of air pollution from road traffic, is a challenging representation learning task due to the lack of supervision. Some recent work building upon graph convolutional network (GCN) models the intrinsic spatiotemporal correlations among the nodes in road networks as graph representations for clustering. However, these existing methods ignore the interactions between spatial and temporal variations in vehicle emissions, resulting in incomplete descriptions and inaccurate detection of the evolution pattern of air pollution. To address this issue, this article proposes a two-way self-supervised spatiotemporal representation learning scheme, in which the temporal and spatial features are progressively learned in a mutually reinforced manner. Our proposed method is based on the observation that though the variation in vehicle emissions in the road network is consistent in the spatial and temporal domains, its expression is more distinct in temporal sequences. To this end, the input emission data are first projected into an initial temporal representation space spanned by the captured features from a pretrained BiLSTM network. Then the generated distribution of temporal features is used to construct an objective constraint for high-purity clustering through a two-way self-supervised mechanism, which is leveraged as a constraint for the feature clustering of a GCN. Furthermore, to eliminate the initial errors, a joint optimization scheme is presented to generate the decoupled clustering results through the progressive refinement of representation and clustering. Our proposed method is evaluated on the traffic emission dataset of Xian city in 2020, and the experimental results have demonstrated the superiority against the state-of-the-art.},
  archive      = {J_TNNLS},
  author       = {Lihong Pei and Yang Cao and Yu Kang and Zhenyi Xu and Zhenyi Zhao},
  doi          = {10.1109/TNNLS.2023.3293463},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16301-16312},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised spatiotemporal clustering of vehicle emissions with graph convolutional network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hundreds guide millions: Adaptive offline reinforcement
learning with expert guidance. <em>TNNLS</em>, <em>35</em>(11),
16288–16300. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline reinforcement learning (RL) optimizes the policy on a previously collected dataset without any interactions with the environment, yet usually suffers from the distributional shift problem. To mitigate this issue, a typical solution is to impose a policy constraint on a policy improvement objective. However, existing methods generally adopt a “one-size-fits-all” practice, i.e., keeping only a single improvement-constraint balance for all the samples in a mini-batch or even the entire offline dataset. In this work, we argue that different samples should be treated with different policy constraint intensities. Based on this idea, a novel plug-in approach named guided offline RL (GORL) is proposed. GORL employs a guiding network, along with only a few expert demonstrations, to adaptively determine the relative importance of the policy improvement and policy constraint for every sample. We theoretically prove that the guidance provided by our method is rational and near-optimal. Extensive experiments on various environments suggest that GORL can be easily installed on most offline RL algorithms with statistically significant performance improvements.},
  archive      = {J_TNNLS},
  author       = {Qisen Yang and Shenzhi Wang and Qihang Zhang and Gao Huang and Shiji Song},
  doi          = {10.1109/TNNLS.2023.3293508},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16288-16300},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hundreds guide millions: Adaptive offline reinforcement learning with expert guidance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-grained visual–text prompt-driven self-training for
open-vocabulary object detection. <em>TNNLS</em>, <em>35</em>(11),
16277–16287. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the success of vision–language methods (VLMs) in zero-shot classification, recent works attempt to extend this line of work into object detection by leveraging the localization ability of pretrained VLMs and generating pseudolabels for unseen classes in a self-training manner. However, since the current VLMs are usually pretrained with aligning sentence embedding with global image embedding, the direct use of them lacks fine-grained alignment for object instances, which is the core of detection. In this article, we propose a simple but effective fine-grained visual-text prompt-driven self-training paradigm for open-vocabulary detection (VTP-OVD) that introduces a fine-grained visual–text prompt adapting stage to enhance the current self-training paradigm with a more powerful fine-grained alignment. During the adapting stage, we enable VLM to obtain fine-grained alignment using learnable text prompts to resolve an auxiliary dense pixelwise prediction task. Furthermore, we propose a visual prompt module to provide the prior task information (i.e., the categories need to be predicted) for the vision branch to better adapt the pretrained VLM to the downstream tasks. Experiments show that our method achieves the state-of-the-art performance for open-vocabulary object detection, e.g., 31.5% mAP on unseen classes of COCO.},
  archive      = {J_TNNLS},
  author       = {Yanxin Long and Jianhua Han and Runhui Huang and Hang Xu and Yi Zhu and Chunjing Xu and Xiaodan Liang},
  doi          = {10.1109/TNNLS.2023.3293484},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16277-16287},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fine-grained Visual–Text prompt-driven self-training for open-vocabulary object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eigenimage2Eigenimage (E2E): A self-supervised deep learning
network for hyperspectral image denoising. <em>TNNLS</em>,
<em>35</em>(11), 16262–16276. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of deep learning-based denoisers highly depends on the quantity and quality of training data. However, paired noisy–clean training images are generally unavailable in hyperspectral remote sensing areas. To solve this problem, this work resorts to the self-supervised learning technique, where our proposed model can train itself to learn one part of noisy input from another part of noisy input. We study a general hyperspectral image (HSI) denoising framework, called Eigenimage2Eigenimage (E2E), which turns the HSI denoising problem into an eigenimage (i.e., the subspace representation coefficients of the HSI) denoising problem and proposes a learning strategy to generate noisy–noisy paired training eigenimages from noisy eigenimages. Consequently, the E2E denoising framework can be trained without clean data and applied to denoise HSIs without the constraint with the number of frequency bands. Experimental results are provided to demonstrate the performance of the proposed method that is better than the other existing deep learning methods for denoising HSIs. A MATLAB demo of this work is available at https://github.com/LinaZhuang/HSI-denoiser-Eigenimage2Eigenimage for the sake of reproducibility.},
  archive      = {J_TNNLS},
  author       = {Lina Zhuang and Michael K. Ng and Lianru Gao and Joseph Michalski and Zhicheng Wang},
  doi          = {10.1109/TNNLS.2023.3293328},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16262-16276},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Eigenimage2Eigenimage (E2E): A self-supervised deep learning network for hyperspectral image denoising},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). GeSeNet: A general semantic-guided network with couple mask
ensemble for medical image fusion. <em>TNNLS</em>, <em>35</em>(11),
16248–16261. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, multimodal medical image fusion technology has become an essential means for researchers and doctors to predict diseases and study pathology. Nevertheless, how to reserve more unique features from different modal source images on the premise of ensuring time efficiency is a tricky problem. To handle this issue, we propose a flexible semantic-guided architecture with a mask-optimized framework in an end-to-end manner, termed as GeSeNet. Specifically, a region mask module is devised to deepen the learning of important information while pruning redundant computation for reducing the runtime. An edge enhancement module and a global refinement module are presented to modify the extracted features for boosting the edge textures and adjusting overall visual performance. In addition, we introduce a semantic module that is cascaded with the proposed fusion network to deliver semantic information into our generated results. Sufficient qualitative and quantitative comparative experiments (i.e., MRI-CT, MRI-PET, and MRI-SPECT) are deployed between our proposed method and ten state-of-the-art methods, which shows our generated images lead the way. Moreover, we also conduct operational efficiency comparisons and ablation experiments to prove that our proposed method can perform excellently in the field of multimodal medical image fusion. The code is available at https://github.com/lok-18/GeSeNet .},
  archive      = {J_TNNLS},
  author       = {Jiawei Li and Jinyuan Liu and Shihua Zhou and Qiang Zhang and Nikola K. Kasabov},
  doi          = {10.1109/TNNLS.2023.3293274},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16248-16261},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GeSeNet: A general semantic-guided network with couple mask ensemble for medical image fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Copula variational LSTM for high-dimensional cross-market
multivariate dependence modeling. <em>TNNLS</em>, <em>35</em>(11),
16233–16247. (<a
href="https://doi.org/10.1109/TNNLS.2023.3293131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address a challenging problem—modeling high-dimensional, long-range dependencies between nonnormal multivariates, which is important for demanding applications such as cross-market modeling (CMM). With heterogeneous indicators and markets, CMM aims to capture between-market financial couplings and influence over time and within-market interactions between financial variables. We make the first attempt to integrate deep variational sequential learning with copula-based statistical dependence modeling and characterize both temporal dependence degrees and structures between hidden variables representing nonnormal multivariates. Our copula variational learning network weighted partial regular vine copula-based variational long short-term memory (WPVC-VLSTM) integrates variational long short-term memory (LSTM) networks and regular vine copula to model variational sequential dependence degrees and structures. The regular vine copula models nonnormal distributional dependence degrees and structures. VLSTM captures variational long-range dependencies coupling high-dimensional dynamic hidden variables without strong hypotheses and multivariate constraints. WPVC-VLSTM outperforms benchmarks, including linear models, stochastic volatility models, deep neural networks, and variational recurrent networks in terms of both technical significance and portfolio forecasting performance. WPVC-VLSTM shows a step-forward for CMM and deep variational learning.},
  archive      = {J_TNNLS},
  author       = {Jia Xu and Longbing Cao},
  doi          = {10.1109/TNNLS.2023.3293131},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16233-16247},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Copula variational LSTM for high-dimensional cross-market multivariate dependence modeling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STEdge: Self-training edge detection with multilayer
teaching and regularization. <em>TNNLS</em>, <em>35</em>(11),
16222–16232. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based edge detection has hereunto been strongly supervised with pixel-wise annotations which are tedious to obtain manually. We study the problem of self-training edge detection, leveraging the untapped wealth of large-scale unlabeled image datasets. We design a self-supervised framework with multilayer regularization and self-teaching. In particular, we impose a consistency regularization which enforces the outputs from each of the multiple layers to be consistent for the input image and its perturbed counterpart. We adopt L0-smoothing as the “perturbation” to encourage edge prediction lying on salient boundaries following the cluster assumption in self-supervised learning. Meanwhile, the network is trained with multilayer supervision by pseudo labels which are initialized with Canny edges and then iteratively refined by the network as the training proceeds. The regularization and self-teaching together attain a good balance of precision and recall, leading to a significant performance boost over supervised methods, with lightweight refinement on the target dataset. Through extensive experiments, our method demonstrates strong cross-dataset generality and can improve the original performance of edge detectors after self-training and fine-tuning.},
  archive      = {J_TNNLS},
  author       = {Yunfan Ye and Renjiao Yi and Zhiping Cai and Kai Xu},
  doi          = {10.1109/TNNLS.2023.3292905},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16222-16232},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {STEdge: Self-training edge detection with multilayer teaching and regularization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A dynamic weights-based wavelet attention neural network
for defect detection. <em>TNNLS</em>, <em>35</em>(11), 16211–16221. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic defect detection plays an important role in industrial production. Deep learning-based defect detection methods have achieved promising results. However, there are still two challenges in the current defect detection methods: 1) high-precision detection of weak defects is limited and 2) it is difficult for current defect detection methods to achieve satisfactory results dealing with strong background noise. This article proposes a dynamic weights-based wavelet attention neural network (DWWA-Net) to address these issues, which can enhance the feature representation of defects and simultaneously denoise the image, thereby improving the detection accuracy of weak defects and defects under strong background noise. First, wavelet neural networks and dynamic wavelet convolution networks (DWCNets) are presented, which can effectively filter background noise and improve model convergence. Second, a multiview attention module is designed, which can direct the network attention toward potential targets, thereby guaranteeing the accuracy for detecting weak defects. Finally, a feature feedback module is proposed, which can enhance the feature information of defects to further improve the weak defect detection accuracy. The DWWA-Net can be used for defect detection in multiple industrial fields. Experiment results illustrate that the proposed method outperforms the state-of-the-art methods (mean precision: GC10-DET: 6.0%; NEU: 4.3%). The code is made in https://github.com/781458112/DWWA .},
  archive      = {J_TNNLS},
  author       = {Jinhai Liu and He Zhao and Zhaolin Chen and Qiannan Wang and Xiangkai Shen and Huaguang Zhang},
  doi          = {10.1109/TNNLS.2023.3292512},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16211-16221},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A dynamic weights-based wavelet attention neural network for defect detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overcoming catastrophic forgetting in continual learning by
exploring eigenvalues of hessian matrix. <em>TNNLS</em>,
<em>35</em>(11), 16196–16210. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks tend to suffer performance deterioration on previous tasks when they are applied to multiple tasks sequentially without access to previous data. The problem is commonly known as catastrophic forgetting, a significant challenge in continual learning (CL). To overcome the catastrophic forgetting, regularization-based CL methods construct a regularization-based term, which can be considered as the approximation loss function of previous tasks, to penalize the update of parameters. However, the rigorous theoretical analysis of regularization-based methods is limited. Therefore, we theoretically analyze the forgetting and the convergence properties of regularization-based methods. The theoretical results demonstrate that the upper bound of the forgetting has a relationship with the maximum eigenvalue of the Hessian matrix. Hence, to decrease the upper bound of the forgetting, we propose eiGenvalues ExplorAtion Regularization-based (GEAR) method, which explores the geometric properties of the approximation loss of prior tasks regarding the maximum eigenvalue. Extensive experimental results demonstrate that our method mitigates catastrophic forgetting and outperforms existing regularization-based methods.},
  archive      = {J_TNNLS},
  author       = {Yajing Kong and Liu Liu and Huanhuan Chen and Janusz Kacprzyk and Dacheng Tao},
  doi          = {10.1109/TNNLS.2023.3292359},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16196-16210},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Overcoming catastrophic forgetting in continual learning by exploring eigenvalues of hessian matrix},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Granger causal inference based on dual laplacian
distribution and its application to MI-BCI classification.
<em>TNNLS</em>, <em>35</em>(11), 16181–16195. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Granger causality-based effective brain connectivity provides a powerful tool to probe the neural mechanism for information processing and the potential features for brain computer interfaces. However, in real applications, traditional Granger causality is prone to the influence of outliers, such as inevitable ocular artifacts, resulting in unreasonable brain linkages and the failure to decipher inherent cognition states. In this work, motivated by constructing the sparse causality brain networks under the strong physiological outlier noise conditions, we proposed a dual Laplacian Granger causality analysis (DLap-GCA) by imposing Laplacian distributions on both model parameters and residuals. In essence, the first Laplacian assumption on residuals will resist the influence of outliers in electroencephalogram (EEG) on causality inference, and the second Laplacian assumption on model parameters will sparsely characterize the intrinsic interactions among multiple brain regions. Through simulation study, we quantitatively verified its effectiveness in suppressing the influence of complex outliers, the stable capacity for model estimation, and sparse network inference. The application to motor-imagery (MI) EEG further reveals that our method can effectively capture the inherent hemispheric lateralization of MI tasks with sparse patterns even under strong noise conditions. The MI classification based on the network features derived from the proposed approach shows higher accuracy than other existing traditional approaches, which is attributed to the discriminative network structures being captured in a timely manner by DLap-GCA even under the single-trial online condition. Basically, these results consistently show its robustness to the influence of complex outliers and the capability of characterizing representative brain networks for cognition information processing, which has the potential to offer reliable network structures for both cognitive studies and future brain–computer interface (BCI) realization.},
  archive      = {J_TNNLS},
  author       = {Peiyang Li and Xiaohui Gao and Cunbo Li and Chanlin Yi and Weijie Huang and Yajing Si and Fali Li and Zehong Cao and Yin Tian and Peng Xu},
  doi          = {10.1109/TNNLS.2023.3292179},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16181-16195},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Granger causal inference based on dual laplacian distribution and its application to MI-BCI classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Dynamic event-driven finite-horizon optimal consensus
control for constrained multiagent systems. <em>TNNLS</em>,
<em>35</em>(11), 16167–16180. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the event-driven finite-horizon optimal consensus control problem for multiagent systems with symmetric or asymmetric input constraints. Initially, in order to overcome the difficulty that the Hamilton–Jacobi–Bellman equation is time-varying in finite-horizon optimal control, a single critic neural network (NN) with time-varying activation function is applied to obtain the approximate optimal control. Meanwhile, for minimizing the terminal error to satisfy the terminal constraint of the value function, an augmented error vector containing the Bellman residual and the terminal error is constructed to update the weight of the NN. Furthermore, an improved learning law is proposed, which relaxes the tricky persistence excitation condition and eliminates the requirement of initial stability control. Moreover, a specific algorithm is designed to update the historical dataset, which can effectively accelerate the convergence rate of network weight. In addition, to improve the utilization rate of the communication resource, an effective dynamic event-triggering mechanism (DETM) composed of dynamic threshold parameters (DTPs) and auxiliary dynamic variables (ADVs) is designed, which is more flexible compared with the ADV-based DETM or DTP-based DETM. Finally, to support the effectiveness of the proposed method and the superiority of the designed DETM, a simulation example is provided.},
  archive      = {J_TNNLS},
  author       = {Lijie Wang and Jiahong Xu and Yang Liu and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2023.3292154},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16167-16180},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic event-driven finite-horizon optimal consensus control for constrained multiagent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Three-stage global channel pruning for resources-limited
platform. <em>TNNLS</em>, <em>35</em>(11), 16153–16166. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have demonstrated remarkable performance in many fields, and deploying them on resource-limited devices has drawn more and more attention in industry and academia. Typically, there are great challenges for intelligent networked vehicles and drones to deploy object detection tasks due to the limited memory and computing power of embedded devices. To meet these challenges, hardware-friendly model compression approaches are required to reduce model parameters and computation. Three-stage global channel pruning, which involves sparsity training, channel pruning, and fine-tuning, is very popular in the field of model compression for its hardware-friendly structural pruning and ease of implementation. However, existing methods suffer from problems such as uneven sparsity, damage to the network structure, and reduced pruning ratio due to channel protection. To solve these issues, the present article makes the following significant contributions. First, we present an element-level heatmap-guided sparsity training method to achieve even sparsity, resulting in higher pruning ratio and improved performance. Second, we propose a global channel pruning method that fuses both global and local channel importance metrics to identify unimportant channels for pruning. Third, we present a channel replacement policy (CRP) to protect layers, ensuring that the pruning ratio can be guaranteed even under high pruning rate conditions. Evaluations show that our proposed method significantly outperforms the state-of-the-art (SOTA) methods in terms of pruning efficiency, making it more suitable for deployment on resource-limited devices.},
  archive      = {J_TNNLS},
  author       = {Yijie Chen and Rui Li and Wanli Li and Jilong Wang and Renfa Li},
  doi          = {10.1109/TNNLS.2023.3292152},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16153-16166},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Three-stage global channel pruning for resources-limited platform},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Valid RBFNN adaptive control for nonlinear systems with
unmatched uncertainties. <em>TNNLS</em>, <em>35</em>(11), 16139–16152.
(<a href="https://doi.org/10.1109/TNNLS.2023.3292115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive tracking controller based on radial basis function neural networks (RBFNNs) is proposed for nonlinear plants with unmatched uncertainties and smooth reference signals. The concept of valid RBFNN adaptive control is introduced where all closed-loop arguments of the involved RBFNNs should always remain inside their corresponding compact sets. Considering the local approximation capacity of RBFNNs, validity requirements are necessary for ensuring reliable closed-loop approximation accuracy and stability. To obtain valid RBFNN adaptive controllers, a novel iterative design method is proposed and embedded into the traditional backstepping approach. In the initial iteration, an ideal RBFNN and its online estimated version are introduced in each step where the initial compact set guarantees the validity requirement for only a finite time interval. Then, by carefully investigating the dependence among different signals and introducing some auxiliary variables, the compact sets are redesigned for prolonging the time interval satisfying validity requirements to infinity as the iteration goes on. Consequently, a closed-loop system model can be formulated during the entire control process, which underlies a rigorous proof on closed-loop stability and some guidelines on practical implementation. Meanwhile, rigorous analysis from validity requirements reveals, for the first time, a new feature of RBFNN adaptive controllers in the presence of unmatched uncertainties: excessively large scales of RBFNNs in intermediate steps may impair the closed-loop performance. Finally, simulation results are provided to illustrate the efficiency and feasibility of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Hao Yu and Tongwen Chen},
  doi          = {10.1109/TNNLS.2023.3292115},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16139-16152},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Valid RBFNN adaptive control for nonlinear systems with unmatched uncertainties},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised time series representation learning via
cross reconstruction transformer. <em>TNNLS</em>, <em>35</em>(11),
16129–16138. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since labeled samples are typically scarce in real-world scenarios, self-supervised representation learning in time series is critical. Existing approaches mainly employ the contrastive learning framework, which automatically learns to understand similar and dissimilar data pairs. However, they are constrained by the request for cumbersome sampling policies and prior knowledge of constructing pairs. Also, few works have focused on effectively modeling temporal-spectral correlations to improve the capacity of representations. In this article, we propose the cross reconstruction transformer (CRT) to solve the aforementioned issues. CRT achieves time series representation learning through a cross-domain dropping-reconstruction task. Specifically, we obtain the frequency domain of the time series via the fast Fourier transform (FFT) and randomly drop certain patches in both time and frequency domains. Dropping is employed to maximally preserve the global context while masking leads to the distribution shift. Then a Transformer architecture is utilized to adequately discover the cross-domain correlations between temporal and spectral information through reconstructing data in both domains, which is called Dropped Temporal-Spectral Modeling. To discriminate the representations in global latent space, we propose instance discrimination constraint (IDC) to reduce the mutual information between different time series samples and sharpen the decision boundaries. Additionally, a specified curriculum learning (CL) strategy is employed to improve the robustness during the pretraining phase, which progressively increases the dropping ratio in the training process. We conduct extensive experiments to evaluate the effectiveness of the proposed method on multiple real-world datasets. Results show that CRT consistently achieves the best performance over existing methods by 2%–9%. The code is publicly available at https://github.com/BobZwr/Cross-Reconstruction-Transformer .},
  archive      = {J_TNNLS},
  author       = {Wenrui Zhang and Ling Yang and Shijia Geng and Shenda Hong},
  doi          = {10.1109/TNNLS.2023.3292066},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16129-16138},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised time series representation learning via cross reconstruction transformer},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-distillation for randomized neural networks.
<em>TNNLS</em>, <em>35</em>(11), 16119–16128. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) is a conventional method in the field of deep learning that enables the transfer of dark knowledge from a teacher model to a student model, consequently improving the performance of the student model. In randomized neural networks, due to the simple topology of network architecture and the insignificant relationship between model performance and model size, KD is not able to improve model performance. In this work, we propose a self-distillation pipeline for randomized neural networks: the predictions of the network itself are regarded as the additional target, which are mixed with the weighted original target as a distillation target containing dark knowledge to supervise the training of the model. All the predictions during multi-generation self-distillation process can be integrated by a multi-teacher method. By induction, we have additionally arrived at the methods for infinite self-distillation (ISD) of randomized neural networks. We then provide relevant theoretical analysis about the self-distillation method for randomized neural networks. Furthermore, we demonstrated the effectiveness of the proposed method in practical applications on several benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Minghui Hu and Ruobin Gao and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1109/TNNLS.2023.3292063},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16119-16128},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-distillation for randomized neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate complementarity learning for graph-based multiview
clustering. <em>TNNLS</em>, <em>35</em>(11), 16106–16118. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real scenarios, graph-based multiview clustering has clearly shown popularity owing to the high efficiency in fusing the information from multiple views. Practically, the multiview graphs offer both consistent and inconsistent cues as they usually come from heterogeneous sources. Previous methods illustrated the importance of leveraging the multiview consistency and inconsistency for accurate modeling. However, when fusing the graphs, the inconsistent parts are generally ignored and hence the valued view-specific attributes are lost. To solve this problem, we propose an accurate complementarity learning (ACL) model for graph-based multiview clustering. ACL clearly distinguishes the consistent, complementary, and noise and corruption terms from the initial multiview graphs. In contrast to existing models that overlooked the complementary information, we argue that the view-specific characteristics extracted from the complementary terms are beneficial for affinity learning. In addition, ACL exploits only the positive parts of the complementary information for preserving the evidence on the positive sample relationship, and ignores the negative cues to avoid the vanishing of effective affinity strengths. This way, the learned affinity matrix is able to properly balance the consistent and complementary information. To solve the ACL model, we introduce an efficient alternating optimization algorithm with a varying penalty parameter. Experiments on synthetic and real-world databases clearly demonstrated the superiority of ACL.},
  archive      = {J_TNNLS},
  author       = {Xiaolin Xiao and Yue-Jiao Gong},
  doi          = {10.1109/TNNLS.2023.3292057},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16106-16118},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accurate complementarity learning for graph-based multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Select, purify, and exchange: A multisource unsupervised
domain adaptation method for building extraction. <em>TNNLS</em>,
<em>35</em>(11), 16091–16105. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately extracting buildings from aerial images has essential research significance for timely understanding human intervention on the land. The distribution discrepancies between diversified unlabeled remote sensing images (changes in imaging sensor, location, and environment) and labeled historical images significantly degrade the generalization performance of deep learning algorithms. Unsupervised domain adaptation (UDA) algorithms have recently been proposed to eliminate the distribution discrepancies without re-annotating training data for new domains. Nevertheless, due to the limited information provided by a single-source domain, single-source UDA (SSUDA) is not an optimal choice when multitemporal and multiregion remote sensing images are available. We propose a multisource UDA (MSUDA) framework SPENet for building extraction, aiming at selecting, purifying, and exchanging information from multisource domains to better adapt the model to the target domain. Specifically, the framework effectively utilizes richer knowledge by extracting target-relevant information from multiple-source domains, purifying target domain information with low-level features of buildings, and exchanging target domain information in an interactive learning manner. Extensive experiments and ablation studies constructed on 12 city datasets prove the effectiveness of our method against existing state-of-the-art methods, e.g., our method achieves 59.1% intersection over union (IoU) on Austin and Kitsap $\longrightarrow $ Potsdam, which surpasses the target domain supervised method by 2.2%. The code is available at https://github.com/QZangXDU/SPENet .},
  archive      = {J_TNNLS},
  author       = {Shuang Wang and Qi Zang and Dong Zhao and Chaowei Fang and Dou Quan and Yutong Wan and Yanhe Guo and Licheng Jiao},
  doi          = {10.1109/TNNLS.2023.3291876},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16091-16105},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Select, purify, and exchange: A multisource unsupervised domain adaptation method for building extraction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible label-induced manifold broad learning system for
multiclass recognition. <em>TNNLS</em>, <em>35</em>(11), 16076–16090.
(<a href="https://doi.org/10.1109/TNNLS.2023.3291793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broad learning system (BLS), which emerges as a lightweight network paradigm, has recently attracted great attention for recognition problems due to its good balance between efficiency and accuracy. However, the supervision mechanism in BLS and its variants generally relies on the strict binary label matrix, which imposes limitations on approximation and fails to adequately align with the data distribution. To address this issue, in this article, two novel flexible label-induced BLS models with the manifold manner are proposed, whose notable characteristics are as follows. First, two proposed label relaxation strategies can both enlarge the margins between different categories and simultaneously enhance the diversity within labels. Second, the integration of manifold geometrical criterion enables the models to capture local feature structures, ensuring the obtained flexible labels align better with the similarity between samples. Third, the proposed models can be optimized efficiently with the alternating direction method of multipliers. Each iteration benefits from a closed-form solution, facilitating the optimization process. Extensive experiments and thorough theoretical analysis are intended to show the advantages of our proposed models compared to other state-of-the-art recognition algorithms.},
  archive      = {J_TNNLS},
  author       = {Junwei Jin and Biao Geng and Yanting Li and Jing Liang and Yang Xiao and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2023.3291793},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16076-16090},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Flexible label-induced manifold broad learning system for multiclass recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust symbol detection based on quaternion neural networks
in wireless polarization-shift-keying communications. <em>TNNLS</em>,
<em>35</em>(11), 16064–16075. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quaternion neural networks (QNNs) form a class of neural networks constructed with quaternion numbers. They are suitable for processing 3-D features with fewer trainable free parameters than real-valued neural networks (RVNNs). This article proposes symbol detection in wireless polarization-shift-keying (PolSK) communications by employing QNNs. We demonstrate that quaternion plays a crucial role in the symbol detection of PolSK signals. Existing artificial-intelligence communication studies mainly focus on RVNN-based symbol detection in digital modulations having constellations in complex plane. However, in PolSK, information symbols are represented as the state of polarization, which can be mapped on the Poincare sphere and thus its symbols have a 3-D data structure. Quaternion algebra offers a unified representation to process 3-D data with rotational invariance and, therefore, it keeps the internal relationship among three components of a PolSK symbol. Hence, we can expect that QNNs learn the distribution of received symbols on the Poincare sphere with higher consistency to detect the transmitted symbols more efficiently than RVNNs. We compare PolSK symbol detection accuracy of two types of QNNs, RVNN, existing methods such as least-square and minimum-mean-square-error channel estimations, as well as detection knowing perfect channel state information (CSI). Simulation results including symbol error rate show that the proposed QNNs outperform the existing estimation methods and that they reach better results with two to three times fewer free parameters than the RVNN. We find that QNN processing will bring practical use of PolSK communications.},
  archive      = {J_TNNLS},
  author       = {Haotian Chen and Ryo Natsuaki and Akira Hirose},
  doi          = {10.1109/TNNLS.2023.3291702},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16064-16075},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust symbol detection based on quaternion neural networks in wireless polarization-shift-keying communications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unpaired multi-view graph clustering with cross-view
structure matching. <em>TNNLS</em>, <em>35</em>(11), 16049–16063. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC), which effectively fuses information from multiple views for better performance, has received increasing attention. Most existing MVC methods assume that multi-view data are fully paired, which means that the mappings of all corresponding samples between views are predefined or given in advance. However, the data correspondence is often incomplete in real-world applications due to data corruption or sensor differences, referred to as the data-unpaired problem (DUP) in multi-view literature. Although several attempts have been made to address the DUP issue, they suffer from the following drawbacks: 1) most methods focus on the feature representation while ignoring the structural information of multi-view data, which is essential for clustering tasks; 2) existing methods for partially unpaired problems rely on pregiven cross-view alignment information, resulting in their inability to handle fully unpaired problems; and 3) their inevitable parameters degrade the efficiency and applicability of the models. To tackle these issues, we propose a novel parameter-free graph clustering framework termed unpaired multi-view graph clustering framework with cross-view structure matching (UPMGC-SM). Specifically, unlike the existing methods, UPMGC-SM effectively utilizes the structural information from each view to refine cross-view correspondences. Besides, our UPMGC-SM is a unified framework for both the fully and partially unpaired multi-view graph clustering. Moreover, existing graph clustering methods can adopt our UPMGC-SM to enhance their ability for unpaired scenarios. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both paired and unpaired datasets.},
  archive      = {J_TNNLS},
  author       = {Yi Wen and Siwei Wang and Qing Liao and Weixuan Liang and Ke Liang and Xinhang Wan and Xinwang Liu},
  doi          = {10.1109/TNNLS.2023.3291696},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16049-16063},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unpaired multi-view graph clustering with cross-view structure matching},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Separating noisy samples from tail classes for long-tailed
image classification with label noise. <em>TNNLS</em>, <em>35</em>(11),
16036–16048. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing methods that cope with noisy labels usually assume that the classwise data distributions are well balanced. They are difficult to deal with the practical scenarios where training samples have imbalanced distributions, since they are not able to differentiate noisy samples from tail classes’ clean samples. This article makes an early effort to tackle the image classification task in which the provided labels are noisy and have a long-tailed distribution. To deal with this problem, we propose a new learning paradigm which can screen out noisy samples by matching between inferences on weak and strong data augmentations. A leave-noise-out regularization (LNOR) is further introduced to eliminate the effect of the recognized noisy samples. Besides, we propose a prediction penalty based on the online classwise confidence levels to avoid the bias toward easy classes which are dominated by head classes. Extensive experiments on five datasets including CIFAR-10, CIFAR-100, MNIST, FashionMNIST, and Clothing1M demonstrate that the proposed method outperforms the existing algorithms for learning with long-tailed distribution and label noise.},
  archive      = {J_TNNLS},
  author       = {Chaowei Fang and Lechao Cheng and Yining Mao and Dingwen Zhang and Yixiang Fang and Guanbin Li and Huiyan Qi and Licheng Jiao},
  doi          = {10.1109/TNNLS.2023.3291695},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16036-16048},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Separating noisy samples from tail classes for long-tailed image classification with label noise},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transient stability-constrained unit commitment using input
convex neural network. <em>TNNLS</em>, <em>35</em>(11), 16023–16035. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a transient stability-constrained unit commitment (TSC-UC) model using input convex neural networks (ICNNs). An ICNN is trained to learn the transient function that maps prefault operation conditions (e.g., generator power output) to the transient stability index (TSI), which can be further utilized to identify the transient status (e.g., stable or unstable). Transient stability evaluation is conducted using the learned ICNN, without discretizing differential-algebraic equations (DAEs) or interacting with the time-domain simulation tools. Based on the convexity of ICNNs, the trained ICNN is exactly encoded as a linear programming (LP) model and integrated into conventional UC models to form a TSC-UC model. To impose transient stability constraints and expedite the solution process, the proposed TSC-UC model is decomposed into a UC master problem and two subproblems (i.e., network feasibility check subproblems and transient stability check subproblems). The decomposed problem is then iteratively solved using the Benders decomposition. Simulation tests are conducted in the New England 39-bus test system and IEEE 118-bus test system to verify the validity of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Tao Wu and Jianhui Wang},
  doi          = {10.1109/TNNLS.2023.3291673},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16023-16035},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transient stability-constrained unit commitment using input convex neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperparameter learning under data poisoning: Analysis of
the influence of regularization via multiobjective bilevel optimization.
<em>TNNLS</em>, <em>35</em>(11), 16008–16022. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) algorithms are vulnerable to poisoning attacks, where a fraction of the training data is manipulated to deliberately degrade the algorithms’ performance. Optimal attacks can be formulated as bilevel optimization problems and help to assess their robustness in worst case scenarios. We show that current approaches, which typically assume that hyperparameters remain constant, lead to an overly pessimistic view of the algorithms’ robustness and of the impact of regularization. We propose a novel optimal attack formulation that considers the effect of the attack on the hyperparameters and models the attack as a multiobjective bilevel optimization problem. This allows us to formulate optimal attacks, learn hyperparameters, and evaluate robustness under worst case conditions. We apply this attack formulation to several ML classifiers using $L_{2}$ and $L_{1}$ regularization. Our evaluation on multiple datasets shows that choosing an “a priori” constant value for the regularization hyperparameter can be detrimental to the performance of the algorithms. This confirms the limitations of previous strategies and evidences the benefits of using $L_{2}$ and $L_{1}$ regularization to dampen the effect of poisoning attacks, when hyperparameters are learned using a small trusted dataset. Additionally, our results show that the use of regularization plays an important robustness and stability role in complex models, such as deep neural networks (DNNs), where the attacker can have more flexibility to manipulate the decision boundary.},
  archive      = {J_TNNLS},
  author       = {Javier Carnerero-Cano and Luis Muñoz-González and Phillippa Spencer and Emil C. Lupu},
  doi          = {10.1109/TNNLS.2023.3291648},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {16008-16022},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperparameter learning under data poisoning: Analysis of the influence of regularization via multiobjective bilevel optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camouflaged object segmentation based on
matching–recognition–refinement network. <em>TNNLS</em>,
<em>35</em>(11), 15993–16007. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the biosphere, camouflaged objects take the advantage of visional wholeness by keeping the color and texture of the objects highly consistent with the background, thereby confusing the visual mechanism of other creatures and achieving a concealed effect. This is also the main reason why the task of camouflaged object detection is challenging. In this article, we break the visual wholeness and see through the camouflage from the perspective of matching the appropriate field of view. We propose a matching–recognition–refinement network (MRR-Net), which consists of two key modules, i.e., the visual field matching and recognition module (VFMRM) and the stepwise refinement module (SWRM). In the VFMRM, various feature receptive fields are used to match candidate areas of camouflaged objects of different sizes and shapes and adaptively activate and recognize the approximate area of the real camouflaged object. The SWRM then uses the features extracted by the backbone to gradually refine the camouflaged region obtained by VFMRM, thus yielding the complete camouflaged object. In addition, a more efficient deep supervision method is exploited, making the features from the backbone input into the SWRM more critical and not redundant. Extensive experimental results demonstrate that our MRR-Net runs in real-time (82.6 frames/s) and significantly outperforms 30 state-of-the-art models on three challenging datasets under three standard metrics. Furthermore, MRR-Net is applied to four downstream tasks of camouflaged object segmentation (COS), and the results validate its practical application value. Our code is publicly available at: https://github.com/XinyuYanTJU/MRR-Net .},
  archive      = {J_TNNLS},
  author       = {Xinyu Yan and Meijun Sun and Yahong Han and Zheng Wang},
  doi          = {10.1109/TNNLS.2023.3291595},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15993-16007},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Camouflaged object segmentation based on Matching–Recognition–Refinement network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-based optimal synchronization of heterogeneous
multiagent systems in graphical games via reinforcement learning.
<em>TNNLS</em>, <em>35</em>(11), 15984–15992. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the optimal synchronization of linear heterogeneous multiagent systems (MASs) with partial unknown knowledge of the system dynamics. The object is to realize system synchronization as well as minimize the performance index of each agent. A framework of heterogeneous multiagent graphical games is formulated first. In the graphical games, it is proved that the optimal control policy relying on the solution of the Hamilton–Jacobian–Bellmen (HJB) equation is not only in Nash equilibrium, but also the best response to fixed control policies of its neighbors. To solve the optimal control policy and the minimum value of the performance index, a model-based policy iteration (PI) algorithm is proposed. Then, according to the model-based algorithm, a data-based off-policy integral reinforcement learning (IRL) algorithm is put forward to handle the partially unknown system dynamics. Furthermore, a single-critic neural network (NN) structure is used to implement the data-based algorithm. Based on the data collected by the behavior policy of the data-based off-policy algorithm, the gradient descent method is used to train NNs to approach the ideal weights. In addition, it is proved that all the proposed algorithms are convergent, and the weight-tuning law of the single-critic NNs can promote optimal synchronization. Finally, a numerical example is proposed to show the effectiveness of the theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Chunping Xiong and Qian Ma and Jian Guo and Frank L. Lewis},
  doi          = {10.1109/TNNLS.2023.3291542},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15984-15992},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-based optimal synchronization of heterogeneous multiagent systems in graphical games via reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting graph contrastive learning via adaptive sampling.
<em>TNNLS</em>, <em>35</em>(11), 15971–15983. (<a
href="https://doi.org/10.1109/TNNLS.2023.3291358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning (CL) is a prominent technique for self-supervised representation learning, which aims to contrast semantically similar (i.e., positive) and dissimilar (i.e., negative) pairs of examples under different augmented views. Recently, CL has provided unprecedented potential for learning expressive graph representations without external supervision. In graph CL, the negative nodes are typically uniformly sampled from augmented views to formulate the contrastive objective. However, this uniform negative sampling strategy limits the expressive power of contrastive models. To be specific, not all the negative nodes can provide sufficiently meaningful knowledge for effective contrastive representation learning. In addition, the negative nodes that are semantically similar to the anchor are undesirably repelled from it, leading to degraded model performance. To address these limitations, in this article, we devise an adaptive sampling strategy termed “AdaS. ” The proposed AdaS framework can be trained to adaptively encode the importance of different negative nodes, so as to encourage learning from the most informative graph nodes. Meanwhile, an auxiliary polarization regularizer is proposed to suppress the adverse impacts of the false negatives and enhance the discrimination ability of AdaS. The experimental results on a variety of real-world datasets firmly verify the effectiveness of our AdaS in improving the performance of graph CL.},
  archive      = {J_TNNLS},
  author       = {Sheng Wan and Yibing Zhan and Shuo Chen and Shirui Pan and Jian Yang and Dacheng Tao and Chen Gong},
  doi          = {10.1109/TNNLS.2023.3291358},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15971-15983},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Boosting graph contrastive learning via adaptive sampling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic keyphrase generation from copy and generating
spaces. <em>TNNLS</em>, <em>35</em>(11), 15956–15970. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyphrase generation is one of the most fundamental tasks in natural language processing (NLP). Most existing works on keyphrase generation mainly focus on using holistic distribution to optimize the negative log-likelihood loss, but they do not directly manipulate the copy and generating spaces, which may reduce the generability of the decoder. Additionally, existing keyphrase models are either unable to determine the dynamic numbers of keyphrases or produce the number of keyphrases implicitly. In this article, we propose a probabilistic keyphrase generation model from copy and generating spaces. The proposed model is built upon the vanilla variational encoder–decoder (VED) framework. On top of VED, two separate latent variables are adopted to model the distribution of data within the latent copy and generating spaces, respectively. Specifically, we adopt a von Mises–Fisher (vMF) distribution to obtain a condensed variable for modifying the generating probability distribution over the predefined vocabulary. Meanwhile, we utilize a clustering module, which is designed to promote Gaussian Mixture learning and subsequently extract a latent variable for the copy probability distribution. Moreover, we utilize a natural property of the Gaussian mixture network and use the number of filtered components to determine the number of keyphrases. The approach is trained based on latent variable probabilistic modeling, neural variational inference, and self-supervised learning. Experiments on social media and scientific article datasets outperform the state-of-the-art baselines in generating accurate predictions and controllable keyphrase numbers.},
  archive      = {J_TNNLS},
  author       = {Yu Yao and Peng Yang and Guangzhen Zhao and Yanyan Ge and Ying Yang},
  doi          = {10.1109/TNNLS.2023.3290789},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15956-15970},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic keyphrase generation from copy and generating spaces},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiview structural large margin classifier and its safe
acceleration strategy. <em>TNNLS</em>, <em>35</em>(11), 15944–15955. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview learning (MVL) concentrates on the problem where each instance is represented by multiple different feature sets. Efficiently exploring and exploiting the common and complementary information among different views remains challenging in MVL. Nevertheless, many existing algorithms deal with multiview problems via pairwise strategies, which limit the exploration of relationships among different views and dramatically increase the computational cost. In this article, we propose a multiview structural large margin classifier (MvSLMC) that simultaneously satisfies the consensus and complementarity principles in all views. Specifically, on the one hand, MvSLMC employs a structural regularization term to promote cohesion within-class and separability between-class in each view. On the other hand, different views provide extra structural information to each other, which favors the diversity of the classifier. Moreover, the introduction of hinge loss in MvSLMC results in sample sparsity, which we leverage to construct a safe screening rule (SSR) for accelerating MvSLMC. To the best of our knowledge, this is the first attempt at safe screening in MVL. Numerical experimental results demonstrate the effectiveness of MvSLMC and its safe acceleration method.},
  archive      = {J_TNNLS},
  author       = {Jie Zhao and Yitian Xu},
  doi          = {10.1109/TNNLS.2023.3290540},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15944-15955},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview structural large margin classifier and its safe acceleration strategy},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parametrized constant-depth quantum neuron. <em>TNNLS</em>,
<em>35</em>(11), 15932–15943. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing has been revolutionizing the development of algorithms. However, only noisy intermediate-scale quantum devices are available currently, which imposes several restrictions on the circuit implementation of quantum algorithms. In this article, we propose a framework that builds quantum neurons based on kernel machines, where the quantum neurons differ from each other by their feature space mappings. Besides contemplating previous quantum neurons, our generalized framework has the capacity to instantiate other feature mappings that allow us to solve real problems better. Under that framework, we present a neuron that applies a tensor-product feature mapping to an exponentially larger space. The proposed neuron is implemented by a circuit of constant depth with a linear number of elementary single-qubit gates. The previous quantum neuron applies a phase-based feature mapping with an exponentially expensive circuit implementation, even using multiqubit gates. Additionally, the proposed neuron has parameters that can change its activation function shape. Here, we show the activation function shape of each quantum neuron. It turns out that parametrization allows the proposed neuron to optimally fit underlying patterns that the existing neuron cannot fit, as demonstrated in the nonlinear toy classification problems addressed here. The feasibility of those quantum neuron solutions is also contemplated in the demonstration through executions on a quantum simulator. Finally, we compare those kernel-based quantum neurons in the problem of handwritten digit recognition, where the performances of quantum neurons that implement classical activation functions are also contrasted here. The repeated evidence of the parametrization potential achieved in real-life problems allows concluding that this work provides a quantum neuron with improved discriminative abilities. As a consequence, the generalized framework of quantum neurons can contribute toward practical quantum advantage.},
  archive      = {J_TNNLS},
  author       = {Jonathan H. A. de Carvalho and Fernando M. de Paula Neto},
  doi          = {10.1109/TNNLS.2023.3290535},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15932-15943},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parametrized constant-depth quantum neuron},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Demand peak forecasting of the fused magnesia furnace group
with model prediction and adaptive deep learning. <em>TNNLS</em>,
<em>35</em>(11), 15920–15931. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the fused magnesia production process (FMPP), there is a demand peak phenomenon that the demand rises first and then falls. Once the demand exceeds its limit value, the power will be cut off. To avoid mistaken power off caused by demand peak, demand peak needs to be forecast, so multistep demand forecasting is required. In this article, we develop a dynamic model of demand based on the closed-loop control system of smelting current in the FMPP. Using the model prediction method, we develop a multistep demand forecasting model consisting of a linear model and an unknown nonlinear dynamic system. Combining system identification with adaptive deep learning, an intelligent forecasting method for furnace group demand peak based on end-edge-cloud collaboration is proposed. It is verified that the proposed forecasting method can accurately forecast demand peak by utilizing industrial big data and end-edge-cloud collaboration technology.},
  archive      = {J_TNNLS},
  author       = {Yuheng Liu and Tianyou Chai},
  doi          = {10.1109/TNNLS.2023.3290185},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15920-15931},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Demand peak forecasting of the fused magnesia furnace group with model prediction and adaptive deep learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Regularized simple multiple kernel k-means with kernel
average alignment. <em>TNNLS</em>, <em>35</em>(11), 15910–15919. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel clustering (MKC) aims to learn an optimal kernel to better serve for clustering from several precomputed basic kernels. Most MKC algorithms adhere to a common assumption that an optimal kernel is linearly combined by basic kernels. Based on a min–max framework, a newly proposed MKC method termed simple multiple kernel $k$ -means (SimpleMKKM) can acquire a high-quality unified kernel. Although SimpleMKKM has achieved promising clustering performance, we observe that it cannot benefit from any prior knowledge. This would cause the learned partition matrix may seriously deviate from the expected one, especially in clustering tasks where the ground truth is absent during the learning course. To tackle this issue, we propose a novel algorithm termed regularized simple multiple kernel $k$ -means with kernel average alignment (R-SMKKM-KAA). According to the experimental results of existing MKC algorithms, the average partition is a strong baseline to reflect true clustering. To gain knowledge from the average partition, we add the average alignment as a regularization term to prevent the learned unified partition from being far from the average partition. After that, we have designed an efficient solving algorithm to optimize the new resulting problem. In this way, both the incorporated prior knowledge and the combination of basic kernels are helpful to learn better unified partition. Consequently, the clustering performance can be significantly improved. Extensive experiments on nine common datasets have sufficiently demonstrated the effectiveness of incorporation of prior knowledge into SimpleMKKM.},
  archive      = {J_TNNLS},
  author       = {Miaomiao Li and Yi Zhang and Chuan Ma and Suyuan Liu and Zhe Liu and Jianping Yin and Xinwang Liu and Qing Liao},
  doi          = {10.1109/TNNLS.2023.3290219},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15910-15919},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Regularized simple multiple kernel k-means with kernel average alignment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretability diversity for decision-tree-initialized
dendritic neuron model ensemble. <em>TNNLS</em>, <em>35</em>(11),
15896–15909. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To construct a strong classifier ensemble, base classifiers should be accurate and diverse. However, there is no uniform standard for the definition and measurement of diversity. This work proposes a learners’ interpretability diversity (LID) to measure the diversity of interpretable machine learners. It then proposes a LID-based classifier ensemble. Such an ensemble concept is novel because: 1) interpretability is used as an important basis for diversity measurement and 2) before its training, the difference between two interpretable base learners can be measured. To verify the proposed method’s effectiveness, we choose a decision-tree-initialized dendritic neuron model (DDNM) as a base learner for ensemble design. We apply it to seven benchmark datasets. The results show that the DDNM ensemble combined with LID obtains superior performance in terms of accuracy and computational efficiency compared to some popular classifier ensembles. A random-forest-initialized dendritic neuron model (RDNM) combined with LID is an outstanding representative of the DDNM ensemble.},
  archive      = {J_TNNLS},
  author       = {Xudong Luo and Long Ye and Xiaolan Liu and Xiaohao Wen and Mengchu Zhou and Qin Zhang},
  doi          = {10.1109/TNNLS.2023.3290203},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15896-15909},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interpretability diversity for decision-tree-initialized dendritic neuron model ensemble},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A collaborative multimodal learning-based framework for
COVID-19 diagnosis. <em>TNNLS</em>, <em>35</em>(11), 15883–15895. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pandemic of coronavirus disease 2019 (COVID-19) has led to a global public health crisis, which caused millions of deaths and billions of infections, greatly increasing the pressure on medical resources. With the continuous emergence of viral mutations, developing automated tools for COVID-19 diagnosis is highly desired to assist the clinical diagnosis and reduce the tedious workload of image interpretation. However, medical images in a single site are usually of a limited amount or weakly labeled, while integrating data scattered around different institutions to build effective models is not allowed due to data policy restrictions. In this article, we propose a novel privacy-preserving cross-site framework for COVID-19 diagnosis with multimodal data, seeking to effectively leverage heterogeneous data from multiple parties while preserving patients’ privacy. Specifically, a Siamese branched network is introduced as the backbone to capture inherent relationships across heterogeneous samples. The redesigned network is capable of handling semisupervised inputs in multimodalities and conducting task-specific training, in order to improve the model performance of various scenarios. The framework achieves significant improvement compared with state-of-the-art methods, as we demonstrate through extensive simulations on real-world datasets.},
  archive      = {J_TNNLS},
  author       = {Yuan Gao and Maoguo Gong and Yew-Soon Ong and A. K. Qin and Yue Wu and Fei Xie},
  doi          = {10.1109/TNNLS.2023.3290188},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15883-15895},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A collaborative multimodal learning-based framework for COVID-19 diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware proposal–boundary network with structural
consistency for audiovisual event localization. <em>TNNLS</em>,
<em>35</em>(11), 15872–15882. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audiovisual event localization aims to localize the event that is both visible and audible in a video. Previous works focus on segment-level audio and visual feature sequence encoding and neglect the event proposals and boundaries, which are crucial for this task. The event proposal features provide event internal consistency between several consecutive segments constructing one proposal, while the event boundary features offer event boundary consistency to make segments located at boundaries be aware of the event occurrence. In this article, we explore the proposal-level feature encoding and propose a novel context-aware proposal–boundary (CAPB) network to address audiovisual event localization. In particular, we design a local–global context encoder (LGCE) to aggregate local–global temporal context information for visual sequence, audio sequence, event proposals, and event boundaries, respectively. The local context from temporally adjacent segments or proposals contributes to event discrimination, while the global context from the entire video provides semantic guidance of temporal relationship. Furthermore, we enhance the structural consistency between segments by exploiting the above-encoded proposal and boundary representations. CAPB leverages the context information and structural consistency to obtain context-aware event-consistent cross-modal representation for accurate event localization. Extensive experiments conducted on the audiovisual event (AVE) dataset show that our approach outperforms the state-of-the-art methods by clear margins in both supervised event localization and cross-modality localization.},
  archive      = {J_TNNLS},
  author       = {Hao Wang and Zheng-Jun Zha and Liang Li and Xuejin Chen and Jiebo Luo},
  doi          = {10.1109/TNNLS.2023.3290083},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15872-15882},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Context-aware Proposal–Boundary network with structural consistency for audiovisual event localization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A shadow imaging bilinear model and three-branch residual
network for shadow removal. <em>TNNLS</em>, <em>35</em>(11),
15857–15871. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current shadow removal pipeline relies on the detected shadow masks, which have limitations for penumbras and tiny shadows, and results in an excessively long pipeline. To address these issues, we propose a shadow imaging bilinear model and design a novel three-branch residual (TBR) network for shadow removal. Our bilinear model reveals the single-image shadow removal process and can explain why simply increasing the brightness of shadow areas cannot remove shadows without artifacts. We considerably shorten the shadow removal pipeline by modeling illumination compensation and developing a single-stage shadow removal network without additional detection and refinement networks. Specifically, our network consists of three task branches, i.e., shadow image reconstruction, shadow matte estimation, and shadow removal. To merge these three branches and enhance the shadow removal branch, we design a model-based TBR module. Multiple TBR modules are cascaded to generate an intensive information flow and facilitate feature integration among the three branches. Thus, our network ensures the fidelity of nonshadow areas and restores the light intensity of shadow areas through three-branch collaboration. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods. The model and code are available at https://github.com/nachifur/TBRNet .},
  archive      = {J_TNNLS},
  author       = {Jiawei Liu and Qiang Wang and Huijie Fan and Jiandong Tian and Yandong Tang},
  doi          = {10.1109/TNNLS.2023.3290078},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15857-15871},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A shadow imaging bilinear model and three-branch residual network for shadow removal},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed online learning algorithm for noncooperative
games over unbalanced digraphs. <em>TNNLS</em>, <em>35</em>(11),
15846–15856. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates constrained online noncooperative games (NGs) of multiagent systems over unbalanced digraphs, where the cost functions of players are time-varying and are gradually revealed to corresponding players only after decisions are made. Moreover, in the problem, the players are subject to local convex set constraints and time-varying coupling nonlinear inequality constraints. To the best of our knowledge, no result about online games with unbalanced digraphs has been reported, let alone constrained online games. To seek the variational generalized Nash equilibrium (GNE) of the game online, a distributed learning algorithm is proposed based on gradient descent, projection, and primal-dual methods. Under the algorithm, sublinear dynamic regrets and constraint violations are established. Finally, online electricity market games illustrate the algorithm.},
  archive      = {J_TNNLS},
  author       = {Zhenhua Deng and Xiaolong Zuo},
  doi          = {10.1109/TNNLS.2023.3290049},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15846-15856},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed online learning algorithm for noncooperative games over unbalanced digraphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A practical contrastive learning framework for single-image
super-resolution. <em>TNNLS</em>, <em>35</em>(11), 15834–15845. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has achieved remarkable success on various high-level tasks, but there are fewer contrastive learning-based methods proposed for low-level tasks. It is challenging to adopt vanilla contrastive learning technologies proposed for high-level visual tasks to low-level image restoration problems straightly. Because the acquired high-level global visual representations are insufficient for low-level tasks requiring rich texture and context information. In this article, we investigate the contrastive learning-based single-image super-resolution (SISR) from two perspectives: positive and negative sample construction and feature embedding. The existing methods take naive sample construction approaches (e.g., considering the low-quality input as a negative sample and the ground truth as a positive sample) and adopt a prior model (e.g., pretrained very deep convolutional networks proposed by visual geometry group (VGG) model) to obtain the feature embedding. To this end, we propose a practical contrastive learning framework for SISR (PCL-SR). We involve the generation of many informative positive and hard negative samples in frequency space. Instead of utilizing an additional pretrained network, we design a simple but effective embedding network inherited from the discriminator network, which is more task-friendly. Compared with the existing benchmark methods, we retrain them by our proposed PCL-SR framework and achieve superior performance. Extensive experiments have been conducted to show the effectiveness and technical contributions of our proposed PCL-SR thorough ablation studies. The code and resulting models will be released via https://github.com/Aitical/PCL-SISR .},
  archive      = {J_TNNLS},
  author       = {Gang Wu and Junjun Jiang and Xianming Liu},
  doi          = {10.1109/TNNLS.2023.3290038},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15834-15845},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A practical contrastive learning framework for single-image super-resolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modified noise-immune fuzzy neural network for solving the
quadratic programming with equality constraint problem. <em>TNNLS</em>,
<em>35</em>(11), 15825–15833. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quadratic programming with equality constraint (QPEC) problems have extensive applicability in many industries as a versatile nonlinear programming modeling tool. However, noise interference is inevitable when solving QPEC problems in complex environments, so research on noise interference suppression or elimination methods is of great interest. This article proposes a modified noise-immune fuzzy neural network (MNIFNN) model and use it to solve QPEC problems. Compared with the traditional gradient recurrent neural network (TGRNN) and traditional zeroing recurrent neural network (TZRNN) models, the MNIFNN model has the advantage of inherent noise tolerance ability and stronger robustness, which is achieved by combining proportional, integral, and differential elements. Furthermore, the design parameters of the MNIFNN model adopt two disparate fuzzy parameters generated by two fuzzy logic systems (FLSs) related to the residual and residual integral term, which can improve the adaptability of the MNIFNN model. Numerical simulations demonstrate the effectiveness of the MNIFNN model in noise tolerance.},
  archive      = {J_TNNLS},
  author       = {Jianhua Dai and Liu Luo and Lin Xiao and Lei Jia and Penglin Cao and Jichun Li and Natalio Krasnogor and Yaonan Wang},
  doi          = {10.1109/TNNLS.2023.3290030},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15825-15833},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modified noise-immune fuzzy neural network for solving the quadratic programming with equality constraint problem},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consensus seeking in large-scale multiagent systems with
hierarchical switching-backbone topology. <em>TNNLS</em>,
<em>35</em>(11), 15810–15824. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in multiagent consensus problems have heightened the role of network topology when the agent number increases largely. The existing works assume that the convergence evolution typically proceeds over a peer-to-peer architecture where agents are treated equally and communicate directly with perceived one-hop neighbors, thus resulting in slower convergence speed. In this article, we first extract the backbone network topology to provide a hierarchical organization over the original multiagent system (MAS). Second, we introduce a geometric convergence method based on the constraint set (CS) under periodically extracted switching-backbone topologies. Finally, we derive a fully decentralized framework named hierarchical switching-backbone MAS (HSBMAS) that is designed to conduct agents converge to a common stable equilibrium. Provable connectivity and convergence guarantees of the framework are provided when the initial topology is connected. Extensive simulation results on different-type and varying-density topologies have shown the superiority of the proposed framework.},
  archive      = {J_TNNLS},
  author       = {Guangqiang Xie and Haoran Xu and Yang Li and Chang-Dong Wang and Biwei Zhong and Xianbiao Hu},
  doi          = {10.1109/TNNLS.2023.3290015},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15810-15824},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consensus seeking in large-scale multiagent systems with hierarchical switching-backbone topology},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning-based adaptive optimal control for
nonlinear systems with asymmetric hysteresis. <em>TNNLS</em>,
<em>35</em>(11), 15800–15809. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive optimal tracking problem for a class of nonlinear affine systems with asymmetric Prandtl–Ishlinskii (PI) hysteresis nonlinearities based on actor-critic (A-C) learning mechanisms. Considering the huge obstacles arising from the uncertainty of hysteresis nonlinearity in actuators, we develop a scheme for the conflict between the construction of Hamilton functions and hysteresis nonlinearity. The actuator hysteresis forces the input into a hysteresis delay, thus preventing the Hamilton function from getting the current moment’s input instantly and thus making optimization impossible. In the first step, an inverse model is constructed to compensate for the hysteresis model with a shift factor. In the second step, we compensate for the control input by designing a feedback controller and incorporating the estimation and approximation errors into the Hamilton error. Optimal control, the other part of the actual control input, is obtained by taking partial derivatives of the Hamiltonian function after the nonlinearities have been circumvented. At the end, a simulation is given to validate the developed solution.},
  archive      = {J_TNNLS},
  author       = {Licheng Zheng and Zhi Liu and Yaonan Wang and C. L. Philip Chen and Yun Zhang and Zongze Wu},
  doi          = {10.1109/TNNLS.2023.3289978},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15800-15809},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based adaptive optimal control for nonlinear systems with asymmetric hysteresis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Deep hierarchical multimodal metric learning.
<em>TNNLS</em>, <em>35</em>(11), 15787–15799. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal metric learning aims to transform heterogeneous data into a common subspace where cross-modal similarity computing can be directly performed and has received much attention in recent years. Typically, the existing methods are designed for nonhierarchical labeled data. Such methods fail to exploit the intercategory correlations in the label hierarchy and, therefore, cannot achieve optimal performance on hierarchical labeled data. To address this problem, we propose a novel metric learning method for hierarchical labeled multimodal data, named deep hierarchical multimodal metric learning (DHMML). It learns the multilayer representations for each modality by establishing a layer-specific network corresponding to each layer in the label hierarchy. In particular, a multilayer classification mechanism is introduced to enable the layerwise representations to not only preserve the semantic similarities within each layer, but also retain the intercategory correlations across different layers. In addition, an adversarial learning mechanism is proposed to bridge the cross-modality gap by producing indistinguishable features for different modalities. Through integration of the multilayer classification and adversarial learning mechanisms, DHMML can obtain hierarchical discriminative modality-invariant representations for multimodal data. Experiments on two benchmark datasets are used to demonstrate the superiority of the proposed DHMML method over several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Di Wang and Aqiang Ding and Yumin Tian and Quan Wang and Lihuo He and Xinbo Gao},
  doi          = {10.1109/TNNLS.2023.3289971},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15787-15799},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep hierarchical multimodal metric learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wavelet pyramid recurrent structure-preserving attention
network for single image super-resolution. <em>TNNLS</em>,
<em>35</em>(11), 15772–15786. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many single image super-resolution (SISR) methods that use convolutional neural networks (CNNs) learn the relationship between low- and high-resolution images directly, without considering the context structure and detail fidelity. This can limit the potential of CNNs and result in unrealistic, distorted edges and textures in the reconstructed images. A more effective approach is to incorporate prior knowledge about the image into the model to aid in image reconstruction. In this study, we propose a novel recurrent structure-preserving mechanism that innovatively uses the multiscale wavelet transform (WT) as an image prior, namely, wavelet pyramid recurrent structure-preserving attention network (WRSANet), to process both low- and high-frequency subnetworks at each level separately and recursively. We propose a novel structure scale preservation (SSP) architecture that differs from traditional WTs. This architecture allows us to incorporate and learn structure preservation subnetworks at each level. By using our proposed structure scale fusion (SSF) combined with inverse WT, we can recursively restore and preserve rich low-frequency image structure through the combination of SSP at various levels. Furthermore, we also propose novel low-to-high-frequency information transmission (L2HIT) and detail enhancement (DE) mechanisms to address the issue of detail distortion in high-frequency images by transferring information from structure preservation subnetworks. This allows us to preserve the low-frequency structure while reconstructing high-frequency details, improving detail fidelity and avoiding structural distortion. Finally, a joint loss function is also used to balance the fusion of low- and high-frequency information at different degrees, with hyperparameters being adjusted during training. The experimental results demonstrate that the proposed WRSANet achieves better performance and visual presentation than the state-of-the-art (SOTA) on synthetic and real datasets, especially in terms of context structure and texture details.},
  archive      = {J_TNNLS},
  author       = {Wei-Yen Hsu and Pei-Wen Jian},
  doi          = {10.1109/TNNLS.2023.3289958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15772-15786},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Wavelet pyramid recurrent structure-preserving attention network for single image super-resolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-light image enhancement by retinex-based algorithm
unrolling and adjustment. <em>TNNLS</em>, <em>35</em>(11), 15758–15771.
(<a href="https://doi.org/10.1109/TNNLS.2023.3289626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement (LIE) has attracted tremendous research interests in recent years. Retinex theory-based deep learning methods, following a decomposition-adjustment pipeline, have achieved promising performance due to their physical interpretability. However, existing Retinex-based deep learning methods are still suboptimal, failing to leverage useful insights from traditional approaches. Meanwhile, the adjustment step is either oversimplified or overcomplicated, resulting in unsatisfactory performance in practice. To address these issues, we propose a novel deep-learning framework for LIE. The framework consists of a decomposition network (DecNet) inspired by algorithm unrolling and adjustment networks considering both global and local brightness. The algorithm unrolling allows the integration of both implicit priors learned from data and explicit priors inherited from traditional methods, facilitating better decomposition. Meanwhile, considering global and local brightness guides the design of effective yet lightweight adjustment networks. Moreover, we introduce a self-supervised fine-tuning strategy that achieves promising performance without manual hyperparameter tuning. Extensive experiments on benchmark LIE datasets demonstrate the superiority of our approach over existing state-of-the-art methods both quantitatively and qualitatively. Code is available at https://github.com/Xinyil256/RAUNA2023 .},
  archive      = {J_TNNLS},
  author       = {Xinyi Liu and Qi Xie and Qian Zhao and Hong Wang and Deyu Meng},
  doi          = {10.1109/TNNLS.2023.3289626},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15758-15771},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low-light image enhancement by retinex-based algorithm unrolling and adjustment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zoom text detector. <em>TNNLS</em>, <em>35</em>(11),
15745–15757. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To pursue comprehensive performance, recent text detectors improve detection speed at the expense of accuracy. They adopt shrink-mask-based text representation strategies, which leads to a high dependence of detection accuracy on shrink-masks. Unfortunately, three disadvantages cause unreliable shrink-masks. Specifically, these methods try to strengthen the discrimination of shrink-masks from the background by semantic information. However, the feature defocusing phenomenon that coarse layers are optimized by fine-grained objectives limits the extraction of semantic features. Meanwhile, since both shrink-masks and the margins belong to texts, the detail loss phenomenon that the margins are ignored hinders the distinguishment of shrink-masks from the margins, which causes ambiguous shrink-mask edges. Moreover, false-positive samples enjoy similar visual features with shrink-masks. They aggravate the decline of shrink-masks recognition. To avoid the above problems, we propose a zoom text detector (ZTD) inspired by the zoom process of the camera. Specifically, zoomed-out view module (ZOM) is introduced to provide coarse-grained optimization objectives for coarse layers to avoid feature defocusing. Meanwhile, zoomed-in view module (ZIM) is presented to enhance the margins recognition to prevent detail loss. Furthermore, sequential-visual discriminator (SVD) is designed to suppress false-positive samples by sequential and visual features. Experiments verify the superior comprehensive performance of ZTD.},
  archive      = {J_TNNLS},
  author       = {Chuang Yang and Mulin Chen and Yuan Yuan and Qi Wang},
  doi          = {10.1109/TNNLS.2023.3289327},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15745-15757},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Zoom text detector},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-in-the-loop reinforcement learning in
continuous-action space. <em>TNNLS</em>, <em>35</em>(11), 15735–15744.
(<a href="https://doi.org/10.1109/TNNLS.2023.3289315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-in-the-loop for reinforcement learning (RL) is usually employed to overcome the challenge of sample inefficiency, in which the human expert provides advice for the agent when necessary. The current human-in-the-loop RL (HRL) results mainly focus on discrete action space. In this article, we propose a $Q$ value-dependent policy (QDP)-based HRL (QDP-HRL) algorithm for continuous action space. Considering the cognitive costs of human monitoring, the human expert only selectively gives advice in the early stage of agent learning, where the agent implements human-advised action instead. The QDP framework is adapted to the twin delayed deep deterministic policy gradient algorithm (TD3) in this article for the convenience of comparison with the state-of-the-art TD3. Specifically, the human expert in the QDP-HRL considers giving advice in the case that the difference between the twin $Q$ -networks’ output exceeds the maximum difference in the current queue. Moreover, to guide the update of the critic network, the advantage loss function is developed using expert experience and agent policy, which provides the learning direction for the QDP-HRL algorithm to some extent. To verify the effectiveness of QDP-HRL, the experiments are conducted on several continuous action space tasks in the OpenAI gym environment, and the results demonstrate that QDP-HRL greatly improves learning speed and performance.},
  archive      = {J_TNNLS},
  author       = {Biao Luo and Zhengke Wu and Fei Zhou and Bing-Chuan Wang},
  doi          = {10.1109/TNNLS.2023.3289315},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15735-15744},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Human-in-the-loop reinforcement learning in continuous-action space},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronization of markov jump neural networks with
communication constraints via asynchronous output feedback control.
<em>TNNLS</em>, <em>35</em>(11), 15724–15734. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the synchronization issue of discrete Markov jump neural networks (MJNNs). First, to save communication resources, a universal communication model, including event-triggered transmission, logarithmic quantization, and asynchronous phenomenon, is proposed, which is close to the actual situation. Here, to further reduce conservatism, a more general event-triggered protocol is constructed by developing the threshold parameter as a diagonal matrix. To cope with mode mismatch between the nodes and controllers due to potentially occurring time lag and packet dropouts, a hidden Markov model (HMM) method is adopted. Second, considering that state information of nodes may not be available, the asynchronous output feedback controllers are devised by a novel decoupling strategy. Then, sufficient conditions based on linear matrix inequalities (LMIs) for dissipative synchronization of MJNNs are proposed with the virtue of Lyapunov techniques. Third, by eliminating asynchronous terms, a corollary with less computational cost is devised. Finally, two numerical examples verify the effectiveness of the above results.},
  archive      = {J_TNNLS},
  author       = {Jie Tao and Zhenyu Wu and Zehui Xiao and Hongxia Rao and Yong Xu and Peng Shi},
  doi          = {10.1109/TNNLS.2023.3289297},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15724-15734},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of markov jump neural networks with communication constraints via asynchronous output feedback control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). The perturbation analysis of nonconvex low-rank matrix
robust recovery. <em>TNNLS</em>, <em>35</em>(11), 15710–15723. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we bring forward a completely perturbed nonconvex Schatten $p$ -minimization to address a model of completely perturbed low-rank matrix recovery (LRMR). This article based on the restricted isometry property (RIP) and the Schatten- $p$ null space property (NSP) generalizes the investigation to a complete perturbation model thinking over not only noise but also perturbation, and it gives the RIP condition and the Schatten- $p$ NSP assumption that guarantee the recovery of low-rank matrix and the corresponding reconstruction error bounds. In particular, the analysis of the result reveals that in the case that $p$ decreases 0 and $a&amp;gt;1$ for the complete perturbation and low-rank matrix, the condition is the optimal sufficient condition $\delta _{2r} &amp;lt; 1$ (Recht et al., 2010). In addition, we study the connection between RIP and Schatten- $p$ NSP and discern that Schatten- $p$ NSP can be inferred from the RIP. The numerical experiments are conducted to show better performance and provide outperformance of the nonconvex Schatten $p$ -minimization method comparing with the convex nuclear norm minimization approach in the completely perturbed scenario.},
  archive      = {J_TNNLS},
  author       = {Jianwen Huang and Feng Zhang and Jianjun Wang and Xinling Liu and Jinping Jia},
  doi          = {10.1109/TNNLS.2023.3289209},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15710-15723},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The perturbation analysis of nonconvex low-rank matrix robust recovery},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MGCNRF: Prediction of disease-related miRNAs based on
multiple graph convolutional networks and random forest. <em>TNNLS</em>,
<em>35</em>(11), 15701–15709. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing microRNAs (miRNAs) have been confirmed to be inextricably linked to various diseases, and the discovery of their associations has become a routine way of treating diseases. To overcome the time-consuming and laborious shortcoming of traditional experiments in verifying the associations of miRNAs and diseases (MDAs), a variety of computational methods have emerged. However, these methods still have many shortcomings in terms of predictive performance and accuracy. In this study, a model based on multiple graph convolutional networks and random forest (MGCNRF) was proposed for the prediction MDAs. Specifically, MGCNRF first mapped miRNA functional similarity and sequence similarity, disease semantic similarity and target similarity, and the known MDAs into four different two-layer heterogeneous networks. Second, MGCNRF applied four heterogeneous networks into four different layered attention graph convolutional networks (GCNs), respectively, to extract MDA embeddings. Finally, MGCNRF integrated the embeddings of every MDA into the features of the miRNA-disease pair and predicted potential MDAs through the random forest (RF). Fivefold cross-validation was applied to verify the prediction performance of MGCNRF, which outperforms the other seven state-of-the-art methods by area under curve. Furthermore, the accuracy and the case studies of different diseases further demonstrate the scientific rationale of MGCNRF. In conclusion, MGCNRF can serve as a scientific tool for predicting potential MDAs.},
  archive      = {J_TNNLS},
  author       = {Yi Yang and Yan Sun and Feng Li and Boxin Guan and Jin-Xing Liu and Junliang Shang},
  doi          = {10.1109/TNNLS.2023.3289182},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15701-15709},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MGCNRF: Prediction of disease-related miRNAs based on multiple graph convolutional networks and random forest},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Highly efficient active learning with tracklet-aware
co-cooperative annotators for person re-identification. <em>TNNLS</em>,
<em>35</em>(11), 15687–15700. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised person re-identification (ReID) has attracted widespread attentions in the computer vision community due to its great potential in real-world applications. However, the demand of human annotation heavily limits the application as it is costly to annotate identical pedestrians appearing from different cameras. Thus, how to reduce the annotation cost while preserving the performance remains challenging and has been studied extensively. In this article, we propose a tracklet-aware co-cooperative annotators’ framework to reduce the demand of human annotation. Specifically, we partition the training samples into different clusters and associate adjacent images in each cluster to produce the robust tracklet which decreases the annotation requirements significantly. Besides, to further reduce the cost, we introduce a powerful teacher model in our framework to implement the active learning strategy and select the most informative tracklets for human annotator, the teacher model itself, in our setting, also acts as an annotator to label the relatively certain tracklets. Thus, our final model could be well-trained with both confident pseudo-labels and human-given annotations. Extensive experiments on three popular person ReID datasets demonstrate that our approach could achieve competitive performance compared with state-of-the-art methods in both active learning and unsupervised learning (USL) settings.},
  archive      = {J_TNNLS},
  author       = {Xiao Teng and Long Lan and Jing Zhao and Xueqiong Li and Yuhua Tang},
  doi          = {10.1109/TNNLS.2023.3289178},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15687-15700},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Highly efficient active learning with tracklet-aware co-cooperative annotators for person re-identification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep multirepresentation learning for data clustering.
<em>TNNLS</em>, <em>35</em>(11), 15675–15686. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering incorporates embedding into clustering in order to find a lower-dimensional space suitable for clustering tasks. Conventional deep clustering methods aim to obtain a single global embedding subspace (aka latent space) for all the data clusters. In contrast, in this article, we propose a deep multirepresentation learning (DML) framework for data clustering whereby each difficult-to-cluster data group is associated with its own distinct optimized latent space and all the easy-to-cluster data groups are associated with a general common latent space. Autoencoders (AEs) are employed for generating cluster-specific and general latent spaces. To specialize each AE in its associated data cluster(s), we propose a novel and effective loss function which consists of weighted reconstruction and clustering losses of the data points, where higher weights are assigned to the samples more probable to belong to the corresponding cluster(s). Experimental results on benchmark datasets demonstrate that the proposed DML framework and loss function outperform state-of-the-art clustering approaches. In addition, the results show that the DML method significantly outperforms the SOTA on imbalanced datasets as a result of assigning an individual latent space to the difficult clusters.},
  archive      = {J_TNNLS},
  author       = {Mohammadreza Sadeghi and Narges Armanfard},
  doi          = {10.1109/TNNLS.2023.3289158},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15675-15686},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep multirepresentation learning for data clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond photometric consistency: Geometry-based
occlusion-aware unsupervised light field disparity estimation.
<em>TNNLS</em>, <em>35</em>(11), 15660–15674. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although learning-based light field disparity estimation has achieved great progress in the most recent years, the performance of unsupervised light field learning is still hindered by occlusions and noises. By analyzing the overall strategy underlying the unsupervised methodology and the light field geometry implied in epipolar plane images (EPIs), we look beyond the photometric consistency assumption, and design an occlusion-aware unsupervised framework to deal with the situations of photometric consistency conflict. Specifically, we present a geometry-based light field occlusion modeling, which predicts a group of visibility masks and occlusion maps, respectively, by forward warping and backward EPI-line tracing. In order to learn better the noise- and occlusion-invariant representations of the light field, we propose two occlusion-aware unsupervised losses: occlusion-aware SSIM and statistics-based EPI loss. Experiment results demonstrate that our method can improve the estimation accuracy of light field depth over the occluded and noisy regions, and preserve the occlusion boundaries better.},
  archive      = {J_TNNLS},
  author       = {Wenhui Zhou and Lili Lin and Yongjie Hong and Qiujian Li and Xingfa Shen and Ercan Engin Kuruoglu},
  doi          = {10.1109/TNNLS.2023.3289056},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15660-15674},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Beyond photometric consistency: Geometry-based occlusion-aware unsupervised light field disparity estimation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learn to adapt for self-supervised monocular depth
estimation. <em>TNNLS</em>, <em>35</em>(11), 15647–15659. (<a
href="https://doi.org/10.1109/TNNLS.2023.3289051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation is one of the fundamental tasks in environmental perception and has achieved tremendous progress by virtue of deep learning. However, the performance of trained models tends to degrade or deteriorate when employed on other new datasets due to the gap between different datasets. Though some methods utilize domain adaptation technologies to jointly train different domains and narrow the gap between them, the trained models cannot generalize to new domains that are not involved in training. To boost the transferability of self-supervised monocular depth estimation models and mitigate the issue of meta-overfitting, we train the model in the pipeline of meta-learning and propose an adversarial depth estimation task. We adopt model-agnostic meta-learning (MAML) to obtain universal initial parameters for further adaptation and train the network in an adversarial manner to extract domain-invariant representations for easing meta-overfitting. In addition, we propose a constraint to impose upon cross-task depth consistency to compel the depth estimation to be identical in different adversarial tasks, which improves the performance of our method and smoothens the training process. Experiments on four new datasets demonstrate that our method adapts quite fast to new domains. Our method trained after 0.5 epoch achieves comparable results with the state-of-the-art methods trained at least 20 epochs.},
  archive      = {J_TNNLS},
  author       = {Qiyu Sun and Gary G. Yen and Yang Tang and Chaoqiang Zhao},
  doi          = {10.1109/TNNLS.2023.3289051},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15647-15659},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learn to adapt for self-supervised monocular depth estimation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projective synchronization for uncertain fractional
reaction-diffusion systems via adaptive sliding mode control based on
finite-time scheme. <em>TNNLS</em>, <em>35</em>(11), 15638–15646. (<a
href="https://doi.org/10.1109/TNNLS.2023.3288849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the projective synchronization of uncertain fractional-order (FO) reaction-diffusion systems is studied for the first time via the fractional adaptive sliding mode control (SMC) method. A FO integral type switching function is designed, and corresponding adaptive SMC laws are derived which ensure the FO sliding mode surface (SMS) is reachable after a finite time interval. The improved version of these control laws which have smaller oscillation and better control performance are also derived. A new lemma for proving the finite-time reachability of the FO SMS is developed. At last, numerical examples are provided to verify the effectiveness of our theories.},
  archive      = {J_TNNLS},
  author       = {Yonggui Kao and Yue Cao and Yangquan Chen},
  doi          = {10.1109/TNNLS.2023.3288849},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15638-15646},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Projective synchronization for uncertain fractional reaction-diffusion systems via adaptive sliding mode control based on finite-time scheme},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-adjusted recommendation via matrix factorization
with weighted losses. <em>TNNLS</em>, <em>35</em>(11), 15624–15637. (<a
href="https://doi.org/10.1109/TNNLS.2023.3288769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recommender systems (RSs) dataset, observed ratings are subject to unequal amounts of noise. Some users might be consistently more conscientious in choosing the ratings they provide for the content they consume. Some items may be very divisive and elicit highly noisy reviews. In this article, we perform a nuclear-norm-based matrix factorization method which relies on side information in the form of an estimate of the uncertainty of each rating. A rating with a higher uncertainty is considered more likely to be erroneous or subject to large amounts of noise, and therefore more likely to mislead the model. Our uncertainty estimate is used as a weighting factor in the loss we optimize. To maintain the favorable scaling and theoretical guarantees coming with nuclear norm regularization even in this weighted context, we introduce an adjusted version of the trace norm regularizer which takes the weights into account. This regularization strategy is inspired from the weighted trace norm which was introduced to tackle nonuniform sampling regimes in matrix completion. Our method exhibits state-of-the-art performance on both synthetic and real life datasets in terms of various performance measures, confirming that we have successfully used the auxiliary information extracted.},
  archive      = {J_TNNLS},
  author       = {Rodrigo Alves and Antoine Ledent and Marius Kloft},
  doi          = {10.1109/TNNLS.2023.3288769},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15624-15637},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Uncertainty-adjusted recommendation via matrix factorization with weighted losses},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data-driven bayesian koopman learning method for modeling
hysteresis dynamics. <em>TNNLS</em>, <em>35</em>(11), 15615–15623. (<a
href="https://doi.org/10.1109/TNNLS.2023.3288752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring the mechanism of hysteresis dynamics may facilitate the analysis and controller design to alleviate detrimental effects. Conventional models, such as the Bouc–Wen and Preisach models consist of complicated nonlinear structures, limiting the applications of hysteresis systems for high-speed and high-precision positioning, detection, execution, and other operations. In this article, a Bayesian Koopman (B-Koopman) learning algorithm is therefore developed to characterize hysteresis dynamics. Essentially, the proposed scheme establishes a simplified linear representation with time delay for hysteresis dynamics, where the properties of the original nonlinear system are preserved. Furthermore, model parameters are optimized via sparse Bayesian learning together with an iterative strategy, which simplifies the identification procedure and reduces modeling errors. Extensive experimental results on piezoelectric positioning are elaborated to substantiate the effectiveness and superiority of the proposed B-Koopman algorithm for learning hysteresis dynamics.},
  archive      = {J_TNNLS},
  author       = {Xiang Huang and Hai-Tao Zhang and Jun Wang},
  doi          = {10.1109/TNNLS.2023.3288752},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15615-15623},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A data-driven bayesian koopman learning method for modeling hysteresis dynamics},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised feature selection via controllable adaptive
graph learning and discriminative feature learning. <em>TNNLS</em>,
<em>35</em>(11), 15600–15614. (<a
href="https://doi.org/10.1109/TNNLS.2023.3288734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection is challenging in machine learning, pattern recognition, and data mining. The crucial difficulty is to learn a moderate subspace that preserves the intrinsic structure and to find uncorrelated or independent features simultaneously. The most common solution is first to project the original data into a lower dimensional space and then force them to preserve the similar intrinsic structure under linear uncorrelation constraint. However, there are three shortcomings. First, the final graph generated by the iterative learning process differs significantly from the initial graph in which the original intrinsic structure is embedded. Second, it requires prior knowledge about a moderate dimension of subspace. Third, it is inefficient when dealing with high-dimensional datasets. The first shortcoming, which is longstanding and undiscovered, makes the previous methods fail to achieve their expected results. The last two ones increase the difficulty of applying in different fields. Therefore, two unsupervised feature selection methods are proposed based on controllable adaptive graph learning and uncorrelated/independent feature learning (CAG-U and CAG-I) to address the abovementioned issues. In the proposed methods, the final graph that preserves intrinsic structure can be adaptively learned while the difference between the two graphs can be well controlled. Besides, relatively uncorrelated/independent features can be selected using a discrete projection matrix. The experimental results on 12 datasets in different fields show the superiority of CAG-U and CAG-I.},
  archive      = {J_TNNLS},
  author       = {Pei Huang and Mengying Xie and Xiaowei Yang},
  doi          = {10.1109/TNNLS.2023.3288734},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15600-15614},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised feature selection via controllable adaptive graph learning and discriminative feature learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Random polynomial neural networks: Analysis and design.
<em>TNNLS</em>, <em>35</em>(11), 15589–15599. (<a
href="https://doi.org/10.1109/TNNLS.2023.3288577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose the concept of random polynomial neural networks (RPNNs) realized based on the architecture of polynomial neural networks (PNNs) with random polynomial neurons (RPNs). RPNs exhibit generalized polynomial neurons (PNs) based on random forest (RF) architecture. In the design of RPNs, the target variables are no longer directly used in conventional decision trees, and the polynomial of these target variables is exploited here to determine the average prediction. Unlike the conventional performance index used in the selection of PNs, the correlation coefficient is adopted here to select the RPNs of each layer. When compared with the conventional PNs used in PNNs, the proposed RPNs exhibit the following advantages: first, RPNs are insensitive to outliers; second, RPNs can obtain the importance of each input variable after training; third, RPNs can alleviate the overfitting problem with the use of an RF structure. The overall nonlinearity of a complex system is captured by means of PNNs. Moreover, particle swarm optimization (PSO) is exploited to optimize the parameters when constructing RPNNs. The RPNNs take advantage of both RF and PNNs: it exhibits high accuracy based on ensemble learning used in the RF and is beneficial to describe high-order nonlinear relations between input and output variables stemming from PNNs. Experimental results based on a series of well-known modeling benchmarks illustrate that the proposed RPNNs outperform other state-of-the-art models reported in the literature.},
  archive      = {J_TNNLS},
  author       = {Wei Huang and Yueyue Xiao and Sung-Kwun Oh and Witold Pedrycz and Liehuang Zhu},
  doi          = {10.1109/TNNLS.2023.3288577},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15589-15599},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Random polynomial neural networks: Analysis and design},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A tree-structured multitask model architectures
recommendation system. <em>TNNLS</em>, <em>35</em>(11), 15578–15588. (<a
href="https://doi.org/10.1109/TNNLS.2023.3288537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks with branched architectures, namely, tree-structured models, have been employed to jointly tackle multiple vision tasks in the context of multitask learning (MTL). Such tree-structured networks typically start with a number of shared layers, after which different tasks branch out into their own sequence of layers. Hence, the major challenge is to determine where to branch out for each task given a backbone model to optimize for both task accuracy and computation efficiency. To address the challenge, this article proposes a recommendation system that, given a set of tasks and a convolutional neural network-based backbone model, automatically suggests tree-structured multitask architectures that could achieve a high task performance while meeting a user-specified computation budget without performing model training. Extensive evaluations on popular MTL benchmarks show that the recommended architectures could achieve competitive task accuracy and computation efficiency compared with state-of-the-art MTL methods. Our tree-structured multitask model recommender is open-sourced and available at https://github.com/zhanglijun95/TreeMTL .},
  archive      = {J_TNNLS},
  author       = {Lijun Zhang and Xiao Liu and Hui Guan},
  doi          = {10.1109/TNNLS.2023.3288537},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15578-15588},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A tree-structured multitask model architectures recommendation system},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-aware DropEdge toward deep graph convolutional
networks. <em>TNNLS</em>, <em>35</em>(11), 15565–15577. (<a
href="https://doi.org/10.1109/TNNLS.2023.3288484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been discovered that graph convolutional networks (GCNs) encounter a remarkable drop in performance when multiple layers are piled up. The main factor that accounts for why deep GCNs fail lies in oversmoothing, which isolates the network output from the input with the increase of network depth, weakening expressivity and trainability. In this article, we start by investigating refined measures upon DropEdge—an existing simple yet effective technique to relieve oversmoothing. We term our method as DropEdge++ for its two structure-aware samplers in contrast to DropEdge: layer-dependent (LD) sampler and feature-dependent (FD) sampler. Regarding the LD sampler, we interestingly find that increasingly sampling edges from the bottom layer yields superior performance than the decreasing counterpart as well as DropEdge. We theoretically reveal this phenomenon with mean-edge-number (MEN), a metric closely related to oversmoothing. For the FD sampler, we associate the edge sampling probability with the feature similarity of node pairs and prove that it further correlates the convergence subspace of the output layer with the input features. Extensive experiments on several node classification benchmarks, including both full- and semi-supervised tasks, illustrate the efficacy of DropEdge++ and its compatibility with a variety of backbones by achieving generally better performance over DropEdge and the no-drop version.},
  archive      = {J_TNNLS},
  author       = {Jiaqi Han and Wenbing Huang and Yu Rong and Tingyang Xu and Fuchun Sun and Junzhou Huang},
  doi          = {10.1109/TNNLS.2023.3288484},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15565-15577},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Structure-aware DropEdge toward deep graph convolutional networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Microseismic signal reconstruction from strong complex noise
using low-rank structure extraction and dual convolutional neural
networks. <em>TNNLS</em>, <em>35</em>(11), 15554–15564. (<a
href="https://doi.org/10.1109/TNNLS.2023.3288142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microseismic signal reconstruction from complex nonrandom noise is challenging, especially when the signal is disrupted or completely covered by strong field noise. Various methods often assume that signals are laterally coherent or the noise is predictable. In this article, we propose a dual convolutional neural network preceded by a low-rank structure extraction module to reconstruct signals hidden by strong complex field noise. Preconditioning by low-rank structure extraction is the first step in removing high-energy regular noise. The module is followed by two convolutional neural networks with different complexity to achieve better signal reconstruction and noise removal. In addition to the combination of synthetic and field microseismic data, natural images are also used in the training due to their correlation, complexity, and completeness, which contributes to increasing the generalization of the networks. The results from synthetic and real datasets demonstrate superior signal recovery, which cannot be achieved by using solely deep learning, low-rank structure extraction, or curvelet thresholding. Algorithmic generalization is demonstrated using independently acquired array data excluded during training.},
  archive      = {J_TNNLS},
  author       = {Chao Zhang and Mirko van der Baan},
  doi          = {10.1109/TNNLS.2023.3288142},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15554-15564},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Microseismic signal reconstruction from strong complex noise using low-rank structure extraction and dual convolutional neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly supervised contrastive learning for unsupervised
vehicle reidentification. <em>TNNLS</em>, <em>35</em>(11), 15543–15553.
(<a href="https://doi.org/10.1109/TNNLS.2023.3288139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reidentification (Re-id) of vehicles in a multicamera system is an essential process for traffic control automation. Previously, there have been efforts to reidentify vehicles based on shots of images with identity (id) labels, where the model training relies on the quality and quantity of the labels. However, labeling vehicle ids is a labor-intensive procedure. Instead of relying on expensive labels, we propose to exploit camera and tracklet ids that are automatically obtainable during a Re-id dataset construction. In this article, we present weakly supervised contrastive learning (WSCL) and domain adaptation (DA) techniques using camera and tracklet ids for unsupervised vehicle Re-id. We define each camera id as a subdomain and tracklet id as a label of a vehicle within each subdomain, i.e., weak label in the Re-id scenario. Within each subdomain, contrastive learning using tracklet ids is applied to learn a representation of vehicles. Then, DA is performed to match vehicle ids across the subdomains. We demonstrate the effectiveness of our method for unsupervised vehicle Re-id using various benchmarks. Experimental results show that the proposed method outperforms the recent state-of-the-art unsupervised Re-id methods. The source code is publicly available on https://github.com/andreYoo/WSCL_VeReid .},
  archive      = {J_TNNLS},
  author       = {Jongmin Yu and Hyeontaek Oh and Minkyung Kim and Junsik Kim},
  doi          = {10.1109/TNNLS.2023.3288139},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15543-15553},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weakly supervised contrastive learning for unsupervised vehicle reidentification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visually guided sound source separation with audio-visual
predictive coding. <em>TNNLS</em>, <em>35</em>(11), 15528–15542. (<a
href="https://doi.org/10.1109/TNNLS.2023.3288022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The framework of visually guided sound source separation generally consists of three parts: visual feature extraction, multimodal feature fusion, and sound signal processing. An ongoing trend in this field has been to tailor involved visual feature extractor for informative visual guidance and separately devise module for feature fusion, while utilizing U-Net by default for sound analysis. However, such a divide-and-conquer paradigm is parameter-inefficient and, meanwhile, may obtain suboptimal performance as jointly optimizing and harmonizing various model components is challengeable. By contrast, this article presents a novel approach, dubbed audio-visual predictive coding (AVPC), to tackle this task in a parameter-efficient and more effective manner. The network of AVPC features a simple ResNet-based video analysis network for deriving semantic visual features, and a predictive coding (PC)-based sound separation network that can extract audio features, fuse multimodal information, and predict sound separation masks in the same architecture. By iteratively minimizing the prediction error between features, AVPC integrates audio and visual information recursively, leading to progressively improved performance. In addition, we develop a valid self-supervised learning strategy for AVPC via copredicting two audio-visual representations of the same sound source. Extensive evaluations demonstrate that AVPC outperforms several baselines in separating musical instrument sounds, while reducing the model size significantly. Code is available at: https://github.com/zjsong/Audio-Visual-Predictive-Coding .},
  archive      = {J_TNNLS},
  author       = {Zengjie Song and Zhaoxiang Zhang},
  doi          = {10.1109/TNNLS.2023.3288022},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15528-15542},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Visually guided sound source separation with audio-visual predictive coding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stabilization and synchronization of neural networks via
impulsive adaptive control. <em>TNNLS</em>, <em>35</em>(11),
15517–15527. (<a
href="https://doi.org/10.1109/TNNLS.2023.3287997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the stabilization and synchronization problems of coupled neural networks (NNs) via an impulsive adaptive control (IAC) strategy. Unlike the traditional fixed-gain-based impulsive methods, a novel discrete-time-based adaptive updating law for the impulsive gain is designed to maintain the stabilization and synchronization performance of the coupled NNs, where the adaptive generator only intermittently updates its data at the impulsive instants. Several stabilization and synchronization criteria for the coupled NNs are established based on the impulsive adaptive feedback protocols. Additionally, the corresponding convergence analysis are also provided. Finally, the effectiveness of the obtained theoretical results is illustrated using two comparison simulation examples.},
  archive      = {J_TNNLS},
  author       = {Xuegang Tan and Wangli He and Jinde Cao and Tingwen Huang},
  doi          = {10.1109/TNNLS.2023.3287997},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15517-15527},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stabilization and synchronization of neural networks via impulsive adaptive control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). NN-based reinforcement learning optimal control for
inequality-constrained nonlinear discrete-time systems with
disturbances. <em>TNNLS</em>, <em>35</em>(11), 15507–15516. (<a
href="https://doi.org/10.1109/TNNLS.2023.3287881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on actor-critic neural networks (NNs), an optimal controller is proposed for solving the constrained control problem of an affine nonlinear discrete-time system with disturbances. The actor NNs provide the control signals and the critic NNs work as the performance indicators of the controller. By converting the original state constraints into new input constraints and state constraints, the penalty functions are introduced into the cost function, and then the constrained optimal control problem is transformed into an unconstrained one. Further, the relationship between the optimal control input and worst-case disturbance is obtained using the Game theory. With Lyapunov stability theory, the control signals are ensured to be uniformly ultimately bounded (UUB). Finally, the effectiveness of the control algorithms is tested through a numeral simulation using a third-order dynamic system.},
  archive      = {J_TNNLS},
  author       = {Shu Li and Liang Ding and Miao Zheng and Zixuan Liu and Xinyu Li and Huaiguang Yang and Haibo Gao and Zongquan Deng},
  doi          = {10.1109/TNNLS.2023.3287881},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15507-15516},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {NN-based reinforcement learning optimal control for inequality-constrained nonlinear discrete-time systems with disturbances},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal–frequency attention focusing for time series
extrinsic regression via auxiliary task. <em>TNNLS</em>,
<em>35</em>(11), 15494–15506. (<a
href="https://doi.org/10.1109/TNNLS.2023.3287318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series extrinsic regression (TSER) aims at predicting numeric values based on the knowledge of the entire time series. The key to solving the TSER problem is to extract and use the most representative and contributed information from raw time series. To build a regression model that focuses on those information suitable for the extrinsic regression characteristic, there are two major issues to be addressed. That is, how to quantify the contributions of those information extracted from raw time series and then how to focus the attention of the regression model on those critical information to improve the model’s regression performance. In this article, a multitask learning framework called temporal-frequency auxiliary task (TFAT) is designed to solve the mentioned problems. To explore the integral information from the time and frequency domains, we decompose the raw time series into multiscale subseries in various frequencies via a deep wavelet decomposition network. To address the first problem, the transformer encoder with the multihead self-attention mechanism is integrated in our TFAT framework to quantify the contribution of temporal–frequency information. To address the second problem, an auxiliary task in a manner of self-supervised learning is proposed to reconstruct the critical temporal–frequency features so as to focusing the regression model’s attention on those essential information for facilitating TSER performance. We estimated three kinds of attention distribution on those temporal–frequency features to perform auxiliary task. To evaluate the performances of our method under various application scenarios, the experiments are carried out on the 12 datasets of the TSER problem. Also, ablation studies are used to examine the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Lei Ren and Tingyu Mo and Xuejun Cheng and Xi Li},
  doi          = {10.1109/TNNLS.2023.3287318},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15494-15506},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Temporal–Frequency attention focusing for time series extrinsic regression via auxiliary task},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual attention relation network with fine-tuning for
few-shot EEG motor imagery classification. <em>TNNLS</em>,
<em>35</em>(11), 15479–15493. (<a
href="https://doi.org/10.1109/TNNLS.2023.3287181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, motor imagery (MI) electroencephalography (EEG) classification techniques using deep learning have shown improved performance over conventional techniques. However, improving the classification accuracy on unseen subjects is still challenging due to intersubject variability, scarcity of labeled unseen subject data, and low signal-to-noise ratio (SNR). In this context, we propose a novel two-way few-shot network able to efficiently learn how to learn representative features of unseen subject categories and classify them with limited MI EEG data. The pipeline includes an embedding module that learns feature representations from a set of signals, a temporal-attention module to emphasize important temporal features, an aggregation-attention module for key support signal discovery, and a relation module for final classification based on relation scores between a support set and a query signal. In addition to the unified learning of feature similarity and a few-shot classifier, our method can emphasize informative features in support data relevant to the query, which generalizes better on unseen subjects. Furthermore, we propose to fine-tune the model before testing by arbitrarily sampling a query signal from the provided support set to adapt to the distribution of the unseen subject. We evaluate our proposed method with three different embedding modules on cross-subject and cross-dataset classification tasks using brain–computer interface (BCI) competition IV 2a, 2b, and GIST datasets. Extensive experiments show that our model significantly improves over the baselines and outperforms existing few-shot approaches.},
  archive      = {J_TNNLS},
  author       = {Sion An and Soopil Kim and Philip Chikontwe and Sang Hyun Park},
  doi          = {10.1109/TNNLS.2023.3287181},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15479-15493},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual attention relation network with fine-tuning for few-shot EEG motor imagery classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics embedded graph convolution neural network for power
flow calculation considering uncertain injections and topology.
<em>TNNLS</em>, <em>35</em>(11), 15467–15478. (<a
href="https://doi.org/10.1109/TNNLS.2023.3287028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic analysis tool is important to quantify the impacts of the uncertainties on power system operations. However, the repetitive calculations of power flow are time-consuming. To address this issue, data-driven approaches are proposed but they are not robust to the uncertain injections and varying topology. This article proposes a model-driven graph convolution neural network (MD-GCN) for power flow calculation with high-computational efficiency and good robustness to topology changes. Compared with the basic graph convolution neural network (GCN), the construction of MD-GCN considers the physical connection relationships among different nodes. This is achieved by embedding the linearized power flow model into the layer-wise propagation. Such a structure enhances the interpretability of the network forward propagation. To ensure that enough features are extracted in MD-GCN, a new input feature construction method with multiple neighborhood aggregations and a global pooling layer are developed. This allows us to integrate both global features and neighborhood features, yielding the complete features representation of the system-wide impacts on every single node. Numerical results on the IEEE 30-bus, 57-bus, 118-bus, and 1354-bus systems demonstrate that the proposed method achieves much better performance as compared to other approaches in the presence of uncertain power injections and system topology.},
  archive      = {J_TNNLS},
  author       = {Maosheng Gao and Juan Yu and Zhifang Yang and Junbo Zhao},
  doi          = {10.1109/TNNLS.2023.3287028},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15467-15478},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Physics embedded graph convolution neural network for power flow calculation considering uncertain injections and topology},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RORNet: Partial-to-partial registration network with
reliable overlapping representations. <em>TNNLS</em>, <em>35</em>(11),
15453–15466. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional point cloud registration is an important field in computer vision. Recently, due to the increasingly complex scenes and incomplete observations, many partial-overlap registration methods based on overlap estimation have been proposed. These methods heavily rely on the extracted overlapping regions with their performances greatly degraded when the overlapping region extraction underperforms. To solve this problem, we propose a partial-to-partial registration network (RORNet) to find reliable overlapping representations from the partially overlapping point clouds and use these representations for registration. The idea is to select a small number of key points called reliable overlapping representations from the estimated overlapping points, reducing the side effect of overlap estimation errors on registration. Although it may filter out some inliers, the inclusion of outliers has a much bigger influence than the omission of inliers on the registration task. The RORNet is composed of overlapping points’ estimation module and representations’ generation module. Different from the previous methods of direct registration after extraction of overlapping areas, RORNet adds the step of extracting reliable representations before registration, where the proposed similarity matrix downsampling method is used to filter out the points with low similarity and retain reliable representations, and thus reduce the side effects of overlap estimation errors on the registration. Besides, compared with previous similarity-based and score-based overlap estimation methods, we use the dual-branch structure to combine the benefits of both, which is less sensitive to noise. We perform overlap estimation experiments and registration experiments on the ModelNet40 dataset, outdoor large scene dataset KITTI, and natural data Stanford Bunny dataset. The experimental results demonstrate that our method is superior to other partial registration methods. Our code is available at https://github.com/superYuezhang/RORNet .},
  archive      = {J_TNNLS},
  author       = {Yue Wu and Yue Zhang and Wenping Ma and Maoguo Gong and Xiaolong Fan and Mingyang Zhang and A. K. Qin and Qiguang Miao},
  doi          = {10.1109/TNNLS.2023.3286943},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15453-15466},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RORNet: Partial-to-partial registration network with reliable overlapping representations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Information recovery-driven deep incomplete multiview
clustering network. <em>TNNLS</em>, <em>35</em>(11), 15442–15452. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multiview clustering (IMC) is a hot and emerging topic. It is well known that unavoidable data incompleteness greatly weakens the effective information of multiview data. To date, existing IMC methods usually bypass unavailable views according to prior missing information, which is considered a second-best scheme based on evasion. Other methods that attempt to recover missing information are mostly applicable to specific two-view datasets. To handle these problems, in this article, we propose an information-recovery-driven-deep IMC network, termed as RecFormer. Concretely, a two-stage autoencoder network with self-attention structure is built to synchronously extract high-level semantic representations of multiple views and recover the missing data. Besides, we develop a recurrent graph reconstruction mechanism that cleverly leverages the restored views to promote representation learning and further data reconstruction. Visualization of recovery results are given and sufficient experimental results confirm that our RecFormer has obvious advantages over other top methods.},
  archive      = {J_TNNLS},
  author       = {Chengliang Liu and Jie Wen and Zhihao Wu and Xiaoling Luo and Chao Huang and Yong Xu},
  doi          = {10.1109/TNNLS.2023.3286918},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15442-15452},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Information recovery-driven deep incomplete multiview clustering network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangling stochastic PDE dynamics for unsupervised video
prediction. <em>TNNLS</em>, <em>35</em>(11), 15427–15441. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised video prediction aims to predict future outcomes based on the observed video frames, thus removing the need for supervisory annotations. This research task has been argued as a key component of intelligent decision-making systems, as it presents the potential capacities of modeling the underlying patterns of videos. Essentially, the challenge of video prediction is to effectively model the complex spatiotemporal and often uncertain dynamics of high-dimensional video data. In this context, an appealing way of modeling spatiotemporal dynamics is to explore prior physical knowledge, such as partial differential equations (PDEs). In this article, considering real-world video data as a partly observed stochastic environment, we introduce a new stochastic PDE predictor (SPDE-predictor), which models the spatiotemporal dynamics by approximating a generalized form of PDEs while dealing with the stochasticity. A second contribution is that we disentangle the high-dimensional video prediction into low-level dimensional factors of variations: time-varying stochastic PDE dynamics and time-invariant content factors. Extensive experiments on four various video datasets show that SPDE video prediction model (SPDE-VP) outperforms both deterministic and stochastic state-of-the-art methods. Ablation studies highlight our superiority driven by both PDE dynamics modeling and disentangled representation learning and their relevance in long-term video prediction.},
  archive      = {J_TNNLS},
  author       = {Xinheng Wu and Jie Lu and Zheng Yan and Guangquan Zhang},
  doi          = {10.1109/TNNLS.2023.3286890},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15427-15441},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disentangling stochastic PDE dynamics for unsupervised video prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast broad multiview multi-instance multilabel learning
(FBM3L) with viewwise intercorrelation. <em>TNNLS</em>, <em>35</em>(11),
15415–15426. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview multi-instance multilabel learning (M3L) is a popular research topic during the past few years in modeling complex real-world objects such as medical images and subtitled video. However, existing M3L methods suffer from relatively low accuracy and training efficiency for large datasets due to several issues: 1) the viewwise intercorrelation (i.e., the correlations of instances and/or bags between different views) are neglected; 2) the diverse correlations (e.g., viewwise intercorrelation, interinstance correlation, and interlabel correlation) are not jointly considered; and 3) high computation burden for training process over bags, instances, and labels across different views. To resolve these issues, a novel framework called fast broad M3L (FBM3L) is proposed with three innovations: 1) utilization of viewwise intercorrelation for better modeling of M3L tasks while existing M3L methods have not considered; 2) based on graph convolutional network (GCN) and broad learning system (BLS), a viewwise subnetwork is newly designed to achieve joint learning among the diverse correlations; and 3) under BLS platform, FBM3L can learn multiple subnetworks jointly across all views with significantly less training time. Experiments show that FBM3L is highly competitive (or even better than) in all evaluation metrics [up to 64% in average precision (AP)] and much faster than most M3L (or MIML) methods (up to 1030 times), especially on large multiview datasets (≥260 K objects).},
  archive      = {J_TNNLS},
  author       = {Qi Lai and Chi-Man Vong and Jianhang Zhou and Yimin Zhou and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2023.3286876},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15415-15426},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast broad multiview multi-instance multilabel learning (FBM3L) with viewwise intercorrelation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exponential synchronization of coupled inertial neural
networks with hybrid delays and stochastic impulses. <em>TNNLS</em>,
<em>35</em>(11), 15402–15414. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synchronization problem of the coupled delayed inertial neural networks (DINNs) with stochastic delayed impulses is studied. Based on the properties of stochastic impulses and the definition of average impulsive interval (AII), some synchronization criteria of the considered DINNs are obtained in this article. In addition, compared with previous related works, the requirement on the relationship among the impulsive time intervals, system delays, and impulsive delays is removed. Furthermore, the potential effect of impulsive delay is studied by rigorous mathematical proof. It is shown that within a certain range, the larger the impulsive delay, the faster the system converges. Numerical examples are provided to show the correctness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Lulu Li and Qian Cui and Jinde Cao and Jianlong Qiu and Yifan Sun},
  doi          = {10.1109/TNNLS.2023.3286825},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15402-15414},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential synchronization of coupled inertial neural networks with hybrid delays and stochastic impulses},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative learning and style-adaptive pooling network
for perceptual evaluation of arbitrary style transfer. <em>TNNLS</em>,
<em>35</em>(11), 15387–15401. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the research of arbitrary style transfer (AST) has achieved great progress in recent years, few studies pay special attention to the perceptual evaluation of AST images that are usually influenced by complicated factors, such as structure-preserving, style similarity, and overall vision (OV). Existing methods rely on elaborately designed hand-crafted features to obtain quality factors and apply a rough pooling strategy to evaluate the final quality. However, the importance weights between the factors and the final quality will lead to unsatisfactory performances by simple quality pooling. In this article, we propose a learnable network, named collaborative learning and style-adaptive pooling network (CLSAP-Net) to better address this issue. The CLSAP-Net contains three parts, i.e., content preservation estimation network (CPE-Net), style resemblance estimation network (SRE-Net), and OV target network (OVT-Net). Specifically, CPE-Net and SRE-Net use the self-attention mechanism and a joint regression strategy to generate reliable quality factors for fusion and weighting vectors for manipulating the importance weights. Then, grounded on the observation that style type can influence human judgment of the importance of different factors, our OVT-Net utilizes a novel style-adaptive pooling strategy guiding the importance weights of factors to collaboratively learn the final quality based on the trained CPE-Net and SRE-Net parameters. In our model, the quality pooling process can be conducted in a self-adaptive manner because the weights are generated after understanding the style type. The effectiveness and robustness of the proposed CLSAP-Net are well validated by extensive experiments on the existing AST image quality assessment (IQA) databases. Our code will be released at https://github.com/Hangwei-Chen/CLSAP-Net},
  archive      = {J_TNNLS},
  author       = {Hangwei Chen and Feng Shao and Xiongli Chai and Qiuping Jiang and Xiangchao Meng and Yo-Sung Ho},
  doi          = {10.1109/TNNLS.2023.3286542},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15387-15401},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collaborative learning and style-adaptive pooling network for perceptual evaluation of arbitrary style transfer},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). An adaptive deep metric learning loss function for
class-imbalance learning via intraclass diversity and interclass
distillation. <em>TNNLS</em>, <em>35</em>(11), 15372–15386. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning (DML) has been widely applied in various tasks (e.g., medical diagnosis and face recognition) due to the effective extraction of discriminant features via reducing data overlapping. However, in practice, these tasks also easily suffer from two class-imbalance learning (CIL) problems: data scarcity and data density, causing misclassification. Existing DML losses rarely consider these two issues, while CIL losses cannot reduce data overlapping and data density. In fact, it is a great challenge for a loss function to mitigate the impact of these three issues simultaneously, which is the objective of our proposed intraclass diversity and interclass distillation (IDID) loss with adaptive weight in this article. IDID-loss generates diverse features within classes regardless of the class sample size (to alleviate the issues of data scarcity and data density) and simultaneously preserves the semantic correlations between classes using learnable similarity when pushing different classes away from each other (to reduce overlapping). In summary, our IDID-loss provides three advantages: 1) it can simultaneously mitigate all the three issues while DML and CIL losses cannot; 2) it generates more diverse and discriminant feature representations with higher generalization ability, compared with DML losses; and 3) it provides a larger improvement on the classes of data scarcity and density with a smaller sacrifice on easy class accuracy, compared with CIL losses. Experimental results on seven public real-world datasets show that our IDID-loss achieves the best performances in terms of G-mean, F1-score, and accuracy when compared with both state-of-the-art (SOTA) DML and CIL losses. In addition, it gets rid of the time-consuming fine-tuning process over the hyperparameters of loss function.},
  archive      = {J_TNNLS},
  author       = {Jie Du and Xiaoci Zhang and Peng Liu and Chi-Man Vong and Tianfu Wang},
  doi          = {10.1109/TNNLS.2023.3286484},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15372-15386},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An adaptive deep metric learning loss function for class-imbalance learning via intraclass diversity and interclass distillation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Self-lateral propagation elevates synaptic modifications in
spiking neural networks for the efficient spatial and temporal
classification. <em>TNNLS</em>, <em>35</em>(11), 15359–15371. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain’s mystery for efficient and intelligent computation hides in the neuronal encoding, functional circuits, and plasticity principles in natural neural networks. However, many plasticity principles have not been fully incorporated into artificial or spiking neural networks (SNNs). Here, we report that incorporating a novel feature of synaptic plasticity found in natural networks, whereby synaptic modifications self-propagate to nearby synapses, named self-lateral propagation (SLP), could further improve the accuracy of SNNs in three benchmark spatial and temporal classification tasks. The SLP contains lateral pre ( ${\mathrm{ SLP}}_{\mathrm{ pre}}$ ) and lateral post ( ${\mathrm{ SLP}}_{\mathrm{ post}}$ ) synaptic propagation, describing the spread of synaptic modifications among output synapses made by axon collaterals or among converging synapses on the postsynaptic neuron, respectively. The SLP is biologically plausible and can lead to a coordinated synaptic modification within layers that endow higher efficiency without losing much accuracy. Furthermore, the experimental results showed the impressive role of SLP in sharpening the normal distribution of synaptic weights and broadening the more uniform distribution of misclassified samples, which are both considered essential for understanding the learning convergence and network generalization of neural networks.},
  archive      = {J_TNNLS},
  author       = {Tielin Zhang and Qingyu Wang and Bo Xu},
  doi          = {10.1109/TNNLS.2023.3286458},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15359-15371},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-lateral propagation elevates synaptic modifications in spiking neural networks for the efficient spatial and temporal classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representation-enhanced status replay network for
multisource remote-sensing image classification. <em>TNNLS</em>,
<em>35</em>(11), 15346–15358. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning-based methods are widely used in multisource remote-sensing image classification, and the improvement in their performance confirms the effectiveness of deep learning for classification tasks. However, the inherent underlying problems of deep-learning models still hinder the further improvement of classification accuracy. For example, after multiple rounds of optimization learning, representation bias and classifier bias are accumulated, which prevents the further optimization of network performance. In addition, the imbalance of fusion information among multisource images also leads to insufficient information interaction throughout the fusion process, thus making it difficult to fully utilize the complementary information of multisource data. To address these issues, a Representation-enhanced Status Replay Network (RSRNet) is proposed. First, a dual augmentation including modal augmentation and semantic augmentation is proposed to enhance the transferability and discreteness of feature representation, to reduce the impact of representation bias in the feature extractor. Then, to alleviate the classifier bias and maintain the stability of the decision boundary, a status replay strategy (SRS) is built to regulate the learning and optimization of the classifier. Finally, aiming to improve the interactivity of modal fusion, a novel cross-modal interactive fusion (CMIF) method is employed to jointly optimize the parameters of different branches by combining multisource information. Quantitative and qualitative results on three datasets demonstrate the superiority of RSRNet in multisource remote-sensing image classification, and its outperformance compared with other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Junjie Wang and Wei Li and Yinjian Wang and Ran Tao and Qian Du},
  doi          = {10.1109/TNNLS.2023.3286422},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15346-15358},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Representation-enhanced status replay network for multisource remote-sensing image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor learning meets dynamic anchor learning: From complete
to incomplete multiview clustering. <em>TNNLS</em>, <em>35</em>(11),
15332–15345. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering (MVC), which can dexterously uncover the underlying intrinsic clustering structures of the data, has been particularly attractive in recent years. However, previous methods are designed for either complete or incomplete multiview only, without a unified framework that handles both tasks simultaneously. To address this issue, we propose a unified framework to efficiently tackle both tasks in approximately linear complexity, which integrates tensor learning to explore the inter-view low-rankness and dynamic anchor learning to explore the intra-view low-rankness for scalable clustering (TDASC). Specifically, TDASC efficiently learns smaller view-specific graphs by anchor learning, which not only explores the diversity embedded in multiview data, but also yields approximately linear complexity. Meanwhile, unlike most current approaches that only focus on pair-wise relationships, the proposed TDASC incorporates multiple graphs into an inter-view low-rank tensor, which elegantly models the high-order correlations across views and further guides the anchor learning. Extensive experiments on both complete and incomplete multiview datasets clearly demonstrate the effectiveness and efficiency of TDASC compared with several state-of-the-art techniques.},
  archive      = {J_TNNLS},
  author       = {Yongyong Chen and Xiaojia Zhao and Zheng Zhang and Youfa Liu and Jingyong Su and Yicong Zhou},
  doi          = {10.1109/TNNLS.2023.3286430},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15332-15345},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tensor learning meets dynamic anchor learning: From complete to incomplete multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiamondNet: A neural-network-based heterogeneous sensor
attentive fusion for human activity recognition. <em>TNNLS</em>,
<em>35</em>(11), 15321–15331. (<a
href="https://doi.org/10.1109/TNNLS.2023.3285547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of intelligent sensors integrated into mobile devices, fine-grained human activity recognition (HAR) based on lightweight sensors has emerged as a useful tool for personalized applications. Although shallow and deep learning algorithms have been proposed for HAR problems in the past decades, these methods have limited capability to exploit semantic features from multiple sensor types. To address this limitation, we propose a novel HAR framework, DiamondNet, which can create heterogeneous multisensor modalities, denoise, extract, and fuse features from a fresh perspective. In DiamondNet, we leverage multiple 1-D convolutional denoising autoencoders (1-D-CDAEs) to extract robust encoder features. We further introduce an attention-based graph convolutional network to construct new heterogeneous multisensor modalities, which adaptively exploit the potential relationship between different sensors. Moreover, the proposed attentive fusion subnet, which jointly employs a global-attention mechanism and shallow features, effectively calibrates different-level features of multiple sensor modalities. This approach amplifies informative features and provides a comprehensive and robust perception for HAR. The efficacy of the DiamondNet framework is validated on three public datasets. The experimental results demonstrate that our proposed DiamondNet outperforms other state-of-the-art baselines, achieving remarkable and consistent accuracy improvements. Overall, our work introduces a new perspective on HAR, leveraging the power of multiple sensor modalities and attention mechanisms to significantly improve the performance.},
  archive      = {J_TNNLS},
  author       = {Yida Zhu and Haiyong Luo and Runze Chen and Fang Zhao},
  doi          = {10.1109/TNNLS.2023.3285547},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15321-15331},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DiamondNet: A neural-network-based heterogeneous sensor attentive fusion for human activity recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing for in-memory deep learning with emerging memory
technology. <em>TNNLS</em>, <em>35</em>(11), 15306–15320. (<a
href="https://doi.org/10.1109/TNNLS.2023.3285488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory deep learning executes neural network models where they are stored, thus avoiding long-distance communication between memory and computation units, resulting in considerable savings in energy and time. In-memory deep learning has already demonstrated orders of magnitude higher performance density and energy efficiency. The use of emerging memory technology (EMT) promises to increase density, energy, and performance even further. However, EMT is intrinsically unstable, resulting in random data read fluctuations. This can translate to nonnegligible accuracy loss, potentially nullifying the gains. In this article, we propose three optimization techniques that can mathematically overcome the instability problem of EMT. They can improve the accuracy of the in-memory deep learning model while maximizing its energy efficiency. Experiments show that our solution can fully recover most models’ state-of-the-art (SOTA) accuracy and achieves at least an order of magnitude higher energy efficiency than the SOTA.},
  archive      = {J_TNNLS},
  author       = {Zhehui Wang and Tao Luo and Rick Siow Mong Goh and Wei Zhang and Weng-Fai Wong},
  doi          = {10.1109/TNNLS.2023.3285488},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15306-15320},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimizing for in-memory deep learning with emerging memory technology},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BoostXML: Gradient boosting for extreme multilabel text
classification with tail labels. <em>TNNLS</em>, <em>35</em>(11),
15292–15305. (<a
href="https://doi.org/10.1109/TNNLS.2023.3285294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel learning involving hundreds of thousands or even millions of labels is referred to as extreme multilabel learning (XML), in which the labels often follow a power-law distribution with the majority occurring in very few data points as tail labels. Recent years have witnessed the intensive use of deep-learning methods for high-performance XML, but they are typically optimized for the head labels with abundant training instances and less consider the performance on tail labels, which, however, like the needles in haystacks, are often the focus of attention in real-life applications. In light of this, we present BoostXML, a deep learning-based XML method for extreme multilabel text classification, enhanced greatly by gradient boosting. In BoostXML, we pay more attention to tail labels in each Boosting Step by optimizing the residual mostly from unfitted training instances with tail labels. A Corrective Step is further proposed to avoid the mismatching between the text encoder and weak learners during optimization, which reduces the risk of falling into local optima and improves model performance. A Pretraining Step is also introduced in the initial stage of BoostXML to avoid exorbitant bias to tail labels. Extensive experiments on five benchmark datasets with state-of-the-art baselines demonstrate the advantage of BoostXML in tail-label prediction.},
  archive      = {J_TNNLS},
  author       = {Fengzhi Li and Yuan Zuo and Hao Lin and Junjie Wu},
  doi          = {10.1109/TNNLS.2023.3285294},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15292-15305},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BoostXML: Gradient boosting for extreme multilabel text classification with tail labels},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning feature embedding refiner for solving vehicle
routing problems. <em>TNNLS</em>, <em>35</em>(11), 15279–15291. (<a
href="https://doi.org/10.1109/TNNLS.2023.3285077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the encoder–decoder structure is widely used in the recent neural construction methods for learning to solve vehicle routing problems (VRPs), they are less effective in searching solutions due to deterministic feature embeddings and deterministic probability distributions. In this article, we propose the feature embedding refiner (FER) with a novel and generic encoder–refiner–decoder structure to boost the existing encoder–decoder structured deep models. It is model-agnostic that the encoder and the decoder can be from any pretrained neural construction method. Regarding the introduced refiner network, we design its architecture by combining the standard gated recurrent units (GRU) cell with two new layers, i.e., an accumulated graph attention (AGA) layer and a gated nonlinear (GNL) layer. The former extracts dynamic graph topological information of historical solutions stored in a diversified solution pool to generate aggregated pool embeddings that are further improved by the GRU, and the latter adaptively refines the feature embeddings from the encoder with the guidance of the improved pool embeddings. To this end, our FER allows current neural construction methods to not only iteratively refine the feature embeddings for boarder search range but also dynamically update the probability distributions for more diverse search. We apply FER to two prevailing neural construction methods including attention model (AM) and policy optimization with multiple optima (POMO) to solve the traveling salesman problem (TSP) and the capacitated VRP (CVRP). Experimental results show that our method achieves lower gaps and better generalization than the original ones and also exhibits competitive performance to the state-of-the-art neural improvement methods.},
  archive      = {J_TNNLS},
  author       = {Jingwen Li and Yining Ma and Zhiguang Cao and Yaoxin Wu and Wen Song and Jie Zhang and Yeow Meng Chee},
  doi          = {10.1109/TNNLS.2023.3285077},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15279-15291},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning feature embedding refiner for solving vehicle routing problems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Which samples should be learned first: Easy or hard?
<em>TNNLS</em>, <em>35</em>(11), 15264–15278. (<a
href="https://doi.org/10.1109/TNNLS.2023.3284430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treating each training sample unequally is prevalent in many machine-learning tasks. Numerous weighting schemes have been proposed. Some schemes take the easy-first mode, whereas others take the hard-first one. Naturally, an interesting yet realistic question is raised. Given a new learning task, which samples should be learned first, easy or hard? To answer this question, both theoretical analysis and experimental verification are conducted. First, a general objective function is proposed and the optimal weight can be derived from it, which reveals the relationship between the difficulty distribution of the training set and the priority mode. Two novel findings are subsequently obtained: besides the easy-first and hard-first modes, there are two other typical modes, namely, medium-first and two-ends-first; the priority mode may be varied if the difficulty distribution of the training set changes greatly. Second, inspired by the findings, a flexible weighting scheme (FlexW) is proposed for selecting the optimal priority mode when there is no prior knowledge or theoretical clues. The four priority modes can be flexibly switched in the proposed solution, thus suitable for various scenarios. Third, a wide range of experiments is conducted to verify the effectiveness of our proposed FlexW and further compare the weighting schemes in different modes under various learning scenarios. On the basis of these works, reasonable and comprehensive answers are obtained for the easy-or-hard question.},
  archive      = {J_TNNLS},
  author       = {Xiaoling Zhou and Ou Wu},
  doi          = {10.1109/TNNLS.2023.3284430},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15264-15278},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Which samples should be learned first: Easy or hard?},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explore the influence of shallow information on point cloud
registration. <em>TNNLS</em>, <em>35</em>(11), 15251–15263. (<a
href="https://doi.org/10.1109/TNNLS.2023.3284035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction is a key step for deep-learning-based point cloud registration. In the correspondence-free point cloud registration task, the previous work commonly aggregates deep information for global feature extraction and numerous shallow information which is positive to point cloud registration will be ignored with the deepening of the neural network. Shallow information tends to represent the structural information of the point cloud, while deep information tends to represent the semantic information of the point cloud. In addition, fusing information of different dimensions is conducive to making full use of shallow information. Inspired by this, we verify shallow information in the middle layers can bring a positive impact on the point cloud registration task. We design various architectures to combine shallow information and deep information to extract global features for point cloud registration. Experimental results on the ModelNet40 dataset illustrate that feature extractors that incorporate shallow information will bring positive performance.},
  archive      = {J_TNNLS},
  author       = {Wenping Ma and Mingyu Yue and Yue Wu and Yongzhe Yuan and Hao Zhu and Biao Hou and Licheng Jiao},
  doi          = {10.1109/TNNLS.2023.3284035},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15251-15263},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Explore the influence of shallow information on point cloud registration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive prototype interaction network for few-shot
knowledge graph completion. <em>TNNLS</em>, <em>35</em>(11),
15237–15250. (<a
href="https://doi.org/10.1109/TNNLS.2023.3283545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot knowledge graph completion (FKGC), which aims to infer new triples for a relation using only a few reference triples of the relation, has attracted much attention in recent years. Most existing FKGC methods learn a transferable embedding space, where entity pairs belonging to the same relations are close to each other. In real-world knowledge graphs (KGs), however, some relations may involve multiple semantics, and their entity pairs are not always close due to having different meanings. Hence, the existing FKGC methods may yield suboptimal performance when handling multiple semantic relations in the few-shot scenario. To solve this problem, we propose a new method named adaptive prototype interaction network (APINet) for FKGC. Our model consists of two major components: 1) an interaction attention encoder (InterAE) to capture the underlying relational semantics of entity pairs by modeling the interactive information between head and tail entities and 2) an adaptive prototype net (APNet) to generate relation prototypes adaptive to different query triples by extracting query-relevant reference pairs and reducing the data inconsistency between support and query sets. Experimental results on two public datasets demonstrate that APINet outperforms several state-of-the-art FKGC methods. The ablation study demonstrates the rationality and effectiveness of each component of APINet.},
  archive      = {J_TNNLS},
  author       = {Yuling Li and Kui Yu and Yuhong Zhang and Jiye Liang and Xindong Wu},
  doi          = {10.1109/TNNLS.2023.3283545},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15237-15250},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive prototype interaction network for few-shot knowledge graph completion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable integrated motion prediction and planning
with learnable cost function for autonomous driving. <em>TNNLS</em>,
<em>35</em>(11), 15222–15236. (<a
href="https://doi.org/10.1109/TNNLS.2023.3283542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the future states of surrounding traffic participants and planning a safe, smooth, and socially compliant trajectory accordingly are crucial for autonomous vehicles (AVs). There are two major issues with the current autonomous driving system: the prediction module is often separated from the planning module, and the cost function for planning is hard to specify and tune. To tackle these issues, we propose a differentiable integrated prediction and planning (DIPP) framework that can also learn the cost function from data. Specifically, our framework uses a differentiable nonlinear optimizer as the motion planner, which takes as input the predicted trajectories of surrounding agents given by the neural network and optimizes the trajectory for the AV, enabling all operations to be differentiable, including the cost function weights. The proposed framework is trained on a large-scale real-world driving dataset to imitate human driving trajectories in the entire driving scene and validated in both open-loop and closed-loop manners. The open-loop testing results reveal that the proposed method outperforms the baseline methods across a variety of metrics and delivers planning-centric prediction results, allowing the planning module to output trajectories close to those of human drivers. In closed-loop testing, the proposed method outperforms various baseline methods, showing the ability to handle complex urban driving scenarios and robustness against the distributional shift. Importantly, we find that joint training of planning and prediction modules achieves better performance than planning with a separate trained prediction module in both open-loop and closed-loop tests. Moreover, the ablation study indicates that the learnable components in the framework are essential to ensure planning stability and performance. Code and Supplementary Videos are available at https://mczhi.github.io/DIPP/ .},
  archive      = {J_TNNLS},
  author       = {Zhiyu Huang and Haochen Liu and Jingda Wu and Chen Lv},
  doi          = {10.1109/TNNLS.2023.3283542},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15222-15236},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Differentiable integrated motion prediction and planning with learnable cost function for autonomous driving},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A word-level adversarial attack method based on sememes and
an improved quantum-behaved particle swarm optimization. <em>TNNLS</em>,
<em>35</em>(11), 15210–15221. (<a
href="https://doi.org/10.1109/TNNLS.2023.3283308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of textual adversarial attack methods is to replace some words in an input text in order to make the victim model misbehave. This article proposes an effective word-level adversarial attack method based on sememes and an improved quantum-behaved particle swarm optimization (QPSO) algorithm. The sememe-based substitute method, which uses the words sharing the same sememes as the substitutes of the original words, is first employed to form the reduced search space. Then, an improved QPSO algorithm, called historical information-guided QPSO with random drift local attractor (HIQPSO-RD), is proposed to search the reduced search space for adversarial examples. The HIQPSO-RD introduces historical information into the current mean best position of the QPSO, for the purpose of improving the convergence speed of the algorithm, by enhancing its exploration ability and preventing the premature convergence of the swarm. The proposed algorithm uses the random drift local attractor technique to make a good balance between its exploration and exploitation, so that the algorithm can find a better adversarial attack example with low grammaticality and perplexity (PPL). In addition, it employs a two-stage diversity control strategy to enhance the search performance of the algorithm. Experiments on three natural language processing (NLP) datasets, with three commonly used nature language processing models as victim models, show that our method achieves higher attack success rates but lower modification rates than the state-of-the-art adversarial attack methods. Moreover, the results of human evaluations show that adversarial examples generated by our method can better maintain the semantic similarity and grammatical correctness of the original input.},
  archive      = {J_TNNLS},
  author       = {Qidong Chen and Jun Sun and Vasile Palade},
  doi          = {10.1109/TNNLS.2023.3283308},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15210-15221},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A word-level adversarial attack method based on sememes and an improved quantum-behaved particle swarm optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural architecture selection as a nash equilibrium with
batch entanglement. <em>TNNLS</em>, <em>35</em>(11), 15195–15209. (<a
href="https://doi.org/10.1109/TNNLS.2023.3283239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the architecture search process on a supernet and applying a differentiable method to find the importance of architecture are among the leading tools for differentiable neural architectures search (DARTS). One fundamental problem in DARTS is how to discretize or select a single-path architecture from the pretrained one-shot architecture. Previous approaches mainly exploit heuristic or progressive search methods for discretization and selection, which are not efficient and easily trapped by local optimizations. To address these issues, we formulate the task of finding a proper single-path architecture as an architecture game among the edges and operations with the strategies “keep” and “drop” and show that the optimal one-shot architecture is a Nash equilibrium of the architecture game. Then, we propose a novel and effective approach for discretizing and selecting a proper single-path architecture, which is based on extracting the single-path architecture that associates the maximal coefficient of the Nash equilibrium with the strategy “keep” in the architecture game. To further improve the efficiency, we employ a mechanism of entangled Gaussian representation of mini-batches, inspired by the classic Parrondo’s paradox. If some mini-batch formed uncompetitive strategies, the entanglement of mini-batches would ensure the games be combined and, thus, turn into strong ones. We conduct extensive experiments on benchmark datasets and demonstrate that our approach is significantly faster than the state-of-the-art progressive discretizing methods while maintaining competitive performance with higher maximum accuracy.},
  archive      = {J_TNNLS},
  author       = {Qian Li and Chao Xue and Mingming Li and Chun-Guang Li and Chao Ma and Xiaokang Yang},
  doi          = {10.1109/TNNLS.2023.3283239},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15195-15209},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural architecture selection as a nash equilibrium with batch entanglement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Hierarchical graph convolutional network built by
multiscale atlases for brain disorder diagnosis using functional
connectivity. <em>TNNLS</em>, <em>35</em>(11), 15182–15194. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional connectivity network (FCN) data from functional magnetic resonance imaging (fMRI) is increasingly used for the diagnosis of brain disorders. However, state-of-the-art studies used to build the FCN using a single brain parcellation atlas at a certain spatial scale, which largely neglected functional interactions across different spatial scales in hierarchical manners. In this study, we propose a novel framework to perform multiscale FCN analysis for brain disorder diagnosis. We first use a set of well-defined multiscale atlases to compute multiscale FCNs. Then, we utilize biologically meaningful brain hierarchical relationships among the regions in multiscale atlases to perform nodal pooling across multiple spatial scales, namely “Atlas-guided Pooling (AP).” Accordingly, we propose a multiscale-atlases-based hierarchical graph convolutional network (MAHGCN), built on the stacked layers of graph convolution and the AP, for a comprehensive extraction of diagnostic information from multiscale FCNs. Experiments on neuroimaging data from 1792 subjects demonstrate the effectiveness of our proposed method in the diagnoses of Alzheimer’s disease (AD), the prodromal stage of AD [i.e., mild cognitive impairment (MCI)], as well as autism spectrum disorder (ASD), with the accuracy of 88.9%, 78.6%, and 72.7%, respectively. All results show significant advantages of our proposed method over other competing methods. This study not only demonstrates the feasibility of brain disorder diagnosis using resting-state fMRI empowered by deep learning but also highlights that the functional interactions in the multiscale brain hierarchy are worth being explored and integrated into deep learning network architectures for a better understanding of the neuropathology of brain disorders. The codes for MAHGCN are publicly available at “ https://github.com/MianxinLiu/MAHGCN-code .”},
  archive      = {J_TNNLS},
  author       = {Mianxin Liu and Han Zhang and Feng Shi and Dinggang Shen},
  doi          = {10.1109/TNNLS.2023.3282961},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15182-15194},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical graph convolutional network built by multiscale atlases for brain disorder diagnosis using functional connectivity},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised domain-adaptive object detection via
localization regression alignment. <em>TNNLS</em>, <em>35</em>(11),
15170–15181. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain-adaptive object detection uses labeled source domain data and unlabeled target domain data to alleviate the domain shift and reduce the dependence on the target domain data labels. For object detection, the features responsible for classification and localization are different. However, the existing methods basically only consider classification alignment, which is not conducive to cross-domain localization. To address this issue, in this article, we focus on the alignment of localization regression in domain-adaptive object detection and propose a novel localization regression alignment (LRA) method. The idea is that the domain-adaptive localization regression problem can be transformed into a general domain-adaptive classification problem first, and then adversarial learning is applied to the converted classification problem. Specifically, LRA first discretizes the continuous regression space, and the discrete regression intervals are treated as bins. Then, a novel binwise alignment (BA) strategy is proposed through adversarial learning. BA can further contribute to the overall cross-domain feature alignment for object detection. Extensive experiments are conducted on different detectors in various scenarios, and the state-of-the-art performance is achieved; these results demonstrate the effectiveness of our method. The code will be available at: https://github.com/zqpiao/LRA .},
  archive      = {J_TNNLS},
  author       = {Zhengquan Piao and Linbo Tang and Baojun Zhao},
  doi          = {10.1109/TNNLS.2023.3282958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15170-15181},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised domain-adaptive object detection via localization regression alignment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Learning cross-attention discriminators via alternating
time–space transformers for visual tracking. <em>TNNLS</em>,
<em>35</em>(11), 15156–15169. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, visual tracking methods with convolution neural networks (CNNs) have gained great popularity and success. However, the convolution operation of CNNs struggles to relate spatially distant information, which limits the discriminative power of trackers. Very recently, several Transformer-assisted tracking approaches have emerged to alleviate the above issue by combining CNNs with Transformers to enhance the feature representation. In contrast to the methods mentioned above, this article explores a pure Transformer-based model with a novel semi-Siamese architecture. Both the time–space self-attention module used to construct the feature extraction backbone and the cross-attention discriminator used to estimate the response map solely leverage attention without convolution. Inspired by the recent vision transformers (ViTs), we propose the multistage alternating time–space Transformers (ATSTs) to learn robust feature representation. Specifically, temporal and spatial tokens at each stage are alternately extracted and encoded by separate Transformers. Subsequently, a cross-attention discriminator is proposed to directly generate response maps of the search region without additional prediction heads or correlation filters. Experimental results show that our ATST-based model attains favorable results against state-of-the-art convolutional trackers. Moreover, it shows comparable performance with recent “CNN + Transformer” trackers on various benchmarks while our ATST requires significantly less training data.},
  archive      = {J_TNNLS},
  author       = {Wuwei Wang and Ke Zhang and Yu Su and Jingyu Wang and Qi Wang},
  doi          = {10.1109/TNNLS.2023.3282905},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15156-15169},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning cross-attention discriminators via alternating Time–Space transformers for visual tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel k-means framework via constrained relaxation and
spectral rotation. <em>TNNLS</em>, <em>35</em>(11), 15142–15155. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to its simplicity, the traditional $k$ - means (Lloyd heuristic) clustering method plays a vital role in a variety of machine-learning applications. Disappointingly, the Lloyd heuristic is prone to local minima. In this article, we propose $k$ - mRSR, which converts the sum-of-squared error (SSE) (Lloyd) into a combinatorial optimization problem and incorporates a relaxed trace maximization term and an improved spectral rotation term. The main advantage of $k$ - mRSR is that it only needs to solve the membership matrix instead of computing the cluster centers in each iteration. Furthermore, we present a nonredundant coordinate descent method that brings the discrete solution infinitely close to the scaled partition matrix. Two novel findings from the experiments are that $k$ - mRSR can further decrease (increase) the objective function values of the $k$ - means obtained by Lloyd (CD), while Lloyd (CD) cannot decrease (increase) the objective function obtained by $k$ - mRSR. In addition, the results of extensive experiments on 15 datasets indicate that $k$ - mRSR outperforms both Lloyd and CD in terms of the objective function value and outperforms other state-of-the-art methods in terms of clustering performance.},
  archive      = {J_TNNLS},
  author       = {Jingwei Chen and Shiyu Xie and Hongyun Jiang and Hui Yang and Feiping Nie},
  doi          = {10.1109/TNNLS.2023.3282938},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15142-15155},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel k-means framework via constrained relaxation and spectral rotation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative training sample augmentation for enhancing land
cover change detection performance with deep learning neural network.
<em>TNNLS</em>, <em>35</em>(11), 15128–15141. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labeled samples are important in achieving land cover change detection (LCCD) tasks via deep learning techniques with remote sensing images. However, labeling samples for change detection with bitemporal remote sensing images is labor-intensive and time-consuming. Moreover, manually labeling samples between bitemporal images requires professional knowledge for practitioners. To address this problem in this article, an iterative training sample augmentation (ITSA) strategy to couple with a deep learning neural network for improving LCCD performance is proposed here. In the proposed ITSA, we start by measuring the similarity between an initial sample and its four-quarter-overlapped neighboring blocks. If the similarity satisfies a predefined constraint, then a neighboring block will be selected as the potential sample. Next, a neural network is trained with renewed samples and used to predict an intermediate result. Finally, these operations are fused into an iterative algorithm to achieve the training and prediction of a neural network. The performance of the proposed ITSA strategy is verified with some widely used change detection deep learning networks using seven pairs of real remote sensing images. The excellent visual performance and quantitative comparisons from the experiments clearly indicate that detection accuracies of LCCD can be effectively improved when a deep learning network is coupled with the proposed ITSA. For example, compared with some state-of-the-art methods, the quantitative improvement is 0.38%–7.53% in terms of overall accuracy. Moreover, the improvement is robust, generic to both homogeneous and heterogeneous images, and universally adaptive to various neural networks of LCCD. The code will be available at https://github.com/ImgSciGroup/ITSA .},
  archive      = {J_TNNLS},
  author       = {Zhiyong Lv and Haitao Huang and Weiwei Sun and Meng Jia and Jón Atli Benediktsson and Fengrui Chen},
  doi          = {10.1109/TNNLS.2023.3282935},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15128-15141},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative training sample augmentation for enhancing land cover change detection performance with deep learning neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural inference search for multiloss segmentation models.
<em>TNNLS</em>, <em>35</em>(11), 15113–15127. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is vital for many emerging surveillance applications, but current models cannot be relied upon to meet the required tolerance, particularly in complex tasks that involve multiple classes and varied environments. To improve performance, we propose a novel algorithm, neural inference search (NIS), for hyperparameter optimization pertaining to established deep learning segmentation models in conjunction with a new multiloss function. It incorporates three novel search behaviors, i.e., Maximized Standard Deviation Velocity Prediction, Local Best Velocity Prediction, and $n$ -dimensional Whirlpool Search. The first two behaviors are exploratory, leveraging long short-term memory (LSTM)-convolutional neural network (CNN)-based velocity predictions, while the third employs $n$ -dimensional matrix rotation for local exploitation. A scheduling mechanism is also introduced in NIS to manage the contributions of these three novel search behaviors in stages. NIS optimizes learning and multiloss parameters simultaneously. Compared with state-of-the-art segmentation methods and those optimized with other well-known search algorithms, NIS-optimized models show significant improvements across multiple performance metrics on five segmentation datasets. NIS also reliably yields better solutions as compared with a variety of search methods for solving numerical benchmark functions.},
  archive      = {J_TNNLS},
  author       = {Sam Slade and Li Zhang and Haoqian Huang and Houshyar Asadi and Chee Peng Lim and Yonghong Yu and Dezong Zhao and Hanhe Lin and Rong Gao},
  doi          = {10.1109/TNNLS.2023.3282799},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15113-15127},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural inference search for multiloss segmentation models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sentiment analysis: Comprehensive reviews, recent advances,
and open challenges. <em>TNNLS</em>, <em>35</em>(11), 15092–15112. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis (SA) aims to understand the attitudes and views of opinion holders with computers. Previous studies have achieved significant breakthroughs and extensive applications in the past decade, such as public opinion analysis and intelligent voice service. With the rapid development of deep learning, SA based on various modalities has become a research hotspot. However, only individual modality has been analyzed separately, lacking a systematic carding of comprehensive SA methods. Meanwhile, few surveys covering the topic of multimodal SA (MSA) have been explored yet. In this article, we first take the modality as the thread to design a novel framework of SA tasks to provide researchers with a comprehensive understanding of relevant advances in SA. Then, we introduce the general workflows and recent advances of single-modal in detail, discuss the similarities and differences of single-modal SA in data processing and modeling to guide MSA, and summarize the commonly used datasets to provide guidance on data and methods for researchers according to different task types. Next, a new taxonomy is proposed to fill the research gaps in MSA, which is divided into multimodal representation learning and multimodal data fusion. The similarities and differences between these two methods and the latest advances are described in detail, such as dynamic interaction between multimodalities, and the multimodal fusion technologies are further expanded. Moreover, we explore the advanced studies on multimodal alignment, chatbots, and Chat Generative Pre-trained Transformer (ChatGPT) in SA. Finally, we discuss the open research challenges of MSA and provide four potential aspects to improve future works, such as cross-modal contrastive learning and multimodal pretraining models.},
  archive      = {J_TNNLS},
  author       = {Qiang Lu and Xia Sun and Yunfei Long and Zhizezhang Gao and Jun Feng and Tao Sun},
  doi          = {10.1109/TNNLS.2023.3294810},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15092-15112},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sentiment analysis: Comprehensive reviews, recent advances, and open challenges},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning for time-series prediction in IIoT: Progress,
challenges, and prospects. <em>TNNLS</em>, <em>35</em>(11), 15072–15091.
(<a href="https://doi.org/10.1109/TNNLS.2023.3291371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series prediction plays a crucial role in the Industrial Internet of Things (IIoT) to enable intelligent process control, analysis, and management, such as complex equipment maintenance, product quality management, and dynamic process monitoring. Traditional methods face challenges in obtaining latent insights due to the growing complexity of IIoT. Recently, the latest development of deep learning provides innovative solutions for IIoT time-series prediction. In this survey, we analyze the existing deep learning-based time-series prediction methods and present the main challenges of time-series prediction in IIoT. Furthermore, we propose a framework of state-of-the-art solutions to overcome the challenges of time-series prediction in IIoT and summarize its application in practical scenarios, such as predictive maintenance, product quality prediction, and supply chain management. Finally, we conclude with comments on possible future directions for the development of time-series prediction to enable extensible knowledge mining for complex tasks in IIoT.},
  archive      = {J_TNNLS},
  author       = {Lei Ren and Zidi Jia and Yuanjun Laili and Di Huang},
  doi          = {10.1109/TNNLS.2023.3291371},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15072-15091},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning for time-series prediction in IIoT: Progress, challenges, and prospects},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Challenges and opportunities in deep reinforcement learning
with graph neural networks: A comprehensive review of algorithms and
applications. <em>TNNLS</em>, <em>35</em>(11), 15051–15071. (<a
href="https://doi.org/10.1109/TNNLS.2023.3283523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) has empowered a variety of artificial intelligence fields, including pattern recognition, robotics, recommendation systems, and gaming. Similarly, graph neural networks (GNNs) have also demonstrated their superior performance in supervised learning for graph-structured data. In recent times, the fusion of GNN with DRL for graph-structured environments has attracted a lot of attention. This article provides a comprehensive review of these hybrid works. These works can be classified into two categories: 1) algorithmic contributions, where DRL and GNN complement each other with an objective of addressing each other’s shortcomings and 2) application-specific contributions that leverage a combined GNN-DRL formulation to address problems specific to different applications. This fusion effectively addresses various complex problems in engineering and life sciences. Based on the review, we further analyze the applicability and benefits of fusing these two domains, especially in terms of increasing generalizability and reducing computational complexity. Finally, the key challenges in integrating DRL and GNN, and potential future research directions are highlighted, which will be of interest to the broader machine learning community.},
  archive      = {J_TNNLS},
  author       = {Sai Munikoti and Deepesh Agarwal and Laya Das and Mahantesh Halappanavar and Balasubramaniam Natarajan},
  doi          = {10.1109/TNNLS.2023.3283523},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15051-15071},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Challenges and opportunities in deep reinforcement learning with graph neural networks: A comprehensive review of algorithms and applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on hyperlink prediction. <em>TNNLS</em>,
<em>35</em>(11), 15034–15050. (<a
href="https://doi.org/10.1109/TNNLS.2023.3286280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a natural extension of link prediction on graphs, hyperlink prediction aims for the inference of missing hyperlinks in hypergraphs, where a hyperlink can connect more than two nodes. Hyperlink prediction has applications in a wide range of systems, from chemical reaction networks and social communication networks to protein–protein interaction networks. In this article, we provide a systematic and comprehensive survey on hyperlink prediction. We adopt a classical taxonomy from link prediction to classify the existing hyperlink prediction methods into four categories: similarity-based, probability-based, matrix optimization-based, and deep learning-based methods. To compare the performance of methods from different categories, we perform a benchmark study on various hypergraph applications using representative methods from each category. Notably, deep learning-based methods prevail over other methods in hyperlink prediction.},
  archive      = {J_TNNLS},
  author       = {Can Chen and Yang-Yu Liu},
  doi          = {10.1109/TNNLS.2023.3286280},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {11},
  number       = {11},
  pages        = {15034-15050},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on hyperlink prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Influence of imperfections on the operational correctness of
DNN-kWTA model. <em>TNNLS</em>, <em>35</em>(10), 15021–15029. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dual neural network (DNN)-based $k$ -winner-take-all (WTA) model is able to identify the $k$ largest numbers from its $m$ input numbers. When there are imperfections, such as non-ideal step function and Gaussian input noise, in the realization, the model may not output the correct result. This brief analyzes the influence of the imperfections on the operational correctness of the model. Due to the imperfections, it is not efficient to use the original DNN- $k$ WTA dynamics for analyzing the influence. In this regard, this brief first derives an equivalent model to describe the dynamics of the model under the imperfections. From the equivalent model, we derive a sufficient condition for which the model outputs the correct result. Thus, we apply the sufficient condition to design an efficiently estimation method for the probability of the model outputting the correct result. Furthermore, for the inputs with uniform distribution, a closed form expression for the probability value is derived. Finally, we extend our analysis for handling non-Gaussian input noise. Simulation results are provided to validate our theoretical results.},
  archive      = {J_TNNLS},
  author       = {Wenhao Lu and Chi-Sing Leung and John Sum},
  doi          = {10.1109/TNNLS.2023.3281523},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {15021-15029},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Influence of imperfections on the operational correctness of DNN-kWTA model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient discrete clustering with anchor graph.
<em>TNNLS</em>, <em>35</em>(10), 15012–15020. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering (SC) has been applied to analyze varieties of data structures over the past few decades owing to its outstanding breakthrough in graph learning. However, the time-consuming eigenvalue decomposition (EVD) and information loss during relaxation and discretization impact the efficiency and accuracy especially for large-scale data. To address above issues, this brief proposes a simple and fast method named efficient discrete clustering with anchor graph (EDCAG) to circumvent postprocessing by binary label optimization. First of all, sparse anchors are adopted to accelerate graph construction and obtain a parameter-free anchor similarity matrix. Subsequently, inspired by intraclass similarity maximization in SC, we design an intraclass similarity maximization model between anchor–sample layer to cope with anchor graph cut problem and exploit more explicit data structures. Meanwhile, a fast coordinate rising (CR) algorithm is employed to alternatively optimize discrete labels of samples and anchors in designed model. Experimental results show excellent rapidity and competitive clustering effect of EDCAG.},
  archive      = {J_TNNLS},
  author       = {Jingyu Wang and Zhenyu Ma and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3279380},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {15012-15020},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient discrete clustering with anchor graph},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Long-run behavior estimation of temporal boolean networks
with multiple data losses. <em>TNNLS</em>, <em>35</em>(10), 15004–15011.
(<a href="https://doi.org/10.1109/TNNLS.2023.3270450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief devotes to investigating the long-run behavior estimation of temporal Boolean networks (TBNs) with multiple data losses, especially the asymptotical stability. The information transmission is modeled by Bernoulli variables, based on which an augmented system is constructed to facilitate the analysis. A theorem guarantees that the asymptotical stability of the original system can be converted to that of the augmented system. Subsequently, one necessary and sufficient condition is obtained for asymptotical stability. Furthermore, an auxiliary system is derived to study the synchronization issue of the ideal TBNs with normal data transmission and TBNs with multiple data losses, as well as an effective criterion for verifying synchronization. Finally, numerical examples are given to illustrate the validity of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Bowen Li and Qinyao Pan and Jie Zhong and Wenying Xu},
  doi          = {10.1109/TNNLS.2023.3270450},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {15004-15011},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Long-run behavior estimation of temporal boolean networks with multiple data losses},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extraordinarily time- and memory-efficient large-scale
canonical correlation analysis in fourier domain: From shallow to deep.
<em>TNNLS</em>, <em>35</em>(10), 14989–15003. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical correlation analysis (CCA) is a correlation analysis technique that is widely used in statistics and the machine-learning community. However, the high complexity involved in the training process lays a heavy burden on the processing units and memory system, making CCA nearly impractical in large-scale data. To overcome this issue, a novel CCA method that tries to carry out analysis on the dataset in the Fourier domain is developed in this article. Appling Fourier transform on the data, we can convert the traditional eigenvector computation of CCA into finding some predefined discriminative Fourier bases that can be learned with only element-wise dot product and sum operations, without complex time-consuming calculations. As the eigenvalues come from the sum of individual sample products, they can be estimated in parallel. Besides, thanks to the data characteristic of pattern repeatability, the eigenvalues can be well estimated with partial samples. Accordingly, a progressive estimate scheme is proposed, in which the eigenvalues are estimated through feeding data batch by batch until the eigenvalues sequence is stable in order. As a result, the proposed method shows its characteristics of extraordinarily fast and memory efficiencies. Furthermore, we extend this idea to the nonlinear kernel and deep models and obtained satisfactory accuracy and extremely fast training time consumption as expected. An extensive discussion on the fast Fourier transform (FFT)-CCA is made in terms of time and memory efficiencies. Experimental results on several large-scale correlation datasets, such as MNIST8M, X-RAY MICROBEAM SPEECH, and Twitter Users Data, demonstrate the superiority of the proposed algorithm over state-of-the-art (SOTA) large-scale CCA methods, as our proposed method achieves almost same accuracy with the training time of our proposed method being 1000 times faster. This makes our proposed models best practice models for dealing with large-scale correlation datasets. The source code is available at https://github.com/Mrxuzhao/FFTCCA .},
  archive      = {J_TNNLS},
  author       = {Xiang-Jun Shen and Zhaorui Xu and Liangjun Wang and Zechao Li and Guangcan Liu and Jianping Fan and ZhengJun Zha},
  doi          = {10.1109/TNNLS.2023.3282785},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14989-15003},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Extraordinarily time- and memory-efficient large-scale canonical correlation analysis in fourier domain: From shallow to deep},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WPConvNet: An interpretable wavelet packet
kernel-constrained convolutional network for noise-robust fault
diagnosis. <em>TNNLS</em>, <em>35</em>(10), 14974–14988. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has present great diagnostic results in fault diagnosis field. However, the poor interpretability and noise robustness of DL-based methods are still the main factors limiting their wide application in industry. To address these issues, an interpretable wavelet packet kernel-constrained convolutional network (WPConvNet) is proposed for noise-robust fault diagnosis, which combines the feature extraction ability of wavelet bases and the learning ability of convolutional kernels together. First, the wavelet packet convolutional (WPConv) layer is proposed, and constraints are imposed to convolutional kernels, so that each convolution layer is a learnable discrete wavelet transform. Second, a soft threshold activation is proposed to reduce the noise component in feature maps, whose threshold is adaptively learned by estimating the standard deviation of noise. Third, we link the cascaded convolutional structure of convolutional neutral network (CNN) with wavelet packet decomposition and reconstruction using Mallat algorithm, which is interpretable in model architecture. Extensive experiments are carried out on two bearing fault datasets, and the results show that the proposed architecture outperforms other diagnosis models in terms of interpretability and noise robustness.},
  archive      = {J_TNNLS},
  author       = {Sinan Li and Tianfu Li and Chuang Sun and Xuefeng Chen and Ruqiang Yan},
  doi          = {10.1109/TNNLS.2023.3282599},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14974-14988},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {WPConvNet: An interpretable wavelet packet kernel-constrained convolutional network for noise-robust fault diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heuristic heterogeneous graph reasoning networks for fact
verification. <em>TNNLS</em>, <em>35</em>(10), 14959–14973. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies on table-based fact verification generally capture linguistic evidence from claim-table subgraphs or logical evidence from program-table subgraphs independently. However, there is insufficient association interaction between the two types of evidence, which makes it difficult to obtain valuable consistency features between them. In this work, we propose heuristic heterogeneous graph reasoning networks (H2GRN) to capture the shared consistent evidence by strengthening associations between linguistic and logical evidence from two perspectives of graph construction and reasoning mechanism. Specifically, 1) to enhance the close connectivity of the two subgraphs, rather than simply connecting two subgraphs by the nodes with the same content (the constructed graph in this way has severe sparsity), we construct a heuristic heterogeneous graph, which relies on claim semantics as heuristic knowledge to guide the connections of the program-table subgraph, and in turn expands the connectivity of the claim-table subgraph through logical information of programs as heuristic knowledge; and 2) to establish adequate association interaction between linguistic evidence and logical evidence, we design multiview reasoning networks. In detail, we propose local-view multihop knowledge reasoning (MKR) networks to enable the current node to establish association not only with one-hop neighbors, but also with multihop neighbors, to capture context-richer evidence information. We execute MKR on heuristic claim-table and program-table subgraphs to learn context-richer linguistic evidence and logical evidence, respectively. Meanwhile, we develop global-view graph dual-attention networks (DAN) that execute on the entire heuristic heterogeneous graph, reinforcing global-level significant consistency evidence. Finally, the consistency fusion layer is devised to weaken the disagreement between the three types of evidence to assist in capturing consistent shared evidence for verifying claims. Experiments on TABFACT and FEVEROUS demonstrate the effectiveness of H2GRN.},
  archive      = {J_TNNLS},
  author       = {Lianwei Wu and Dengxiu Yu and Pusheng Liu and Chao Gao and Zhen Wang},
  doi          = {10.1109/TNNLS.2023.3282380},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14959-14973},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heuristic heterogeneous graph reasoning networks for fact verification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Shunting at arbitrary feature levels via spatial
disentanglement: Toward selective image translation. <em>TNNLS</em>,
<em>35</em>(10), 14945–14958. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past few years have witnessed considerable efforts devoted to translating images from one domain to another, mainly aiming at editing global style. Here, we focus on a more general case, selective image translation (SLIT), under an unsupervised setting. SLIT essentially operates through a shunt mechanism that involves learning gates to manipulate only the contents of interest (CoIs), which can be either local or global, while leaving the irrelevant parts unchanged. Existing methods typically rely on a flawed implicit assumption that CoIs are separable at arbitrary levels, ignoring the entangled nature of DNN representations. This leads to unwanted changes and learning inefficiency. In this work, we revisit SLIT from an information-theoretical perspective and introduce a novel framework, which equips two opposite forces to disentangle the visual features. One force encourages independence between spatial locations on the features, while the other force unites multiple locations to form a “block” that jointly characterizes an instance or attribute that a single location may not independently characterize. Importantly, this disentanglement paradigm can be applied to visual features of any layer, enabling shunting at arbitrary feature levels, which is a significant advantage not explored in existing works. Our approach has undergone extensive evaluation and analysis, confirming its effectiveness in significantly outperforming the state-of-the-art baselines.},
  archive      = {J_TNNLS},
  author       = {Jian Wang and Xue Yang and Yuxin Tian and Jizhe Zhou and Jiancheng Lv},
  doi          = {10.1109/TNNLS.2023.3282306},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14945-14958},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Shunting at arbitrary feature levels via spatial disentanglement: Toward selective image translation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Device-performance-driven heterogeneous multiparty learning
for arbitrary images. <em>TNNLS</em>, <em>35</em>(10), 14932–14944. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiparty learning (MPL) is an emerging framework for privacy-preserving collaborative learning. It enables individual devices to build a knowledge-shared model and remaining sensitive data locally. However, with the continuous increase of users, the heterogeneity gap between data and equipment becomes wider, which leads to the problem of model heterogeneous. In this article, we concentrate on two practical issues: data heterogeneous problem and model heterogeneous problem, and propose a novel personal MPL method named device-performance-driven heterogeneous MPL (HMPL). First, facing the data heterogeneous problem, we focus on the problem of various devices holding arbitrary data sizes. We introduce a heterogeneous feature-map integration method to adaptively unify the various feature maps. Meanwhile, to handle the model heterogeneous problem, as it is essential to customize models for adapting to the various computing performances, we propose a layer-wise model generation and aggregation strategy. The method can generate customized models based on the device’s performance. In the aggregation process, the shared model parameters are updated through the rules that the network layers with the same semantics are aggregated with each other. Extensive experiments are conducted on four popular datasets, and the result demonstrates that our proposed framework outperforms the state of the art (SOTA).},
  archive      = {J_TNNLS},
  author       = {Yuanqiao Zhang and Maoguo Gong and Yuan Gao and A. K. Qin and Kun Wang and Yiming Lin and Shanfeng Wang},
  doi          = {10.1109/TNNLS.2023.3282242},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14932-14944},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Device-performance-driven heterogeneous multiparty learning for arbitrary images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully complex-valued gated recurrent neural network for
ultrasound imaging. <em>TNNLS</em>, <em>35</em>(10), 14918–14931. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound imaging is widely used in medical diagnosis. It has the advantages of being performed in real time, cost-efficient, noninvasive, and nonionizing. The traditional delay-and-sum (DAS) beamformer has low resolution and contrast. Several adaptive beamformers (ABFs) have been proposed to improve them. Although they improve image quality, they incur high computation cost because of the dependence on data at the expense of real-time performance. Deep-learning methods have been successful in many areas. They train an ultrasound imaging model that can be used to quickly handle ultrasound signals and construct images. Real-valued radio-frequency signals are typically used to train a model, whereas complex-valued ultrasound signals with complex weights enable the fine-tuning of time delay for enhancing image quality. This work, for the first time, proposes a fully complex-valued gated recurrent neural network to train an ultrasound imaging model for improving ultrasound image quality. The model considers the time attributes of ultrasound signals and uses complete complex-number calculation. The model parameter and architecture are analyzed to select the best setup. The effectiveness of complex batch normalization is evaluated in training the model. The effect of analytic signals and complex weights is analyzed, and the results verify that analytic signals with complex weights enhance the model performance to reconstruct high-quality ultrasound images. The proposed model is finally compared with seven state-of-the-art methods. Experimental results reveal its great performance.},
  archive      = {J_TNNLS},
  author       = {Zhenyu Lei and Shangce Gao and Hideyuki Hasegawa and Zhiming Zhang and Mengchu Zhou and Khaled Sedraoui},
  doi          = {10.1109/TNNLS.2023.3282231},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14918-14931},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully complex-valued gated recurrent neural network for ultrasound imaging},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive graph gradual pruning for sparse training in
graph neural networks. <em>TNNLS</em>, <em>35</em>(10), 14903–14917. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) tend to suffer from high computation costs due to the exponentially increasing scale of graph data and a large number of model parameters, which restricts their utility in practical applications. To this end, some recent works focus on sparsifying GNNs (including graph structures and model parameters) with the lottery ticket hypothesis (LTH) to reduce inference costs while maintaining performance levels. However, the LTH-based methods suffer from two major drawbacks: 1) they require exhaustive and iterative training of dense models, resulting in an extremely large training computation cost, and 2) they only trim graph structures and model parameters but ignore the node feature dimension, where vast redundancy exists. To overcome the above limitations, we propose a comprehensive graph gradual pruning framework termed CGP. This is achieved by designing a during-training graph pruning paradigm to dynamically prune GNNs within one training process. Unlike LTH-based methods, the proposed CGP approach requires no retraining, which significantly reduces the computation costs. Furthermore, we design a cosparsifying strategy to comprehensively trim all the three core elements of GNNs: graph structures, node features, and model parameters. Next, to refine the pruning operation, we introduce a regrowth process into our CGP framework, to reestablish the pruned but important connections. The proposed CGP is evaluated over a node classification task across six GNN architectures, including shallow models [graph convolutional network (GCN) and graph attention network (GAT)], shallow-but-deep-propagation models [simple graph convolution (SGC) and approximate personalized propagation of neural predictions (APPNP)], and deep models [GCN via initial residual and identity mapping (GCNII) and residual GCN (ResGCN)], on a total of 14 real-world graph datasets, including large-scale graph datasets from the challenging Open Graph Benchmark (OGB). Experiments reveal that the proposed strategy greatly improves both training and inference efficiency while matching or even exceeding the accuracy of the existing methods.},
  archive      = {J_TNNLS},
  author       = {Chuang Liu and Xueqi Ma and Yibing Zhan and Liang Ding and Dapeng Tao and Bo Du and Wenbin Hu and Danilo P. Mandic},
  doi          = {10.1109/TNNLS.2023.3282049},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14903-14917},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Comprehensive graph gradual pruning for sparse training in graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Description-enhanced label embedding contrastive learning
for text classification. <em>TNNLS</em>, <em>35</em>(10), 14889–14902.
(<a href="https://doi.org/10.1109/TNNLS.2023.3282020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification is one of the fundamental tasks in natural language processing, which requires an agent to determine the most appropriate category for input sentences. Recently, deep neural networks have achieved impressive performance in this area, especially pretrained language models (PLMs). Usually, these methods concentrate on input sentences and corresponding semantic embedding generation. However, for another essential component: labels, most existing works either treat them as meaningless one-hot vectors or use vanilla embedding methods to learn label representations along with model training, underestimating the semantic information and guidance that these labels reveal. To alleviate this problem and better exploit label information, in this article, we employ self-supervised learning (SSL) in model learning process and design a novel self-supervised relation of relation ( $\text{R}^{2}$ ) classification task for label utilization from a one-hot manner perspective. Then, we propose a novel relation of relation learning network( $\text{R}^{2}$ -Net) for text classification, in which text classification and $\text{R}^{2}$ classification are treated as optimization targets. Meanwhile, triplet loss is employed to enhance the analysis of differences and connections among labels. Moreover, considering that one-hot usage is still short of exploiting label information, we incorporate external knowledge from WordNet to obtain multiaspect descriptions for label semantic learning and extend $\text{R}^{2}$ -Net to a novel description-enhanced label embedding network(DELE) from a label embedding perspective. One step further, since these fine-grained descriptions may introduce unexpected noise, we develop a mutual interaction module to select appropriate parts from input sentences and labels simultaneously based on contrastive learning (CL) for noise mitigation. Extensive experiments on different text classification tasks reveal that $\text{R}^{2}$ -Net can effectively improve the classification performance and DELE can make better use of label information and further improve the performance. As a byproduct, we have released the codes to facilitate other research.},
  archive      = {J_TNNLS},
  author       = {Kun Zhang and Le Wu and Guangyi Lv and Enhong Chen and Shulan Ruan and Jing Liu and Zhiqiang Zhang and Jun Zhou and Meng Wang},
  doi          = {10.1109/TNNLS.2023.3282020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14889-14902},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Description-enhanced label embedding contrastive learning for text classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the effectiveness of adversarial training against
backdoor attacks. <em>TNNLS</em>, <em>35</em>(10), 14878–14888. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although adversarial training (AT) is regarded as a potential defense against backdoor attacks, AT and its variants have only yielded unsatisfactory results or have even inversely strengthened backdoor attacks. The large discrepancy between expectations and reality motivates us to thoroughly evaluate the effectiveness of AT against backdoor attacks across various settings for AT and backdoor attacks. We find that the type and budget of perturbations used in AT are important, and AT with common perturbations is only effective for certain backdoor trigger patterns. Based on these empirical findings, we present some practical suggestions for backdoor defense, including relaxed adversarial perturbation and composite AT. This work not only boosts our confidence in AT’s ability to defend against backdoor attacks but also provides some important insights for future research.},
  archive      = {J_TNNLS},
  author       = {Yinghua Gao and Dongxian Wu and Jingfeng Zhang and Guanhao Gan and Shu-Tao Xia and Gang Niu and Masashi Sugiyama},
  doi          = {10.1109/TNNLS.2023.3281872},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14878-14888},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the effectiveness of adversarial training against backdoor attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mesh convolution with continuous filters for 3-d surface
parsing. <em>TNNLS</em>, <em>35</em>(10), 14863–14877. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric feature learning for 3-D surfaces is critical for many applications in computer graphics and 3-D vision. However, deep learning currently lags in hierarchical modeling of 3-D surfaces due to the lack of required operations and/or their efficient implementations. In this article, we propose a series of modular operations for effective geometric feature learning from 3-D triangle meshes. These operations include novel mesh convolutions, efficient mesh decimation, and associated mesh (un)poolings. Our mesh convolutions exploit spherical harmonics as orthonormal bases to create continuous convolutional filters. The mesh decimation module is graphics processing unit (GPU)-accelerated and able to process batched meshes on-the-fly, while the (un)pooling operations compute features for upsampled/downsampled meshes. We provide an open-source implementation of these operations, collectively termed Picasso. Picasso supports heterogeneous mesh batching and processing. Leveraging its modular operations, we further contribute a novel hierarchical neural network for perceptual parsing of 3-D surfaces, named PicassoNet++. It achieves highly competitive performance for shape analysis and scene segmentation on prominent 3-D benchmarks. The code, data, and trained models are available at https://github.com/EnyaHermite/Picasso .},
  archive      = {J_TNNLS},
  author       = {Huan Lei and Naveed Akhtar and Mubarak Shah and Ajmal Mian},
  doi          = {10.1109/TNNLS.2023.3281871},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14863-14877},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mesh convolution with continuous filters for 3-D surface parsing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative multiview subspace learning for unpaired multiview
clustering. <em>TNNLS</em>, <em>35</em>(10), 14848–14862. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real applications, several unpredictable or uncertain factors could result in unpaired multiview data, i.e., the observed samples between views cannot be matched. Since joint clustering among views is more effective than individual clustering in each view, we investigate unpaired multiview clustering (UMC), which is a valuable but insufficiently studied problem. Due to lack of matched samples between views, we could fail to build the connection between views. Therefore, we aim to learn the latent subspace shared by views. However, existing multiview subspace learning methods usually rely on the matched samples between views. To address this issue, we propose an iterative multiview subspace learning strategy [iterative unpaired multiview clustering (IUMC)], aiming to learn a complete and consistent subspace representation among views for UMC. Moreover, based on IUMC, we design two effective UMC methods: 1) Iterative unpaired multiview clustering via covariance matrix alignment (IUMC-CA) that further aligns the covariance matrix of subspace representations and then performs clustering on the subspace and 2) iterative unpaired multiview clustering via one-stage clustering assignments (IUMC-CY) that performs one-stage multiview clustering (MVC) by replacing the subspace representations with clustering assignments. Extensive experiments show the excellent performance of our methods for UMC, compared with the state-of-the-art methods. Also, the clustering performance of observed samples in each view can be considerably improved by those observed samples from the other views. In addition, our methods have good applicability in incomplete MVC.},
  archive      = {J_TNNLS},
  author       = {Wanqi Yang and Like Xin and Lei Wang and Ming Yang and Wenzhu Yan and Yang Gao},
  doi          = {10.1109/TNNLS.2023.3281739},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14848-14862},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative multiview subspace learning for unpaired multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Motif graph neural network. <em>TNNLS</em>,
<em>35</em>(10), 14833–14847. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs can model complicated interactions between entities, which naturally emerge in many important applications. These applications can often be cast into standard graph learning tasks, in which a crucial step is to learn low-dimensional graph representations. Graph neural networks (GNNs) are currently the most popular model in graph embedding approaches. However, standard GNNs in the neighborhood aggregation paradigm suffer from limited discriminative power in distinguishing high-order graph structures as opposed to low-order structures. To capture high-order structures, researchers have resorted to motifs and developed motif-based GNNs. However, the existing motif-based GNNs still often suffer from less discriminative power on high-order structures. To overcome the above limitations, we propose motif GNN (MGNN), a novel framework to better capture high-order structures, hinging on our proposed motif redundancy minimization operator and injective motif combination. First, MGNN produces a set of node representations with respect to each motif. The next phase is our proposed redundancy minimization among motifs which compares the motifs with each other and distills the features unique to each motif. Finally, MGNN performs the updating of node representations by combining multiple representations from different motifs. In particular, to enhance the discriminative power, MGNN uses an injective function to combine the representations with respect to different motifs. We further show that our proposed architecture increases the expressive power of GNNs with a theoretical analysis. We demonstrate that MGNN outperforms state-of-the-art methods on seven public benchmarks on both the node classification and graph classification tasks.},
  archive      = {J_TNNLS},
  author       = {Xuexin Chen and Ruichu Cai and Yuan Fang and Min Wu and Zijian Li and Zhifeng Hao},
  doi          = {10.1109/TNNLS.2023.3281716},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14833-14847},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Motif graph neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exponential synchronization control of reaction-diffusion
fuzzy memristive neural networks: Hardy–poincarè inequality.
<em>TNNLS</em>, <em>35</em>(10), 14825–14832. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is devoted to solving the exponential synchronization problem of a new type of fuzzy memristive neural network with reaction-diffusion terms. By introducing adaptive laws, two controllers are designed. After combining the inequality technique with the Lyapunov function approach, some easily verified sufficient conditions are established to ensure the exponential synchronization of the reaction-diffusion fuzzy memristive system under the proposed adaptive scheme. In addition, by using the Hardy–Poincarè inequality, the diffusion terms are estimated associated with the information of the reaction-diffusion coefficients and the regional feature, which improves some existing conclusions. Finally, an illustrative example is presented to demonstrate the validity of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Hongzhi Wei and Ruoxia Li},
  doi          = {10.1109/TNNLS.2023.3281645},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14825-14832},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential synchronization control of reaction-diffusion fuzzy memristive neural networks: Hardy–Poincarè inequality},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural network-based node deployment for throughput
enhancement. <em>TNNLS</em>, <em>35</em>(10), 14810–14824. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent rapid growth in mobile data traffic entails a pressing demand for improving the throughput of the underlying wireless communication networks. Network node deployment has been considered as an effective approach for throughput enhancement which, however, often leads to highly nontrivial nonconvex optimizations. Although convex-approximation-based solutions are considered in the literature, their approximation to the actual throughput may be loose and sometimes lead to unsatisfactory performance. With this consideration, in this article, we propose a novel graph neural network (GNN) method for the network node deployment problem. Specifically, we fit a GNN to the network throughput and use the gradients of this GNN to iteratively update the locations of the network nodes. Besides, we show that an expressive GNN has the capacity to approximate both the function value and the gradients of a multivariate permutation-invariant function, as a theoretic support to the proposed method. To further improve the throughput, we also study a hybrid node deployment method based on this approach. To train the desired GNN, we adopt a policy gradient algorithm to create datasets containing good training samples. Numerical experiments show that the proposed methods produce competitive results compared with the baselines.},
  archive      = {J_TNNLS},
  author       = {Yifei Yang and Dongmian Zou and Xiaofan He},
  doi          = {10.1109/TNNLS.2023.3281643},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14810-14824},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph neural network-based node deployment for throughput enhancement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient bayesian policy reuse with a scalable observation
model in deep reinforcement learning. <em>TNNLS</em>, <em>35</em>(10),
14797–14809. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian policy reuse (BPR) is a general policy transfer framework for selecting a source policy from an offline library by inferring the task belief based on some observation signals and a trained observation model. In this article, we propose an improved BPR method to achieve more efficient policy transfer in deep reinforcement learning (DRL). First, most BPR algorithms use the episodic return as the observation signal that contains limited information and cannot be obtained until the end of an episode. Instead, we employ the state transition sample, which is informative and instantaneous, as the observation signal for faster and more accurate task inference. Second, BPR algorithms usually require numerous samples to estimate the probability distribution of the tabular-based observation model, which may be expensive and even infeasible to learn and maintain, especially when using the state transition sample as the signal. Hence, we propose a scalable observation model based on fitting state transition functions of source tasks from only a small number of samples, which can generalize to any signals observed in the target task. Moreover, we extend the offline-mode BPR to the continual learning setting by expanding the scalable observation model in a plug-and-play fashion, which can avoid negative transfer when faced with new unknown tasks. Experimental results show that our method can consistently facilitate faster and more efficient policy transfer.},
  archive      = {J_TNNLS},
  author       = {Jinmei Liu and Zhi Wang and Chunlin Chen and Daoyi Dong},
  doi          = {10.1109/TNNLS.2023.3281604},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14797-14809},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient bayesian policy reuse with a scalable observation model in deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual-masked deep structural clustering network with
adaptive bidirectional information delivery. <em>TNNLS</em>,
<em>35</em>(10), 14783–14796. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured clustering networks, which alleviate the oversmoothing issue by delivering hidden features from autoencoder (AE) to graph convolutional networks (GCNs), involve two shortcomings for the clustering task. For one thing, they used vanilla structure to learn clustering representations without considering feature and structure corruption; for another thing, they exhibit network degradation and vanishing gradient issues after stacking multilayer GCNs. In this article, we propose a clustering method called dual-masked deep structural clustering network (DMDSC) with adaptive bidirectional information delivery (ABID). Specifically, DMDSC enables generative self-supervised learning to mine deeper interstructure and interfeature correlations by simultaneously reconstructing corrupted structures and features. Furthermore, DMDSC develops an ABID module to establish an information transfer channel between each pairwise layer of AE and GCNs to alleviate the oversmoothing and vanishing gradient problems. Numerous experiments on six benchmark datasets have shown that the proposed DMDSC outperforms the most advanced deep clustering algorithms.},
  archive      = {J_TNNLS},
  author       = {Yachao Yang and Yanfeng Sun and Shaofan Wang and Junbin Gao and Fujiao Ju and Baocai Yin},
  doi          = {10.1109/TNNLS.2023.3281570},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14783-14796},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A dual-masked deep structural clustering network with adaptive bidirectional information delivery},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Logical relation inference and multiview information
interaction for domain adaptation person re-identification.
<em>TNNLS</em>, <em>35</em>(10), 14770–14782. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation person re-identification (Re-ID) is a challenging task, which aims to transfer the knowledge learned from the labeled source domain to the unlabeled target domain. Recently, some clustering-based domain adaptation Re-ID methods have achieved great success. However, these methods ignore the inferior influence on pseudo-label prediction due to the different camera styles. The reliability of the pseudo-label plays a key role in domain adaptation Re-ID, while the different camera styles bring great challenges for pseudo-label prediction. To this end, a novel method is proposed, which bridges the gap of different cameras and extracts more discriminative features from an image. Specifically, an intra-to-intermechanism is introduced, in which samples from their own cameras are first grouped and then aligned at the class level across different cameras followed by our logical relation inference (LRI). Thanks to these strategies, the logical relationship between simple classes and hard classes is justified, preventing sample loss caused by discarding the hard samples. Furthermore, we also present a multiview information interaction (MvII) module that takes features of different images from the same pedestrian as patch tokens, obtaining the global consistency of a pedestrian that contributes to the discriminative feature extraction. Unlike the existing clustering-based methods, our method employs a two-stage framework that generates reliable pseudo-labels from the views of the intracamera and intercamera, respectively, to differentiate the camera styles, subsequently increasing its robustness. Extensive experiments on several benchmark datasets show that the proposed method outperforms a wide range of state-of-the-art methods. The source code has been released at https://github.com/lhf12278/LRIMV .},
  archive      = {J_TNNLS},
  author       = {Shuang Li and Fan Li and Jinxing Li and Huafeng Li and Bob Zhang and Dapeng Tao and Xinbo Gao},
  doi          = {10.1109/TNNLS.2023.3281504},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14770-14782},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Logical relation inference and multiview information interaction for domain adaptation person re-identification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Enhancing unsupervised anomaly detection with score-guided
network. <em>TNNLS</em>, <em>35</em>(10), 14754–14769. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection plays a crucial role in various real-world applications, including healthcare and finance systems. Owing to the limited number of anomaly labels in these complex systems, unsupervised anomaly detection methods have attracted great attention in recent years. Two major challenges faced by the existing unsupervised methods are as follows: 1) distinguishing between normal and abnormal data when they are highly mixed together and 2) defining an effective metric to maximize the gap between normal and abnormal data in a hypothesis space, which is built by a representation learner. To that end, this work proposes a novel scoring network with a score-guided regularization to learn and enlarge the anomaly score disparities between normal and abnormal data, enhancing the capability of anomaly detection. With such score-guided strategy, the representation learner can gradually learn more informative representation during the model training stage, especially for the samples in the transition field. Moreover, the scoring network can be incorporated into most of the deep unsupervised representation learning (URL)-based anomaly detection models and enhances them as a plug-in component. We next integrate the scoring network into an autoencoder (AE) and four state-of-the-art models to demonstrate the effectiveness and transferability of the design. These score-guided models are collectively called SG-Models. Extensive experiments on both synthetic and real-world datasets confirm the state-of-the-art performance of SG-Models.},
  archive      = {J_TNNLS},
  author       = {Zongyuan Huang and Baohua Zhang and Guoqiang Hu and Longyuan Li and Yanyan Xu and Yaohui Jin},
  doi          = {10.1109/TNNLS.2023.3281501},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14754-14769},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhancing unsupervised anomaly detection with score-guided network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient multitask reinforcement learning without
performance loss. <em>TNNLS</em>, <em>35</em>(10), 14739–14753. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an iterative sparse Bayesian policy optimization (ISBPO) scheme as an efficient multitask reinforcement learning (RL) method for industrial control applications that require both high performance and cost-effective implementation. Under continual learning scenarios in which multiple control tasks are sequentially learned, the proposed ISBPO scheme preserves the previously learned knowledge without performance loss (PL), enables efficient resource use, and improves the sample efficiency of learning new tasks. Specifically, the proposed ISBPO scheme continually adds new tasks to a single policy neural network while completely preserving the control performance of previously learned tasks through an iterative pruning method. To create a free-weight space for adding new tasks, each task is learned through a pruning-aware policy optimization method called the sparse Bayesian policy optimization (SBPO), which ensures efficient allocation of limited policy network resources for multiple tasks. Furthermore, the weights allocated to the previous tasks are shared and reused in new task learning, thereby improving sample efficiency and the performance of new task learning. Simulations and practical experiments demonstrate that the proposed ISBPO scheme is highly suitable for sequentially learning multiple tasks in terms of performance conservation, efficient resource use, and sample efficiency.},
  archive      = {J_TNNLS},
  author       = {Jongchan Baek and Seungmin Baek and Soohee Han},
  doi          = {10.1109/TNNLS.2023.3281473},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14739-14753},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient multitask reinforcement learning without performance loss},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Referring image segmentation with fine-grained semantic
funneling infusion. <em>TNNLS</em>, <em>35</em>(10), 14727–14738. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, referring image segmentation has attracted wide attention given its huge potential in human–robot interaction. Networks to identify the referred region must have a deep understanding of both the image and language semantics. To do so, existing works tend to design various mechanisms to achieve cross-modality fusion, for example, tile and concatenation and vanilla nonlocal manipulation. However, the plain fusion usually is either coarse or constrained by the exorbitant computation overhead, finally causing not enough understanding of the referent. In this work, we propose a fine-grained semantic funneling infusion (FSFI) mechanism to solve the problem. The FSFI introduces a constant spatial constraint on the querying entities from different encoding stages and dynamically infuses the gleaned language semantic into the vision branch. Moreover, it decomposes the features from different modalities into more delicate components, allowing the fusion to happen in multiple low-dimensional spaces. The fusion is more effective than the one only happening in one high-dimensional space, given its ability to sink more representative information along the channel dimension. Another problem haunting the task is that the instilling of high-abstract semantic will blur the details of the referent. Targetedly, we propose a multiscale attention-enhanced decoder (MAED) to alleviate the problem. We design a detail enhancement operator (DeEh) and apply it in a multiscale and progressive way. Features from the higher level are used to generate attention guidance to enlighten the lower-level features to more attend to the detail regions. Extensive results on the challenging benchmarks show that our network performs favorably against the state-of-the-arts (SOTAs).},
  archive      = {J_TNNLS},
  author       = {Jiaxing Yang and Lihe Zhang and Huchuan Lu},
  doi          = {10.1109/TNNLS.2023.3281372},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14727-14738},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Referring image segmentation with fine-grained semantic funneling infusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FuBay: An integrated fusion framework for hyperspectral
super-resolution based on bayesian tensor ring. <em>TNNLS</em>,
<em>35</em>(10), 14712–14726. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusion with corresponding finer-resolution images has been a promising way to enhance hyperspectral images (HSIs) spatially. Recently, low-rank tensor-based methods have shown advantages compared with other kind of ones. However, these current methods either relent to blind manual selection of latent tensor rank, whereas the prior knowledge about tensor rank is surprisingly limited, or resort to regularization to make the role of low rankness without exploration on the underlying low-dimensional factors, both of which are leaving the computational burden of parameter tuning. To address that, a novel Bayesian sparse learning-based tensor ring (TR) fusion model is proposed, named as FuBay. Through specifying hierarchical sprasity-inducing prior distribution, the proposed method becomes the first fully Bayesian probabilistic tensor framework for hyperspectral fusion. With the relationship between component sparseness and the corresponding hyperprior parameter being well studied, a component pruning part is established to asymptotically approaching true latent rank. Furthermore, a variational inference (VI)-based algorithm is derived to learn the posterior of TR factors, circumventing nonconvex optimization that bothers the most tensor decomposition-based fusion methods. As a Bayesian learning methods, our model is characterized to be parameter tuning-free. Finally, extensive experiments demonstrate its superior performance when compared with state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yinjian Wang and Wei Li and Na Liu and Yuanyuan Gui and Ran Tao},
  doi          = {10.1109/TNNLS.2023.3281355},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14712-14726},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FuBay: An integrated fusion framework for hyperspectral super-resolution based on bayesian tensor ring},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TeKo: Text-rich graph neural networks with external
knowledge. <em>TNNLS</em>, <em>35</em>(10), 14699–14711. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have gained great prevalence in tackling various analytical tasks on graph-structured data (i.e., networks). Typical GNNs and their variants adopt a message-passing principle that obtains network representations by the attribute propagates along network topology, which however ignores the rich textual semantics (e.g., local word-sequence) that exist in numerous real-world networks. Existing methods for text-rich networks integrate textual semantics by mainly using internal information such as topics or phrases/words, which often suffer from an inability to comprehensively mine the textual semantics, limiting the reciprocal guidance between network structure and textual semantics. To address these problems, we present a novel text-rich GNN with external knowledge (TeKo), in order to make full use of both structural and textual information within text-rich networks. Specifically, we first present a flexible heterogeneous semantic network that integrates high-quality entities as well as interactions among documents and entities. We then introduce two types of external knowledge, that is, structured triplets and unstructured entity descriptions, to gain a deeper insight into textual semantics. Furthermore, we devise a reciprocal convolutional mechanism for the constructed heterogeneous semantic network, enabling network structure and textual semantics to collaboratively enhance each other and learn high-level network representations. Extensive experiments illustrate that TeKo achieves state-of-the-art performance on a variety of text-rich networks as well as a large-scale e-commerce searching dataset.},
  archive      = {J_TNNLS},
  author       = {Zhizhi Yu and Di Jin and Jianguo Wei and Yawen Li and Ziyang Liu and Yue Shang and Jiawei Han and Lingfei Wu},
  doi          = {10.1109/TNNLS.2023.3281354},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14699-14711},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TeKo: Text-rich graph neural networks with external knowledge},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lifelong generative adversarial autoencoder. <em>TNNLS</em>,
<em>35</em>(10), 14684–14698. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong learning describes an ability that enables humans to continually acquire and learn new information without forgetting. This capability, common to humans and animals, has lately been identified as an essential function for an artificial intelligence system aiming to learn continuously from a stream of data during a certain period of time. However, modern neural networks suffer from degenerated performance when learning multiple domains sequentially and fail to recognize past learned tasks after being retrained. This corresponds to catastrophic forgetting and is ultimately induced by replacing the parameters associated with previously learned tasks with new values. One approach in lifelong learning is the generative replay mechanism (GRM) that trains a powerful generator as the generative replay network, implemented by a variational autoencoder (VAE) or a generative adversarial network (GAN). In this article, we study the forgetting behavior of GRM-based learning systems by developing a new theoretical framework in which the forgetting process is expressed as an increase in the model’s risk during the training. Although many recent attempts have provided high-quality generative replay samples by using GANs, they are limited to mainly downstream tasks due to the lack of inference. Inspired by the theoretical analysis while aiming to address the drawbacks of existing approaches, we propose the lifelong generative adversarial autoencoder (LGAA). LGAA consists of a generative replay network and three inference models, each addressing the inference of a different type of latent variable. The experimental results show that LGAA learns novel visual concepts without forgetting and can be applied to a wide range of downstream tasks.},
  archive      = {J_TNNLS},
  author       = {Fei Ye and Adrian G. Bors},
  doi          = {10.1109/TNNLS.2023.3281091},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14684-14698},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Lifelong generative adversarial autoencoder},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGP: Neural network pruning through regular graph with edges
swapping. <em>TNNLS</em>, <em>35</em>(10), 14671–14683. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning technology has found a promising application in lightweight model design, for which pruning is an effective means of achieving a large reduction in both model parameters and float points operations (FLOPs). The existing neural network pruning methods mostly start from the consideration of the importance of model parameters and design parameter evaluation metrics to perform parameter pruning iteratively. These methods were not studied from the perspective of network model topology, so they might be effective but not efficient, and they require completely different pruning for different datasets. In this article, we study the graph structure of the neural network and propose a regular graph pruning (RGP) method to perform a one-shot neural network pruning. Specifically, we first generate a regular graph and set its node-degree values to meet the preset pruning ratio. Then, we reduce the average shortest path-length (ASPL) of the graph by swapping edges to obtain the optimal edge distribution. Finally, we map the obtained graph to a neural network structure to realize pruning. Our experiments demonstrate that the ASPL of the graph is negatively correlated with the classification accuracy of the neural network and that RGP has a strong precision retention capability with high parameter reduction (more than 90%) and FLOPs reduction (more than 90%) (the code for quick use and reproduction is available at https://github.com/Holidays1999/Neural-Network-Pruning-through-its-RegularGraph-Structure ).},
  archive      = {J_TNNLS},
  author       = {Zhuangzhi Chen and Jingyang Xiang and Yao Lu and Qi Xuan and Zhen Wang and Guanrong Chen and Xiaoniu Yang},
  doi          = {10.1109/TNNLS.2023.3280899},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14671-14683},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RGP: Neural network pruning through regular graph with edges swapping},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A signed subgraph encoding approach via linear optimization
for link sign prediction. <em>TNNLS</em>, <em>35</em>(10), 14659–14670.
(<a href="https://doi.org/10.1109/TNNLS.2023.3280924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the problem of inferring the sign of a link based on known sign data in signed networks. Regarding this link sign prediction problem, signed directed graph neural networks (SDGNNs) provides the best prediction performance currently to the best of our knowledge. In this article, we propose a different link sign prediction architecture called subgraph encoding via linear optimization (SELO), which obtains overall leading prediction performances compared to the state-of-the-art algorithm SDGNN. The proposed model utilizes a subgraph encoding approach to learn edge embeddings for signed directed networks. In particular, a signed subgraph encoding approach is introduced to embed each subgraph into a likelihood matrix instead of the adjacency matrix through a linear optimization (LO) method. Comprehensive experiments are conducted on five real-world signed networks with area under curve (AUC), F1, micro-F1, and macro-F1 as the evaluation metrics. The experiment results show that the proposed SELO model outperforms existing baseline feature-based methods and embedding-based methods on all the five real-world networks and in all the four evaluation metrics.},
  archive      = {J_TNNLS},
  author       = {Zhihong Fang and Shaolin Tan and Yaonan Wang},
  doi          = {10.1109/TNNLS.2023.3280924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14659-14670},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A signed subgraph encoding approach via linear optimization for link sign prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Painless stochastic conjugate gradient for large-scale
machine learning. <em>TNNLS</em>, <em>35</em>(10), 14645–14658. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conjugate gradient (CG), as an effective technique to speed up gradient descent algorithms, has shown great potential and has widely been used for large-scale machine-learning problems. However, CG and its variants have not been devised for the stochastic setting, which makes them extremely unstable, and even leads to divergence when using noisy gradients. This article develops a novel class of stable stochastic CG (SCG) algorithms with a faster convergence rate via the variance-reduced technique and an adaptive step size rule in the mini-batch setting. Actually, replacing the use of a line search in the CG-type approaches which is time-consuming, or even fails for SCG, this article considers using the random stabilized Barzilai–Borwein (RSBB) method to obtain an online step size. We rigorously analyze the convergence properties of the proposed algorithms and show that the proposed algorithms attain a linear convergence rate for both the strongly convex and nonconvex settings. Also, we show that the total complexity of the proposed algorithms matches that of modern stochastic optimization algorithms under different cases. Scores of numerical experiments on machine-learning problems demonstrate that the proposed algorithms outperform state-of-the-art stochastic optimization algorithms.},
  archive      = {J_TNNLS},
  author       = {Zhuang Yang},
  doi          = {10.1109/TNNLS.2023.3280826},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14645-14658},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Painless stochastic conjugate gradient for large-scale machine learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distance transformation deep forest framework with
hybrid-feature fusion for CXR image classification. <em>TNNLS</em>,
<em>35</em>(10), 14633–14644. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting pneumonia, especially coronavirus disease 2019 (COVID-19), from chest X-ray (CXR) images is one of the most effective ways for disease diagnosis and patient triage. The application of deep neural networks (DNNs) for CXR image classification is limited due to the small sample size of the well-curated data. To tackle this problem, this article proposes a distance transformation-based deep forest framework with hybrid-feature fusion (DTDF-HFF) for accurate CXR image classification. In our proposed method, hybrid features of CXR images are extracted in two ways: hand-crafted feature extraction and multigrained scanning. Different types of features are fed into different classifiers in the same layer of the deep forest (DF), and the prediction vector obtained at each layer is transformed to form distance vector based on a self-adaptive scheme. The distance vectors obtained by different classifiers are fused and concatenated with the original features, then input into the corresponding classifier at the next layer. The cascade grows until DTDF-HFF can no longer gain benefits from the new layer. We compare the proposed method with other methods on the public CXR datasets, and the experimental results show that the proposed method can achieve state-of-the art (SOTA) performance. The code will be made publicly available at https://github.com/hongqq/DTDF-HFF .},
  archive      = {J_TNNLS},
  author       = {Qingqi Hong and Lingli Lin and Zihan Li and Qingde Li and Junfeng Yao and Qingqiang Wu and Kunhong Liu and Jie Tian},
  doi          = {10.1109/TNNLS.2023.3280646},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14633-14644},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A distance transformation deep forest framework with hybrid-feature fusion for CXR image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OpenHoldem: A benchmark for large-scale
imperfect-information game research. <em>TNNLS</em>, <em>35</em>(10),
14618–14632. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the unremitting efforts from a few institutes, researchers have recently made significant progress in designing superhuman artificial intelligence (AI) in no-limit Texas hold’em (NLTH), the primary testbed for large-scale imperfect-information game research. However, it remains challenging for new researchers to study this problem since there are no standard benchmarks for comparing with existing methods, which hinders further developments in this research area. This work presents OpenHoldem, an integrated benchmark for large-scale imperfect-information game research using NLTH. OpenHoldem makes three main contributions to this research direction: 1) a standardized evaluation protocol for thoroughly evaluating different NLTH AIs; 2) four publicly available strong baselines for NLTH AI; and 3) an online testing platform with easy-to-use APIs for public NLTH AI evaluation. We will publicly release OpenHoldem and hope it facilitates further studies on the unsolved theoretical and computational issues in this area and cultivates crucial research problems like opponent modeling and human–computer interactive learning.},
  archive      = {J_TNNLS},
  author       = {Kai Li and Hang Xu and Enmin Zhao and Zhe Wu and Junliang Xing},
  doi          = {10.1109/TNNLS.2023.3280186},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14618-14632},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {OpenHoldem: A benchmark for large-scale imperfect-information game research},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust low-tubal-rank tensor completion based on tensor
factorization and maximum correntopy criterion. <em>TNNLS</em>,
<em>35</em>(10), 14603–14617. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of tensor completion is to recover a tensor from a subset of its entries, often by exploiting its low-rank property. Among several useful definitions of tensor rank, the low tubal rank was shown to give a valuable characterization of the inherent low-rank structure of a tensor. While some low-tubal-rank tensor completion algorithms with favorable performance have been recently proposed, these algorithms utilize second-order statistics to measure the error residual, which may not work well when the observed entries contain large outliers. In this article, we propose a new objective function for low-tubal-rank tensor completion, which uses correntropy as the error measure to mitigate the effect of the outliers. To efficiently optimize the proposed objective, we leverage a half-quadratic minimization technique whereby the optimization is transformed to a weighted low-tubal-rank tensor factorization problem. Subsequently, we propose two simple and efficient algorithms to obtain the solution and provide their convergence and complexity analysis. Numerical results using both synthetic and real data demonstrate the robust and superior performance of the proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Yicong He and George K. Atia},
  doi          = {10.1109/TNNLS.2023.3280086},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14603-14617},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust low-tubal-rank tensor completion based on tensor factorization and maximum correntopy criterion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Dynamics-adaptive continual reinforcement learning via
progressive contextualization. <em>TNNLS</em>, <em>35</em>(10),
14588–14602. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge of continual reinforcement learning (CRL) in dynamic environments is to promptly adapt the reinforcement learning (RL) agent’s behavior as the environment changes over its lifetime while minimizing the catastrophic forgetting of the learned information. To address this challenge, in this article, we propose DaCoRL, that is, dynamics-adaptive continual RL. DaCoRL learns a context-conditioned policy using progressive contextualization, which incrementally clusters a stream of stationary tasks in the dynamic environment into a series of contexts and opts for an expandable multihead neural network to approximate the policy. Specifically, we define a set of tasks with similar dynamics as an environmental context and formalize context inference as a procedure of online Bayesian infinite Gaussian mixture clustering on environment features, resorting to online Bayesian inference to infer the posterior distribution over contexts. Under the assumption of a Chinese restaurant process (CRP) prior, this technique can accurately classify the current task as a previously seen context or instantiate a new context as needed without relying on any external indicator to signal environmental changes in advance. Furthermore, we employ an expandable multihead neural network whose output layer is synchronously expanded with the newly instantiated context and a knowledge distillation regularization term for retaining the performance on learned tasks. As a general framework that can be coupled with various deep RL algorithms, DaCoRL features consistent superiority over existing methods in terms of stability, overall performance, and generalization ability, as verified by extensive experiments on several robot navigation and MuJoCo locomotion tasks.},
  archive      = {J_TNNLS},
  author       = {Tiantian Zhang and Zichuan Lin and Yuxing Wang and Deheng Ye and Qiang Fu and Wei Yang and Xueqian Wang and Bin Liang and Bo Yuan and Xiu Li},
  doi          = {10.1109/TNNLS.2023.3280085},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14588-14602},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamics-adaptive continual reinforcement learning via progressive contextualization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Behind-the-meter load and PV disaggregation via deep
spatiotemporal graph generative sparse coding with capsule network.
<em>TNNLS</em>, <em>35</em>(10), 14573–14587. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, rooftop photovoltaic (PV) panels are getting enormous attention as clean and sustainable sources of energy due to the increasing energy demand, depreciating physical assets, and global environmental challenges. In residential areas, the large-scale integration of these generation resources influences the customer load profile and introduces uncertainty to the distribution system’s net load. Since such resources are typically located behind the meter (BtM), an accurate estimation of BtM load and PV power will be crucial for distribution network operation. This article proposes the spatiotemporal graph sparse coding (SC) capsule network that incorporates SC into deep generative graph modeling and capsule networks for accurate BtM load and PV generation estimation. A set of neighboring residential units are modeled as a dynamic graph in which the edges represent the correlation among their net demands. A generative encoder–decoder model, i.e., spectral graph convolution (SGC) attention peephole long short-term memory (PLSTM), is devised to extract the highly nonlinear spatiotemporal patterns from the formed dynamic graph. Later, to enrich the latent space sparsity, a dictionary is learned in the hidden layer of the proposed encoder–decoder, and the corresponding sparse codes are procured. Such sparse representation is used by a capsule network to estimate the BtM PV generation and the load of the entire residential units. Experimental results on two real-world energy disaggregation (ED) datasets, Pecan Street and Ausgrid, demonstrate more than 9.8% and 6.3% root mean square error (RMSE) improvements in BtM PV and load estimation over the state-of-the-art, respectively.},
  archive      = {J_TNNLS},
  author       = {Mohsen Saffari and Mahdi Khodayar and Mohammad E. Khodayar and Mohammad Shahidehpour},
  doi          = {10.1109/TNNLS.2023.3280078},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14573-14587},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Behind-the-meter load and PV disaggregation via deep spatiotemporal graph generative sparse coding with capsule network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Retinex image enhancement based on sequential decomposition
with a plug-and-play framework. <em>TNNLS</em>, <em>35</em>(10),
14559–14572. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Retinex model is one of the most representative and effective methods for low-light image enhancement. However, the Retinex model does not explicitly tackle the noise problem and shows unsatisfactory enhancing results. In recent years, due to the excellent performance, deep learning models have been widely used in low-light image enhancement. However, these methods have two limitations. First, the desirable performance can only be achieved by deep learning when a large number of labeled data are available. However, it is not easy to curate massive low-/normal-light paired data. Second, deep learning is notoriously a black-box model. It is difficult to explain their inner working mechanism and understand their behaviors. In this article, using a sequential Retinex decomposition strategy, we design a plug-and-play framework based on the Retinex theory for simultaneous image enhancement and noise removal. Meanwhile, we develop a convolutional neural network-based (CNN-based) denoiser into our proposed plug-and-play framework to generate a reflectance component. The final image is enhanced by integrating the illumination and reflectance with gamma correction. The proposed plug-and-play framework can facilitate both post hoc and ad hoc interpretability. Extensive experiments on different datasets demonstrate that our framework outcompetes the state-of-the-art methods in both image enhancement and denoising.},
  archive      = {J_TNNLS},
  author       = {Tingting Wu and Wenna Wu and Ying Yang and Feng-Lei Fan and Tieyong Zeng},
  doi          = {10.1109/TNNLS.2023.3280037},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14559-14572},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Retinex image enhancement based on sequential decomposition with a plug-and-play framework},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking pan-sharpening in closed-loop regularization.
<em>TNNLS</em>, <em>35</em>(10), 14544–14558. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally known that pan-sharpening is fundamentally a PAN-guided multispectral (MS) image super-resolution problem that involves learning the nonlinear mapping from low-resolution (LR) to high-resolution (HR) MS images. Since an infinite number of HR-MS images can be downsampled to produce the same corresponding LR-MS image, learning the mapping from LR-MS to HR-MS image is typically ill-posed and the space of the possible pan-sharpening functions can be extremely large, making it difficult to estimate the optimal mapping solution. To address the above issue, we propose a closed-loop scheme that learns the two opposite mapping including the pan-sharpening and its corresponding degradation process simultaneously to regularize the solution space in a single pipeline. More specifically, an invertible neural network (INN) is introduced to perform a bidirectional closed-loop: the forward operation for LR-MS pan-sharpening and the backward operation for learning the corresponding HR-MS image degradation process. In addition, given the vital importance of high-frequency textures for the Pan-sharpened MS images, we further strengthen the INN by designing a specified multiscale high-frequency texture extraction module. Extensive experimental results demonstrate that the proposed algorithm performs favorably against state-of-the-art methods qualitatively and quantitatively with fewer parameters. Ablation studies also verify the effectiveness of the closed-loop mechanism in pan-sharpening. The source code is made publicly available at https://github.com/manman1995/pan-sharpening-Team-zhouman/ .},
  archive      = {J_TNNLS},
  author       = {Man Zhou and Jie Huang and Danfeng Hong and Feng Zhao and Chongyi Li and Jocelyn Chanussot},
  doi          = {10.1109/TNNLS.2023.3279931},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14544-14558},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rethinking pan-sharpening in closed-loop regularization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observer-based fault-tolerant finite-time control of
nonlinear multiagent systems. <em>TNNLS</em>, <em>35</em>(10),
14534–14543. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive neural containment control for a class of nonlinear multiagent systems considering actuator faults is introduced. By using the general approximation property of neural networks, a neuro-adaptive observer is designed to estimate unmeasured states. In addition, in order to reduce the computational burden, a novel event-triggered control law is designed. Furthermore, the finite-time performance function is presented to improve the transient and steady-state performance of the synchronization error. Utilizing the Lyapunov stability theory, it will be shown that the closed-loop system is cooperatively semiglobally uniformly ultimately bounded (CSGUUB), and the followers’ outputs reach the convex hull constructed by the leaders. Moreover, it is shown that the containment errors are limited to the prescribed level in a finite time. Eventually, a simulation example is presented to corroborate the capability of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Yasaman Salmanpour and Mohammad Mehdi Arefi and Alireza Khayatian and Shen Yin},
  doi          = {10.1109/TNNLS.2023.3279890},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14534-14543},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based fault-tolerant finite-time control of nonlinear multiagent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complete stability of neural networks with extended
memristors. <em>TNNLS</em>, <em>35</em>(10), 14519–14533. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article considers a large class of delayed neural networks (NNs) with extended memristors obeying the Stanford model. This is a widely used and popular model that accurately describes the switching dynamics of real nonvolatile memristor devices implemented in nanotechnology. The article studies via the Lyapunov method complete stability (CS), i.e., convergence of trajectories in the presence of multiple equilibrium points (EPs), for delayed NNs with Stanford memristors. The obtained conditions for CS are robust with respect to variations of the interconnections and they hold for any value of the concentrated delay. Moreover, they can be checked either numerically, via a linear matrix inequality (LMI), or analytically, via the concept of Lyapunov diagonally stable (LDS) matrices. The conditions ensure that at the end of the transient capacitor voltages and NN power vanish. In turn, this leads to advantages in terms of power consumption. This notwithstanding, the nonvolatile memristors can retain the result of computation in accordance with the in-memory computing principle. The results are verified and illustrated via numerical simulations. From a methodological viewpoint, the article faces new challenges to prove CS since due to the presence of nonvolatile memristors the NNs possess a continuum of nonisolated EPs. Also, for physical reasons, the memristor state variables are constrained to lie in some given intervals so that the dynamics of the NNs need to be modeled via a class of differential inclusions named differential variational inequalities.},
  archive      = {J_TNNLS},
  author       = {Mauro Di Marco and Mauro Forti and Riccardo Moretti and Luca Pancioni and Alberto Tesi},
  doi          = {10.1109/TNNLS.2023.3279406},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14519-14533},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Complete stability of neural networks with extended memristors},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-organizing a latent hierarchy of sketch patterns for
controllable sketch synthesis. <em>TNNLS</em>, <em>35</em>(10),
14506–14518. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encoding sketches as Gaussian mixture model (GMM)-distributed latent codes is an effective way to control sketch synthesis. Each Gaussian component represents a specific sketch pattern, and a code randomly sampled from the Gaussian can be decoded to synthesize a sketch with the target pattern. However, existing methods treat the Gaussians as individual clusters, which neglects the relationships between them. For example, the giraffe and horse sketches heading left are related to each other by their face orientation. The relationships between sketch patterns are important messages to reveal cognitive knowledge in sketch data. Thus, it is promising to learn accurate sketch representations by modeling the pattern relationships into a latent structure. In this article, we construct a tree-structured taxonomic hierarchy over the clusters of sketch codes. The clusters with the more specific descriptions of sketch patterns are placed at the lower levels, while the ones with the more general patterns are ranked at the higher levels. The clusters at the same rank relate to each other through the inheritance of features from common ancestors. We propose a hierarchical expectation-maximization (EM)-like algorithm to explicitly learn the hierarchy, jointly with the training of encoder–decoder network. Moreover, the learned latent hierarchy is utilized to regularize sketch codes with structural constraints. Experimental results show that our method significantly improves controllable synthesis performance and obtains effective sketch analogy results.},
  archive      = {J_TNNLS},
  author       = {Sicong Zang and Shikui Tu and Lei Xu},
  doi          = {10.1109/TNNLS.2023.3279410},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14506-14518},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-organizing a latent hierarchy of sketch patterns for controllable sketch synthesis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noninvasive blood glucose monitoring using spatiotemporal
ECG and PPG feature fusion and weight-based choquet integral multimodel
approach. <em>TNNLS</em>, <em>35</em>(10), 14491–14505. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {change of blood glucose (BG) level stimulates the autonomic nervous system leading to variation in both human’s electrocardiogram (ECG) and photoplethysmogram (PPG). In this article, we aimed to construct a novel multimodal framework based on ECG and PPG signal fusion to establish a universal BG monitoring model. This is proposed as a spatiotemporal decision fusion strategy that uses weight-based Choquet integral for BG monitoring. Specifically, the multimodal framework performs three-level fusion. First, ECG and PPG signals are collected and coupled into different pools. Second, the temporal statistical features and spatial morphological features in the ECG and PPG signals are extracted through numerical analysis and residual networks, respectively. Furthermore, the suitable temporal statistical features are determined with three feature selection techniques, and the spatial morphological features are compressed by deep neural networks (DNNs). Lastly, weight-based Choquet integral multimodel fusion is integrated for coupling different BG monitoring algorithms based on the temporal statistical features and spatial morphological features. To verify the feasibility of the model, a total of 103 days of ECG and PPG signals encompassing 21 participants were collected in this article. The BG levels of participants ranged between 2.2 and 21.8 mmol/L. The results obtained show that the proposed model has excellent BG monitoring performance with a root-mean-square error (RMSE) of 1.49 mmol/L, mean absolute relative difference (MARD) of 13.42%, and Zone A + B of 99.49% in tenfold cross-validation. Therefore, we conclude that the proposed fusion approach for BG monitoring has potentials in practical applications of diabetes management.},
  archive      = {J_TNNLS},
  author       = {Jingzhen Li and Jingjing Ma and Olatunji Mumini Omisore and Yuhang Liu and Huajie Tang and Pengfei Ao and Yan Yan and Lei Wang and Zedong Nie},
  doi          = {10.1109/TNNLS.2023.3279383},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14491-14505},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Noninvasive blood glucose monitoring using spatiotemporal ECG and PPG feature fusion and weight-based choquet integral multimodel approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified analysis of AdaGrad with weighted aggregation and
momentum acceleration. <em>TNNLS</em>, <em>35</em>(10), 14482–14490. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating adaptive learning rate and momentum techniques into stochastic gradient descent (SGD) leads to a large class of efficiently accelerated adaptive stochastic algorithms, such as AdaGrad, RMSProp, Adam, AccAdaGrad, and so on. In spite of their effectiveness in practice, there is still a large gap in their theories of convergences, especially in the difficult nonconvex stochastic setting. To fill this gap, we propose weighted AdaGrad with unified momentum and dubbed AdaUSM, which has the main characteristics that: 1) it incorporates a unified momentum scheme that covers both the heavy ball (HB) momentum and the Nesterov accelerated gradient (NAG) momentum and 2) it adopts a novel weighted adaptive learning rate that can unify the learning rates of AdaGrad, AccAdaGrad, Adam, and RMSProp. Moreover, when we take polynomially growing weights in AdaUSM, we obtain its $\mathcal {O}(\log (T)/\sqrt {T})$ convergence rate in the nonconvex stochastic setting. We also show that the adaptive learning rates of Adam and RMSProp correspond to taking exponentially growing weights in AdaUSM, thereby providing a new perspective for understanding Adam and RMSProp. Finally, comparative experiments of AdaUSM against SGD with momentum, AdaGrad, AdaEMA, Adam, and AMSGrad on various deep learning models and datasets are also carried out.},
  archive      = {J_TNNLS},
  author       = {Li Shen and Congliang Chen and Fangyu Zou and Zequn Jie and Ju Sun and Wei Liu},
  doi          = {10.1109/TNNLS.2023.3279381},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14482-14490},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A unified analysis of AdaGrad with weighted aggregation and momentum acceleration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overcoming the barrier of incompleteness: A hyperspectral
image classification full model. <em>TNNLS</em>, <em>35</em>(10),
14467–14481. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have shown promising outcomes in many fields. However, the performance gain is always limited to a large extent in classifying hyperspectral image (HSI). We discover that the reason behind this phenomenon lies in the incomplete classification of HSI, i.e., existing works only focus on a certain stage that contributes to the classification, while ignoring other equally or even more significant phases. To address the above issue, we creatively put forward three elements needed for complete classification: the extensive exploration of available features, adequate reuse of representative features, and differential fusion of multidomain features. To the best of our knowledge, these three elements are being established for the first time, providing a fresh perspective on designing HSI-tailored models. On this basis, an HSI classification full model (HSIC-FM) is proposed to overcome the barrier of incompleteness. Specifically, a recurrent transformer corresponding to Element 1 is presented to comprehensively extract short-term details and long-term semantics for local-to-global geographical representation. Afterward, a feature reuse strategy matching Element 2 is designed to sufficiently recycle valuable information aimed at refined classification using few annotations. Eventually, a discriminant optimization is formulized in accordance with Element 3 to distinctly integrate multidomain features for the purpose of constraining the contribution of different domains. Numerous experiments on four datasets at small-, medium-, and large-scale demonstrate that the proposed method outperforms the state-of-the-art (SOTA) methods, such as convolutional neural network (CNN)-, fully convolutional network (FCN)-, recurrent neural network (RNN)-, graph convolutional network (GCN)-, and transformer-based models (e.g., accuracy improvement of more than 9% with only five training samples per class). The code will be available soon at https://github.com/jqyang22/ HSIC-FM.},
  archive      = {J_TNNLS},
  author       = {Jiaqi Yang and Bo Du and Liangpei Zhang},
  doi          = {10.1109/TNNLS.2023.3279377},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14467-14481},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Overcoming the barrier of incompleteness: A hyperspectral image classification full model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered model-free adaptive control for nonlinear
multiagent systems under jamming attacks. <em>TNNLS</em>,
<em>35</em>(10), 14458–14466. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the security problem of tracking control for nonlinear multiagent systems against jamming attacks. It is assumed that the communication networks among agents are unreliable due to the existence of jamming attacks, and a Stackelberg game is introduced to depict the interaction process between multiagent systems and malicious jammer. First, the dynamic linearization model of the system is established by applying a pseudo-partial derivative method. Then, a novel model-free security adaptive control strategy is proposed, so that the multiagent systems can achieve bounded tracking control in the mathematical expectation sense in spite of jamming attacks. Furthermore, a fixed threshold event-triggered scheme is utilized to reduce communication cost. It is worth noting that the proposed methods only require the input and output information of the agents. Finally, the validity of the proposed methods is illustrated through two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Xijuan Wang and Changchun Hua and Yunfei Qiu},
  doi          = {10.1109/TNNLS.2023.3279144},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14458-14466},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered model-free adaptive control for nonlinear multiagent systems under jamming attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Optimal consensus control for continuous-time linear
multiagent systems: A dynamic event-triggered approach. <em>TNNLS</em>,
<em>35</em>(10), 14449–14457. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the optimal consensus problem for general linear multiagent systems (MASs) via a dynamic event-triggered approach. First, a modified interaction-related cost function is proposed. Second, a dynamic event-triggered approach is developed by constructing a new distributed dynamic triggering function and a new distributed event-triggered consensus protocol. Consequently, the modified interaction-related cost function can be minimized by applying the distributed control laws, which overcomes the difficulty in the optimal consensus problem that seeking the interaction-related cost function needs all agents’ information. Then, some sufficient conditions are obtained to guarantee optimality. It is shown that the developed optimal consensus gain matrices are only related to the designed triggering parameters and the desirable modified interaction-related cost function, relaxing the constraint that the controller design requires the knowledge of system dynamics, initial states, and network scale. Meanwhile, the tradeoff between optimal consensus performance and event-triggered behavior is also considered. Finally, a simulation example is provided to verify the validity of the designed distributed event-triggered optimal controller.},
  archive      = {J_TNNLS},
  author       = {Hao Zhang and Anqing Wang and Wenqiang Ji and Jianbin Qiu and Huaicheng Yan},
  doi          = {10.1109/TNNLS.2023.3279137},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14449-14457},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal consensus control for continuous-time linear multiagent systems: A dynamic event-triggered approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view and multi-order structured graph learning.
<em>TNNLS</em>, <em>35</em>(10), 14437–14448. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph-based multi-view clustering (GMC) has attracted extensive attention from researchers, in which multi-view clustering based on structured graph learning (SGL) can be considered as one of the most interesting branches, achieving promising performance. However, most of the existing SGL methods suffer from sparse graphs lacking useful information, which normally appears in practice. To alleviate this problem, we propose a novel multi-view and multi-order SGL ( $\text{M}^{2}$ SGL) model which introduces multiple different orders (multi-order) graphs into the SGL procedure reasonably. To be more specific, $\text{M}^{2}$ SGL designs a two-layer weighted-learning mechanism, in which the first layer truncatedly selects part of views in different orders to retain the most useful information, and the second layer assigns smooth weights into retained multi-order graphs to fuse them attentively. Moreover, an iterative optimization algorithm is derived to solve the optimization problem involved in $\text{M}^{2}$ SGL, and the corresponding theoretical analyses are provided. In experiments, extensive empirical results demonstrate that the proposed $\text{M}^{2}$ SGL model achieves the state-of-the-art performance in several benchmarks.},
  archive      = {J_TNNLS},
  author       = {Rong Wang and Penglei Wang and Danyang Wu and Zhensheng Sun and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3279133},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14437-14448},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-view and multi-order structured graph learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Staleness-reduction mini-batch k-means. <em>TNNLS</em>,
<em>35</em>(10), 14424–14436. (<a
href="https://doi.org/10.1109/TNNLS.2023.3279122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = { $K$ -means (km) is a clustering algorithm that has been widely adopted due to its simple implementation and high clustering quality. However, the standard km suffers from high computational complexity and is therefore time-consuming. Accordingly, the mini-batch (mbatch) km is proposed to significantly reduce computational costs in a manner that updates centroids after performing distance computations on just a mbatch, rather than a full batch, of samples. Even though the mbatch km converges faster, it leads to a decrease in convergence quality because it introduces staleness during iterations. To this end, in this article, we propose the staleness-reduction mbatch (srmbatch) km, which achieves the best of two worlds: low computational costs like the mbatch km and high clustering quality like the standard km. Moreover, srmbatch still exposes massive parallelism to be efficiently implemented on multicore CPUs and many-core GPUs. The experimental results show that srmbatch can converge up to $40\times $ – $130\times $ faster than mbatch when reaching the same target loss, and srmbatch is able to reach 0.2%–1.7% lower final loss than that of mbatch.},
  archive      = {J_TNNLS},
  author       = {Xueying Zhu and Jie Sun and Zhenhao He and Jiantong Jiang and Zeke Wang},
  doi          = {10.1109/TNNLS.2023.3279122},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14424-14436},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Staleness-reduction mini-batch K-means},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep into the domain shift: Transfer learning through
dependence regularization. <em>TNNLS</em>, <em>35</em>(10), 14409–14423.
(<a href="https://doi.org/10.1109/TNNLS.2023.3279099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical domain adaptation methods acquire transferability by regularizing the overall distributional discrepancies between features in the source domain (labeled) and features in the target domain (unlabeled). They often do not differentiate whether the domain differences come from the marginals or the dependence structures. In many business and financial applications, the labeling function usually has different sensitivities to the changes in the marginals versus changes in the dependence structures. Measuring the overall distributional differences will not be discriminative enough in acquiring transferability. Without the needed structural resolution, the learned transfer is less optimal. This article proposes a new domain adaptation approach in which one can measure the differences in the internal dependence structure separately from those in the marginals. By optimizing the relative weights among them, the new regularization strategy greatly relaxes the rigidness of the existing approaches. It allows a learning machine to pay special attention to places where the differences matter the most. Experiments on three real-world datasets show that the improvements are quite notable and robust compared to various benchmark domain adaptation models.},
  archive      = {J_TNNLS},
  author       = {Shumin Ma and Zhiri Yuan and Qi Wu and Yiyan Huang and Xixu Hu and Cheuk Hang Leung and Dongdong Wang and Zhixiang Huang},
  doi          = {10.1109/TNNLS.2023.3279099},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14409-14423},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep into the domain shift: Transfer learning through dependence regularization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot relation extraction with dual graph neural network
interaction. <em>TNNLS</em>, <em>35</em>(10), 14396–14408. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in relation extraction with deep neural architectures have achieved excellent performance. However, current models still suffer from two main drawbacks: 1) they require enormous volumes of training data to avoid model overfitting and 2) there is a sharp decrease in performance when the data distribution during training and testing shift from one domain to the other. It is thus vital to reduce the data requirement in training and explicitly model the distribution difference when transferring knowledge from one domain to another. In this work, we concentrate on few-shot relation extraction under domain adaptation settings. Specifically, we propose DUAL GRAPH, a novel graph neural network (GNN) based approach for few-shot relation extraction. DUAL GRAPH leverages an edge-labeling dual graph (i.e., an instance graph and a distribution graph) to explicitly model the intraclass similarity and interclass dissimilarity in each individual graph, as well as the instance-level and distribution-level relations across graphs. A dual graph interaction mechanism is proposed to adequately fuse the information between the two graphs in a cyclic flow manner. We extensively evaluate DUAL GRAPH on FewRel1.0 and FewRel2.0 benchmarks under four few-shot configurations. The experimental results demonstrate that DUAL GRAPH can match or outperform previously published approaches. We also perform experiments to further investigate the parameter settings and architectural choices, and we offer a qualitative analysis.},
  archive      = {J_TNNLS},
  author       = {Jing Li and Shanshan Feng and Billy Chiu},
  doi          = {10.1109/TNNLS.2023.3278938},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14396-14408},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Few-shot relation extraction with dual graph neural network interaction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable model-driven deep network for hyperspectral,
multispectral, and panchromatic image fusion. <em>TNNLS</em>,
<em>35</em>(10), 14382–14395. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneously fusing hyperspectral (HS), multispectral (MS), and panchromatic (PAN) images brings a new paradigm to generate a high-resolution HS (HRHS) image. In this study, we propose an interpretable model-driven deep network for HS, MS, and PAN image fusion, called HMPNet. We first propose a new fusion model that utilizes a deep before describing the complicated relationship between the HRHS and PAN images owing to their large resolution difference. Consequently, the difficulty of traditional model-based approaches in designing suitable hand-crafted priors can be alleviated because this deep prior is learned from data. We further solve the optimization problem of this fusion model based on the proximal gradient descent (PGD) algorithm, achieved by a series of iterative steps. By unrolling these iterative steps into several network modules, we finally obtain the HMPNet. Therefore, all parameters besides the deep prior are learned in the deep network, simplifying the selection of optimal parameters in the fusion and achieving a favorable equilibrium between the spatial and spectral qualities. Meanwhile, all modules contained in the HMPNet have explainable physical meanings, which can improve its generalization capability. In the experiment, we exhibit the advantages of the HMPNet over other state-of-the-art methods from the aspects of visual comparison and quantitative analysis, where a series of simulated as well as real datasets are utilized for validation.},
  archive      = {J_TNNLS},
  author       = {Xin Tian and Kun Li and Wei Zhang and Zhongyuan Wang and Jiayi Ma},
  doi          = {10.1109/TNNLS.2023.3278928},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14382-14395},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interpretable model-driven deep network for hyperspectral, multispectral, and panchromatic image fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust mean-field actor-critic reinforcement learning
against adversarial perturbations on agent states. <em>TNNLS</em>,
<em>35</em>(10), 14370–14381. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent deep reinforcement learning (DRL) makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The mean-field actor-critic (MFAC) reinforcement learning is well-known in the multiagent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust MFAC (RoMFAC) reinforcement learning that has two innovations: 1) a new objective function of training actors, composed of a policy gradient function that is related to the expected cumulative discount reward on sampled clean states and an action loss function that represents the difference between actions taken on clean and adversarial states and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a game model named a state-adversarial stochastic game (SASG). Despite the Nash equilibrium of SASG may not exist, adversarial perturbations to states in the RoMFAC are proven to be defensible based on SASG. Experimental results show that RoMFAC is robust against adversarial perturbations while maintaining its competitive performance in environments without perturbations.},
  archive      = {J_TNNLS},
  author       = {Ziyuan Zhou and Guanjun Liu and Mengchu Zhou},
  doi          = {10.1109/TNNLS.2023.3278715},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14370-14381},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A robust mean-field actor-critic reinforcement learning against adversarial perturbations on agent states},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interest HD: An interest frame model for recommendation
based on HD image generation. <em>TNNLS</em>, <em>35</em>(10),
14356–14369. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is inspired by high-definition (HD) image generation techniques. When the user’s interests are viewed as different frames of varying clarity, the unclear parts of one interest frame can be clarified by other interest frames. The user’s overall HD interest portrait can be viewed as a fusion of multiple interest frames through detail compensation. Based on this inspiration, we propose a model for generating HD interest portrait called interest frame for recommendation (IF4Rec). First, we present a fine-grained pixel-level user interest mining method, Pixel embedding (PE) uses positional coding techniques to mine atomic-level interest pixel matrices in multiple dimensions, such as time, space, and frequency. Then, using an atomic-level interest pixel matrix, we propose Item2Frame to generate several interest frames for a user. The similarity score of each item is calculated to fill the multi-interest pixel clusters, through an improved self-attention mechanism. Finally, stimulated by HD image generation techniques, we initially present an interest frame noise compensation method. By utilizing the multihead attention mechanism, pixel-level optimization and noise complementation are performed between multi-interest frames, and an HD interest portrait is achieved. Experiments show that our model mines users’ interests well. On five publicly available datasets, our model outperforms the baselines.},
  archive      = {J_TNNLS},
  author       = {Weikang He and Yunpeng Xiao and Tun Li and Rong Wang and Qian Li},
  doi          = {10.1109/TNNLS.2023.3278673},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14356-14369},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interest HD: An interest frame model for recommendation based on HD image generation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semisupervised progressive representation learning for deep
multiview clustering. <em>TNNLS</em>, <em>35</em>(10), 14341–14355. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering has become a research hotspot in recent years due to its excellent capability of heterogeneous data fusion. Although a great deal of related works has appeared one after another, most of them generally overlook the potentials of prior knowledge utilization and progressive sample learning, resulting in unsatisfactory clustering performance in real-world applications. To deal with the aforementioned drawbacks, in this article, we propose a semisupervised progressive representation learning approach for deep multiview clustering (namely, SPDMC). Specifically, to make full use of the discriminative information contained in prior knowledge, we design a flexible and unified regularization, which models the sample pairwise relationship by enforcing the learned view-specific representation of must-link (ML) samples (cannot-link (CL) samples) to be similar (dissimilar) with cosine similarity. Moreover, we introduce the self-paced learning (SPL) paradigm and take good care of two characteristics in terms of both complexity and diversity when progressively learning multiview representations, such that the complementarity across multiple views can be squeezed thoroughly. Through comprehensive experiments on eight widely used image datasets, we prove that the proposed approach can perform better than the state-of-the-art opponents.},
  archive      = {J_TNNLS},
  author       = {Rui Chen and Yongqiang Tang and Yuan Xie and Wenlong Feng and Wensheng Zhang},
  doi          = {10.1109/TNNLS.2023.3278379},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14341-14355},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised progressive representation learning for deep multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered adaptive neural impedance control of robotic
systems. <em>TNNLS</em>, <em>35</em>(10), 14330–14340. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an event-triggered adaptive neural impedance control (ETANIC) scheme for robotic systems, where the combination of impedance control (IC) and event-triggered mechanism can significantly reduce the computational burden and the communication cost under the premise of ensuring the stability and tracking performances of the robotic systems. The IC is used to achieve the compliant behavior of the robotic systems in response to the environment. The uncertainties of the robotic systems are estimated by the radial basis function neural network (RBFNN), and the update laws for RBFNN are derived from the designed Lyapunov function. The stability of the whole closed-loop control system is analyzed by the Lyapunov theory, and the event-triggered conditions are designed to avoid the Zeno behavior. The numerical simulation and experimental tests demonstrate that the proposed ETANIC scheme can achieve better efficiency for controlling the robotic systems to perform the interaction tasks with the environment in comparison to the adaptive neural IC (ANIC).},
  archive      = {J_TNNLS},
  author       = {Shuai Ding and Jinzhu Peng and Hui Zhang and Yaonan Wang},
  doi          = {10.1109/TNNLS.2023.3278301},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14330-14340},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive neural impedance control of robotic systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatial–channel–temporal-fused attention for spiking
neural networks. <em>TNNLS</em>, <em>35</em>(10), 14315–14329. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) mimic brain computational strategies, and exhibit substantial capabilities in spatiotemporal information processing. As an essential factor for human perception, visual attention refers to the dynamic process for selecting salient regions in biological vision systems. Although visual attention mechanisms have achieved great success in computer vision applications, they are rarely introduced into SNNs. Inspired by experimental observations on predictive attentional remapping, we propose a new spatial-channel–temporal-fused attention (SCTFA) module that can guide SNNs to efficiently capture underlying target regions by utilizing accumulated historical spatial–channel information in the present study. Through a systematic evaluation on three event stream datasets (DVS Gesture, SL-Animals-DVS, and MNIST-DVS), we demonstrate that the SNN with the SCTFA module (SCTFA-SNN) not only significantly outperforms the baseline SNN (BL-SNN) and two other SNN models with degenerated attention modules, but also achieves competitive accuracy with the existing state-of-the-art (SOTA) methods. Additionally, our detailed analysis shows that the proposed SCTFA-SNN model has strong robustness to noise and outstanding stability when faced with incomplete data, while maintaining acceptable complexity and efficiency. Overall, these findings indicate that incorporating appropriate cognitive mechanisms of the brain may provide a promising approach to elevate the capabilities of SNNs.},
  archive      = {J_TNNLS},
  author       = {Wuque Cai and Hongze Sun and Rui Liu and Yan Cui and Jun Wang and Yang Xia and Dezhong Yao and Daqing Guo},
  doi          = {10.1109/TNNLS.2023.3278265},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14315-14329},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A Spatial–Channel–Temporal-fused attention for spiking neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph contrastive learning with adaptive proximity-based
graph augmentation. <em>TNNLS</em>, <em>35</em>(10), 14301–14314. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been successful in a variety of graph-based applications. Recently, it is shown that capturing long-range relationships between nodes helps improve the performance of GNNs. The phenomenon is mostly confirmed in a supervised learning setting. In this article, inspired by contrastive learning (CL), we propose an unsupervised learning pipeline, in which different types of long-range similarity information are injected into the GNN model in an efficient way. We reconstruct the original graph in feature and topology spaces to generate three augmented views. During training, our model alternately picks an augmented view, and maximizes an agreement between the representations of the view and the original graph. Importantly, we identify the issue of diminishing utility of the augmented views as the model gradually learns useful information from the views. Hence, we propose a view update scheme that adaptively adjusts the augmented views, so that the views can continue to provide new information that helps with CL. The updated augmented views and the original graph are jointly used to train a shared GNN encoder by optimizing an efficient channel-level contrastive objective. We conduct extensive experiments on six assortative graphs and three disassortative graphs, which demonstrate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Wei Zhuo and Guang Tan},
  doi          = {10.1109/TNNLS.2023.3278183},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14301-14314},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph contrastive learning with adaptive proximity-based graph augmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative deep canonical correlation analysis for
multi-view data. <em>TNNLS</em>, <em>35</em>(10), 14288–14300. (<a
href="https://doi.org/10.1109/TNNLS.2023.3277633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, multimodal data analysis has emerged as an inevitable method for identifying sample categories. In the multi-view data classification problem, it is expected that the joint representation should include the supervised information of sample categories so that the similarity in the latent space implies the similarity in the corresponding concepts. Since each view has different statistical properties, the joint representation should be able to encapsulate the underlying nonlinear data distribution of the given observations. Another important aspect is the coherent knowledge of the multiple views. It is required that the learning objective of the multi-view model efficiently captures the nonlinear correlated structures across different modalities. In this context, this article introduces a novel architecture, termed discriminative deep canonical correlation analysis (D2CCA), for classifying given observations into multiple categories. The learning objective of the proposed architecture includes the merits of generative models to identify the underlying probability distribution of the given observations. In order to improve the discriminative ability of the proposed architecture, the supervised information is incorporated into the learning objective of the proposed model. It also enables the architecture to serve as both a feature extractor as well as a classifier. The theory of CCA is integrated with the objective function so that the joint representation of the multi-view data is learned from maximally correlated subspaces. The proposed framework is consolidated with corresponding convergence analysis. The efficacy of the proposed architecture is studied on different domains of applications, namely, object recognition, document classification, multilingual categorization, face recognition, and cancer subtype identification with reference to several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Debamita Kumar and Pradipta Maji},
  doi          = {10.1109/TNNLS.2023.3277633},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14288-14300},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative deep canonical correlation analysis for multi-view data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning constrained dynamic correlations in spatiotemporal
graphs for motion prediction. <em>TNNLS</em>, <em>35</em>(10),
14273–14287. (<a
href="https://doi.org/10.1109/TNNLS.2023.3277476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion prediction is challenging due to the complex spatiotemporal feature modeling. Among all methods, graph convolution networks (GCNs) are extensively utilized because of their superiority in explicit connection modeling. Within a GCN, the graph correlation adjacency matrix drives feature aggregation, and thus, is the key to extracting predictive motion features. State-of-the-art methods decompose the spatiotemporal correlation into spatial correlations for each frame and temporal correlations for each joint. Directly parameterizing these correlations introduces redundant parameters to represent common relations shared by all frames and all joints. Besides, the spatiotemporal graph adjacency matrix is the same for different motion samples, and thus, cannot reflect samplewise correspondence variances. To overcome these two bottlenecks, we propose dynamic spatiotemporal decompose GC (DSTD-GC), which only takes 28.6% parameters of the state-of-the-art GC. The key of DSTD-GC is constrained dynamic correlation modeling, which explicitly parameterizes the common static constraints as a spatial/temporal vanilla adjacency matrix shared by all frames/joints and dynamically extracts correspondence variances for each frame/joint with an adjustment modeling function. For each sample, the common constrained adjacency matrices are fixed to represent generic motion patterns, while the extracted variances complete the matrices with specific pattern adjustments. Meanwhile, we mathematically reformulate GCs on spatiotemporal graphs into a unified form and find that DSTD-GC relaxes certain constraints of other GC, which contributes to a better representation capability. Moreover, by combining DSTD-GC with prior knowledge like body connection and temporal context, we propose a powerful spatiotemporal GCN called DSTD-GCN. On the Human3.6M, Carnegie Mellon University (CMU) Mocap, and 3D Poses in the Wild (3DPW) datasets, DSTD-GCN outperforms state-of-the-art methods by 3.9%–8.7% in prediction accuracy with 55.0%–96.9% fewer parameters. Codes are available at https://github.com/Jaakk0F/DSTD-GCN .},
  archive      = {J_TNNLS},
  author       = {Jiajun Fu and Fuxing Yang and Yonghao Dang and Xiaoli Liu and Jianqin Yin},
  doi          = {10.1109/TNNLS.2023.3277476},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14273-14287},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning constrained dynamic correlations in spatiotemporal graphs for motion prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware distillation for semi-supervised few-shot
class-incremental learning. <em>TNNLS</em>, <em>35</em>(10),
14259–14272. (<a
href="https://doi.org/10.1109/TNNLS.2023.3277018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a model well-trained with a large-scale base dataset, few-shot class-incremental learning (FSCIL) aims at incrementally learning novel classes from a few labeled samples by avoiding overfitting, without catastrophically forgetting all encountered classes previously. Currently, semi-supervised learning technique that harnesses freely available unlabeled data to compensate for limited labeled data can boost the performance in numerous vision tasks, which heuristically can be applied to tackle issues in FSCIL, i.e., the semi-supervised FSCIL (Semi-FSCIL). So far, very limited work focuses on the Semi-FSCIL task, leaving the adaptability issue of semi-supervised learning to the FSCIL task unresolved. In this article, we focus on this adaptability issue and present a simple yet efficient Semi-FSCIL framework named uncertainty-aware distillation with class-equilibrium (UaD-ClE), encompassing two modules: uncertainty-aware distillation (UaD) and class equilibrium (ClE). Specifically, when incorporating unlabeled data into each incremental session, we introduce the ClE module that employs a class-balanced self-training (CB_ST) to avoid the gradual dominance of easy-to-classified classes on pseudo-label generation. To distill reliable knowledge from the reference model, we further implement the UaD module that combines uncertainty-guided knowledge refinement with adaptive distillation. Comprehensive experiments on three benchmark datasets demonstrate that our method can boost the adaptability of unlabeled data with the semi-supervised learning technique in FSCIL tasks. The code is available at https://github.com/yawencui/UaD-ClE .},
  archive      = {J_TNNLS},
  author       = {Yawen Cui and Wanxia Deng and Haoyu Chen and Li Liu},
  doi          = {10.1109/TNNLS.2023.3277018},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14259-14272},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Uncertainty-aware distillation for semi-supervised few-shot class-incremental learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). BCAN: Bidirectional correct attention network for
cross-modal retrieval. <em>TNNLS</em>, <em>35</em>(10), 14247–14258. (<a
href="https://doi.org/10.1109/TNNLS.2023.3276796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental topic in bridging the gap between vision and language, cross-modal retrieval purposes to obtain the correspondences’ relationship between fragments, i.e., subregions in images and words in texts. Compared with earlier methods that focus on learning the visual semantic embedding from images and sentences to the shared embedding space, the existing methods tend to learn the correspondences between words and regions via cross-modal attention. However, such attention-based approaches invariably result in semantic misalignment between subfragments for two reasons: 1) without modeling the relationship between subfragments and the semantics of the entire images or sentences, it will be hard for such approaches to distinguish images or sentences with multiple same semantic fragments and 2) such approaches focus attention evenly on all subfragments, including nonvisual words and a lot of redundant regions, which also will face the problem of semantic misalignment. To solve these problems, this article proposes a bidirectional correct attention network (BCAN), which introduces a novel concept of the relevance between subfragments and the semantics of the entire images or sentences and designs a novel correct attention mechanism by modeling the local and global similarity between images and sentences to correct the attention weights focused on the wrong fragments. Specifically, we introduce a concept about the semantic relationship between subfragments and entire images or sentences and use this concept to solve the semantic misalignment from two aspects. In our correct attention mechanism, we design two independent units to correct the weight of attention focused on the wrong fragments. Global correct unit (GCU) with modeling the global similarity between images and sentences into the attention mechanism to solve the semantic misalignment problem caused by focusing attention on relevant subfragments in irrelevant pairs (RI) and the local correct unit (LCU) consider the difference in the attention weights between fragments among two steps to solve the semantic misalignment problem caused by focusing attention on irrelevant subfragments in relevant pairs (IR). Extensive experiments on large-scale MS-COCO and Flickr30K show that our proposed method outperforms all the attention-based methods and is competitive to the state-of-the-art. Our code and pretrained model are publicly available at: https://github.com/liuyyy111/BCAN .},
  archive      = {J_TNNLS},
  author       = {Yang Liu and Hong Liu and Huaqiu Wang and Fanyang Meng and Mengyuan Liu},
  doi          = {10.1109/TNNLS.2023.3276796},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14247-14258},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BCAN: Bidirectional correct attention network for cross-modal retrieval},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Routing user-interest markov tree for scalable personalized
knowledge-aware recommendation. <em>TNNLS</em>, <em>35</em>(10),
14233–14246. (<a
href="https://doi.org/10.1109/TNNLS.2023.3276395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To facilitate more accurate and explainable recommendation, it is crucial to incorporate side information into user-item interactions. Recently, knowledge graph (KG) has attracted much attention in a variety of domains due to its fruitful facts and abundant relations. However, the expanding scale of real-world data graphs poses severe challenges. In general, most existing KG-based algorithms adopt exhaustively hop-by-hop enumeration strategy to search all the possible relational paths, this manner involves extremely high-cost computations and is not scalable with the increase of hop numbers. To overcome these difficulties, in this article, we propose an end-to-end framework Knowledge-tree-routed UseR-Interest Trajectories Network (KURIT-Net). KURIT-Net employs the user-interest Markov trees (UIMTs) to reconfigure a recommendation-based KG, striking a good balance for routing knowledge between short-distance and long-distance relations between entities. Each tree starts from the preferred items for a user and routes the association reasoning paths along the entities in the KG to provide a human-readable explanation for model prediction. KURIT-Net receives entity and relation trajectory embedding (RTE) and fully reflects potential interests of each user by summarizing all reasoning paths in a KG. Besides, we conduct extensive experiments on six public datasets, our KURIT-Net significantly outperforms state-of-the-art approaches and shows its interpretability in recommendation.},
  archive      = {J_TNNLS},
  author       = {Yongsen Zheng and Pengxu Wei and Ziliang Chen and Chengpei Tang and Liang Lin},
  doi          = {10.1109/TNNLS.2023.3276395},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14233-14246},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Routing user-interest markov tree for scalable personalized knowledge-aware recommendation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An evidential multi-target domain adaptation method based on
weighted fusion for cross-domain pattern classification. <em>TNNLS</em>,
<em>35</em>(10), 14218–14232. (<a
href="https://doi.org/10.1109/TNNLS.2023.3275759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For cross-domain pattern classification, the supervised information (i.e., labeled patterns) in the source domain is often employed to help classify the unlabeled target domain patterns. In practice, multiple target domains are usually available. The unlabeled patterns (in different target domains) which have high-confidence predictions, can also provide some pseudo-supervised information for the downstream classification task. The performance in each target domain would be further improved if the pseudo-supervised information in different target domains can be effectively used. To this end, we propose an evidential multi-target domain adaptation (EMDA) method to take full advantage of the useful information in the single-source and multiple target domains. In EMDA, we first align distributions of the source and target domains by reducing maximum mean discrepancy (MMD) and covariance difference across domains. After that, we use the classifier learned by the labeled source domain data to classify query patterns in the target domains. The query patterns with high-confidence predictions are then selected to train a new classifier for yielding an extra piece of soft classification results of query patterns. The two pieces of soft classification results are then combined by evidence theory. In practice, their reliabilities/weights are usually diverse, and an equal treatment of them often yields the unreliable combination result. Thus, we propose to use the distribution discrepancy across domains to estimate their weighting factors, and discount them before fusing. The evidential combination of the two pieces of discounted soft classification results is employed to make the final class decision. The effectiveness of EMDA was verified by comparing with many advanced domain adaptation methods on several cross-domain pattern classification benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Linqing Huang and Wangbo Zhao and Yong Liu and Duo Yang and Alan Wee-Chung Liew and Yang You},
  doi          = {10.1109/TNNLS.2023.3275759},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14218-14232},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An evidential multi-target domain adaptation method based on weighted fusion for cross-domain pattern classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TrustGNN: Graph neural network-based trust evaluation via
learnable propagative and composable nature. <em>TNNLS</em>,
<em>35</em>(10), 14205–14217. (<a
href="https://doi.org/10.1109/TNNLS.2023.3275634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust evaluation is critical for many applications such as cyber security, social communication, and recommender systems. Users and trust relationships among them can be seen as a graph. Graph neural networks (GNNs) show their powerful ability for analyzing graph-structural data. Very recently, existing work attempted to introduce the attributes and asymmetry of edges into GNNs for trust evaluation, while failed to capture some essential properties (e.g., the propagative and composable nature) of trust graphs. In this work, we propose a new GNN-based trust evaluation method named TrustGNN, which integrates smartly the propagative and composable nature of trust graphs into a GNN framework for better trust evaluation. Specifically, TrustGNN designs specific propagative patterns for different propagative processes of trust, and distinguishes the contribution of different propagative processes to create new trust. Thus, TrustGNN can learn comprehensive node embeddings and predict trust relationships based on these embeddings. Experiments on some widely-used real-world datasets indicate that TrustGNN significantly outperforms the state-of-the-art methods. We further perform analytical experiments to demonstrate the effectiveness of the key designs in TrustGNN.},
  archive      = {J_TNNLS},
  author       = {Cuiying Huo and Dongxiao He and Chundong Liang and Di Jin and Tie Qiu and Lingfei Wu},
  doi          = {10.1109/TNNLS.2023.3275634},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14205-14217},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TrustGNN: Graph neural network-based trust evaluation via learnable propagative and composable nature},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum imitation learning. <em>TNNLS</em>, <em>35</em>(10),
14190–14204. (<a
href="https://doi.org/10.1109/TNNLS.2023.3275075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite remarkable successes in solving various complex decision-making tasks, training an imitation learning (IL) algorithm with deep neural networks (DNNs) suffers from the high-computational burden. In this work, we propose quantum IL (QIL) with a hope to utilize quantum advantage to speed up IL. Concretely, we develop two QIL algorithms: quantum behavioral cloning (Q-BC) and quantum generative adversarial IL (Q-GAIL). Q-BC is trained with a negative log-likelihood (NLL) loss in an offline manner that suits extensive expert data cases, whereas Q-GAIL works in an inverse reinforcement learning (IRL) scheme, which is online, on-policy, and is suitable for limited expert data cases. For both QIL algorithms, we adopt variational quantum circuits (VQCs) in place of DNNs for representing policies, which are modified with data reuploading and scaling parameters to enhance the expressivity. We first encode classical data into quantum states as inputs, then perform VQCs, and finally measure quantum outputs to obtain control signals of agents. Experiment results demonstrate that both Q-BC and Q-GAIL can achieve comparable performance compared to classical counterparts, with the potential of quantum speedup. To our knowledge, we are the first to propose the concept of QIL and conduct pilot studies, which paves the way for the quantum era.},
  archive      = {J_TNNLS},
  author       = {Zhihao Cheng and Kaining Zhang and Li Shen and Dacheng Tao},
  doi          = {10.1109/TNNLS.2023.3275075},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14190-14204},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quantum imitation learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronization analysis of discrete-time fractional-order
quaternion-valued uncertain neural networks. <em>TNNLS</em>,
<em>35</em>(10), 14178–14189. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies synchronization issues for a class of discrete-time fractional-order quaternion-valued uncertain neural networks (DFQUNNs) using nonseparation method. First, based on the theory of discrete-time fractional calculus and quaternion properties, two equalities on the nabla Laplace transform and nabla sum are strictly proved, whereafter three Caputo difference inequalities are rigorously demonstrated. Next, based on our established inequalities and equalities, some simple and verifiable quasi-synchronization criteria are derived under the quaternion-valued nonlinear controller, and complete synchronization is achieved using quaternion-valued adaptive controller. Finally, numerical simulations are presented to substantiate the validity of derived results.},
  archive      = {J_TNNLS},
  author       = {Hong-Li Li and Jinde Cao and Cheng Hu and Haijun Jiang and Fawaz E. Alsaadi},
  doi          = {10.1109/TNNLS.2023.3274959},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14178-14189},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization analysis of discrete-time fractional-order quaternion-valued uncertain neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image enhancement guided object detection in visually
degraded scenes. <em>TNNLS</em>, <em>35</em>(10), 14164–14177. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection accuracy degrades seriously in visually degraded scenes. A natural solution is to first enhance the degraded image and then perform object detection. However, it is suboptimal and does not necessarily lead to the improvement of object detection due to the separation of the image enhancement and object detection tasks. To solve this problem, we propose an image enhancement guided object detection method, which refines the detection network with an additional enhancement branch in an end-to-end way. Specifically, the enhancement branch and detection branch are organized in a parallel way, and a feature guided module is designed to connect the two branches, which optimizes the shallow feature of the input image in the detection branch to be as consistent as possible with that of the enhanced image. As the enhancement branch is frozen during training, such a design plays a role in using the features of enhanced images to guide the learning of object detection branch, so as to make the learned detection branch being aware of both image quality and object detection. When testing, the enhancement branch and feature guided module are removed, and so no additional computation cost is introduced for detection. Extensive experimental results, on underwater, hazy, and low-light object detection datasets, demonstrate that the proposed method can improve the detection performance of popular detection networks (YOLO v3, Faster R-CNN, DetectoRS) significantly in visually degraded scenes.},
  archive      = {J_TNNLS},
  author       = {Hongmin Liu and Fan Jin and Hui Zeng and Huayan Pu and Bin Fan},
  doi          = {10.1109/TNNLS.2023.3274926},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14164-14177},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Image enhancement guided object detection in visually degraded scenes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Taming overconfident prediction on unlabeled data from
hindsight. <em>TNNLS</em>, <em>35</em>(10), 14151–14163. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing prediction uncertainty on unlabeled data is a key factor to achieve good performance in semi-supervised learning (SSL). The prediction uncertainty is typically expressed as the entropy computed by the transformed probabilities in output space. Most existing works distill low-entropy prediction by either accepting the determining class (with the largest probability) as the true label or suppressing subtle predictions (with the smaller probabilities). Unarguably, these distillation strategies are usually heuristic and less informative for model training. From this discernment, this article proposes a dual mechanism, named adaptive sharpening (ADS), which first applies a soft-threshold to adaptively mask out determinate and negligible predictions, and then seamlessly sharpens the informed predictions, distilling certain predictions with the informed ones only. More importantly, we theoretically analyze the traits of ADS by comparing it with various distillation strategies. Numerous experiments verify that ADS significantly improves state-of-the-art SSL methods by making it a plug-in. Our proposed ADS forges a cornerstone for future distillation-based SSL research.},
  archive      = {J_TNNLS},
  author       = {Jing Li and Yuangang Pan and Ivor W. Tsang},
  doi          = {10.1109/TNNLS.2023.3274845},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14151-14163},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Taming overconfident prediction on unlabeled data from hindsight},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A patch diversity transformer for domain generalized
semantic segmentation. <em>TNNLS</em>, <em>35</em>(10), 14138–14150. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) is one of the critical issues for deep learning in unknown domains. How to effectively represent domain-invariant context (DIC) is a difficult problem that DG needs to solve. Transformers have shown the potential to learn generalized features, since the powerful ability to learn global context. In this article, a novel method named patch diversity Transformer (PDTrans) is proposed to improve the DG for scene segmentation by learning global multidomain semantic relations. Specifically, patch photometric perturbation (PPP) is proposed to improve the representation of multidomain in the global context information, which helps the Transformer learn the relationship between multiple domains. Besides, patch statistics perturbation (PSP) is proposed to model the feature statistics of patches under different domain shifts, which enables the model to encode domain-invariant semantic features and improve generalization. PPP and PSP can help to diversify the source domain at the patch level and feature level. PDTrans learns context across diverse patches and takes advantage of self-attention to improve DG. Extensive experiments demonstrate the tremendous performance advantages of the PDTrans over state-of-the-art DG methods.},
  archive      = {J_TNNLS},
  author       = {Pei He and Licheng Jiao and Ronghua Shang and Xu Liu and Fang Liu and Shuyuan Yang and Xiangrong Zhang and Shuang Wang},
  doi          = {10.1109/TNNLS.2023.3274760},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14138-14150},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A patch diversity transformer for domain generalized semantic segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep ring-block-wise network for hyperspectral image
classification. <em>TNNLS</em>, <em>35</em>(10), 14125–14137. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved many successes in the field of the hyperspectral image (HSI) classification. Most of existing deep learning-based methods have no consideration of feature distribution, which may yield lowly separable and discriminative features. From the perspective of spatial geometry, one excellent feature distribution form requires to satisfy both properties, i.e., block and ring. The block means that in a feature space, the distance of intraclass samples is close and the one of interclass samples is far. The ring represents that all class samples are overall distributed in a ring topology. Accordingly, in this article, we propose a novel deep ring-block-wise network (DRN) for the HSI classification, which takes full consideration of feature distribution. To obtain the good distribution used for high classification performance, in this DRN, a ring-block perception (RBP) layer is built by integrating the self-representation and ring loss into a perception model. By such way, the exported features are imposed to follow the requirements of both block and ring, so as to be more separably and discriminatively distributed compared with traditional deep networks. Besides, we also design an optimization strategy with alternating update to obtain the solution of this RBP layer model. Extensive results on the Salinas, Pavia Centre, Indian Pines, and Houston datasets have demonstrated that the proposed DRN method achieves the better classification performance in contrast to the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Changda Xing and Jianlong Zhao and Zhisheng Wang and Meiling Wang},
  doi          = {10.1109/TNNLS.2023.3274745},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14125-14137},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep ring-block-wise network for hyperspectral image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Rumor detection with hierarchical representation on
bipartite ad hoc event trees. <em>TNNLS</em>, <em>35</em>(10),
14112–14124. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of social media has caused tremendous effects on information propagation, raising extreme challenges in detecting rumors. Existing rumor detection methods typically exploit the reposting propagation of a rumor candidate for detection by regarding all reposts to a rumor candidate as a temporal sequence and learning semantics representations of the repost sequence. However, extracting informative support from the topological structure of propagation and the influence of reposting authors for debunking rumors is crucial, which generally has not been well addressed by existing methods. In this article, we organize a claim post in circulation as an ad hoc event tree, extract event elements, and convert it into bipartite ad hoc event trees in terms of both posts and authors, i.e., author tree and post tree. Accordingly, we propose a novel rumor detection model with hierarchical representation on the bipartite ad hoc event trees called BAET. Specifically, we introduce word embedding and feature encoder for the author and post tree, respectively, and design a root-aware attention module to perform node representation. Then we adopt the tree-like RNN model to capture the structural correlations and propose a tree-aware attention module to learn tree representation for the author tree and post tree, respectively. Extensive experimental results on two public Twitter datasets demonstrate the effectiveness of BAET in exploring and exploiting the rumor propagation structure and the superior detection performance of BAET over state-of-the-art baseline methods.},
  archive      = {J_TNNLS},
  author       = {Qi Zhang and Yayi Yang and Chongyang Shi and An Lao and Liang Hu and Shoujin Wang and Usman Naseem},
  doi          = {10.1109/TNNLS.2023.3274694},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14112-14124},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rumor detection with hierarchical representation on bipartite ad hoc event trees},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Talking face generation with audio-deduced emotional
landmarks. <em>TNNLS</em>, <em>35</em>(10), 14099–14111. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of talking face generation is to synthesize a sequence of face images of the specified identity, ensuring the mouth movements are synchronized with the given audio. Recently, image-based talking face generation has emerged as a popular approach. It could generate talking face images synchronized with the audio merely depending on a facial image of arbitrary identity and an audio clip. Despite the accessible input, it forgoes the exploitation of the audio emotion, inducing the generated faces to suffer from emotion unsynchronization, mouth inaccuracy, and image quality deficiency. In this article, we build a bistage audio emotion-aware talking face generation (AMIGO) framework, to generate high-quality talking face videos with cross-modally synced emotion. Specifically, we propose a sequence-to-sequence (seq2seq) cross-modal emotional landmark generation network to generate vivid landmarks, whose lip and emotion are both synchronized with input audio. Meantime, we utilize a coordinated visual emotion representation to improve the extraction of the audio one. In stage two, a feature-adaptive visual translation network is designed to translate the synthesized landmarks into facial images. Concretely, we proposed a feature-adaptive transformation module to fuse the high-level representations of landmarks and images, resulting in significant improvement in image quality. We perform extensive experiments on the multi-view emotional audio-visual dataset (MEAD) and crowd-sourced emotional multimodal actors dataset (CREMA-D) benchmark datasets, demonstrating that our model outperforms state-of-the-art benchmarks.},
  archive      = {J_TNNLS},
  author       = {Shuyan Zhai and Meng Liu and Yongqiang Li and Zan Gao and Lei Zhu and Liqiang Nie},
  doi          = {10.1109/TNNLS.2023.3274676},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14099-14111},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Talking face generation with audio-deduced emotional landmarks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering aided weakly supervised training to detect
anomalous events in surveillance videos. <em>TNNLS</em>,
<em>35</em>(10), 14085–14098. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formulating learning systems for the detection of real-world anomalous events using only video-level labels is a challenging task mainly due to the presence of noisy labels as well as the rare occurrence of anomalous events in the training data. We propose a weakly supervised anomaly detection system that has multiple contributions including a random batch selection mechanism to reduce interbatch correlation and a normalcy suppression block (NSB) which learns to minimize anomaly scores over normal regions of a video by utilizing the overall information available in a training batch. In addition, a clustering loss block (CLB) is proposed to mitigate the label noise and to improve the representation learning for the anomalous and normal regions. This block encourages the backbone network to produce two distinct feature clusters representing normal and anomalous events. An extensive analysis of the proposed approach is provided using three popular anomaly detection datasets including UCF-Crime, ShanghaiTech, and UCSD Ped2. The experiments demonstrate the superior anomaly detection capability of our approach.},
  archive      = {J_TNNLS},
  author       = {Muhammad Zaigham Zaheer and Arif Mahmood and Marcella Astrid and Seung-Ik Lee},
  doi          = {10.1109/TNNLS.2023.3274611},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14085-14098},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Clustering aided weakly supervised training to detect anomalous events in surveillance videos},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MultiplexSAGE: A multiplex embedding algorithm for
inter-layer link prediction. <em>TNNLS</em>, <em>35</em>(10),
14075–14084. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on graph representation learning has received great attention in recent years. However, most of the studies so far have focused on the embedding of single-layer graphs. The few studies dealing with the problem of representation learning of multilayer structures rely on the strong hypothesis that the inter-layer links are known, and this limits the range of possible applications. Here we propose MultiplexSAGE, a generalization of the GraphSAGE algorithm that allows embedding multiplex networks. We show that MultiplexSAGE is capable to reconstruct both the intra-layer and the inter-layer connectivity, outperforming competing methods. Next, through a comprehensive experimental analysis, we shed light also on the performance of the embedding, both in simple and multiplex networks, showing that both the density of the graph and the randomness of the links strongly influences the quality of the embedding.},
  archive      = {J_TNNLS},
  author       = {Luca Gallo and Vito Latora and Alfredo Pulvirenti},
  doi          = {10.1109/TNNLS.2023.3274565},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14075-14084},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MultiplexSAGE: A multiplex embedding algorithm for inter-layer link prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). New adversarial image detection based on sentiment
analysis. <em>TNNLS</em>, <em>35</em>(10), 14060–14074. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are vulnerable to adversarial examples, while adversarial attack models, e.g., DeepFool, are on the rise and outrunning adversarial example detection techniques. This article presents a new adversarial example detector that outperforms state-of-the-art detectors in identifying the latest adversarial attacks on image datasets. Specifically, we propose to use sentiment analysis for adversarial example detection, qualified by the progressively manifesting impact of an adversarial perturbation on the hidden-layer feature maps of a DNN under attack. Accordingly, we design a modularized embedding layer with the minimum learnable parameters to embed the hidden-layer feature maps into word vectors and assemble sentences ready for sentiment analysis. Extensive experiments demonstrate that the new detector consistently surpasses the state-of-the-art detection algorithms in detecting the latest attacks launched against ResNet and Inception neutral networks on the CIFAR-10, CIFAR-100, and SVHN datasets. The detector only has about 2 million parameters and takes less than 4.6 ms to detect an adversarial example generated by the latest attack models using a Tesla K80 GPU card.},
  archive      = {J_TNNLS},
  author       = {Yulong Wang and Tianxiang Li and Shenghong Li and Xin Yuan and Wei Ni},
  doi          = {10.1109/TNNLS.2023.3274538},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14060-14074},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {New adversarial image detection based on sentiment analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PeriodNet: Noise-robust fault diagnosis method under varying
speed conditions. <em>TNNLS</em>, <em>35</em>(10), 14045–14059. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rolling bearings are critical components in modern mechanical systems and have been extensively equipped in various rotating machinery. However, their operating conditions are becoming increasingly complex due to diverse working requirements, dramatically increasing their failure risks. Worse still, the interference of strong background noises and the modulation of varying speed conditions make intelligent fault diagnosis very challenging for conventional methods with limited feature extraction capability. To this end, this study proposes a periodic convolutional neural network (PeriodNet), which is an intelligent end-to-end framework for bearing fault diagnosis. The proposed PeriodNet is constructed by inserting a periodic convolutional module (PeriodConv) before a backbone network. PeriodConv is developed based on the generalized short-time noise resist correlation (GeSTNRC) method, which can effectively capture features from noisy vibration signals collected under varying speed conditions. In PeriodConv, GeSTNRC is extended to the weighted version through deep learning (DL) techniques, whose parameters can be optimized during training. Two open-source datasets collected under constant and varying speed conditions are adopted to assess the proposed method. Case studies demonstrate that PeriodNet has excellent generalizability and is effective under varying speed conditions. Experiments adding noise interference further reveal that PeriodNet is highly robust in noisy environments.},
  archive      = {J_TNNLS},
  author       = {Ruixian Li and Jianguo Wu and Yongxiang Li and Yao Cheng},
  doi          = {10.1109/TNNLS.2023.3274290},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14045-14059},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PeriodNet: Noise-robust fault diagnosis method under varying speed conditions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-step multiview fuzzy clustering with collaborative
learning between common and specific hidden space information.
<em>TNNLS</em>, <em>35</em>(10), 14031–14044. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview data are widespread in real-world applications, and multiview clustering is a commonly used technique to effectively mine the data. Most of the existing algorithms perform multiview clustering by mining the commonly hidden space between views. Although this strategy is effective, there are two challenges that still need to be addressed to further improve the performance. First, how to design an efficient hidden space learning method so that the learned hidden spaces contain both shared and specific information of multiview data. Second, how to design an efficient mechanism to make the learned hidden space more suitable for the clustering task. In this study, a novel one-step multiview fuzzy clustering (OMFC-CS) method is proposed to address the two challenges by collaborative learning between the common and specific space information. To tackle the first challenge, we propose a mechanism to extract the common and specific information simultaneously based on matrix factorization. For the second challenge, we design a one-step learning framework to integrate the learning of common and specific spaces and the learning of fuzzy partitions. The integration is achieved in the framework by performing the two learning processes alternately and thereby yielding mutual benefit. Furthermore, the Shannon entropy strategy is introduced to obtain the optimal views weight assignment during clustering. The experimental results based on benchmark multiview datasets demonstrate that the proposed OMFC-CS outperforms many existing methods.},
  archive      = {J_TNNLS},
  author       = {Wei Zhang and Zhaohong Deng and Te Zhang and Kup-Sze Choi and Shitong Wang},
  doi          = {10.1109/TNNLS.2023.3274289},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14031-14044},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {One-step multiview fuzzy clustering with collaborative learning between common and specific hidden space information},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). QDN: A quadruplet distributor network for temporal
knowledge graph completion. <em>TNNLS</em>, <em>35</em>(10),
14018–14030. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal knowledge graph completion (TKGC) is an extension of the traditional static knowledge graph completion (SKGC) by introducing the timestamp. The existing TKGC methods generally translate the original quadruplet to the form of the triplet by integrating the timestamp into the entity/relation, and then use SKGC methods to infer the missing item. However, such an integrating operation largely limits the expressive ability of temporal information and ignores the semantic loss problem due to the fact that entities, relations, and timestamps are located in different spaces. In this article, we propose a novel TKGC method called the quadruplet distributor network (QDN), which independently models the embeddings of entities, relations, and timestamps in their specific spaces to fully capture the semantics and builds the QD to facilitate the information aggregation and distribution among them. Furthermore, the interaction among entities, relations, and timestamps is integrated using a novel quadruplet-specific decoder, which stretches the third-order tensor to the fourth-order to satisfy the TKGC criterion. Equally important, we design a novel temporal regularization that imposes a smoothness constraint on temporal embeddings. Experimental results show that the proposed method outperforms the existing state-of-the-art TKGC methods. The source codes of this article are available at https://github.com/QDN for Temporal Knowledge Graph Completion.git.},
  archive      = {J_TNNLS},
  author       = {Jiapu Wang and Boyue Wang and Junbin Gao and Xiaoyan Li and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TNNLS.2023.3274230},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14018-14030},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {QDN: A quadruplet distributor network for temporal knowledge graph completion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Closing the gap between theory and practice during
alternating optimization for GANs. <em>TNNLS</em>, <em>35</em>(10),
14005–14017. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing high-quality and diverse samples is the main goal of generative models. Despite recent great progress in generative adversarial networks (GANs), mode collapse is still an open problem, and mitigating it will benefit the generator to better capture the target data distribution. This article rethinks alternating optimization in GANs, which is a classic approach to training GANs in practice. We find that the theory presented in the original GANs does not accommodate this practical solution. Under the alternating optimization manner, the vanilla loss function provides an inappropriate objective for the generator. This objective forces the generator to produce the output with the highest discriminative probability of the discriminator, which leads to mode collapse in GANs. To address this problem, we introduce a novel loss function for the generator to adapt to the alternating optimization nature. When updating the generator by the proposed loss function, the reverse Kullback–Leibler divergence between the model distribution and the target distribution is theoretically optimized, which encourages the model to learn the target distribution. The results of extensive experiments demonstrate that our approach can consistently boost model performance on various datasets and network structures.},
  archive      = {J_TNNLS},
  author       = {Yuanqi Chen and Shangkun Sun and Ge Li and Wei Gao and Thomas H. Li},
  doi          = {10.1109/TNNLS.2023.3274221},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {14005-14017},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Closing the gap between theory and practice during alternating optimization for GANs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual attention adversarial attacks with limited
perturbations. <em>TNNLS</em>, <em>35</em>(10), 13990–14004. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of undetectable adversarial examples with few perturbances remains a difficult problem in adversarial attacks. At present, most solutions use the standard gradient optimization algorithm to build adversarial examples by applying global perturbations to benign samples and then launch attacks on the targets (e.g., face recognition systems). However, when the perturbance size is limited, the performance of these approaches suffers substantially. The content of crucial places in an image, on the other hand, will impact the final prediction; if these areas can be investigated and limited perturbances introduced, an acceptable adversarial example will be constructed. Based on the foregoing research, this article offers a dual attention adversarial network (DAAN) to produce adversarial examples with limited perturbations. DAAN initially searches for effective areas in an input image using the spatial attention network and channel attention network, and then creates space and channel weights. Following that, these weights direct an encoder and a decoder to generate effective perturbation, which is then combined with the input to produce an adversarial example. Finally, the discriminator determines if the created adversarial examples are true or false, and the attacked model is utilized to determine whether the generated samples fit the attack targets. Extensive studies on various datasets show that DAAN not only delivers the best attack performance across all comparison algorithms with few perturbations, but it can also significantly improve the defensiveness of the attacked models.},
  archive      = {J_TNNLS},
  author       = {Mingxing Duan and Yunchuan Qin and Jiayan Deng and Kenli Li and Bin Xiao},
  doi          = {10.1109/TNNLS.2023.3274142},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13990-14004},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual attention adversarial attacks with limited perturbations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). One-click-based perception for interactive image
segmentation. <em>TNNLS</em>, <em>35</em>(10), 13975–13989. (<a
href="https://doi.org/10.1109/TNNLS.2023.3274127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning-based interactive image segmentation methods have significantly reduced the user’s interaction burden with simple click interactions. However, they still require excessive numbers of clicks to continuously correct the segmentation for satisfactory results. This article explores how to harvest accurate segmentation of interested targets while minimizing the user interaction cost. To achieve the above goal, we propose a one-click-based interactive segmentation approach in this work. For this particularly challenging problem in the interactive segmentation task, we build a top-down framework dividing the original problem into a one-click-based coarse localization followed by a fine segmentation. A two-stage interactive object localization network is first designed, which aims to completely enclose the target of interest based on the supervision of object integrity (OI). Click centrality (CC) is also utilized to overcome the overlapping problem between objects. This coarse localization helps to reduce the search space and increase the focus of the click at a higher resolution. A principled multilayer segmentation network is then designed by a progressive layer-by-layer structure, which aims to accurately perceive the target with extremely limited prior guidance. A diffusion module is also designed to enhance the information flow between layers. Besides, the proposed model can be naturally extended to multiobject segmentation task. Our method achieves the state-of-the-art performance under one-click interaction on several benchmarks.},
  archive      = {J_TNNLS},
  author       = {Tao Wang and Haochen Li and Yuhui Zheng and Quansen Sun},
  doi          = {10.1109/TNNLS.2023.3274127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13975-13989},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {One-click-based perception for interactive image segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A switching memory-based event-trigger scheme for
synchronization of lur’e systems with actuator saturation: A hybrid
lyapunov method. <em>TNNLS</em>, <em>35</em>(10), 13963–13974. (<a
href="https://doi.org/10.1109/TNNLS.2023.3273917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the event-triggered synchronization of Lur’e systems subject to actuator saturation. Aiming at reducing control costs, a switching-memory-based event-trigger (SMBET) scheme, which allows a switching between the sleeping interval and the memory-based event-trigger (MBET) interval, is first presented. In consideration of the characteristics of SMBET, a piecewise-defined but continuous looped-functional is newly constructed, under which the requirement of positive definiteness and symmetry on some Lyapunov matrices is dropped within the sleeping interval. Then, a hybrid Lyapunov method (HLM), which bridges the gap between the continuous-time Lyapunov theory (CTLT) and the discrete-time Lyapunov theory (DTLT), is used to make the local stability analysis of the closed-loop system. Meanwhile, using a combination of inequality estimation techniques and the generalized sector condition, two sufficient local synchronization criteria and a codesign algorithm for the controller gain and triggering matrix are developed. Furthermore, two optimization strategies are, respectively, put forward to enlarge the estimated domain of attraction (DoA) and the allowable upper bound of sleeping intervals on the premise of ensuring local synchronization. Finally, a three-neuron neural network and the classical Chua’s circuit are used to carry out some comparison analyses and to display the advantages of the designed SMBET strategy and the constructed HLM, respectively. Also, an application to image encryption is provided to substantiate the feasibility of the obtained local synchronization results.},
  archive      = {J_TNNLS},
  author       = {Yanyan Ni and Zhen Wang and Yingjie Fan and Jianquan Lu and Hao Shen},
  doi          = {10.1109/TNNLS.2023.3273917},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13963-13974},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A switching memory-based event-trigger scheme for synchronization of lur’e systems with actuator saturation: A hybrid lyapunov method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dissimilarity-preserving representation learning for
one-class time series classification. <em>TNNLS</em>, <em>35</em>(10),
13951–13962. (<a
href="https://doi.org/10.1109/TNNLS.2023.3273503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to embed time series in a latent space where pairwise Euclidean distances (EDs) between samples are equal to pairwise dissimilarities in the original space, for a given dissimilarity measure. To this end, we use auto-encoder (AE) and encoder-only neural networks to learn elastic dissimilarity measures, e.g., dynamic time warping (DTW), that are central to time series classification (Bagnall et al., 2017). The learned representations are used in the context of one-class classification (Mauceri et al., 2020) on the datasets of UCR/UEA archive (Dau et al., 2019). Using a 1-nearest neighbor (1NN) classifier, we show that learned representations allow classification performance that is close to that of raw data, but in a space of substantially lower dimensionality. This implies substantial and compelling savings in terms of computational and storage requirements for nearest neighbor time series classification.},
  archive      = {J_TNNLS},
  author       = {Stefano Mauceri and James Sweeney and Miguel Nicolau and James McDermott},
  doi          = {10.1109/TNNLS.2023.3273503},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13951-13962},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dissimilarity-preserving representation learning for one-class time series classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A sequential stein’s method for faster training of additive
index models. <em>TNNLS</em>, <em>35</em>(10), 13941–13950. (<a
href="https://doi.org/10.1109/TNNLS.2023.3273474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The additive index models (AIMs) can be viewed as a kind of artificial neural networks based on nonparametric activation or so-called ridge functions. Recently, they are shown to achieve enhanced explainability after incorporating various interpretability constraints. However, the training of AIMs by either the backfitting algorithm or the joint stochastic optimization is known to be very slow for especially high dimensional inputs. In this article, we propose a novel sequential approach based on the celebrated Stein’s lemma. The proposed SeqStein method can successfully decouple the training of AIMs into two separable steps, namely, the following: 1) Stein’s estimation of the projection indices and 2) nonparametric estimation of ridge functions using the smoothing splines. We show through numerical experiments that the SeqStein algorithm is not only more efficient for training AIMs, but also inclined to produce more interpretable models that have smooth ridge functions with sparse and nearly orthogonal projection indices.},
  archive      = {J_TNNLS},
  author       = {Hengtao Zhang and Zebin Yang and Agus Sudjianto and Aijun Zhang},
  doi          = {10.1109/TNNLS.2023.3273474},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13941-13950},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A sequential stein’s method for faster training of additive index models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class-level logit perturbation. <em>TNNLS</em>,
<em>35</em>(10), 13926–13940. (<a
href="https://doi.org/10.1109/TNNLS.2023.3273355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Features, logits, and labels are the three primary data when a sample passes through a deep neural network (DNN). Feature perturbation and label perturbation receive increasing attention in recent years. They have been proven to be useful in various deep learning approaches. For example, (adversarial) feature perturbation can improve the robustness or even generalization capability of learned models. However, limited studies have explicitly explored for the perturbation of logit vectors. This work discusses several existing methods related to class-level logit perturbation. A unified viewpoint between regular/irregular data augmentation and loss variations incurred by logit perturbation is established. A theoretical analysis is provided to illuminate why class-level logit perturbation is useful. Accordingly, new methodologies are proposed to explicitly learn to perturb logits for both the single-label and multilabel classification tasks. Meta-learning is also leveraged to determine the regular or irregular augmentation for each class. Extensive experiments on benchmark image classification datasets and their long-tail versions indicated the competitive performance of our learning method. As it only perturbs on logit, it can be used as a plug-in to fuse with any existing classification algorithms. All the codes are available at https://github.com/limengyang1992/lpl .},
  archive      = {J_TNNLS},
  author       = {Mengyang Li and Fengguang Su and Ou Wu and Ji Zhang},
  doi          = {10.1109/TNNLS.2023.3273355},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13926-13940},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Class-level logit perturbation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HeGCL: Advance self-supervised learning in heterogeneous
graph-level representation. <em>TNNLS</em>, <em>35</em>(10),
13914–13925. (<a
href="https://doi.org/10.1109/TNNLS.2023.3273255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning in heterogeneous graphs with massive unlabeled data has aroused great interest. The heterogeneity of graphs not only contains rich information, but also raises difficult barriers to designing unsupervised or self-supervised learning (SSL) strategies. Existing methods such as random walk-based approaches are mainly dependent on the proximity information of neighbors and lack the ability to integrate node features into a higher-level representation. Furthermore, previous self-supervised or unsupervised frameworks are usually designed for node-level tasks, which are commonly short of capturing global graph properties and may not perform well in graph-level tasks. Therefore, a label-free framework that can better capture the global properties of heterogeneous graphs is urgently required. In this article, we propose a self-supervised heterogeneous graph neural network (GNN) based on cross-view contrastive learning (HeGCL). The HeGCL presents two views for encoding heterogeneous graphs: the meta-path view and the outline view. Compared with the meta-path view that provides semantic information, the outline view encodes the complex edge relations and captures graph-level properties by using a nonlocal block. Thus, the HeGCL learns node embeddings through maximizing mutual information (MI) between global and semantic representations coming from the outline and meta-path view, respectively. Experiments on both node-level and graph-level tasks show the superiority of the proposed model over other methods, and further exploration studies also show that the introduction of nonlocal block brings a significant contribution to graph-level tasks.},
  archive      = {J_TNNLS},
  author       = {Gen Shi and Yifan Zhu and Jian K. Liu and Xuesong Li},
  doi          = {10.1109/TNNLS.2023.3273255},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13914-13925},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HeGCL: Advance self-supervised learning in heterogeneous graph-level representation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analytical bounds on the local lipschitz constants of ReLU
networks. <em>TNNLS</em>, <em>35</em>(10), 13902–13913. (<a
href="https://doi.org/10.1109/TNNLS.2023.3273228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we determine analytical upper bounds on the local Lipschitz constants of feedforward neural networks with rectified linear unit (ReLU) activation functions. We do so by deriving Lipschitz constants and bounds for ReLU, affine–ReLU, and max pooling functions, and combining the results to determine a network-wide bound. Our method uses several insights to obtain tight bounds, such as keeping track of the zero elements of each layer, and analyzing the composition of affine and ReLU functions. Furthermore, we employ a careful computational approach which allows us to apply our method to large networks such as AlexNet and VGG-16. We present several examples using different networks, which show how our local Lipschitz bounds are tighter than the global Lipschitz bounds. We also show how our method can be applied to provide adversarial bounds for classification networks. These results show that our method produces the largest known bounds on minimum adversarial perturbations for large networks such as AlexNet and VGG-16.},
  archive      = {J_TNNLS},
  author       = {Trevor Avant and Kristi A. Morgansen},
  doi          = {10.1109/TNNLS.2023.3273228},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13902-13913},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Analytical bounds on the local lipschitz constants of ReLU networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intraoperative hypotension prediction based on features
automatically generated within an interpretable deep learning model.
<em>TNNLS</em>, <em>35</em>(10), 13887–13901. (<a
href="https://doi.org/10.1109/TNNLS.2023.3273187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The monitoring of arterial blood pressure (ABP) in anesthetized patients is crucial for preventing hypotension, which can lead to adverse clinical outcomes. Several efforts have been devoted to develop artificial intelligence-based hypotension prediction indices. However, the use of such indices is limited because they may not provide a compelling interpretation of the association between the predictors and hypotension. Herein, an interpretable deep learning model is developed that forecasts hypotension occurrence 10 min before a given 90-s ABP record. Internal and external validations of the model performance show the area under the receiver operating characteristic curves of 0.9145 and 0.9035, respectively. Furthermore, the hypotension prediction mechanism can be physiologically interpreted using the predictors automatically generated from the proposed model for representing ABP trends. Finally, the applicability of a deep learning model with high accuracy is demonstrated, thus providing an interpretation of the association between ABP trends and hypotension in clinical practice.},
  archive      = {J_TNNLS},
  author       = {Eugene Hwang and Yong-Seok Park and Jin-Young Kim and Sung-Hyuk Park and Junetae Kim and Sung-Hoon Kim},
  doi          = {10.1109/TNNLS.2023.3273187},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13887-13901},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intraoperative hypotension prediction based on features automatically generated within an interpretable deep learning model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PS-net: A learning strategy for accurately exposing the
professional photoshop inpainting. <em>TNNLS</em>, <em>35</em>(10),
13874–13886. (<a
href="https://doi.org/10.1109/TNNLS.2023.3272733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring missing areas without leaving visible traces has become a trivial task with Photoshop inpainting tools. However, such tools have potentially illegal or unethical uses, such as removing specific objects in images to deceive the public. Despite the emergence of many forensics methods of image inpainting, their detection ability is still insufficient when attending to professional Photoshop inpainting. Motivated by this, we propose a novel method termed primary–secondary network (PS-Net) to localize the Photoshop inpainted regions in images. To the best of our knowledge, this is the first forensic method devoted specifically to Photoshop inpainting. The PS-Net is designed to deal with the problems of delicate and professional inpainted images. It consists of two subnetworks: the primary network (P-Net) and the secondary network (S-Net). The P-Net aims at mining the frequency clues of subtle inpainting features through the convolutional network and further identifying the tampered region. The S-Net enables the model to mitigate compression and noise attacks to some extent by increasing the co-occurring feature weights and providing features that are not captured by the P-Net. Furthermore, the dense connection, Ghost modules, and channel attention blocks (C-A blocks) are adopted to further strengthen the localization ability of PS-Net. Extensive experimental results illustrate that PS-Net can successfully distinguish forged regions in elaborate inpainted images, outperforming several state-of-the-art solutions. The proposed PS-Net is also robust against some postprocessing operations commonly used in Photoshop.},
  archive      = {J_TNNLS},
  author       = {Yushu Zhang and Zhibin Fu and Shuren Qi and Mingfu Xue and Xiaochun Cao and Yong Xiang},
  doi          = {10.1109/TNNLS.2023.3272733},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13874-13886},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PS-net: A learning strategy for accurately exposing the professional photoshop inpainting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diversifying collaborative filtering via graph spreading
network and selective sampling. <em>TNNLS</em>, <em>35</em>(10),
13860–13873. (<a
href="https://doi.org/10.1109/TNNLS.2023.3272475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network (GNN) is a robust model for processing non-Euclidean data, such as graphs, by extracting structural information and learning high-level representations. GNN has achieved state-of-the-art recommendation performance on collaborative filtering (CF) for accuracy. Nevertheless, the diversity of the recommendations has not received good attention. Existing work using GNN for recommendation suffers from the accuracy-diversity dilemma, where slightly increases diversity while accuracy drops significantly. Furthermore, GNN-based recommendation models lack the flexibility to adapt to different scenarios’ demands concerning the accuracy-diversity ratio of their recommendation lists. In this work, we endeavor to address the above problems from the perspective of aggregate diversity, which modifies the propagation rule and develops a new sampling strategy. We propose graph spreading network (GSN), a novel model that leverages only neighborhood aggregation for CF. Specifically, GSN learns user and item embeddings by propagating them over the graph structure, utilizing both diversity-oriented and accuracy-oriented aggregations. The final representations are obtained by taking the weighted sum of the embeddings learned at all layers. We also present a new sampling strategy that selects potentially accurate and diverse items as negative samples to assist model training. GSN effectively addresses the accuracy-diversity dilemma and achieves improved diversity while maintaining accuracy with the help of a selective sampler. Moreover, a hyper-parameter in GSN allows for adjustment of the accuracy-diversity ratio of recommendation lists to satisfy the diverse demands. Compared to the state-of-the-art model, GSN improved $R$ @20 by 1.62%, $N$ @20 by 0.67%, $G$ @20 by 3.59%, and $E$ @20 by 4.15% on average over three real-world datasets, verifying the effectiveness of our proposed model in diversifying overall collaborative recommendations.},
  archive      = {J_TNNLS},
  author       = {Yueting Fang and Hao Wu and Yiji Zhao and Lei Zhang and Shaowei Qin and Xin Wang},
  doi          = {10.1109/TNNLS.2023.3272475},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13860-13873},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Diversifying collaborative filtering via graph spreading network and selective sampling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial spatiotemporal contrastive learning for
electrocardiogram signals. <em>TNNLS</em>, <em>35</em>(10), 13845–13859.
(<a href="https://doi.org/10.1109/TNNLS.2023.3272153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting invariant representations in unlabeled electrocardiogram (ECG) signals is a challenge for deep neural networks (DNNs). Contrastive learning is a promising method for unsupervised learning. However, it should improve its robustness to noise and learn the spatiotemporal and semantic representations of categories, just like cardiologists. This article proposes a patient-level adversarial spatiotemporal contrastive learning (ASTCL) framework, which includes ECG augmentations, an adversarial module, and a spatiotemporal contrastive module. Based on the ECG noise attributes, two distinct but effective ECG augmentations, ECG noise enhancement, and ECG noise denoising, are introduced. These methods are beneficial for ASTCL to enhance the robustness of the DNN to noise. This article proposes a self-supervised task to increase the antiperturbation ability. This task is represented as a game between the discriminator and encoder in the adversarial module, which pulls the extracted representations into the shared distribution between the positive pairs to discard the perturbation representations and learn the invariant representations. The spatiotemporal contrastive module combines spatiotemporal prediction and patient discrimination to learn the spatiotemporal and semantic representations of categories. To learn category representations effectively, this article only uses patient-level positive pairs and alternately uses the predictor and the stop-gradient to avoid model collapse. To verify the effectiveness of the proposed method, various groups of experiments are conducted on four ECG benchmark datasets and one clinical dataset compared with the state-of-the-art methods. Experimental results showed that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Ning Wang and Panpan Feng and Zhaoyang Ge and Yanjie Zhou and Bing Zhou and Zongmin Wang},
  doi          = {10.1109/TNNLS.2023.3272153},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13845-13859},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial spatiotemporal contrastive learning for electrocardiogram signals},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning relation-enhanced hierarchical solver for math word
problems. <em>TNNLS</em>, <em>35</em>(10), 13830–13844. (<a
href="https://doi.org/10.1109/TNNLS.2023.3272114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically solving math word problems (MWPs) is a challenging task for artificial intelligence (AI) and machine learning (ML) research, which aims to answer the problem with a mathematical expression. Many existing solutions simply model the MWP as a sequence of words, which is far from precise solving. To this end, we turn to how humans solve MWPs. Humans read the problem part-by-part and capture dependencies between words for a thorough understanding and infer the expression precisely in a goal-driven manner with knowledge. Moreover, humans can associate different MWPs to help solve the target with related experience. In this article, we present a focused study on an MWP solver by imitating such procedure. Specifically, we first propose a novel hierarchical math solver (HMS) to exploit semantics in one MWP. First, to imitate human reading habits, we propose a novel encoder to learn the semantics guided by dependencies between words following a hierarchical “word-clause-problem” paradigm. Next, we develop a goal-driven tree-based decoder with knowledge application to generate the expression. One step further, to imitate human associating different MWPs for related experience in problem-solving, we extend HMS to the Relation-enHanced Math Solver (RHMS) to utilize the relation between MWPs. First, to capture the structural similarity relation, we develop a meta-structure tool to measure the similarity based on the logical structure of MWPs and construct a graph to associate related MWPs. Then, based on the graph, we learn an improved solver to exploit related experience for higher accuracy and robustness. Finally, we conduct extensive experiments on two large datasets, which demonstrates the effectiveness of the two proposed methods and the superiority of RHMS.},
  archive      = {J_TNNLS},
  author       = {Xin Lin and Zhenya Huang and Hongke Zhao and Enhong Chen and Qi Liu and Defu Lian and Xin Li and Hao Wang},
  doi          = {10.1109/TNNLS.2023.3272114},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13830-13844},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning relation-enhanced hierarchical solver for math word problems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous optimization of discrete and continuous
parameters defining a robot morphology and controller. <em>TNNLS</em>,
<em>35</em>(10), 13816–13829. (<a
href="https://doi.org/10.1109/TNNLS.2023.3272068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The morphology and controller design of robots is often a labor-intensive task performed by experienced and intuitive engineers. Automatic robot design using machine learning is attracting increasing attention in the hope that it will reduce the design workload and result in better-performing robots. Most robots are created by joining several rigid parts and then mounting actuators and their controllers. Many studies limit the possible types of rigid parts to a finite set to reduce the computational burden. However, this not only limits the search space, but also prohibits the use of powerful optimization techniques. To find a robot closer to the global optimal design, a method that explores a richer set of robots is desirable. In this article, we propose a novel method to efficiently search for various robot designs. The method combines three different optimization methods with different characteristics. We apply proximal policy optimization (PPO) or soft actor-critic (SAC) as the controller, the REINFORCE algorithm to determine the lengths and other numerical parameters of the rigid parts, and a newly proposed method to determine the number and layout of the rigid parts and joints. Experiments with physical simulations confirm that when this method is used to handle two types of tasks—walking and manipulation—it performs better than simple combinations of existing methods. The source code and videos of our experiments are available online ( https://github.com/r-koike/eagent ).},
  archive      = {J_TNNLS},
  author       = {Ryosuke Koike and Ryo Ariizumi and Fumitoshi Matsuno},
  doi          = {10.1109/TNNLS.2023.3272068},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13816-13829},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Simultaneous optimization of discrete and continuous parameters defining a robot morphology and controller},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical and robust federated learning with highly scalable
regression training. <em>TNNLS</em>, <em>35</em>(10), 13801–13815. (<a
href="https://doi.org/10.1109/TNNLS.2023.3271859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving federated learning, as one of the privacy-preserving computation techniques, is a promising distributed and privacy-preserving machine learning (ML) approach for Internet of Medical Things (IoMT), due to its ability to train a regression model without collecting raw data of data owners (DOs). However, traditional interactive federated regression training (IFRT) schemes rely on multiple rounds of communication to train a global model and are still under various privacy and security threats. To overcome these problems, several noninteractive federated regression training (NFRT) schemes have been proposed and applied in a variety of scenarios. However, there are still several challenges: 1) how to protect the privacy of DOs’ local dataset; 2) how to realize highly scalable regression training without linear dependence on sample dimension; 3) how to tolerate DOs’ dropout; and 4) how to enable DOs to verify the correctness of aggregated results returned from the cloud service provider (CSP). In this article, we propose two practical noninteractive federated learning schemes with privacy-preserving for IoMT, named homomorphic encryption based NFRT (HE-NFRT) and double-masking protocol based NFRT (Mask-NFRT), respectively, which are based on a comprehensive consideration of NFRT, privacy concerns, high-efficiency, robustness, and verification mechanism. The security analyses display that our proposed schemes are able to protect the privacy of DOs’ local training data, resist collusion attack, and support strong verification to each DO. The performance evaluation results demonstrate that our proposed HE-NFRT scheme is desirable for a high-dimensional and high-security IoMT application while Mask-NFRT scheme is desirable for a high-dimensional and large-scale IoMT application.},
  archive      = {J_TNNLS},
  author       = {Song Han and Hongxin Ding and Shuai Zhao and Siqi Ren and Zhibo Wang and Jianhong Lin and Shuhao Zhou},
  doi          = {10.1109/TNNLS.2023.3271859},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13801-13815},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Practical and robust federated learning with highly scalable regression training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Simple contrastive graph clustering. <em>TNNLS</em>,
<em>35</em>(10), 13789–13800. (<a
href="https://doi.org/10.1109/TNNLS.2023.3271871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has recently attracted plenty of attention in deep graph clustering due to its promising performance. However, complicated data augmentations and time-consuming graph convolutional operations undermine the efficiency of these methods. To solve this problem, we propose a simple contrastive graph clustering (SCGC) algorithm to improve the existing methods from the perspectives of network architecture, data augmentation, and objective function. As to the architecture, our network includes two main parts, that is, preprocessing and network backbone. A simple low-pass denoising operation conducts neighbor information aggregation as an independent preprocessing, and only two multilayer perceptrons (MLPs) are included as the backbone. For data augmentation, instead of introducing complex operations over graphs, we construct two augmented views of the same vertex by designing parameter unshared Siamese encoders and perturbing the node embeddings directly. Finally, as to the objective function, to further improve the clustering performance, a novel cross-view structural consistency objective function is designed to enhance the discriminative capability of the learned network. Extensive experimental results on seven benchmark datasets validate our proposed algorithm’s effectiveness and superiority. Significantly, our algorithm outperforms the recent contrastive deep clustering competitors with at least seven times speedup on average. The code of SCGC is released at SCGC. Besides, we share a collection of deep graph clustering, including papers, codes, and datasets at ADGC.},
  archive      = {J_TNNLS},
  author       = {Yue Liu and Xihong Yang and Sihang Zhou and Xinwang Liu and Siwei Wang and Ke Liang and Wenxuan Tu and Liang Li},
  doi          = {10.1109/TNNLS.2023.3271871},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13789-13800},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Simple contrastive graph clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Out-of-distribution detection by cross-class vicinity
distribution of in-distribution data. <em>TNNLS</em>, <em>35</em>(10),
13777–13788. (<a
href="https://doi.org/10.1109/TNNLS.2023.3271856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks for image classification only learn to map in-distribution inputs to their corresponding ground-truth labels in training without differentiating out-of-distribution samples from in-distribution ones. This results from the assumption that all samples are independent and identically distributed (IID) without distributional distinction. Therefore, a pretrained network learned from in-distribution samples treats out-of-distribution samples as in-distribution and makes high-confidence predictions on them in the test phase. To address this issue, we draw out-of-distribution samples from the vicinity distribution of training in-distribution samples for learning to reject the prediction on out-of-distribution inputs. A cross-class vicinity distribution is introduced by assuming that an out-of-distribution sample generated by mixing multiple in-distribution samples does not share the same classes of its constituents. We, thus, improve the discriminability of a pretrained network by finetuning it with out-of-distribution samples drawn from the cross-class vicinity distribution, where each out-of-distribution input corresponds to a complementary label. Experiments on various in-/out-of-distribution datasets show that the proposed method significantly outperforms the existing methods in improving the capacity of discriminating between in- and out-of-distribution samples.},
  archive      = {J_TNNLS},
  author       = {Zhilin Zhao and Longbing Cao and Kun-Yu Lin},
  doi          = {10.1109/TNNLS.2023.3271856},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13777-13788},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Out-of-distribution detection by cross-class vicinity distribution of in-distribution data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). AGNN: Alternating graph-regularized neural networks to
alleviate over-smoothing. <em>TNNLS</em>, <em>35</em>(10), 13764–13776.
(<a href="https://doi.org/10.1109/TNNLS.2023.3271623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional network (GCN) with the powerful capacity to explore graph-structural data has gained noticeable success in recent years. Nonetheless, most of the existing GCN-based models suffer from the notorious over-smoothing issue, owing to which shallow networks are extensively adopted. This may be problematic for complex graph datasets because a deeper GCN should be beneficial to propagating information across remote neighbors. Recent works have devoted effort to addressing over-smoothing problems, including establishing residual connection structure or fusing predictions from multilayer models. Because of the indistinguishable embeddings from deep layers, it is reasonable to generate more reliable predictions before conducting the combination of outputs from various layers. In light of this, we propose an alternating graph-regularized neural network (AGNN) composed of graph convolutional layer (GCL) and graph embedding layer (GEL). GEL is derived from the graph-regularized optimization containing Laplacian embedding term, which can alleviate the over-smoothing problem by periodic projection from the low-order feature space onto the high-order space. With more distinguishable features of distinct layers, an improved Adaboost strategy is utilized to aggregate outputs from each layer, which explores integrated embeddings of multi-hop neighbors. The proposed model is evaluated via a large number of experiments including performance comparison with some multilayer or multi-order graph neural networks, which reveals the superior performance improvement of AGNN compared with the state-of-the-art models.},
  archive      = {J_TNNLS},
  author       = {Zhaoliang Chen and Zhihao Wu and Zhenghong Lin and Shiping Wang and Claudia Plant and Wenzhong Guo},
  doi          = {10.1109/TNNLS.2023.3271623},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13764-13776},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AGNN: Alternating graph-regularized neural networks to alleviate over-smoothing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Deeply coupled convolution–transformer with
spatial–temporal complementary learning for video-based person
re-identification. <em>TNNLS</em>, <em>35</em>(10), 13753–13763. (<a
href="https://doi.org/10.1109/TNNLS.2023.3271353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced deep convolutional neural networks (CNNs) have shown great success in video-based person re-identification (Re-ID). However, they usually focus on the most obvious regions of persons with a limited global representation ability. Recently, it witnesses that Transformers explore the interpatch relationships with global observations for performance improvements. In this work, we take both the sides and propose a novel spatial–temporal complementary learning framework named deeply coupled convolution–transformer (DCCT) for high-performance video-based person Re-ID. First, we couple CNNs and Transformers to extract two kinds of visual features and experimentally verify their complementarity. Furthermore, in spatial, we propose a complementary content attention (CCA) to take advantages of the coupled structure and guide independent features for spatial complementary learning. In temporal, a hierarchical temporal aggregation (HTA) is proposed to progressively capture the interframe dependencies and encode temporal information. Besides, a gated attention (GA) is used to deliver aggregated temporal information into the CNN and Transformer branches for temporal complementary learning. Finally, we introduce a self-distillation training strategy to transfer the superior spatial–temporal knowledge to backbone networks for higher accuracy and more efficiency. In this way, two kinds of typical features from same videos are integrated mechanically for more informative representations. Extensive experiments on four public Re-ID benchmarks demonstrate that our framework could attain better performances than most state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Xuehu Liu and Chenyang Yu and Pingping Zhang and Huchuan Lu},
  doi          = {10.1109/TNNLS.2023.3271353},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13753-13763},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deeply coupled Convolution–Transformer with Spatial–Temporal complementary learning for video-based person re-identification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Temporal autoregressive matrix factorization for
high-dimensional time series prediction of OSS. <em>TNNLS</em>,
<em>35</em>(10), 13741–13752. (<a
href="https://doi.org/10.1109/TNNLS.2023.3271327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-source software (OSS) plays an increasingly significant role in modern software development tendency, so accurate prediction of the future development of OSS has become an essential topic. The behavioral data of different open-source software are closely related to their development prospects. However, most of these behavioral data are typical high-dimensional time series data streams with noise and missing values. Hence, accurate prediction on such cluttered data requires the model to be highly scalable, which is not a property of traditional time series prediction models. To this end, we propose a temporal autoregressive matrix factorization (TAMF) framework that supports data-driven temporal learning and prediction. Specifically, we first construct a trend and period autoregressive model to extract trend and period features from OSS behavioral data, and then combine the regression model with a graph-based matrix factorization (MF) to complete the missing values by exploiting the correlations among the time series data. Finally, use the trained regression model to make predictions on the target data. This scheme ensures that TAMF can be applied to different types of high-dimensional time series data and thus has high versatility. We selected ten real developer behavior data from GitHub for case analysis. The experimental results show that TAMF has good scalability and prediction accuracy.},
  archive      = {J_TNNLS},
  author       = {Liang Chen and Yun Yang and Wei Wang},
  doi          = {10.1109/TNNLS.2023.3271327},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13741-13752},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Temporal autoregressive matrix factorization for high-dimensional time series prediction of OSS},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised structure-adaptive graph contrastive learning.
<em>TNNLS</em>, <em>35</em>(10), 13728–13740. (<a
href="https://doi.org/10.1109/TNNLS.2023.3271140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning, which to date has always been guided by node features and fixed-intrinsic structures, has become a prominent technique for unsupervised graph representation learning through contrasting positive–negative counterparts. However, the fixed-intrinsic structure cannot represent the potential relationships beneficial for models, leading to suboptimal results. To this end, we propose a structure-adaptive graph contrastive learning framework to capture potential discriminative relationships. More specifically, a structure learning layer is first proposed for generating the adaptive structure with contrastive loss. Next, a denoising supervision mechanism is designed to perform supervised learning on the structure to promote structure learning, which introduces the pseudostructure through the clustering results and denoises the pseudostructure to provide more reliable supervised information. In this way, under the dual constraints of denoising supervision and contrastive learning, the optimal adaptive structure can be obtained to promote graph representation learning. Extensive experiments on several graph datasets demonstrate that our proposed method outperforms state-of-the-art approaches on various tasks.},
  archive      = {J_TNNLS},
  author       = {Han Zhao and Xu Yang and Cheng Deng and Dacheng Tao},
  doi          = {10.1109/TNNLS.2023.3271140},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13728-13740},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised structure-adaptive graph contrastive learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MHW-GAN: Multidiscriminator hierarchical wavelet generative
adversarial network for multimodal image fusion. <em>TNNLS</em>,
<em>35</em>(10), 13713–13727. (<a
href="https://doi.org/10.1109/TNNLS.2023.3271059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion technology aims to obtain a comprehensive image containing a specific target or detailed information by fusing data of different modalities. However, many deep learning-based algorithms consider edge texture information through loss functions instead of specifically constructing network modules. The influence of the middle layer features is ignored, which leads to the loss of detailed information between layers. In this article, we propose a multidiscriminator hierarchical wavelet generative adversarial network (MHW-GAN) for multimodal image fusion. First, we construct a hierarchical wavelet fusion (HWF) module as the generator of MHW-GAN to fuse feature information at different levels and scales, which avoids information loss in the middle layers of different modalities. Second, we design an edge perception module (EPM) to integrate edge information from different modalities to avoid the loss of edge information. Third, we leverage the adversarial learning relationship between the generator and three discriminators for constraining the generation of fusion images. The generator aims to generate a fusion image to fool the three discriminators, while the three discriminators aim to distinguish the fusion image and edge fusion image from two source images and the joint edge image, respectively. The final fusion image contains both intensity information and structure information via adversarial learning. Experiments on public and self-collected four types of multimodal image datasets show that the proposed algorithm is superior to the previous algorithms in terms of both subjective and objective evaluation.},
  archive      = {J_TNNLS},
  author       = {Cheng Zhao and Peng Yang and Feng Zhou and Guanghui Yue and Shuigen Wang and Huisi Wu and Guoliang Chen and Tianfu Wang and Baiying Lei},
  doi          = {10.1109/TNNLS.2023.3271059},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13713-13727},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MHW-GAN: Multidiscriminator hierarchical wavelet generative adversarial network for multimodal image fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hue guidance network for single image reflection removal.
<em>TNNLS</em>, <em>35</em>(10), 13701–13712. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reflection from glasses is ubiquitous in daily life, but it is usually undesirable in photographs. To remove these unwanted noises, existing methods utilize either correlative auxiliary information or handcrafted priors to constrain this ill-posed problem. However, due to their limited capability to describe the properties of reflections, these methods are unable to handle strong and complex reflection scenes. In this article, we propose a hue guidance network (HGNet) with two branches for single image reflection removal (SIRR) by integrating image information and corresponding hue information. The complementarity between image information and hue information has not been noticed. The key to this idea is that we found that hue information can describe reflections well and thus can be used as a superior constraint for the specific SIRR task. Accordingly, the first branch extracts the salient reflection features by directly estimating the hue map. The second branch leverages these effective features, which can help locate salient reflection regions to obtain a high-quality restored image. Furthermore, we design a new cyclic hue loss to provide a more accurate optimization direction for the network training. Experiments substantiate the superiority of our network, especially its excellent generalization ability to various reflection scenes, as compared with state-of-the-arts both qualitatively and quantitatively. Source codes are available at https://github.com/zhuyr97/HGRR},
  archive      = {J_TNNLS},
  author       = {Yurui Zhu and Xueyang Fu and Zheyu Zhang and Aiping Liu and Zhiwei Xiong and Zheng-Jun Zha},
  doi          = {10.1109/TNNLS.2023.3270938},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13701-13712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hue guidance network for single image reflection removal},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based slip detection for dexterous manipulation
using GelStereo sensing. <em>TNNLS</em>, <em>35</em>(10), 13691–13700.
(<a href="https://doi.org/10.1109/TNNLS.2023.3270579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Endowing the robot with tactile perception can effectively improve manipulation dexterity, along with various benefits of human-like touch. Using GelStereo (GS) tactile sensing, which gives high-resolution contact geometry information, including 2-D displacement field, and 3-D point cloud of the contact surface, we present a learning-based slip detection system in this study. The results reveal that the well-trained network achieves 95.79% accuracy on the never-seen testing dataset, which surpasses the current model-based and learning-based methods using visuotactile sensing. We also propose a general framework for slip feedback adaptive control for dexterous robot manipulation tasks. The experimental results show the effectiveness and efficiency of the proposed control framework using GS tactile feedback when deployed on real-world grasping and screwing manipulation tasks on various robot setups.},
  archive      = {J_TNNLS},
  author       = {Shaowei Cui and Shuo Wang and Rui Wang and Shaolin Zhang and Chaofan Zhang},
  doi          = {10.1109/TNNLS.2023.3270579},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13691-13700},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning-based slip detection for dexterous manipulation using GelStereo sensing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic-varying parameter enhanced ZNN model for solving
time-varying complex-valued tensor inversion with its application to
image encryption. <em>TNNLS</em>, <em>35</em>(10), 13681–13690. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying complex-valued tensor inverse (TVCTI) is a public problem worthy of being studied, while numerical solutions for the TVCTI are not effective enough. This work aims to find the accurate solution to the TVCTI using zeroing neural network (ZNN), which is an effective tool in terms of solving time-varying problems and is improved in this article to solve the TVCTI problem for the first time. Based on the design idea of ZNN, an error-adaptive dynamic parameter and a new enhanced segmented signum exponential activation function (ESS-EAF) are first designed and applied to the ZNN. Then a dynamic-varying parameter-enhanced ZNN (DVPEZNN) model is proposed to solve the TVCTI problem. The convergence and robustness of the DVPEZNN model are theoretically analyzed and discussed. In order to highlight better convergence and robustness of the DVPEZNN model, it is compared with four varying-parameter ZNN models in the illustrative example. The results show that the DVPEZNN model has better convergence and robustness than the other four ZNN models in different situations. In addition, the state solution sequence generated by the DVPEZNN model in the process of solving the TVCTI cooperates with the chaotic system and deoxyribonucleic acid (DNA) coding rules to obtain the chaotic-ZNN-DNA (CZD) image encryption algorithm, which can encrypt and decrypt images with good performance.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Xiaopeng Li and Pengling Cao and Yongjun He and Wensheng Tang and Jichun Li and Yaonan Wang},
  doi          = {10.1109/TNNLS.2023.3270563},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13681-13690},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A dynamic-varying parameter enhanced ZNN model for solving time-varying complex-valued tensor inversion with its application to image encryption},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visualizing and understanding patch interactions in vision
transformer. <em>TNNLS</em>, <em>35</em>(10), 13671–13680. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformer (ViT) has become a leading tool in various computer vision tasks, owing to its unique self-attention mechanism that learns visual representations explicitly through cross-patch information interactions. Despite having good success, the literature seldom explores the explainability of ViT, and there is no clear picture of how the attention mechanism with respect to the correlation across comprehensive patches will impact the performance and what is the further potential. In this work, we propose a novel explainable visualization approach to analyze and interpret the crucial attention interactions among patches for ViT. Specifically, we first introduce a quantification indicator to measure the impact of patch interaction and verify such quantification on attention window design and indiscriminative patches removal. Then, we exploit the effective responsive field of each patch in ViT and devise a window-free transformer (WinfT) architecture accordingly. Extensive experiments on ImageNet demonstrate that the exquisitely designed quantitative method is shown able to facilitate ViT model learning, leading the top-1 accuracy by 4.28% at most. More remarkably, the results on downstream fine-grained recognition tasks further validate the generalization of our proposal.},
  archive      = {J_TNNLS},
  author       = {Jie Ma and Yalong Bai and Bineng Zhong and Wei Zhang and Ting Yao and Tao Mei},
  doi          = {10.1109/TNNLS.2023.3270479},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13671-13680},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Visualizing and understanding patch interactions in vision transformer},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep convolutional tables: Deep learning without
convolutions. <em>TNNLS</em>, <em>35</em>(10), 13658–13670. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel formulation of deep networks that do not use dot-product neurons and rely on a hierarchy of voting tables instead, denoted as convolutional tables (CTs), to enable accelerated CPU-based inference. Convolutional layers are the most time-consuming bottleneck in contemporary deep learning techniques, severely limiting their use in the Internet of Things and CPU-based devices. The proposed CT performs a fern operation at each image location: it encodes the location environment into a binary index and uses the index to retrieve the desired local output from a table. The results of multiple tables are combined to derive the final output. The computational complexity of a CT transformation is independent of the patch (filter) size and grows gracefully with the number of channels, outperforming comparable convolutional layers. It is shown to have a better capacity:compute ratio than dot-product neurons, and that deep CT networks exhibit a universal approximation property similar to neural networks. As the transformation involves computing discrete indices, we derive a soft relaxation and gradient-based approach for training the CT hierarchy. Deep CT networks have been experimentally shown to have accuracy comparable to that of CNNs of similar architectures. In the low-compute regime, they enable an error:speed tradeoff superior to alternative efficient CNN architectures.},
  archive      = {J_TNNLS},
  author       = {Shay Dekel and Yosi Keller and Aharon Bar-Hillel},
  doi          = {10.1109/TNNLS.2023.3270402},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13658-13670},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep convolutional tables: Deep learning without convolutions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design, analysis, and application of a discrete error
redefinition neural network for time-varying quadratic programming.
<em>TNNLS</em>, <em>35</em>(10), 13646–13657. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying quadratic programming (TV-QP) is widely used in artificial intelligence, robotics, and many other fields. To solve this important problem, a novel discrete error redefinition neural network (D-ERNN) is proposed. By redefining the error monitoring function and discretization, the proposed neural network is superior to some traditional neural networks in terms of convergence speed, robustness, and overshoot. Compared with the continuous ERNN, the proposed discrete neural network is more suitable for computer implementation. Unlike continuous neural networks, this article also analyzes and proves how to select the parameters and step size of the proposed neural networks to ensure the reliability of the network. Moreover, how to achieve the discretization of the ERNN is presented and discussed. The convergence of the proposed neural network without disturbance is proven, and bounded time-varying disturbances can be resisted in theory. Furthermore, the comparison results with other related neural networks show that the proposed D-ERNN has a faster convergence speed, better antidisturbance ability, and lower overshoot.},
  archive      = {J_TNNLS},
  author       = {Lunan Zheng and Weiqi Yu and Zongqing Xu and Zhijun Zhang and Feiqi Deng},
  doi          = {10.1109/TNNLS.2023.3270381},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13646-13657},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design, analysis, and application of a discrete error redefinition neural network for time-varying quadratic programming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HKNAS: Classification of hyperspectral imagery based on
hyper kernel neural architecture search. <em>TNNLS</em>,
<em>35</em>(10), 13631–13645. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent neural architecture search (NAS)-based approaches have made great progress in the hyperspectral image (HSI) classification tasks. However, the architectures are usually optimized independently of the network weights, increasing searching time, and restricting model performances. To tackle these issues, in this article, different from previous methods that extra define structural parameters, we propose to directly generate structural parameters by utilizing the specifically designed hyper kernels, ingeniously converting the original complex dual optimization problem into easily implemented one-tier optimizations, and greatly shrinking searching costs. Then, we develop a hierarchical multimodule search space whose candidate operations only contain convolutions, and these operations can be integrated into unified kernels. Using the above searching strategy and searching space, we obtain three kinds of networks to separately conduct pixel-level or image-level classifications with 1-D or 3-D convolutions. In addition, by combining the proposed hyper kernel searching scheme with the 3-D convolution decomposition mechanism, we obtain diverse architectures to simulate 3-D convolutions, greatly improving network flexibilities. A series of quantitative and qualitative experiments on six public datasets demonstrate that the proposed methods achieve state-of-the-art results compared with other advanced NAS-based HSI classification approaches.},
  archive      = {J_TNNLS},
  author       = {Di Wang and Bo Du and Liangpei Zhang and Dacheng Tao},
  doi          = {10.1109/TNNLS.2023.3270369},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13631-13645},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HKNAS: Classification of hyperspectral imagery based on hyper kernel neural architecture search},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronization of delayed memristor-based neural networks
via pinning control with local information. <em>TNNLS</em>,
<em>35</em>(10), 13619–13630. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel pinning control method, only requiring information from partial nodes, is developed to synchronize drive-response memristor-based neural networks (MNNs) with time delay. An improved mathematical model of MNNs is established to describe the dynamic behaviors of MNNs accurately. In the existing literature, pinning controllers for synchronization of drive-response systems were designed based on information of all nodes, but in some specific situations, the control gains may be very large and challenging to realize in practice. To overcome this problem, a novel pinning control policy is developed to achieve synchronization of delayed MNNs, which depends only on local information of MNNs, for reducing communication and calculation burdens. Furthermore, sufficient conditions for synchronization of delayed MNNs are provided. Finally, numerical simulation and comparative experiments are conducted to verify the effectiveness and superiority of the proposed pinning control method.},
  archive      = {J_TNNLS},
  author       = {Zhanyu Yang and Bo Zhao and Derong Liu},
  doi          = {10.1109/TNNLS.2023.3270345},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13619-13630},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of delayed memristor-based neural networks via pinning control with local information},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-reinforcement learning in nonstationary and
nonparametric environments. <em>TNNLS</em>, <em>35</em>(10),
13604–13618. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent state-of-the-art artificial agents lack the ability to adapt rapidly to new tasks, as they are trained exclusively for specific objectives and require massive amounts of interaction to learn new skills. Meta-reinforcement learning (meta-RL) addresses this challenge by leveraging knowledge learned from training tasks to perform well in previously unseen tasks. However, current meta-RL approaches limit themselves to narrow parametric and stationary task distributions, ignoring qualitative differences and nonstationary changes between tasks that occur in the real world. In this article, we introduce a Task-Inference-based meta-RL algorithm using explicitly parameterized Gaussian variational autoencoders (VAEs) and gated Recurrent units (TIGR), designed for nonparametric and nonstationary environments. We employ a generative model involving a VAE to capture the multimodality of the tasks. We decouple the policy training from the task-inference learning and efficiently train the inference mechanism on the basis of an unsupervised reconstruction objective. We establish a zero-shot adaptation procedure to enable the agent to adapt to nonstationary task changes. We provide a benchmark with qualitatively distinct tasks based on the half-cheetah environment and demonstrate the superior performance of TIGR compared with state-of-the-art meta-RL approaches in terms of sample efficiency (three to ten times faster), asymptotic performance, and applicability in nonparametric and nonstationary environments with zero-shot adaptation. Videos can be viewed at https://videoviewsite.wixsite.com/tigr .},
  archive      = {J_TNNLS},
  author       = {Zhenshan Bing and Lukas Knak and Long Cheng and Fabrice O. Morin and Kai Huang and Alois Knoll},
  doi          = {10.1109/TNNLS.2023.3270298},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13604-13618},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Meta-reinforcement learning in nonstationary and nonparametric environments},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-structure-based multigranular belief fusion for human
activity recognition. <em>TNNLS</em>, <em>35</em>(10), 13589–13603. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The belief functions (BFs) introduced by Shafer in the mid of 1970s are widely applied in information fusion to model epistemic uncertainty and to reason about uncertainty. Their success in applications is however limited because of their high-computational complexity in the fusion process, especially when the number of focal elements is large. To reduce the complexity of reasoning with BFs, we can envisage as a first method to reduce the number of focal elements involved in the fusion process to convert the original basic belief assignments (BBAs) into simpler ones, or as a second method to use a simple rule of combination with potentially a loss of the specificity and pertinence of the fusion result, or to apply both methods jointly. In this article, we focus on the first method and propose a new BBA granulation method inspired by the community clustering of nodes in graph networks. This article studies a novel efficient multigranular belief fusion (MGBF) method. Specifically, focal elements are regarded as nodes in the graph structure, and the distance between nodes will be used to discover the local community relationship of focal elements. Afterward, the nodes belonging to the decision-making community are specially selected, and then the derived multigranular sources of evidence can be efficiently combined. To evaluate the effectiveness of the proposed graph-based MGBF, we further apply this new approach to combine the outputs of convolutional neural networks + attention (CNN + Attention) in the human activity recognition (HAR) problem. The experimental results obtained with real datasets prove the potential interest and feasibility of our proposed strategy with respect to classical BF fusion methods.},
  archive      = {J_TNNLS},
  author       = {Yilin Dong and Xinde Li and Jean Dezert and Rigui Zhou and Kezhu Zuo and Shuzhi Sam Ge},
  doi          = {10.1109/TNNLS.2023.3270290},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13589-13603},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-structure-based multigranular belief fusion for human activity recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolving memristive reservoir. <em>TNNLS</em>,
<em>35</em>(10), 13574–13588. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In light of the dynamic plasticity, nanosize, and energy efficiency of memristors, memristive reservoirs have attracted increasing attention in diverse fields of research recently. However, limited by deterministic hardware implementation, hardware reservoir adaptation is hard to realize. Existing evolutionary algorithms for evolving reservoirs are not designed for hardware implementation. They often ignore the circuit scalability and feasibility of the memristive reservoirs. In this work, based on the reconfigurable memristive units (RMUs), we first propose an evolvable memristive reservoir circuit that is capable of adaptive evolution for varying tasks, where the configuration signals of memristor are evolved directly avoiding the device variance of the memristors. Second, considering the feasibility and scalability of memristive circuits, we propose a scalable algorithm for evolving the proposed reconfigurable memristive reservoir circuit, where the reservoir circuit will not only be valid according to the circuit laws but also has the sparse topology, alleviating the scalability issue and ensuring the circuit feasibility during the evolution. Finally, we apply our proposed scalable algorithm to evolve the reconfigurable memristive reservoir circuits for a wave generation task, six prediction tasks, and one classification task. Through experiments, the feasibility and superiority of our proposed evolvable memristive reservoir circuit are demonstrated.},
  archive      = {J_TNNLS},
  author       = {Xinming Shi and Leandro L. Minku and Xin Yao},
  doi          = {10.1109/TNNLS.2023.3270224},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13574-13588},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Evolving memristive reservoir},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Informative data selection with uncertainty for multimodal
object detection. <em>TNNLS</em>, <em>35</em>(10), 13561–13573. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise has always been nonnegligible trouble in object detection by creating confusion in model reasoning, thereby reducing the informativeness of the data. It can lead to inaccurate recognition due to the shift in the observed pattern, that requires a robust generalization of the models. To implement a general vision model, we need to develop deep learning models that can adaptively select valid information from multimodal data. This is mainly based on two reasons. Multimodal learning can break through the inherent defects of single-modal data, and adaptive information selection can reduce chaos in multimodal data. To tackle this problem, we propose a universal uncertainty-aware multimodal fusion model. It adopts a multipipeline loosely coupled architecture to combine the features and results from point clouds and images. To quantify the correlation in multimodal information, we model the uncertainty, as the inverse of data information, in different modalities and embed it in the bounding box generation. In this way, our model reduces the randomness in fusion and generates reliable output. Moreover, we conducted a completed investigation on the KITTI 2-D object detection dataset and its derived dirty data. Our fusion model is proven to resist severe noise interference like Gaussian, motion blur, and frost, with only slight degradation. The experiment results demonstrate the benefits of our adaptive fusion. Our analysis on the robustness of multimodal fusion will provide further insights for future research.},
  archive      = {J_TNNLS},
  author       = {Xinyu Zhang and Zhiwei Li and Zhenhong Zou and Xin Gao and Yijin Xiong and Dafeng Jin and Jun Li and Huaping Liu},
  doi          = {10.1109/TNNLS.2023.3270159},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13561-13573},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Informative data selection with uncertainty for multimodal object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FBANet: An effective data mining method for food olfactory
EEG recognition. <em>TNNLS</em>, <em>35</em>(10), 13550–13560. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the sensory evaluation of food mostly depends on artificial sensory evaluation and machine perception, but artificial sensory evaluation is greatly interfered with by subjective factors, and machine perception is difficult to reflect human feelings. In this article, a frequency band attention network (FBANet) for olfactory electroencephalogram (EEG) was proposed to distinguish the difference in food odor. First, the olfactory EEG evoked experiment was designed to collect the olfactory EEG, and the preprocessing of olfactory EEG, such as frequency division, was completed. Second, the FBANet consisted of frequency band feature mining and frequency band feature self-attention, in which frequency band feature mining can effectively mine multiband features of olfactory EEG with different scales, and frequency band feature self-attention can integrate the extracted multiband features and realize classification. Finally, compared with other advanced models, the performance of the FBANet was evaluated. The results show that FBANet was better than the state-of-the-art techniques. In conclusion, FBANet effectively mined the olfactory EEG data information and distinguished the differences between the eight food odors, which proposed a new idea for food sensory evaluation based on multiband olfactory EEG analysis.},
  archive      = {J_TNNLS},
  author       = {Xiuxin Xia and Yan Shi and Pengwei Li and Xiaosong Liu and Jingjing Liu and Hong Men},
  doi          = {10.1109/TNNLS.2023.3269949},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13550-13560},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FBANet: An effective data mining method for food olfactory EEG recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Divide and retain: A dual-phase modeling for long-tailed
visual recognition. <em>TNNLS</em>, <em>35</em>(10), 13538–13549. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work explores visual recognition models on real-world datasets exhibiting a long-tailed distribution. Most of previous works are based on a holistic perspective that the overall gradient for training model is directly obtained by considering all classes jointly. However, due to the extreme data imbalance in long-tailed datasets, joint consideration of different classes tends to induce the gradient distortion problem; i.e., the overall gradient tends to suffer from shifted direction toward data-rich classes and enlarged variances caused by data-poor classes. The gradient distortion problem impairs the training of our models. To avoid such drawbacks, we propose to disentangle the overall gradient and aim to consider the gradient on data-rich classes and that on data-poor classes separately. We tackle the long-tailed visual recognition problem via a dual-phase-based method. In the first phase, only data-rich classes are concerned to update model parameters, where only separated gradient on data-rich classes is used. In the second phase, the rest data-poor classes are involved to learn a complete classifier for all classes. More importantly, to ensure the smooth transition from phase I to phase II, we propose an exemplar bank and a memory-retentive loss. In general, the exemplar bank reserves a few representative examples from data-rich classes. It is used to maintain the information of data-rich classes when transiting. The memory-retentive loss constrains the change of model parameters from phase I to phase II based on the exemplar bank and data-poor classes. The extensive experimental results on four commonly used long-tailed benchmarks, including CIFAR100-LT, Places-LT, ImageNet-LT, and iNaturalist 2018, highlight the excellent performance of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Hu Zhang and Linchao Zhu and Xiaohan Wang and Yi Yang},
  doi          = {10.1109/TNNLS.2023.3269907},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13538-13549},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Divide and retain: A dual-phase modeling for long-tailed visual recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Split-level evolutionary neural architecture search with
elite weight inheritance. <em>TNNLS</em>, <em>35</em>(10), 13523–13537.
(<a href="https://doi.org/10.1109/TNNLS.2023.3269816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has recently gained extensive interest in the deep learning community because of its great potential in automating the construction process of deep models. Among a variety of NAS approaches, evolutionary computation (EC) plays a pivotal role with its merit of gradient-free search ability. However, a massive number of the current EC-based NAS approaches evolve neural architectures in an absolutely discrete manner, which makes it tough to flexibly handle the number of filters for each layer, since they often reduce it to a limit set rather than searching for all possible values. Moreover, EC-based NAS methods are often criticized for their inefficiency in performance evaluation, which usually requires laborious full training for hundreds of candidate architectures generated. To address the inflexible search issue on the number of filters, this work proposes a split-level particle swarm optimization (PSO) approach. Each dimension of the particle is subdivided into an integer part and a fractional part, encoding the configurations of the corresponding layer, and the number of filters within a large range, respectively. In addition, the evaluation time is greatly saved by a novel elite weight inheritance method based on an online updating weight pool, and a customized fitness function considering multiple objectives is developed to well control the complexity of the searched candidate architectures. The proposed method, termed split-level evolutionary NAS (SLE-NAS), is computationally efficient, and outperforms many state-of-the-art peer competitors at much lower complexity across three popular image classification benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Junhao Huang and Bing Xue and Yanan Sun and Mengjie Zhang and Gary G. Yen},
  doi          = {10.1109/TNNLS.2023.3269816},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13523-13537},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Split-level evolutionary neural architecture search with elite weight inheritance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal clustering with deep correlated information
bottleneck method. <em>TNNLS</em>, <em>35</em>(10), 13508–13522. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal clustering (CMC) intends to improve the clustering accuracy (ACC) by exploiting the correlations across modalities. Although recent research has made impressive advances, it remains a challenge to sufficiently capture the correlations across modalities due to the high-dimensional nonlinear characteristics of individual modalities and the conflicts in heterogeneous modalities. In addition, the meaningless modality-private information in each modality might become dominant in the process of correlation mining, which also interferes with the clustering performance. To tackle these challenges, we devise a novel deep correlated information bottleneck (DCIB) method, which aims at exploring the correlation information between multiple modalities while eliminating the modality-private information in each modality in an end-to-end manner. Specifically, DCIB treats the CMC task as a two-stage data compression procedure, in which the modality-private information in each modality is eliminated under the guidance of the shared representation of multiple modalities. Meanwhile, the correlations between multiple modalities are preserved from the aspects of feature distributions and clustering assignments simultaneously. Finally, the objective of DCIB is formulated as an objective function based on a mutual information measurement, in which a variational optimization approach is proposed to ensure its convergence. Experimental results on four cross-modal datasets validate the superiority of the DCIB. Code is released at https://github.com/Xiaoqiang-Yan/DCIB .},
  archive      = {J_TNNLS},
  author       = {Xiaoqiang Yan and Yiqiao Mao and Yangdong Ye and Hui Yu},
  doi          = {10.1109/TNNLS.2023.3269789},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13508-13522},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cross-modal clustering with deep correlated information bottleneck method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network robustness prediction: Influence of training data
distributions. <em>TNNLS</em>, <em>35</em>(10), 13496–13507. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network robustness refers to the ability of a network to continue its functioning against malicious attacks, which is critical for various natural and industrial networks. Network robustness can be quantitatively measured by a sequence of values that record the remaining functionality after a sequential node- or edge-removal attacks. Robustness evaluations are traditionally determined by attack simulations, which are computationally very time-consuming and sometimes practically infeasible. The convolutional neural network (CNN)-based prediction provides a cost-efficient approach to fast evaluating the network robustness. In this article, the prediction performances of the learning feature representation-based CNN (LFR-CNN) and PATCHY-SAN methods are compared through extensively empirical experiments. Specifically, three distributions of network size in the training data are investigated, including the uniform, Gaussian, and extra distributions. The relationship between the CNN input size and the dimension of the evaluated network is studied. Extensive experimental results reveal that compared to the training data of uniform distribution, the Gaussian and extra distributions can significantly improve both the prediction performance and the generalizability, for both LFR-CNN and PATCHY-SAN, and for various functionality robustness. The extension ability of LFR-CNN is significantly better than PATCHY-SAN, verified by extensive comparisons on predicting the robustness of unseen networks. In general, LFR-CNN outperforms PATCHY-SAN, and thus LFR-CNN is recommended over PATCHY-SAN. However, since both LFR-CNN and PATCHY-SAN have advantages for different scenarios, the optimal settings of the input size of CNN are recommended under different configurations.},
  archive      = {J_TNNLS},
  author       = {Yang Lou and Chengpei Wu and Junli Li and Lin Wang and Guanrong Chen},
  doi          = {10.1109/TNNLS.2023.3269753},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13496-13507},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Network robustness prediction: Influence of training data distributions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Coupling global context and local contents for
weakly-supervised semantic segmentation. <em>TNNLS</em>,
<em>35</em>(10), 13483–13495. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the advantages of the friendly annotations and the satisfactory performance, weakly-supervised semantic segmentation (WSSS) approaches have been extensively studied. Recently, the single-stage WSSS (SS-WSSS) was awakened to alleviate problems of the expensive computational costs and the complicated training procedures in multistage WSSS. However, the results of such an immature model suffer from problems of background incompleteness and object incompleteness. We empirically find that they are caused by the insufficiency of the global object context and the lack of local regional contents, respectively. Under these observations, we propose an SS-WSSS model with only the image-level class label supervisions, termed weakly supervised feature coupling network (WS-FCN), which can capture the multiscale context formed from the adjacent feature grids, and encode the fine-grained spatial information from the low-level features into the high-level ones. Specifically, a flexible context aggregation (FCA) module is proposed to capture the global object context in different granular spaces. Besides, a semantically consistent feature fusion (SF2) module is proposed in a bottom-up parameter-learnable fashion to aggregate the fine-grained local contents. Based on these two modules, WS-FCN lies in a self-supervised end-to-end training fashion. Extensive experimental results on the challenging PASCAL VOC 2012 and MS COCO 2014 demonstrate the effectiveness and efficiency of WS-FCN, which can achieve state-of-the-art results by 65.02% and 64.22% mIoU on PASCAL VOC 2012 val set and test set, 34.12% mIoU on MS COCO 2014 val set, respectively. The code and weight have been released at:WS-FCN.},
  archive      = {J_TNNLS},
  author       = {Chunyan Wang and Dong Zhang and Liyan Zhang and Jinhui Tang},
  doi          = {10.1109/TNNLS.2023.3269513},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13483-13495},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coupling global context and local contents for weakly-supervised semantic segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Cross-channel specific-mutual feature transfer learning for
motor imagery EEG signals decoding. <em>TNNLS</em>, <em>35</em>(10),
13472–13482. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the rapid development of deep learning, various deep learning frameworks have been widely used in brain-computer interface (BCI) research for decoding motor imagery (MI) electroencephalogram (EEG) signals to understand brain activity accurately. The electrodes, however, record the mixed activities of neurons. If different features are directly embedded in the same feature space, the specific and mutual features of different neuron regions are not considered, which will reduce the expression ability of the feature itself. We propose a cross-channel specific-mutual feature transfer learning (CCSM-FT) network model to solve this problem. The multibranch network extracts the specific and mutual features of brain’s multiregion signals. Effective training tricks are used to maximize the distinction between the two kinds of features. Suitable training tricks can also improve the effectiveness of the algorithm compared with novel models. Finally, we transfer two kinds of features to explore the potential of mutual and specific features to enhance the expressive power of the feature and use the auxiliary set to improve identification performance. The experimental results show that the network has a better classification effect in the BCI Competition IV-2a and the HGD datasets.},
  archive      = {J_TNNLS},
  author       = {Donglin Li and Jianhui Wang and Jiacan Xu and Xiaoke Fang and Ying Ji},
  doi          = {10.1109/TNNLS.2023.3269512},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13472-13482},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cross-channel specific-mutual feature transfer learning for motor imagery EEG signals decoding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive neurodynamic approach to multiple constrained
distributed resource allocation. <em>TNNLS</em>, <em>35</em>(10),
13461–13471. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive neurodynamic approach over multiagent systems is designed to solve nonsmooth distributed resource allocation problems (DRAPs) with affine-coupled equality constraints, coupled inequality constraints, and private set constraints. It is to say, agents focus on tracking the optimal allocation to minimize the team cost under more general constraints. Among the considered constraints, multiple coupled constraints are dealt with by introducing auxiliary variables to make Lagrange multipliers reach consensus. Furthermore, aiming to address private set constraints, an adaptive controller is proposed with the aid of the penalty method, thus avoiding the disclosure of global information. Through using the Lyapunov stability theory, the convergence of this neurodynamic approach is analyzed. In addition, to reduce the communication burden of systems, the proposed neurodynamic approach is improved by introducing an event-triggered mechanism. In this case, the convergence property is also explored, and the Zeno phenomenon is excluded. Finally, a numerical example and a simplified problem on a virtual 5G system are implemented to demonstrate the effectiveness of the proposed neurodynamic approaches.},
  archive      = {J_TNNLS},
  author       = {Linhua Luan and Sitian Qin},
  doi          = {10.1109/TNNLS.2023.3269426},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13461-13471},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neurodynamic approach to multiple constrained distributed resource allocation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discrete-time reinforcement learning adaptive control for
non-gaussian distribution of sampling intervals. <em>TNNLS</em>,
<em>35</em>(10), 13453–13460. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an optimal controller based on reinforcement learning (RL) for a class of unknown discrete-time systems with non-Gaussian distribution of sampling intervals. The critic and actor networks are implemented using the MiFRENc and MiFRENa architectures, respectively. The learning algorithm is developed with learning rates determined through convergence analysis of internal signals and tracking errors. Experimental systems with a comparative controller are conducted to validate the proposed scheme, and comparative results show superior performance for non-Gaussian distributions, with weight transfer for the critic network omitted. Additionally, the proposed learning laws, using the estimated co-state, significantly improve dead-zone compensation and nonlinear variation.},
  archive      = {J_TNNLS},
  author       = {C. Treesatayapun},
  doi          = {10.1109/TNNLS.2023.3269441},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13453-13460},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discrete-time reinforcement learning adaptive control for non-gaussian distribution of sampling intervals},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). HVCMM: A hybrid-view abstractive model of automatic summary
generation for teaching. <em>TNNLS</em>, <em>35</em>(10), 13441–13452.
(<a href="https://doi.org/10.1109/TNNLS.2023.3269013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of educational informatization, more and more emerging technologies are applied in teaching activities. These technologies provide massive and multidimensional information for teaching research, but at the same time, the information obtained by teachers and students presents an explosive increase. Extracting the core content of the class record text through text summarization technology to generate concise class minutes can significantly improve the efficiency of teachers and students to obtain information. This article proposes a hybrid-view class minutes automatic generation model (HVCMM). The HVCMM model uses a multilevel encoding strategy to encode the long text of the input class records to avoid memory overflow in the calculation after the long text is input into the single-level encoder. The HVCMM model uses the method of coreference resolution and adds role vectors to solve the problem that the excessive number of participants in the class may lead to confusion about the referential logic. Machine learning algorithms are used to analyze the topic and Section of the sentence to capture structural information. We test the HVCMM model on the Chinese class minutes dataset (CCM) and the augmented multiparty interaction (AMI) dataset, and the results show that the HVCMM model outperforms other baseline models on the ROUGE metric. With the help of the HVCMM model, teachers can improve the efficiency of reflection after class and improve the teaching level. Students can review the key content to strengthen their understanding of what they have learned with the help of the class minutes automatically generated by the model.},
  archive      = {J_TNNLS},
  author       = {Hao Zhang and Sanya Liu and Yaopeng Li and Wang Ren and Hang Tong and Jiaheng Cao and Qiankun Zhou},
  doi          = {10.1109/TNNLS.2023.3269013},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13441-13452},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HVCMM: A hybrid-view abstractive model of automatic summary generation for teaching},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). FedTP: Federated learning by transformer personalization.
<em>TNNLS</em>, <em>35</em>(10), 13426–13440. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is an emerging learning paradigm where multiple clients collaboratively train a machine learning model in a privacy-preserving manner. Personalized federated learning extends this paradigm to overcome heterogeneity across clients by learning personalized models. Recently, there have been some initial attempts to apply transformers to federated learning. However, the impacts of federated learning algorithms on self-attention have not yet been studied. In this article, we investigate this relationship and reveal that federated averaging (FedAvg) algorithms actually have a negative impact on self-attention in cases of data heterogeneity, which limits the capabilities of the transformer model in federated learning settings. To address this issue, we propose FedTP, a novel transformer-based federated learning framework that learns personalized self-attention for each client while aggregating the other parameters among the clients. Instead of using a vanilla personalization mechanism that maintains personalized self-attention layers of each client locally, we develop a learn-to-personalize mechanism to further encourage the cooperation among clients and to increase the scalability and generalization of FedTP. Specifically, we achieve this by learning a hypernetwork on the server that outputs the personalized projection matrices of self-attention layers to generate clientwise queries, keys, and values. Furthermore, we present the generalization bound for FedTP with the learn-to-personalize mechanism. Extensive experiments verify that FedTP with the learn-to-personalize mechanism yields state-of-the-art performance in the non-IID scenarios. Our code is available online https://github.com/zhyczy/FedTP .},
  archive      = {J_TNNLS},
  author       = {Hongxia Li and Zhongyi Cai and Jingya Wang and Jiangnan Tang and Weiping Ding and Chin-Teng Lin and Ye Shi},
  doi          = {10.1109/TNNLS.2023.3269062},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13426-13440},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FedTP: Federated learning by transformer personalization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A novel composite graph neural network. <em>TNNLS</em>,
<em>35</em>(10), 13411–13425. (<a
href="https://doi.org/10.1109/TNNLS.2023.3268766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have achieved great success in many fields due to their powerful capabilities of processing graph-structured data. However, most GNNs can only be applied to scenarios where graphs are known, but real-world data are often noisy or even do not have available graph structures. Recently, graph learning has attracted increasing attention in dealing with these problems. In this article, we develop a novel approach to improving the robustness of the GNNs, called composite GNN. Different from existing methods, our method uses composite graphs (C-graphs) to characterize both sample and feature relations. The C-graph is a unified graph that unifies these two kinds of relations, where edges between samples represent sample similarities, and each sample has a tree-based feature graph to model feature importance and combination preference. By jointly learning multiaspect C-graphs and neural network parameters, our method improves the performance of semisupervised node classification and ensures robustness. We conduct a series of experiments to evaluate the performance of our method and the variants of our method that only learn sample relations or feature relations. Extensive experimental results on nine benchmark datasets demonstrate that our proposed method achieves the best performance on almost all the datasets and is robust to feature noises.},
  archive      = {J_TNNLS},
  author       = {Zhaogeng Liu and Jielong Yang and Xionghu Zhong and Wenwu Wang and Hechang Chen and Yi Chang},
  doi          = {10.1109/TNNLS.2023.3268766},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13411-13425},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel composite graph neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MobCovid: Confirmed cases dynamics driven time series
prediction of crowd in urban hotspot. <em>TNNLS</em>, <em>35</em>(10),
13397–13410. (<a
href="https://doi.org/10.1109/TNNLS.2023.3268291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring the crowd in urban hot spot has been an important research topic in the field of urban management and has high social impact. It can allow more flexible allocation of public resources such as public transportation schedule adjustment and arrangement of police force. After 2020, because of the epidemic of COVID-19 virus, the public mobility pattern is deeply affected by the situation of epidemic as the physical close contact is the dominant way of infection. In this study, we propose a confirmed case-driven time-series prediction of crowd in urban hot spot named MobCovid. The model is a deviation of Informer, a popular time-serial prediction model proposed in 2021. The model takes both the number of nighttime staying people in downtown and confirmed cases of COVID-19 as input and predicts both the targets. In the current period of COVID, many areas and countries have relaxed the lockdown measures on public mobility. The outdoor travel of public is based on individual decision. Report of large amount of confirmed cases would restrict the public visitation of crowded downtown. But, still, government would publish some policies to try to intervene in the public mobility and control the spread of virus. For example, in Japan, there are no compulsory measures to force people to stay at home, but measures to persuade people to stay away from downtown area. Therefore, we also merge the encoding of policies on measures of mobility restriction made by government in the model to improve the precision. We use historical data of nighttime staying people in crowded downtown and confirmed cases of Tokyo and Osaka area as study case. Multiple times of comparison with other baselines including the original Informer model prove the effectiveness of our proposed method. We believe our work can make contribution to the current knowledge on forecasting the number of crowd in urban downtown during the Covid epidemic.},
  archive      = {J_TNNLS},
  author       = {Jinyu Chen and Xiaodan Shi and Haoran Zhang and Wenjing Li and Peiran Li and Yuhao Yao and Satoshi Miyazawa and Xuan Song and Ryosuke Shibasaki},
  doi          = {10.1109/TNNLS.2023.3268291},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13397-13410},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MobCovid: Confirmed cases dynamics driven time series prediction of crowd in urban hotspot},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Exploiting neighbor effect: Conv-agnostic GNN framework for
graphs with heterophily. <em>TNNLS</em>, <em>35</em>(10), 13383–13396.
(<a href="https://doi.org/10.1109/TNNLS.2023.3267902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the homophily assumption in graph convolution networks (GCNs), a common consensus in the graph node classification task is that graph neural networks (GNNs) perform well on homophilic graphs but may fail on heterophilic graphs with many interclass edges. However, the previous interclass edges’ perspective and related homo-ratio metrics cannot well explain the GNNs’ performance under some heterophilic datasets, which implies that not all the interclass edges are harmful to GNNs. In this work, we propose a new metric based on the von Neumann entropy to reexamine the heterophily problem of GNNs and investigate the feature aggregation of interclass edges from an entire neighbor identifiable perspective. Moreover, we propose a simple yet effective Conv-Agnostic GNN framework (CAGNNs) to enhance the performance of most GNNs on the heterophily datasets by learning the neighbor effect for each node. Specifically, we first decouple the feature of each node into the discriminative feature for downstream tasks and the aggregation feature for graph convolution (GC). Then, we propose a shared mixer module to adaptively evaluate the neighbor effect of each node to incorporate the neighbor information. The proposed framework can be regarded as a plug-in component and is compatible with most GNNs. The experimental results over nine well-known benchmark datasets indicate that our framework can significantly improve performance, especially for the heterophily graphs. The average performance gain is 9.81%, 25.81%, and 20.61% compared with graph isomorphism network (GIN), graph attention network (GAT), and GCN, respectively. Extensive ablation studies and robustness analysis further verify the effectiveness, robustness, and interpretability of our framework. Code is available at https://github.com/JC-202/CAGNN .},
  archive      = {J_TNNLS},
  author       = {Jie Chen and Shouzhen Chen and Junbin Gao and Zengfeng Huang and Junping Zhang and Jian Pu},
  doi          = {10.1109/TNNLS.2023.3267902},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13383-13396},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploiting neighbor effect: Conv-agnostic GNN framework for graphs with heterophily},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Full transformer framework for robust point cloud
registration with deep information interaction. <em>TNNLS</em>,
<em>35</em>(10), 13368–13382. (<a
href="https://doi.org/10.1109/TNNLS.2023.3267333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is an essential technology in computer vision and robotics. Recently, transformer-based methods have achieved advanced performance in point cloud registration by utilizing the advantages of the transformer in order-invariance and modeling dependencies to aggregate information. However, they still suffer from indistinct feature extraction, sensitivity to noise, and outliers, owing to three major limitations: 1) the adoption of CNNs fails to model global relations due to their local receptive fields, resulting in extracted features susceptible to noise; 2) the shallow-wide architecture of transformers and the lack of positional information lead to indistinct feature extraction due to inefficient information interaction; and 3) the insufficient consideration of geometrical compatibility leads to the ambiguous identification of incorrect correspondences. To address the above-mentioned limitations, a novel full transformer network for point cloud registration is proposed, named the deep interaction transformer (DIT), which incorporates: 1) a point cloud structure extractor (PSE) to retrieve structural information and model global relations with the local feature integrator (LFI) and transformer encoders; 2) a deep-narrow point feature transformer (PFT) to facilitate deep information interaction across a pair of point clouds with positional information, such that transformers establish comprehensive associations and directly learn the relative position between points; and 3) a geometric matching-based correspondence confidence evaluation (GMCCE) method to measure spatial consistency and estimate correspondence confidence by the designed triangulated descriptor. Extensive experiments on the ModelNet40, ScanObjectNN, and 3DMatch datasets demonstrate that our method is capable of precisely aligning point clouds, consequently, achieving superior performance compared with state-of-the-art methods. The code is publicly available at https://github.com/CGuangyan-BIT/DIT .},
  archive      = {J_TNNLS},
  author       = {Guangyan Chen and Meiling Wang and Qingxiang Zhang and Li Yuan and Yufeng Yue},
  doi          = {10.1109/TNNLS.2023.3267333},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13368-13382},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Full transformer framework for robust point cloud registration with deep information interaction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Double-structured sparsity guided flexible embedding
learning for unsupervised feature selection. <em>TNNLS</em>,
<em>35</em>(10), 13354–13367. (<a
href="https://doi.org/10.1109/TNNLS.2023.3267184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel unsupervised feature selection model combined with clustering, named double-structured sparsity guided flexible embedding learning (DSFEL) for unsupervised feature selection. DSFEL includes a module for learning a block-diagonal structural sparse graph that represents the clustering structure and another module for learning a completely row-sparse projection matrix using the $\ell _{2,0}$ -norm constraint to select distinctive features. Compared with the commonly used $\ell _{2,1}$ -norm regularization term, the $\ell _{2,0}$ -norm constraint can avoid the drawbacks of sparsity limitation and parameter tuning. The optimization of the $\ell _{2,0}$ -norm constraint problem, which is a nonconvex and nonsmooth problem, is a formidable challenge, and previous optimization algorithms have only been able to provide approximate solutions. In order to address this issue, this article proposes an efficient optimization strategy that yields a closed-form solution. Eventually, through comprehensive experimentation on nine real-world datasets, it is demonstrated that the proposed method outperforms existing state-of-the-art unsupervised feature selection methods.},
  archive      = {J_TNNLS},
  author       = {Yu Guo and Yuan Sun and Zheng Wang and Feiping Nie and Fei Wang},
  doi          = {10.1109/TNNLS.2023.3267184},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13354-13367},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Double-structured sparsity guided flexible embedding learning for unsupervised feature selection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sinkhorn distance minimization for adaptive semi-supervised
social network alignment. <em>TNNLS</em>, <em>35</em>(10), 13340–13353.
(<a href="https://doi.org/10.1109/TNNLS.2023.3267126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social network alignment, aiming at linking identical identities across different social platforms, is a fundamental task in social graph mining. Most existing approaches are supervised models and require a large number of manually labeled data, which are infeasible in practice considering the yawning gap between social platforms. Recently, isomorphism across social networks is incorporated as complementary to link identities from the distribution level, which contributes to alleviating the dependency on sample-level annotations. Adversarial learning is adopted to learn a shared projection function by minimizing the distance between two social distributions. However, the hypothesis of isomorphism might not always hold true as social user behaviors are generally unpredictable, and thus a shared projection function is insufficient to handle the sophisticated cross-platform correlations. In addition, adversarial learning suffers from training instability and uncertainty, which may hinder model performance. In this article, we propose a novel meta-learning-based social network alignment model Meta-SNA to effectively capture the isomorphism and the unique characteristics of each identity. Our motivation lies in learning a shared meta-model to preserve the global cross-platform knowledge and an adaptor to learn a specific projection function for each identity. Sinkhorn distance is further introduced as the distribution closeness measurement to tackle the limitations of adversarial learning, which owns an explicitly optimal solution and can be efficiently computed by the matrix scaling algorithm. Empirically, we evaluate the proposed model over multiple datasets, and the experimental results demonstrate the superiority of Meta-SNA.},
  archive      = {J_TNNLS},
  author       = {Jie Xu and Chaozhuo Li and Feiran Huang and Zhoujun Li and Xing Xie and Philip S. Yu},
  doi          = {10.1109/TNNLS.2023.3267126},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13340-13353},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sinkhorn distance minimization for adaptive semi-supervised social network alignment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An iterative method for unsupervised robust anomaly
detection under data contamination. <em>TNNLS</em>, <em>35</em>(10),
13327–13339. (<a
href="https://doi.org/10.1109/TNNLS.2023.3267028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep anomaly detection models are based on learning normality from datasets due to the difficulty of defining abnormality by its diverse and inconsistent nature. Therefore, it has been a common practice to learn normality under the assumption that anomalous data are absent in a training dataset, which we call normality assumption. However, in practice, the normality assumption is often violated due to the nature of real data distributions that includes anomalous tails, i.e., a contaminated dataset. Thereby, the gap between the assumption and actual training data affects detrimentally in learning of an anomaly detection model. In this work, we propose a learning framework to reduce this gap and achieve better normality representation. Our key idea is to identify sample-wise normality and utilize it as an importance weight, which is updated iteratively during the training. Our framework is designed to be model-agnostic and hyperparameter insensitive so that it applies to a wide range of existing methods without careful parameter tuning. We apply our framework to three different representative approaches of deep anomaly detection that are classified into one-class classification-, probabilistic model-, and reconstruction-based approaches. In addition, we address the importance of a termination condition for iterative methods and propose a termination criterion inspired by the anomaly detection objective. We validate that our framework improves the robustness of the anomaly detection models under different levels of contamination ratios on five anomaly detection benchmark datasets and two image datasets. On various contaminated datasets, our framework improves the performance of three representative anomaly detection methods, measured by area under the ROC curve.},
  archive      = {J_TNNLS},
  author       = {Minkyung Kim and Jongmin Yu and Junsik Kim and Tae-Hyun Oh and Jun Kyun Choi},
  doi          = {10.1109/TNNLS.2023.3267028},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13327-13339},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An iterative method for unsupervised robust anomaly detection under data contamination},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time decoding of snapshot compressive imaging using
tensor FISTA-net. <em>TNNLS</em>, <em>35</em>(10), 13312–13326. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snapshot compressive imaging (SCI) cameras compress high-speed videos or hyperspectral images into measurement frames. However, decoding the data frames from measurement frames is compute-intensive. Existing state-of-the-art decoding algorithms suffer from low decoding quality or heavy running time or both, which are not practical for real-time applications. In this article, we exploit the powerful learning ability of deep neural networks (DNN) and propose a novel tensor fast iterative shrinkage-thresholding algorithm net (Tensor FISTA-Net) as a real-time decoder for SCI cameras. Since SCI cameras have an accurate physical model, we can trade training time for the decoding time by generating abundant synthetic data and training a decoder on the cloud. Tensor FISTA-Net not only learns a sparse representation of the frames through convolution layers but also reduces the decoding time and memory consumption significantly through tensor operations, which makes Tensor FISTA-Net an appropriate approach for a real-time decoder. Our proposed Tensor FISTA-Net obtains an average PSNR improvement of 0.79–2.84 dB (video images) and 2.61–4.43 dB (hyperspectral images) over the state-of-the-art algorithms, along with more clear and detailed visual results on real SCI datasets, Hammer and Wheel , respectively. Our Tensor FISTA-Net reaches 45 frames per second in video datasets and 70 frames per second in hyperspectral datasets, meeting the real-time requirement. Besides, the trained model occupies only a 12-MB memory footprint, making it applicable to real-time Internet of Things (IoT) applications.},
  archive      = {J_TNNLS},
  author       = {Xiao-Yang Liu and Qifan Huang and Xiaochen Han and Bo Wu and Linghe Kong and Anwar Walid and Xiaodong Wang},
  doi          = {10.1109/TNNLS.2023.3266998},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13312-13326},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Real-time decoding of snapshot compressive imaging using tensor FISTA-net},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor completion using bilayer multimode low-rank prior and
total variation. <em>TNNLS</em>, <em>35</em>(10), 13297–13311. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel bilayer low-rankness measure and two models based on it to recover a low-rank (LR) tensor. The global low rankness of underlying tensor is first encoded by LR matrix factorizations (MFs) to the all-mode matricizations, which can exploit multiorientational spectral low rankness. Presumably, the factor matrices of all-mode decomposition are LR, since local low-rankness property exists in within-mode correlation. In the decomposed subspace, to describe the refined local LR structures of factor/subspace, a new low-rankness insight of subspace: a double nuclear norm scheme is designed to explore the so-called second-layer low rankness. By simultaneously representing the bilayer low rankness of the all modes of the underlying tensor, the proposed methods aim to model multiorientational correlations for arbitrary $N$ -way ( $N\geq 3$ ) tensors. A block successive upper-bound minimization (BSUM) algorithm is designed to solve the optimization problem. Subsequence convergence of our algorithms can be established, and the iterates generated by our algorithms converge to the coordinatewise minimizers in some mild conditions. Experiments on several types of public datasets show that our algorithm can recover a variety of LR tensors from significantly fewer samples than its counterparts.},
  archive      = {J_TNNLS},
  author       = {Haijin Zeng and Shaoguang Huang and Yongyong Chen and Sheng Liu and Hiêp Q. Luong and Wilfried Philips},
  doi          = {10.1109/TNNLS.2023.3266841},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13297-13311},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tensor completion using bilayer multimode low-rank prior and total variation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active learning for deep visual tracking. <em>TNNLS</em>,
<em>35</em>(10), 13284–13296. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been successfully applied to the single target tracking task in recent years. Generally, training a deep CNN model requires numerous labeled training samples, and the number and quality of these samples directly affect the representational capability of the trained model. However, this approach is restrictive in practice, because manually labeling such a large number of training samples is time-consuming and prohibitively expensive. In this article, we propose an active learning method for deep visual tracking, which selects and annotates the unlabeled samples to train the deep CNN model. Under the guidance of active learning, the tracker based on the trained deep CNN model can achieve competitive tracking performance while reducing the labeling cost. More specifically, to ensure the diversity of selected samples, we propose an active learning method based on multiframe collaboration to select those training samples that should be and need to be annotated. Meanwhile, considering the representativeness of these selected samples, we adopt a nearest-neighbor discrimination method based on the average nearest-neighbor distance to screen isolated samples and low-quality samples. Therefore, the training samples’ subset selected based on our method requires only a given budget to maintain the diversity and representativeness of the entire sample set. Furthermore, we adopt a Tversky loss to improve the bounding box estimation of our tracker, which can ensure that the tracker achieves more accurate target states. Extensive experimental results confirm that our active-learning-based tracker (ALT) achieves competitive tracking accuracy and speed compared with state-of-the-art trackers on the seven most challenging evaluation benchmarks. Project website: https://sites.google.com/view/altrack/ .},
  archive      = {J_TNNLS},
  author       = {Di Yuan and Xiaojun Chang and Qiao Liu and Yi Yang and Dehua Wang and Minglei Shu and Zhenyu He and Guangming Shi},
  doi          = {10.1109/TNNLS.2023.3266837},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13284-13296},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Active learning for deep visual tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3-d brain reconstruction by hierarchical shape-perception
network from a single incomplete image. <em>TNNLS</em>, <em>35</em>(10),
13271–13283. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3-D shape reconstruction is essential in the navigation of minimally invasive and auto robot-guided surgeries whose operating environments are indirect and narrow, and there have been some works that focused on reconstructing the 3-D shape of the surgical organ through limited 2-D information available. However, the lack and incompleteness of such information caused by intraoperative emergencies (such as bleeding) and risk control conditions have not been considered. In this article, a novel hierarchical shape-perception network (HSPN) is proposed to reconstruct the 3-D point clouds (PCs) of specific brains from one single incomplete image with low latency. A branching predictor and several hierarchical attention pipelines are constructed to generate PCs that accurately describe the incomplete images and then complete these PCs with high quality. Meanwhile, attention gate blocks (AGBs) are designed to efficiently aggregate geometric local features of incomplete PCs transmitted by hierarchical attention pipelines and internal features of reconstructing PCs. With the proposed HSPN, 3-D shape perception and completion can be achieved spontaneously. Comprehensive results measured by Chamfer distance (CD) and PC-to-PC error demonstrate that the performance of the proposed HSPN outperforms other competitive methods in terms of qualitative displays, quantitative experiment, and classification evaluation.},
  archive      = {J_TNNLS},
  author       = {Bowen Hu and Choujun Zhan and Buzhou Tang and Bingchuan Wang and Baiying Lei and Shu-Qiang Wang},
  doi          = {10.1109/TNNLS.2023.3266819},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13271-13283},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {3-D brain reconstruction by hierarchical shape-perception network from a single incomplete image},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical organ-aware total-body standard-dose PET
reconstruction from low-dose PET and CT images. <em>TNNLS</em>,
<em>35</em>(10), 13258–13270. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positron emission tomography (PET) is an important functional imaging technology in early disease diagnosis. Generally, the gamma ray emitted by standard-dose tracer inevitably increases the exposure risk to patients. To reduce dosage, a lower dose tracer is often used and injected into patients. However, this often leads to low-quality PET images. In this article, we propose a learning-based method to reconstruct total-body standard-dose PET (SPET) images from low-dose PET (LPET) images and corresponding total-body computed tomography (CT) images. Different from previous works focusing only on a certain part of human body, our framework can hierarchically reconstruct total-body SPET images, considering varying shapes and intensity distributions of different body parts. Specifically, we first use one global total-body network to coarsely reconstruct total-body SPET images. Then, four local networks are designed to finely reconstruct head-neck, thorax, abdomen-pelvic, and leg parts of human body. Moreover, to enhance each local network learning for the respective local body part, we design an organ-aware network with a residual organ-aware dynamic convolution (RO-DC) module by dynamically adapting organ masks as additional inputs. Extensive experiments on 65 samples collected from uEXPLORER PET/CT system demonstrate that our hierarchical framework can consistently improve the performance of all body parts, especially for total-body PET images with PSNR of 30.6 dB, outperforming the state-of-the-art methods in SPET image reconstruction.},
  archive      = {J_TNNLS},
  author       = {Jiadong Zhang and Zhiming Cui and Caiwen Jiang and Shanshan Guo and Fei Gao and Dinggang Shen},
  doi          = {10.1109/TNNLS.2023.3266551},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13258-13270},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical organ-aware total-body standard-dose PET reconstruction from low-dose PET and CT images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medical-knowledge-based graph neural network for medication
combination prediction. <em>TNNLS</em>, <em>35</em>(10), 13246–13257.
(<a href="https://doi.org/10.1109/TNNLS.2023.3266490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medication combination prediction (MCP) can provide assistance for experts in the more thorough comprehension of complex mechanisms behind health and disease. Many recent studies focus on the patient representation from the historical medical records, but neglect the value of the medical knowledge, such as the prior knowledge and the medication knowledge. This article develops a medical-knowledge-based graph neural network (MK-GNN) model which incorporates the representation of patients and the medical knowledge into the neural network. More specifically, the features of patients are extracted from their medical records in different feature subspaces. Then these features are concatenated to obtain the feature representation of patients. The prior knowledge, which is calculated according to the mapping relationship between medications and diagnoses, provides heuristic medication features according to the diagnosis results. Such medication features can help the MK-GNN model learn optimal parameters. Moreover, the medication relationship in prescriptions is formulated as a drug network to integrate the medication knowledge into medication representation vectors. The results reveal the superior performance of the MK-GNN model compared with the state-of-the-art baselines on different evaluation metrics. The case study manifests the application potential of the MK-GNN model.},
  archive      = {J_TNNLS},
  author       = {Chao Gao and Shu Yin and Haiqiang Wang and Zhen Wang and Zhanwei Du and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3266490},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13246-13257},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Medical-knowledge-based graph neural network for medication combination prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LRAF-net: Long-range attention fusion network for
visible–infrared object detection. <em>TNNLS</em>, <em>35</em>(10),
13232–13245. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible–infrared object detection aims to improve the detector performance by fusing the complementarity of visible and infrared images. However, most existing methods only use local intramodality information to enhance the feature representation while ignoring the efficient latent interaction of long-range dependence between different modalities, which leads to unsatisfactory detection performance under complex scenes. To solve these problems, we propose a feature-enhanced long-range attention fusion network (LRAF-Net), which improves detection performance by fusing the long-range dependence of the enhanced visible and infrared features. First, a two-stream CSPDarknet53 network is used to extract the deep features from visible and infrared images, in which a novel data augmentation (DA) method is designed to reduce the bias toward a single modality through asymmetric complementary masks. Then, we propose a cross-feature enhancement (CFE) module to improve the intramodality feature representation by exploiting the discrepancy between visible and infrared images. Next, we propose a long-range dependence fusion (LDF) module to fuse the enhanced features by associating the positional encoding of multimodality features. Finally, the fused features are fed into a detection head to obtain the final detection results. Experiments on several public datasets, i.e., VEDAI, FLIR, and LLVIP, show that the proposed method obtains state-of-the-art performance compared with other methods.},
  archive      = {J_TNNLS},
  author       = {Haolong Fu and Shixun Wang and Puhong Duan and Changyan Xiao and Renwei Dian and Shutao Li and Zhiyong Li},
  doi          = {10.1109/TNNLS.2023.3266452},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13232-13245},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LRAF-net: Long-range attention fusion network for Visible–Infrared object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian-learning-based diffusion least mean square
algorithms over networks. <em>TNNLS</em>, <em>35</em>(10), 13217–13231.
(<a href="https://doi.org/10.1109/TNNLS.2023.3266402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the learning performance of the conventional diffusion least mean square (DLMS) algorithms, this article proposes Bayesian-learning-based DLMS (BL-DLMS) algorithms. First, the proposed BL-DLMS algorithms are inferred from a Gaussian state-space model-based Bayesian learning perspective. By performing Bayesian inference in the given Gaussian state-space model, a variable step-size and an estimation of the uncertainty of information of interest at each node are obtained for the proposed BL-DLMS algorithms. Next, a control method at each node is designed to improve the tracking performance of the proposed BL-DLMS algorithms in the sudden change scenario. Then, a lower bound on the variable step-size of each node of the proposed BL-DLMS algorithms is derived to maintain the optimal steady-state performance in the nonstationary scenario (unknown parameter vector of interest is time-varying). Afterward, the mean stability and the transient and steady-state mean square performance of the proposed BL-DLMS algorithms are analyzed in the nonstationary scenario. In addition, two Bayesian-learning-based diffusion bias-compensated LMS algorithms are proposed to handle the noisy inputs. Finally, the superior learning performance of the proposed learning algorithms is verified by numerical simulations, and the simulated results are in good agreement with the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Fuyi Huang and Sheng Zhang and Wei Xing Zheng},
  doi          = {10.1109/TNNLS.2023.3266402},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13217-13231},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bayesian-learning-based diffusion least mean square algorithms over networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multichannel convolutional decoding network for graph
classification. <em>TNNLS</em>, <em>35</em>(10), 13206–13216. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have shown superior performance on graph classification tasks, and their structure can be considered as an encoder-decoder pair. However, most existing methods lack the comprehensive consideration of global and local in decoding, resulting in the loss of global information or ignoring some local information of large graphs. And the commonly used cross-entropy loss is essentially an encoder-decoder global loss, which cannot supervise the training states of the two local components (encoder and decoder). We propose a multichannel convolutional decoding network (MCCD) to solve the above-mentioned problems. MCCD first adopts a multichannel GCN encoder, which has better generalization than a single-channel GCN encoder since different channels can extract graph information from different perspectives. Then, we propose a novel decoder with a global-to-local learning pattern to decode graph information, and this decoder can better extract global and local information. We also introduce a balanced regularization loss to supervise the training states of the encoder and decoder so that they are sufficiently trained. Experiments on standard datasets demonstrate the effectiveness of our MCCD in terms of accuracy, runtime, and computational complexity.},
  archive      = {J_TNNLS},
  author       = {Mingjian Guang and Chungang Yan and Yuhua Xu and Junli Wang and Changjun Jiang},
  doi          = {10.1109/TNNLS.2023.3266243},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13206-13216},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A multichannel convolutional decoding network for graph classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of automated data augmentation for image
classification: Learning to compose, mix, and generate. <em>TNNLS</em>,
<em>35</em>(10), 13185–13205. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is an effective way to improve the generalization of deep learning models. However, the underlying augmentation methods mainly rely on handcrafted operations, such as flipping and cropping for image data. These augmentation methods are often designed based on human expertise or repeated trials. Meanwhile, automated data augmentation (AutoDA) is a promising research direction that frames the data augmentation process as a learning task and finds the most effective way to augment the data. In this survey, we categorize recent AutoDA methods into the composition-, mixing-, and generation-based approaches and analyze each category in detail. Based on the analysis, we discuss the challenges and future prospects as well as provide guidelines for applying AutoDA methods by considering the dataset, computation effort, and availability of domain-specific transformations. It is hoped that this article can provide a useful list of AutoDA methods and guidelines for data partitioners when deploying AutoDA in practice. The survey can also serve as a reference for further study by researchers in this emerging research area.},
  archive      = {J_TNNLS},
  author       = {Tsz-Him Cheung and Dit-Yan Yeung},
  doi          = {10.1109/TNNLS.2023.3282258},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13185-13205},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of automated data augmentation for image classification: Learning to compose, mix, and generate},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on reinforcement learning for recommender systems.
<em>TNNLS</em>, <em>35</em>(10), 13164–13184. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have been widely applied in different real-life scenarios to help us find useful information. In particular, reinforcement learning (RL)-based recommender systems have become an emerging research topic in recent years, owing to the interactive nature and autonomous learning ability. Empirical results show that RL-based recommendation methods often surpass supervised learning methods. Nevertheless, there are various challenges in applying RL in recommender systems. To understand the challenges and relevant solutions, there should be a reference for researchers and practitioners working on RL-based recommender systems. To this end, we first provide a thorough overview, comparisons, and summarization of RL approaches applied in four typical recommendation scenarios, including interactive recommendation, conversational recommendation, sequential recommendation, and explainable recommendation. Furthermore, we systematically analyze the challenges and relevant solutions on the basis of existing literature. Finally, under discussion for open issues of RL and its limitations of recommender systems, we highlight some potential research directions in this field.},
  archive      = {J_TNNLS},
  author       = {Yuanguo Lin and Yong Liu and Fan Lin and Lixin Zou and Pengcheng Wu and Wenhua Zeng and Huanhuan Chen and Chunyan Miao},
  doi          = {10.1109/TNNLS.2023.3280161},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13164-13184},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on reinforcement learning for recommender systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Hyperspectral image denoising: From model-driven,
data-driven, to model-data-driven. <em>TNNLS</em>, <em>35</em>(10),
13143–13163. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed noise pollution in HSI severely disturbs subsequent interpretations and applications. In this technical review, we first give the noise analysis in different noisy HSIs and conclude crucial points for programming HSI denoising algorithms. Then, a general HSI restoration model is formulated for optimization. Later, we comprehensively review existing HSI denoising methods, from model-driven strategy (nonlocal mean, total variation, sparse representation, low-rank matrix approximation, and low-rank tensor factorization), data-driven strategy [2-D convolutional neural network (CNN), 3-D CNN, hybrid, and unsupervised networks], to model-data-driven strategy. The advantages and disadvantages of each strategy for HSI denoising are summarized and contrasted. Behind this, we present an evaluation of the HSI denoising methods for various noisy HSIs in simulated and real experiments. The classification results of denoised HSIs and execution efficiency are depicted through these HSI denoising methods. Finally, prospects of future HSI denoising methods are listed in this technical review to guide the ongoing road for HSI denoising. The HSI denoising dataset could be found at https://qzhang95.github.io .},
  archive      = {J_TNNLS},
  author       = {Qiang Zhang and Yaming Zheng and Qiangqiang Yuan and Meiping Song and Haoyang Yu and Yi Xiao},
  doi          = {10.1109/TNNLS.2023.3278866},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13143-13163},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral image denoising: From model-driven, data-driven, to model-data-driven},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of convex clustering from multiple perspectives:
Models, optimizations, statistical properties, applications, and
connections. <em>TNNLS</em>, <em>35</em>(10), 13122–13142. (<a
href="https://doi.org/10.1109/TNNLS.2023.3276393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional partition-based clustering is very sensitive to the initialized centroids, which are easily stuck in the local minimum due to their nonconvex objectives. To this end, convex clustering is proposed by relaxing $K$ -means clustering or hierarchical clustering. As an emerging and excellent clustering technology, convex clustering can solve the instability problems of partition-based clustering methods. Generally, convex clustering objective consists of the fidelity and the shrinkage terms. The fidelity term encourages the cluster centroids to estimate the observations and the shrinkage term shrinks the cluster centroids matrix so that their observations share the same cluster centroid in the same category. Regularized by the $\ell _{p_{n}}$ -norm ( $p_{n}\in \{1,2,+\infty \}$ ), the convex objective guarantees the global optimal solution of the cluster centroids. This survey conducts a comprehensive review of convex clustering. It starts with the convex clustering as well as its nonconvex variants and then concentrates on the optimization algorithms and the hyperparameters setting. In particular, the statistical properties, the applications, and the connections of convex clustering with other methods are reviewed and discussed thoroughly for a better understanding the convex clustering. Finally, we briefly summarize the development of convex clustering and present some potential directions for future research.},
  archive      = {J_TNNLS},
  author       = {Qiying Feng and C. L. Philip Chen and Licheng Liu},
  doi          = {10.1109/TNNLS.2023.3276393},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13122-13142},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A review of convex clustering from multiple perspectives: Models, optimizations, statistical properties, applications, and connections},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward explainable affective computing: A review.
<em>TNNLS</em>, <em>35</em>(10), 13101–13121. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective computing has an unprecedented potential to change the way humans interact with technology. While the last decades have witnessed vast progress in the field, multimodal affective computing systems are generally black box by design. As affective systems start to be deployed in real-world scenarios, such as education or healthcare, a shift of focus toward improved transparency and interpretability is needed. In this context, how do we explain the output of affective computing models? and how to do so without limiting predictive performance? In this article, we review affective computing work from an explainable AI (XAI) perspective, collecting and synthesizing relevant papers into three major XAI approaches: premodel (applied before training), in-model (applied during training), and postmodel (applied after training). We present and discuss the most fundamental challenges in the field, namely, how to relate explanations back to multimodal and time-dependent data, how to integrate context and inductive biases into explanations using mechanisms such as attention, generative modeling, or graph-based methods, and how to capture intramodal and cross-modal interactions in post hoc explanations. While explainable affective computing is still nascent, existing methods are promising, contributing not only toward improved transparency but, in many cases, surpassing state-of-the-art results. Based on these findings, we explore directions for future research and discuss the importance of data-driven XAI and explanation goals, and explainee needs definition, as well as causability or the extent to which a given method leads to human understanding.},
  archive      = {J_TNNLS},
  author       = {Karina Cortiñas-Lorenzo and Gerard Lacey},
  doi          = {10.1109/TNNLS.2023.3270027},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13101-13121},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward explainable affective computing: A review},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying appropriate intellectual property protection
mechanisms for machine learning models: A systematization of
watermarking, fingerprinting, model access, and attacks. <em>TNNLS</em>,
<em>35</em>(10), 13082–13100. (<a
href="https://doi.org/10.1109/TNNLS.2023.3270135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The commercial use of machine learning (ML) is spreading; at the same time, ML models are becoming more complex and more expensive to train, which makes intellectual property protection (IPP) of trained models a pressing issue. Unlike other domains that can build on a solid understanding of the threats, attacks, and defenses available to protect their IP, ML-related research in this regard is still very fragmented. This is also due to a missing unified view as well as a common taxonomy of these aspects. In this article, we systematize our findings on IPP in ML while focusing on threats and attacks identified and defenses proposed at the time of writing. We develop a comprehensive threat model for IP in ML, categorizing attacks and defenses within a unified and consolidated taxonomy, thus bridging research from both the ML and security communities.},
  archive      = {J_TNNLS},
  author       = {Isabell Lederer and Rudolf Mayer and Andreas Rauber},
  doi          = {10.1109/TNNLS.2023.3270135},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {10},
  number       = {10},
  pages        = {13082-13100},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Identifying appropriate intellectual property protection mechanisms for machine learning models: A systematization of watermarking, fingerprinting, model access, and attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast multiview clustering by optimal graph mining.
<em>TNNLS</em>, <em>35</em>(9), 13071–13077. (<a
href="https://doi.org/10.1109/TNNLS.2023.3256066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering (MVC) aims to exploit heterogeneous information from different sources and was extensively investigated in the past decade. However, far less attention has been paid to handling large-scale multiview data. In this brief, we fill this gap and propose a fast multiview clustering by an optimal graph mining model to handle large-scale data. We mine a consistent clustering structure from landmark-based graphs of different views, from which the optimal graph based on the one-hot encoding of cluster labels is recovered. Our model is parameter-free, so intractable hyperparameter tuning is avoided. An efficient algorithm of linear complexity to the number of samples is developed to solve the optimization problems. Extensive experiments on real-world datasets of various scales demonstrate the superiority of our proposal.},
  archive      = {J_TNNLS},
  author       = {Jitao Lu and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3256066},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {13071-13077},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast multiview clustering by optimal graph mining},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multidimensional pruning and its extension: A unified
framework for model compression. <em>TNNLS</em>, <em>35</em>(9),
13056–13070. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observing that the existing model compression approaches only focus on reducing the redundancies in convolutional neural networks (CNNs) along one particular dimension (e.g., the channel or spatial or temporal dimension), in this work, we propose our multidimensional pruning (MDP) framework, which can compress both 2-D CNNs and 3-D CNNs along multiple dimensions in an end-to-end fashion. Specifically, MDP indicates the simultaneous reduction of channels and more redundancy on other additional dimensions. The redundancy of additional dimensions depends on the input data, i.e., spatial dimension for 2-D CNNs when using images as the input data, and spatial and temporal dimensions for 3-D CNNs when using videos as the input data. We further extend our MDP framework to the MDP-Point approach for compressing point cloud neural networks (PCNNs) whose inputs are irregular point clouds (e.g., PointNet). In this case, the redundancy along the additional dimension indicates the point dimension (i.e., the number of points). Comprehensive experiments on six benchmark datasets demonstrate the effectiveness of our MDP framework and its extended version MDP-Point for compressing CNNs and PCNNs, respectively.},
  archive      = {J_TNNLS},
  author       = {Jinyang Guo and Dong Xu and Wanli Ouyang},
  doi          = {10.1109/TNNLS.2023.3266435},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {13056-13070},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multidimensional pruning and its extension: A unified framework for model compression},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast yet effective machine unlearning. <em>TNNLS</em>,
<em>35</em>(9), 13046–13055. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlearning the data observed during the training of a machine learning (ML) model is an important task that can play a pivotal role in fortifying the privacy and security of ML-based applications. This article raises the following questions: 1) can we unlearn a single or multiple class(es) of data from an ML model without looking at the full training data even once? and 2) can we make the process of unlearning fast and scalable to large datasets, and generalize it to different deep networks? We introduce a novel machine unlearning framework with error-maximizing noise generation and impair-repair based weight manipulation that offers an efficient solution to the above questions. An error-maximizing noise matrix is learned for the class to be unlearned using the original model. The noise matrix is used to manipulate the model weights to unlearn the targeted class of data. We introduce impair and repair steps for a controlled manipulation of the network weights. In the impair step, the noise matrix along with a very high learning rate is used to induce sharp unlearning in the model. Thereafter, the repair step is used to regain the overall performance. With very few update steps, we show excellent unlearning while substantially retaining the overall model accuracy. Unlearning multiple classes requires a similar number of update steps as for a single class, making our approach scalable to large problems. Our method is quite efficient in comparison to the existing methods, works for multiclass unlearning, does not put any constraints on the original optimization mechanism or network design, and works well in both small and large-scale vision tasks. This work is an important step toward fast and easy implementation of unlearning in deep networks. Source code: https://github.com/vikram2000b/Fast-Machine-Unlearning .},
  archive      = {J_TNNLS},
  author       = {Ayush K. Tarun and Vikram S. Chundawat and Murari Mandal and Mohan Kankanhalli},
  doi          = {10.1109/TNNLS.2023.3266233},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {13046-13055},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast yet effective machine unlearning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly supervised temporal action localization with
bidirectional semantic consistency constraint. <em>TNNLS</em>,
<em>35</em>(9), 13032–13045. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised temporal action localization (WTAL) aims to classify and localize temporal boundaries of actions for the video, given only video-level category labels in the training datasets. Due to the lack of boundary information during training, existing approaches formulate WTAL as a classification problem, i.e., generating the temporal class activation map (T-CAM) for localization. However, with only classification loss, the model would be suboptimized, i.e., the action-related scenes are enough to distinguish different class labels. Regarding other actions in the action-related scene (i.e., the scene same as positive actions) as co-scene actions, this suboptimized model would misclassify the co-scene actions as positive actions. To address this misclassification, we propose a simple yet efficient method, named bidirectional semantic consistency constraint (Bi-SCC), to discriminate the positive actions from co-scene actions. The proposed Bi-SCC first adopts a temporal context augmentation to generate an augmented video that breaks the correlation between positive actions and their co-scene actions in the inter-video. Then, a semantic consistency constraint (SCC) is used to enforce the predictions of the original video and augmented video to be consistent, hence suppressing the co-scene actions. However, we find that this augmented video would destroy the original temporal context. Simply applying the consistency constraint would affect the completeness of localized positive actions. Hence, we boost the SCC in a bidirectional way to suppress co-scene actions while ensuring the integrity of positive actions, by cross-supervising the original and augmented videos. Finally, our proposed Bi-SCC can be applied to current WTAL approaches and improve their performance. Experimental results show that our approach outperforms the state-of-the-art methods on THUMOS14 and ActivityNet. The code is available at https://github.com/lgzlIlIlI/BiSCC .},
  archive      = {J_TNNLS},
  author       = {Guozhang Li and De Cheng and Xinpeng Ding and Nannan Wang and Jie Li and Xinbo Gao},
  doi          = {10.1109/TNNLS.2023.3266062},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {13032-13045},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weakly supervised temporal action localization with bidirectional semantic consistency constraint},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised deep tensor network for
hyperspectral–multispectral image fusion. <em>TNNLS</em>,
<em>35</em>(9), 13017–13031. (<a
href="https://doi.org/10.1109/TNNLS.2023.3266038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusing low-resolution (LR) hyperspectral images (HSIs) with high-resolution (HR) multispectral images (MSIs) is a significant technology to enhance the resolution of HSIs. Despite the encouraging results from deep learning (DL) in HSI–MSI fusion, there are still some issues. First, the HSI is a multidimensional signal, and the representability of current DL networks for multidimensional features has not been thoroughly investigated. Second, most DL HSI–MSI fusion networks need HR HSI ground truth for training, but it is often unavailable in reality. In this study, we integrate tensor theory with DL and propose an unsupervised deep tensor network (UDTN) for HSI–MSI fusion. We first propose a tensor filtering layer prototype and further build a coupled tensor filtering module. It jointly represents the LR HSI and HR MSI as several features revealing the principal components of spectral and spatial modes and a sharing code tensor describing the interaction among different modes. Specifically, the features on different modes are represented by the learnable filters of tensor filtering layers, the sharing code tensor is learned by a projection module, in which a co-attention is proposed to encode the LR HSI and HR MSI and then project them onto the sharing code tensor. The coupled tensor filtering module and projection module are jointly trained from the LR HSI and HR MSI in an unsupervised and end-to-end way. The latent HR HSI is inferred with the sharing code tensor, the features on spatial modes of HR MSIs, and the spectral mode of LR HSIs. Experiments on simulated and real remote-sensing datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jingxiang Yang and Liang Xiao and Yong-Qiang Zhao and Jonathan Cheung-Wai Chan},
  doi          = {10.1109/TNNLS.2023.3266038},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {13017-13031},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised deep tensor network for Hyperspectral–Multispectral image fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic neural network predictive compensation-based
point-to-point iterative learning control with nonuniform batch length.
<em>TNNLS</em>, <em>35</em>(9), 13005–13016. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses the problem of nonuniform running length in incomplete tracking control, which often occurs in industrial processes due to artificial or environmental changes, such as chemical engineering. It affects the design and application of iterative learning control (ILC) that relies on the strictly repetitive property. Therefore, a dynamic neural network (NN) predictive compensation strategy is proposed under the point-to-point ILC framework. To handle the difficulty of establishing an accurate mechanism model for real process control, the data-driven approach is also introduced. First, applying the iterative dynamic linearization (IDL) technique and radial basis function NN (RBFNN) to construct the iterative dynamic predictive data model (IDPDM) relies on input-output (I/O) signal, and the extended variable is defined by a predictive model to compensate for the incomplete operation length. Then, a learning algorithm based on multiple iteration errors is proposed using an objective function. This learning gain is constantly updated through the NN to adapt to changes in the system. In addition, the composite energy function (CEF) and compression mapping prove that the system is convergent. Finally, two numerical simulation examples are given.},
  archive      = {J_TNNLS},
  author       = {Rui Hou and Li Jia and Xuhui Bu and Chengyu Zhou},
  doi          = {10.1109/TNNLS.2023.3265930},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {13005-13016},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic neural network predictive compensation-based point-to-point iterative learning control with nonuniform batch length},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully distributed event-triggered anti-windup consensus of
heterogeneous systems with input saturation and an active leader.
<em>TNNLS</em>, <em>35</em>(9), 12993–13004. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the event-based fully distributed consensus problem for linear heterogeneous multiagent systems (MASs) subject to input saturation. A leader with unknown but bounded control input is also considered. Based on an adaptive dynamic event-triggered protocol, all the agents can reach output consensus without knowing any global knowledge. Moreover, by applying a multiple-level saturation technique, the input-constrained leader-following consensus control is achieved. The given event-triggered algorithm can be utilized for the directed graph containing a spanning tree with the leader as the root. One distinct feature compared with previous works is that the proposed protocol can achieve saturated control without any a priori condition, instead, the local information is needed. Finally, the numerical simulations are illustrated to verify the performance of the proposed protocol.},
  archive      = {J_TNNLS},
  author       = {Chenhang Yan and Liping Yan and Yuezu Lv and Yuanqing Xia},
  doi          = {10.1109/TNNLS.2023.3265637},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12993-13004},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully distributed event-triggered anti-windup consensus of heterogeneous systems with input saturation and an active leader},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain emotion perception inspired EEG emotion recognition
with deep reinforcement learning. <em>TNNLS</em>, <em>35</em>(9),
12979–12992. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the well-known Papez circuit theory and neuroscience knowledge of reinforcement learning, a double dueling deep $Q$ network (DQN) is built incorporating the electroencephalogram (EEG) signals of the frontal lobe as prior information, which is named frontal lobe double dueling DQN (FLD3QN). The framework of FLD3QN is constructed in accord with the brain emotion mechanism which takes the frontal lobe and the thalamus as the core, in which the part of the Papez circuit is simulated by the bifrontal lobe residual convolution neural network (BiFRCNN). Moreover, a step penalty factor is designed to constrain the number of mistakes of the agent. The ablation studies results on the public EEG emotion dataset DEAP verified the important roles of the frontal lobe and the Papez circuit in modeling the procedure of learning rewards during the perception of emotions, with a great increase in the average accuracies by 25.24% and 23.31% in valence and arousal dimensions.},
  archive      = {J_TNNLS},
  author       = {Dongdong Li and Li Xie and Zhe Wang and Hai Yang},
  doi          = {10.1109/TNNLS.2023.3265730},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12979-12992},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Brain emotion perception inspired EEG emotion recognition with deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep multiview adaptive clustering with semantic invariance.
<em>TNNLS</em>, <em>35</em>(9), 12965–12978. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering has attracted significant attention in various fields, due to the superiority in mining patterns of multiview data. However, previous methods are still confronted with two challenges. First, they do not fully consider the semantic invariance of multiview data in aggregating complementary information, degrading semantic robustness of fusion representations. Second, they rely on predefined clustering strategies to mine patterns, lacking adequate explorations of data structures. To address the challenges, deep multiview adaptive clustering via semantic invariance (DMAC-SI) is proposed, which learns an adaptive clustering strategy on semantics-robust fusion representations to fully explore structures in mining patterns. Specifically, a mirror fusion architecture is devised to explore interview invariance and intrainstance invariance hidden in multiview data, which captures invariant semantics of complementary information to learn semantics-robust fusion representations. Then, a Markov decision process of multiview data partitions is proposed within the reinforcement learning framework, which learns an adaptive clustering strategy on semantics-robust fusion representations to guarantee the structure explorations in mining patterns. The two components seamlessly collaborate in an end-to-end manner to accurately partition multiview data. Finally, extensive experiment results on five benchmark datasets demonstrate that DMAC-SI outperforms the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Jing Gao and Meng Liu and Peng Li and Jianing Zhang and Zhikui Chen},
  doi          = {10.1109/TNNLS.2023.3265699},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12965-12978},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep multiview adaptive clustering with semantic invariance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convolutional neural network-based lane-change strategy via
motion image representation for automated and connected vehicles.
<em>TNNLS</em>, <em>35</em>(9), 12953–12964. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lane-change decision-making module of automated and connected vehicles (ACVs) is one of the most crucial and challenging issues to be addressed. Motivated by human beings’ underlying driving paradigm and the convolutional neural network’s (CNN) dramatic capability of extracting features and learning strategies, this article proposes a CNN-based lane-change decision-making method via the dynamic motion image representation. Human drivers take proper driving maneuvers after they subconsciously construct the dynamic traffic scene representation in their brains, so this study first proposes the dynamic motion image representation method to reveal informative traffic situations in the motion-sensitive area (MSA), which provides a full view of surrounding cars. Then, this article develops a CNN model to extract the underlying features and learn driving policies from labeled datasets of MSA motion images. Besides, a safety-constrained layer is added to avoid vehicle collisions. We build a simulation platform based on the simulation of urban mobility (SUMO) to collect traffic datasets and test our proposed method. In addition, real-world traffic datasets are also involved to further investigate the proposed method’s performance. The rule-based strategy and reinforcement learning (RL)-based method are used to compare with our approach. All results demonstrate that the proposed method performs lane-change decision-making much better than prevailing methods, which suggests our scheme has huge potential to accelerate the deployment of ACVs and is worth further study.},
  archive      = {J_TNNLS},
  author       = {Shuo Cheng and Zheng Wang and Bo Yang and Kimihiko Nakano},
  doi          = {10.1109/TNNLS.2023.3265662},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12953-12964},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convolutional neural network-based lane-change strategy via motion image representation for automated and connected vehicles},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generic self-supervised framework of learning invariant
discriminative features. <em>TNNLS</em>, <em>35</em>(9), 12938–12952.
(<a href="https://doi.org/10.1109/TNNLS.2023.3265607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) has become a popular method for generating invariant representations without the need for human annotations. Nonetheless, the desired invariant representation is achieved by utilizing prior online transformation functions on the input data. As a result, each SSL framework is customized for a particular data type, for example, visual data, and further modifications are required if it is used for other dataset types. On the other hand, autoencoder (AE), which is a generic and widely applicable framework, mainly focuses on dimension reduction and is not suited for learning invariant representation. This article proposes a generic SSL framework based on a constrained self-labeling assignment process that prevents degenerate solutions. Specifically, the prior transformation functions are replaced with a self-transformation mechanism, derived through an unsupervised training process of adversarial training, for imposing invariant representations. Via the self-transformation mechanism, pairs of augmented instances can be generated from the same input data. Finally, a training objective based on contrastive learning is designed by leveraging both the self-labeling assignment and the self-transformation mechanism. Despite the fact that the self-transformation process is very generic, the proposed training strategy outperforms a majority of state-of-the-art representation learning methods based on AE structures. To validate the performance of our method, we conduct experiments on four types of data, namely visual, audio, text, and mass spectrometry data and compare them in terms of four quantitative metrics. Our comparison results demonstrate that the proposed method is effective and robust in identifying patterns within the tested datasets.},
  archive      = {J_TNNLS},
  author       = {Foivos Ntelemis and Yaochu Jin and Spencer A. Thomas},
  doi          = {10.1109/TNNLS.2023.3265607},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12938-12952},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A generic self-supervised framework of learning invariant discriminative features},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral-spatial global graph reasoning for hyperspectral
image classification. <em>TNNLS</em>, <em>35</em>(9), 12924–12937. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been widely applied to hyperspectral image classification (HSIC). However, traditional convolutions can not effectively extract features for objects with irregular distributions. Recent methods attempt to address this issue by performing graph convolutions on spatial topologies, but fixed graph structures and local perceptions limit their performances. To tackle these problems, in this article, different from previous approaches, we perform the superpixel generation on intermediate features during network training to adaptively produce homogeneous regions, obtain graph structures, and further generate spatial descriptors, which are served as graph nodes. Besides spatial objects, we also explore the graph relationships between channels by reasonably aggregating channels to generate spectral descriptors. The adjacent matrices in these graph convolutions are obtained by considering the relationships among all descriptors to realize global perceptions. By combining the extracted spatial and spectral graph features, we finally obtain a spectral-spatial graph reasoning network (SSGRN). The spatial and spectral parts of SSGRN are separately called spatial and spectral graph reasoning subnetworks. Comprehensive experiments on four public datasets demonstrate the competitiveness of the proposed methods compared with other state-of-the-art graph convolution-based approaches.},
  archive      = {J_TNNLS},
  author       = {Di Wang and Bo Du and Liangpei Zhang},
  doi          = {10.1109/TNNLS.2023.3265560},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12924-12937},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spectral-spatial global graph reasoning for hyperspectral image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An energy-efficient bayesian neural network implementation
using stochastic computing method. <em>TNNLS</em>, <em>35</em>(9),
12913–12923. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of Bayesian neural networks (BNNs) to real-world uncertainties and incompleteness has led to their application in some safety-critical fields. However, evaluating uncertainty during BNN inference requires repeated sampling and feed-forward computing, making them challenging to deploy in low-power or embedded devices. This article proposes the use of stochastic computing (SC) to optimize the hardware performance of BNN inference in terms of energy consumption and hardware utilization. The proposed approach adopts bitstream to represent Gaussian random number and applies it in the inference phase. This allows for the omission of complex transformation computations in the central limit theorem-based Gaussian random number generating (CLT-based GRNG) method and the simplification of multipliers as AND operations. Furthermore, an asynchronous parallel pipeline calculation technique is proposed in computing block to enhance operation speed. Compared with conventional binary radix-based BNN, SC-based BNN (StocBNN) realized by FPGA with 128-bit bitstream consumes much less energy consumption and hardware resources with less than 0.1% accuracy decrease when dealing with MNIST/Fashion-MNIST datasets.},
  archive      = {J_TNNLS},
  author       = {Xiaotao Jia and Huiyi Gu and Yuhao Liu and Jianlei Yang and Xueyan Wang and Weitao Pan and Youguang Zhang and Sorin Cotofana and Weisheng Zhao},
  doi          = {10.1109/TNNLS.2023.3265533},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12913-12923},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An energy-efficient bayesian neural network implementation using stochastic computing method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Demystifying and mitigating bias for node representation
learning. <em>TNNLS</em>, <em>35</em>(9), 12899–12912. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node representation learning has attracted increasing attention due to its efficacy for various applications on graphs. However, fairness is a largely under-explored territory within the field, although it is shown that the use of graph structure in learning amplifies bias. To this end, this work theoretically explains the sources of bias in node representations obtained via graph neural networks (GNNs). It is revealed that both nodal features and graph structure lead to bias in the obtained representations. Building upon the analysis, fairness-aware data augmentation frameworks are developed to reduce the intrinsic bias. Our theoretical analysis and proposed schemes can be readily employed in understanding and mitigating bias for various GNN-based learning mechanisms. Extensive experiments on node classification and link prediction over multiple real networks are carried out, and it is shown that the proposed augmentation strategies can improve fairness while providing comparable utility to state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {O. Deniz Kose and Yanning Shen},
  doi          = {10.1109/TNNLS.2023.3265370},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12899-12912},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Demystifying and mitigating bias for node representation learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoGMap: Learning to map large-scale sparse graphs on
memristive crossbars. <em>TNNLS</em>, <em>35</em>(9), 12888–12898. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse representation of graphs has shown great potential for accelerating the computation of graph applications (e.g., social networks and knowledge graphs) on traditional computing architectures (CPU, GPU, or TPU). But, the exploration of large-scale sparse graph computing on processing-in-memory (PIM) platforms (typically with memristive crossbars) is still in its infancy. To implement the computation or storage of large-scale or batch graphs on memristive crossbars, a natural assumption is that a large-scale crossbar is demanded, but with low utilization. Some recent works question this assumption; to avoid the waste of storage and computational resource, the fixed-size or progressively scheduled “block partition” schemes are proposed. However, these methods are coarse-grained or static and are not effectively sparsity-aware. This work proposes the dynamic sparsity-aware mapping scheme generating method that models the problem with a sequential decision-making model, and optimizes it by reinforcement learning (RL) algorithm (REINFORCE). Our generating model [long short-term memory (LSTM), combined with the dynamic-fill scheme] generates remarkable mapping performance on the small-scale graph/matrix data (complete mapping costs 43% area of the original matrix) and two large-scale matrix data (costing 22.5% area on qh882 and 17.1% area on qh1484). Our method may be extended to sparse graph computing on other PIM architectures, not limited to the memristive device-based platforms.},
  archive      = {J_TNNLS},
  author       = {Bo Lyu and Shengbo Wang and Shiping Wen and Kaibo Shi and Yin Yang and Lingfang Zeng and Tingwen Huang},
  doi          = {10.1109/TNNLS.2023.3265383},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12888-12898},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AutoGMap: Learning to map large-scale sparse graphs on memristive crossbars},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiagent trust region policy optimization. <em>TNNLS</em>,
<em>35</em>(9), 12873–12887. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend trust region policy optimization (TRPO) to cooperative multiagent reinforcement learning (MARL) for partially observable Markov games (POMGs). We show that the policy update rule in TRPO can be equivalently transformed into a distributed consensus optimization for networked agents when the agents’ observation is sufficient. By using a local convexification and trust-region method, we propose a fully decentralized MARL algorithm based on a distributed alternating direction method of multipliers (ADMM). During training, agents only share local policy ratios with neighbors via a peer-to-peer communication network. Compared with traditional centralized training methods in MARL, the proposed algorithm does not need a control center to collect global information, such as global state, collective reward, or shared policy and value network parameters. Experiments on two cooperative environments demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Hepeng Li and Haibo He},
  doi          = {10.1109/TNNLS.2023.3265358},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12873-12887},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiagent trust region policy optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyclical curriculum learning. <em>TNNLS</em>,
<em>35</em>(9), 12864–12872. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs) are inspired by human learning. However, unlike human education, classical ANN does not use a curriculum. Curriculum learning (CL) refers to the process of ANN training in which samples are used in a meaningful order. When using CL, training begins with a subset of the dataset and new samples are added throughout the training, or training begins with the entire dataset and the number of samples used is reduced. With these changes in training dataset size, better results can be obtained with curriculum, anti-curriculum, or random-curriculum methods than the vanilla method. However, a generally efficient CL method for various architectures and datasets is not found. In this article, we propose cyclical CL (CCL), in which the data size used during training changes cyclically rather than simply increasing or decreasing. Instead of using only the vanilla method or only the curriculum method, using both methods cyclically like in CCL provides more successful results. We tested the method on 18 different datasets and 15 architectures in image and text classification tasks and obtained more successful results than no-CL and existing CL methods. We also have shown theoretically that it is less erroneous to apply CL and vanilla cyclically instead of using only CL or only the vanilla method. The code of the cyclical curriculum is available at https://github.com/CyclicalCurriculum/Cyclical-Curriculum .},
  archive      = {J_TNNLS},
  author       = {H. Toprak Kesgin and M. Fatih Amasyali},
  doi          = {10.1109/TNNLS.2023.3265331},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12864-12872},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cyclical curriculum learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Effective value analysis of fuzzy similarity relation in
HQSS for efficient granulation. <em>TNNLS</em>, <em>35</em>(9),
12849–12863. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical quotient space structure (HQSS), as a typical description of granular computing (GrC), focuses on hierarchically granulating fuzzy data and mining hidden knowledge. The key step of constructing HQSS is to transform the fuzzy similarity relation into fuzzy equivalence relation. However, on one hand, the transformation process has high time complexity. On the other hand, it is difficult to mine knowledge directly from fuzzy similarity relation due to its information redundancy, i.e., sparsity of effective information. Therefore, this article mainly focuses on proposing an efficient granulation approach for constructing HQSS by quickly extracting the effective value of fuzzy similarity relation. First, the effective value and effective position of fuzzy similarity relation are defined according to whether they could be retained in fuzzy equivalence relation. Second, the number and composition of effective values are presented to confirm that which elements are effective values. Based on these above theories, redundant information and sparse effective information in fuzzy similarity relation could be completely distinguished. Next, both isomorphism and similarity between two fuzzy similarity relations are researched based on the effective value. The isomorphism between two fuzzy equivalence relations is discussed based on the effective value. Then, the algorithm with low time complexity for extracting effective values of fuzzy similarity relation is introduced. On the basis, the algorithm for constructing HQSS is presented to realize efficient granulation of fuzzy data. The proposed algorithms could accurately extract effective information from the fuzzy similarity relation and construct the same HQSS with the fuzzy equivalence relation while greatly reducing the time complexity. Finally, relevant experiments on 15 UCI datasets, 3 UKB datasets, and 5 image datasets are shown and analyzed to verify the effectiveness and efficiency of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Qinghua Zhang and Fan Zhao and Yunlong Cheng and Man Gao and Guoyin Wang and Shuyin Xia and Weiping Ding},
  doi          = {10.1109/TNNLS.2023.3265310},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12849-12863},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effective value analysis of fuzzy similarity relation in HQSS for efficient granulation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural adaptive intermittent output feedback control for
autonomous underwater vehicles with full-state quantitative designs.
<em>TNNLS</em>, <em>35</em>(9), 12836–12848. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a neural adaptive intermittent output feedback control is investigated for autonomous underwater vehicles (AUVs) with full-state quantitative designs (FSQDs). To achieve the prespecified tracking performance determined by quantitative indices (e.g., overshoot, convergence time, steady-state accuracy, and maximum deviation) at both kinematic and kinetic levels, FSQDs are designed by transforming constrained AUV model into an unconstrained model via one-sided hyperbolic cosecant boundaries and nonlinear mapping functions. An intermittent sampling-based neural estimator (ISNE) is devised to reconstruct the matched and mismatched lumped disturbances as well as immeasurable velocity states of transformed AUV model, where only system outputs after intermittent sampling are required. Using the estimations of ISNE and the system outputs after triggering, an intermittent output feedback control law incorporated with hybrid threshold event-triggered mechanism (HTETM) is designed to achieve ultimately uniformly bounded (UUB) results. Simulation results are provided and analyzed to validate the effectiveness of the studied control strategy with application to an omnidirectional intelligent navigator (ODIN).},
  archive      = {J_TNNLS},
  author       = {Yi Shi and Wei Xie and Weixing Chen and Lantao Xing and Weidong Zhang},
  doi          = {10.1109/TNNLS.2023.3265321},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12836-12848},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural adaptive intermittent output feedback control for autonomous underwater vehicles with full-state quantitative designs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Template-based contrastive distillation pretraining for math
word problem solving. <em>TNNLS</em>, <em>35</em>(9), 12823–12835. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since math word problem (MWP) solving aims to transform natural language problem description into executable solution equations, an MWP solver needs to not only comprehend the real-world narrative described in the problem text but also identify the relationships among the quantifiers and variables implied in the problem and maps them into a reasonable solution equation logic. Recently, although deep learning models have made great progress in MWPs, they ignore the grounding equation logic implied by the problem text. Besides, as we all know, pretrained language models (PLM) have a wealth of knowledge and high-quality semantic representations, which may help solve MWPs, but they have not been explored in the MWP-solving task. To harvest the equation logic and real-world knowledge, we propose a template-based contrastive distillation pretraining (TCDP) approach based on a PLM-based encoder to incorporate mathematical logic knowledge by multiview contrastive learning while retaining rich real-world knowledge and high-quality semantic representation via knowledge distillation. We named the pretrained PLM-based encoder by our approach as MathEncoder. Specifically, the mathematical logic is first summarized by clustering the symbolic solution templates among MWPs and then injected into the deployed PLM-based encoder by conducting supervised contrastive learning based on the symbolic solution templates, which can represent the underlying solving logic in the problems. Meanwhile, the rich knowledge and high-quality semantic representation are retained by distilling them from a well-trained PLM-based teacher encoder into our MathEncoder. To validate the effectiveness of our pretrained MathEncoder, we construct a new solver named MathSolver by replacing the GRU-based encoder with our pretrained MathEncoder in GTS, which is a state-of-the-art MWP solver. The experimental results demonstrate that our method can carry a solver’s understanding ability of MWPs to a new stage by outperforming existing state-of-the-art methods on two widely adopted benchmarks Math23K and CM17K. Code will be available at https://github.com/QinJinghui/tcdp .},
  archive      = {J_TNNLS},
  author       = {Jinghui Qin and Zhicheng Yang and Jiaqi Chen and Xiaodan Liang and Liang Lin},
  doi          = {10.1109/TNNLS.2023.3265173},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12823-12835},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Template-based contrastive distillation pretraining for math word problem solving},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive feature embedding for infrared and visible image
fusion. <em>TNNLS</em>, <em>35</em>(9), 12810–12822. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General deep learning-based methods for infrared and visible image fusion rely on the unsupervised mechanism for vital information retention by utilizing elaborately designed loss functions. However, the unsupervised mechanism depends on a well-designed loss function, which cannot guarantee that all vital information of source images is sufficiently extracted. In this work, we propose a novel interactive feature embedding in a self-supervised learning framework for infrared and visible image fusion, attempting to overcome the issue of vital information degradation. With the help of a self-supervised learning framework, hierarchical representations of source images can be efficiently extracted. In particular, interactive feature embedding models are tactfully designed to build a bridge between self-supervised learning and infrared and visible image fusion learning, achieving vital information retention. Qualitative and quantitative evaluations exhibit that the proposed method performs favorably against state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Fan Zhao and Wenda Zhao and Huchuan Lu},
  doi          = {10.1109/TNNLS.2023.3264911},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12810-12822},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interactive feature embedding for infrared and visible image fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustered federated learning in heterogeneous environment.
<em>TNNLS</em>, <em>35</em>(9), 12796–12809. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a distributed machine learning framework that allows resource-constrained clients to train a global model jointly without compromising data privacy. Although FL is widely adopted, high degrees of systems and statistical heterogeneity are still two main challenges, which leads to potential divergence and nonconvergence. Clustered FL handles the problem of statistical heterogeneity straightly by discovering the geometric structure of clients with various data generation distributions and getting multiple global models. The number of clusters contains prior knowledge about the clustering structure and has a significant impact on the performance of clustered FL methods. Existing clustered FL methods are inadequate for adaptively inferring the optimal number of clusters in environments with high systems’ heterogeneity. To address this issue, we propose an iterative clustered FL (ICFL) framework in which the server dynamically discovers the clustering structure by successively performing incremental clustering and clustering in one iteration. We focus on the average connectivity within each cluster and give incremental clustering and clustering methods that are compatible with ICFL based on mathematical analysis. We evaluate ICFL in experiments on high degrees of systems and statistical heterogeneity, multiple datasets, and convex and nonconvex objectives. Experimental results verify our theoretical analysis and show that ICFL outperforms several clustered FL baseline methods.},
  archive      = {J_TNNLS},
  author       = {Yihan Yan and Xiaojun Tong and Shen Wang},
  doi          = {10.1109/TNNLS.2023.3264740},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12796-12809},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Clustered federated learning in heterogeneous environment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint entity and relation extraction with set prediction
networks. <em>TNNLS</em>, <em>35</em>(9), 12784–12795. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint entity and relation extraction is an important task in natural language processing, which aims to extract all relational triples mentioned in a given sentence. In essence, the relational triples mentioned in a sentence are in the form of a set, which has no intrinsic order between elements and exhibits the permutation invariant feature. However, previous seq2seq-based models require sorting the set of relational triples into a sequence beforehand with some heuristic global rules, which destroys the natural set structure. In order to break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model is not burdened with predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. In contrast to autoregressive approaches that generate triples one by one in a specific order, the proposed networks are able to directly output the final set of relational triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions through bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Various experiments on two benchmark datasets demonstrate that our proposed model significantly outperforms the current state-of-the-art (SoTA) models. Training code and trained models are now publicly available at https://github.com/DianboWork/SPN4RE .},
  archive      = {J_TNNLS},
  author       = {Dianbo Sui and Xiangrong Zeng and Yubo Chen and Kang Liu and Jun Zhao},
  doi          = {10.1109/TNNLS.2023.3264735},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12784-12795},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint entity and relation extraction with set prediction networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). BViT: Broad attention-based vision transformer.
<em>TNNLS</em>, <em>35</em>(9), 12772–12783. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have demonstrated that transformer can achieve promising performance in computer vision, by exploiting the relationship among image patches with self-attention. They only consider the attention in a single feature layer, but ignore the complementarity of attention in different layers. In this article, we propose broad attention to improve the performance by incorporating the attention relationship of different layers for vision transformer (ViT), which is called BViT. The broad attention is implemented by broad connection and parameter-free attention. Broad connection of each transformer layer promotes the transmission and integration of information for BViT. Without introducing additional trainable parameters, parameter-free attention jointly focuses on the already available attention information in different layers for extracting useful information and building their relationship. Experiments on image classification tasks demonstrate that BViT delivers superior accuracy of 75.0%/81.6% top-1 accuracy on ImageNet with 5M/22M parameters. Moreover, we transfer BViT to downstream object recognition benchmarks to achieve 98.9% and 89.9% on CIFAR10 and CIFAR100, respectively, that exceed ViT with fewer parameters. For the generalization test, the broad attention in Swin Transformer, T2T-ViT and LVT also brings an improvement of more than 1%. To sum up, broad attention is promising to promote the performance of attention-based models. Code and pretrained models are available at https://github.com/DRL/BViT .},
  archive      = {J_TNNLS},
  author       = {Nannan Li and Yaran Chen and Weifan Li and Zixiang Ding and Dongbin Zhao and Shuai Nie},
  doi          = {10.1109/TNNLS.2023.3264730},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12772-12783},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BViT: Broad attention-based vision transformer},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing urban traffic congestion using deep learning and
model predictive control. <em>TNNLS</em>, <em>35</em>(9), 12760–12771.
(<a href="https://doi.org/10.1109/TNNLS.2023.3264709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a deep learning (DL)-based control algorithm—DL velocity-based model predictive control (VMPC)—for reducing traffic congestion with slowly time-varying traffic signal controls. This control algorithm consists of system identification using DL and traffic signal control using VMPC. For the training process of DL, we established a modeling error entropy loss as the criteria inspired by the theory of stochastic distribution control (SDC) originated by the fourth author. Simulation results show that the proposed algorithm can reduce traffic congestion with a slowly varying traffic signal control input. Results of an ablation study demonstrate that this algorithm compares favorably to other model-based controllers in terms of prediction error, signal varying speed, and control effectiveness.},
  archive      = {J_TNNLS},
  author       = {Zhun Yin and Tong Liu and Chieh Wang and Hong Wang and Zhong-Ping Jiang},
  doi          = {10.1109/TNNLS.2023.3264709},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12760-12771},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reducing urban traffic congestion using deep learning and model predictive control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Higher order polynomial transformer for fine-grained
freezing of gait detection. <em>TNNLS</em>, <em>35</em>(9), 12746–12759.
(<a href="https://doi.org/10.1109/TNNLS.2023.3264647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freezing of Gait (FoG) is a common symptom of Parkinson’s disease (PD), manifesting as a brief, episodic absence, or marked reduction in walking, despite a patient’s intention to move. Clinical assessment of FoG events from manual observations by experts is both time-consuming and highly subjective. Therefore, machine learning-based FoG identification methods would be desirable. In this article, we address this task as a fine-grained human action recognition problem based on vision inputs. A novel deep learning architecture, namely, higher order polynomial transformer (HP-Transformer), is proposed to incorporate pose and appearance feature sequences to formulate fine-grained FoG patterns. In particular, a higher order self-attention mechanism is proposed based on higher order polynomials. To this end, linear, bilinear, and trilinear transformers are formulated in pursuit of discriminative fine-grained representations. These representations are treated as multiple streams and further fused by a cross-order fusion strategy for FoG detection. Comprehensive experiments on a large in-house dataset collected during clinical assessments demonstrate the effectiveness of the proposed method, and an area under the receiver operating characteristic (ROC) curve (AUC) of 0.92 is achieved for detecting FoG.},
  archive      = {J_TNNLS},
  author       = {Renfei Sun and Kun Hu and Kaylena A. Ehgoetz Martens and Markus Hagenbuchner and Ah Chung Tsoi and Mohammed Bennamoun and Simon J. G. Lewis and Zhiyong Wang},
  doi          = {10.1109/TNNLS.2023.3264647},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12746-12759},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Higher order polynomial transformer for fine-grained freezing of gait detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A broad generative network for two-stage image outpainting.
<em>TNNLS</em>, <em>35</em>(9), 12731–12745. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image outpainting is a challenge for image processing since it needs to produce a big scenery image from a few patches. In general, two-stage frameworks are utilized to unpack complex tasks and complete them step-by-step. However, the time consumption caused by training two networks will hinder the method from adequately optimizing the parameters of networks with limited iterations. In this article, a broad generative network (BG-Net) for two-stage image outpainting is proposed. As a reconstruction network in the first stage, it can be quickly trained by utilizing ridge regression optimization. In the second stage, a seam line discriminator (SLD) is designed for transition smoothing, which greatly improves the quality of images. Compared with state-of-the-art image outpainting methods, the experimental results on the Wiki-Art and Place365 datasets show that the proposed method achieves the best results under evaluation metrics: the Fréchet inception distance (FID) and the kernel inception distance (KID). The proposed BG-Net has good reconstructive ability with faster training speed than those of deep learning-based networks. It reduces the overall training duration of the two-stage framework to the same level as the one-stage framework. Furthermore, the proposed method is adapted to image recurrent outpainting, demonstrating the powerful associative drawing capability of the model.},
  archive      = {J_TNNLS},
  author       = {Zongyan Zhang and Haohan Weng and Tong Zhang and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2023.3264617},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12731-12745},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A broad generative network for two-stage image outpainting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PP-NAS: Searching for plug-and-play blocks on convolutional
neural networks. <em>TNNLS</em>, <em>35</em>(9), 12718–12730. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiscale features are of great importance in modern convolutional neural networks, showing consistent performance gains on numerous vision tasks. Therefore, many plug-and-play blocks are introduced to upgrade existing convolutional neural networks for stronger multiscale representation ability. However, the design of plug-and-play blocks is getting more and more complex, and these manually designed blocks are not optimal. In this work, we propose PP-NAS to develop plug-and-play blocks based on neural architecture search (NAS). Specifically, we design a new search space PPConv and develop a search algorithm consisting of one-level optimization, zero-one loss, and connection existence loss. PP-NAS minimizes the optimization gap between super-net and subarchitectures and can achieve good performance even without retraining. Extensive experiments on image classification, object detection, and semantic segmentation verify the superiority of PP-NAS over state-of-the-art CNNs (e.g., ResNet, ResNeXt, and Res2Net). Our code is available at https://github.com/ainieli/PP-NAS .},
  archive      = {J_TNNLS},
  author       = {Anqi Xiao and Biluo Shen and Jie Tian and Zhenhua Hu},
  doi          = {10.1109/TNNLS.2023.3264551},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12718-12730},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PP-NAS: Searching for plug-and-play blocks on convolutional neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Toward subgraph-guided knowledge graph question generation
with graph neural networks. <em>TNNLS</em>, <em>35</em>(9), 12706–12717.
(<a href="https://doi.org/10.1109/TNNLS.2023.3264519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) question generation (QG) aims to generate natural language questions from KGs and target answers. Previous works mostly focus on a simple setting that is to generate questions from a single KG triple. In this work, we focus on a more realistic setting where we aim to generate questions from a KG subgraph and target answers. In addition, most previous works built on either RNN- or Transformer-based models to encode a linearized KG subgraph, which totally discards the explicit structure information of a KG subgraph. To address this issue, we propose to apply a bidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we enhance our RNN decoder with a node-level copying mechanism to allow direct copying of node attributes from the KG subgraph to the output question. Both automatic and human evaluation results demonstrate that our model achieves new state-of-the-art scores, outperforming existing methods by a significant margin on two QG benchmarks. Experimental results also show that our QG model can consistently benefit the question-answering (QA) task as a means of data augmentation.},
  archive      = {J_TNNLS},
  author       = {Yu Chen and Lingfei Wu and Mohammed J. Zaki},
  doi          = {10.1109/TNNLS.2023.3264519},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12706-12717},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward subgraph-guided knowledge graph question generation with graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network-based robust guaranteed cost control for
image-based visual servoing of quadrotor. <em>TNNLS</em>,
<em>35</em>(9), 12693–12705. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a neural network (NN)-based robust guaranteed cost control design is proposed for image-based visual servoing (IBVS) control of quadrotors. According to the dynamics of three subsystems (yaw, height, and lateral subsystems) derived from the quadrotor IBVS dynamic model, the main control design is to solve the robust control problem for the time-varying lateral subsystem with angle constraints and uncertain disturbances. Considering the system dynamics, a two-loop structure is conducted. The outer loop uses the linear quadratic regulator to solve the Riccati equation for the lateral image feature system, and the inner loop adopts the optimal robust guaranteed cost control to solve the lateral velocity system. For the lateral velocity system, the optimal robust control problem is transformed to solve the modified Hamilton–Jacobi–Bellman equation of the corresponding optimal control problem utilizing adaptive dynamic programming. The implementation is accomplished with the time-varying NN and the designed estimated weight update law. In addition, the stability and effectiveness are proved by the theoretic proof and simulations.},
  archive      = {J_TNNLS},
  author       = {Xinning Yi and Biao Luo and Yuqian Zhao},
  doi          = {10.1109/TNNLS.2023.3264511},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12693-12705},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based robust guaranteed cost control for image-based visual servoing of quadrotor},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Multiexperience-assisted efficient multiagent reinforcement
learning. <em>TNNLS</em>, <em>35</em>(9), 12678–12692. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multiagent reinforcement learning (MARL) has shown great potential for learning cooperative policies in multiagent systems (MASs). However, a noticeable drawback of current MARL is the low sample efficiency, which causes a huge amount of interactions with environment. Such amount of interactions greatly hinders the real-world application of MARL. Fortunately, effectively incorporating experience knowledge can assist MARL to quickly find effective solutions, which can significantly alleviate the drawback. In this article, a novel multiexperience-assisted reinforcement learning (MEARL) method is proposed to improve the learning efficiency of MASs. Specifically, monotonicity-constrained reward shaping is innovatively designed using expert experience to provide additional individual rewards to guide multiagent learning efficiently, with the invariance guarantee of the team optimization objective. Furthermore, a reward distribution estimator is specially developed to model an implicated reward distribution of environment by using transition experience from environment, containing collected samples (state–action pair, reward, and next state). This estimator can predict the expectation reward of each agent for the taken action to accurately estimate the state value function and accelerate its convergence. Besides, the performance of MEARL is evaluated on two multiagent environment platforms: our designed unmanned aerial vehicle combat (UAV-C) and StarCraft II Micromanagement (SCII-M). Simulation results demonstrate that the proposed MEARL can greatly improve the learning efficiency and performance of MASs and is superior to the state-of-the-art methods in multiagent tasks.},
  archive      = {J_TNNLS},
  author       = {Tianle Zhang and Zhen Liu and Jianqiang Yi and Shiguang Wu and Zhiqiang Pu and Yanjie Zhao},
  doi          = {10.1109/TNNLS.2023.3264275},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12678-12692},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiexperience-assisted efficient multiagent reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing adversarial training by injecting booster signal.
<em>TNNLS</em>, <em>35</em>(9), 12665–12677. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarial attacks. To defend against adversarial attacks, many defense strategies have been proposed, among which adversarial training (AT) has been demonstrated to be the most effective strategy. However, it has been known that AT sometimes hurts natural accuracy. Then, many works focus on optimizing model parameters to handle the problem. Different from the previous approaches, in this article, we propose a new approach to improve the adversarial robustness using an external signal rather than model parameters. In the proposed method, a well-optimized universal external signal called a booster signal is injected into the outside of the image which does not overlap with the original content. Then, it boosts both adversarial robustness and natural accuracy. The booster signal is optimized in parallel to model parameters step by step collaboratively. Experimental results show that the booster signal can improve both the natural and robust accuracies over the recent state-of-the-art AT methods. Also, optimizing the booster signal is general and flexible enough to be adopted on any existing AT methods.},
  archive      = {J_TNNLS},
  author       = {Hong Joo Lee and Youngjoon Yu and Yong Man Ro},
  doi          = {10.1109/TNNLS.2023.3264256},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12665-12677},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Advancing adversarial training by injecting booster signal},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Moment-based reinforcement learning for ensemble control.
<em>TNNLS</em>, <em>35</em>(9), 12653–12664. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problems involving controlling the collective behavior of a population of structurally similar dynamical systems, the so-called ensemble control, arise in diverse emerging applications and pose a grand challenge in systems science and control engineering. Owing to the severely under-actuated nature and the difficulty of placing large-scale sensor networks, ensemble systems are limited to being actuated and monitored at the population level. Moreover, mathematical models describing the dynamics of ensemble systems are often elusive. Therefore, it is essential to design broadcast controls that excite the entire population in such a way that the heterogeneity in system dynamics is robustly compensated. In this article, we propose a reinforcement learning (RL)-based data-driven control framework incorporating population-level aggregated measurement data to learn a global control signal for steering a dynamic population in the desired manner. In particular, we introduce the notion of ensemble moments induced by aggregated measurements and derive the associated moment system to the original ensemble system. Then, using the moment system, we learn an approximation of optimal value functions and the associated policies in terms of ensemble moments through RL. We illustrate the feasibility and scalability of the proposed moment-based approach via numerical experiments using a population of linear, bilinear, and nonlinear dynamic ensemble systems. We report that the proposed method achieves the desired control objectives of various ensemble control tasks and obtains significantly better averaged-reward when compared with three existing methods.},
  archive      = {J_TNNLS},
  author       = {Yao-Chi Yu and Vignesh Narayanan and Jr-Shin Li},
  doi          = {10.1109/TNNLS.2023.3264151},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12653-12664},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Moment-based reinforcement learning for ensemble control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel swarm exploring varying parameter recurrent neural
network for solving non-convex nonlinear programming. <em>TNNLS</em>,
<em>35</em>(9), 12642–12652. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at solving non-convex nonlinear programming efficiently and accurately, a swarm exploring varying parameter recurrent neural network (SE-VPRNN) method is proposed in this article. First, the local optimal solutions are searched accurately by the proposed varying parameter recurrent neural network. After each network converges to the local optimal solutions, information is exchanged through a particle swarm optimization (PSO) framework to update the velocities and positions. The neural network searches for the local optimal solutions again from the updated position until all the neural networks are searched to the same local optimal solution. For improving the global searching ability, wavelet mutation is applied to increase the diversity of particles. Computer simulations show that the proposed method can solve the non-convex nonlinear programming effectively. Compared with three existing algorithms, the proposed method has advantages in accuracy and convergence time.},
  archive      = {J_TNNLS},
  author       = {Zhijun Zhang and Xiaohui Ren and Jilong Xie and Yamei Luo},
  doi          = {10.1109/TNNLS.2023.3263975},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12642-12652},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel swarm exploring varying parameter recurrent neural network for solving non-convex nonlinear programming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distilling privileged knowledge for anomalous event
detection from weakly labeled videos. <em>TNNLS</em>, <em>35</em>(9),
12627–12641. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised video anomaly detection (WS-VAD) aims to identify the snippets involving anomalous events in long untrimmed videos, with solely text video-level binary labels. A typical paradigm among the existing text WS-VAD methods is to employ multiple modalities as inputs, e.g., RGB, optical flow, and audio, as they can provide sufficient discriminative clues that are robust to the diverse, complicated real-world scenes. However, such a pipeline has high reliance on the availability of multiple modalities and is computationally expensive and storage demanding in processing long sequences, which limits its use in some applications. To address this dilemma, we propose a privileged knowledge distillation (KD) framework dedicated to the WS-VAD task, which can maintain the benefits of exploiting additional modalities, while avoiding the need for using multimodal data in the inference phase. We argue that the performance of the privileged KD framework mainly depends on two factors: 1) the effectiveness of the multimodal teacher network and 2) the completeness of the useful information transfer. To obtain a reliable teacher network, we propose a text cross-modal interactive learning strategy and an anomaly normal discrimination loss, which target learning task-specific cross-modal features and encourage the separability of anomalous and normal representations, respectively. Furthermore, we design both representation- and text logits-level distillation loss functions, which force the unimodal student network to distill abundant privileged knowledge from the text well-trained multimodal teacher network, in a snippet-to-video fashion. Extensive experimental results on three public benchmarks demonstrate that the proposed privileged KD framework can train a lightweight yet effective detector, for localizing anomaly events under the supervision of video-level annotations.},
  archive      = {J_TNNLS},
  author       = {Tianshan Liu and Kin-Man Lam and Jun Kong},
  doi          = {10.1109/TNNLS.2023.3263966},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12627-12641},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distilling privileged knowledge for anomalous event detection from weakly labeled videos},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). PDE-based boundary adaptive consensus control of multiagent
systems with input constraints. <em>TNNLS</em>, <em>35</em>(9),
12617–12626. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The leader–follower adaptive consensus control problem is addressed for partial differential equations (PDEs) multiagent systems (MASs), and these agents are composed of flexible manipulator systems with input nonlinearity, boundary uncertainties, and time-varying disturbances. Because of the spatial variables in the model, the design of adaptive protocols is more difficult than that of ordinary differential equation (ODE) MASs. By designing the Lyapunov function, a novel distributed boundary control (BC) protocol is constructed, which not only ensures the consensus of angular positions but also suppresses the boundary vibration of each agent. The hybrid effects of dead zones and input saturation on flexible manipulator systems are addressed using the approximation properties of neural networks (NNs). In addition, the disturbance adaptive laws are proposed to provide a control solution for bounded and time-varying disturbances. Furthermore, by applying the Lyapunov stability theory, the uniformly bounded stability of the multiflexible manipulator can be ensured. Finally, the feasibility of the presented control approach is verified using numerical examples.},
  archive      = {J_TNNLS},
  author       = {Wei Zhao and Yu Liu and Xiangqian Yao},
  doi          = {10.1109/TNNLS.2023.3263946},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12617-12626},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PDE-based boundary adaptive consensus control of multiagent systems with input constraints},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computation-efficient fault detection framework for
partially known nonlinear distributed parameter systems. <em>TNNLS</em>,
<em>35</em>(9), 12604–12616. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault detection for distributed parameter systems (DPSs) generally requires the complete model information to be known so far. However, for numerous industrial applications, it is common that accurate first-principles physical models are extremely difficult to obtain. Hence, the applicability of traditional model-based methods is being restricted. To pave the way, an adaptive neural network (AdNN) is constructed to simultaneously estimate the state variable and the unknown nonlinearity for a class of partially known nonlinear DPSs. Moreover, considering that full-state measurement is unrealistic in applications, the proposed adaptive neural observer is based on a reduced-order model, which also increases the computation efficiency. Then, the residual generation and evaluation are conducted using the output estimation error of the proposed adaptive neural observer. Bearing the effects of the neglected fast dynamics in mind, a data-driven threshold generation scheme is proposed. Extensive experimental results are presented and analyzed to validate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Yun Feng and Yaonan Wang and Yang Mo and Yiming Jiang and Zhijie Liu and Wei He and Han-Xiong Li},
  doi          = {10.1109/TNNLS.2023.3263840},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12604-12616},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Computation-efficient fault detection framework for partially known nonlinear distributed parameter systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural networks with high-order polynomial spectral
filters. <em>TNNLS</em>, <em>35</em>(9), 12590–12603. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General graph neural networks (GNNs) implement convolution operations on graphs based on polynomial spectral filters. Existing filters with high-order polynomial approximations can detect more structural information when reaching high-order neighborhoods but produce indistinguishable representations of nodes, which indicates their inefficiency of processing information in high-order neighborhoods, resulting in performance degradation. In this article, we theoretically identify the feasibility of avoiding this problem and attribute it to overfitting polynomial coefficients. To cope with it, the coefficients are restricted in two steps, dimensionality reduction of the coefficients’ domain and sequential assignment of the forgetting factor. We transform the optimization of coefficients to the tuning of a hyperparameter and propose a flexible spectral-domain graph filter, which significantly reduces the memory demand and the adverse impacts on message transmission under large receptive fields. Utilizing our filter, the performance of GNNs is improved significantly in large receptive fields and the receptive fields of GNNs are multiplied as well. Meanwhile, the superiority of applying a high-order approximation is verified across various datasets, notably in strongly hyperbolic datasets. Codes are publicly available at: https://github.com/cengzeyuan/TNNLS-FFKSF .},
  archive      = {J_TNNLS},
  author       = {Zeyuan Zeng and Qinke Peng and Xu Mou and Ying Wang and Ruimeng Li},
  doi          = {10.1109/TNNLS.2023.3263676},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12590-12603},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph neural networks with high-order polynomial spectral filters},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-based unsupervised feature selection for
interval-valued information system. <em>TNNLS</em>, <em>35</em>(9),
12576–12589. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection has become one of the hot research topics in the era of big data. At the same time, as an extension of single-valued data, interval-valued data with its inherent uncertainty tend to be more applicable than single-valued data in some fields for characterizing inaccurate and ambiguous information, such as medical test results and qualified product indicators. However, there are relatively few studies on unsupervised attribute reduction for interval-valued information systems (IVISs), and it remains to be studied how to effectively control the dramatic increase of time cost in feature selection of large sample datasets. For these reasons, we propose a feature selection method for IVISs based on graph theory. Then, the model complexity could be greatly reduced after we utilize the properties of the matrix power series to optimize the calculation of the original model. Our approach can be divided into two steps. The first is feature ranking with the principles of relevance and nonredundancy, and the second is selecting top-ranked attributes when the number of features to keep is fixed as a priori. In this article, experiments are performed on 14 public datasets and the corresponding seven comparative algorithms. The results of the experiments verify that our algorithm is effective and efficient for feature selection in IVISs.},
  archive      = {J_TNNLS},
  author       = {Weihua Xu and Man Huang and Zongying Jiang and Yuhua Qian},
  doi          = {10.1109/TNNLS.2023.3263684},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12576-12589},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-based unsupervised feature selection for interval-valued information system},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic-aware dynamic generation networks for few-shot
human–object interaction recognition. <em>TNNLS</em>, <em>35</em>(9),
12564–12575. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing human–object interaction (HOI) aims at inferring various relationships between actions and objects. Although great progress in HOI has been made, the long-tail problem and combinatorial explosion problem are still practical challenges. To this end, we formulate HOI as a few-shot task to tackle both challenges and design a novel dynamic generation method to address this task. The proposed approach is called semantic-aware dynamic generation networks (SADG-Nets). Specifically, SADG-Net first assigns semantic-aware task representations for different batches of data, which further generates dynamic parameters. It obtains the features that highlight intercategory discriminability and intracategory commonality adaptively. In addition, we also design a dual semantic-aware encoder module (DSAE-Module), that is, verb-aware and noun-aware branches, to yield both action and object prototypes of HOI for each task space, which generalizes to novel combinations by transferring similarities among interactions. Extensive experimental results on two benchmark datasets, that is, humans interacting with common objects (HICO)-FS and trento universal HOI (TUHOI)-FS, illustrate that our SADG-Net achieves superior performance over state-of-the-art approaches, which proves its impressive effectiveness on few-shot HOI recognition.},
  archive      = {J_TNNLS},
  author       = {Zhong Ji and Ping An and Xiyao Liu and Changxin Gao and Yanwei Pang and Ling Shao},
  doi          = {10.1109/TNNLS.2023.3263660},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12564-12575},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semantic-aware dynamic generation networks for few-shot Human–Object interaction recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modified RNN for solving comprehensive sylvester equation
with TDOA application. <em>TNNLS</em>, <em>35</em>(9), 12553–12563. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The augmented Sylvester equation, as a comprehensive equation, is of great significance and its special cases (e.g., Lyapunov equation, Sylvester equation, Stein equation) are frequently encountered in various fields. It is worth pointing out that the current research on simultaneously eliminating the lagging error and handling noises in the nonstationary complex-valued field is rather rare. Therefore, this article focuses on solving a nonstationary complex-valued augmented Sylvester equation (NCASE) in real time and proposes two modified recurrent neural network (RNN) models. The first proposed modified RNN model possesses gradient search and velocity compensation, termed as RNN-GV model. The superiority of the proposed RNN-GV model to traditional algorithms including the complex-valued gradient-based RNN (GRNN) model lies in completely eliminating the lagging error when employed in the nonstationary problem. The second model named complex-valued integration enhanced RNN-GV with the nonlinear acceleration (IERNN-GVN) model is proposed to adapt to a noisy environment and accelerate the convergence process. Besides, the convergence and robustness of these two proposed models are proved via theoretical analysis. Simulative results on an illustrative example and an application to the moving source localization coincide with the theoretical analysis and illustrate the excellent performance of the proposed models.},
  archive      = {J_TNNLS},
  author       = {Jingkun Yan and Long Jin and Xin Luo and Shuai Li},
  doi          = {10.1109/TNNLS.2023.3263565},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12553-12563},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modified RNN for solving comprehensive sylvester equation with TDOA application},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metafeature selection via multivariate sparse-group lasso
learning for automatic hyperparameter configuration recommendation.
<em>TNNLS</em>, <em>35</em>(9), 12540–12552. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of classification algorithms is mainly governed by the hyperparameter settings deployed in applications, and the search for desirable hyperparameter configurations usually is quite challenging due to the complexity of datasets. Metafeatures are a group of measures that characterize the underlying dataset from various aspects, and the corresponding recommendation algorithm fully relies on the appropriate selection of metafeatures. Metalearning (MtL), aiming to improve the learning algorithm itself, requires development in integrating features, models, and algorithm learning to accomplish its goal. In this article, we develop a multivariate sparse-group Lasso (SGLasso) model embedded with MtL capacity in recommending suitable configurations via learning. The main idea is to select the principal metafeatures by removing those redundant or irregular ones, promoting both efficiency and performance in the hyperparameter configuration recommendation. To be specific, we first extract the metafeatures and classification performance of a set of configurations from the collection of historical datasets, and then, a metaregression task is established through SGLasso to capture the main characteristics of the underlying relationship between metafeatures and historical performance. For a new dataset, the classification performance of configurations can be estimated through the selected metafeatures so that the configuration with the highest predictive performance in terms of the new dataset can be generated. Furthermore, a general MtL architecture combined with our model is developed. Extensive experiments are conducted on 136 UCI datasets, demonstrating the effectiveness of the proposed approach. The empirical results on the well-known SVM show that our model can effectively recommend suitable configurations and outperform the existing MtL-based methods and the well-known search-based algorithms, such as random search, Bayesian optimization, and Hyperband.},
  archive      = {J_TNNLS},
  author       = {Liping Deng and Wen-Sheng Chen and Mingqing Xiao},
  doi          = {10.1109/TNNLS.2023.3263506},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12540-12552},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Metafeature selection via multivariate sparse-group lasso learning for automatic hyperparameter configuration recommendation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised mixture learning for graph neural networks
with neighbor dependence. <em>TNNLS</em>, <em>35</em>(9), 12528–12539.
(<a href="https://doi.org/10.1109/TNNLS.2023.3263463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph neural network (GNN) is a powerful architecture for semi-supervised learning (SSL). However, the data-driven mode of GNNs raises some challenging problems. In particular, these models suffer from the limitations of incomplete attribute learning, insufficient structure capture, and the inability to distinguish between node attribute and graph structure, especially on label-scarce or attribute-missing data. In this article, we propose a novel framework, called graph coneighbor neural network (GCoNN), for node classification. It is composed of two modules: GCoNN $_{\Gamma }$ and GCoNN $_{\mathop {\Gamma }\limits ^{\circ }}$ . GCoNN $_{\Gamma }$ is trained to establish the fundamental prototype for attribute learning on labeled data, while GCoNN $_{ {\mathop {\Gamma }\limits ^{\circ }}}$ learns neighbor dependence on transductive data through pseudolabels generated by GCoNN $_{\Gamma }$ . Next, GCoNN $_{\Gamma }$ is retrained to improve integration of node attribute and neighbor structure through feedback from GCoNN $_{ {\mathop {\Gamma }\limits ^{\circ }}}$ . GCoNN tends to convergence iteratively using such an approach. From a theoretical perspective, we analyze this iteration process from a generalized expectation–maximization (GEM) framework perspective which optimizes an evidence lower bound (ELBO) by amortized variational inference. Empirical evidence demonstrates that the state-of-the-art performance of the proposed approach outperforms other methods. We also apply GCoNN to brain functional networks, the results of which reveal response features across the brain which are physiologically plausible with respect to known language and visual functions.},
  archive      = {J_TNNLS},
  author       = {Kai Liu and Hongbo Liu and Tao Wang and Guoqiang Hu and Tomas E. Ward and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2023.3263463},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12528-12539},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semi-supervised mixture learning for graph neural networks with neighbor dependence},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SALR: Sharpness-aware learning rate scheduler for improved
generalization. <em>TNNLS</em>, <em>35</em>(9), 12518–12527. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an effort to improve generalization in deep learning and automate the process of learning rate scheduling, we propose SALR: a sharpness-aware learning rate update technique designed to recover flat minimizers. Our method dynamically updates the learning rate of gradient-based optimizers based on the local sharpness of the loss function. This allows optimizers to automatically increase learning rates at sharp valleys to increase the chance of escaping them. We demonstrate the effectiveness of SALR when adopted by various algorithms over a broad range of networks. Our experiments indicate that SALR improves generalization, converges faster, and drives solutions to significantly flatter regions.},
  archive      = {J_TNNLS},
  author       = {Xubo Yue and Maher Nouiehed and Raed Al Kontar},
  doi          = {10.1109/TNNLS.2023.3263393},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12518-12527},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SALR: Sharpness-aware learning rate scheduler for improved generalization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). CoSeg: Cognitively inspired unsupervised generic event
segmentation. <em>TNNLS</em>, <em>35</em>(9), 12507–12517. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some cognitive research has discovered that humans accomplish event segmentation as a side effect of event anticipation. Inspired by this discovery, we propose a simple yet effective end-to-end self-supervised learning framework for event segmentation/boundary detection. Unlike the mainstream clustering-based methods, our framework exploits a transformer-based feature reconstruction scheme to detect event boundaries by reconstruction errors. This is consistent with the fact that humans spot new events by leveraging the deviation between their prediction and what is perceived. Thanks to their heterogeneity in semantics, the frames at boundaries are difficult to be reconstructed (generally with large reconstruction errors), which is favorable for event boundary detection. In addition, since the reconstruction occurs on the semantic feature level instead of the pixel level, we develop a temporal contrastive feature embedding (TCFE) module to learn the semantic visual representation for frame feature reconstruction (FFR). This procedure is like humans building up experiences with “long-term memory.” The goal of our work is to segment generic events rather than localize some specific ones. We focus on achieving accurate event boundaries. As a result, we adopt the F1 score (Precision/Recall) as our primary evaluation metric for a fair comparison with previous approaches. Meanwhile, we also calculate the conventional frame-based mean over frames (MoF) and intersection over union (IoU) metric. We thoroughly benchmark our work on four publicly available datasets and demonstrate much better results. The source code is available at https://github.com/wang3702/CoSeg .},
  archive      = {J_TNNLS},
  author       = {Xiao Wang and Jingen Liu and Tao Mei and Jiebo Luo},
  doi          = {10.1109/TNNLS.2023.3263387},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12507-12517},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CoSeg: Cognitively inspired unsupervised generic event segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MetaSeg: Content-aware meta-net for omni-supervised semantic
segmentation. <em>TNNLS</em>, <em>35</em>(9), 12494–12506. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy labels, inevitably existing in pseudo-segmentation labels generated from weak object-level annotations, severely hamper model optimization for semantic segmentation. Previous works often rely on massive handcrafted losses and carefully tuned hyperparameters to resist noise, suffering poor generalization capability and high model complexity. Inspired by recent advances in meta-learning, we argue that rather than struggling to tolerate noise hidden behind clean labels passively, a more feasible solution would be to find out the noisy regions actively, so as to simply ignore them during model optimization. With this in mind, this work presents a novel meta-learning-based semantic segmentation method, MetaSeg, that comprises a primary content-aware meta-net (CAM-Net) to serve as a noise indicator for an arbitrary segmentation model counterpart. Specifically, CAM-Net learns to generate pixel-wise weights to suppress noisy regions with incorrect pseudo-labels while highlighting clean ones by exploiting hybrid strengthened features from image content, providing straightforward and reliable guidance for optimizing the segmentation model. Moreover, to break the barrier of time-consuming training when applying meta-learning to common large segmentation models, we further present a new decoupled training strategy that optimizes different model layers in a divide-and-conquer manner. Extensive experiments on object, medical, remote sensing, and human segmentation show that our method achieves superior performance, approaching that of fully supervised settings, which paves a new promising way for omni-supervised semantic segmentation.},
  archive      = {J_TNNLS},
  author       = {Shenwang Jiang and Jianan Li and Ying Wang and Wenxuan Wu and Jizhou Zhang and Bo Huang and Tingfa Xu},
  doi          = {10.1109/TNNLS.2023.3263335},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12494-12506},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MetaSeg: Content-aware meta-net for omni-supervised semantic segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dini-derivative-aided zeroing neural network for
time-variant quadratic programming involving multi-type constraints with
robotic applications. <em>TNNLS</em>, <em>35</em>(9), 12482–12493. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-variant quadratic programming (QP) with multi-type constraints including equality, inequality, and bound constraints is ubiquitous in practice. In the literature, there exist a few zeroing neural networks (ZNNs) that are applicable to time-variant QPs with multi-type constraints. These ZNN solvers involve continuous and differentiable elements for handling inequality and/or bound constraints, and they possess their own drawbacks such as the failure in solving problems, the approximated optimal solutions, and the boring and sometimes difficult process of tuning parameters. Differing from the existing ZNN solvers, this article aims to propose a novel ZNN solver for time-variant QPs with multi-type constraints based on a continuous but not differentiable projection operator that is deemed unsuitable for designing ZNN solvers in the community, due to the lack of the required time derivative information. To achieve the aforementioned aim, the upper right-hand Dini derivative of the projection operator with respect to its input is introduced to serve as a mode switcher, leading to a novel ZNN solver, termed Dini-derivative-aided ZNN (Dini-ZNN). In theory, the convergent optimal solution of the Dini-ZNN solver is rigorously analyzed and proved. Comparative validations are performed, verifying the effectiveness of the Dini-ZNN solver that has merits such as guaranteed capability to solve problems, high solution accuracy, and no extra hyperparameter to be tuned. To illustrate potential applications, the Dini-ZNN solver is successfully applied to kinematic control of a joint-constrained robot with simulation and experimentation conducted.},
  archive      = {J_TNNLS},
  author       = {Weibing Li and Yongping Pan},
  doi          = {10.1109/TNNLS.2023.3263263},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12482-12493},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A dini-derivative-aided zeroing neural network for time-variant quadratic programming involving multi-type constraints with robotic applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bit reduction for locality-sensitive hashing.
<em>TNNLS</em>, <em>35</em>(9), 12470–12481. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locality-sensitive hashing (LSH) has gained ever-increasing popularity in similarity search for large-scale data. It has competitive search performance when the number of generated hash bits is large, reversely bringing adverse dilemmas for its wide applications. The first purpose of this work is to introduce a novel hash bit reduction schema for hashing techniques to derive shorter binary codes, which has not yet received sufficient concerns. To briefly show how the reduction schema works, the second purpose is to present an effective bit reduction method for LSH under the reduction schema. Specifically, after the hash bits are generated by LSH, they will be put into bit pool as candidates. Then mutual information and data labels are exploited to measure the correlation and structural properties between the hash bits, respectively. Eventually, highly correlated and redundant hash bits can be distinguished and then removed accordingly, without deteriorating the performance greatly. The advantages of our reduction method include that it can not only reduce the number of hash bits effectively but also boost retrieval performance of LSH, making it more appealing and practical in real-world applications. Comprehensive experiments were conducted on three public real-world datasets. The experimental results with representative bit selection methods and the state-of-the-art hashing algorithms demonstrate that the proposed method has encouraging and competitive performance.},
  archive      = {J_TNNLS},
  author       = {Huawen Liu and Wenhua Zhou and Hong Zhang and Gang Li and Shichao Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3263195},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12470-12481},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bit reduction for locality-sensitive hashing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Marginalized augmented few-shot domain adaptation.
<em>TNNLS</em>, <em>35</em>(9), 12459–12469. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) has recently drawn a lot of attention, as it facilitates unlabeled target learning by borrowing knowledge from an external source domain. Most existing DA solutions seek to align feature representations between the labeled source and unlabeled target data. However, the scarcity of target data easily results in negative transfer, as it misleads the cross DA to the dominance of the source. To address the challenging few-shot domain adaptation (FSDA) problem, in this article, we propose a novel marginalized augmented FSDA (MAF) approach to address the cross-domain distribution disparity and insufficiency of target data simultaneously. On the one hand, cross-domain continuity augmentation (CCA) synthesizes abundant intermediate patterns across domains leading to a continuous domain-invariant latent space. On the other hand, sufficient source-supervised semantic augmentation (SSA) is explored to progressively diversify the conditional distribution within and across domains. Moreover, the proposed augmentation strategies are implemented efficiently via an expected transferable cross-entropy (CE) loss over the augmented distribution instead of explicit data synthesis, and minimizing the upper bound of the expected loss introduces negligible extra computing cost. Experimentally, our method outperforms the state of the art in various FSDA benchmarks, which demonstrates the effectiveness and contribution of our work. Our source code is provided at https://github.com/scottjingtt/MAF.git .},
  archive      = {J_TNNLS},
  author       = {Taotao Jing and Haifeng Xia and Jihun Hamm and Zhengming Ding},
  doi          = {10.1109/TNNLS.2023.3263176},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12459-12469},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Marginalized augmented few-shot domain adaptation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive modular neural control for online gait
synchronization and adaptation of an assistive lower-limb exoskeleton.
<em>TNNLS</em>, <em>35</em>(9), 12449–12458. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait synchronization has attracted significant attention in research on assistive lower-limb exoskeletons because it can circumvent conflicting movements and improve the assistance performance. This study proposes an adaptive modular neural control (AMNC) for online gait synchronization and the adaptation of a lower-limb exoskeleton. The AMNC comprises several distributed and interpretable neural modules that interact with each other to effectively exploit neural dynamics and adopt feedback signals to quickly reduce the tracking error, thereby smoothly synchronizing the exoskeleton movement with the user’s movement on the fly. Taking state-of-the-art control as the benchmark, the proposed AMNC provides further improvements in the locomotion phase, frequency, and shape adaptation. Accordingly, under the physical interaction between the user and the exoskeleton, the control can reduce the optimized tracking error and unseen interaction torque by up to 80% and 30%, respectively. Accordingly, this study contributes to the advancement of exoskeleton and wearable robotics research in gait assistance for the next generation of personalized healthcare.},
  archive      = {J_TNNLS},
  author       = {Arthicha Srisuchinnawong and Chaicharn Akkawutvanich and Poramate Manoonpong},
  doi          = {10.1109/TNNLS.2023.3263044},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12449-12458},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive modular neural control for online gait synchronization and adaptation of an assistive lower-limb exoskeleton},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CapsRule: Explainable deep learning for classifying network
attacks. <em>TNNLS</em>, <em>35</em>(9), 12434–12448. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the potential deep learning (DL) algorithms have shown, their lack of transparency hinders their widespread application. Extracting if-then rules from deep neural networks is a powerful explanation method to capture nonlinear local behaviors. However, existing rule extraction methods suffer from inefficiency, incomprehensibility, infidelity, and not scaling well. Concerning security applications, they are not optimized regarding the decision boundary, data types and ranges, classification tasks, and dataset size. In this article, we propose CapsRule, an effective and efficient rule-based DL explanation method dedicated to classifying network attacks. It extracts high-fidelity rules from the feed-forward capsule network that explains how an input sample is classified. Using precomputed coupling coefficients, the training phase overlaps the rule extraction process to increase efficiency. The activation vector of a capsule can represent semantic intelligence about the attributes of the input sample. The rules extracted from CapsRule address the major concerns of network attack detection. The rules: 1) approximate the nonlinear decision boundary of the underlying data; 2) reduce the number of false positives significantly; 3) increase transparency; and 4) help find errors and noise in the data. We evaluate CapsRule on the CICDDoS2019 dataset that contains over a million of the most advanced Distributed Denial-of-Service (DDoS) attacks. The extensive evaluation shows that it generates accurate, high-fidelity, and comprehensible rules. CapsRule achieves an average accuracy of 99.0% and a false positive rate of 0.70% for reflection- and exploitation-based attacks. We verify that the learned features from the rulesets match our domain-specific knowledge. They also help find flaws in the dataset generation process and erroneous patterns caused by attack simulators.},
  archive      = {J_TNNLS},
  author       = {Samaneh Mahdavifar and Ali A. Ghorbani},
  doi          = {10.1109/TNNLS.2023.3262981},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12434-12448},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CapsRule: Explainable deep learning for classifying network attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TAG: Teacher-advice mechanism with gaussian process for
reinforcement learning. <em>TNNLS</em>, <em>35</em>(9), 12419–12433. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) still suffers from the problem of sample inefficiency and struggles with the exploration issue, particularly in situations with long-delayed rewards, sparse rewards, and deep local optimum. Recently, learning from demonstration (LfD) paradigm was proposed to tackle this problem. However, these methods usually require a large number of demonstrations. In this study, we present a sample efficient teacher-advice mechanism with Gaussian process (TAG) by leveraging a few expert demonstrations. In TAG, a teacher model is built to provide both an advice action and its associated confidence value. Then, a guided policy is formulated to guide the agent in the exploration phase via the defined criteria. Through the TAG mechanism, the agent is capable of exploring the environment more intentionally. Moreover, with the confidence value, the guided policy can guide the agent precisely. Also, due to the strong generalization ability of Gaussian process, the teacher model can utilize the demonstrations more effectively. Therefore, substantial improvement in performance and sample efficiency can be attained. Considerable experiments on sparse reward environments demonstrate that the TAG mechanism can help typical RL algorithms achieve significant performance gains. In addition, the TAG mechanism with soft actor-critic algorithm (TAG-SAC) attains the state-of-the-art performance over other LfD counterparts on several delayed reward and complicated continuous control environments.},
  archive      = {J_TNNLS},
  author       = {Ke Lin and Duantengchuan Li and Yanjie Li and Shiyu Chen and Qi Liu and Jianqi Gao and Yanrui Jin and Liang Gong},
  doi          = {10.1109/TNNLS.2023.3262956},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12419-12433},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TAG: Teacher-advice mechanism with gaussian process for reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preassigned time adaptive neural tracking control for
stochastic nonlinear multiagent systems with deferred constraints.
<em>TNNLS</em>, <em>35</em>(9), 12409–12418. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a preassigned time adaptive tracking control problem for stochastic multiagent systems (MASs) with deferred full state constraints and deferred prescribed performance. A modified nonlinear mapping is designed, which incorporates a class of shift functions, to eliminate the constraints on the initial value conditions. By virtue of this nonlinear mapping, the feasibility conditions of the full state constraints for stochastic MASs can also be circumvented. In addition, the Lyapunov function codesigned by the shift function and the fixed-time prescribed performance function is constructed. The unknown nonlinear terms of the converted systems are handled based on the approximation property of the neural networks. Furthermore, a preassigned time adaptive tracking controller is established, which can achieve deferred prescribed performance for stochastic MASs that provide only local information. Finally, a numerical example is given to demonstrate the effectiveness of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Xiyue Guo and Huaguang Zhang and Jiayue Sun and Yu Zhou},
  doi          = {10.1109/TNNLS.2023.3262799},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12409-12418},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Preassigned time adaptive neural tracking control for stochastic nonlinear multiagent systems with deferred constraints},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite-time neural network-based hierarchical sliding mode
antiswing control for underactuated dual ship-mounted cranes with
unmatched sea wave disturbances suppression. <em>TNNLS</em>,
<em>35</em>(9), 12396–12408. (<a
href="https://doi.org/10.1109/TNNLS.2023.3257508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As typical mechanical transportation equipment, cooperative dual ship-mounted cranes are widely used to transport large goods or containers in the marine environment. However, the control problem of the dual ship-mounted crane system is much more complex due to its underactuated characteristic and persistent unmatched disturbances. To solve these problems, we propose a novel neural network (NN)-based hierarchical sliding mode adaptive (HSMA) control method in this article. More specifically, an appropriate hierarchical sliding mode surface is first designed to connect the actuated and underactuated system state variables effectively. At the same time, the NNs are constructed to compensate for the unmatched interference of ship motions induced by sea waves simultaneously. Not only can the booms and the rope lengths reach their desired positions in finite time, but also the synchronous swing angles of the payload can be effectively eliminated. The asymptotic convergence of the closed-loop system’s equilibrium points is achieved through rigorous mathematical proofs. Furthermore, the stability of each sliding mode surface is also analyzed utilizing the Lyapunov technique and Barbalat’s lemma. Finally, numerous groups of compared numerical simulation results are investigated to further show the effectiveness and strong robustness of the proposed NN-based HSMA controller.},
  archive      = {J_TNNLS},
  author       = {Yuzhe Qian and Haibo Zhang and Die Hu},
  doi          = {10.1109/TNNLS.2023.3257508},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12396-12408},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time neural network-based hierarchical sliding mode antiswing control for underactuated dual ship-mounted cranes with unmatched sea wave disturbances suppression},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic graph representation learning via coupling-process
model. <em>TNNLS</em>, <em>35</em>(9), 12383–12395. (<a
href="https://doi.org/10.1109/TNNLS.2023.3257488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning based on dynamic graphs has received a lot of attention in recent years due to its wide range of application scenarios. Although many discrete or continuous dynamic graph representation learning methods have been proposed, many of them ignore the role of edge types. Through the observation of dynamic graphs in the real world, it is found that the types of various edges are very different in nature. They are roughly divided into two categories according to the frequency of occurrence: evolutive edges that appear infrequently and interactive edges that appear frequently. For both types of edges, we propose a coupling-process model (DyCPM) to capture the dynamic mechanisms of them. The model not only generates low-dimensional embedding vectors of nodes, but also aggregates the structural information and temporal information of two kinds of edges. In particular, we design a neural network parameterized discrete process to depict the change law of the topology of evolutive edges and a neural network parameterized temporal point process (TPP) to characterize the temporal dynamic rule of interactive edges. More importantly, we propose a coupling mechanism to transfer the information of the two processes through a shared embedding matrix and finally generate an embedding matrix that aggregates the topology information and temporal information of the two kinds of edges for the dynamic link prediction task. We evaluate our model and several baselines on real datasets. The experimental results show that our model can better aggregate the topology information and temporal information of the two kinds of edges according to their properties and outperforms several state-of-the-art baselines in the performance of dynamic link prediction.},
  archive      = {J_TNNLS},
  author       = {Pingtao Duan and Chuan Zhou and Yuting Liu},
  doi          = {10.1109/TNNLS.2023.3257488},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12383-12395},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic graph representation learning via coupling-process model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective active learning method for spiking neural
networks. <em>TNNLS</em>, <em>35</em>(9), 12373–12382. (<a
href="https://doi.org/10.1109/TNNLS.2023.3257333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large quantity of labeled data is required to train high-performance deep spiking neural networks (SNNs), but obtaining labeled data is expensive. Active learning is proposed to reduce the quantity of labeled data required by deep learning models. However, conventional active learning methods in SNNs are not as effective as that in conventional artificial neural networks (ANNs) because of the difference in feature representation and information transmission. To address this issue, we propose an effective active learning method for a deep SNN model in this article. Specifically, a loss prediction module ActiveLossNet is proposed to extract features and select valuable samples for deep SNNs. Then, we derive the corresponding active learning algorithm for deep SNN models. Comprehensive experiments are conducted on CIFAR-10, MNIST, Fashion-MNIST, and SVHN on different SNN frameworks, including seven-layer CIFARNet and 20-layer ResNet-18. The comparison results demonstrate that the proposed active learning algorithm outperforms random selection and conventional ANN active learning methods. In addition, our method converges faster than conventional active learning methods.},
  archive      = {J_TNNLS},
  author       = {Xiurui Xie and Bei Yu and Guisong Liu and Qiugang Zhan and Huajin Tang},
  doi          = {10.1109/TNNLS.2023.3257333},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12373-12382},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effective active learning method for spiking neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Homophily-enhanced self-supervision for graph structure
learning: Insights and directions. <em>TNNLS</em>, <em>35</em>(9),
12358–12372. (<a
href="https://doi.org/10.1109/TNNLS.2023.3257325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have recently achieved remarkable success on a variety of graph-related tasks, while such success relies heavily on a given graph structure that may not always be available in real-world applications. To address this problem, graph structure learning (GSL) is emerging as a promising research topic where task-specific graph structure and GNN parameters are jointly learned in an end-to-end unified framework. Despite their great progress, existing approaches mostly focus on the design of similarity metrics or graph construction, but directly default to adopting downstream objectives as supervision, which lacks deep insight into the power of supervision signals. More importantly, these approaches struggle to explain how GSL helps GNNs, and when and why this help fails. In this article, we conduct a systematic experimental evaluation to reveal that GSL and GNNs enjoy consistent optimization goals in terms of improving the graph homophily. Furthermore, we demonstrate theoretically and experimentally that task-specific downstream supervision may be insufficient to support the learning of both graph structure and GNN parameters, especially when the labeled data are extremely limited. Therefore, as a complement to downstream supervision, we propose homophily-enhanced self-supervision for GSL (HES-GSL), a method that provides more supervision for learning an underlying graph structure. A comprehensive experimental study demonstrates that HES-GSL scales well to various datasets and outperforms other leading methods. Our code will be available in https://github.com/LirongWu/Homophily-Enhanced-Self-supervision .},
  archive      = {J_TNNLS},
  author       = {Lirong Wu and Haitao Lin and Zihan Liu and Zicheng Liu and Yufei Huang and Stan Z. Li},
  doi          = {10.1109/TNNLS.2023.3257325},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12358-12372},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Homophily-enhanced self-supervision for graph structure learning: Insights and directions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Modal-regression-based broad learning system for robust
regression and classification. <em>TNNLS</em>, <em>35</em>(9),
12344–12357. (<a
href="https://doi.org/10.1109/TNNLS.2023.3256999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel neural network, namely, broad learning system (BLS), has shown impressive performance on various regression and classification tasks. Nevertheless, most BLS models may suffer serious performance degradation for contaminated data, since they are derived under the least-squares criterion which is sensitive to noise and outliers. To enhance the model robustness, in this article we proposed a modal-regression-based BLS (MRBLS) to tackle the regression and classification tasks of data corrupted by noise and outliers. Specifically, modal regression is adopted to train the output weights instead of the minimum mean square error (MMSE) criterion. Moreover, the $\ell _{2,1}$ -norm-induced constraint is used to encourage row sparsity of the connection weight matrix and achieve feature selection. To effectively and efficiently train the network, the half-quadratic theory is used to optimize MRBLS. The validity and robustness of the proposed method are verified on various regression and classification datasets. The experimental results demonstrate that the proposed MRBLS achieves better performance than the existing state-of-the-art BLS methods in terms of both accuracy and robustness.},
  archive      = {J_TNNLS},
  author       = {Licheng Liu and Tingyun Liu and C. L. Philip Chen and Yaonan Wang},
  doi          = {10.1109/TNNLS.2023.3256999},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12344-12357},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modal-regression-based broad learning system for robust regression and classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New results on rapid dynamical pattern recognition via
deterministic learning from sampling sequences. <em>TNNLS</em>,
<em>35</em>(9), 12330–12343. (<a
href="https://doi.org/10.1109/TNNLS.2023.3256464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid dynamical pattern recognition based on the deterministic learning method (DLM-based RDPR) aims to rapidly recognize the most similar dynamical pattern pair from perspectives of differences in inherent system dynamics. The basic mechanism is to use available recognition errors to reflect the differences in the dynamics of dynamical pattern pairs and then to make a decision based on a minimal recognition error (MRE) principle. This article focuses on providing a rigorous theoretical analysis of the MRE principle in DLM-based RDPR under the sampled-data framework. Specifically, we seek a unified methodology from the similarity definition to the measure implementation and then to derive general sufficient conditions and necessary conditions for the MRE principle. The main idea is to: 1) from the average signal energy aspect, define a time-dependent dynamics-based similarity in dynamical pattern pairs and reestablish the measure of recognition errors generated from the DLM-based RDPR; 2) introduce the energy-based Lyapunov method to establish the interrelation between the dynamical distance and the recognition error; and 3) derive sufficient conditions and necessary conditions from two directions of the interrelation. The proposed conditions distinguish themselves from virtually all of the existing DLM-based RDPR works with only sufficient conditions in the sense that it is shown in a rigorous analysis that under what conditions, the pattern pair recognized based on the MRE principle is indeed the most similar one. Therefore, the proposed work makes the DLM-based RDPR possess good interpretability and provides strong theoretical guidance in engineering applications.},
  archive      = {J_TNNLS},
  author       = {Weiming Wu and Jingtao Hu and Fukai Zhang and Cong Wang},
  doi          = {10.1109/TNNLS.2023.3256464},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12330-12343},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {New results on rapid dynamical pattern recognition via deterministic learning from sampling sequences},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distribution matching for machine teaching. <em>TNNLS</em>,
<em>35</em>(9), 12316–12329. (<a
href="https://doi.org/10.1109/TNNLS.2023.3256412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine teaching is an inverse problem of machine learning that aims at steering the student toward its target hypothesis, in which the teacher has already known the student’s learning parameters. Previous studies on machine teaching focused on balancing the teaching risk and cost to find the best teaching examples deriving from the student model. This optimization solver is in general ineffective when the student does not disclose any cue of the learning parameters. To supervise such a teaching scenario, this article presents a distribution matching-based machine teaching strategy via iteratively shrinking the teaching cost in a smooth surrogate, which eliminates boundary perturbations from the version space. Technically, our strategy could be redefined as a cost-controlled optimization process that finds the optimal teaching examples without further exploring the parameter distribution of the student. Then, given any limited teaching cost, the training examples would have a closed-form expression. Theoretical analysis and experiment results demonstrate the effectiveness of this strategy.},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Cao and Ivor W. Tsang},
  doi          = {10.1109/TNNLS.2023.3256412},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12316-12329},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distribution matching for machine teaching},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Online multi-view learning with knowledge registration
units. <em>TNNLS</em>, <em>35</em>(9), 12301–12315. (<a
href="https://doi.org/10.1109/TNNLS.2023.3256390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate online multi-view learning according to the multi-view complementarity and consistency principles to memorably process online multi-view data when fused across views. Online diverse features through different deep feature extractors under different views are used as input to an online learning method to privately and memorably optimize in each view for the discovery and memorization of the view-specific information. More specifically, according to the multi-view complementarity principle, a softmax-weighted reducible (SWR) loss is proposed to selectively retain credible views and neglect incredible ones for the online model’s cross-view complementarity fusion. According to the multi-view consistency principle, we design a cross-view embedding consistency (CVEC) loss and a cross-view Kullback–Leibler (CVKL) divergence loss to maintain the cross-view consistency of the online model. Since the online multi-view learning setup needs to avoid repeatedly accessing online data to handle the knowledge forgetting in each view, we propose a knowledge registration unit (KRU) based on dictionary learning to incrementally register newly view-specific knowledge of online unlabeled data to the learnable and adjustable dictionary. Finally, by using the above strategies, we propose an online multi-view KRU approach and evaluate it with comprehensive experiments, thereby showing its superiority in online multi-view learning.},
  archive      = {J_TNNLS},
  author       = {Sheng Wu and Ancong Wu and Wei-Shi Zheng},
  doi          = {10.1109/TNNLS.2023.3256390},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12301-12315},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online multi-view learning with knowledge registration units},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving deep neural networks’ training for image
classification with nonlinear conjugate gradient-style adaptive
momentum. <em>TNNLS</em>, <em>35</em>(9), 12288–12300. (<a
href="https://doi.org/10.1109/TNNLS.2023.3255783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Momentum is crucial in stochastic gradient-based optimization algorithms for accelerating or improving training deep neural networks (DNNs). In deep learning practice, the momentum is usually weighted by a well-calibrated constant. However, tuning the hyperparameter for momentum can be a significant computational burden. In this article, we propose a novel adaptive momentum for improving DNNs training; this adaptive momentum, with no momentum-related hyperparameter required, is motivated by the nonlinear conjugate gradient (NCG) method. Stochastic gradient descent (SGD) with this new adaptive momentum eliminates the need for the momentum hyperparameter calibration, allows using a significantly larger learning rate, accelerates DNN training, and improves the final accuracy and robustness of the trained DNNs. For instance, SGD with this adaptive momentum reduces classification errors for training ResNet110 for CIFAR10 and CIFAR100 from 5.25% to 4.64% and 23.75% to 20.03%, respectively. Furthermore, SGD, with the new adaptive momentum, also benefits adversarial training and, hence, improves the adversarial robustness of the trained DNNs.},
  archive      = {J_TNNLS},
  author       = {Bao Wang and Qiang Ye},
  doi          = {10.1109/TNNLS.2023.3255783},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12288-12300},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving deep neural networks’ training for image classification with nonlinear conjugate gradient-style adaptive momentum},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical weight averaging for deep neural networks.
<em>TNNLS</em>, <em>35</em>(9), 12276–12287. (<a
href="https://doi.org/10.1109/TNNLS.2023.3255540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite simplicity, stochastic gradient descent (SGD)-like algorithms are successful in training deep neural networks (DNNs). Among various attempts to improve SGD, weight averaging (WA), which averages the weights of multiple models, has recently received much attention in the literature. Broadly, WA falls into two categories: 1) online WA, which averages the weights of multiple models trained in parallel, is designed for reducing the gradient communication overhead of parallel mini-batch SGD and 2) offline WA, which averages the weights of one model at different checkpoints, is typically used to improve the generalization ability of DNNs. Though online and offline WA are similar in form, they are seldom associated with each other. Besides, these methods typically perform either offline parameter averaging or online parameter averaging, but not both. In this work, we first attempt to incorporate online and offline WA into a general training framework termed hierarchical WA (HWA). By leveraging both the online and offline averaging manners, HWA is able to achieve both faster convergence speed and superior generalization performance without any fancy learning rate adjustment. Besides, we also analyze the issues faced by the existing WA methods, and how our HWA addresses them, empirically. Finally, extensive experiments verify that HWA outperforms the state-of-the-art methods significantly.},
  archive      = {J_TNNLS},
  author       = {Xiaozhe Gu and Zixun Zhang and Yuncheng Jiang and Tao Luo and Ruimao Zhang and Shuguang Cui and Zhen Li},
  doi          = {10.1109/TNNLS.2023.3255540},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12276-12287},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical weight averaging for deep neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). An FPGA-implemented antinoise fuzzy recurrent neural
network for motion planning of redundant robot manipulators.
<em>TNNLS</em>, <em>35</em>(9), 12263–12275. (<a
href="https://doi.org/10.1109/TNNLS.2023.3253801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a robot completes end-effector tasks, internal error noises always exist. To resist internal error noises of robots, a novel fuzzy recurrent neural network (FRNN) is proposed, designed, and implemented on field-programmable gated array (FPGA). The implementation is pipeline-based, which guarantees the order of overall operations. The data processing is based on across-clock domain, which is beneficial for computing units’ acceleration. Compared with traditional gradient-based neural networks (NNs) and zeroing neural networks (ZNNs), the proposed FRNN has faster convergence rate and higher correctness. Practical experiments on a 3 degree-of-freedom (DOs) planar robot manipulator show that the proposed fuzzy RNN coprocessor needs 496 lookup table random access memories (LUTRAMs), 205.5 block random access memories (BRAMs), 41384 lookup tables (LUTs), and 16743 flip-flops (FFs) of the Xilinx XCZU9EG chip.},
  archive      = {J_TNNLS},
  author       = {Zhijun Zhang and Haotian He and Xianzhi Deng},
  doi          = {10.1109/TNNLS.2023.3253801},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12263-12275},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An FPGA-implemented antinoise fuzzy recurrent neural network for motion planning of redundant robot manipulators},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring separable attention for multi-contrast MR image
super-resolution. <em>TNNLS</em>, <em>35</em>(9), 12251–12262. (<a
href="https://doi.org/10.1109/TNNLS.2023.3253557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolving the magnetic resonance (MR) image of a target contrast under the guidance of the corresponding auxiliary contrast, which provides additional anatomical information, is a new and effective solution for fast MR imaging. However, current multi-contrast super-resolution (SR) methods tend to concatenate different contrasts directly, ignoring their relationships in different clues, e.g., in the high- and low-intensity regions. In this study, we propose a separable attention network (comprising high-intensity priority (HP) attention and low-intensity separation (LS) attention), named SANet. Our SANet could explore the areas of high- and low-intensity regions in the “forward” and “reverse” directions with the help of the auxiliary contrast while learning clearer anatomical structure and edge information for the SR of a target-contrast MR image. SANet provides three appealing benefits: First, it is the first model to explore a separable attention mechanism that uses the auxiliary contrast to predict the high- and low-intensity regions, diverting more attention to refining any uncertain details between these regions and correcting the fine areas in the reconstructed results. Second, a multistage integration module is proposed to learn the response of multi-contrast fusion at multiple stages, get the dependency between the fused representations, and boost their representation ability. Third, extensive experiments with various state-of-the-art multi-contrast SR methods on fastMRI and clinical in vivo datasets demonstrate the superiority of our model. The code is released at https://github.com/chunmeifeng/SANet .},
  archive      = {J_TNNLS},
  author       = {Chun-Mei Feng and Yunlu Yan and Kai Yu and Yong Xu and Huazhu Fu and Jian Yang and Ling Shao},
  doi          = {10.1109/TNNLS.2023.3253557},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12251-12262},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploring separable attention for multi-contrast MR image super-resolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep attentional guided image filtering. <em>TNNLS</em>,
<em>35</em>(9), 12236–12250. (<a
href="https://doi.org/10.1109/TNNLS.2023.3253472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided filter is a fundamental tool in computer vision and computer graphics, which aims to transfer structure information from the guide image to the target image. Most existing methods construct filter kernels from the guidance itself without considering the mutual dependency between the guidance and the target. However, since there typically exist significantly different edges in two images, simply transferring all structural information from the guide to the target would result in various artifacts. To cope with this problem, we propose an effective framework named deep attentional guided image filtering, the filtering process of which can fully integrate the complementary information contained in both images. Specifically, we propose an attentional kernel learning module to generate dual sets of filter kernels from the guidance and the target and then adaptively combine them by modeling the pixelwise dependency between the two images. Meanwhile, we propose a multiscale guided image filtering module to progressively generate the filtering result with the constructed kernels in a coarse-to-fine manner. Correspondingly, a multiscale fusion strategy is introduced to reuse the intermediate results in the coarse-to-fine process. Extensive experiments show that the proposed framework compares favorably with the state-of-the-art methods in a wide range of guided image filtering applications, such as guided super-resolution (SR), cross-modality restoration, and semantic segmentation. Moreover, our scheme achieved the first place in the real depth map SR challenge held in ACM ICMR 2021. The codes can be found at https://github.com/zhwzhong/DAGF .},
  archive      = {J_TNNLS},
  author       = {Zhiwei Zhong and Xianming Liu and Junjun Jiang and Debin Zhao and Xiangyang Ji},
  doi          = {10.1109/TNNLS.2023.3253472},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12236-12250},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep attentional guided image filtering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and effective one-step multiview clustering.
<em>TNNLS</em>, <em>35</em>(9), 12224–12235. (<a
href="https://doi.org/10.1109/TNNLS.2023.3253246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering algorithms have attracted intensive attention and achieved superior performance in various fields recently. Despite the great success of multiview clustering methods in realistic applications, we observe that most of them are difficult to apply to large-scale datasets due to their cubic complexity. Moreover, they usually use a two-stage scheme to obtain the discrete clustering labels, which inevitably causes a suboptimal solution. In light of this, an efficient and effective one-step multiview clustering (E2OMVC) method is proposed to directly obtain clustering indicators with a small-time burden. Specifically, according to the anchor graphs, the smaller similarity graph of each view is constructed, from which the low-dimensional latent features are generated to form the latent partition representation. By introducing a label discretization mechanism, the binary indicator matrix can be directly obtained from the unified partition representation which is formed by fusing all latent partition representations from different views. In addition, by coupling the fusion of all latent information and the clustering task into a joint framework, the two processes can help each other and obtain a better clustering result. Extensive experimental results demonstrate that the proposed method can achieve comparable or better performance than the state-of-the-art methods. The demo code of this work is publicly available at https://github.com/WangJun2023/EEOMVC .},
  archive      = {J_TNNLS},
  author       = {Jun Wang and Chang Tang and Zhiguo Wan and Wei Zhang and Kun Sun and Albert Y. Zomaya},
  doi          = {10.1109/TNNLS.2023.3253246},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12224-12235},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient and effective one-step multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed adaptive output feedback consensus for nonlinear
stochastic multiagent systems by reference generator approach.
<em>TNNLS</em>, <em>35</em>(9), 12211–12223. (<a
href="https://doi.org/10.1109/TNNLS.2023.3253080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the distributed leader-following consensus for a class of nonlinear stochastic multiagent systems (MASs) under directed communication topology. In order to estimate unmeasured system states, a dynamic gain filter is designed for each control input with reduced filtering variables. Then, a novel reference generator is proposed, which plays a key role in relaxing the restriction on communication topology. Based on the reference generators and filters, a distributed output feedback consensus protocol is proposed by a recursive control design approach, which incorporates adaptive radial basis function (RBF) neural networks to approximate the unknown parameters and functions. Compared with existing works on stochastic MASs, the proposed approach can significantly reduce the number of dynamic variables in filters. Furthermore, the agents considered in this article are quite general with multiple uncertain/unmatched inputs and stochastic disturbance. Finally, a simulation example is given to demonstrate the effectiveness of our results.},
  archive      = {J_TNNLS},
  author       = {Guopin Liu and Ju H. Park and Changchun Hua and Hongshuang Xu and Yafeng Li},
  doi          = {10.1109/TNNLS.2023.3253080},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12211-12223},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed adaptive output feedback consensus for nonlinear stochastic multiagent systems by reference generator approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pivotal-aware principal component analysis. <em>TNNLS</em>,
<em>35</em>(9), 12201–12210. (<a
href="https://doi.org/10.1109/TNNLS.2023.3252602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A conventional principal component analysis (PCA) frequently suffers from the disturbance of outliers, and thus, spectra of extensions and variations of PCA have been developed. However, all the existing extensions of PCA derive from the same motivation, which aims to alleviate the negative effect of the occlusion. In this article, we design a novel collaborative-enhanced learning framework that aims to highlight the pivotal data points in contrast. As for the proposed framework, only a part of well-fitting samples are adaptively highlighted, which indicates more significance during training. Meanwhile, the framework can collaboratively reduce the disturbance of the polluted samples as well. In other words, two contrary mechanisms could work cooperatively under the proposed framework. Based on the proposed framework, we further develop a pivotal-aware PCA (PAPCA), which utilizes the framework to simultaneously augment positive samples and constrain negative ones by retaining the rotational invariance property. Accordingly, extensive experiments demonstrate that our model has superior performance compared with the existing methods that only focus on the negative samples.},
  archive      = {J_TNNLS},
  author       = {Xuelong Li and Pei Li and Hongyuan Zhang and Kangjia Zhu and Rui Zhang},
  doi          = {10.1109/TNNLS.2023.3252602},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12201-12210},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pivotal-aware principal component analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active clustering ensemble with self-paced learning.
<em>TNNLS</em>, <em>35</em>(9), 12186–12200. (<a
href="https://doi.org/10.1109/TNNLS.2023.3252586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A clustering ensemble provides an elegant framework to learn a consensus result from multiple prespecified clustering partitions. Though conventional clustering ensemble methods achieve promising performance in various applications, we observe that they may usually be misled by some unreliable instances due to the absence of labels. To tackle this issue, we propose a novel active clustering ensemble method, which selects the uncertain or unreliable data for querying the annotations in the process of the ensemble. To fulfill this idea, we seamlessly integrate the active clustering ensemble method into a self-paced learning framework, leading to a novel self-paced active clustering ensemble (SPACE) method. The proposed SPACE can jointly select unreliable data to label via automatically evaluating their difficulty and applying easy data to ensemble the clusterings. In this way, these two tasks can be boosted by each other, with the aim to achieve better clustering performance. The experimental results on benchmark datasets demonstrate the significant effectiveness of our method. The codes of this article are released in https://Doctor-Nobody.github.io/codes/space.zip .},
  archive      = {J_TNNLS},
  author       = {Peng Zhou and Bicheng Sun and Xinwang Liu and Liang Du and Xuejun Li},
  doi          = {10.1109/TNNLS.2023.3252586},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12186-12200},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Active clustering ensemble with self-paced learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Task-oriented robot cognitive manipulation planning using
affordance segmentation and logic reasoning. <em>TNNLS</em>,
<em>35</em>(9), 12172–12185. (<a
href="https://doi.org/10.1109/TNNLS.2023.3252578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of task-oriented robot cognitive manipulation planning is to enable robots to select appropriate actions to manipulate appropriate parts of an object according to different tasks, so as to complete the human-like task execution. This ability is crucial for robots to understand how to manipulate and grasp objects under given tasks. This article proposes a task-oriented robot cognitive manipulation planning method using affordance segmentation and logic reasoning, which can provide robots with semantic reasoning skills about the most appropriate parts of the object to be manipulated and oriented by tasks. Object affordance can be obtained by constructing a convolutional neural network based on the attention mechanism. In view of the diversity of service tasks and objects in service environments, object/task ontologies are constructed to realize the management of objects and tasks, and the object-task affordances are established through causal probability logic. On this basis, the Dempster–Shafer theory is used to design a robot cognitive manipulation planning framework, which can reason manipulation regions’ configuration for the intended task. The experimental results demonstrate that our proposed method can effectively improve the cognitive manipulation ability of robots and make robots preform various tasks more intelligently.},
  archive      = {J_TNNLS},
  author       = {Zhongli Wang and Guohui Tian},
  doi          = {10.1109/TNNLS.2023.3252578},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12172-12185},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Task-oriented robot cognitive manipulation planning using affordance segmentation and logic reasoning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). SSGCNet: A sparse spectra graph convolutional network for
epileptic EEG signal classification. <em>TNNLS</em>, <em>35</em>(9),
12157–12171. (<a
href="https://doi.org/10.1109/TNNLS.2023.3252569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a sparse spectra graph convolutional network (SSGCNet) for epileptic electroencephalogram (EEG) signal classification. The goal is to develop a lightweighted deep learning model while retaining a high level of classification accuracy. To do so, we propose a weighted neighborhood field graph (WNFG) to represent EEG signals. The WNFG reduces redundant edges between graph nodes and has lower graph generation time and memory usage than the baseline solution. The sequential graph convolutional network is further developed from a WNFG by combining sparse weight pruning and the alternating direction method of multipliers (ADMM). Compared with the state-of-the-art method, our method has the same classification accuracy on the Bonn public dataset and the spikes and slow waves (SSW) clinical real dataset when the connection rate is ten times smaller.},
  archive      = {J_TNNLS},
  author       = {Jialin Wang and Rui Gao and Haotian Zheng and Hao Zhu and C.-J. Richard Shi},
  doi          = {10.1109/TNNLS.2023.3252569},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12157-12171},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SSGCNet: A sparse spectra graph convolutional network for epileptic EEG signal classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling high-order relationships: Brain-inspired
hypergraph-induced multimodal-multitask framework for semantic
comprehension. <em>TNNLS</em>, <em>35</em>(9), 12142–12156. (<a
href="https://doi.org/10.1109/TNNLS.2023.3252359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic comprehension aims to reasonably reproduce people’s real intentions or thoughts, e.g., sentiment, humor, sarcasm, motivation, and offensiveness, from multiple modalities. It can be instantiated as a multimodal-oriented multitask classification issue and applied to scenarios, such as online public opinion supervision and political stance analysis. Previous methods generally employ multimodal learning alone to deal with varied modalities or solely exploit multitask learning to solve various tasks, a few to unify both into an integrated framework. Moreover, multimodal-multitask cooperative learning could inevitably encounter the challenges of modeling high-order relationships, i.e., intramodal, intermodal, and intertask relationships. Related research of brain sciences proves that the human brain possesses multimodal perception and multitask cognition for semantic comprehension via decomposing, associating, and synthesizing processes. Thus, establishing a brain-inspired semantic comprehension framework to bridge the gap between multimodal and multitask learning becomes the primary motivation of this work. Motivated by the superiority of the hypergraph in modeling high-order relations, in this article, we propose a hypergraph-induced multimodal-multitask (HIMM) network for semantic comprehension. HIMM incorporates monomodal, multimodal, and multitask hypergraph networks to, respectively, mimic the decomposing, associating, and synthesizing processes to tackle the intramodal, intermodal, and intertask relationships accordingly. Furthermore, temporal and spatial hypergraph constructions are designed to model the relationships in the modality with sequential and spatial structures, respectively. Also, we elaborate a hypergraph alternative updating algorithm to ensure that vertices aggregate to update hyperedges and hyperedges converge to update their connected vertices. Experiments on the dataset with two modalities and five tasks verify the effectiveness of HIMM on semantic comprehension.},
  archive      = {J_TNNLS},
  author       = {Xian Sun and Fanglong Yao and Chibiao Ding},
  doi          = {10.1109/TNNLS.2023.3252359},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12142-12156},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modeling high-order relationships: Brain-inspired hypergraph-induced multimodal-multitask framework for semantic comprehension},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning heterogeneous spatial–temporal context for
skeleton-based action recognition. <em>TNNLS</em>, <em>35</em>(9),
12130–12141. (<a
href="https://doi.org/10.1109/TNNLS.2023.3252172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolution networks (GCNs) have been widely used and achieved fruitful progress in the skeleton-based action recognition task. In GCNs, node interaction modeling dominates the context aggregation and, therefore, is crucial for a graph-based convolution kernel to extract representative features. In this article, we introduce a closer look at a powerful graph convolution formulation to capture rich movement patterns from these skeleton-based graphs. Specifically, we propose a novel heterogeneous graph convolution (HetGCN) that can be considered as the middle ground between the extremes of (2 + 1)-D and 3-D graph convolution. The core observation of HetGCN is that multiple information flows are jointly intertwined in a 3-D convolution kernel, including spatial, temporal, and spatial–temporal cues. Since spatial and temporal information flows characterize different cues for action recognition, HetGCN first dynamically analyzes pairwise interactions between each node and its cross-space–time neighbors and then encourages heterogeneous context aggregation among them. Considering the HetGCN as a generic convolution formulation, we further develop it into two specific instantiations (i.e., intra-scale and inter-scale HetGCN) that significantly facilitate cross-space–time and cross-scale learning on skeleton graphs. By integrating these modules, we propose a strong human action recognition system that outperforms state-of-the-art methods with the accuracy of 93.1% on NTU-60 cross-subject (X-Sub) benchmark, 88.9% on NTU-120 X-Sub benchmark, and 38.4% on kinetics skeleton.},
  archive      = {J_TNNLS},
  author       = {Xuehao Gao and Yang Yang and Yang Wu and Shaoyi Du},
  doi          = {10.1109/TNNLS.2023.3252172},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12130-12141},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning heterogeneous Spatial–Temporal context for skeleton-based action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class-imbalanced-aware distantly supervised named entity
recognition. <em>TNNLS</em>, <em>35</em>(9), 12117–12129. (<a
href="https://doi.org/10.1109/TNNLS.2023.3252084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distantly supervised named entity recognition (NER), which automatically learns NER models without manually labeling data, has gained much attention recently. In distantly supervised NER, positive unlabeled (PU) learning methods have achieved notable success. However, existing PU learning-based NER methods are unable to automatically handle the class imbalance and further depend on the estimation of the unknown class prior; thus, the class imbalance and imperfect estimation of the class prior degenerate the NER performance. To address these issues, this article proposes a novel PU learning method for distantly supervised NER. The proposed method can automatically handle the class imbalance and does not need to engage in class prior estimation, which enables the proposed methods to achieve the state-of-the-art performance. Extensive experiments support our theoretical analysis and validate the superiority of our method.},
  archive      = {J_TNNLS},
  author       = {Yuren Mao and Yu Hao and Weiwei Liu and Xuemin Lin and Xin Cao},
  doi          = {10.1109/TNNLS.2023.3252084},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12117-12129},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Class-imbalanced-aware distantly supervised named entity recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven-based cooperative resilient learning method for
nonlinear MASs under DoS attacks. <em>TNNLS</em>, <em>35</em>(9),
12107–12116. (<a
href="https://doi.org/10.1109/TNNLS.2023.3252080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the cooperative tracking problem for a class of nonlinear multiagent systems (MASs) with unknown dynamics under denial-of-service (DoS) attacks. To solve such a problem, a hierarchical cooperative resilient learning method, which involves a distributed resilient observer and a decentralized learning controller, is introduced in this article. Due to the existence of communication layers in the hierarchical control architecture, it may lead to communication delays and DoS attacks. Motivated by this consideration, a resilient model-free adaptive control (MFAC) method is developed to withstand the influence of communication delays and DoS attacks. First, a virtual reference signal is designed for each agent to estimate the time-varying reference signal under DoS attacks. To facilitate the tracking of each agent, the virtual reference signal is discretized. Then, a decentralized MFAC algorithm is designed for each agent such that each agent can track the reference signal by only using the obtained local information. Finally, a simulation example is proposed to verify the effectiveness of the developed method.},
  archive      = {J_TNNLS},
  author       = {Chao Deng and Xiao-Zheng Jin and Zheng-Guang Wu and Wei-Wei Che},
  doi          = {10.1109/TNNLS.2023.3252080},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12107-12116},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven-based cooperative resilient learning method for nonlinear MASs under DoS attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Security versus accuracy: Trade-off data modeling to safe
fault classification systems. <em>TNNLS</em>, <em>35</em>(9),
12095–12106. (<a
href="https://doi.org/10.1109/TNNLS.2023.3251999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the data-driven fault classification systems have achieved great success and been widely deployed, machine-learning-based models have recently been shown to be unsafe and vulnerable to tiny perturbations, i.e., adversarial attack. For the safety-critical industrial scenarios, the adversarial security (i.e., adversarial robustness) of the fault system should be taken into serious consideration. However, security and accuracy are intrinsically conflicting, which is a trade-off issue. In this article, we first study this new trade-off issue in the design of fault classification models and solve it from a brand new view, hyperparameter optimization (HPO). Meanwhile, to reduce the computational expense of HPO, we propose a new multiobjective (MO), multifidelity (MF) Bayesian optimization (BO) algorithm, MMTPE. The proposed algorithm is evaluated on safety-critical industrial datasets with the mainstream machine learning (ML) models. The results show that the following hold: 1) MMTPE is superior to other advanced optimization algorithms in both efficiency and performance and 2) fault classification models with optimized hyperparameters are competitive with advanced adversarially defensive methods. Moreover, insights into the model security are given, including the model intrinsic security properties and the correlations between hyperparameters and security.},
  archive      = {J_TNNLS},
  author       = {Yue Zhuo and Zhihuan Song and Zhiqiang Ge},
  doi          = {10.1109/TNNLS.2023.3251999},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12095-12106},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Security versus accuracy: Trade-off data modeling to safe fault classification systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust-EQA: Robust learning for embodied question answering
with noisy labels. <em>TNNLS</em>, <em>35</em>(9), 12083–12094. (<a
href="https://doi.org/10.1109/TNNLS.2023.3251984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied question answering (EQA) is a recently emerged research field in which an agent is asked to answer the user’s questions by exploring the environment and collecting visual information. Plenty of researchers turn their attention to the EQA field due to its broad potential application areas, such as in-home robots, self-driven mobile, and personal assistants. High-level visual tasks, such as EQA, are susceptible to noisy inputs, because they have complex reasoning processes. Before the profits of the EQA field can be applied to practical applications, good robustness against label noise needs to be equipped. To tackle this problem, we propose a novel label noise-robust learning algorithm for the EQA task. First, a joint training co-regularization noise-robust learning method is proposed for noisy filtering of the visual question answering (VQA) module, which trains two parallel network branches by one loss function. Then, a two-stage hierarchical robust learning algorithm is proposed to filter out noisy navigation labels in both trajectory level and action level. Finally, by taking purified labels as inputs, a joint robust learning mechanism is given to coordinate the work of the whole EQA system. Empirical results demonstrate that, under extremely noisy environments (45% of noisy labels) and low-level noisy environments (20% of noisy labels), the robustness of deep learning models trained by our algorithm is superior to the existing EQA models in noisy environments.},
  archive      = {J_TNNLS},
  author       = {Haonan Luo and Guosheng Lin and Fumin Shen and Xingguo Huang and Yazhou Yao and Hengtao Shen},
  doi          = {10.1109/TNNLS.2023.3251984},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12083-12094},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust-EQA: Robust learning for embodied question answering with noisy labels},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature-based interpolation and geodesics in the latent
spaces of generative models. <em>TNNLS</em>, <em>35</em>(9),
12068–12082. (<a
href="https://doi.org/10.1109/TNNLS.2023.3251848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpolating between points is a problem connected simultaneously with finding geodesics and study of generative models. In the case of geodesics, we search for the curves with the shortest length, while in the case of generative models, we typically apply linear interpolation in the latent space. However, this interpolation uses implicitly the fact that Gaussian is unimodal. Thus, the problem of interpolating in the case when the latent density is non-Gaussian is an open problem. In this article, we present a general and unified approach to interpolation, which simultaneously allows us to search for geodesics and interpolating curves in latent space in the case of arbitrary density. Our results have a strong theoretical background based on the introduced quality measure of an interpolating curve. In particular, we show that maximizing the quality measure of the curve can be equivalently understood as a search of geodesic for a certain redefinition of the Riemannian metric on the space. We provide examples in three important cases. First, we show that our approach can be easily applied to finding geodesics on manifolds. Next, we focus our attention in finding interpolations in pretrained generative models. We show that our model effectively works in the case of arbitrary density. Moreover, we can interpolate in the subset of the space consisting of data possessing a given feature. The last case is focused on finding interpolation in the space of chemical compounds.},
  archive      = {J_TNNLS},
  author       = {Łukasz Struski and Michał Sadowski and Tomasz Danel and Jacek Tabor and Igor T. Podolak},
  doi          = {10.1109/TNNLS.2023.3251848},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12068-12082},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature-based interpolation and geodesics in the latent spaces of generative models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Newton–raphson meets sparsity: Sparse learning via a novel
penalty and a fast solver. <em>TNNLS</em>, <em>35</em>(9), 12057–12067.
(<a href="https://doi.org/10.1109/TNNLS.2023.3251748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning and statistics, the penalized regression methods are the main tools for variable selection (or feature selection) in high-dimensional sparse data analysis. Due to the nonsmoothness of the associated thresholding operators of commonly used penalties such as the least absolute shrinkage and selection operator (LASSO), the smoothly clipped absolute deviation (SCAD), and the minimax concave penalty (MCP), the classical Newton–Raphson algorithm cannot be used. In this article, we propose a cubic Hermite interpolation penalty (CHIP) with a smoothing thresholding operator. Theoretically, we establish the nonasymptotic estimation error bounds for the global minimizer of the CHIP penalized high-dimensional linear regression. Moreover, we show that the estimated support coincides with the target support with a high probability. We derive the Karush–Kuhn–Tucker (KKT) condition for the CHIP penalized estimator and then develop a support detection-based Newton–Raphson (SDNR) algorithm to solve it. Simulation studies demonstrate that the proposed method performs well in a wide range of finite sample situations. We also illustrate the application of our method with a real data example.},
  archive      = {J_TNNLS},
  author       = {Yongxiu Cao and Lican Kang and Xuerui Li and Yanyan Liu and Yuan Luo and Yueyong Shi},
  doi          = {10.1109/TNNLS.2023.3251748},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12057-12067},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Newton–Raphson meets sparsity: Sparse learning via a novel penalty and a fast solver},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual learning of generative models with limited data:
From wasserstein-1 barycenter to adaptive coalescence. <em>TNNLS</em>,
<em>35</em>(9), 12042–12056. (<a
href="https://doi.org/10.1109/TNNLS.2023.3251096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning generative models is challenging for a network edge node with limited data and computing power. Since tasks in similar environments share a model similarity, it is plausible to leverage pretrained generative models from other edge nodes. Appealing to optimal transport theory tailored toward Wasserstein-1 generative adversarial networks (WGANs), this study aims to develop a framework that systematically optimizes continual learning of generative models using local data at the edge node while exploiting adaptive coalescence of pretrained generative models. Specifically, by treating the knowledge transfer from other nodes as Wasserstein balls centered around their pretrained models, continual learning of generative models is cast as a constrained optimization problem, which is further reduced to a Wasserstein-1 barycenter problem. A two-stage approach is devised accordingly: 1) the barycenters among the pretrained models are computed offline, where displacement interpolation is used as the theoretic foundation for finding adaptive barycenters via a “recursive” WGAN configuration and 2) the barycenter computed offline is used as metamodel initialization for continual learning, and then, fast adaptation is carried out to find the generative model using the local samples at the target edge node. Finally, a weight ternarization method, based on joint optimization of weights and threshold for quantization, is developed to compress the generative model further. Extensive experimental studies corroborate the effectiveness of the proposed framework.},
  archive      = {J_TNNLS},
  author       = {Mehmet Dedeoglu and Sen Lin and Zhaofeng Zhang and Junshan Zhang},
  doi          = {10.1109/TNNLS.2023.3251096},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12042-12056},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Continual learning of generative models with limited data: From wasserstein-1 barycenter to adaptive coalescence},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse personalized federated learning. <em>TNNLS</em>,
<em>35</em>(9), 12027–12041. (<a
href="https://doi.org/10.1109/TNNLS.2023.3250658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a collaborative machine learning technique to train a global model (GM) without obtaining clients’ private data. The main challenges in FL are statistical diversity among clients, limited computing capability among clients’ equipment, and the excessive communication overhead between the server and clients. To address these challenges, we propose a novel sparse personalized FL scheme via maximizing correlation ( FedMac ). By incorporating an approximated $\ell _{1}$ -norm and the correlation between client models and GM into standard FL loss function, the performance on statistical diversity data is improved and the communicational and computational loads required in the network are reduced compared with nonsparse FL. Convergence analysis shows that the sparse constraints in FedMac do not affect the convergence rate of the GM, and theoretical results show that FedMac can achieve good sparse personalization, which is better than the personalized methods based on the $\ell _{2}$ -norm. Experimentally, we demonstrate the benefits of this sparse personalization architecture compared with the state-of-the-art personalization methods (e.g., FedMac , respectively, achieves 98.95%, 99.37%, 90.90%, 89.06%, and 73.52% accuracy on the MNIST, FMNIST, CIFAR-100, Synthetic, and CINIC-10 datasets under non-independent and identically distributed (i.i.d.) variants).},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Liu and Yinchuan Li and Qing Wang and Xu Zhang and Yunfeng Shao and Yanhui Geng},
  doi          = {10.1109/TNNLS.2023.3250658},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12027-12041},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparse personalized federated learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Memristive circuit implementation of caenorhabditis elegans
mechanism for neuromorphic computing. <em>TNNLS</em>, <em>35</em>(9),
12015–12026. (<a
href="https://doi.org/10.1109/TNNLS.2023.3250655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the energy efficiency bottleneck of the von Neumann architecture and scaling limit of silicon transistors, an emerging but promising solution is neuromorphic computing, a new computing paradigm inspired by how biological neural networks handle the massive amount of information in a parallel and efficient way. Recently, there is a surge of interest in the nematode worm Caenorhabditis elegans (C. elegans), an ideal model organism to probe the mechanisms of biological neural networks. In this article, we propose a neuron model for C. elegans with leaky integrate-and-fire (LIF) dynamics and adjustable integration time. We utilize these neurons to build the C. elegans neural network according to their neural physiology, which comprises: 1) sensory modules; 2) interneuron modules; and 3) motoneuron modules. Leveraging these block designs, we develop a serpentine robot system, which mimics the locomotion behavior of C. elegans upon external stimulus. Moreover, experimental results of C. elegans neurons presented in this article reveals the robustness (1% error w.r.t. 10% random noise) and flexibility of our design in term of parameter setting. The work paves the way for future intelligent systems by mimicking the C. elegans neural system.},
  archive      = {J_TNNLS},
  author       = {Hegan Chen and Qinghui Hong and Zhongrui Wang and Chunhua Wang and Xiangxiang Zeng and Jiliang Zhang},
  doi          = {10.1109/TNNLS.2023.3250655},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12015-12026},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memristive circuit implementation of caenorhabditis elegans mechanism for neuromorphic computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multicontrast MRI super-resolution via transformer-empowered
multiscale contextual matching and aggregation. <em>TNNLS</em>,
<em>35</em>(9), 12004–12014. (<a
href="https://doi.org/10.1109/TNNLS.2023.3250491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) possesses the unique versatility to acquire images under a diverse array of distinct tissue contrasts, which makes multicontrast super-resolution (SR) techniques possible and needful. Compared with single-contrast MRI SR, multicontrast SR is expected to produce higher quality images by exploiting a variety of complementary information embedded in different imaging contrasts. However, existing approaches still have two shortcomings: 1) most of them are convolution-based methods and, hence, weak in capturing long-range dependencies, which are essential for MR images with complicated anatomical patterns and 2) they ignore to make full use of the multicontrast features at different scales and lack effective modules to match and aggregate these features for faithful SR. To address these issues, we develop a novel multicontrast MRI SR network via transformer-empowered multiscale feature matching and aggregation, dubbed McMRSR $^{++}$ . First, we tame transformers to model long-range dependencies in both reference and target images at different scales. Then, a novel multiscale feature matching and aggregation method is proposed to transfer corresponding contexts from reference features at different scales to the target features and interactively aggregate them. Furthermore, a texture-preserving branch and a contrastive constraint are incorporated into our framework for enhancing the textural details in the SR images. Experimental results on both public and clinical in vivo datasets show that McMRSR $^{++}$ outperforms state-of-the-art methods under peak signal to noise ratio (PSNR), structure similarity index measure (SSIM), and root mean square error (RMSE) metrics significantly. Visual results demonstrate the superiority of our method in restoring structures, demonstrating its great potential to improve scan efficiency in clinical practice.},
  archive      = {J_TNNLS},
  author       = {Jun Lyu and Guangyuan Li and Chengyan Wang and Qing Cai and Qi Dou and David Zhang and Jing Qin},
  doi          = {10.1109/TNNLS.2023.3250491},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {12004-12014},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multicontrast MRI super-resolution via transformer-empowered multiscale contextual matching and aggregation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSAGCN: Social soft attention graph convolution network for
pedestrian trajectory prediction. <em>TNNLS</em>, <em>35</em>(9),
11989–12003. (<a
href="https://doi.org/10.1109/TNNLS.2023.3250485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction is an important technique of autonomous driving. In order to accurately predict the reasonable future trajectory of pedestrians, it is inevitable to consider social interactions among pedestrians and the influence of surrounding scene simultaneously, which can fully represent the complex behavior information and ensure the rationality of predicted trajectories obeyed realistic rules. In this article, we propose one new prediction model named social soft attention graph convolution network (SSAGCN), which aims to simultaneously handle social interactions among pedestrians and scene interactions between pedestrians and environments. In detail, when modeling social interaction, we propose a new social soft attention function, which fully considers various interaction factors among pedestrians. Also, it can distinguish the influence of pedestrians around the agent based on different factors under various situations. For the scene interaction, we propose one new sequential scene sharing mechanism. The influence of the scene on one agent at each moment can be shared with other neighbors through social soft attention; therefore, the influence of the scene is expanded both in spatial and temporal dimensions. With the help of these improvements, we successfully obtain socially and physically acceptable predicted trajectories. The experiments on public available datasets prove the effectiveness of SSAGCN and have achieved state-of-the-art results. The project code is available at https://github.com/WW-Tong/ssagcn_for_path_prediction},
  archive      = {J_TNNLS},
  author       = {Pei Lv and Wentong Wang and Yunxin Wang and Yuzhen Zhang and Mingliang Xu and Changsheng Xu},
  doi          = {10.1109/TNNLS.2023.3250485},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11989-12003},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SSAGCN: Social soft attention graph convolution network for pedestrian trajectory prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing efficient bit-level sparsity-tolerant memristive
networks. <em>TNNLS</em>, <em>35</em>(9), 11979–11988. (<a
href="https://doi.org/10.1109/TNNLS.2023.3250437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid progress of deep neural network (DNN) applications on memristive platforms, there has been a growing interest in the acceleration and compression of memristive networks. As an emerging model optimization technique for memristive platforms, bit-level sparsity training (with the fixed-point quantization) can significantly reduce the demand for analog-to-digital converters (ADCs) resolution, which is critical for energy and area consumption. However, the bit sparsity and the fixed-point quantization will inevitably lead to a large performance loss. Different from the existing training and optimization techniques, this work attempts to explore more sparsity-tolerant architectures to compensate for performance degradation. We first empirically demonstrate that in a certain search space (e.g., 4-bit quantized DARTS space), network architectures differ in bit-level sparsity tolerance. It is reasonable and necessary to search the architectures for efficient deployment on memristive platforms by the neural architecture search (NAS) technology. We further introduce bit-level sparsity-tolerant NAS (BST-NAS), which encapsulates low-precision quantization and bit-level sparsity training into the differentiable NAS, to explore the optimal bit-level sparsity-tolerant architectures. Experimentally, with the same degree of sparsity and experiment settings, our searched architectures obtain a promising performance, which outperform the normal NAS-based DARTS-series architectures (about 5.8% higher than that of DARTS-V2 and 2.7% higher than that of PC-DARTS) on CIFAR10.},
  archive      = {J_TNNLS},
  author       = {Bo Lyu and Shiping Wen and Yin Yang and Xiaojun Chang and Junwei Sun and Yiran Chen and Tingwen Huang},
  doi          = {10.1109/TNNLS.2023.3250437},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11979-11988},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Designing efficient bit-level sparsity-tolerant memristive networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot object detection: A comprehensive survey.
<em>TNNLS</em>, <em>35</em>(9), 11958–11978. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans are able to learn to recognize new objects even from a few examples. In contrast, training deep-learning-based object detectors requires huge amounts of annotated data. To avoid the need to acquire and annotate these huge amounts of data, few-shot object detection (FSOD) aims to learn from few object instances of new categories in the target domain. In this survey, we provide an overview of the state of the art in FSOD. We categorize approaches according to their training scheme and architectural layout. For each type of approach, we describe the general realization as well as concepts to improve the performance on novel categories. Whenever appropriate, we give short takeaways regarding these concepts in order to highlight the best ideas. Eventually, we introduce commonly used datasets and their evaluation protocols and analyze the reported benchmark results. As a result, we emphasize common challenges in evaluation and identify the most promising current trends in this emerging field of FSOD.},
  archive      = {J_TNNLS},
  author       = {Mona Köhler and Markus Eisenbach and Horst-Michael Gross},
  doi          = {10.1109/TNNLS.2023.3265051},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11958-11978},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Few-shot object detection: A comprehensive survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning versus evolution strategies: A
comparative survey. <em>TNNLS</em>, <em>35</em>(9), 11939–11957. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) and evolution strategies (ESs) have surpassed human-level control in many sequential decision-making problems, yet many open challenges still exist. To get insights into the strengths and weaknesses of DRL versus ESs, an analysis of their respective capabilities and limitations is provided. After presenting their fundamental concepts and algorithms, a comparison is provided on key aspects, such as scalability, exploration, adaptation to dynamic environments, and multiagent learning. Current research challenges are also discussed, including sample efficiency, exploration versus exploitation, dealing with sparse rewards, and learning to plan. Then, the benefits of hybrid algorithms that combine DRL and ESs are highlighted.},
  archive      = {J_TNNLS},
  author       = {Amjad Yousef Majid and Serge Saaybi and Vincent Francois-Lavet and R. Venkatesha Prasad and Chris Verhoeven},
  doi          = {10.1109/TNNLS.2023.3264540},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11939-11957},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning versus evolution strategies: A comparative survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards fairness-aware federated learning. <em>TNNLS</em>,
<em>35</em>(9), 11922–11938. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in federated learning (FL) have brought large-scale collaborative machine learning opportunities for massively distributed clients with performance and data privacy guarantees. However, most current works focus on the interest of the central controller in FL and overlook the interests of the FL clients. This may result in unfair treatment of clients, which discourages them from actively participating in the learning process and damages the sustainability of the FL ecosystem. Therefore, the topic of ensuring fairness in FL is attracting a great deal of research interest. In recent years, diverse fairness-aware FL (FAFL) approaches have been proposed in an effort to achieve fairness in FL from different perspectives. However, there is no comprehensive survey that helps readers gain insight into this interdisciplinary field. This article aims to provide such a survey. By examining the fundamental and simplifying assumptions, as well as the notions of fairness adopted by the existing literature in this field, we propose a taxonomy of FAFL approaches covering major steps in FL, including client selection, optimization, contribution evaluation, and incentive distribution. In addition, we discuss the main metrics for experimentally evaluating the performance of FAFL approaches and suggest promising future research directions toward FAFL.},
  archive      = {J_TNNLS},
  author       = {Yuxin Shi and Han Yu and Cyril Leung},
  doi          = {10.1109/TNNLS.2023.3263594},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11922-11938},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Towards fairness-aware federated learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Backpropagation-based learning techniques for deep spiking
neural networks: A survey. <em>TNNLS</em>, <em>35</em>(9), 11906–11921.
(<a href="https://doi.org/10.1109/TNNLS.2023.3263008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the adoption of smart systems, artificial neural networks (ANNs) have become ubiquitous. Conventional ANN implementations have high energy consumption, limiting their use in embedded and mobile applications. Spiking neural networks (SNNs) mimic the dynamics of biological neural networks by distributing information over time through binary spikes. Neuromorphic hardware has emerged to leverage the characteristics of SNNs, such as asynchronous processing and high activation sparsity. Therefore, SNNs have recently gained interest in the machine learning community as a brain-inspired alternative to ANNs for low-power applications. However, the discrete representation of the information makes the training of SNNs by backpropagation-based techniques challenging. In this survey, we review training strategies for deep SNNs targeting deep learning applications such as image processing. We start with methods based on the conversion from an ANN to an SNN and compare these with backpropagation-based techniques. We propose a new taxonomy of spiking backpropagation algorithms into three categories, namely, spatial, spatiotemporal, and single-spike approaches. In addition, we analyze different strategies to improve accuracy, latency, and sparsity, such as regularization methods, training hybridization, and tuning of the parameters specific to the SNN neuron model. We highlight the impact of input encoding, network architecture, and training strategy on the accuracy–latency tradeoff. Finally, in light of the remaining challenges for accurate and efficient SNN solutions, we emphasize the importance of joint hardware–software codevelopment.},
  archive      = {J_TNNLS},
  author       = {Manon Dampfhoffer and Thomas Mesquida and Alexandre Valentian and Lorena Anghel},
  doi          = {10.1109/TNNLS.2023.3263008},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11906-11921},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Backpropagation-based learning techniques for deep spiking neural networks: A survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRA: Graph representation alignment for semi-supervised
action recognition. <em>TNNLS</em>, <em>35</em>(9), 11896–11905. (<a
href="https://doi.org/10.1109/TNNLS.2023.3347593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have emerged as a powerful tool for action recognition, leveraging skeletal graphs to encapsulate human motion. Despite their efficacy, a significant challenge remains the dependency on huge labeled datasets. Acquiring such datasets is often prohibitive, and the frequent occurrence of incomplete skeleton data, typified by absent joints and frames, complicates the testing phase. To tackle these issues, we present graph representation alignment (GRA), a novel approach with two main contributions: 1) a self-training (ST) paradigm that substantially reduces the need for labeled data by generating high-quality pseudo-labels, ensuring model stability even with minimal labeled inputs and 2) a representation alignment (RA) technique that utilizes consistency regularization to effectively reduce the impact of missing data components. Our extensive evaluations on the NTU RGB+D and Northwestern-UCLA (N-UCLA) benchmarks demonstrate that GRA not only improves GCN performance in data-constrained environments but also retains impressive performance in the face of data incompleteness.},
  archive      = {J_TNNLS},
  author       = {Kuan-Hung Huang and Yao-Bang Huang and Yong-Xiang Lin and Kai-Lung Hua and M. Tanveer and Xuequan Lu and Imran Razzak},
  doi          = {10.1109/TNNLS.2023.3347593},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11896-11905},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GRA: Graph representation alignment for semi-supervised action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object counting via group and graph attention network.
<em>TNNLS</em>, <em>35</em>(9), 11884–11895. (<a
href="https://doi.org/10.1109/TNNLS.2023.3336894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object counting, defined as the task of accurately predicting the number of objects in static images or videos, has recently attracted considerable interest. However, the unavoidable presence of background noise prevents counting performance from advancing further. To address this issue, we created a group and graph attention network (GGANet) for dense object counting. GGANet is an encoder–decoder architecture incorporating a group channel attention (GCA) module and a learnable graph attention (LGA) module. The GCA module groups the feature map into several subfeatures, each of which is assigned an attention factor through the identical channel attention. The LGA module views the feature map as a graph structure in which the different channels represent diverse feature vertices, and the responses between channels represent edges. The GCA and LGA modules jointly avoid the interference of irrelevant pixels and suppress the background noise. Experiments are conducted on four crowd-counting datasets, two vehicle-counting datasets, one remote-sensing counting dataset, and one few-shot object-counting dataset. Comparative results prove that the proposed GGANet achieves superior counting performance.},
  archive      = {J_TNNLS},
  author       = {Xiangyu Guo and Mingliang Gao and Guofeng Zou and Alessandro Bruno and Abdellah Chehri and Gwanggil Jeon},
  doi          = {10.1109/TNNLS.2023.3336894},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11884-11895},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Object counting via group and graph attention network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relation-aware heterogeneous graph network for learning
intermodal semantics in textbook question answering. <em>TNNLS</em>,
<em>35</em>(9), 11872–11883. (<a
href="https://doi.org/10.1109/TNNLS.2024.3385436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textbook question answering (TQA) task aims to infer answers for given questions from a multimodal context, including text and diagrams. The existing studies have aggregated intramodal semantics extracted from a single modality but have yet to capture the intermodal semantics between different modalities. A major challenge in learning intermodal semantics is maintaining lossless intramodal semantics while bridging the gap of semantics caused by heterogeneity. In this article, we propose an intermodal relation-aware heterogeneous graph network (IMR-HGN) to extract the intermodal semantics for TQA, which aggregates different modalities while learning features rather than representing them independently. First, we design a multidomain consistent representation (MDCR) to eliminate semantic gaps by capturing intermodal features while maintaining lossless intramodal semantics in multidomains. Furthermore, we present neighbor-based relation inpainting (NRI) to reduce semantic ambiguity via repairing fuzzy relations with correlations of relations. Finally, we propose hierarchical multisemantics aggregation (HMSA) to guarantee the completeness of semantics by aggregating features of nodes and relations with a reconstruction network (RN). Experimental results show that IMR-HGN could extract the intermodal semantics of answers, achieving a 2.16% improvement on the validation set of the TQA dataset and a 3.04% increase on the test set of the AI2D dataset.},
  archive      = {J_TNNLS},
  author       = {Sai Zhang and Yunjie Wu and Xiaowang Zhang and Zhiyong Feng and Liang Wan and Zhiqiang Zhuang},
  doi          = {10.1109/TNNLS.2024.3385436},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11872-11883},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Relation-aware heterogeneous graph network for learning intermodal semantics in textbook question answering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning joint 2-d and 3-d graph diffusion models for
complete molecule generation. <em>TNNLS</em>, <em>35</em>(9),
11857–11871. (<a
href="https://doi.org/10.1109/TNNLS.2024.3416328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing new molecules is essential for drug discovery and material science. Recently, deep generative models that aim to model molecule distribution have made promising progress in narrowing down the chemical research space and generating high-fidelity molecules. However, current generative models only focus on modeling 2-D bonding graphs or 3-D geometries, which are two complementary descriptors for molecules. The lack of ability to jointly model them limits the improvement of generation quality and further downstream applications. In this article, we propose a joint 2-D and 3-D graph diffusion model (JODO) that generates geometric graphs representing complete molecules with atom types, formal charges, bond information, and 3-D coordinates. To capture the correlation between 2-D molecular graphs and 3-D geometries in the diffusion process, we develop a diffusion graph transformer (DGT) to parameterize the data prediction model that recovers the original data from noisy data. The DGT uses a relational attention mechanism that enhances the interaction between node and edge representations. This mechanism operates concurrently with the propagation and update of scalar attributes and geometric vectors. Our model can also be extended for inverse molecular design targeting single or multiple quantum properties. In our comprehensive evaluation pipeline for unconditional joint generation, the experimental results show that JODO remarkably outperforms the baselines on the QM9 and GEOM-Drugs datasets. Furthermore, our model excels in few-step fast sampling, as well as in inverse molecule design and molecular graph generation. Our code is provided in https://github.com/GRAPH-0/JODO .},
  archive      = {J_TNNLS},
  author       = {Han Huang and Leilei Sun and Bowen Du and Weifeng Lv},
  doi          = {10.1109/TNNLS.2024.3416328},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11857-11871},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning joint 2-D and 3-D graph diffusion models for complete molecule generation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized federated graph learning on non-IID electronic
health records. <em>TNNLS</em>, <em>35</em>(9), 11843–11856. (<a
href="https://doi.org/10.1109/TNNLS.2024.3370297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the latent disease patterns embedded in electronic health records (EHRs) is crucial for making precise and proactive healthcare decisions. Federated graph learning-based methods are commonly employed to extract complex disease patterns from the distributed EHRs without sharing the client-side raw data. However, the intrinsic characteristics of the distributed EHRs are typically non-independent and identically distributed (Non-IID), significantly bringing challenges related to data imbalance and leading to a notable decrease in the effectiveness of making healthcare decisions derived from the global model. To address these challenges, we introduce a novel personalized federated learning framework named PEARL, which is designed for disease prediction on Non-IID EHRs. Specifically, PEARL incorporates disease diagnostic code attention and admission record attention to extract patient embeddings from all EHRs. Then, PEARL integrates self-supervised learning into a federated learning framework to train a global model for hierarchical disease prediction. To improve the performance of the client model, we further introduce a fine-tuning scheme to personalize the global model using local EHRs. During the global model updating process, a differential privacy (DP) scheme is implemented, providing a high-level privacy guarantee. Extensive experiments conducted on the real-world MIMIC-III dataset validate the effectiveness of PEARL, demonstrating competitive results when compared with baselines.},
  archive      = {J_TNNLS},
  author       = {Tao Tang and Zhuoyang Han and Zhen Cai and Shuo Yu and Xiaokang Zhou and Taiwo Oseni and Sajal K. Das},
  doi          = {10.1109/TNNLS.2024.3370297},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11843-11856},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Personalized federated graph learning on non-IID electronic health records},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multitask dynamic graph attention autoencoder for
imbalanced multilabel time series classification. <em>TNNLS</em>,
<em>35</em>(9), 11829–11842. (<a
href="https://doi.org/10.1109/TNNLS.2024.3369064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph learning is widely applied to process various complex data structures (e.g., time series) in different domains. Due to multidimensional observations and the requirement for accurate data representation, time series are usually represented in the form of multilabels. Accurately classifying multilabel time series can provide support for personalized predictions and risk assessments. It requires effectively capturing complex label relevance and overcoming imbalanced label distributions of multilabel time series. However, the existing methods are unable to model label relevance for multilabel time series or fail to fully exploit it. In addition, the existing multilabel classification balancing strategies suffer from limitations, such as disregarding label relevance, information loss, and sampling bias. This article proposes a dynamic graph attention autoencoder-based multitask (DGAAE-MT) learning framework for multilabel time series classification. It can fully and accurately model label relevance for each instance by using a dynamic graph attention-based graph autoencoder to improve multilabel classification accuracy. DGAAE-MT employs a dual-sampling strategy and cooperative training approach to improve the classification accuracy of low-frequency classes while maintaining the classification accuracy of high-frequency and mid-frequency classes. It avoids information loss and sampling bias. DGAAE-MT achieves a mean average precision (mAP) of 0.955 and an $F1$ score of 0.978 on a mixed medical time series dataset. It outperforms state-of-the-art works in the past two years.},
  archive      = {J_TNNLS},
  author       = {Le Sun and Chenyang Li and Yongjun Ren and Yanchun Zhang},
  doi          = {10.1109/TNNLS.2024.3369064},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11829-11842},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A multitask dynamic graph attention autoencoder for imbalanced multilabel time series classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Reconstructed graph neural network with knowledge
distillation for lightweight anomaly detection. <em>TNNLS</em>,
<em>35</em>(9), 11817–11828. (<a
href="https://doi.org/10.1109/TNNLS.2024.3389714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet-of-Things (IoT) technologies in modern smart society enables massive data exchange for offering intelligent services. It becomes essential to ensure secure communications while exchanging highly sensitive IoT data efficiently, which leads to high demands for lightweight models or algorithms with limited computation capability provided by individual IoT devices. In this study, a graph representation learning model, which seamlessly incorporates graph neural network (GNN) and knowledge distillation (KD) techniques, named reconstructed graph with global–local distillation (RG-GLD), is designed to realize the lightweight anomaly detection across IoT communication networks. In particular, a new graph network reconstruction strategy, which treats data communications as nodes in a directed graph while edges are then connected according to two specifically defined rules, is devised and applied to facilitate the graph representation learning in secure and efficient IoT communications. Both the structural and traffic features are then extracted from the graph data and flow data respectively, based on the graph attention network (GAT) and multilayer perceptron (MLP) techniques. These can benefit the GNN-based KD process in accordance with the more effective feature fusion and representation, considering both structural and data levels across the dynamic IoT networks. Furthermore, a lightweight local subgraph preservation mechanism improved by the graph attention mechanism and downsampling scheme to better utilize the topological information, and a so-called global information alignment defined based on the self-attention mechanism to effectively preserve the global information, are developed and incorporated in a refined graph attention based KD scheme. Compared with four different baseline methods, experiments and evaluations conducted based on two public datasets demonstrate the usefulness and effectiveness of our proposed model in improving the efficiency of knowledge transfer with higher classification accuracy but lower computational load, which can be deployed for lightweight anomaly detection in sustainable IoT computing environments.},
  archive      = {J_TNNLS},
  author       = {Xiaokang Zhou and Jiayi Wu and Wei Liang and Kevin I-Kai Wang and Zheng Yan and Laurence T. Yang and Qun Jin},
  doi          = {10.1109/TNNLS.2024.3389714},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11817-11828},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reconstructed graph neural network with knowledge distillation for lightweight anomaly detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correlation-aware spatial–temporal graph learning for
multivariate time-series anomaly detection. <em>TNNLS</em>,
<em>35</em>(9), 11802–11816. (<a
href="https://doi.org/10.1109/TNNLS.2023.3325667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time-series anomaly detection is critically important in many applications, including retail, transportation, power grid, and water treatment plants. Existing approaches for this problem mostly employ either statistical models which cannot capture the nonlinear relations well or conventional deep learning (DL) models [e.g., convolutional neural network (CNN) and long short-term memory (LSTM)] that do not explicitly learn the pairwise correlations among variables. To overcome these limitations, we propose a novel method, correlation-aware spatial–temporal graph learning (termed CST-GL ), for time-series anomaly detection. CST-GL explicitly captures the pairwise correlations via a multivariate time series correlation learning (MTCL) module based on which a spatial–temporal graph neural network (STGNN) can be developed. Then, by employing a graph convolution network (GCN) that exploits one- and multihop neighbor information, our STGNN component can encode rich spatial information from complex pairwise dependencies between variables. With a temporal module that consists of dilated convolutional functions, the STGNN can further capture long-range dependence over time. A novel anomaly scoring component is further integrated into CST-GL to estimate the degree of an anomaly in a purely unsupervised manner. Experimental results demonstrate that CST-GL can detect and diagnose anomalies effectively in general settings as well as enable early detection across different time delays. Our code is available at https://github.com/huankoh/CST-GL .},
  archive      = {J_TNNLS},
  author       = {Yu Zheng and Huan Yee Koh and Ming Jin and Lianhua Chi and Khoa T. Phan and Shirui Pan and Yi-Ping Phoebe Chen and Wei Xiang},
  doi          = {10.1109/TNNLS.2023.3325667},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11802-11816},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Correlation-aware Spatial–Temporal graph learning for multivariate time-series anomaly detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning for dynamic graphs: Models and benchmarks.
<em>TNNLS</em>, <em>35</em>(9), 11788–11801. (<a
href="https://doi.org/10.1109/TNNLS.2024.3379735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in research on deep graph networks (DGNs) has led to a maturation of the domain of learning on graphs. Despite the growth of this research field, there are still important challenges that are yet unsolved. Specifically, there is an urge of making DGNs suitable for predictive tasks on real-world systems of interconnected entities, which evolve over time. With the aim of fostering research in the domain of dynamic graphs, first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs. Second, we conduct a fair performance comparison among the most popular proposed approaches on node- and edge-level tasks, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches.},
  archive      = {J_TNNLS},
  author       = {Alessio Gravina and Davide Bacciu},
  doi          = {10.1109/TNNLS.2024.3379735},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11788-11801},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning for dynamic graphs: Models and benchmarks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAP-LSTM: Graph-based autocorrelation preserving networks
for geo-distributed forecasting. <em>TNNLS</em>, <em>35</em>(9),
11773–11787. (<a
href="https://doi.org/10.1109/TNNLS.2024.3398441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting methods are important decision support tools in geo-distributed sensor networks. However, challenges such as the multivariate nature of data, the existence of multiple nodes, and the presence of spatio-temporal autocorrelation increase the complexity of the task. Existing forecasting methods are unable to address these challenges in a combined manner, resulting in a suboptimal model accuracy. In this article, we propose GAP-LSTM, a novel geo-distributed forecasting method that leverages the synergic interaction of graph convolution, attention-based long short-term memory (LSTM), 2-D-convolution, and latent memory states to effectively exploit spatio-temporal autocorrelation in multivariate data generated by multiple nodes, resulting in improved modeling capabilities. Our extensive evaluation, involving real-world datasets on traffic, energy, and pollution domains, showcases the ability of our method to outperform state-of-the-art forecasting methods. An ablation study confirms that all method components provide a positive contribution to the accuracy of the extracted forecasts. The method also provides an interpretable visualization that complements forecasts with additional insights for domain experts.},
  archive      = {J_TNNLS},
  author       = {Massimiliano Altieri and Roberto Corizzo and Michelangelo Ceci},
  doi          = {10.1109/TNNLS.2024.3398441},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11773-11787},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GAP-LSTM: Graph-based autocorrelation preserving networks for geo-distributed forecasting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed explainable continual learning on graphs.
<em>TNNLS</em>, <em>35</em>(9), 11761–11772. (<a
href="https://doi.org/10.1109/TNNLS.2023.3347453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graph learning has attracted great attention with its ability to deal with dynamic graphs. Although current methods are reasonably accurate, most of them are unexplainable due to their black-box nature. It remains a challenge to explain how temporal graph learning models adapt to information evolution. Furthermore, with the increasing application of artificial intelligence in various scientific domains, such as chemistry and biomedicine, the importance of delivering not only precise outcomes but also offering explanations regarding the learning models becomes paramount. This transparency aids users in comprehending the decision-making procedures and instills greater confidence in the generated models. To address this issue, this article proposes a novel physics-informed explainable continual learning (PiECL), focusing on temporal graphs. Our proposed method utilizes physical and mathematical algorithms to quantify the disturbance of new data to previous knowledge for obtaining changed information over time. As the proposed model is based on theories in physics, it can provide a transparent underlying mechanism for information evolution detection, thus enhancing explainability. The experimental results on three real-world datasets demonstrate that PiECL can explain the learning process, and the generated model outperforms other state-of-the-art methods. PiECL shows tremendous potential for explaining temporal graph learning in various scientific contexts.},
  archive      = {J_TNNLS},
  author       = {Ciyuan Peng and Tao Tang and Qiuyang Yin and Xiaomei Bai and Suryani Lim and Charu C. Aggarwal},
  doi          = {10.1109/TNNLS.2023.3347453},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11761-11772},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Physics-informed explainable continual learning on graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A graph-neural-network-powered solver framework for graph
optimization problems. <em>TNNLS</em>, <em>35</em>(9), 11746–11760. (<a
href="https://doi.org/10.1109/TNNLS.2024.3397926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backtracking combined with branching heuristics is a prevalent approach for tackling constraint satisfaction problems (CSPs) and combinatorial optimization problems (COPs). While branching heuristics specifically designed for certain problems can be theoretically efficient, they are often complex and difficult to implement in practice. On the other hand, general branching heuristics can be applied across various problems, but at the risk of suboptimality. We introduce a solver framework that leverages the Shannon entropy in branching heuristics to bridge the gap between generality and specificity in branching heuristics. This enables backtracking to follow the path of least uncertainty, based on probability distributions that conform to problem constraints. We employ graph neural network (GNN) models with loss functions derived from the probabilistic method to learn these probability distributions. We have evaluated our approach by its applications to two NP-hard problems: the (minimum) dominating-clique problem and the edge-clique-cover problem. Compared with the state-of-the-art solvers for both problems, our solver framework outputs competitive results. Specifically, for the (minimum) dominating-clique problem, our approach generates fewer branches than the solver presented by Culberson et al. (2005). For the edge-clique-cover problem, our approach produces smaller-sized edge clique covers (ECCs) than the solvers referenced by Conte et al. (2020) and Kellerman (1973).},
  archive      = {J_TNNLS},
  author       = {Congsong Zhang and Yong Gao and James Nastos},
  doi          = {10.1109/TNNLS.2024.3397926},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11746-11760},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A graph-neural-network-powered solver framework for graph optimization problems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gegenbauer graph neural networks for time-varying signal
reconstruction. <em>TNNLS</em>, <em>35</em>(9), 11734–11745. (<a
href="https://doi.org/10.1109/TNNLS.2024.3381069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques that have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand the complexity of the model and offer a more accurate solution for recovering time-varying graph signals. Building upon GegenConv, we design the Gegenbauer-based time graph neural network (GegenGNN) architecture, which adopts an encoder–decoder structure. Likewise, our approach also uses a dedicated loss function that incorporates a mean squared error (MSE) component alongside Sobolev smoothness regularization. This combination enables GegenGNN to capture both the fidelity to ground truth and the underlying smoothness properties of the signals, enhancing the reconstruction performance. We conduct extensive experiments on real datasets to evaluate the effectiveness of our proposed approach. The experimental results demonstrate that GegenGNN outperforms state-of-the-art methods, showcasing its superior capability in recovering time-varying graph signals.},
  archive      = {J_TNNLS},
  author       = {Jhon A. Castro-Correa and Jhony H. Giraldo and Mohsen Badiey and Fragkiskos D. Malliaros},
  doi          = {10.1109/TNNLS.2024.3381069},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11734-11745},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gegenbauer graph neural networks for time-varying signal reconstruction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Refining euclidean obfuscatory nodes helps: A joint-space
graph learning method for graph neural networks. <em>TNNLS</em>,
<em>35</em>(9), 11720–11733. (<a
href="https://doi.org/10.1109/TNNLS.2024.3405898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many graph neural networks (GNNs) are inapplicable when the graph structure representing the node relations is unavailable. Recent studies have shown that this problem can be effectively solved by jointly learning the graph structure and the parameters of GNNs. However, most of these methods learn graphs by using either a Euclidean or hyperbolic metric, which means that the space curvature is assumed to be either constant zero or constant negative. Graph embedding spaces usually have nonconstant curvatures, and thus, such an assumption may produce some obfuscatory nodes, which are improperly embedded and close to multiple categories. In this article, we propose a joint-space graph learning (JSGL) method for GNNs. JSGL learns a graph based on Euclidean embeddings and identifies Euclidean obfuscatory nodes. Then, the graph topology near the identified obfuscatory nodes is refined in hyperbolic space. We also present a theoretical justification of our method for identifying obfuscatory nodes and conduct a series of experiments to test the performance of JSGL. The results show that JSGL outperforms many baseline methods. To obtain more insights, we analyze potential reasons for this superior performance.},
  archive      = {J_TNNLS},
  author       = {Zhaogeng Liu and Feng Ji and Jielong Yang and Xiaofeng Cao and Muhan Zhang and Hechang Chen and Yi Chang},
  doi          = {10.1109/TNNLS.2024.3405898},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11720-11733},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Refining euclidean obfuscatory nodes helps: A joint-space graph learning method for graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motif-based contrastive learning for community detection.
<em>TNNLS</em>, <em>35</em>(9), 11706–11719. (<a
href="https://doi.org/10.1109/TNNLS.2024.3367873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection has become a prominent task in complex network analysis. However, most of the existing methods for community detection only focus on the lower order structure at the level of individual nodes and edges and ignore the higher order connectivity patterns that characterize the fundamental building blocks within the network. In recent years, researchers have shown interest in motifs and their role in network analysis. However, most of the existing higher order approaches are based on shallow methods, failing to capture the intricate nonlinear relationships between nodes. In order to better fuse higher order and lower order structural information, a novel deep learning framework called motif-based contrastive learning for community detection (MotifCC) is proposed. First, a higher order network is constructed based on motifs. Subnetworks are then obtained by removing isolated nodes, addressing the fragmentation issue in the higher order network. Next, the concept of contrastive learning is applied to effectively fuse various kinds of information from nodes, edges, and higher order and lower order structures. This aims to maximize the similarity of corresponding node information, while distinguishing different nodes and different communities. Finally, based on the community structure of subnetworks, the community labels of all nodes are obtained by using the idea of label propagation. Extensive experiments on real-world datasets validate the effectiveness of MotifCC.},
  archive      = {J_TNNLS},
  author       = {Xunxun Wu and Chang-Dong Wang and Jia-Qi Lin and Wu-Dong Xi and Philip S. Yu},
  doi          = {10.1109/TNNLS.2024.3367873},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11706-11719},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Motif-based contrastive learning for community detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graph alignment under scarce supervision: A
general framework with active cross-view contrastive learning.
<em>TNNLS</em>, <em>35</em>(9), 11692–11705. (<a
href="https://doi.org/10.1109/TNNLS.2023.3321900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over recent years, a number of knowledge graphs (KGs) have emerged. Nevertheless, a KG can never reach full completeness. A viable approach to increase the coverage of a KG is KG alignment (KGA). The majority of previous efforts merely focus on the matching between entities, while largely neglect relations. Besides, they heavily rely on labeled data, which are difficult to obtain in practice. To address these issues, in this work, we put forward a general framework to simultaneously align entities and relations under scarce supervision. Our proposal consists of two main components, relation-enhanced active instance selection (RAS), and cross-view contrastive learning (CCL). RAS aims to select the most valuable instances to be labeled with the guidance of relations, while CCL contrasts cross-view representations to augment scarce supervision signals. Our proposal is agnostic to the underlying entity and relation alignment models, and can be used to improve their performance under limited supervision. We conduct experiments on a wide range of popular KG pairs, and the results demonstrate that our proposed model and its components can consistently boost the alignment performance under scarce supervision.},
  archive      = {J_TNNLS},
  author       = {Weixin Zeng and Xiang Zhao and Jiuyang Tang and Changjun Fan},
  doi          = {10.1109/TNNLS.2023.3321900},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11692-11705},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge graph alignment under scarce supervision: A general framework with active cross-view contrastive learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Affinity uncertainty-based hard negative mining in graph
contrastive learning. <em>TNNLS</em>, <em>35</em>(9), 11681–11691. (<a
href="https://doi.org/10.1109/TNNLS.2023.3339770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hard negative mining has shown effective in enhancing self-supervised contrastive learning (CL) on diverse data types, including graph CL (GCL). The existing hardness-aware CL methods typically treat negative instances that are most similar to the anchor instance as hard negatives, which helps improve the CL performance, especially on image data. However, this approach often fails to identify the hard negatives but leads to many false negatives on graph data. This is mainly due to that the learned graph representations are not sufficiently discriminative due to oversmooth representations and/or non-independent and identically distributed (non-i.i.d.) issues in graph data. To tackle this problem, this article proposes a novel approach that builds a discriminative model on collective affinity information (i.e., two sets of pairwise affinities between the negative instances and the anchor instance) to mine hard negatives in GCL. In particular, the proposed approach evaluates how confident/uncertain the discriminative model is about the affinity of each negative instance to an anchor instance to determine its hardness weight relative to the anchor instance. This uncertainty information is then incorporated into the existing GCL loss functions via a weighting term to enhance their performance. The enhanced GCL is theoretically grounded that the resulting GCL loss is equivalent to a triplet loss with an adaptive margin being exponentially proportional to the learned uncertainty of each negative instance. Extensive experiments on ten graph datasets show that our approach does the following: 1) consistently enhances different state-of-the-art (SOTA) GCL methods in both graph and node classification tasks and 2) significantly improves their robustness against adversarial attacks. Code is available at https://github.com/mala-lab/AUGCL .},
  archive      = {J_TNNLS},
  author       = {Chaoxi Niu and Guansong Pang and Ling Chen},
  doi          = {10.1109/TNNLS.2023.3339770},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11681-11691},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Affinity uncertainty-based hard negative mining in graph contrastive learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph embedded intuitionistic fuzzy random vector functional
link neural network for class imbalance learning. <em>TNNLS</em>,
<em>35</em>(9), 11671–11680. (<a
href="https://doi.org/10.1109/TNNLS.2024.3353531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The domain of machine learning is confronted with a crucial research area known as class imbalance (CI) learning, which presents considerable hurdles in the precise classification of minority classes. This issue can result in biased models where the majority class takes precedence in the training process, leading to the underrepresentation of the minority class. The random vector functional link (RVFL) network is a widely used and effective learning model for classification due to its good generalization performance and efficiency. However, it suffers when dealing with imbalanced datasets. To overcome this limitation, we propose a novel graph-embedded intuitionistic fuzzy RVFL for CI learning (GE-IFRVFL-CIL) model incorporating a weighting mechanism to handle imbalanced datasets. The proposed GE-IFRVFL-CIL model offers a plethora of benefits: 1) leveraging graph embedding (GE) to preserve the inherent topological structure of the datasets; 2) employing intuitionistic fuzzy (IF) theory to handle uncertainty and imprecision in the data; and 3) the most important, it tackles CI learning. The amalgamation of a weighting scheme, GE, and IF sets leads to the superior performance of the proposed models on KEEL benchmark imbalanced datasets with and without Gaussian noise. Furthermore, we implemented the proposed GE-IFRVFL-CIL on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset and achieved promising results, demonstrating the model’s effectiveness in real-world applications. The proposed GE-IFRVFL-CIL model offers a promising solution to address the CI issue, mitigates the detrimental effect of noise and outliers, and preserves the inherent geometrical structures of the dataset.},
  archive      = {J_TNNLS},
  author       = {M. A. Ganaie and M. Sajid and A. K. Malik and M. Tanveer},
  doi          = {10.1109/TNNLS.2024.3353531},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11671-11680},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph embedded intuitionistic fuzzy random vector functional link neural network for class imbalance learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward robust graph semi-supervised learning against extreme
data scarcity. <em>TNNLS</em>, <em>35</em>(9), 11661–11670. (<a
href="https://doi.org/10.1109/TNNLS.2024.3351938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of graph neural networks (GNNs) in graph-based web mining highly relies on abundant human-annotated data, which is laborious to obtain in practice. When only a few labeled nodes are available, how to improve their robustness is key to achieving replicable and sustainable graph semi-supervised learning. Though self-training is powerful for semi-supervised learning, its application on graph-structured data may fail because 1) larger receptive fields are not leveraged to capture long-range node interactions, which exacerbates the difficulty of propagating feature-label patterns from labeled nodes to unlabeled nodes and 2) limited labeled data makes it challenging to learn well-separated decision boundaries for different node classes without explicitly capturing the underlying semantic structure. To address the challenges of capturing informative structural and semantic knowledge, we propose a new graph data augmentation framework, augmented graph self-training (AGST), which is built with two new (i.e., structural and semantic) augmentation modules on top of a decoupled GST backbone. In this work, we investigate whether this novel framework can learn a robust graph predictive model under the low-data context. We conduct comprehensive evaluations on semi-supervised node classification under different scenarios of limited labeled-node data. The experimental results demonstrate the unique contributions of the novel data augmentation framework for node classification with few labeled data.},
  archive      = {J_TNNLS},
  author       = {Kaize Ding and Elnaz Nouri and Guoqing Zheng and Huan Liu and Ryen White},
  doi          = {10.1109/TNNLS.2024.3351938},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11661-11670},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward robust graph semi-supervised learning against extreme data scarcity},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MARML: Motif-aware deep representation learning in
multilayer networks. <em>TNNLS</em>, <em>35</em>(9), 11649–11660. (<a
href="https://doi.org/10.1109/TNNLS.2023.3341347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid increase in high-throughput, complex, and heterogeneous data has led to the adoption of network-structured models and analyses for interpretation. However, these data are inherently complex and challenging to understand, prompting researchers to turn to graph embedding methods to facilitate analysis. While general network embedding techniques have shown promise in improving downstream prediction and classification tasks, real-world data are complicated due to cross-domain interactions between different types of entities. Multilayered networks have been successful in integrating biological data to represent biological systems’ hierarchy, but embedding nodes based on different types of interactions remains an unsolved problem. To address this challenge, we propose the Motif-aware deep representation learning in multilayer (MARML) networks for learning network representations. Our method considers recurring motif patterns, topological information, and attributive information from other sources as node features. We validated the MARML method using various multilayer network datasets. In addition, by incorporating motif information, MARML considers higher order connections across different hierarchies. The learned features exhibited excellent accuracy in tasks related to link prediction and link differentiation, enabling us to distinguish between existing and disconnected triplets. Through the integration of both intrinsic node attributes and topological network structures, we enhance our understanding of complex biological systems.},
  archive      = {J_TNNLS},
  author       = {Da Zhang and Mansur R. Kabuka},
  doi          = {10.1109/TNNLS.2023.3341347},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11649-11660},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MARML: Motif-aware deep representation learning in multilayer networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Permutation equivariant graph framelets for heterophilous
graph learning. <em>TNNLS</em>, <em>35</em>(9), 11634–11648. (<a
href="https://doi.org/10.1109/TNNLS.2024.3370918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nature of heterophilous graphs is significantly different from that of homophilous graphs, which causes difficulties in early graph neural network (GNN) models and suggests aggregations beyond the one-hop neighborhood. In this article, we develop a new way to implement multiscale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further design a graph framelet neural network model permutation equivariant graph framelet augmented network (PEGFAN) based on our constructed graph framelets. The experiments are conducted on a synthetic dataset and nine benchmark datasets to compare the performance with other state-of-the-art models. The result shows that our model can achieve the best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.},
  archive      = {J_TNNLS},
  author       = {Jianfei Li and Ruigang Zheng and Han Feng and Ming Li and Xiaosheng Zhuang},
  doi          = {10.1109/TNNLS.2024.3370918},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11634-11648},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Permutation equivariant graph framelets for heterophilous graph learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Special issue on graph learning.
<em>TNNLS</em>, <em>35</em>(9), 11630–11633. (<a
href="https://doi.org/10.1109/TNNLS.2024.3427528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TNNLS},
  author       = {Feng Xia and Renaud Lambiotte and Neil Shah and Hanghang Tong and Irwin King},
  doi          = {10.1109/TNNLS.2024.3427528},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {9},
  number       = {9},
  pages        = {11630-11633},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial: Special issue on graph learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Central-smoothing hypergraph neural networks for predicting
drug–drug interactions. <em>TNNLS</em>, <em>35</em>(8), 11620–11625. (<a
href="https://doi.org/10.1109/TNNLS.2023.3261860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting drug–drug interactions (DDIs) is the problem of predicting side effects (unwanted outcomes) of a pair of drugs using drug information and known side effects of many pairs. This problem can be formulated as predicting labels (i.e., side effects) for each pair of nodes in a DDI graph, of which nodes are drugs and edges are interacting drugs with known labels. State-of-the-art methods for this problem are graph neural networks (GNNs), which leverage neighborhood information in the graph to learn node representations. For DDI, however, there are many labels with complicated relationships due to the nature of side effects. Usual GNNs often fix labels as one-hot vectors that do not reflect label relationships and potentially do not obtain the highest performance in the difficult cases of infrequent labels. In this brief, we formulate DDI as a hypergraph where each hyperedge is a triple: two nodes for drugs and one node for a label. We then present CentSmoothie, a hypergraph neural network (HGNN) that learns representations of nodes and labels altogether with a novel “ central-smoothing ” formulation. We empirically demonstrate the performance advantages of CentSmoothie in simulations as well as real datasets.},
  archive      = {J_TNNLS},
  author       = {Duc Anh Nguyen and Canh Hao Nguyen and Hiroshi Mamitsuka},
  doi          = {10.1109/TNNLS.2023.3261860},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11620-11625},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Central-smoothing hypergraph neural networks for predicting Drug–Drug interactions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized h2 control for discrete-time networked systems
with positivity constraint. <em>TNNLS</em>, <em>35</em>(8), 11613–11619.
(<a href="https://doi.org/10.1109/TNNLS.2023.3248682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we study the decentralized $H_{2}$ state-feedback control problem for networked discrete-time systems with positivity constraint. This problem (for a single positive system), raised recently in the area of positive systems theory, is known to be challenging due to its inherent nonconvexity. In contrast to most works, which only provide sufficient synthesis conditions for a single positive system, we study this problem within a primal–dual scheme , in which necessary and sufficient synthesis conditions are proposed for networked positive systems. Based on the equivalent conditions, we develop a primal–dual iterative algorithm for solution, which helps prevent from converging to a local minimum. In the simulation, two illustrative examples are employed for verification of our proposed results.},
  archive      = {J_TNNLS},
  author       = {Jason J. R. Liu and Ka-Wai Kwok and James Lam},
  doi          = {10.1109/TNNLS.2023.3248682},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11613-11619},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized h2 control for discrete-time networked systems with positivity constraint},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The GroupMax neural network approximation of convex
functions. <em>TNNLS</em>, <em>35</em>(8), 11608–11612. (<a
href="https://doi.org/10.1109/TNNLS.2023.3240183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new neural network to approximate convex functions. This network has the particularity to approximate the function with cuts which is, for example, a necessary feature to approximate Bellman values when solving linear stochastic optimization problems. The network can be easily adapted to partial convexity. We give an universal approximation theorem in the full convex case and give many numerical results proving its efficiency. The network is competitive with the most efficient convexity-preserving neural networks and can be used to approximate functions in high dimensions.},
  archive      = {J_TNNLS},
  author       = {Xavier Warin},
  doi          = {10.1109/TNNLS.2023.3240183},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11608-11612},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The GroupMax neural network approximation of convex functions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CATRO: Channel pruning via class-aware trace ratio
optimization. <em>TNNLS</em>, <em>35</em>(8), 11595–11607. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks are shown to be overkill with high parametric and computational redundancy in many application scenarios, and an increasing number of works have explored model pruning to obtain lightweight and efficient networks. However, most existing pruning approaches are driven by empirical heuristics and rarely consider the joint impact of channels, leading to unguaranteed and suboptimal performance. In this article, we propose a novel channel pruning method via c lass-aware t race r atio o ptimization (CATRO) to reduce the computational burden and accelerate the model inference. Utilizing class information from a few samples, CATRO measures the joint impact of multiple channels by feature space discriminations and consolidates the layerwise impact of preserved channels. By formulating channel pruning as a submodular set function maximization problem, CATRO solves it efficiently via a two-stage greedy iterative optimization procedure. More importantly, we present theoretical justifications on convergence of CATRO and performance of pruned networks. Experimental results demonstrate that CATRO achieves higher accuracy with similar computation cost or lower computation cost with similar accuracy than other state-of-the-art channel pruning algorithms. In addition, because of its class-aware property, CATRO is suitable to prune efficient networks adaptively for various classification subtasks, enhancing handy deployment and usage of deep networks in real-world applications.},
  archive      = {J_TNNLS},
  author       = {Wenzheng Hu and Zhengping Che and Ning Liu and Mingyang Li and Jian Tang and Changshui Zhang and Jianqiang Wang},
  doi          = {10.1109/TNNLS.2023.3262952},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11595-11607},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CATRO: Channel pruning via class-aware trace ratio optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Position-aware relational transformer for knowledge graph
embedding. <em>TNNLS</em>, <em>35</em>(8), 11580–11594. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Transformer has achieved success in language and vision tasks, its capacity for knowledge graph (KG) embedding has not been fully exploited. Using the self-attention (SA) mechanism in Transformer to model the subject-relation-object triples in KGs suffers from training inconsistency as SA is invariant to the order of input tokens. As a result, it is unable to distinguish a (real) relation triple from its shuffled (fake) variants (e.g., object-relation-subject) and, thus, fails to capture the correct semantics. To cope with this issue, we propose a novel Transformer architecture, namely, Knowformer, for KG embedding. It incorporates relational compositions in entity representations to explicitly inject semantics and capture the role of an entity based on its position (subject or object) in a relation triple. The relational composition for a subject (or object) entity of a relation triple refers to an operator on the relation and the object (or subject). We borrow ideas from the typical translational and semantic-matching embedding techniques to design relational compositions. We carefully design a residual block to integrate relational compositions into SA and efficiently propagate the composed relational semantics layer by layer. We formally prove that the SA with relational compositions is able to distinguish the entity roles in different positions and correctly capture relational semantics. Extensive experiments and analyses on six benchmark datasets show that Knowformer achieves state-of-the-art performance on both link prediction and entity alignment.},
  archive      = {J_TNNLS},
  author       = {Guangyao Li and Zequn Sun and Wei Hu and Gong Cheng and Yuzhong Qu},
  doi          = {10.1109/TNNLS.2023.3262937},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11580-11594},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Position-aware relational transformer for knowledge graph embedding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network-based fixed-time tracking and containment
control of second-order heterogeneous nonlinear multiagent systems.
<em>TNNLS</em>, <em>35</em>(8), 11565–11579. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study concentrates on the fixed-time tracking consensus and containment control of second-order heterogeneous nonlinear multiagent systems (MASs) with and without measurable velocity under directed topology. By defining a time-varying scaling function and approximating the unknown nonlinear dynamics with radial basis function neural networks (RBFNNs), a novel distributed protocol for solving the fixed-time tracking consensus and containment control problems of second-order heterogeneous nonlinear MASs with full states available is proposed based on a nonsingular sliding-mode control method constructed by designing a prescribed-time convergent sliding surface. For the scenario of immeasurable velocity, a fixed-time convergent states’ observer is designed to reveal the velocity information when the unknown linearity is bounded. Subsequently, a distributed fixed-time consensus protocol based on observed velocity information is proposed for the extended results. Ultimately, the acquired results are verified by three simulation examples.},
  archive      = {J_TNNLS},
  author       = {Chongyang Chen and Yiyan Han and Song Zhu and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2023.3262925},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11565-11579},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based fixed-time tracking and containment control of second-order heterogeneous nonlinear multiagent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning multi-agent cooperation via considering actions of
teammates. <em>TNNLS</em>, <em>35</em>(8), 11553–11564. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently value-based centralized training with decentralized execution (CTDE) multi-agent reinforcement learning (MARL) methods have achieved excellent performance in cooperative tasks. However, the most representative method among these methods, Q-network MIXing (QMIX), restricts the joint action $Q$ values to be a monotonic mixing of each agent’s utilities. Furthermore, current methods cannot generalize to unseen environments or different agent configurations, which is known as ad hoc team play situation. In this work, we propose a novel $Q$ values decomposition that considers both the return of an agent acting on its own and cooperating with other observable agents to address the nonmonotonic problem. Based on the decomposition, we propose a greedy action searching method that can improve exploration and is not affected by changes in observable agents or changes in the order of agents’ actions. In this way, our method can adapt to ad hoc team play situation. Furthermore, we utilize an auxiliary loss related to environmental cognition consistency and a modified prioritized experience replay (PER) buffer to assist training. Our extensive experimental results show that our method achieves significant performance improvements in both challenging monotonic and nonmonotonic domains, and can handle the ad hoc team play situation perfectly.},
  archive      = {J_TNNLS},
  author       = {Shanqi Liu and Weiwei Liu and Wenzhou Chen and Guanzhong Tian and Jun Chen and Yao Tong and Junjie Cao and Yong Liu},
  doi          = {10.1109/TNNLS.2023.3262921},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11553-11564},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning multi-agent cooperation via considering actions of teammates},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering ancestral instrumental variables for causal
inference from observational data. <em>TNNLS</em>, <em>35</em>(8),
11542–11552. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrumental variable (IV) is a powerful approach to inferring the causal effect of a treatment on an outcome of interest from observational data even when there exist latent confounders between the treatment and the outcome. However, existing IV methods require that an IV is selected and justified with domain knowledge. An invalid IV may lead to biased estimates. Hence, discovering a valid IV is critical to the applications of IV methods. In this article, we study and design a data-driven algorithm to discover valid IVs from data under mild assumptions. We develop the theory based on partial ancestral graphs (PAGs) to support the search for a set of candidate ancestral IVs (AIVs), and for each possible AIV, the identification of its conditioning set. Based on the theory, we propose a data-driven algorithm to discover a pair of IVs from data. The experiments on synthetic and real-world datasets show that the developed IV discovery algorithm estimates accurate estimates of causal effects in comparison with the state-of-the-art IV-based causal effect estimators.},
  archive      = {J_TNNLS},
  author       = {Debo Cheng and Jiuyong Li and Lin Liu and Kui Yu and Thuc Duy Le and Jixue Liu},
  doi          = {10.1109/TNNLS.2023.3262848},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11542-11552},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discovering ancestral instrumental variables for causal inference from observational data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Neural observer with lyapunov stability guarantee for
uncertain nonlinear systems. <em>TNNLS</em>, <em>35</em>(8),
11527–11541. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel nonlinear observer based on neural networks (NNs), called neural observers, for observation tasks of linear time-invariant (LTI) systems and uncertain nonlinear systems. In particular, the neural observer designed for uncertain systems is inspired by the active disturbance rejection control, which can measure the uncertainty in real time. The stability analysis (e.g., exponential convergence rate) of LTI and uncertain nonlinear systems (involving neural observers) are presented and guaranteed, where it is shown that the observation problems can be solved only using the linear matrix inequalities (LMIs). Also, it is revealed that the observability and controllability of the system matrices are required to demonstrate the existence of solutions for LMIs. Finally, the effectiveness of neural observers is verified in three simulation cases, including the X-29A aircraft model, the nonlinear pendulum, and the four-wheel steering vehicle.},
  archive      = {J_TNNLS},
  author       = {Song Chen and Shengze Cai and Tehuan Chen and Chao Xu and Jian Chu},
  doi          = {10.1109/TNNLS.2023.3262820},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11527-11541},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural observer with lyapunov stability guarantee for uncertain nonlinear systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Cross-scene joint classification of multisource data with
multilevel domain adaption network. <em>TNNLS</em>, <em>35</em>(8),
11514–11526. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaption (DA) is a challenging task that integrates knowledge from source domain (SD) to perform data analysis for target domain. Most of the existing DA approaches only focus on single-source-single-target setting. In contrast, multisource (MS) data collaborative utilization has been extensively used in various applications, while how to integrate DA with MS collaboration still faces great challenges. In this article, we propose a multilevel DA network (MDA-NET) for promoting information collaboration and cross-scene (CS) classification based on hyperspectral image (HSI) and light detection and ranging (LiDAR) data. In this framework, modality-related adapters are built, and then a mutual-aid classifier is used to aggregate all the discriminative information captured from different modalities for boosting CS classification performance. Experimental results on two cross-domain datasets show that the proposed method consistently provides better performance than other state-of-the-art DA approaches.},
  archive      = {J_TNNLS},
  author       = {Mengmeng Zhang and Xudong Zhao and Wei Li and Yuxiang Zhang and Ran Tao and Qian Du},
  doi          = {10.1109/TNNLS.2023.3262599},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11514-11526},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cross-scene joint classification of multisource data with multilevel domain adaption network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-stage shifted laplacian refining for multiple kernel
clustering. <em>TNNLS</em>, <em>35</em>(8), 11501–11513. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph learning can effectively characterize the similarity structure of sample pairs, hence multiple kernel clustering based on graph learning (MKC-GL) achieves promising results on nonlinear clustering tasks. However, previous methods confine to a “three-stage” scheme, that is, affinity graph learning, Laplacian construction, and clustering indicator extracting, which results in the information distortion in the step alternating. Meanwhile, the energy of Laplacian reconstruction and the necessary cluster information cannot be preserved simultaneously. To address these problems, we propose a one-stage shifted Laplacian refining (OSLR) method for multiple kernel clustering (MKC), where using the “one-stage” scheme focuses on Laplacian learning rather than traditional graph learning. Concretely, our method treats each kernel matrix as an affinity graph rather than ordinary data and constructs its corresponding Laplacian matrix in advance. Compared to the traditional Laplacian methods, we transform each Laplacian to an approximately shifted Laplacian (ASL) for refining a consensus Laplacian. Then, we project the consensus Laplacian onto a Fantope space to ensure that reconstruction information and clustering information concentrate on larger eigenvalues. Theoretically, our OSLR reduces the memory complexity and computation complexity to $O(n)$ and $O(n^{2})$ , respectively. Moreover, experimental results have shown that it outperforms state-of-the-art MKC methods on multiple benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Jiali You and Zhenwen Ren and F. Richard Yu and Xiaojian You},
  doi          = {10.1109/TNNLS.2023.3262590},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11501-11513},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {One-stage shifted laplacian refining for multiple kernel clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PlaneSeg: Building a plug-in for boosting planar region
segmentation. <em>TNNLS</em>, <em>35</em>(8), 11486–11500. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods in planar region segmentation suffer the problems of vague boundaries and failure to detect small-sized regions. To address these, this study presents an end-to-end framework, named PlaneSeg, which can be easily integrated into various plane segmentation models. Specifically, PlaneSeg contains three modules, namely, the edge feature extraction module, the multiscale module, and the resolution-adaptation module. First, the edge feature extraction module produces edge-aware feature maps for finer segmentation boundaries. The learned edge information acts as a constraint to mitigate inaccurate boundaries. Second, the multiscale module combines feature maps of different layers to harvest spatial and semantic information from planar objects. The multiformity of object information can help recognize small-sized objects to produce more accurate segmentation results. Third, the resolution-adaptation module fuses the feature maps produced by the two aforementioned modules. For this module, a pairwise feature fusion is adopted to resample the dropped pixels and extract more detailed features. Extensive experiments demonstrate that PlaneSeg outperforms other state-of-the-art approaches on three downstream tasks, including plane segmentation, 3-D plane reconstruction, and depth prediction. Code is available at https://github.com/nku-zhichengzhang/PlaneSeg .},
  archive      = {J_TNNLS},
  author       = {Zhicheng Zhang and Song Chen and Zichuan Wang and Jufeng Yang},
  doi          = {10.1109/TNNLS.2023.3262544},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11486-11500},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PlaneSeg: Building a plug-in for boosting planar region segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Bilaterally normalized scale-consistent sinkhorn distance
for few-shot image classification. <em>TNNLS</em>, <em>35</em>(8),
11475–11485. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot image classification aims at exploring transferable features from base classes to recognize images of the unseen novel classes with only a few labeled images. Existing methods usually compare the support features and query features, which are implemented by either matching the global feature vectors or matching the local feature maps at the same position. However, few labeled images fail to capture all the diverse context and intraclass variations, leading to mismatch issues for existing methods. On one hand, due to the misaligned position and cluttered background, existing methods suffer from the object mismatch issue. On the other hand, due to the scale inconsistency between images, existing methods suffer from the scale mismatch issue. In this article, we propose the bilaterally normalized scale-consistent Sinkhorn distance (BSSD) to solve these issues. First, instead of same-position matching, we use the Sinkhorn distance to find an optimal matching between images, mitigating the object mismatch caused by misaligned position. Meanwhile, we propose the intraimage and interimage attentions as the bilateral normalization on the Sinkhorn distance to suppress the object mismatch caused by background clutter. Second, local feature maps are enhanced with the multiscale pooling strategy, making the Sinkhorn distance possible to find a consistent matching scale between images. Experimental results show the effectiveness of the proposed approach, and we achieve the state-of-the-art on three few-shot benchmarks.},
  archive      = {J_TNNLS},
  author       = {Yanbin Liu and Linchao Zhu and Xiaohan Wang and Makoto Yamada and Yi Yang},
  doi          = {10.1109/TNNLS.2023.3262351},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11475-11485},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bilaterally normalized scale-consistent sinkhorn distance for few-shot image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Output regularization with cluster-based soft targets.
<em>TNNLS</em>, <em>35</em>(8), 11463–11474. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While supervised learning of over-parameterized neural networks achieved state-of-the-art performance in image classification, it tends to over-fit the labeled training samples to give inferior generalization ability. Output regularization deals with over-fitting by using soft targets as additional training signals. Although clustering is one of the most fundamental data analysis tools for discovering general-purpose and data-driven structures, it has been ignored in existing output regularization approaches. In this article, we leverage this underlying structural information by proposing Cluster-based soft targets for Output Regularization (CluOReg). This approach provides a unified way for simultaneous clustering in embedding space and neural classifier training with cluster-based soft targets via output regularization. By explicitly calculating a class relationship matrix in the cluster space, we obtain classwise soft targets shared by all samples in each class. Results of image classification experiments under various settings on a number of benchmark datasets are provided. Without resorting to external models or designed data augmentation, we get consistent and significant reductions in classification error compared with other approaches, demonstrating that cluster-based soft targets effectively complement the ground-truth label.},
  archive      = {J_TNNLS},
  author       = {Jian-Ping Mei and Wenhao Qiu and Defang Chen and Rui Yan and Jing Fan},
  doi          = {10.1109/TNNLS.2023.3262267},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11463-11474},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Output regularization with cluster-based soft targets},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). M2DCapsN: Multimodal, multichannel, and dual-step capsule
network for natural language moment localization. <em>TNNLS</em>,
<em>35</em>(8), 11448–11462. (<a
href="https://doi.org/10.1109/TNNLS.2023.3261927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language moment localization aims to localize the target moment that matches a given natural language query in an untrimmed video. The key to this challenging task is to capture fine-grained video-language correlations to establish the alignment between the query and target moment. Most existing works establish a single-pass interaction schema to capture correlations between queries and moments. Considering the complex feature space of lengthy video and diverse information between frames, the weight distribution of information interaction flow is prone to dispersion or misalignment, which leads to redundant information flow affecting the final prediction. We address this issue by proposing a capsule-based approach to model the query–video interactions, termed the Multimodal, Multichannel, and Dual-step Capsule Network ( $\text{M}^{2}$ DCapsN), which is derived from the intuition that “multiple people viewing multiple times is better than one person viewing one time.” First, we introduce a multimodal capsule network, replacing the single-pass interaction schema of “one person viewing one time” with the iterative interaction schema of “one person viewing multiple times,” which cyclically updates cross-modal interactions and modifies potential redundant interactions via its routing-by-agreement. Then, considering that the conventional routing mechanism only learns a single iterative interaction schema, we further propose a multichannel dynamic routing mechanism to learn multiple iterative interaction schemas, where each channel performs independent routing iteration to collectively capture cross-modal correlations from multiple subspaces, that is, “multiple people viewing.” Moreover, we design a dual-step capsule network structure based on the multimodal, multichannel capsule network, bringing together the query and query-guided key moments to jointly enhance the original video, so as to select the target moments according to the enhanced part. Experimental results on three public datasets demonstrate the superiority of our approach in comparison with state-of-the-art methods, and comprehensive ablation and visualization analysis validate the effectiveness of each component of the proposed model.},
  archive      = {J_TNNLS},
  author       = {Nayu Liu and Xian Sun and Hongfeng Yu and Fanglong Yao and Guangluan Xu and Kun Fu},
  doi          = {10.1109/TNNLS.2023.3261927},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11448-11462},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {M2DCapsN: Multimodal, multichannel, and dual-step capsule network for natural language moment localization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient multi-view clustering via unified and discrete
bipartite graph learning. <em>TNNLS</em>, <em>35</em>(8), 11436–11447.
(<a href="https://doi.org/10.1109/TNNLS.2023.3261460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although previous graph-based multi-view clustering (MVC) algorithms have gained significant progress, most of them are still faced with three limitations. First, they often suffer from high computational complexity, which restricts their applications in large-scale scenarios. Second, they usually perform graph learning either at the single-view level or at the view–consensus level, but often neglect the possibility of the joint learning of single-view and consensus graphs. Third, many of them rely on the $k$ -means for discretization of the spectral embeddings, which lack the ability to directly learn the graph with discrete cluster structure. In light of this, this article presents an efficient MVC approach via $\boldsymbol {u}$ nified and $\boldsymbol {d}$ iscrete $\boldsymbol {b}$ ipartite $\boldsymbol {g}$ raph $\boldsymbol {l}$ earning (UDBGL). Specifically, the anchor-based subspace learning is incorporated to learn the view-specific bipartite graphs from multiple views, upon which the bipartite graph fusion is leveraged to learn a view–consensus bipartite graph with adaptive weight learning. Furthermore, the Laplacian rank constraint is imposed to ensure that the fused bipartite graph has discrete cluster structures (with a specific number of connected components). By simultaneously formulating the view-specific bipartite graph learning, the view–consensus bipartite graph learning, and the discrete cluster structure learning into a unified objective function, an efficient minimization algorithm is then designed to tackle this optimization problem and directly achieve a discrete clustering solution without requiring additional partitioning, which notably has linear time complexity in data size. Experiments on a variety of multi-view datasets demonstrate the robustness and efficiency of our UDBGL approach. The code is available at https://github.com/huangdonghere/UDBGL .},
  archive      = {J_TNNLS},
  author       = {Si-Guo Fang and Dong Huang and Xiao-Sha Cai and Chang-Dong Wang and Chaobo He and Yong Tang},
  doi          = {10.1109/TNNLS.2023.3261460},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11436-11447},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient multi-view clustering via unified and discrete bipartite graph learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Fusion PCAM r-CNN of automatic segmentation for magnetic
flux leakage defects. <em>TNNLS</em>, <em>35</em>(8), 11424–11435. (<a
href="https://doi.org/10.1109/TNNLS.2023.3261363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic leakage detection technology plays an important role in the long-oil pipeline. Automatic segmentation of defecting images is crucial for the detection of magnetic flux leakage (MFL) works. At present, accurate segmentation for small defects has always been a difficult problem. In contrast to the state-of-the-art MFL detection methodologies based on convolution neural network (CNN), an optimization method is devised in our study by integrating mask region-based CNN (Mask R-CNN) and information entropy constraint (IEC). To be precise, the principal component analysis (PCA) is utilized to improve the feature learning and network segmentation ability of the convolution kernel. The similarity constraint rule of information entropy is proposed to be inserted into the convolution layer in the Mask R-CNN network. The Mask R-CNN optimizes the convolutional kernel with similar weights or higher similarity, meanwhile, the PCA network reduces the dimension of the feature image to reconstruct the original feature vector. As such, the feature extraction of MFL defects is optimized in the convolution check. The research results can be applied in the field of MFL detection.},
  archive      = {J_TNNLS},
  author       = {Zhujun Wang and Lijian Yang and Tianhe Sun and Weizhe Yan},
  doi          = {10.1109/TNNLS.2023.3261363},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11424-11435},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fusion PCAM R-CNN of automatic segmentation for magnetic flux leakage defects},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HPRN: Holistic prior-embedded relation network for spectral
super-resolution. <em>TNNLS</em>, <em>35</em>(8), 11409–11423. (<a
href="https://doi.org/10.1109/TNNLS.2023.3260828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral super-resolution (SSR) refers to the hyperspectral image (HSI) recovery from an RGB counterpart. Due to the one-to-many nature of the SSR problem, a single RGB image can be reprojected to many HSIs. The key to tackle this ill-posed problem is to plug into multisource prior information such as the natural spatial context prior of RGB images, deep feature prior, or inherent statistical prior of HSIs so as to effectively alleviate the degree of ill-posedness. However, most current approaches only consider the general and limited priors in their customized convolutional neural networks (CNNs), which leads to the inability to guarantee the confidence and fidelity of reconstructed spectra. In this article, we propose a novel holistic prior-embedded relation network (HPRN) to integrate comprehensive priors to regularize and optimize the solution space of SSR. Basically, the core framework is delicately assembled by several multiresidual relation blocks (MRBs) that fully facilitate the transmission and utilization of the low-frequency content prior of RGBs. Innovatively, the semantic prior of RGB inputs is introduced to mark category attributes, and a semantic-driven spatial relation module (SSRM) is invented to perform the feature aggregation of clustered similar ranges for refining recovered characteristics. In addition, we develop a transformer-based channel relation module (TCRM), which breaks the habit of employing scalars as the descriptors of channelwise relations in the previous deep feature prior and replaces them with certain vectors to make the mapping function more robust and smoother. In order to maintain the mathematical correlation and spectral consistency between hyperspectral bands, the second-order prior constraints (SOPCs) are incorporated into the loss function to guide the HSI reconstruction. Finally, extensive experimental results on four benchmarks demonstrate that our HPRN can reach the state-of-the-art performance for SSR quantitatively and qualitatively. Furthermore, the effectiveness and usefulness of the reconstructed spectra are verified by the classification results on the remote sensing dataset. Codes are available at https://github.com/Deep-imagelab/HPRN .},
  archive      = {J_TNNLS},
  author       = {Chaoxiong Wu and Jiaojiao Li and Rui Song and Yunsong Li and Qian Du},
  doi          = {10.1109/TNNLS.2023.3260828},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11409-11423},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HPRN: Holistic prior-embedded relation network for spectral super-resolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep double incomplete multi-view multi-label learning with
incomplete labels and missing views. <em>TNNLS</em>, <em>35</em>(8),
11396–11408. (<a
href="https://doi.org/10.1109/TNNLS.2023.3260349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {View missing and label missing are two challenging problems in the applications of multi-view multi-label classification scenery. In the past years, many efforts have been made to address the incomplete multi-view learning or incomplete multi-label learning problem. However, few works can simultaneously handle the challenging case with both the incomplete issues. In this article, we propose a new incomplete multi-view multi-label learning network to address this challenging issue. The proposed method is composed of four major parts: view-specific deep feature extraction network, weighted representation fusion module, classification module, and view-specific deep decoder network. By, respectively, integrating the view missing information and label missing information into the weighted fusion module and classification module, the proposed method can effectively reduce the negative influence caused by two such incomplete issues and sufficiently explore the available data and label information to obtain the most discriminative feature extractor and classifier. Furthermore, our method can be trained in both supervised and semi-supervised manners, which has important implications for flexible deployment. Experimental results on five benchmarks in supervised and semi-supervised cases demonstrate that the proposed method can greatly enhance the classification performance on the difficult incomplete multi-view multi-label classification tasks with missing labels and missing views.},
  archive      = {J_TNNLS},
  author       = {Jie Wen and Chengliang Liu and Shijie Deng and Yicheng Liu and Lunke Fei and Ke Yan and Yong Xu},
  doi          = {10.1109/TNNLS.2023.3260349},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11396-11408},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep double incomplete multi-view multi-label learning with incomplete labels and missing views},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiview subspace clustering via low-rank symmetric
affinity graph. <em>TNNLS</em>, <em>35</em>(8), 11382–11395. (<a
href="https://doi.org/10.1109/TNNLS.2023.3260258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview subspace clustering (MVSC) has been used to explore the internal structure of multiview datasets by revealing unique information from different views. Most existing methods ignore the consistent information and angular information of different views. In this article, we propose a novel MVSC via low-rank symmetric affinity graph (LSGMC) to tackle these problems. Specifically, considering the consistent information, we pursue a consistent low-rank structure across views by decomposing the coefficient matrix into three factors. Then, the symmetry constraint is utilized to guarantee weight consistency for each pair of data samples. In addition, considering the angular information, we utilize the fusion mechanism to capture the inherent structure of data. Furthermore, to alleviate the effect brought by the noise and the high redundant data, the Schatten p-norm is employed to obtain a low-rank coefficient matrix. Finally, an adaptive information reduction strategy is designed to generate a high-quality similarity matrix for spectral clustering. Experimental results on 11 datasets demonstrate the superiority of LSGMC in clustering performance compared with ten state-of-the-art multiview clustering methods.},
  archive      = {J_TNNLS},
  author       = {Wei Lan and Tianchuan Yang and Qingfeng Chen and Shichao Zhang and Yi Dong and Huiyu Zhou and Yi Pan},
  doi          = {10.1109/TNNLS.2023.3260258},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11382-11395},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview subspace clustering via low-rank symmetric affinity graph},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-cell RNA-seq debiased clustering via batch effect
disentanglement. <em>TNNLS</em>, <em>35</em>(8), 11371–11381. (<a
href="https://doi.org/10.1109/TNNLS.2023.3260003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of single-cell RNA-seq (scRNA-seq) clustering methods has achieved great success in discovering cellular phenotypes. However, it remains challenging when the data confounds with batch effects brought by different experimental conditions or technologies. Namely, the data partitions would be biased toward these nonbiological factors. Meanwhile, the batch differences are not always much smaller than true biological variations, hindering the cooperation of batch integration and clustering methods. To overcome this challenge, we propose single-cell RNA-seq debiased clustering (SCDC), an end-to-end clustering method that is debiased toward batch effects by disentangling the biological and nonbiological information from scRNA-seq data during data partitioning. In six analyses, SCDC qualitatively and quantitatively outperforms both the state-of-the-art clustering and batch integration methods in handling scRNA-seq data with batch effects. Furthermore, SCDC clusters data with a linearly increasing running time with respect to cell numbers and a fixed graphics processing unit (GPU) memory consumption, making it scalable to large datasets. The code will be released on Github.},
  archive      = {J_TNNLS},
  author       = {Yunfan Li and Yijie Lin and Peng Hu and Dezhong Peng and Han Luo and Xi Peng},
  doi          = {10.1109/TNNLS.2023.3260003},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11371-11381},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Single-cell RNA-seq debiased clustering via batch effect disentanglement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-in-the-loop behavior modeling via an integral
concurrent adaptive inverse reinforcement learning. <em>TNNLS</em>,
<em>35</em>(8), 11359–11370. (<a
href="https://doi.org/10.1109/TNNLS.2023.3259581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One goal of artificial intelligence (AI) research is to teach machines how to learn from humans, such that they can perform a certain task in a natural human-like way. In this article, an online adaptive inverse reinforcement learning (IRL) approach to human behavior modeling is proposed to enhance machine intelligence for a class of linear human-in-the-loop (HiTL) systems using the state data only, where the human behavior is described by a linear quadratic optimal control model with an unknown weighting matrix for the quadratic cost function. First, an integral concurrent adaptive law is developed to learn the human feedback gain matrix online using the demonstrated state data only, which removes the persistent excitation (PE) conditions required by traditional adaptive estimation approaches and thus is more in line with real applications. Then, with the learned feedback gain matrix, the IRL problem is formulated as a linear matrix inequality (LMI) optimization problem, which can be efficiently solved to retrieve the weighting matrix of the human cost function. Finally, a simulation example is provided to illustrate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Huai-Ning Wu and Mi Wang},
  doi          = {10.1109/TNNLS.2023.3259581},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11359-11370},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Human-in-the-loop behavior modeling via an integral concurrent adaptive inverse reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ES-dRNN: A hybrid exponential smoothing and dilated
recurrent neural network model for short-term load forecasting.
<em>TNNLS</em>, <em>35</em>(8), 11346–11358. (<a
href="https://doi.org/10.1109/TNNLS.2023.3259149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term load forecasting (STLF) is challenging due to complex time series (TS) which express three seasonal patterns and a nonlinear trend. This article proposes a novel hybrid hierarchical deep-learning (DL) model that deals with multiple seasonality and produces both point forecasts and predictive intervals (PIs). It combines exponential smoothing (ES) and a recurrent neural network (RNN). ES extracts dynamically the main components of each individual TS and enables on-the-fly deseasonalization, which is particularly useful when operating on a relatively small dataset. A multilayer RNN is equipped with a new type of dilated recurrent cell designed to efficiently model both short and long-term dependencies in TS. To improve the internal TS representation and thus the model’s performance, RNN learns simultaneously both the ES parameters and the main mapping function transforming inputs into forecasts. We compare our approach against several baseline methods, including classical statistical methods and machine learning (ML) approaches, on STLF problems for 35 European countries. The empirical study clearly shows that the proposed model has high expressive power to solve nonlinear stochastic forecasting problems with TS including multiple seasonality and significant random fluctuations. In fact, it outperforms both statistical and state-of-the-art ML models in terms of accuracy.},
  archive      = {J_TNNLS},
  author       = {Slawek Smyl and Grzegorz Dudek and Paweł Pełka},
  doi          = {10.1109/TNNLS.2023.3259149},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11346-11358},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ES-dRNN: A hybrid exponential smoothing and dilated recurrent neural network model for short-term load forecasting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class-incremental learning method with fast update and high
retainability based on broad learning system. <em>TNNLS</em>,
<em>35</em>(8), 11332–11345. (<a
href="https://doi.org/10.1109/TNNLS.2023.3259016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning aims to generate a predictive model from a training dataset of a fixed number of known classes. However, many real-world applications (such as health monitoring and elderly care) are data streams in which new data arrive continually in a short time. Such new data may even belong to previously unknown classes. Hence, class-incremental learning (CIL) is necessary, which incrementally and rapidly updates an existing model with the data of new classes while retaining the existing knowledge of old classes. However, most current CIL methods are designed based on deep models that require a computationally expensive training and update process. In addition, deep learning based CIL (DCIL) methods typically employ stochastic gradient descent (SGD) as an optimizer that forgets the old knowledge to a certain extent. In this article, a broad learning system-based CIL (BLS-CIL) method with fast update and high retainability of old class knowledge is proposed. Traditional BLS is a fast and effective shallow neural network, but it does not work well on CIL tasks. However, our proposed BLS-CIL can overcome these issues and provide the following: 1) high accuracy due to our novel class-correlation loss function that considers the correlations between old and new classes; 2) significantly short training/update time due to the newly derived closed-form solution for our class-correlation loss without iterative optimization; and 3) high retainability of old class knowledge due to our newly derived recursive update rule for CIL (RULL) that does not replay the exemplars of all old classes, as contrasted to the exemplars-replaying methods with the SGD optimizer. The proposed BLS-CIL has been evaluated over 12 real-world datasets, including seven tabular/numerical datasets and six image datasets, and the compared methods include one shallow network and seven classical or state-of-the-art DCIL methods. Experimental results show that our BIL-CIL can significantly improve the classification performance over a shallow network by a large margin (8.80%–48.42%). It also achieves comparable or even higher accuracy than DCIL methods, but greatly reduces the training time from hours to minutes and the update time from minutes to seconds.},
  archive      = {J_TNNLS},
  author       = {Jie Du and Peng Liu and Chi-Man Vong and Chuangquan Chen and Tianfu Wang and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2023.3259016},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11332-11345},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Class-incremental learning method with fast update and high retainability based on broad learning system},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One size fits all: A unified traffic predictor for capturing
the essential spatial–temporal dependency. <em>TNNLS</em>,
<em>35</em>(8), 11317–11331. (<a
href="https://doi.org/10.1109/TNNLS.2023.3259045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is a keystone for building smart cities in the new era and has found wide applications in traffic scheduling and management, environment policy making, public safety, and so on. Instead of creating a traffic predictor for each city, this article focuses on designing a unified network model that could be directly applied for traffic prediction in any city, by learning the essential spatial–temporal dependencies, i.e., the mutual relationship between traffic and the corresponding fine-grained road network. To achieve this goal, this article proposes a joint knowledge- and data-driven mechanism that novelly divides dependencies into three kinds of correlations, i.e., road segment, intra-intersection, and inter-intersection correlation, which capture the microcosmic, middle, and macroscopic dependencies between traffic and the road network, respectively. Specifically, we first construct traffic datasets that could cover all road segments from real-world trajectory datasets, which makes it possible to model the whole road network as a graph, with the help of fine-grained road topology. Then, we propose meta road segment learner, connection-aware spatial–temporal graph convolutional network (GCN), and multiscale residual networks for capturing the microcosmic, middle, and macroscopic dependencies, respectively. Our experiments on three real-world datasets demonstrate that our proposed method could: 1) achieve better prediction accuracy compared with several approaches and 2) capture the mutual relationship between traffic and the fine-grained road network since our model trained only using data from the source city achieves good performance when it is directly applied for traffic prediction in the target city, without any fine-tuning. The codes will be made publicly available.},
  archive      = {J_TNNLS},
  author       = {Guiyang Luo and Hui Zhang and Quan Yuan and Jinglin Li and Wendong Wang and Fei-Yue Wang},
  doi          = {10.1109/TNNLS.2023.3259045},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11317-11331},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {One size fits all: A unified traffic predictor for capturing the essential Spatial–Temporal dependency},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive weighted ranking-oriented label distribution
learning. <em>TNNLS</em>, <em>35</em>(8), 11302–11316. (<a
href="https://doi.org/10.1109/TNNLS.2023.3258976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution learning (LDL) is a novel machine-learning paradigm generalized from multilabel learning (MLL). LDL attaches a label distribution to each instance, giving the description degree of different labels. In many real-world applications, key labels, that is, labels with relatively higher description degrees, are preferable to be better predicted. Unfortunately, existing LDL metrics measure the distance or similarity between label distributions from a global perspective, failing to give sufficient attention to key labels. Therefore, we design a novel LDL metric, the description-degree percentile average (DPA), which simultaneously integrates both the exact ranking value and the description degree of each label. The DPA can enhance accuracy in predicting key labels. Furthermore, noting the shape characteristics of the label distributions, we minimize the variance distance between the predicted and the ground-truth label distributions, to better maintain the distinguishability of labels. Finally, we propose an adaptive weighted ranking-oriented LDL algorithm, which is more suitable for realistic LDL problems that require higher accuracy in predicting key labels. We conduct extensive comparison experiments on various types of LDL datasets. Experimental results on both traditional and newly introduced metrics demonstrate the effectiveness of our proposal.},
  archive      = {J_TNNLS},
  author       = {Xiuyi Jia and Tian Qin and Yunan Lu and Weiwei Li},
  doi          = {10.1109/TNNLS.2023.3258976},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11302-11316},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive weighted ranking-oriented label distribution learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial multi-teacher distillation for semi-supervised
relation extraction. <em>TNNLS</em>, <em>35</em>(8), 11291–11301. (<a
href="https://doi.org/10.1109/TNNLS.2023.3258967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shortage of labeled data has been a long-standing challenge for relation extraction (RE) tasks. Semi-supervised RE (SSRE) is a promising way through annotating unlabeled samples with pseudolabels as additional training data. However, some pseudolabels on unlabeled data might be erroneous and will bring misleading knowledge into SSRE models. For this reason, we propose a novel adversarial multi-teacher distillation (AMTD) framework, which includes multi-teacher knowledge distillation and adversarial training (AT), to capture the knowledge on unlabeled data in a refined way. Specifically, we first develop a general knowledge distillation (KD) technique to learn not only from pseudolabels but also from the class distribution of predictions by different models in existing SSRE methods. To improve the robustness of the model, we further empower the distillation process with a language model-based AT technique. Extensive experimental results on two public datasets demonstrate that our framework significantly promotes the performance of the base SSRE methods.},
  archive      = {J_TNNLS},
  author       = {Wanli Li and Tieyun Qian and Xuhui Li and Lixin Zou},
  doi          = {10.1109/TNNLS.2023.3258967},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11291-11301},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial multi-teacher distillation for semi-supervised relation extraction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DecouplingNet: A stable knowledge distillation decoupling
net for fault detection of rotating machines under varying speeds.
<em>TNNLS</em>, <em>35</em>(8), 11276–11290. (<a
href="https://doi.org/10.1109/TNNLS.2023.3258748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault detection, also known as anomaly detection (AD), is at the heart of prediction and health management (PHM), which plays a vital role in ensuring the safe operation of mechanical equipment. Nonetheless, the lack of anomaly data creates a significant obstacle to the AD of the mechanical system. In particular, the complex modulation effects induced by time-varying speeds make AD much more challenging. For rapid and accurate AD, a stable knowledge distillation decoupling net (DecouplingNet) is provided to overcome these difficulties. First, an adversarial network consisting of an encoder, a decoder, and an encoder-discriminator is developed to model normal samples well by imposing constraints on the latent space. Then, a causal decoupling framework is suggested to disentangle equipment state-related information from operating conditions-related features, enabling stable condition monitoring at varying speeds. Finally, feature-based knowledge distillation is employed to boost the efficiency of AD while maintaining the detection accuracy. The proposed method is tested on two experimental scenarios and compared with some typical AD methods. The finding demonstrates that the net outperforms others in terms of accuracy and efficiency when it comes to detecting anomalies in the mechanical equipment that runs under varying speeds.},
  archive      = {J_TNNLS},
  author       = {Zhen Shi and Jinglong Chen and Yanyang Zi and Zhenyi Chen},
  doi          = {10.1109/TNNLS.2023.3258748},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11276-11290},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DecouplingNet: A stable knowledge distillation decoupling net for fault detection of rotating machines under varying speeds},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hamiltonian identification via quantum ensemble
classification. <em>TNNLS</em>, <em>35</em>(8), 11261–11275. (<a
href="https://doi.org/10.1109/TNNLS.2023.3258622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the Hamiltonian of an unknown quantum system is a critical task in the area of quantum information. In this article, we propose a systematic Hamiltonian identification approach via quantum ensemble multiclass classification (HI-QEMC). This approach is implemented by a three-step iterative refining process, i.e., parameter interval guess , verification , and judgment . In the parameter interval guess step, the parameter interval is divided into several sub-intervals and the true Hamiltonian parameter is guessed in one of them. In the parameter interval verification step, cross verification is applied to verify the accuracy of the guess. In the parameter interval judgment step, an adaptive interval judgment (AIJ) algorithm is designed to determine the sub-interval containing the true Hamiltonian parameter. Numerical results on two typical quantum systems, i.e., two-level quantum systems and three-level quantum systems, demonstrate the effectiveness and superior performance of the proposed approach for quantum Hamiltonian identification.},
  archive      = {J_TNNLS},
  author       = {Haixu Yu and Xudong Zhao and Daoyi Dong and Chunlin Chen},
  doi          = {10.1109/TNNLS.2023.3258622},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11261-11275},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hamiltonian identification via quantum ensemble classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ordinal regression with pinball loss. <em>TNNLS</em>,
<em>35</em>(8), 11246–11260. (<a
href="https://doi.org/10.1109/TNNLS.2023.3258464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal regression (OR) aims to solve multiclass classification problems with ordinal classes. Support vector OR (SVOR) is a typical OR algorithm and has been extensively used in OR problems. In this article, based on the characteristics of OR problems, we propose a novel pinball loss function and present an SVOR method with pinball loss (pin-SVOR). Pin-SVOR is fundamentally different from traditional SVOR with hinge loss. Traditional SVOR employs the hinge loss function, and the classifier is determined by only a few data points near the class boundary, called support vectors, which may lead to a noise sensitive and re-sampling unstable classifier. Distinctively, pin-SVOR employs the pinball loss function. It attaches an extra penalty to correctly classified data that lies inside the class, such that all the training data is involved in deciding the classifier. The data near the middle of each class has a small penalty, and that near the class boundary has a large penalty. Thus, the training data tend to lie near the middle of each class instead of on the class boundary, which leads to scatter minimization in the middle of each class and noise insensitivity. The experimental results show that pin-SVOR has better classification performance than state-of-the-art OR methods.},
  archive      = {J_TNNLS},
  author       = {Guangzheng Zhong and Yanshan Xiao and Bo Liu and Liang Zhao and Xiangjun Kong},
  doi          = {10.1109/TNNLS.2023.3258464},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11246-11260},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ordinal regression with pinball loss},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonfactoid question answering as query-focused summarization
with graph-enhanced multihop inference. <em>TNNLS</em>, <em>35</em>(8),
11231–11245. (<a
href="https://doi.org/10.1109/TNNLS.2023.3258413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonfactoid question answering (QA) is one of the most extensive yet challenging applications and research areas in natural language processing (NLP). Existing methods fall short of handling the long-distance and complex semantic relations between the question and the document sentences. In this work, we propose a novel query-focused summarization method, namely a graph-enhanced multihop query-focused summarizer (GMQS), to tackle the nonfactoid QA problem. Specifically, we leverage graph-enhanced reasoning techniques to elaborate the multihop inference process in nonfactoid QA. Three types of graphs with different semantic relations, namely semantic relevance, topical coherence, and coreference linking, are constructed for explicitly capturing the question-document and sentence-sentence interrelationships. Relational graph attention network (RGAT) is then developed to aggregate the multirelational information accordingly. In addition, the proposed method can be adapted to both extractive and abstractive applications as well as be mutually enhanced by joint learning. Experimental results show that the proposed method consistently outperforms both existing extractive and abstractive methods on two nonfactoid QA datasets, WikiHow and PubMedQA, and possesses the capability of performing explainable multihop reasoning.},
  archive      = {J_TNNLS},
  author       = {Yang Deng and Wenxuan Zhang and Weiwen Xu and Ying Shen and Wai Lam},
  doi          = {10.1109/TNNLS.2023.3258413},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11231-11245},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonfactoid question answering as query-focused summarization with graph-enhanced multihop inference},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta learning with graph attention networks for low-data
drug discovery. <em>TNNLS</em>, <em>35</em>(8), 11218–11230. (<a
href="https://doi.org/10.1109/TNNLS.2023.3250324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding candidate molecules with favorable pharmacological activity, low toxicity, and proper pharmacokinetic properties is an important task in drug discovery. Deep neural networks have made impressive progress in accelerating and improving drug discovery. However, these techniques rely on a large amount of label data to form accurate predictions of molecular properties. At each stage of the drug discovery pipeline, usually, only a few biological data of candidate molecules and derivatives are available, indicating that the application of deep neural networks for low-data drug discovery is still a formidable challenge. Here, we propose a meta learning architecture with graph attention network, Meta-GAT, to predict molecular properties in low-data drug discovery. The GAT captures the local effects of atomic groups at the atom level through the triple attentional mechanism and implicitly captures the interactions between different atomic groups at the molecular level. GAT is used to perceive molecular chemical environment and connectivity, thereby effectively reducing sample complexity. Meta-GAT further develops a meta learning strategy based on bilevel optimization, which transfers meta knowledge from other attribute prediction tasks to low-data target tasks. In summary, our work demonstrates how meta learning can reduce the amount of data required to make meaningful predictions of molecules in low-data scenarios. Meta learning is likely to become the new learning paradigm in low-data drug discovery. The source code is publicly available at: https://github.com/lol88/Meta-GAT .},
  archive      = {J_TNNLS},
  author       = {Qiujie Lv and Guanxing Chen and Ziduo Yang and Weihe Zhong and Calvin Yu-Chian Chen},
  doi          = {10.1109/TNNLS.2023.3250324},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11218-11230},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Meta learning with graph attention networks for low-data drug discovery},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unambiguous and high-fidelity backdoor watermarking for deep
neural networks. <em>TNNLS</em>, <em>35</em>(8), 11204–11217. (<a
href="https://doi.org/10.1109/TNNLS.2023.3250210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unprecedented success of deep learning could not be achieved without the synergy of big data, computing power, and human knowledge, among which none is free. This calls for the copyright protection of deep neural networks (DNNs), which has been tackled via DNN watermarking. Due to the special structure of DNNs, backdoor watermarks have been one of the popular solutions. In this article, we first present a big picture of DNN watermarking scenarios with rigorous definitions unifying the black- and white-box concepts across watermark embedding, attack, and verification phases. Then, from the perspective of data diversity, especially adversarial and open set examples overlooked in the existing works, we rigorously reveal the vulnerability of backdoor watermarks against black-box ambiguity attacks. To solve this problem, we propose an unambiguous backdoor watermarking scheme via the design of deterministically dependent trigger samples and labels, showing that the cost of ambiguity attacks will increase from the existing linear complexity to exponential complexity. Furthermore, noting that the existing definition of backdoor fidelity is solely concerned with classification accuracy, we propose to more rigorously evaluate fidelity via examining training data feature distributions and decision boundaries before and after backdoor embedding. Incorporating the proposed prototype guided regularizer (PGR) and fine-tune all layers (FTAL) strategy, we show that backdoor fidelity can be substantially improved. Experimental results using two versions of the basic ResNet18, advanced wide residual network (WRN28_10) and EfficientNet-B0, on MNIST, CIFAR-10, CIFAR-100, and FOOD-101 classification tasks, respectively, illustrate the advantages of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Guang Hua and Andrew Beng Jin Teoh and Yong Xiang and Hao Jiang},
  doi          = {10.1109/TNNLS.2023.3250210},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11204-11217},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unambiguous and high-fidelity backdoor watermarking for deep neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A ranking-based cross-entropy loss for early classification
of time series. <em>TNNLS</em>, <em>35</em>(8), 11194–11203. (<a
href="https://doi.org/10.1109/TNNLS.2023.3250203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early classification tasks aim to classify time series before observing full data. It is critical in time-sensitive applications such as early sepsis diagnosis in the intensive care unit (ICU). Early diagnosis can provide more opportunities for doctors to rescue lives. However, there are two conflicting goals in the early classification task—accuracy and earliness. Most existing methods try to find a balance between them by weighing one goal against the other. But we argue that a powerful early classifier should always make highly accurate predictions at any moment. The main obstacle is that the key features suitable for classification are not obvious in the early stage, resulting in the excessive overlap of time series distributions in different time stages. The indistinguishable distributions make it difficult for classifiers to recognize. To solve this problem, this article proposes a novel ranking-based cross-entropy ( RCE ) loss to jointly learn the feature of classes and the order of earliness from time series data. In this way, RCE can help classifier to generate probability distributions of time series in different stages with more distinguishable boundary. Thus, the classification accuracy at each time step is finally improved. Besides, for the applicability of the method, we also accelerate the training process by focusing the learning process on high-ranking samples. Experiments on three real-world datasets show that our method can perform classification more accurately than all baselines at all moments.},
  archive      = {J_TNNLS},
  author       = {Chenxi Sun and Hongyan Li and Moxian Song and Shenda Hong},
  doi          = {10.1109/TNNLS.2023.3250203},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11194-11203},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A ranking-based cross-entropy loss for early classification of time series},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FET-LM: Flow-enhanced variational autoencoder for
topic-guided language modeling. <em>TNNLS</em>, <em>35</em>(8),
11180–11193. (<a
href="https://doi.org/10.1109/TNNLS.2023.3249253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoder (VAE) is widely used in tasks of unsupervised text generation due to its potential of deriving meaningful latent spaces, which, however, often assumes that the distribution of texts follows a common yet poor-expressed isotropic Gaussian. In real-life scenarios, sentences with different semantics may not follow simple isotropic Gaussian. Instead, they are very likely to follow a more intricate and diverse distribution due to the inconformity of different topics in texts. Considering this, we propose a flow-enhanced VAE for topic-guided language modeling (FET-LM). The proposed FET-LM models topic and sequence latent separately, and it adopts a normalized flow composed of householder transformations for sequence posterior modeling, which can better approximate complex text distributions. FET-LM further leverages a neural latent topic component by considering learned sequence knowledge, which not only eases the burden of learning topic without supervision but also guides the sequence component to coalesce topic information during training. To make the generated texts more correlative to topics, we additionally assign the topic encoder to play the role of a discriminator. Encouraging results on abundant automatic metrics and three generation tasks demonstrate that the FET-LM not only learns interpretable sequence and topic representations but also is fully capable of generating high-quality paragraphs that are semantically consistent.},
  archive      = {J_TNNLS},
  author       = {Haoqin Tu and Zhongliang Yang and Jinshuai Yang and Linna Zhou and Yongfeng Huang},
  doi          = {10.1109/TNNLS.2023.3249253},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11180-11193},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FET-LM: Flow-enhanced variational autoencoder for topic-guided language modeling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble clustering via co-association matrix
self-enhancement. <em>TNNLS</em>, <em>35</em>(8), 11168–11179. (<a
href="https://doi.org/10.1109/TNNLS.2023.3249207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble clustering integrates a set of base clustering results to generate a stronger one. Existing methods usually rely on a co-association (CA) matrix that measures how many times two samples are grouped into the same cluster according to the base clusterings to achieve ensemble clustering. However, when the constructed CA matrix is of low quality, the performance will degrade. In this article, we propose a simple, yet effective CA matrix self-enhancement framework that can improve the CA matrix to achieve better clustering performance. Specifically, we first extract the high-confidence (HC) information from the base clusterings to form a sparse HC matrix. By propagating the highly reliable information of the HC matrix to the CA matrix and complementing the HC matrix according to the CA matrix simultaneously, the proposed method generates an enhanced CA matrix for better clustering. Technically, the proposed model is formulated as a symmetric constrained convex optimization problem, which is efficiently solved by an alternating iterative algorithm with convergence and global optimum theoretically guaranteed. Extensive experimental comparisons with 12 state-of-the-art methods on ten benchmark datasets substantiate the effectiveness, flexibility, and efficiency of the proposed model in ensemble clustering. The codes and datasets can be downloaded at https://github.com/Siritao/EC-CMS .},
  archive      = {J_TNNLS},
  author       = {Yuheng Jia and Sirui Tao and Ran Wang and Yongheng Wang},
  doi          = {10.1109/TNNLS.2023.3249207},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11168-11179},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ensemble clustering via co-association matrix self-enhancement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Augmentation-free graph contrastive learning of
invariant-discriminative representations. <em>TNNLS</em>,
<em>35</em>(8), 11157–11167. (<a
href="https://doi.org/10.1109/TNNLS.2023.3248871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) is a promising direction toward alleviating the label dependence, poor generalization and weak robustness of graph neural networks, learning representations with invariance, and discriminability by solving pretasks. The pretasks are mainly built on mutual information estimation, which requires data augmentation to construct positive samples with similar semantics to learn invariant signals and negative samples with dissimilar semantics to empower representation discriminability. However, an appropriate data augmentation configuration depends heavily on lots of empirical trials such as choosing the compositions of data augmentation techniques and the corresponding hyperparameter settings. We propose an augmentation-free GCL method, invariant-discriminative GCL (iGCL), that does not intrinsically require negative samples. iGCL designs the invariant-discriminative loss (ID loss) to learn invariant and discriminative representations. On the one hand, ID loss learns invariant signals by directly minimizing the mean square error (MSE) between the target samples and positive samples in the representation space. On the other hand, ID loss ensures that the representations are discriminative by an orthonormal constraint forcing the different dimensions of representations to be independent of each other. This prevents representations from collapsing to a point or subspace. Our theoretical analysis explains the effectiveness of ID loss from the perspectives of the redundancy reduction criterion, canonical correlation analysis (CCA), and information bottleneck (IB) principle. The experimental results demonstrate that iGCL outperforms all baselines on five node classification benchmark datasets. iGCL also shows superior performance for different label ratios and is capable of resisting graph attacks, which indicates that iGCL has excellent generalization and robustness. The source code is available at https://github.com/lehaifeng/ T-GCN/tree/master/iGCL.},
  archive      = {J_TNNLS},
  author       = {Haifeng Li and Jun Cao and Jiawei Zhu and Qinyao Luo and Silu He and Xuying Wang},
  doi          = {10.1109/TNNLS.2023.3248871},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11157-11167},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Augmentation-free graph contrastive learning of invariant-discriminative representations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lesion-decoupling-based segmentation with large-scale colon
and esophageal datasets for early cancer diagnosis. <em>TNNLS</em>,
<em>35</em>(8), 11142–11156. (<a
href="https://doi.org/10.1109/TNNLS.2023.3248804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lesions of early cancers often show flat, small, and isochromatic characteristics in medical endoscopy images, which are difficult to be captured. By analyzing the differences between the internal and external features of the lesion area, we propose a lesion-decoupling-based segmentation (LDS) network for assisting early cancer diagnosis. We introduce a plug-and-play module called self-sampling similar feature disentangling module (FDM) to obtain accurate lesion boundaries. Then, we propose a feature separation loss (FSL) function to separate pathological features from normal ones. Moreover, since physicians make diagnoses with multimodal data, we propose a multimodal cooperative segmentation network with two different modal images as input: white-light images (WLIs) and narrowband images (NBIs). Our FDM and FSL show a good performance for both single-modal and multimodal segmentations. Extensive experiments on five backbones prove that our FDM and FSL can be easily applied to different backbones for a significant lesion segmentation accuracy improvement, and the maximum increase of mean Intersection over Union (mIoU) is 4.58. For colonoscopy, we can achieve up to mIoU of 91.49 on our Dataset A and 84.41 on the three public datasets. For esophagoscopy, mIoU of 64.32 is best achieved on the WLI dataset and 66.31 on the NBI dataset.},
  archive      = {J_TNNLS},
  author       = {Qing Lin and Weimin Tan and Shilun Cai and Bo Yan and Jichun Li and Yunshi Zhong},
  doi          = {10.1109/TNNLS.2023.3248804},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11142-11156},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Lesion-decoupling-based segmentation with large-scale colon and esophageal datasets for early cancer diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RL-chord: CLSTM-based melody harmonization using deep
reinforcement learning. <em>TNNLS</em>, <em>35</em>(8), 11128–11141. (<a
href="https://doi.org/10.1109/TNNLS.2023.3248793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic music generation is the combination of artificial intelligence and art, in which melody harmonization is a significant and challenging task. However, previous recurrent neural network (RNN)-based work fails to maintain long-term dependency and neglects the guidance of music theory. In this article, we first devise a universal chord representation with a fixed small dimension, which can cover most existing chords and is easy to expand. Then a novel melody harmonization system based on reinforcement learning (RL), RL-Chord, is proposed to generate high-quality chord progressions. Specifically, a melody conditional LSTM (CLSTM) model is put forward that learns the transition and duration of chords well, based on which RL algorithms with three well-designed reward modules are combined to construct RL-Chord. We compare three widely used RL algorithms (i.e., policy gradient, $Q$ -learning, and actor–critic algorithms) on the melody harmonization task for the first time and prove the superiority of deep $Q$ -network (DQN). Furthermore, a style classifier is devised to fine-tune the pretrained DQN-Chord for zero-shot Chinese folk (CF) melody harmonization. Experimental results demonstrate that the proposed model can generate harmonious and fluent chord progressions for diverse melodies. Quantitatively, DQN-Chord achieves better performance than the compared methods on multiple evaluation metrics, such as chord histogram similarity (CHS), chord tonal distance (CTD), and melody–chord tonal distance (MCTD).},
  archive      = {J_TNNLS},
  author       = {Shulei Ji and Xinyu Yang and Jing Luo and Juan Li},
  doi          = {10.1109/TNNLS.2023.3248793},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11128-11141},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RL-chord: CLSTM-based melody harmonization using deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Judgment prediction based on tensor decomposition with
optimized neural networks. <em>TNNLS</em>, <em>35</em>(8), 11116–11127.
(<a href="https://doi.org/10.1109/TNNLS.2023.3248275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of smart justice, handling legal cases through artificial intelligence technology is a research hotspot. Traditional judgment prediction methods are mainly based on feature models and classification algorithms. The former is difficult to describe cases from multiple angles and capture the correlation information between different case modules, while requires a wealth of legal expertise and manual labeling. The latter is unable to accurately extract the most useful information from case documents and produce fine-grained predictions. This article proposes a judgment prediction method based on tensor decomposition with optimized neural networks, which consists of OTenr, GTend, and RnEla. OTenr represents cases as normalized tensors. GTend decomposes normalized tensors into core tensors using the guidance tensor. RnEla intervenes in a case modeling process in GTend by optimizing the guidance tensor, so that core tensors represent tensor structural and elemental information, which is most conducive to improving the accuracy of judgment prediction. RnEla consists of the similarity correlation Bi-LSTM and optimized Elastic-Net regression. RnEla takes the similarity between cases as an important factor for judgment prediction. Experimental results on real legal case dataset show that the accuracy of our method is higher than that of the previous judgment prediction methods.},
  archive      = {J_TNNLS},
  author       = {Xiaoding Guo and Lei Zhang and Zhihong Tian},
  doi          = {10.1109/TNNLS.2023.3248275},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11116-11127},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Judgment prediction based on tensor decomposition with optimized neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast discriminant analysis with adaptive reconstruction
structure preserving. <em>TNNLS</em>, <em>35</em>(8), 11106–11115. (<a
href="https://doi.org/10.1109/TNNLS.2023.3248234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighborhood reconstruction methods have been widely applied to feature engineering. Existing reconstruction-based discriminant analysis methods normally project high-dimensional data into a low-dimensional space while preserving the reconstruction relationships among samples. However, there are three limitations: 1) the reconstruction coefficients are learned based on the collaborative representation of all sample pairs, which requires the training time to be the cube of the number of samples; 2) these coefficients are learned in the original space, ignoring the interference of the noise and redundant features; and 3) there is a reconstruction relationship between heterogeneous samples; this will enlarge the similarity of heterogeneous samples in the subspace. In this article, we propose a fast and adaptive discriminant neighborhood projection model to tackle the above drawbacks. First, the local manifold structure is captured by bipartite graphs in which each sample is reconstructed by anchor points derived from the same class as that sample; this can avoid the reconstruction between heterogeneous samples. Second, the number of anchor points is far less than the number of samples; this strategy can reduce the time complexity substantially. Third, anchor points and reconstruction coefficients of bipartite graphs are updated adaptively in the process of dimensionality reduction, which can enhance the quality of bipartite graphs and extract discriminative features simultaneously. An iterative algorithm is designed to solve this model. Extensive results on toy data and benchmark datasets show the effectiveness and superiority of our model.},
  archive      = {J_TNNLS},
  author       = {Xiaowei Zhao and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3248234},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11106-11115},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast discriminant analysis with adaptive reconstruction structure preserving},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian dictionary learning on robust tubal transformed
tensor factorization. <em>TNNLS</em>, <em>35</em>(8), 11091–11105. (<a
href="https://doi.org/10.1109/TNNLS.2023.3248156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent study on tensor singular value decomposition (t-SVD) that performs the Fourier transform on the tubes of a third-order tensor has gained promising performance on multidimensional data recovery problems. However, such a fixed transformation, e.g., discrete Fourier transform and discrete cosine transform, lacks being self-adapted to the change of different datasets, and thus, it is not flexible enough to exploit the low-rank and sparse property of the variety of multidimensional datasets. In this article, we consider a tube as an atom of a third-order tensor and construct a data-driven learning dictionary from the observed noisy data along the tubes of the given tensor. Then, a Bayesian dictionary learning (DL) model with tensor tubal transformed factorization, aiming to identify the underlying low-tubal-rank structure of the tensor effectively via the data-adaptive dictionary, is developed to solve the tensor robust principal component analysis (TRPCA) problem. With the defined pagewise tensor operators, a variational Bayesian DL algorithm is established and updates the posterior distributions instantaneously along the third dimension to solve the TPRCA. Extensive experiments on real-world applications, such as color image and hyperspectral image denoising and background/foreground separation problems, demonstrate both effectiveness and efficiency of the proposed approach in terms of various standard metrics.},
  archive      = {J_TNNLS},
  author       = {Qilun Luo and Wen Li and Mingqing Xiao},
  doi          = {10.1109/TNNLS.2023.3248156},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11091-11105},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bayesian dictionary learning on robust tubal transformed tensor factorization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). K-means clustering with natural density peaks for
discovering arbitrary-shaped clusters. <em>TNNLS</em>, <em>35</em>(8),
11077–11090. (<a
href="https://doi.org/10.1109/TNNLS.2023.3248064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to simplicity, K-means has become a widely used clustering method. However, its clustering result is seriously affected by the initial centers and the allocation strategy makes it hard to identify manifold clusters. Many improved K-means are proposed to accelerate it and improve the quality of initialize cluster centers, but few researchers pay attention to the shortcoming of K-means in discovering arbitrary-shaped clusters. Using graph distance (GD) to measure the dissimilarity between objects is a good way to solve this problem, but computing the GD is time-consuming. Inspired by the idea that granular ball uses a ball to represent the local data, we select representatives from a local neighborhood, called natural density peaks (NDPs). On the basis of NDPs, we propose a novel K-means algorithm for identifying arbitrary-shaped clusters, called NDP-Kmeans. It defines neighbor-based distance between NDPs and takes advantage of the neighbor-based distance to compute the GD between NDPs. Afterward, an improved K-means with high-quality initial centers and GD is used to cluster NDPs. Finally, each remaining object is assigned according to its representative. The experimental results show that our algorithms can not only recognize spherical clusters but also manifold clusters. Therefore, NDP-Kmeans has more advantages in detecting arbitrary-shaped clusters than other excellent algorithms.},
  archive      = {J_TNNLS},
  author       = {Dongdong Cheng and Jinlong Huang and Sulan Zhang and Shuyin Xia and Guoyin Wang and Jiang Xie},
  doi          = {10.1109/TNNLS.2023.3248064},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11077-11090},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {K-means clustering with natural density peaks for discovering arbitrary-shaped clusters},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neural networks with weighted losses by
uncertainty evaluation for accurate and stable prediction of
manufacturing systems. <em>TNNLS</em>, <em>35</em>(8), 11064–11076. (<a
href="https://doi.org/10.1109/TNNLS.2023.3247163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state prediction of key components in manufacturing systems tends to be risk-sensitive tasks, where prediction accuracy and stability are the two key indicators. The physics-informed neural networks (PINNs), which integrate the advantages of both data-driven models and physics models, are deemed as an effective approach and research trends for stable prediction; however, the potential advantages of PINN are limited for the situations with inaccurate physics models or noisy data, where the balancing of the weights of the data-driven model and physics model is very important for improving the performance of PINN, and it is also a challenge urgently to be addressed. This article proposed a kind of PINN with weighted losses (PNNN-WLs) by uncertainty evaluation for accurate and stable prediction of manufacturing systems, where a novel weight allocation strategy based on uncertainty evaluation by quantifying the variance of prediction errors is proposed, and an improved PINN framework is established for accurate and stable prediction. The proposed approach is verified with open datasets on tool wear prediction, and experimental results show that the prediction accuracy and stability could be obviously improved over existing methods.},
  archive      = {J_TNNLS},
  author       = {Jiaqi Hua and Yingguang Li and Changqing Liu and Peng Wan and Xu Liu},
  doi          = {10.1109/TNNLS.2023.3247163},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11064-11076},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Physics-informed neural networks with weighted losses by uncertainty evaluation for accurate and stable prediction of manufacturing systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Representation learning and reinforcement learning for
dynamic complex motion planning system. <em>TNNLS</em>, <em>35</em>(8),
11049–11063. (<a
href="https://doi.org/10.1109/TNNLS.2023.3247160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor motion planning challenges researchers because of the high density and unpredictability of moving obstacles. Classical algorithms work well in the case of static obstacles but suffer from collisions in the case of dense and dynamic obstacles. Recent reinforcement learning (RL) algorithms provide safe solutions for multiagent robotic motion planning systems. However, these algorithms face challenges in convergence: slow convergence speed and suboptimal converged result. Inspired by RL and representation learning, we introduced the ALN-DSAC: a hybrid motion planning algorithm where attention-based long short-term memory (LSTM) and novel data replay combine with discrete soft actor–critic (SAC). First, we implemented a discrete SAC algorithm, which is the SAC in the setting of discrete action space. Second, we optimized existing distance-based LSTM encoding by attention-based encoding to improve the data quality. Third, we introduced a novel data replay method by combining the online learning and offline learning to improve the efficacy of data replay. The convergence of our ALN-DSAC outperforms that of the trainable state of the arts. Evaluations demonstrate that our algorithm achieves nearly 100% success with less time to reach the goal in motion planning tasks when compared to the state of the arts. The test code is available at https://github.com/CHUENGMINCHOU/ALN-DSAC .},
  archive      = {J_TNNLS},
  author       = {Chengmin Zhou and Bingding Huang and Pasi Fränti},
  doi          = {10.1109/TNNLS.2023.3247160},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11049-11063},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Representation learning and reinforcement learning for dynamic complex motion planning system},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal decouple-and-squeeze contrastive learning for
semisupervised skeleton-based action recognition. <em>TNNLS</em>,
<em>35</em>(8), 11035–11048. (<a
href="https://doi.org/10.1109/TNNLS.2023.3247103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has been successfully leveraged to learn action representations for addressing the problem of semisupervised skeleton-based action recognition. However, most contrastive learning-based methods only contrast global features mixing spatiotemporal information, which confuses the spatial- and temporal-specific information reflecting different semantic at the frame level and joint level. Thus, we propose a novel spatiotemporal decouple-and-squeeze contrastive learning (SDS-CL) framework to comprehensively learn more abundant representations of skeleton-based actions by jointly contrasting spatial-squeezing features, temporal-squeezing features, and global features. In SDS-CL, we design a new spatiotemporal-decoupling intra–inter attention (SIIA) mechanism to obtain the spatiotemporal-decoupling attentive features for capturing spatiotemporal specific information by calculating spatial- and temporal-decoupling intra-attention maps among joint/motion features, as well as spatial- and temporal-decoupling inter-attention maps between joint and motion features. Moreover, we present a new spatial-squeezing temporal-contrasting loss (STL), a new temporal-squeezing spatial-contrasting loss (TSL), and the global-contrasting loss (GL) to contrast the spatial-squeezing joint and motion features at the frame level, temporal-squeezing joint and motion features at the joint level, as well as global joint and motion features at the skeleton level. Extensive experimental results on four public datasets show that the proposed SDS-CL achieves performance gains compared with other competitive methods.},
  archive      = {J_TNNLS},
  author       = {Binqian Xu and Xiangbo Shu and Jiachao Zhang and Guangzhao Dai and Yan Song},
  doi          = {10.1109/TNNLS.2023.3247103},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11035-11048},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatiotemporal decouple-and-squeeze contrastive learning for semisupervised skeleton-based action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-in-the-loop consensus control for multiagent systems
with external disturbances. <em>TNNLS</em>, <em>35</em>(8), 11024–11034.
(<a href="https://doi.org/10.1109/TNNLS.2023.3246567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the human-in-the-loop leader–follower consensus control problem is addressed for multiagent systems (MASs) with unknown external disturbances. A human operator is deployed to monitor the MASs’ team by transmitting an execution signal to a nonautonomous leader in response to any hazard detected, with the control input of the leader unknown to all followers. For each follower, a full-order observer, in which the observer error dynamic system decouples the unknown disturbance input, is designed for asymptotic state estimation. Then, an interval observer is constructed for the consensus error dynamic system, where the unknown disturbances and control inputs of its neighbors and its disturbance are treated as unknown inputs (UIs). To process the UIs, a new asymptotic algebraic UI reconstruction (UIR) scheme is proposed based on the interval observer, and one of the significant features of the UIR is the capacity to decouple the control input of the follower. The subsequent human-in-the-loop asymptotic convergence consensus protocol is developed by applying an observer-based distributed control strategy. Finally, the proposed control scheme is validated through two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Li Ma and Fanglai Zhu and Xudong Zhao},
  doi          = {10.1109/TNNLS.2023.3246567},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11024-11034},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Human-in-the-loop consensus control for multiagent systems with external disturbances},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced results on sampled-data synchronization for chaotic
neural networks with actuator saturation using parameterized control.
<em>TNNLS</em>, <em>35</em>(8), 11009–11023. (<a
href="https://doi.org/10.1109/TNNLS.2023.3246426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a novel sampled-data synchronization controller design method for chaotic neural networks (CNNs) with actuator saturation. The proposed method is based on a parameterization approach which reformulates the activation function as the weighted sum of matrices with the weighting functions. Also, controller gain matrices are combined by affinely transformed weighting functions. The enhanced stabilization criterion is formulated in terms of linear matrix inequalities (LMIs) based on the Lyapunov stability theory and weighting function’s information. As shown in the comparison results of the bench marking example, the presented method much outperforms previous methods, and thus the enhancement of the proposed parameterized control is verified.},
  archive      = {J_TNNLS},
  author       = {Seonghyeon Jo and Wookyong Kwon and Sang Jun Lee and Sangmoon Lee and Yongsik Jin},
  doi          = {10.1109/TNNLS.2023.3246426},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {11009-11023},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhanced results on sampled-data synchronization for chaotic neural networks with actuator saturation using parameterized control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive filter pruning via sensitivity feedback.
<em>TNNLS</em>, <em>35</em>(8), 10996–11008. (<a
href="https://doi.org/10.1109/TNNLS.2023.3246263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Filter pruning is advocated for accelerating deep neural networks without dedicated hardware or libraries, while maintaining high prediction accuracy. Several works have cast pruning as a variant of $\ell _{1}$ -regularized training, which entails two challenges: 1) the $\ell _{1}$ -norm is not scaling-invariant (i.e., the regularization penalty depends on weight values) and 2) there is no rule for selecting the penalty coefficient to trade off high pruning ratio for low accuracy drop. To address these issues, we propose a lightweight pruning method termed adaptive sensitivity-based pruning (ASTER) which: 1) achieves scaling-invariance by refraining from modifying unpruned filter weights and 2) dynamically adjusts the pruning threshold concurrently with the training process. ASTER computes the sensitivity of the loss to the threshold on the fly (without retraining); this is carried efficiently by an application of L-BFGS solely on the batch normalization (BN) layers. It then proceeds to adapt the threshold so as to maintain a fine balance between pruning ratio and model capacity. We have conducted extensive experiments on a number of state-of-the-art CNN models on benchmark datasets to illustrate the merits of our approach in terms of both FLOPs reduction and accuracy. For example, on ILSVRC-2012 our method reduces more than 76% FLOPs for ResNet-50 with only 2.0% Top-1 accuracy degradation, while for the MobileNet v2 model it achieves 46.6% FLOPs Drop with a Top-1 Acc. Drop of only 2.77%. Even for a very lightweight classification model like MobileNet v3-small, ASTER saves 16.1% FLOPs with a negligible Top-1 accuracy drop of 0.03%.},
  archive      = {J_TNNLS},
  author       = {Yuyao Zhang and Nikolaos M. Freris},
  doi          = {10.1109/TNNLS.2023.3246263},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10996-11008},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive filter pruning via sensitivity feedback},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PatchNet: Maximize the exploration of congeneric semantics
for weakly supervised semantic segmentation. <em>TNNLS</em>,
<em>35</em>(8), 10984–10995. (<a
href="https://doi.org/10.1109/TNNLS.2023.3246109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase in the number of image data and the lack of corresponding labels, weakly supervised learning has drawn a lot of attention recently in computer vision tasks, especially in the fine-grained semantic segmentation problem. To alleviate human efforts from expensive pixel-by-pixel annotations, our method focuses on weakly supervised semantic segmentation (WSSS) with image-level labels, which are much easier to obtain. As a considerable gap exists between pixel-level segmentation and image-level labels, how to reflect the image-level semantic information on each pixel is an important question. To explore the congeneric semantic regions from the same class to the maximum, we construct the patch-level semantic augmentation network (PatchNet) based on the self-detected patches from different images that contain the same class labels. Patches can frame the objects as much as possible and include as little background as possible. The patch-level semantic augmentation network that is established with patches as the nodes can maximize the mutual learning of similar objects. We regard the embedding vectors of patches as nodes and use a transformer-based complementary learning module to construct weighted edges according to the embedding similarity between different nodes. Moreover, to better supplement semantic information, we propose softcomplementary loss functions matched with the whole network structure. We conduct experiments on the popular PASCAL VOC 2012 and MS COCO 2014 benchmarks, and our model yields the state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Ke Zhang and Chen Chen and Chun Yuan and Sihong Chen and Xinfeng Wang and Xin He},
  doi          = {10.1109/TNNLS.2023.3246109},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10984-10995},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PatchNet: Maximize the exploration of congeneric semantics for weakly supervised semantic segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototype-guided memory replay for continual learning.
<em>TNNLS</em>, <em>35</em>(8), 10973–10983. (<a
href="https://doi.org/10.1109/TNNLS.2023.3246049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning (CL) is a machine learning paradigm that accumulates knowledge while learning sequentially. The main challenge in CL is catastrophic forgetting of previously seen tasks, which occurs due to shifts in the probability distribution. To retain knowledge, existing CL models often save some past examples and revisit them while learning new tasks. As a result, the size of saved samples dramatically increases as more samples are seen. To address this issue, we introduce an efficient CL method by storing only a few samples to achieve good performance. Specifically, we propose a dynamic prototype-guided memory replay (PMR) module, where synthetic prototypes serve as knowledge representations and guide the sample selection for memory replay. This module is integrated into an online meta-learning (OML) model for efficient knowledge transfer. We conduct extensive experiments on the CL benchmark text classification datasets and examine the effect of training set order on the performance of CL models. The experimental results demonstrate the superiority our approach in terms of accuracy and efficiency.},
  archive      = {J_TNNLS},
  author       = {Stella Ho and Ming Liu and Lan Du and Longxiang Gao and Yong Xiang},
  doi          = {10.1109/TNNLS.2023.3246049},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10973-10983},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prototype-guided memory replay for continual learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust switching time optimization for networked switched
systems via model predictive control. <em>TNNLS</em>, <em>35</em>(8),
10961–10972. (<a
href="https://doi.org/10.1109/TNNLS.2023.3246041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a model predictive control (MPC) strategy to find the optimal switching time sequences of networked switched systems with uncertainties. First, based on predicted trajectories under exact discretization, a large-scale MPC problem is formulated; second, a two-level hierarchical optimization structure coupled with a local compensation mechanism is established to solve the formulated MPC problem, where the proposed hierarchical optimization structure is actually a recurrent neural network consisting of a coordination unit (CU) at the upper level and a series of local optimization units (LOUs) related to each subsystem at the lower level. Finally, a real-time switching time optimization algorithm is designed to calculate the optimal switching time sequences.},
  archive      = {J_TNNLS},
  author       = {Dongxue Peng and Hao Yang and Bin Jiang},
  doi          = {10.1109/TNNLS.2023.3246041},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10961-10972},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust switching time optimization for networked switched systems via model predictive control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A collaborative neurodynamic optimization approach to
distributed chiller loading. <em>TNNLS</em>, <em>35</em>(8),
10950–10960. (<a
href="https://doi.org/10.1109/TNNLS.2023.3245812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a collaborative neurodynamic optimization approach to distributed chiller loading in the presence of nonconvex power consumption functions and binary variables associated with cardinality constraints. We formulate a cardinality-constrained distributed optimization problem with nonconvex objective functions and discrete feasible regions, based on an augmented Lagrangian function. To overcome the difficulty caused by the nonconvexity in the formulated distributed optimization problem, we develop a collaborative neurodynamic optimization method based on multiple coupled recurrent neural networks reinitialized repeatedly using a meta-heuristic rule. We elaborate on experimental results based on two multi-chiller systems with the parameters from the chiller manufacturers to demonstrate the efficacy of the proposed approach in comparison to several baselines.},
  archive      = {J_TNNLS},
  author       = {Zhongying Chen and Jun Wang and Qing-Long Han},
  doi          = {10.1109/TNNLS.2023.3245812},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10950-10960},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A collaborative neurodynamic optimization approach to distributed chiller loading},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive migration collaborative network for multimodal
image classification. <em>TNNLS</em>, <em>35</em>(8), 10935–10949. (<a
href="https://doi.org/10.1109/TNNLS.2023.3245643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multispectral (MS) and the panchromatic (PAN) images belong to different modalities with specific advantageous properties. Therefore, there is a large representation gap between them. Moreover, the features extracted independently by the two branches belong to different feature spaces, which is not conducive to the subsequent collaborative classification. At the same time, different layers also have different representation capabilities for objects with large size differences. In order to dynamically and adaptively transfer the dominant attributes, reduce the gap between them, find the best shared layer representation, and fuse the features of different representation capabilities, this article proposes an adaptive migration collaborative network (AMC-Net) for multimodal remote-sensing (RS) images classification. First, for the input of the network, we combine principal component analysis (PCA) and nonsubsampled contourlet transformation (NSCT) to migrate the advantageous attributes of the PAN and the MS images to each other. This not only improves the quality of images themselves, but also increases the similarity between the two images, thereby reducing the representational gap between them and the pressure on the subsequent classification network. Second, for the interaction on the feature migrate branch, we design a feature progressive migration fusion unit (FPMF-Unit) based on the adaptive cross-stitch unit of correlation coefficient analysis (CCA), which can make the network automatically learn the features that need to be shared and migrated, aiming to find the best shared-layer representation for multifeature learning. And we design an adaptive layer fusion mechanism module (ALFM-Module), which can adaptively fuse features of different layers, aiming to clearly model the dependencies among multiple layers for different sized objects. Finally, for the output of the network, we add the calculation of the correlation coefficient to the loss function, which can make the network converge to the global optimum as much as possible. The experimental results indicate that AMC-Net can achieve competitive performance. And the code for the network framework is available at: https://github.com/ru-willow/A-AFM-ResNet .},
  archive      = {J_TNNLS},
  author       = {Wenping Ma and Mengru Ma and Licheng Jiao and Fang Liu and Hao Zhu and Xu Liu and Shuyuan Yang and Biao Hou},
  doi          = {10.1109/TNNLS.2023.3245643},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10935-10949},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An adaptive migration collaborative network for multimodal image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Convergence and stability of optimal regulation via
generalized n-step value gradient learning. <em>TNNLS</em>,
<em>35</em>(8), 10923–10934. (<a
href="https://doi.org/10.1109/TNNLS.2023.3245630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the generalized $N$ -step value gradient learning (GNSVGL) algorithm, which takes a long-term prediction parameter $\lambda $ into account, is developed for infinite horizon discounted near-optimal control of discrete-time nonlinear systems. The proposed GNSVGL algorithm can accelerate the learning process of adaptive dynamic programming (ADP) and has a better performance by learning from more than one future reward. Compared with the traditional $N$ -step value gradient learning (NSVGL) algorithm with zero initial functions, the proposed GNSVGL algorithm is initialized with positive definite functions. Considering different initial cost functions, the convergence analysis of the value-iteration-based algorithm is provided. The stability condition for the iterative control policy is established to determine the value of the iteration index, under which the control law can make the system asymptotically stable. Under such a condition, if the system is asymptotically stable at the current iteration, then the iterative control laws after this step are guaranteed to be stabilizing. Two critic neural networks and one action network are constructed to approximate the one-return costate function, the $\lambda $ -return costate function, and the control law, respectively. It is emphasized that one-return and $\lambda $ -return critic networks are combined to train the action neural network. Finally, via conducting simulation studies and comparisons, the superiority of the developed algorithm is confirmed.},
  archive      = {J_TNNLS},
  author       = {Ding Wang and Mingming Zhao and Mingming Ha and Junfei Qiao},
  doi          = {10.1109/TNNLS.2023.3245630},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10923-10934},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convergence and stability of optimal regulation via generalized N-step value gradient learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic attention based on gaussian processes for deep
multiple instance learning. <em>TNNLS</em>, <em>35</em>(8), 10909–10922.
(<a href="https://doi.org/10.1109/TNNLS.2023.3245329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple instance learning (MIL) is a weakly supervised learning paradigm that is becoming increasingly popular because it requires less labeling effort than fully supervised methods. This is especially interesting for areas where the creation of large annotated datasets remains challenging, as in medicine. Although recent deep learning MIL approaches have obtained state-of-the-art results, they are fully deterministic and do not provide uncertainty estimations for the predictions. In this work, we introduce the attention Gaussian process (AGP) model, a novel probabilistic attention mechanism based on Gaussian processes (GPs) for deep MIL. AGP provides accurate bag-level predictions as well as instance-level explainability and can be trained end-to-end. Moreover, its probabilistic nature guarantees robustness to overfit on small datasets and uncertainty estimations for the predictions. The latter is especially important in medical applications, where decisions have a direct impact on the patient’s health. The proposed model is validated experimentally as follows. First, its behavior is illustrated in two synthetic MIL experiments based on the well-known MNIST and CIFAR-10 datasets, respectively. Then, it is evaluated in three different real-world cancer detection experiments. AGP outperforms state-of-the-art MIL approaches, including deterministic deep learning ones. It shows a strong performance even on a small dataset with less than 100 labels and generalizes better than competing methods on an external test set. Moreover, we experimentally show that predictive uncertainty correlates with the risk of wrong predictions, and therefore it is a good indicator of reliability in practice. Our code is publicly available.},
  archive      = {J_TNNLS},
  author       = {Arne Schmidt and Pablo Morales-Álvarez and Rafael Molina},
  doi          = {10.1109/TNNLS.2023.3245329},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10909-10922},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic attention based on gaussian processes for deep multiple instance learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Collision-avoiding flocking with multiple fixed-wing UAVs
in obstacle-cluttered environments: A task-specific curriculum- based
MADRL approach. <em>TNNLS</em>, <em>35</em>(8), 10894–10908. (<a
href="https://doi.org/10.1109/TNNLS.2023.3245124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple unmanned aerial vehicles (UAVs) are able to efficiently accomplish a variety of tasks in complex scenarios. However, developing a collision-avoiding flocking policy for multiple fixed-wing UAVs is still challenging, especially in obstacle-cluttered environments. In this article, we propose a novel curriculum-based multiagent deep reinforcement learning (MADRL) approach called task-specific curriculum-based MADRL (TSCAL) to learn the decentralized flocking with obstacle avoidance policy for multiple fixed-wing UAVs. The core idea is to decompose the collision-avoiding flocking task into multiple subtasks and progressively increase the number of subtasks to be solved in a staged manner. Meanwhile, TSCAL iteratively alternates between the procedures of online learning and offline transfer. For online learning, we propose a hierarchical recurrent attention multiagent actor–critic (HRAMA) algorithm to learn the policies for the corresponding subtask(s) in each learning stage. For offline transfer, we develop two transfer mechanisms, i.e., model reload and buffer reuse, to transfer knowledge between two neighboring stages. A series of numerical simulations demonstrate the significant advantages of TSCAL in terms of policy optimality, sample efficiency, and learning stability. Finally, the high-fidelity hardware-in-the-loop (HITL) simulation is conducted to verify the adaptability of TSCAL. A video about the numerical and HITL simulations is available at https://youtu.be/R9yLJNYRIqY .},
  archive      = {J_TNNLS},
  author       = {Chao Yan and Chang Wang and Xiaojia Xiang and Kin Huat Low and Xiangke Wang and Xin Xu and Lincheng Shen},
  doi          = {10.1109/TNNLS.2023.3245124},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10894-10908},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collision-avoiding flocking with multiple fixed-wing UAVs in obstacle-cluttered environments: A task-specific curriculum- based MADRL approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Decentralized control for large-scale systems with actuator
faults and external disturbances: A data-driven method. <em>TNNLS</em>,
<em>35</em>(8), 10882–10893. (<a
href="https://doi.org/10.1109/TNNLS.2023.3245102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates optimal control for a class of large-scale systems using a data-driven method. The existing control methods for large-scale systems in this context separately consider disturbances, actuator faults, and uncertainties. In this article, we build on such methods by proposing an architecture that accommodates simultaneous consideration of all of these effects, and an optimization index is designed for the control problem. This diversifies the class of large-scale systems amenable to optimal control. We first establish a min–max optimization index based on the zero-sum differential game theory. Then, by integrating all the Nash equilibrium solutions of the isolated subsystems, the decentralized zero-sum differential game strategy is obtained to stabilize the large-scale system. Meanwhile, by designing adaptive parameters, the impact of actuator failure on the system performance is eliminated. Afterward, an adaptive dynamic programming (ADP) method is utilized to learn the solution of the Hamilton–Jacobi–Isaac (HJI) equation, which does not need the prior knowledge of system dynamics. A rigorous stability analysis shows that the proposed controller asymptotically stabilizes the large-scale system. Finally, a multipower system example is adopted to illustrate the effectiveness of the proposed protocols.},
  archive      = {J_TNNLS},
  author       = {Yan Li and Hao Zhang and Zhuping Wang and Chao Huang and Huaicheng Yan},
  doi          = {10.1109/TNNLS.2023.3245102},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10882-10893},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized control for large-scale systems with actuator faults and external disturbances: A data-driven method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical threshold pruning based on uniform response
criterion. <em>TNNLS</em>, <em>35</em>(8), 10869–10881. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been successfully applied to various fields. However, CNNs’ overparameterization requires more memory and training time, making it unsuitable for some resource-constrained devices. To address this issue, filter pruning as one of the most efficient ways was proposed. In this article, we propose a feature-discrimination-based filter importance criterion, uniform response criterion (URC), as a key component of filter pruning. It converts the maximum activation responses into probabilities and then measures the importance of the filter through the distribution of these probabilities over classes. However, applying URC directly to global threshold pruning may cause some problems. The first problem is that some layers will be completely pruned under global pruning settings. The second problem is that global threshold pruning neglects that filters in different layers have different importance. To address these issues, we propose hierarchical threshold pruning (HTP) with URC. It performs a pruning step limited in a relatively redundant layer rather than comparing the filters’ importance across all layers, which can avoid some important filters being pruned. The effectiveness of our method benefits from three techniques: 1) measuring filter importance by URC; 2) normalizing filter scores; and 3) conducting prune in relatively redundant layers. Extensive experiments on CIFAR-10/100 and ImageNet show that our method achieves the state-of-the-art performance on multiple benchmarks.},
  archive      = {J_TNNLS},
  author       = {Yaguan Qian and Zhiqiang He and Yuqi Wang and Bin Wang and Xiang Ling and Zhaoquan Gu and Haijiang Wang and Shaoning Zeng and Wassim Swaileh},
  doi          = {10.1109/TNNLS.2023.3244994},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10869-10881},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical threshold pruning based on uniform response criterion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overcoming data deficiency for multi-person pose estimation.
<em>TNNLS</em>, <em>35</em>(8), 10857–10868. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building multi-person pose estimation (MPPE) models that can handle complex foreground and uncommon scenes is an important challenge in computer vision. Aside from designing novel models, strengthening training data is a promising direction but remains largely unexploited for the MPPE task. In this article, we systematically identify the key deficiencies of existing pose datasets that prevent the power of well-designed models from being fully exploited and propose the corresponding solutions. Specifically, we find that the traditional data augmentation techniques are inadequate in addressing the two key deficiencies, imbalanced instance complexity (IC) (evaluated by our new metric IC) and insufficient realistic scenes. To overcome these deficiencies, we propose a model-agnostic full-view data generation (Full-DG) method to enrich the training data from the perspectives of both poses and scenes. By hallucinating images with more balanced pose complexity and richer real-world scenes, Full-DG can help improve pose estimators’ robustness and generalizability. In addition, we introduce a plug-and-play adaptive category-aware loss (AC-loss) to alleviate the severe pixel-level imbalance between keypoints and backgrounds (i.e., around 1:600). Full-DG together with AC-loss can be readily applied to both the bottom-up and top-down models to improve their accuracy. Notably, plugging into the representative estimators HigherHRNet and HRNet, our method achieves substantial performance gains of 1.0%–2.9% AP on the COCO benchmark, and 1.0%–5.1% AP on the CrowdPose benchmark.},
  archive      = {J_TNNLS},
  author       = {Yan Dai and Xuanhan Wang and Lianli Gao and Jingkuan Song and Feng Zheng and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2023.3244957},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10857-10868},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Overcoming data deficiency for multi-person pose estimation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual contrastive learning network for graph clustering.
<em>TNNLS</em>, <em>35</em>(8), 10846–10856. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation is an important part of graph clustering. Recently, contrastive learning, which maximizes the mutual information between augmented graph views that share the same semantics, has become a popular and powerful paradigm for graph representation. However, in the process of patch contrasting, existing literature tends to learn all features into similar variables, i.e., representation collapse, leading to less discriminative graph representations. To tackle this problem, we propose a novel self-supervised learning method called dual contrastive learning network (DCLN), which aims to reduce the redundant information of learned latent variables in a dual manner. Specifically, the dual curriculum contrastive module (DCCM) is proposed, which approximates the node similarity matrix and feature similarity matrix to a high-order adjacency matrix and an identity matrix, respectively. By doing this, the informative information in high-order neighbors could be well collected and preserved while the irrelevant redundant features among representations could be eliminated, hence improving the discriminative capacity of the graph representation. Moreover, to alleviate the problem of sample imbalance during the contrastive process, we design a curriculum learning strategy, which enables the network to simultaneously learn reliable information from two levels. Extensive experiments on six benchmark datasets have demonstrated the effectiveness and superiority of the proposed algorithm compared with state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Xin Peng and Jieren Cheng and Xiangyan Tang and Jingxin Liu and Jiahua Wu},
  doi          = {10.1109/TNNLS.2023.3244397},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10846-10856},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual contrastive learning network for graph clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel robotic pushing and grasping method based on vision
transformer and convolution. <em>TNNLS</em>, <em>35</em>(8),
10832–10845. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic grasping techniques have been widely studied in recent years. However, it is always a challenging problem for robots to grasp in cluttered scenes. In this issue, objects are placed close to each other, and there is no space around for the robot to place the gripper, making it difficult to find a suitable grasping position. To solve this problem, this article proposes to use the combination of pushing and grasping (PG) actions to help grasp pose detection and robot grasping. We propose a pushing–grasping combined grasping network (GN), PG method based on transformer and convolution (PGTC). For the pushing action, we propose a vision transformer (ViT)-based object position prediction network pushing transformer network (PTNet), which can well capture the global and temporal features and can better predict the position of objects after pushing. To perform the grasping detection, we propose a cross dense fusion network (CDFNet), which can make full use of the RGB image and depth image, and fuse and refine them several times. Compared with previous networks, CDFNet is able to detect the optimal grasping position more accurately. Finally, we use the network for both simulation and actual UR3 robot grasping experiments and achieve SOTA performance. Video and dataset are available at https://youtu.be/Q58YE-Cc250 .},
  archive      = {J_TNNLS},
  author       = {Sheng Yu and Di-Hua Zhai and Yuanqing Xia},
  doi          = {10.1109/TNNLS.2023.3244186},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10832-10845},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel robotic pushing and grasping method based on vision transformer and convolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relationship between nonsmoothness in adversarial training,
constraints of attacks, and flatness in the input space. <em>TNNLS</em>,
<em>35</em>(8), 10817–10831. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training (AT) is a promising method to improve the robustness against adversarial attacks. However, its performance is not still satisfactory in practice compared with standard training. To reveal the cause of the difficulty of AT, we analyze the smoothness of the loss function in AT, which determines the training performance. We reveal that nonsmoothness is caused by the constraint of adversarial attacks and depends on the type of constraint. Specifically, the $L_{\infty} $ constraint can cause nonsmoothness more than the $L_{2}$ constraint. In addition, we found an interesting property for AT: the flatter loss surface in the input space tends to have the less smooth adversarial loss surface in the parameter space . To confirm that the nonsmoothness causes the poor performance of AT, we theoretically and experimentally show that smooth adversarial loss by EntropySGD (EnSGD) improves the performance of AT.},
  archive      = {J_TNNLS},
  author       = {Sekitoshi Kanai and Masanori Yamada and Hiroshi Takahashi and Yuki Yamanaka and Yasutoshi Ida},
  doi          = {10.1109/TNNLS.2023.3244172},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10817-10831},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Relationship between nonsmoothness in adversarial training, constraints of attacks, and flatness in the input space},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Self-guided partial graph propagation for incomplete
multiview clustering. <em>TNNLS</em>, <em>35</em>(8), 10803–10816. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study a more realistic challenging scenario in multiview clustering (MVC), referred to as incomplete MVC (IMVC) where some instances in certain views are missing. The key to IMVC is how to adequately exploit complementary and consistency information under the incompleteness of data. However, most existing methods address the incompleteness problem at the instance level and they require sufficient information to perform data recovery. In this work, we develop a new approach to facilitate IMVC based on the graph propagation perspective. Specifically, a partial graph is used to describe the similarity of samples for incomplete views, such that the issue of missing instances can be translated into the missing entries of the partial graph. In this way, a common graph can be adaptively learned to self-guide the propagation process by exploiting the consistency information, and the propagated graph of each view is in turn used to refine the common self-guided graph in an iterative manner. Thus, the associated missing entries can be inferred through graph propagation by exploiting the consistency information across all views. On the other hand, existing approaches focus on the consistency structure only, and the complementary information has not been sufficiently exploited due to the data incompleteness issue. By contrast, under the proposed graph propagation framework, an exclusive regularization term can be naturally adopted to exploit the complementary information in our method. Extensive experiments demonstrate the effectiveness of the proposed method in comparison with state-of-the-art methods. The source code of our method is available at the https://github.com/CLiu272/TNNLS-PGP .},
  archive      = {J_TNNLS},
  author       = {Cheng Liu and Rui Li and Si Wu and Hangjun Che and Dazhi Jiang and Zhiwen Yu and Hau-San Wong},
  doi          = {10.1109/TNNLS.2023.3244021},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10803-10816},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-guided partial graph propagation for incomplete multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic learning rate adaption for memristive deep
learning systems. <em>TNNLS</em>, <em>35</em>(8), 10791–10802. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a possible device to further enhance the performance of the hybrid complementary metal oxide semiconductor (CMOS) technology in the hardware, the memristor has attracted widespread attention in implementing efficient and compact deep learning (DL) systems. In this study, an automatic learning rate tuning method for memristive DL systems is presented. Memristive devices are utilized to adjust the adaptive learning rate in deep neural networks (DNNs). The speed of the learning rate adaptation process is fast at first and then becomes slow, which consist of the memristance or conductance adjustment process of the memristors. As a result, no manual tuning of learning rates is required in the adaptive back propagation (BP) algorithm. While cycle-to-cycle and device-to-device variations could be a significant issue in memristive DL systems, the proposed method appears robust to noisy gradients, various architectures, and different datasets. Moreover, fuzzy control methods for adaptive learning are presented for pattern recognition, such that the over-fitting issue can be well addressed. To our best knowledge, this is the first memristive DL system using an adaptive learning rate for image recognition. Another highlight of the presented memristive adaptive DL system is that quantized neural network architecture is utilized, and there is therefore a significant increase in the training efficiency, without the loss of testing accuracy.},
  archive      = {J_TNNLS},
  author       = {Yang Zhang and Linlin Shen},
  doi          = {10.1109/TNNLS.2023.3244006},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10791-10802},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic learning rate adaption for memristive deep learning systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Typicality-aware adaptive similarity matrix for unsupervised
learning. <em>TNNLS</em>, <em>35</em>(8), 10776–10790. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based clustering approaches, especially the family of spectral clustering, have been widely used in machine learning areas. The alternatives usually engage a similarity matrix that is constructed in advance or learned from a probabilistic perspective. However, unreasonable similarity matrix construction inevitably leads to performance degradation, and the sum-to-one probability constraints may make the approaches sensitive to noisy scenarios. To address these issues, the notion of typicality-aware adaptive similarity matrix learning is presented in this study. The typicality (possibility) rather than the probability of each sample being a neighbor of other samples is measured and adaptively learned. By introducing a robust balance term, the similarity between any pairs of samples is only related to the distance between them, yet it is not affected by other samples. Therefore, the impact caused by the noisy data or outliers can be alleviated, and meanwhile, the neighborhood structures can be well captured according to the joint distance between samples and their spectral embeddings. Moreover, the generated similarity matrix has block diagonal properties that are beneficial to correct clustering. Interestingly, the results optimized by the typicality-aware adaptive similarity matrix learning share the common essence with the Gaussian kernel function, and the latter can be directly derived from the former. Extensive experiments on synthetic and well-known benchmark datasets demonstrate the superiority of the proposed idea when comparing with some state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Jie Zhou and Can Gao and Xizhao Wang and Zhihui Lai and Jun Wan and Xiaodong Yue},
  doi          = {10.1109/TNNLS.2023.3243914},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10776-10790},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Typicality-aware adaptive similarity matrix for unsupervised learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed optimization of graph convolutional network
using subgraph variance. <em>TNNLS</em>, <em>35</em>(8), 10764–10775.
(<a href="https://doi.org/10.1109/TNNLS.2023.3243904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, distributed graph convolutional networks (GCNs) training frameworks have achieved great success in learning the representation of graph-structured data with large sizes. However, existing distributed GCN training frameworks require enormous communication costs since a multitude of dependent graph data need to be transmitted from other processors. To address this issue, we propose a graph augmentation-based distributed GCN framework (GAD). In particular, GAD has two main components: GAD-Partition and GAD-Optimizer . We first propose an augmentation-based graph partition (GAD-Partition) that can divide the input graph into augmented subgraphs to reduce communication by selecting and storing as few significant vertices of other processors as possible. To further speed up distributed GCN training and improve the quality of the training result, we design a subgraph variance-based importance calculation formula and propose a novel weighted global consensus method, collectively referred to as GAD-Optimizer . This optimizer adaptively adjusts the importance of subgraphs to reduce the effect of extra variance introduced by GAD-Partition on distributed GCN training. Extensive experiments on four large-scale real-world datasets demonstrate that our framework significantly reduces the communication overhead ( $\approx 50\%$ ), improves the convergence speed ( $\approx 2 \times $ ) of distributed GCN training, and obtains a slight gain in accuracy ( $\approx 0.45\%$ ) based on minimal redundancy compared to the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Taige Zhao and Xiangyu Song and Man Li and Jianxin Li and Wei Luo and Imran Razzak},
  doi          = {10.1109/TNNLS.2023.3243904},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10764-10775},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed optimization of graph convolutional network using subgraph variance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-related saliency for few-shot image classification.
<em>TNNLS</em>, <em>35</em>(8), 10751–10763. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A weakness of the existing metric-based few-shot classification method is that task-unrelated objects or backgrounds may mislead the model since the small number of samples in the support set is insufficient to reveal the task-related targets. An essential cue of human wisdom in the few-shot classification task is that they can recognize the task-related targets by a glimpse of support images without being distracted by task-unrelated things. Thus, we propose to explicitly learn task-related saliency features and make use of them in the metric-based few-shot learning schema. We divide the tackling of the task into three phases, namely, the modeling, the analyzing, and the matching. In the modeling phase, we introduce a saliency sensitive module (SSM), which is an inexact supervision task jointly trained with a standard multiclass classification task. SSM not only enhances the fine-grained representation of feature embedding but also can locate the task-related saliency features. Meanwhile, we propose a self-training-based task-related saliency network (TRSN) which is a lightweight network to distill task-related salience produced by SSM. In the analyzing phase, we freeze TRSN and use it to handle novel tasks. TRSN extracts task-relevant features while suppressing the disturbing task-unrelated features. We, therefore, can discriminate samples accurately in the matching phase by strengthening the task-related features. We conduct extensive experiments on five-way 1-shot and 5-shot settings to evaluate the proposed method. Results show that our method achieves a consistent performance gain on benchmarks and achieves the state-of-the-art.},
  archive      = {J_TNNLS},
  author       = {Zhenyu Zhou and Lei Luo and Sihang Zhou and Wang Li and Xihong Yang and Xinwang Liu and En Zhu},
  doi          = {10.1109/TNNLS.2023.3243903},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10751-10763},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Task-related saliency for few-shot image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperspectral tensor completion using low-rank modeling and
convex functional analysis. <em>TNNLS</em>, <em>35</em>(8), 10736–10750.
(<a href="https://doi.org/10.1109/TNNLS.2023.3243808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral tensor completion (HTC) for remote sensing, critical for advancing space exploration and other satellite imaging technologies, has drawn considerable attention from recent machine learning community. Hyperspectral image (HSI) contains a wide range of narrowly spaced spectral bands hence forming unique electrical magnetic signatures for distinct materials, and thus plays an irreplaceable role in remote material identification. Nevertheless, remotely acquired HSIs are of low data purity and quite often incompletely observed or corrupted during transmission. Therefore, completing the 3-D hyperspectral tensor, involving two spatial dimensions and one spectral dimension, is a crucial signal processing task for facilitating the subsequent applications. Benchmark HTC methods rely on either supervised learning or nonconvex optimization. As reported in recent machine learning literature, John ellipsoid (JE) in functional analysis is a fundamental topology for effective hyperspectral analysis. We therefore attempt to adopt this key topology in this work, but this induces a dilemma that the computation of JE requires the complete information of the entire HSI tensor that is, however, unavailable under the HTC problem setting. We resolve the dilemma, decouple HTC into convex subproblems ensuring computational efficiency, and show state-of-the-art HTC performances of our algorithm. We also demonstrate that our method has improved the subsequent land cover classification accuracy on the recovered hyperspectral tensor.},
  archive      = {J_TNNLS},
  author       = {Chia-Hsiang Lin and Yangrui Liu and Chong-Yung Chi and Chih-Chung Hsu and Hsuan Ren and Tony Q. S. Quek},
  doi          = {10.1109/TNNLS.2023.3243808},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10736-10750},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral tensor completion using low-rank modeling and convex functional analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid neuromorphic object tracking and classification
framework for real-time systems. <em>TNNLS</em>, <em>35</em>(8),
10726–10735. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning inference that needs to largely take place on the “edge” is a highly computational and memory intensive workload, making it intractable for low-power, embedded platforms such as mobile nodes and remote security applications. To address this challenge, this article proposes a real-time, hybrid neuromorphic framework for object tracking and classification using event-based cameras that possess desirable properties such as low-power consumption (5–14 mW) and high dynamic range (120 dB). Nonetheless, unlike traditional approaches of using event-by-event processing, this work uses a mixed frame and event approach to get energy savings with high performance. Using a frame-based region proposal method based on the density of foreground events, a hardware-friendly object tracking scheme is implemented using the apparent object velocity while tackling occlusion scenarios. The frame-based object track input is converted back to spikes for TrueNorth (TN) classification via the energy-efficient deep network (EEDN) pipeline. Using originally collected datasets, we train the TN model on the hardware track outputs, instead of using ground truth object locations as commonly done, and demonstrate the ability of our system to handle practical surveillance scenarios. As an alternative tracker paradigm, we also propose a continuous-time tracker with C++ implementation where each event is processed individually, which better exploits the low latency and asynchronous nature of neuromorphic vision sensors. Subsequently, we extensively compare the proposed methodologies to state-of-the-art event-based and frame-based methods for object tracking and classification, and demonstrate the use case of our neuromorphic approach for real-time and embedded applications without sacrificing performance. Finally, we also showcase the efficacy of the proposed neuromorphic system to a standard RGB camera setup when simultaneously evaluated over several hours of traffic recordings.},
  archive      = {J_TNNLS},
  author       = {Andrés Ussa and Chockalingam Senthil Rajen and Tarun Pulluri and Deepak Singla and Jyotibdha Acharya and Gideon Fu Chuanrong and Arindam Basu and Bharath Ramesh},
  doi          = {10.1109/TNNLS.2023.3243679},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10726-10735},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid neuromorphic object tracking and classification framework for real-time systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Distributed estimator-based event-triggered neuro-adaptive
control for leader–follower consensus of strict-feedback nonlinear
multiagent systems. <em>TNNLS</em>, <em>35</em>(8), 10713–10725. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the leader–follower consensus problem for strict-feedback nonlinear multiagent systems under a dual-terminal event-triggered mechanism. Compared with the existing event-triggered recursive consensus control design, the primary contribution of this article is the development of a distributed estimator-based event-triggered neuro-adaptive consensus control methodology. In particular, by introducing a dynamic event-triggered communication mechanism without continuous monitoring neighbors’ information, a novel distributed event-triggered estimator in chain form is constructed to provide the leader’s information to the followers. Subsequently, the distributed estimator is utilized to consensus control via backstepping design. To further decrease information transmission, a neuro-adaptive control and an event-triggered mechanism setting on the control channel are codesigned via the function approximate approach. A theoretical analysis shows that all the closed-loop signals are bounded under the developed control methodology, and the estimation of the tracking error asymptotically converges to zero, i.e., the leader–follower consensus is guaranteed. Finally, simulation studies and comparisons are conducted to verify the effectiveness of the proposed control method.},
  archive      = {J_TNNLS},
  author       = {Wei Wang and Yongming Li and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2023.3243627},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10713-10725},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed estimator-based event-triggered neuro-adaptive control for Leader–Follower consensus of strict-feedback nonlinear multiagent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). EEG-based motor imagery recognition framework via
multisubject dynamic transfer and iterative self-training.
<em>TNNLS</em>, <em>35</em>(8), 10698–10712. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust decoding model that can efficiently deal with the subject and period variation is urgently needed to apply the brain–computer interface (BCI) system. The performance of most electroencephalogram (EEG) decoding models depends on the characteristics of specific subjects and periods, which require calibration and training with annotated data prior to application. However, this situation will become unacceptable as it would be difficult for subjects to collect data for an extended period, especially in the rehabilitation process of disability based on motor imagery (MI). To address this issue, we propose an unsupervised domain adaptation framework called iterative self-training multisubject domain adaptation (ISMDA) that focuses on the offline MI task. First, the feature extractor is purposefully designed to map the EEG to a latent space of discriminative representations. Second, the attention module based on dynamic transfer matches the source domain and target domain samples with a higher coincidence degree in latent space. Then, an independent classifier oriented to the target domain is employed in the first stage of the iterative training process to cluster the samples of the target domain through similarity. Finally, a pseudolabel algorithm based on certainty and confidence is employed in the second stage of the iterative training process to adequately calibrate the error between prediction and empirical probabilities. To evaluate the effectiveness of the model, extensive testing has been performed on three publicly available MI datasets, the BCI IV IIa, the High gamma dataset, and Kwon et al. datasets. The proposed method achieved 69.51%, 82.38%, and 90.98% cross-subject classification accuracy on the three datasets, which outperforms the current state-of-the-art offline algorithms. Meanwhile, all results demonstrated that the proposed method could address the main challenges of the offline MI paradigm.},
  archive      = {J_TNNLS},
  author       = {He Wang and Peiyin Chen and Meng Zhang and Jianbo Zhang and Xinlin Sun and Mengyu Li and Xiong Yang and Zhongke Gao},
  doi          = {10.1109/TNNLS.2023.3243339},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10698-10712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {EEG-based motor imagery recognition framework via multisubject dynamic transfer and iterative self-training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network-based adaptive tracking control for
denitrification and aeration processes with time delays. <em>TNNLS</em>,
<em>35</em>(8), 10687–10697. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wastewater treatment process (WWTP), consisting of a class of physical, chemical, and biological phenomena, is an important means to reduce environmental pollution and improve recycling efficiency of water resources. Considering characteristics of the complexities, uncertainties, nonlinearities, and multitime delays in WWTPs, an adaptive neural controller is presented to achieve the satisfying control performance for WWTPs. With the advantages of radial basis function neural networks (RBF NNs), the unknown dynamics in WWTPs are identified. Based on the mechanistic analysis, the time-varying delayed models of the denitrification and aeration processes are established. Based on the established delayed models, the Lyapunov–Krasovskii functional (LKF) is used to compensate for the time-varying delays caused by the push-flow and recycle flow phenomenon. The barrier Lyapunov function (BLF) is used to ensure that the dissolved oxygen (DO) and nitrate concentrations are always kept within the specified ranges though the time-varying delays and disturbances exist. Using Lyapunov theorem, the stability of the closed-loop system is proven. Finally, the proposed control method is carried out on the benchmark simulation model 1 (BSM1) to verify the effectiveness and practicability.},
  archive      = {J_TNNLS},
  author       = {Junfei Qiao and Dapeng Li and Honggui Han},
  doi          = {10.1109/TNNLS.2023.3243299},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10687-10697},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based adaptive tracking control for denitrification and aeration processes with time delays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BiFSMNv2: Pushing binary neural networks for keyword
spotting to real-network performance. <em>TNNLS</em>, <em>35</em>(8),
10674–10686. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks, such as the deep-FSMN, have been widely studied for keyword spotting (KWS) applications while suffering expensive computation and storage. Therefore, network compression technologies such as binarization are studied to deploy KWS models on edge. In this article, we present a strong yet efficient binary neural network for KWS, namely, BiFSMNv2, pushing it to the real-network accuracy performance. First, we present a dual-scale thinnable 1-bit-architecture (DTA) to recover the representation capability of the binarized computation units by dual-scale activation binarization and liberate the speedup potential from an overall architecture perspective. Second, we also construct a frequency-independent distillation (FID) scheme for KWS binarization-aware training, which distills the high- and low-frequency components independently to mitigate the information mismatch between full-precision and binarized representations. Moreover, we propose the learning propagation binarizer (LPB), a general and efficient binarizer that enables the forward and backward propagation of binary KWS networks to be continuously improved through learning. We implement and deploy BiFSMNv2 on ARMv8 real-world hardware with a novel fast bitwise computation kernel (FBCK), which is proposed to fully use registers and increase instruction throughput. Comprehensive experiments show our BiFSMNv2 outperforms the existing binary networks for KWS by convincing margins across different datasets and achieves comparable accuracy with the full-precision networks (only a tiny 1.51% drop on Speech Commands V1–12). We highlight that benefiting from the compact architecture and optimized hardware kernel, BiFSMNv2 can achieve an impressive $25.1\times $ speedup and $20.2\times $ storage-saving on edge hardware.},
  archive      = {J_TNNLS},
  author       = {Haotong Qin and Xudong Ma and Yifu Ding and Xiaoyang Li and Yang Zhang and Zejun Ma and Jiakai Wang and Jie Luo and Xianglong Liu},
  doi          = {10.1109/TNNLS.2023.3243259},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10674-10686},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BiFSMNv2: Pushing binary neural networks for keyword spotting to real-network performance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning complementary spatial–temporal transformer for
video salient object detection. <em>TNNLS</em>, <em>35</em>(8),
10663–10673. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Besides combining appearance and motion information, another crucial factor for video salient object detection (VSOD) is to mine spatial–temporal (ST) knowledge, including complementary long–short temporal cues and global–local spatial context from neighboring frames. However, the existing methods only explored part of them and ignored their complementarity. In this article, we propose a novel complementary ST transformer (CoSTFormer) for VSOD, which has a short-global branch and a long-local branch to aggregate complementary ST contexts. The former integrates the global context from the neighboring two frames using dense pairwise attention, while the latter is designed to fuse long-term temporal information from more consecutive frames with local attention windows. In this way, we decompose the ST context into a short-global part and a long-local part and leverage the powerful transformer to model the context relationship and learn their complementarity. To solve the contradiction between local window attention and object motion, we propose a novel flow-guided window attention (FGWA) mechanism to align the attention windows with object and camera movements. Furthermore, we deploy CoSTFormer on fused appearance and motion features, thus enabling the effective combination of all three VSOD factors. Besides, we present a pseudo video generation method to synthesize sufficient video clips from static images for training ST saliency models. Extensive experiments have verified the effectiveness of our method and illustrated that we achieve new state-of-the-art results on several benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Nian Liu and Kepan Nan and Wangbo Zhao and Xiwen Yao and Junwei Han},
  doi          = {10.1109/TNNLS.2023.3243246},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10663-10673},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning complementary Spatial–Temporal transformer for video salient object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic loss weighting for multiorgan segmentation in
medical images. <em>TNNLS</em>, <em>35</em>(8), 10651–10662. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks often suffer from performance inconsistency for multiorgan segmentation in medical images; some organs are segmented far worse than others. The main reason might be organs with different levels of learning difficulty for segmentation mapping, due to variations such as size, texture complexity, shape irregularity, and imaging quality. In this article, we propose a principled class-reweighting algorithm, termed dynamic loss weighting, which dynamically assigns a larger loss weight to organs if they are discriminated as more difficult to learn according to the data and network’s status, for forcing the network to learn from them more to maximally promote the performance consistency. This new algorithm uses an extra autoencoder to measure the discrepancy between the segmentation network’s output and the ground truth and dynamically estimates the loss weight of organs per the contribution of the organ to the new updated discrepancy. It can capture the variation in organs’ learning difficult during training, and it is neither sensitive to data’s property nor dependent on human priors. We evaluate this algorithm in two multiorgan segmentation tasks: abdominal organs and head–neck structures, on publicly available datasets, with positive results obtained from extensive experiments which confirm the validity and effectiveness. Source codes are available at: https://github.com/YouyiSong/Dynamic-Loss-Weighting .},
  archive      = {J_TNNLS},
  author       = {Youyi Song and Jeremy Yuen-Chun Teoh and Kup-Sze Choi and Jing Qin},
  doi          = {10.1109/TNNLS.2023.3243241},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10651-10662},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic loss weighting for multiorgan segmentation in medical images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Taming self-supervised learning for presentation attack
detection: De-folding and de-mixing. <em>TNNLS</em>, <em>35</em>(8),
10639–10650. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric systems are vulnerable to presentation attacks (PAs) performed using various PA instruments (PAIs). Even though there are numerous PA detection (PAD) techniques based on both deep learning and hand-crafted features, the generalization of PAD for unknown PAI is still a challenging problem. In this work, we empirically prove that the initialization of the PAD model is a crucial factor for generalization, which is rarely discussed in the community. Based on such observation, we proposed a self-supervised learning-based method, denoted as DF-DM. Specifically, DF-DM is based on a global–local view coupled with d e- f olding and d e- m ixing to derive the task-specific representation for PAD. During de-folding, the proposed technique will learn region-specific features to represent samples in a local pattern by explicitly minimizing the generative loss. While de-mixing drives detectors to obtain the instance-specific features with global information for more comprehensive representation by minimizing the interpolation-based consistency. Extensive experimental results show that the proposed method can achieve significant improvements in terms of both face and fingerprint PAD in more complicated and hybrid datasets when compared with the state-of-the-art methods. When training in CASIA-FASD and Idiap Replay-Attack, the proposed method can achieve an 18.60% equal error rate (EER) in OULU-NPU and MSU-MFSD, exceeding the baseline performance by 9.54%. The source code of the proposed technique is available at https://github.com/kongzhecn/dfdm .},
  archive      = {J_TNNLS},
  author       = {Zhe Kong and Wentian Zhang and Feng Liu and Wenhan Luo and Haozhe Liu and Linlin Shen and Raghavendra Ramachandra},
  doi          = {10.1109/TNNLS.2023.3243229},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10639-10650},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Taming self-supervised learning for presentation attack detection: De-folding and de-mixing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Repetitive impedance learning-based physically human–robot
interactive control. <em>TNNLS</em>, <em>35</em>(8), 10629–10638. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based impedance learning control can provide variable impedance regulation for robots through online impedance learning without interaction force sensing. However, the existing related results only guarantee the closed-loop control systems to be uniformly ultimately bounded (UUB) and require the human impedance profiles being periodic, iteration-dependent, or slowly varying. In this article, a repetitive impedance learning control approach is proposed for physical human–robot interaction (PHRI) in repetitive tasks. The proposed control is composed of a proportional-differential (PD) control term, an adaptive control term, and a repetitive impedance learning term. Differential adaptation with projection modification is designed for estimating robotic parameters uncertainties in the time domain, while fully saturated repetitive learning is proposed for estimating time-varying human impedance uncertainties in the iterative domain. Uniform convergence of tracking errors is guaranteed by the PD control and the use of projection and full saturation in the uncertainties estimation and is theoretically proved based on a Lyapunov-like analysis. In impedance profiles, the stiffness and damping are composed of an iteration-independent term and an iteration- dependent disturbance, which are estimated by repetitive learning and compressed by the PD control, respectively. Therefore, the developed approach can be applied to the PHRI where iteration-dependent disturbances exist in the stiffness and damping. The control effectiveness and advantages are validated by simulations on a parallel robot in a repetitive following task.},
  archive      = {J_TNNLS},
  author       = {Tairen Sun and Jiantao Yang and Yongping Pan and Hongliu Yu},
  doi          = {10.1109/TNNLS.2023.3243091},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10629-10638},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Repetitive impedance learning-based physically Human–Robot interactive control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse optimal adaptive neural control for
state-constrained nonlinear systems. <em>TNNLS</em>, <em>35</em>(8),
10617–10628. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing a performance objective during control operation while also ensuring constraint satisfactions at all times is important in practical applications. Existing works on solving this problem usually require a complicated and time-consuming learning procedure by employing neural networks, and the results are only applicable for simple or time-invariant constraints. In this work, these restrictions are removed by a newly proposed adaptive neural inverse approach . In our approach, a new universal barrier function, which is able to handle various dynamic constraints in a unified manner, is proposed to transform the constrained system into an equivalent one with no constraint. Based on this transformation, a switched-type auxiliary controller and a modified criterion for inverse optimal stabilization are proposed to design an adaptive neural inverse optimal controller. It is proven that optimal performance is achieved with a computationally attractive learning mechanism, and all the constraints are never violated. Besides, improved transient performance is obtained in the sense that the bound of the tracking error could be explicitly designed by users. An illustrative example verifies the proposed methods.},
  archive      = {J_TNNLS},
  author       = {Kaixin Lu and Zhi Liu and Haoyong Yu and C. L. Philip Chen and Yun Zhang},
  doi          = {10.1109/TNNLS.2023.3243084},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10617-10628},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Inverse optimal adaptive neural control for state-constrained nonlinear systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STDAN: Deformable attention network for space-time video
super-resolution. <em>TNNLS</em>, <em>35</em>(8), 10606–10616. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The target of space–time video super-resolution (STVSR) is to increase the spatial–temporal resolution of low-resolution (LR) and low-frame-rate (LFR) videos. Recent approaches based on deep learning have made significant improvements, but most of them only use two adjacent frames, that is, short-term features, to synthesize the missing frame embedding, which cannot fully explore the information flow of consecutive input LR frames. In addition, existing STVSR models hardly exploit the temporal contexts explicitly to assist high-resolution (HR) frame reconstruction. To address these issues, in this article, we propose a deformable attention network called STDAN for STVSR. First, we devise a long short-term feature interpolation (LSTFI) module that is capable of excavating abundant content from more neighboring input frames for the interpolation process through a bidirectional recurrent neural network (RNN) structure. Second, we put forward a spatial–temporal deformable feature aggregation (STDFA) module, in which spatial and temporal contexts in dynamic video frames are adaptively captured and aggregated to enhance SR reconstruction. Experimental results on several datasets demonstrate that our approach outperforms state-of-the-art STVSR methods. The code is available at https://github.com/littlewhitesea/STDAN .},
  archive      = {J_TNNLS},
  author       = {Hai Wang and Xiaoyu Xiang and Yapeng Tian and Wenming Yang and Qingmin Liao},
  doi          = {10.1109/TNNLS.2023.3243029},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10606-10616},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {STDAN: Deformable attention network for space-time video super-resolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Spatial–temporal co-attention learning for diagnosis of
mental disorders from resting-state fMRI data. <em>TNNLS</em>,
<em>35</em>(8), 10591–10605. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroimaging techniques have been widely adopted to detect the neurological brain structures and functions of the nervous system. As an effective noninvasive neuroimaging technique, functional magnetic resonance imaging (fMRI) has been extensively used in computer-aided diagnosis (CAD) of mental disorders, e.g., autism spectrum disorder (ASD) and attention deficit/hyperactivity disorder (ADHD). In this study, we propose a spatial–temporal co-attention learning (STCAL) model for diagnosing ASD and ADHD from fMRI data. In particular, a guided co-attention (GCA) module is developed to model the intermodal interactions of spatial and temporal signal patterns. A novel sliding cluster attention module is designed to address global feature dependency of self-attention mechanism in fMRI time series. Comprehensive experimental results demonstrate that our STCAL model can achieve competitive accuracies of 73.0 ± 4.5%, 72.0 ± 3.8%, and 72.5 ± 4.2% on the ABIDE I, ABIDE II, and ADHD-200 datasets, respectively. Moreover, the potential for feature pruning based on the co-attention scores is validated by the simulation experiment. The clinical interpretation analysis of STCAL can allow medical professionals to concentrate on the discriminative regions of interest and key time frames from fMRI data.},
  archive      = {J_TNNLS},
  author       = {Rui Liu and Zhi-An Huang and Yao Hu and Zexuan Zhu and Ka-Chun Wong and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2023.3243000},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10591-10605},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatial–Temporal co-attention learning for diagnosis of mental disorders from resting-state fMRI data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding short-range memory effects in deep neural
networks. <em>TNNLS</em>, <em>35</em>(8), 10576–10590. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) is of fundamental importance in deep learning. Despite its simplicity, elucidating its efficacy remains challenging. Conventionally, the success of SGD is ascribed to the stochastic gradient noise (SGN) incurred in the training process. Based on this consensus, SGD is frequently treated and analyzed as the Euler–Maruyama discretization of stochastic differential equations (SDEs) driven by either Brownian or Lévy stable motion. In this study, we argue that SGN is neither Gaussian nor Lévy stable. Instead, inspired by the short-range correlation emerging in the SGN series, we propose that SGD can be viewed as a discretization of an SDE driven by fractional Brownian motion (FBM). Accordingly, the different convergence behavior of SGD dynamics is well-grounded. Moreover, the first passage time of an SDE driven by FBM is approximately derived. The result suggests a lower escaping rate for a larger Hurst parameter, and thus, SGD stays longer in flat minima. This happens to coincide with the well-known phenomenon that SGD favors flat minima that generalize well. Extensive experiments are conducted to validate our conjecture, and it is demonstrated that short-range memory effects persist across various model architectures, datasets, and training strategies. Our study opens up a new perspective and may contribute to a better understanding of SGD.},
  archive      = {J_TNNLS},
  author       = {Chengli Tan and Jiangshe Zhang and Junmin Liu},
  doi          = {10.1109/TNNLS.2023.3242969},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10576-10590},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding short-range memory effects in deep neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redefining neural architecture search of heterogeneous
multinetwork models by characterizing variation operators and model
components. <em>TNNLS</em>, <em>35</em>(8), 10561–10575. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With neural architecture search (NAS) methods gaining ground on manually designed deep neural networks—even more rapidly as model sophistication escalates—the research trend is shifting toward arranging different and often increasingly complex NAS spaces. In this conjuncture, delineating algorithms which can efficiently explore these search spaces can result in a significant improvement over currently used methods, which, in general, randomly select the structural variation operator, hoping for a performance gain. In this article, we investigate the effect of different variation operators in a complex domain, that of multinetwork heterogeneous neural models. These models have an extensive and complex search space of structures as they require multiple subnetworks within the general model in order to answer different output types. From that investigation, we extract a set of general guidelines whose application is not limited to that particular type of model and are useful to determine the direction in which an architecture optimization method could find the largest improvement. To deduce the set of guidelines, we characterize both the variation operators, according to their effect on the complexity and performance of the model; and the models, relying on diverse metrics which estimate the quality of the different parts composing it.},
  archive      = {J_TNNLS},
  author       = {Unai Garciarena and Roberto Santana and Alexander Mendiburu},
  doi          = {10.1109/TNNLS.2023.3242877},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10561-10575},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Redefining neural architecture search of heterogeneous multinetwork models by characterizing variation operators and model components},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DSIL-DDI: A domain-invariant substructure interaction
learning for generalizable drug–drug interaction prediction.
<em>TNNLS</em>, <em>35</em>(8), 10552–10560. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug–drug interactions (DDIs) trigger unexpected pharmacological effects in vivo, often with unknown causal mechanisms. Deep learning methods have been developed to better understand DDI. However, learning domain-invariant representations for DDI remains a challenge. Generalizable DDI predictions are closer to reality than source domain predictions. For existing methods, it is difficult to achieve out-of-distribution (OOD) predictions. In this article, focusing on substructure interaction, we propose DSIL-DDI, a pluggable substructure interaction module that can learn domain-invariant representations of DDIs from source domain. We evaluate DSIL-DDI on three scenarios: the transductive setting (all drugs in test set appear in training set), the inductive setting (test set contains new drugs that were not present in training set), and OOD generalization setting (training set and test set belong to two different datasets). The results demonstrate that DSIL-DDI improve the generalization and interpretability of DDI prediction modeling and provides valuable insights for OOD DDI predictions. DSIL-DDI can help doctors ensuring the safety of drug administration and reducing the harm caused by drug abuse.},
  archive      = {J_TNNLS},
  author       = {Zhenchao Tang and Guanxing Chen and Hualin Yang and Weihe Zhong and Calvin Yu-Chian Chen},
  doi          = {10.1109/TNNLS.2023.3242656},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10552-10560},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DSIL-DDI: A domain-invariant substructure interaction learning for generalizable Drug–Drug interaction prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projective incomplete multi-view clustering. <em>TNNLS</em>,
<em>35</em>(8), 10539–10551. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid development of multimedia technology and sensor technology, multi-view clustering (MVC) has become a research hotspot in machine learning, data mining, and other fields and has been developed significantly in the past decades. Compared with single-view clustering, MVC improves clustering performance by exploiting complementary and consistent information among different views. Such methods are all based on the assumption of complete views, which means that all the views of all the samples exist. It limits the application of MVC, because there are always missing views in practical situations. In recent years, many methods have been proposed to solve the incomplete MVC (IMVC) problem and a kind of popular method is based on matrix factorization (MF). However, such methods generally cannot deal with new samples and do not take into account the imbalance of information between different views. To address these two issues, we propose a new IMVC method, in which a novel and simple graph regularized projective consensus representation learning model is formulated for incomplete multi-view data clustering task. Compared with the existing methods, our method not only can obtain a set of projections to handle new samples but also can explore information of multiple views in a balanced way by learning the consensus representation in a unified low-dimensional subspace. In addition, a graph constraint is imposed on the consensus representation to mine the structural information inside the data. Experimental results on four datasets show that our method successfully accomplishes the IMVC task and obtain the best clustering performance most of the time. Our implementation is available at https://github.com/Dshijie/PIMVC .},
  archive      = {J_TNNLS},
  author       = {Shijie Deng and Jie Wen and Chengliang Liu and Ke Yan and Gehui Xu and Yong Xu},
  doi          = {10.1109/TNNLS.2023.3242473},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10539-10551},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Projective incomplete multi-view clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The symplectic adjoint method: Memory-efficient
backpropagation of neural-network-based differential equations.
<em>TNNLS</em>, <em>35</em>(8), 10526–10538. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of neural networks and numerical integration can provide highly accurate models of continuous-time dynamical systems and probabilistic distributions. However, if a neural network is used $ \boldsymbol {n}$ times during numerical integration, the whole computation graph can be considered as a network $ \boldsymbol {n}$ times deeper than the original. The backpropagation algorithm consumes memory in proportion to the number of uses times of the network size, causing practical difficulties. This is true even if a checkpointing scheme divides the computation graph into subgraphs. Alternatively, the adjoint method obtains a gradient by a numerical integration backward in time; although this method consumes memory only for single-network use, the computational cost of suppressing numerical errors is high. The symplectic adjoint method proposed in this study, an adjoint method solved by a symplectic integrator, obtains the exact gradient (up to rounding error) with memory proportional to the number of uses plus the network size. The theoretical analysis shows that it consumes much less memory than the naive backpropagation algorithm and checkpointing schemes. The experiments verify the theory, and they also demonstrate that the symplectic adjoint method is faster than the adjoint method and is more robust to rounding errors.},
  archive      = {J_TNNLS},
  author       = {Takashi Matsubara and Yuto Miyatake and Takaharu Yaguchi},
  doi          = {10.1109/TNNLS.2023.3242345},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10526-10538},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The symplectic adjoint method: Memory-efficient backpropagation of neural-network-based differential equations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Learning oriented object detection via naive geometric
computing. <em>TNNLS</em>, <em>35</em>(8), 10513–10525. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting oriented objects along with estimating their rotation information is one crucial step for image analysis, especially for remote sensing images. Despite that many methods proposed recently have achieved remarkable performance, most of them directly learn to predict object directions under the supervision of only one (e.g., the rotation angle) or a few (e.g., several coordinates) groundtruth (GT) values individually. Oriented object detection would be more accurate and robust if extra constraints, with respect to proposal and rotation information regression, are adopted for joint supervision during training. To this end, we propose a mechanism that simultaneously learns the regression of horizontal proposals, oriented proposals, and rotation angles of objects in a consistent manner, via naive geometric computing, as one additional steady constraint . An oriented center prior guided label assignment strategy is proposed for further enhancing the quality of proposals, yielding better performance. Extensive experiments on six datasets demonstrate the model equipped with our idea significantly outperforms the baseline by a large margin and several new state-of-the-art results are achieved without any extra computational burden during inference. Our proposed idea is simple and intuitive that can be readily implemented. Source codes are publicly available at: https://github.com/wangWilson/CGCDet.git .},
  archive      = {J_TNNLS},
  author       = {Yanjie Wang and Zhijun Zhang and Wenhui Xu and Liqun Chen and Guodong Wang and Luxin Yan and Sheng Zhong and Xu Zou},
  doi          = {10.1109/TNNLS.2023.3242323},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10513-10525},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning oriented object detection via naive geometric computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fixed-time noise-tolerance ZNN model for time-variant
inequality-constrained quaternion matrix least-squares problem.
<em>TNNLS</em>, <em>35</em>(8), 10503–10512. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presently, numerical algorithms for solving quaternion least-squares problems have been intensively studied and utilized in various disciplines. However, they are unsuitable for solving the corresponding time-variant problems, and thus few studies have explored the solution to the time-variant inequality-constrained quaternion matrix least-squares problem (TVIQLS). To do so, this article designs a fixed-time noise-tolerance zeroing neural network (FTNTZNN) model to determine the solution of the TVIQLS in a complex environment by exploiting the integral structure and the improved activation function (AF). The FTNTZNN model is immune to the effects of initial values and external noise, which is much superior to the conventional zeroing neural network (CZNN) models. Besides, detailed theoretical derivations about the global stability, the fixed-time (FXT) convergence, and the robustness of the FTNTZNN model are provided. Simulation results indicate that the FTNTZNN model has a shorter convergence time and superior robustness compared to other zeroing neural network (ZNN) models activated by ordinary AFs. At last, the construction method of the FTNTZNN model is successfully applied to the synchronization of Lorenz chaotic systems (LCSs), which shows the practical application value of the FTNTZNN model.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Penglin Cao and Wentong Song and Liu Luo and Wensheng Tang},
  doi          = {10.1109/TNNLS.2023.3242313},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10503-10512},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A fixed-time noise-tolerance ZNN model for time-variant inequality-constrained quaternion matrix least-squares problem},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Semisupervised change detection based on bihierarchical
feature aggregation and extraction network. <em>TNNLS</em>,
<em>35</em>(8), 10488–10502. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of remote sensing (RS) technology, high-resolution RS image change detection (CD) has been widely used in many applications. Pixel-based CD techniques are maneuverable and widely used, but vulnerable to noise interference. Object-based CD techniques can effectively utilize the abundant spectrum, texture, shape, and spatial information but easy-to-ignore details of RS images. How to combine the advantages of pixel-based methods and object-based methods remains a challenging problem. Besides, although supervised methods have the capability to learn from data, the true labels representing changed information of RS images are often hard to obtain. To address these issues, this article proposes a novel semisupervised CD framework for high-resolution RS images, which employs small amounts of true labeled data and a lot of unlabeled data to train the CD network. A bihierarchical feature aggregation and extraction network (BFAEN) is designed to achieve the pixelwise together with objectwise feature concatenation feature representation for the comprehensive utilization of the two-level features. In order to alleviate the coarseness and insufficiency of labeled samples, a confident learning algorithm is used to eliminate noisy labels and a novel loss function is designed for training the model using true- and pseudo-labels in a semisupervised fashion. Experimental results on real datasets demonstrate the effectiveness and superiority of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Mingyang Zhang and Tianqi Gao and Maoguo Gong and Shengqi Zhu and Yue Wu and Hao Li},
  doi          = {10.1109/TNNLS.2023.3242075},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10488-10502},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised change detection based on bihierarchical feature aggregation and extraction network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A target-driven planning approach for goal-directed dialog
systems. <em>TNNLS</em>, <em>35</em>(8), 10475–10487. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing dialog systems mainly build social bonds reactively with users for chitchat or assist users with specific tasks. In this work, we push forward to a promising yet under-explored proactive dialog paradigm called goal-directed dialog systems, where the “goal” refers to achieving the recommendation for a predetermined target topic through social conversations. We focus on how to make plans that naturally lead users to achieve the goal through smooth topic transitions. To this end, we propose a target-driven planning network (TPNet) to drive the system to transit between different conversation stages. Built upon the widely used transformer architecture, TPNet frames the complicated planning process as a sequence generation task, which plans a dialog path consisting of dialog actions and topics. We then apply our TPNet with planned content to guide dialog generation using various backbone models. Extensive experiments show that our approach obtains the state-of-the-art performance in automatic and human evaluations. The results demonstrate that TPNet affects the improvement of goal-directed dialog systems significantly.},
  archive      = {J_TNNLS},
  author       = {Jian Wang and Dongding Lin and Wenjie Li},
  doi          = {10.1109/TNNLS.2023.3242071},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10475-10487},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A target-driven planning approach for goal-directed dialog systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residual sketch learning for a feature-importance-based and
linguistically interpretable ensemble classifier. <em>TNNLS</em>,
<em>35</em>(8), 10461–10474. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by both the commonly used “from wholly coarse to locally fine” cognitive behavior and the recent finding that simple yet interpretable linear regression model should be a basic component of a classifier, a novel hybrid ensemble classifier called hybrid Takagi–Sugeno–Kang fuzzy classifier (H-TSK-FC) and its residual sketch learning (RSL) method are proposed. H-TSK-FC essentially shares the virtues of both deep and wide interpretable fuzzy classifiers and simultaneously has both feature-importance-based and linguistic-based interpretabilities. RSL method is featured as follows: 1) a global linear regression subclassifier on all original features of all training samples is generated quickly by the sparse representation-based linear regression subclassifier training procedure to identify/understand the importance of each feature and partition the output residuals of the incorrectly classified training samples into several residual sketches; 2) by using both the enhanced soft subspace clustering method (ESSC) for the linguistically interpretable antecedents of fuzzy rules and the least learning machine (LLM) for the consequents of fuzzy rules on residual sketches, several interpretable Takagi–Sugeno–Kang (TSK) fuzzy subclassifiers are stacked in parallel through residual sketches and accordingly generated to achieve local refinements; and 3) the final predictions are made to further enhance H-TSK-FC’s generalization capability and decide which interpretable prediction route should be used by taking the minimal-distance-based priority for all the constructed subclassifiers. In contrast to existing deep or wide interpretable TSK fuzzy classifiers, benefiting from the use of feature-importance-based interpretability, H-TSK-FC has been experimentally witnessed to have faster running speed and better linguistic interpretability (i.e., fewer rules and/or TSK fuzzy subclassifiers and smaller model complexities) yet keep at least comparable generalization capability.},
  archive      = {J_TNNLS},
  author       = {Zekang Bian and Jin Zhang and Fu-Lai Chung and Shitong Wang},
  doi          = {10.1109/TNNLS.2023.3242049},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10461-10474},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Residual sketch learning for a feature-importance-based and linguistically interpretable ensemble classifier},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). State estimation of switched time-delay complex networks
with strict decreasing LKF. <em>TNNLS</em>, <em>35</em>(8), 10451–10460.
(<a href="https://doi.org/10.1109/TNNLS.2023.3241955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State estimation issue is investigated for a switched complex network (CN) with time delay and external disturbances. The considered model is general with a one-sided Lipschitz (OSL) nonlinear term, which is less conservative than Lipschitz one and has wide applications. Adaptive mode-dependent nonidentical event-triggered control (ETC) mechanisms for only partial nodes are proposed for state estimators, which are not only more practical and flexible but also reduce the conservatism of the results. By using dwell-time (DT) segmentation and convex combination methods, a novel discretized Lyapunov–Krasovskii functional (LKF) is developed such that the value of LKF at switching instants is strict monotone decreasing, which makes it easy for nonweighted $\mathcal {L}_{2}$ -gain analysis without additional conservative transformation. The main results are given in the form of linear matrix inequalities (LMIs), by which the control gains of the state estimator are designed. A numerical example is given to illustrate the advantages of the novel analytical method.},
  archive      = {J_TNNLS},
  author       = {Meijie Zhang and Xinsong Yang and Qihan Qi and Ju H. Park},
  doi          = {10.1109/TNNLS.2023.3241955},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10451-10460},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {State estimation of switched time-delay complex networks with strict decreasing LKF},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning accurate label-specific features from partially
multilabeled data. <em>TNNLS</em>, <em>35</em>(8), 10436–10450. (<a
href="https://doi.org/10.1109/TNNLS.2023.3241921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an effective dimensionality reduction technique, which can speed up an algorithm and improve model performance such as predictive accuracy and result comprehensibility. The study of selecting label-specific features for each class label has attracted considerable attention since each class label might be determined by some inherent characteristics, where precise label information is required to guide label-specific feature selection. However, obtaining noise-free labels is quite difficult and impractical. In reality, each instance is often annotated by a candidate label set that comprises multiple ground-truth labels and other false-positive labels, termed partial multilabel (PML) learning scenario. Here, false-positive labels concealed in a candidate label set might induce the selection of false label-specific features while masking the intrinsic label correlations, which misleads the selection of relevant features and compromises the selection performance. To address this issue, a novel two-stage partial multilabel feature selection (PMLFS) approach is proposed, which elicits credible labels to guide accurate label-specific feature selection. First, the label confidence matrix is learned to help elicit ground-truth labels from the candidate label set via the label structure reconstruction strategy, each element of which indicates how likely a class label is ground truth. After that, based on distilled credible labels, a joint selection model, including label-specific feature learner and common feature learner, is designed to learn accurate label-specific features to each class label and common features for all class labels. Besides, label correlations are fused into the features selection process to facilitate the generation of an optimal feature subset. Extensive experimental results clearly validate the superiority of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Tiantian Xu and Yuanyuan Xu and Shiyu Yang and Binghao Li and Wenjie Zhang},
  doi          = {10.1109/TNNLS.2023.3241921},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10436-10450},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning accurate label-specific features from partially multilabeled data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangled feature representation for few-shot image
classification. <em>TNNLS</em>, <em>35</em>(8), 10422–10435. (<a
href="https://doi.org/10.1109/TNNLS.2023.3241919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the generalizable feature representation is critical to few-shot image classification. While recent works exploited task-specific feature embedding using meta-tasks for few-shot learning, they are limited in many challenging tasks as being distracted by the excursive features such as the background, domain, and style of the image samples. In this work, we propose a novel disentangled feature representation (DFR) framework, dubbed DFR, for few-shot learning applications. DFR can adaptively decouple the discriminative features that are modeled by the classification branch, from the class-irrelevant component of the variation branch. In general, most of the popular deep few-shot learning methods can be plugged in as the classification branch, thus DFR can boost their performance on various few-shot tasks. Furthermore, we propose a novel FS-DomainNet dataset based on DomainNet, for benchmarking the few-shot domain generalization (DG) tasks. We conducted extensive experiments to evaluate the proposed DFR on general, fine-grained, and cross-domain few-shot classification, as well as few-shot DG, using the corresponding four benchmarks, i.e., mini-ImageNet, tiered-ImageNet, Caltech-UCSD Birds 200–2011 (CUB), and the proposed FS-DomainNet. Thanks to the effective feature disentangling, the DFR-based few-shot classifiers achieved state-of-the-art results on all datasets.},
  archive      = {J_TNNLS},
  author       = {Hao Cheng and Yufei Wang and Haoliang Li and Alex C. Kot and Bihan Wen},
  doi          = {10.1109/TNNLS.2023.3241919},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10422-10435},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disentangled feature representation for few-shot image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STDNet: Rethinking disentanglement learning with information
theory. <em>TNNLS</em>, <em>35</em>(8), 10407–10421. (<a
href="https://doi.org/10.1109/TNNLS.2023.3241791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disentangled representation learning is typically achieved by a generative model, variational encoder (VAE). Existing VAE-based methods try to disentangle all the attributes simultaneously in a single hidden space, while the separation of the attribute from irrelevant information varies in complexity. Thus, it should be conducted in different hidden spaces. Therefore, we propose to disentangle the disentanglement itself by assigning the disentanglement of each attribute to different layers. To achieve this, we present a stair disentanglement net (STDNet), a stair-like structure network with each step corresponding to the disentanglement of an attribute. An information separation principle is employed to peel off the irrelevant information to form a compact representation of the targeted attribute within each step. Compact representations, thus, obtained together form the final disentangled representation. To ensure the final disentangled representation is compressed as well as complete with respect to the input data, we propose a variant of the information bottleneck (IB) principle, the stair IB (SIB) principle, to optimize a tradeoff between compression and expressiveness. In particular, for the assignment to the network steps, we define an attribute complexity metric to assign the attributes by the complexity ascending rule (CAR) that dictates a sequencing of the attribute disentanglement in ascending order of complexity. Experimentally, STDNet achieves state-of-the-art results in representation learning and image generation on multiple benchmarks, including Mixed National Institute of Standards and Technology database (MNIST), dSprites, and CelebA. Furthermore, we conduct thorough ablation experiments to show how the strategies employed here contribute to the performance, including neurons block, CAR, hierarchical structure, and variational form of SIB.},
  archive      = {J_TNNLS},
  author       = {Ziwen Liu and Mingqiang Li and Congying Han and Siqi Tang and Tiande Guo},
  doi          = {10.1109/TNNLS.2023.3241791},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10407-10421},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {STDNet: Rethinking disentanglement learning with information theory},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task credible pseudo-label learning for
semi-supervised crowd counting. <em>TNNLS</em>, <em>35</em>(8),
10394–10406. (<a
href="https://doi.org/10.1109/TNNLS.2023.3241211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a widely used semi-supervised learning strategy, self-training generates pseudo-labels to alleviate the labor-intensive and time-consuming annotation problems in crowd counting while boosting the model performance with limited labeled data and massive unlabeled data. However, the noise in the pseudo-labels of the density maps greatly hinders the performance of semi-supervised crowd counting. Although auxiliary tasks, e.g., binary segmentation, are utilized to help improve the feature representation learning ability, they are isolated from the main task, i.e., density map regression and the multi-task relationships are totally ignored. To address the above issues, we develop a multi-task credible pseudo-label learning (MTCP) framework for crowd counting, consisting of three multi-task branches, i.e., density regression as the main task, and binary segmentation and confidence prediction as the auxiliary tasks. Multi-task learning is conducted on the labeled data by sharing the same feature extractor for all three tasks and taking multi-task relations into account. To reduce epistemic uncertainty, the labeled data are further expanded, by trimming the labeled data according to the predicted confidence map for low-confidence regions, which can be regarded as an effective data augmentation strategy. For unlabeled data, compared with the existing works that only use the pseudo-labels of binary segmentation, we generate credible pseudo-labels of density maps directly, which can reduce the noise in pseudo-labels and therefore decrease aleatoric uncertainty. Extensive comparisons on four crowd-counting datasets demonstrate the superiority of our proposed model over the competing methods. The code is available at: https://github.com/ljq2000/MTCP .},
  archive      = {J_TNNLS},
  author       = {Pengfei Zhu and Jingqing Li and Bing Cao and Qinghua Hu},
  doi          = {10.1109/TNNLS.2023.3241211},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10394-10406},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-task credible pseudo-label learning for semi-supervised crowd counting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network layer algebra: A framework to measure
capacity and compression in deep learning. <em>TNNLS</em>,
<em>35</em>(8), 10380–10393. (<a
href="https://doi.org/10.1109/TNNLS.2023.3241100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new framework to measure the intrinsic properties of (deep) neural networks. While we focus on convolutional networks, our framework can be extrapolated to any network architecture. In particular, we evaluate two network properties, namely, capacity , which is related to expressivity , and compression , which is related to learnability . Both these properties depend only on the network structure and are independent of the network parameters. To this end, we propose two metrics: the first one, called layer complexity , captures the architectural complexity of any network layer; and, the second one, called layer intrinsic power , encodes how data are compressed along the network. The metrics are based on the concept of layer algebra , which is also introduced in this article. This concept is based on the idea that the global properties depend on the network topology, and the leaf nodes of any neural network can be approximated using local transfer functions, thereby allowing a simple computation of the global metrics. We show that our global complexity metric can be calculated and represented more conveniently than the widely used Vapnik–Chervonenkis (VC) dimension. We also compare the properties of various state-of-the-art architectures using our metrics and use the properties to analyze their accuracy on benchmark image classification datasets.},
  archive      = {J_TNNLS},
  author       = {Alberto Badías and Ashis G. Banerjee},
  doi          = {10.1109/TNNLS.2023.3241100},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10380-10393},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network layer algebra: A framework to measure capacity and compression in deep learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sliding mode control based on reinforcement learning for t-s
fuzzy fractional-order multiagent system with time-varying delays.
<em>TNNLS</em>, <em>35</em>(8), 10368–10379. (<a
href="https://doi.org/10.1109/TNNLS.2023.3241070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article researches the sliding mode control (SMC) for fuzzy fractional-order multiagent system (FOMAS) subject to time-varying delays over directed networks based on reinforcement learning (RL), $\alpha \in (0,1)$ . First, since there is information communication between an agent and another agent, a new distributed control policy $\xi _{i}(t)$ is introduced so that the sharing of signals is implemented through RL, whose propose is to minimize the error variables with learning. Then, different from the existed papers studying normal fuzzy MASs, a new stability basis of fuzzy FOMASs with time-varying delay terms is presented to guarantee that the states of each agent eventually converge to the smallest possible domain of 0 using Lyapunov–Krasovskii functionals, free weight matrix, and linear matrix inequality (LMI). Furthermore, in order to provide appropriate parameters for SMC, the RL algorithm is combined with SMC strategy, and the constraints on the initial conditions of the control input $u_{i}(t)$ are eliminated, so that the sliding motion satisfy the reachable condition within a finite time. Finally, to illustrate that the proposed protocol is valid, the results of the simulation and numerical examples are presented.},
  archive      = {J_TNNLS},
  author       = {Yuqing Yan and Huaguang Zhang and Jiayue Sun and Yingchun Wang},
  doi          = {10.1109/TNNLS.2023.3241070},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10368-10379},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sliding mode control based on reinforcement learning for T-S fuzzy fractional-order multiagent system with time-varying delays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PreCNet: Next-frame video prediction based on predictive
coding. <em>TNNLS</em>, <em>35</em>(8), 10353–10367. (<a
href="https://doi.org/10.1109/TNNLS.2023.3240857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive coding, currently a highly influential theory in neuroscience, has not been widely adopted in machine learning yet. In this work, we transform the seminal model of Rao and Ballard (1999) into a modern deep learning framework while remaining maximally faithful to the original schema. The resulting network we propose (PreCNet) is tested on a widely used next-frame video prediction benchmark, which consists of images from an urban environment recorded from a car-mounted camera, and achieves state-of-the-art performance. Performance on all measures (MSE, PSNR, and SSIM) was further improved when a larger training set (2M images from BDD100k) pointed to the limitations of the KITTI training set. This work demonstrates that an architecture carefully based on a neuroscience model, without being explicitly tailored to the task at hand, can exhibit exceptional performance.},
  archive      = {J_TNNLS},
  author       = {Zdenek Straka and Tomáš Svoboda and Matej Hoffmann},
  doi          = {10.1109/TNNLS.2023.3240857},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10353-10367},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PreCNet: Next-frame video prediction based on predictive coding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aperiodically intermittent event-triggered optimal average
consensus for nonlinear multi-agent systems. <em>TNNLS</em>,
<em>35</em>(8), 10338–10352. (<a
href="https://doi.org/10.1109/TNNLS.2023.3240427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with average consensus of multi-agent systems via intermittent event-triggered strategy. First, a novel intermittent event-triggered condition is designed and the corresponding piecewise differential inequality for the condition is established. Using the established inequality, several criteria on average consensus are obtained. Second, the optimality has been investigated based on average consensus. The optimal intermittent event-triggered strategy in the sense of Nash equilibrium and corresponding local Hamilton–Jacobi–Bellman equation are derived. Third, the adaptive dynamic programming algorithm for the optimal strategy and its neural network implementation with actor-critic architecture are also given. Finally, two numerical examples are presented to show the feasibility and effectiveness of our strategies.},
  archive      = {J_TNNLS},
  author       = {Lei Liu and Jinde Cao and Fawaz E. Alsaadi},
  doi          = {10.1109/TNNLS.2023.3240427},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10338-10352},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Aperiodically intermittent event-triggered optimal average consensus for nonlinear multi-agent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stability and bifurcation exploration of delayed neural
networks with radial-ring configuration and bidirectional coupling.
<em>TNNLS</em>, <em>35</em>(8), 10326–10337. (<a
href="https://doi.org/10.1109/TNNLS.2023.3240403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades, studying the dynamic performances of artificial neural networks (ANNs) is widely considered to be a good way to gain a deeper insight into actual neural networks. However, most models of ANNs are focused on a finite number of neurons and a single topology. These studies are inconsistent with actual neural networks composed of thousands of neurons and sophisticated topologies. There is still a discrepancy between theory and practice. In this article, not only a novel construction of a class of delayed neural networks with radial-ring configuration and bidirectional coupling is proposed, but also an effective analytical approach to dynamic performances of large-scale neural networks with a cluster of topologies is developed. First, Coates’ flow diagram is applied to acquire the characteristic equation of the system, which contains multiple exponential terms. Second, by means of the idea of the holistic element, the sum of the neuron synapse transmission delays is regarded as the bifurcation argument to investigate the stability of the zero equilibrium point and the beingness of Hopf bifurcation. Finally, multiple sets of computerized simulations are utilized to confirm the conclusions. The simulation results expound that the increase in transmission delay may cause a leading impact on the generation of Hopf bifurcation. Meanwhile, the number and the self-feedback coefficient of neurons are also playing significant roles in the appearance of periodic oscillations.},
  archive      = {J_TNNLS},
  author       = {Yunxiang Lu and Min Xiao and Jiajin He and Zhengxin Wang},
  doi          = {10.1109/TNNLS.2023.3240403},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10326-10337},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability and bifurcation exploration of delayed neural networks with radial-ring configuration and bidirectional coupling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3-d quantum-inspired self-supervised tensor network for
volumetric segmentation of medical images. <em>TNNLS</em>,
<em>35</em>(8), 10312–10325. (<a
href="https://doi.org/10.1109/TNNLS.2023.3240238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel shallow 3-D self-supervised tensor neural network in quantum formalism for volumetric segmentation of medical images with merits of obviating training and supervision. The proposed network is referred to as the 3-D quantum-inspired self-supervised tensor neural network (3-D-QNet). The underlying architecture of 3-D-QNet is composed of a trinity of volumetric layers, viz., input, intermediate, and output layers interconnected using an $\mathcal {S}$ -connected third-order neighborhood-based topology for voxelwise processing of 3-D medical image data, suitable for semantic segmentation. Each of the volumetric layers contains quantum neurons designated by qubits or quantum bits. The incorporation of tensor decomposition in quantum formalism leads to faster convergence of network operations to preclude the inherent slow convergence problems faced by the classical supervised and self-supervised networks. The segmented volumes are obtained once the network converges. The suggested 3-D-QNet is tailored and tested on the BRATS 2019 Brain MR image dataset and the Liver Tumor Segmentation Challenge (LiTS17) dataset extensively in our experiments. The 3-D-QNet has achieved promising dice similarity (DS) as compared with the time-intensive supervised convolutional neural network (CNN)-based models, such as 3-D-UNet, voxelwise residual network (VoxResNet), Dense-Res-Inception Net (DRINet), and 3-D-ESPNet, thereby showing a potential advantage of our self-supervised shallow network on facilitating semantic segmentation.},
  archive      = {J_TNNLS},
  author       = {Debanjan Konar and Siddhartha Bhattacharyya and Tapan K. Gandhi and Bijaya K. Panigrahi and Richard Jiang},
  doi          = {10.1109/TNNLS.2023.3240238},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10312-10325},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {3-D quantum-inspired self-supervised tensor network for volumetric segmentation of medical images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based deep spiking neural networks for temporal
credit assignment problems. <em>TNNLS</em>, <em>35</em>(8), 10301–10311.
(<a href="https://doi.org/10.1109/TNNLS.2023.3240176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The temporal credit assignment (TCA) problem, which aims to detect predictive features hidden in distracting background streams, remains a core challenge in biological and machine learning. Aggregate-label (AL) learning is proposed by researchers to resolve this problem by matching spikes with delayed feedback. However, the existing AL learning algorithms only consider the information of a single timestep, which is inconsistent with the real situation. Meanwhile, there is no quantitative evaluation method for TCA problems. To address these limitations, we propose a novel attention-based TCA (ATCA) algorithm and a minimum editing distance (MED)-based quantitative evaluation method. Specifically, we define a loss function based on the attention mechanism to deal with the information contained within the spike clusters and use MED to evaluate the similarity between the spike train and the target clue flow. Experimental results on musical instrument recognition (MedleyDB), speech recognition (TIDIGITS), and gesture recognition (DVS128-Gesture) show that the ATCA algorithm can reach the state-of-the-art (SOTA) level compared with other AL learning algorithms.},
  archive      = {J_TNNLS},
  author       = {Lang Qin and Ziming Wang and Rui Yan and Huajin Tang},
  doi          = {10.1109/TNNLS.2023.3240176},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10301-10311},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention-based deep spiking neural networks for temporal credit assignment problems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Value-based subgoal discovery and path planning for reaching
long-horizon goals. <em>TNNLS</em>, <em>35</em>(8), 10288–10300. (<a
href="https://doi.org/10.1109/TNNLS.2023.3240004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to reach long-horizon goals in spatial traversal tasks is a significant challenge for autonomous agents. Recent subgoal graph-based planning methods address this challenge by decomposing a goal into a sequence of shorter-horizon subgoals. These methods, however, use arbitrary heuristics for sampling or discovering subgoals, which may not conform to the cumulative reward distribution. Moreover, they are prone to learning erroneous connections (edges) between subgoals, especially those lying across obstacles. To address these issues, this article proposes a novel subgoal graph-based planning method called learning subgoal graph using value-based subgoal discovery and automatic pruning (LSGVP). The proposed method uses a subgoal discovery heuristic that is based on a cumulative reward (value) measure and yields sparse subgoals, including those lying on the higher cumulative reward paths. Moreover, LSGVP guides the agent to automatically prune the learned subgoal graph to remove the erroneous edges. The combination of these novel features helps the LSGVP agent to achieve higher cumulative positive rewards than other subgoal sampling or discovery heuristics, as well as higher goal-reaching success rates than other state-of-the-art subgoal graph-based planning methods.},
  archive      = {J_TNNLS},
  author       = {Shubham Pateria and Budhitama Subagdja and Ah-Hwee Tan and Chai Quek},
  doi          = {10.1109/TNNLS.2023.3240004},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10288-10300},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Value-based subgoal discovery and path planning for reaching long-horizon goals},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tiny object tracking: A large-scale dataset and a baseline.
<em>TNNLS</em>, <em>35</em>(8), 10273–10287. (<a
href="https://doi.org/10.1109/TNNLS.2023.3239529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tiny objects, frequently appearing in practical applications, have weak appearance and features, and receive increasing interests in many vision tasks, such as object detection and segmentation. To promote the research and development of tiny object tracking, we create a large-scale video dataset, which contains 434 sequences with a total of more than 217K frames. Each frame is carefully annotated with a high-quality bounding box. In data creation, we take 12 challenge attributes into account to cover a broad range of viewpoints and scene complexities, and annotate these attributes for facilitating the attribute-based performance analysis. To provide a strong baseline in tiny object tracking, we propose a novel multilevel knowledge distillation network (MKDNet), which pursues three-level knowledge distillations in a unified framework to effectively enhance the feature representation, discrimination, and localization abilities in tracking tiny objects. Extensive experiments are performed on the proposed dataset, and the results prove the superiority and effectiveness of MKDNet compared with state-of-the-art methods. The dataset, the algorithm code, and the evaluation code are available at https://github.com/mmic-lcl/Datasets-and-benchmark-code .},
  archive      = {J_TNNLS},
  author       = {Yabin Zhu and Chenglong Li and Yao Liu and Xiao Wang and Jin Tang and Bin Luo and Zhixiang Huang},
  doi          = {10.1109/TNNLS.2023.3239529},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10273-10287},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tiny object tracking: A large-scale dataset and a baseline},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective emotion recognition by learning discriminative
graph topologies in EEG brain networks. <em>TNNLS</em>, <em>35</em>(8),
10258–10272. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multichannel electroencephalogram (EEG) is an array signal that represents brain neural networks and can be applied to characterize information propagation patterns for different emotional states. To reveal these inherent spatial graph features and increase the stability of emotion recognition, we propose an effective emotion recognition model that performs multicategory emotion recognition with multiple emotion-related spatial network topology patterns (MESNPs) by learning discriminative graph topologies in EEG brain networks. To evaluate the performance of our proposed MESNP model, we conducted single-subject and multisubject four-class classification experiments on two public datasets, MAHNOB-HCI and DEAP. Compared with existing feature extraction methods, the MESNP model significantly enhances the multiclass emotional classification performance in the single-subject and multisubject conditions. To evaluate the online version of the proposed MESNP model, we designed an online emotion monitoring system. We recruited 14 participants to conduct the online emotion decoding experiments. The average online experimental accuracy of the 14 participants was 84.56%, indicating that our model can be applied in affective brain–computer interface (aBCI) systems. The offline and online experimental results demonstrate that the proposed MESNP model effectively captures discriminative graph topology patterns and significantly improves emotion classification performance. Moreover, the proposed MESNP model provides a new scheme for extracting features from strongly coupled array signals.},
  archive      = {J_TNNLS},
  author       = {Cunbo Li and Peiyang Li and Yangsong Zhang and Ning Li and Yajing Si and Fali Li and Zehong Cao and Huafu Chen and Badong Chen and Dezhong Yao and Peng Xu},
  doi          = {10.1109/TNNLS.2023.3238519},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10258-10272},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effective emotion recognition by learning discriminative graph topologies in EEG brain networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on offline reinforcement learning: Taxonomy,
review, and open problems. <em>TNNLS</em>, <em>35</em>(8), 10237–10257.
(<a href="https://doi.org/10.1109/TNNLS.2023.3250269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks’ properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
  archive      = {J_TNNLS},
  author       = {Rafael Figueiredo Prudencio and Marcos R. O. A. Maximo and Esther Luna Colombini},
  doi          = {10.1109/TNNLS.2023.3250269},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10237-10257},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on offline reinforcement learning: Taxonomy, review, and open problems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A critical review of inductive logic programming techniques
for explainable AI. <em>TNNLS</em>, <em>35</em>(8), 10220–10236. (<a
href="https://doi.org/10.1109/TNNLS.2023.3246980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent advances in modern machine learning algorithms, the opaqueness of their underlying mechanisms continues to be an obstacle in adoption. To instill confidence and trust in artificial intelligence (AI) systems, explainable AI (XAI) has emerged as a response to improve modern machine learning algorithms’ explainability. Inductive logic programming (ILP), a subfield of symbolic AI, plays a promising role in generating interpretable explanations because of its intuitive logic-driven framework. ILP effectively leverages abductive reasoning to generate explainable first-order clausal theories from examples and background knowledge. However, several challenges in developing methods inspired by ILP need to be addressed for their successful application in practice. For example, the existing ILP systems often have a vast solution space, and the induced solutions are very sensitive to noises and disturbances. This survey paper summarizes the recent advances in ILP and a discussion of statistical relational learning (SRL) and neural-symbolic algorithms, which offer synergistic views to ILP. Following a critical review of the recent advances, we delineate observed challenges and highlight potential avenues of further ILP-motivated research toward developing self-explanatory AI systems.},
  archive      = {J_TNNLS},
  author       = {Zheng Zhang and Levent Yilmaz and Bo Liu},
  doi          = {10.1109/TNNLS.2023.3246980},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10220-10236},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A critical review of inductive logic programming techniques for explainable AI},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous-time reinforcement learning control: A review of
theoretical results, insights on performance, and needs for new designs.
<em>TNNLS</em>, <em>35</em>(8), 10199–10219. (<a
href="https://doi.org/10.1109/TNNLS.2023.3245980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This exposition discusses continuous-time reinforcement learning (CT-RL) for the control of affine nonlinear systems. We review four seminal methods that are the centerpieces of the most recent results on CT-RL control. We survey the theoretical results of the four methods, highlighting their fundamental importance and successes by including discussions on problem formulation, key assumptions, algorithm procedures, and theoretical guarantees. Subsequently, we evaluate the performance of the control designs to provide analyses and insights on the feasibility of these design methods for applications from a control designer’s point of view. Through systematic evaluations, we point out when theory diverges from practical controller synthesis. We, furthermore, introduce a new quantitative analytical framework to diagnose the observed discrepancies. Based on the analyses and the insights gained through quantitative evaluations, we point out potential future research directions to unleash the potential of CT-RL control algorithms in addressing the identified challenges.},
  archive      = {J_TNNLS},
  author       = {Brent A. Wallace and Jennie Si},
  doi          = {10.1109/TNNLS.2023.3245980},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10199-10219},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Continuous-time reinforcement learning control: A review of theoretical results, insights on performance, and needs for new designs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open-ended online learning for autonomous visual perception.
<em>TNNLS</em>, <em>35</em>(8), 10178–10198. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual perception systems aim to autonomously collect consecutive visual data and perceive the relevant information online like human beings. In comparison with the classical static visual systems focusing on fixed tasks (e.g., face recognition for visual surveillance), the real-world visual systems (e.g., the robot visual system) often need to handle unpredicted tasks and dynamically changed environments, which need to imitate human-like intelligence with open-ended online learning ability. Therefore, we provide a comprehensive analysis of open-ended online learning problems for autonomous visual perception in this survey. Based on “what to online learn” among visual perception scenarios, we classify the open-ended online learning methods into five categories: instance incremental learning to handle data attributes changing, feature evolution learning for incremental and decremental features with the feature dimension changed dynamically, class incremental learning and task incremental learning aiming at online adding new coming classes/tasks, and parallel and distributed learning for large-scale data to reveal the computational and storage advantages. We discuss the characteristic of each method and introduce several representative works as well. Finally, we introduce some representative visual perception applications to show the enhanced performance when using various open-ended online learning models, followed by a discussion of several future directions.},
  archive      = {J_TNNLS},
  author       = {Haibin Yu and Yang Cong and Gan Sun and Dongdong Hou and Yuyang Liu and Jiahua Dong},
  doi          = {10.1109/TNNLS.2023.3242448},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {8},
  number       = {8},
  pages        = {10178-10198},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Open-ended online learning for autonomous visual perception},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Invertible residual blocks in deep learning networks.
<em>TNNLS</em>, <em>35</em>(7), 10167–10173. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residual blocks have been widely used in deep learning networks. However, information may be lost in residual blocks due to the relinquishment of information in rectifier linear units (ReLUs). To address this issue, invertible residual networks have been proposed recently but are generally under strict restrictions which limit their applications. In this brief, we investigate the conditions under which a residual block is invertible. A sufficient and necessary condition is presented for the invertibility of residual blocks with one layer of ReLU inside the block. In particular, for widely used residual blocks with convolutions, we show that such residual blocks are invertible under weak conditions if the convolution is implemented with certain zero-padding methods. Inverse algorithms are also proposed, and experiments are conducted to show the effectiveness of the proposed inverse algorithms and prove the correctness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Ruhua Wang and Senjian An and Wanquan Liu and Ling Li},
  doi          = {10.1109/TNNLS.2023.3238397},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10167-10173},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Invertible residual blocks in deep learning networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Covert task embedding: Turning a DNN into an insider agent
leaking out private information. <em>TNNLS</em>, <em>35</em>(7),
10159–10166. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the covert task embedding (CTE) attack, a new general threat affecting deep neural networks (DNNs). The new attack consists in hiding a malicious privacy-sensitive task within a seemingly innocuous network, in such a way that the result of the malicious task is delivered together with the legitimate output in a stealthy way. The result of the covert task is further protected by requiring that its extraction depends on a secret key shared by the embedder and the detector. We demonstrate the feasibility of the CTE attack in various settings, wherein a face-based age estimation DNN is trained in such a way as to also detect the gender (binary classification task) or ethnicity (multiclassification task) of the framed individual and stealthily pass along such information together with the estimated age. The results of the experiments we carried out show that, in all cases, the gender and ethnicity information can be reliably extracted without impairing the accuracy of the age estimation functionality. Despite the simplicity of the estting considered in the brief, our experiments show the feasibility of the CTE attack, thus calling for the development of suitable remedies against it.},
  archive      = {J_TNNLS},
  author       = {Li Li and Weiming Zhang and Mauro Barni},
  doi          = {10.1109/TNNLS.2022.3216010},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10159-10166},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Covert task embedding: Turning a DNN into an insider agent leaking out private information},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GLaLT: Global-local attention-augmented light transformer
for scene text recognition. <em>TNNLS</em>, <em>35</em>(7), 10145–10158.
(<a href="https://doi.org/10.1109/TNNLS.2023.3239696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the growing popularity of connectionist temporal classification (CTC) and attention mechanism in scene text recognition (STR). CTC-based methods consume less time with few computational burdens, while they are not as effective as attention-based methods. To retain computational efficiency and effectiveness, we propose the global–local attention-augmented light Transformer (GLaLT), which adopts a Transformer-based encoder–decoder structure to orchestrate CTC and attention mechanism. The encoder integrates the self-attention module with the convolution module to augment the attention, where the self-attention module pays more attention to capturing long-term global dependencies and the convolution module focuses on local context modeling. The decoder consists of two parallel modules: one is the Transformer-decoder-based attention module and the other is the CTC module. The first one is removed in the testing phase and can guide the second one to extract robust features in the training phase. Extensive experiments on standard benchmarks demonstrate that GLaLT achieves state-of-the-art performance for both regular and irregular STR. In terms of tradeoffs, the proposed GLaLT is at or near the frontiers for maximizing speed, accuracy, and computational efficiency at the same time.},
  archive      = {J_TNNLS},
  author       = {Hui Zhang and Guiyang Luo and Jian Kang and Shan Huang and Xiao Wang and Fei-Yue Wang},
  doi          = {10.1109/TNNLS.2023.3239696},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10145-10158},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GLaLT: Global-local attention-augmented light transformer for scene text recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning single spectral abundance for hyperspectral
subpixel target detection. <em>TNNLS</em>, <em>35</em>(7), 10134–10144.
(<a href="https://doi.org/10.1109/TNNLS.2023.3239061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitation of target size and spatial resolution, targets of interest in hyperspectral images (HSIs) often appear as subpixel targets, which makes hyperspectral target detection still faces an important bottleneck, that is, subpixel target detection. In this article, we propose a new detector by learning single spectral abundance for hyperspectral subpixel target detection (denoted as LSSA). Different from most existing hyperspectral detectors that are designed based on a match of the spectrum assisted by spatial information or focusing on the background, the proposed LSSA addresses the problem of detecting subpixel targets by learning a spectral abundance of the target of interest directly. In LSSA, the abundance of the prior target spectrum is updated and learned, while the prior target spectrum is fixed in a nonnegative matrix factorization (NMF) model. It turns out that such a way is quite effective to learn the abundance of subpixel targets and contributes to detecting subpixel targets in hyperspectral imagery (HSI). Numerous experiments are conducted on one simulated dataset and five real datasets, and the results indicate that the LSSA yields superior performance in hyperspectral subpixel target detection and outperforms its counterparts.},
  archive      = {J_TNNLS},
  author       = {Dehui Zhu and Bo Du and Liangpei Zhang},
  doi          = {10.1109/TNNLS.2023.3239061},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10134-10144},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning single spectral abundance for hyperspectral subpixel target detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-collaborated auto-encoder hashing for multiview binary
clustering. <em>TNNLS</em>, <em>35</em>(7), 10121–10133. (<a
href="https://doi.org/10.1109/TNNLS.2023.3239033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised hashing methods have attracted widespread attention with the explosive growth of large-scale data, which can greatly reduce storage and computation by learning compact binary codes. Existing unsupervised hashing methods attempt to exploit the valuable information from samples, which fails to take the local geometric structure of unlabeled samples into consideration. Moreover, hashing based on auto-encoders aims to minimize the reconstruction loss between the input data and binary codes, which ignores the potential consistency and complementarity of multiple sources data. To address the above issues, we propose a hashing algorithm based on auto-encoders for multiview binary clustering, which dynamically learns affinity graphs with low-rank constraints and adopts collaboratively learning between auto-encoders and affinity graphs to learn a unified binary code, called graph-collaborated auto-encoder (GCAE) hashing for multiview binary clustering. Specifically, we propose a multiview affinity graphs’ learning model with low-rank constraint, which can mine the underlying geometric information from multiview data. Then, we design an encoder–decoder paradigm to collaborate the multiple affinity graphs, which can learn a unified binary code effectively. Notably, we impose the decorrelation and code balance constraints on binary codes to reduce the quantization errors. Finally, we use an alternating iterative optimization scheme to obtain the multiview clustering results. Extensive experimental results on five public datasets are provided to reveal the effectiveness of the algorithm and its superior performance over other state-of-the-art alternatives.},
  archive      = {J_TNNLS},
  author       = {Huibing Wang and Mingze Yao and Guangqi Jiang and Zetian Mi and Xianping Fu},
  doi          = {10.1109/TNNLS.2023.3239033},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10121-10133},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-collaborated auto-encoder hashing for multiview binary clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive decentralized control for constrained strong
interconnected nonlinear systems and its application to inverted
pendulum. <em>TNNLS</em>, <em>35</em>(7), 10110–10120. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is dedicated to adaptive decentralized tracking control for a class of strong interconnected nonlinear systems with asymmetric constraints. Currently, there are few related studies on unknown strongly interconnected nonlinear systems with asymmetric time-varying constraints. To deal with the assumptions of the interconnection terms in the design process, which include upper functions and structural restrictions, the properties of Gaussian function in radial basis function (RBF) neural networks are applied to overcome this difficulty. By constructing the nonlinear state-dependent function (NSDF) and using a new coordinate transformation, the conservative step that the original state constraint converts into a new boundary of the tracking error is removed. Meanwhile, the virtual controller’s feasibility condition is removed. It is proven that all the signals are bounded, especially the original tracking error and the new tracking error, which are both bounded. In the end, simulation studies are carried out to verify the effectiveness and benefits of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Zhiguang Feng and Rui-Bing Li and Ligang Wu},
  doi          = {10.1109/TNNLS.2023.3238819},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10110-10120},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive decentralized control for constrained strong interconnected nonlinear systems and its application to inverted pendulum},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic metric search for few-shot learning.
<em>TNNLS</em>, <em>35</em>(7), 10098–10109. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to learn a model that can identify unseen classes using only a few training samples from each class. Most of the existing FSL methods adopt a manually predefined metric function to measure the relationship between a sample and a class, which usually require tremendous efforts and domain knowledge. In contrast, we propose a novel model called automatic metric search (Auto-MS), in which an Auto-MS space is designed for automatically searching task-specific metric functions. This allows us to further develop a new searching strategy to facilitate automated FSL. More specifically, by incorporating the episode-training mechanism into the bilevel search strategy, the proposed search strategy can effectively optimize the network weights and structural parameters of the few-shot model. Extensive experiments on the miniImageNet and tieredImageNet datasets demonstrate that the proposed Auto-MS achieves superior performance in FSL problems.},
  archive      = {J_TNNLS},
  author       = {Yuan Zhou and Jieke Hao and Shuwei Huo and Boyu Wang and Leijiao Ge and Sun-Yuan Kung},
  doi          = {10.1109/TNNLS.2023.3238729},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10098-10109},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic metric search for few-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient federated learning framework for machinery
fault diagnosis with improved model aggregation and local model
training. <em>TNNLS</em>, <em>35</em>(7), 10086–10097. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to device operating environment limitations and data privacy protection, it is frequently difficult to obtain sufficient high-quality labeled data from devices, resulting in an insufficient generalization ability of fault diagnosis model. Therefore, a high-performance federated learning framework is proposed in this work, which makes improvements in the procedure of model aggregation and local model training. In the model aggregation of central server, an optimization aggregation strategy in which forgetting Kalman filter (FKF) is combined with cubic exponential smoothing (CES) is proposed to improve the efficiency of federated learning. In the local model training of multiclient, a deep learning network combined with multiscale convolution, attention mechanism, and multistage residual connection is proposed, which is able to fully extract multiclient data features simultaneously. Meanwhile, experiments on two machinery fault datasets show that the proposed framework is capable of achieving high accuracy and strong generalization of fault diagnosis on the premise of protecting data privacy in actual industrial situations.},
  archive      = {J_TNNLS},
  author       = {Jiahao Du and Na Qin and Deqing Huang and Yiming Zhang and Xinming Jia},
  doi          = {10.1109/TNNLS.2023.3238724},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10086-10097},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient federated learning framework for machinery fault diagnosis with improved model aggregation and local model training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DM-fusion: Deep model-driven network for heterogeneous image
fusion. <em>TNNLS</em>, <em>35</em>(7), 10071–10085. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous image fusion (HIF) is an enhancement technique for highlighting the discriminative information and textural detail from heterogeneous source images. Although various deep neural network-based HIF methods have been proposed, the most widely used single data-driven manner of the convolutional neural network always fails to give a guaranteed theoretical architecture and optimal convergence for the HIF problem. In this article, a deep model-driven neural network is designed for this HIF problem, which adaptively integrates the merits of model-based techniques for interpretability and deep learning-based methods for generalizability. Unlike the general network architecture as a black box, the proposed objective function is tailored to several domain knowledge network modules to model the compact and explainable deep model-driven HIF network termed DM-fusion. The proposed deep model-driven neural network shows the feasibility and effectiveness of three parts, the specific HIF model, an iterative parameter learning scheme, and data-driven network architecture. Furthermore, the task-driven loss function strategy is proposed to achieve feature enhancement and preservation. Numerous experiments on four fusion tasks and downstream applications illustrate the advancement of DM-fusion compared with the state-of-the-art (SOTA) methods both in fusion quality and efficiency. The source code will be available soon.},
  archive      = {J_TNNLS},
  author       = {Guoxia Xu and Chunming He and Hao Wang and Hu Zhu and Weiping Ding},
  doi          = {10.1109/TNNLS.2023.3238511},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10071-10085},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DM-fusion: Deep model-driven network for heterogeneous image fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral super-resolution via model-guided cross-fusion
network. <em>TNNLS</em>, <em>35</em>(7), 10059–10070. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral super-resolution, which reconstructs a hyperspectral image (HSI) from a single red-green-blue (RGB) image, has acquired more and more attention. Recently, convolution neural networks (CNNs) have achieved promising performance. However, they often fail to simultaneously exploit the imaging model of the spectral super-resolution and complex spatial and spectral characteristics of the HSI. To tackle the above problems, we build a novel cross fusion (CF)-based model-guided network (called SSRNet) for spectral super-resolution. In specific, based on the imaging model, we unfold the spectral super-resolution into the HSI prior learning (HPL) module and imaging model guiding (IMG) module. Instead of just modeling one kind of image prior, the HPL module is composed of two subnetworks with different structures, which can effectively learn the complex spatial and spectral priors of the HSI, respectively. Furthermore, a CF strategy is used to establish the connection between the two subnetworks, which further improves the learning performance of the CNN. The IMG module results in solving a strong convex optimization problem, which adaptively optimizes and merges the two features learned by the HPL module by exploiting the imaging model. The two modules are alternately connected to achieve optimal HSI reconstruction performance. Experiments on both the simulated and real data demonstrate that the proposed method can achieve superior spectral reconstruction results with relatively small model size. The code will be available at https://github.com/renweidian .},
  archive      = {J_TNNLS},
  author       = {Renwei Dian and Tianci Shan and Wei He and Haibo Liu},
  doi          = {10.1109/TNNLS.2023.3238506},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10059-10070},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spectral super-resolution via model-guided cross-fusion network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-performance assessment of oscillatory neural networks
based on VO2 devices for future edge AI computing. <em>TNNLS</em>,
<em>35</em>(7), 10045–10058. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oscillatory neural network (ONN) is an emerging neuromorphic architecture composed of oscillators that implement neurons and are coupled by synapses. ONNs exhibit rich dynamics and associative properties, which can be used to solve problems in the analog domain according to the paradigm let physics compute. For example, compact oscillators made of VO2 material are good candidates for building low-power ONN architectures dedicated to AI applications at the edge, like pattern recognition. However, little is known about the ONN scalability and its performance when implemented in hardware. Before deploying ONN, it is necessary to assess its computation time, energy consumption, performance, and accuracy for a given application. Here, we consider a VO2-oscillator as an ONN building block and perform circuit-level simulations to evaluate the ONN performances at the architecture level. Notably, we investigate how the ONN computation time, energy, and memory capacity scale with the number of oscillators. It appears that the ONN energy grows linearly when scaling up the network, making it suitable for large-scale integration at the edge. Furthermore, we investigate the design knobs for minimizing the ONN energy. Assisted by technology computer-aided design (TCAD) simulations, we report on scaling down the dimensions of VO2 devices in crossbar (CB) geometry to decrease the oscillator voltage and energy. We benchmark ONN versus state-of-the-art architectures and observe that the ONN paradigm is a competitive energy-efficient solution for scaled VO2 devices oscillating above 100 MHz. Finally, we present how ONN can efficiently detect edges in images captured on low-power edge devices and compare the results with Sobel and Canny edge detectors.},
  archive      = {J_TNNLS},
  author       = {Corentin Delacour and Stefania Carapezzi and Madeleine Abernot and Aida Todri-Sanial},
  doi          = {10.1109/TNNLS.2023.3238473},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10045-10058},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Energy-performance assessment of oscillatory neural networks based on VO2 devices for future edge AI computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Community detection via multihop nonnegative matrix
factorization. <em>TNNLS</em>, <em>35</em>(7), 10033–10044. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection aims at finding all densely connected communities in a network, which serves as a fundamental graph tool for many applications, such as identification of protein functional modules, image segmentation, social circle discovery, to name a few. Recently, nonnegative matrix factorization (NMF)-based community detection methods have attracted significant attention. However, most existing methods neglect the multihop connectivity patterns in a network, which turn out to be practically useful for community detection. In this article, we first propose a novel community detection method, namely multihop NMF (MHNMF for brevity), which takes into account the multihop connectivity patterns in a network. Subsequently, we derive an efficient algorithm to optimize MHNMF and theoretically analyze its computational complexity and convergence. Experimental results on 12 real-world benchmark networks demonstrate that MHNMF outperforms 12 state-of-the-art community detection methods.},
  archive      = {J_TNNLS},
  author       = {Jiewen Guan and Bilian Chen and Xin Huang},
  doi          = {10.1109/TNNLS.2023.3238419},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10033-10044},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Community detection via multihop nonnegative matrix factorization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GREnet: Gradually REcurrent network with curriculum learning
for 2-d medical image segmentation. <em>TNNLS</em>, <em>35</em>(7),
10018–10032. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is a vital stage in medical image analysis. Numerous deep-learning methods are booming to improve the performance of 2-D medical image segmentation, owing to the fast growth of the convolutional neural network. Generally, the manually defined ground truth is utilized directly to supervise models in the training phase. However, direct supervision of the ground truth often results in ambiguity and distractors as complex challenges appear simultaneously. To alleviate this issue, we propose a gradually recurrent network with curriculum learning, which is supervised by gradual information of the ground truth. The whole model is composed of two independent networks. One is the segmentation network denoted as GREnet, which formulates 2-D medical image segmentation as a temporal task supervised by pixel-level gradual curricula in the training phase. The other is a curriculum-mining network. To a certain degree, the curriculum-mining network provides curricula with an increasing difficulty in the ground truth of the training set by progressively uncovering hard-to-segmentation pixels via a data-driven manner. Given that segmentation is a pixel-level dense-prediction challenge, to the best of our knowledge, this is the first work to function 2-D medical image segmentation as a temporal task with pixel-level curriculum learning. In GREnet, the naive UNet is adopted as the backbone, while ConvLSTM is used to establish the temporal link between gradual curricula. In the curriculum-mining network, UNet++ supplemented by transformer is designed to deliver curricula through the outputs of the modified UNet++ at different layers. Experimental results have demonstrated the effectiveness of GREnet on seven datasets, i.e., three lesion segmentation datasets in dermoscopic images, an optic disc and cup segmentation dataset and a blood vessel segmentation dataset in retinal images, a breast lesion segmentation dataset in ultrasound images, and a lung segmentation dataset in computed tomography (CT).},
  archive      = {J_TNNLS},
  author       = {Jinting Wang and Yujiao Tang and Yang Xiao and Joey Tianyi Zhou and Zhiwen Fang and Feng Yang},
  doi          = {10.1109/TNNLS.2023.3238381},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10018-10032},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GREnet: Gradually REcurrent network with curriculum learning for 2-D medical image segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DCCD: Reducing neural network redundancy via distillation.
<em>TNNLS</em>, <em>35</em>(7), 10006–10017. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural models have achieved remarkable performance on various supervised and unsupervised learning tasks, but it is a challenge to deploy these large-size networks on resource-limited devices. As a representative type of model compression and acceleration methods, knowledge distillation (KD) solves this problem by transferring knowledge from heavy teachers to lightweight students. However, most distillation methods focus on imitating the responses of teacher networks but ignore the information redundancy of student networks. In this article, we propose a novel distillation framework difference-based channel contrastive distillation (DCCD), which introduces channel contrastive knowledge and dynamic difference knowledge into student networks for redundancy reduction. At the feature level, we construct an efficient contrastive objective that broadens student networks’ feature expression space and preserves richer information in the feature extraction stage. At the final output level, more detailed knowledge is extracted from teacher networks by making a difference between multiview augmented responses of the same instance. We enhance student networks to be more sensitive to minor dynamic changes. With the improvement of two aspects of DCCD, the student network gains contrastive and difference knowledge and reduces its overfitting and redundancy. Finally, we achieve surprising results that the student approaches and even outperforms the teacher in test accuracy on CIFAR-100. We reduce the top-1 error to 28.16% on ImageNet classification and 24.15% for cross-model transfer with ResNet-18. Empirical experiments and ablation studies on popular datasets show that our proposed method can achieve state-of-the-art accuracy compared with other distillation methods.},
  archive      = {J_TNNLS},
  author       = {Yuang Liu and Jun Chen and Yong Liu},
  doi          = {10.1109/TNNLS.2023.3238337},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {10006-10017},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DCCD: Reducing neural network redundancy via distillation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural-network-based predefined-time adaptive consensus in
nonlinear multi-agent systems with switching topologies. <em>TNNLS</em>,
<em>35</em>(7), 9995–10005. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A predefined-time adaptive consensus control strategy is developed for a class of multi-agent systems containing unknown nonlinearity. The unknown dynamics and switching topologies are simultaneously considered to adapt to actual scenarios. The time required for tracking error convergence can be easily adjusted using the proposed time-varying decay functions. An efficient method is proposed to determine the expected convergence time. Subsequently, the predefined time is adjustable by regulating the parameters of the time-varying functions (TVFs). The neural network (NN) approximation technique is used to address the issue of unknown nonlinear dynamics through predefined-time consensus control. The Lyapunov stability theory testifies that the predefined-time tracking error signals are bounded and convergent. The feasibility and effectiveness of the proposed predefined-time consensus control scheme are demonstrated through the simulation results.},
  archive      = {J_TNNLS},
  author       = {Yanzheng Zhu and Zuo Wang and Hongjing Liang and Choon Ki Ahn},
  doi          = {10.1109/TNNLS.2023.3238336},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9995-10005},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based predefined-time adaptive consensus in nonlinear multi-agent systems with switching topologies},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quasi-synchronization of discrete-time-delayed
heterogeneous-coupled neural networks via hybrid impulsive control.
<em>TNNLS</em>, <em>35</em>(7), 9985–9994. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores the quasi-synchronization of discrete-time-delayed heterogeneous-coupled neural networks (CNNs) via hybrid impulsive control. By introducing an exponential decay function, two non-negative regions are introduced that are named time-triggering and event-triggering regions, respectively. The hybrid impulsive control is modeled by the dynamical location of Lyapunov functional in two regions. When the Lyapunov functional locates in the time-triggering region, the isolated neuron node releases impulses to corresponding nodes in a periodical manner. Whereas, when the trajectory locates in the event-triggering region, the event-triggered mechanism (ETM) is activated, and there are no impulses. Under the proposed hybrid impulsive control algorithm, sufficient conditions are derived for quasi-synchronization with a definite error convergence level. Compared with pure time-triggered impulsive control (TTIC), the proposed hybrid impulsive control method can effectively reduce the times of impulses and save communication resources on the premise of ensuring performance. Finally, an illustrative example is given to verify the validity of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Sanbo Ding and Mengxin Sun and Xiangpeng Xie},
  doi          = {10.1109/TNNLS.2023.3238331},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9985-9994},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quasi-synchronization of discrete-time-delayed heterogeneous-coupled neural networks via hybrid impulsive control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent power system stability assessment and dominant
instability mode identification using integrated active deep learning.
<em>TNNLS</em>, <em>35</em>(7), 9970–9984. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation analysis is critical for identifying possible hazards and ensuring secure operation of power systems. In practice, large-disturbance rotor angle stability and voltage stability are two frequently intertwined stability problems. Accurately identifying the dominant instability mode (DIM) between them is important for directing power system emergency control action formulation. However, DIM identification has always relied on human expertise. This article proposes an intelligent DIM identification framework that can discriminate among stable status, rotor angle instability, and voltage instability based on active deep learning (ADL). To reduce human expert efforts required to label the DIM dataset when building DL models, a two-stage batch-mode integrated ADL query strategy (preselection and clustering) is designed for the framework. It samples only the most helpful samples to label in each iteration and considers both information contents and diversity in them to improve query efficiency, significantly reducing the required number of labeled samples. Case studies conducted on a benchmark power system (China Electric Power Research Institute (CEPRI) 36-bus system) and a practical large-area power system (Northeast China Power System) reveal that the proposed approach outperforms conventional methods in terms of accuracy, label efficiency, scalability, and adaptability to operational variability.},
  archive      = {J_TNNLS},
  author       = {Zhongtuo Shi and Wei Yao and Yong Tang and Xiaomeng Ai and Jinyu Wen and Shijie Cheng},
  doi          = {10.1109/TNNLS.2023.3238168},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9970-9984},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intelligent power system stability assessment and dominant instability mode identification using integrated active deep learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and effective: A deep matrix factorization framework
for classification. <em>TNNLS</em>, <em>35</em>(7), 9958–9969. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For complex data, high dimension and high noise are challenging problems, and deep matrix factorization shows great potential in data dimensionality reduction. In this article, a novel robust and effective deep matrix factorization framework is proposed. This method constructs a dual-angle feature for single-modal gene data to improve the effectiveness and robustness, which can solve the problem of high-dimensional tumor classification. The proposed framework consists of three parts, deep matrix factorization, double-angle decomposition, and feature purification. First, a robust deep matrix factorization (RDMF) model is proposed in the feature learning, to enhance the classification stability and obtain better feature when faced with noisy data. Second, a double-angle feature (RDMF-DA) is designed by cascading the RDMF features with sparse features, which contains the more comprehensive information in gene data. Third, to avoid the influence of redundant genes on the representation ability, a gene selection method is proposed to purify the features by RDMF-DA, based on the principle of sparse representation (SR) and gene coexpression. Finally, the proposed algorithm is applied to the gene expression profiling datasets, and the performance of the algorithm is fully verified.},
  archive      = {J_TNNLS},
  author       = {Chenxi Tian and Licheng Jiao and Fang Liu and Xu Liu and Shuyuan Yang},
  doi          = {10.1109/TNNLS.2023.3238104},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9958-9969},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust and effective: A deep matrix factorization framework for classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast sparse discriminative k-means for unsupervised feature
selection. <em>TNNLS</em>, <em>35</em>(7), 9943–9957. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedded feature selection approach guides subsequent projection matrix (selection matrix) learning through the acquisition of pseudolabel matrix to conduct feature selection tasks. Yet the continuous pseudolabel matrix learned from relaxed problem based on spectral analysis deviates from reality to some extent. To cope with this issue, we design an efficient feature selection framework inspired by classical least-squares regression (LSR) and discriminative K-means (DisK-means), which is called the fast sparse discriminative K-means (FSDK) for the feature selection method. First, the weighted pseudolabel matrix with discrete trait is introduced to avoid trivial solution from unsupervised LSR. On this condition, any constraint imposed into pseudolabel matrix and selection matrix is dispensable, which is significantly beneficial to simplify the combinational optimization problem. Second, the $\ell _{2,p}$ -norm regularizer is introduced to satisfy the row sparsity of selection matrix with flexible $p$ . Consequently, the proposed FSDK model can be treated as a novel feature selection framework integrated from the DisK-means algorithm and $\ell _{2,p}$ -norm regularizer to optimize the sparse regression problem. Moreover, our model is linearly correlated with the number of samples, which is speedy to handle the large-scale data. Comprehensive tests on various data terminally illuminate the effectiveness and efficiency of FSDK.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Zhenyu Ma and Jingyu Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3238103},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9943-9957},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast sparse discriminative K-means for unsupervised feature selection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised domain adaptation with class-aware memory
alignment. <em>TNNLS</em>, <em>35</em>(7), 9930–9942. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) is to make predictions on unlabeled target domain by learning the knowledge from a label-rich source domain. In practice, existing UDA approaches mainly focus on minimizing the discrepancy between different domains by mini-batch training, where only a few instances are accessible at each iteration. Due to the randomness of sampling, such a batch-level alignment pattern is unstable and may lead to misalignment. To alleviate this risk, we propose class-aware memory alignment (CMA) that models the distributions of the two domains by two auxiliary class-aware memories and performs domain adaptation on these predefined memories. CMA is designed with two distinct characteristics: class-aware memories that create two symmetrical class-aware distributions for different domains and two reliability-based filtering strategies that enhance the reliability of the constructed memory. We further design a unified memory-based loss to jointly improve the transferability and discriminability of features in the memories. State-of-the-art (SOTA) comparisons and careful ablation studies show the effectiveness of our proposed CMA.},
  archive      = {J_TNNLS},
  author       = {Hui Wang and Liangli Zheng and Hanbin Zhao and Shijian Li and Xi Li},
  doi          = {10.1109/TNNLS.2023.3238063},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9930-9942},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised domain adaptation with class-aware memory alignment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiview clustering with propagating information
bottleneck. <em>TNNLS</em>, <em>35</em>(7), 9915–9929. (<a
href="https://doi.org/10.1109/TNNLS.2023.3238041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical applications, massive data are observed from multiple sources, each of which contains multiple cohesive views, called hierarchical multiview (HMV) data, such as image–text objects with different types of visual and textual features. Naturally, the inclusion of source and view relationships offers a comprehensive view of the input HMV data and achieves an informative and correct clustering result. However, most existing multiview clustering (MVC) methods can only process single-source data with multiple views or multisource data with single type of feature, failing to consider all the views across multiple sources. Observing the rich closely related multivariate (i.e., source and view) information and the potential dynamic information flow interacting among them, in this article, a general hierarchical information propagation model is first built to address the above challenging problem. It describes the process from optimal feature subspace learning (OFSL) of each source to final clustering structure learning (CSL). Then, a novel self-guided method named propagating information bottleneck (PIB) is proposed to realize the model. It works in a circulating propagation fashion, so that the resulting clustering structure obtained from the last iteration can “self-guide” the OFSL of each source, and the learned subspaces are in turn used to conduct the subsequent CSL. We theoretically analyze the relationship between the cluster structures learned in the CSL phase and the preservation of relevant information propagated from the OFSL phase. Finally, a two-step alternating optimization method is carefully designed for optimization. Experimental results on various datasets show the superiority of the proposed PIB method over several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Shizhe Hu and Zenglin Shi and Xiaoqiang Yan and Zhengzheng Lou and Yangdong Ye},
  doi          = {10.1109/TNNLS.2023.3238041},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9915-9929},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview clustering with propagating information bottleneck},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain cognition-inspired dual-pathway CNN architecture for
image classification. <em>TNNLS</em>, <em>35</em>(7), 9900–9914. (<a
href="https://doi.org/10.1109/TNNLS.2023.3237962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the global–local information processing mechanism in the human visual system, we propose a novel convolutional neural network (CNN) architecture named cognition-inspired network (CogNet) that consists of a global pathway, a local pathway, and a top-down modulator. We first use a common CNN block to form the local pathway that aims to extract fine local features of the input image. Then, we use a transformer encoder to form the global pathway to capture global structural and contextual information among local parts in the input image. Finally, we construct the learnable top-down modulator where fine local features of the local pathway are modulated by global representations of the global pathway. For ease of use, we encapsulate the dual-pathway computation and modulation process into a building block, called the global–local block (GL block), and a CogNet of any depth can be constructed by stacking a necessary number of GL blocks one after another. Extensive experimental evaluations have revealed that the proposed CogNets have achieved the state-of-the-art performance accuracies on all the six benchmark datasets and are very effective for overcoming the “texture bias” and the “semantic confusion” problems faced by many CNN models.},
  archive      = {J_TNNLS},
  author       = {Songlin Dong and Yihong Gong and Jingang Shi and Miao Shang and Xiaoyu Tao and Xing Wei and Xiaopeng Hong and Tiangang Zhou},
  doi          = {10.1109/TNNLS.2023.3237962},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9900-9914},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Brain cognition-inspired dual-pathway CNN architecture for image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perceptron theory can predict the accuracy of neural
networks. <em>TNNLS</em>, <em>35</em>(7), 9885–9899. (<a
href="https://doi.org/10.1109/TNNLS.2023.3237381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilayer neural networks set the current state of the art for many technical classification problems. But, these networks are still, essentially, black boxes in terms of analyzing them and predicting their performance. Here, we develop a statistical theory for the one-layer perceptron and show that it can predict performances of a surprisingly large variety of neural networks with different architectures. A general theory of classification with perceptrons is developed by generalizing an existing theory for analyzing reservoir computing models and connectionist models for symbolic reasoning known as vector symbolic architectures. Our statistical theory offers three formulas leveraging the signal statistics with increasing detail. The formulas are analytically intractable, but can be evaluated numerically. The description level that captures maximum details requires stochastic sampling methods. Depending on the network model, the simpler formulas already yield high prediction accuracy. The quality of the theory predictions is assessed in three experimental settings, a memorization task for echo state networks (ESNs) from reservoir computing literature, a collection of classification datasets for shallow randomly connected networks, and the ImageNet dataset for deep convolutional neural networks. We find that the second description level of the perceptron theory can predict the performance of types of ESNs, which could not be described previously. Furthermore, the theory can predict deep multilayer neural networks by being applied to their output layer. While other methods for prediction of neural networks performance commonly require to train an estimator model, the proposed theory requires only the first two moments of the distribution of the postsynaptic sums in the output neurons. Moreover, the perceptron theory compares favorably to other methods that do not rely on training an estimator model.},
  archive      = {J_TNNLS},
  author       = {Denis Kleyko and Antonello Rosato and Edward Paxon Frady and Massimo Panella and Friedrich T. Sommer},
  doi          = {10.1109/TNNLS.2023.3237381},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9885-9899},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Perceptron theory can predict the accuracy of neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFILS: Tri-selection via convex and nonconvex
regularizations. <em>TNNLS</em>, <em>35</em>(7), 9871–9884. (<a
href="https://doi.org/10.1109/TNNLS.2023.3237170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world applications, data are represented by multiple instances and simultaneously associated with multiple labels. These data are always redundant and generally contaminated by different noise levels. As a result, several machine learning models fail to achieve good classification and find an optimal mapping. Feature selection, instance selection, and label selection are three effective dimensionality reduction techniques. Nevertheless, the literature was limited to feature and/or instance selection but has, to some extent, neglected label selection, which also plays an essential role in the preprocessing step, as label noises can adversely affect the performance of the underlying learning algorithms. In this article, we propose a novel framework termed multilabel Feature Instance Label Selection (mFILS) that simultaneously performs feature, instance, and label selections in both convex and nonconvex scenarios. To the best of our knowledge, this article offers, for the first time ever, a study using the triple and simultaneous selection of features, instances, and labels based on convex and nonconvex penalties in a multilabel scenario. Experimental results are built on some known benchmark datasets to validate the effectiveness of the proposed mFILS.},
  archive      = {J_TNNLS},
  author       = {Dou El Kefel Mansouri and Khalid Benabdeslem and Seif.Eddine Benkabou and Souleyman Chaib and Mohamed Chohri},
  doi          = {10.1109/TNNLS.2023.3237170},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9871-9884},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MFILS: Tri-selection via convex and nonconvex regularizations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A human-machine agent based on active reinforcement
learning for target classification in wargame. <em>TNNLS</em>,
<em>35</em>(7), 9858–9870. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the requirements of high accuracy and low cost of target classification in modern warfare, and lay the foundation for target threat assessment, the article proposes a human-machine agent for target classification based on active reinforcement learning (TCARL_H-M), inferring when to introduce human experience guidance for model and how to autonomously classify detected targets into predefined categories with equipment information. To simulate different levels of human guidance, we set up two modes for the model: the easier-to-obtain but low-value-type cues simulated by Mode 1 and the labor-intensive but high-value class labels simulated by Mode 2. In addition, to analyze the respective roles of human experience guidance and machine data learning in target classification tasks, the article proposes a machine-based learner (TCARL_M) with zero human participation and a human-based interventionist with full human guidance (TCARL_H). Finally, based on the simulation data from a wargame, we carried out performance evaluation and application analysis for the proposed models in terms of target prediction and target classification, respectively, and the obtained results demonstrate that TCARL_H-M can not only greatly save labor costs, but achieve more competitive classification accuracy compared with our TCARL_M, TCARL_H, a purely supervised model—long short-term memory network (LSTM), a classic active learning algorithm—Query By Committee (QBC), and the common active learning model—uncertainty sampling (Uncertainty).},
  archive      = {J_TNNLS},
  author       = {Li Chen and Yulong Zhang and Yanghe Feng and Longfei Zhang and Zhong Liu},
  doi          = {10.1109/TNNLS.2023.3236944},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9858-9870},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A human-machine agent based on active reinforcement learning for target classification in wargame},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). LSOTB-TIR: A large-scale high-diversity thermal infrared
single object tracking benchmark. <em>TNNLS</em>, <em>35</em>(7),
9844–9857. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike visual object tracking, thermal infrared (TIR) object tracking methods can track the target of interest in poor visibility such as rain, snow, and fog, or even in total darkness. This feature brings a wide range of application prospects for TIR object-tracking methods. However, this field lacks a unified and large-scale training and evaluation benchmark, which has severely hindered its development. To this end, we present a large-scale and high-diversity unified TIR single object tracking benchmark, called LSOTB-TIR, which consists of a tracking evaluation dataset and a general training dataset with a total of 1416 TIR sequences and more than 643 K frames. We annotate the bounding box of objects in every frame of all sequences and generate over 770 K bounding boxes in total. To the best of our knowledge, LSOTB-TIR is the largest and most diverse TIR object tracking benchmark to date. We spilt the evaluation dataset into a short-term tracking subset and a long-term tracking subset to evaluate trackers using different paradigms. What’s more, to evaluate a tracker on different attributes, we also define four scenario attributes and 12 challenge attributes in the short-term tracking evaluation subset. By releasing LSOTB-TIR, we encourage the community to develop deep learning-based TIR trackers and evaluate them fairly and comprehensively. We evaluate and analyze 40 trackers on LSOTB-TIR to provide a series of baselines and give some insights and future research directions in TIR object tracking. Furthermore, we retrain several representative deep trackers on LSOTB-TIR, and their results demonstrate that the proposed training dataset significantly improves the performance of deep TIR trackers. Codes and dataset are available at https://github.com/QiaoLiuHit/LSOTB-TIR .},
  archive      = {J_TNNLS},
  author       = {Qiao Liu and Xin Li and Di Yuan and Chao Yang and Xiaojun Chang and Zhenyu He},
  doi          = {10.1109/TNNLS.2023.3236895},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9844-9857},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LSOTB-TIR: A large-scale high-diversity thermal infrared single object tracking benchmark},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Webly supervised knowledge-embedded model for visual
reasoning. <em>TNNLS</em>, <em>35</em>(7), 9829–9843. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual reasoning between visual images and natural language remains a long-standing challenge in computer vision. Conventional deep supervision methods target at finding answers to the questions relying on the datasets containing only a limited amount of images with textual ground-truth descriptions. Facing learning with limited labels, it is natural to expect to constitute a larger scale dataset consisting of several million visual data annotated with texts, but this approach is extremely time-intensive and laborious. Knowledge-based works usually treat knowledge graphs (KGs) as static flattened tables for searching the answer, but fail to take advantage of the dynamic update of KGs. To overcome these deficiencies, we propose a Webly supervised knowledge-embedded model for the task of visual reasoning. On the one hand, vitalized by the overwhelming successful Webly supervised learning, we make much use readily available images from the Web with their weakly annotated texts for an effective representation. On the other hand, we design a knowledge-embedded model, including the dynamically updated interaction mechanism between semantic representation models and KGs. Experimental results on two benchmark datasets demonstrate that our proposed model significantly achieves the most outstanding performance compared with other state-of-the-art approaches for the task of visual reasoning.},
  archive      = {J_TNNLS},
  author       = {Wenbo Zheng and Lan Yan and Wenwen Zhang and Fei-Yue Wang},
  doi          = {10.1109/TNNLS.2023.3236776},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9829-9843},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Webly supervised knowledge-embedded model for visual reasoning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel continuous- and discrete-time neural networks for
solving quadratic minimax problems with linear equality constraints.
<em>TNNLS</em>, <em>35</em>(7), 9814–9828. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents two novel continuous- and discrete-time neural networks (NNs) for solving quadratic minimax problems with linear equality constraints. These two NNs are established based on the conditions of the saddle point of the underlying function. For the two NNs, a proper Lyapunov function is constructed so that they are stable in the sense of Lyapunov, and will converge to some saddle point(s) for any starting point under some mild conditions. Compared with the existing NNs for solving quadratic minimax problems, the proposed NNs require weaker stability conditions. The validity and transient behavior of the proposed models are illustrated by some simulation results.},
  archive      = {J_TNNLS},
  author       = {Xingbao Gao and Li-Zhi Liao},
  doi          = {10.1109/TNNLS.2023.3236695},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9814-9828},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Novel continuous- and discrete-time neural networks for solving quadratic minimax problems with linear equality constraints},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast clustering via maximizing adaptively within-class
similarity. <em>TNNLS</em>, <em>35</em>(7), 9800–9813. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering aims to make data points in the same group have higher similarity or make data points in different groups have lower similarity. Therefore, we propose three novel fast clustering models motivated by maximizing within-class similarity, which can obtain more instinct clustering structure of data. Different from traditional clustering methods, we divide all $n$ samples into $m$ classes by the pseudo label propagation algorithm first, and then $m$ classes are merged to $c$ classes ( $m&amp;gt;c$ ) by the proposed three co-clustering models, where $c$ is the real number of categories. On the one hand, dividing all samples into more subclasses first can preserve more local information. On the other hand, proposed three co-clustering models are motivated by the thought of maximizing the sum of within-class similarity, which can utilize the dual information between rows and columns. Besides, the proposed pseudo label propagation algorithm can be a new method to construct anchor graphs with linear time complexity. A series of experiments are conducted on both synthetic and real-world datasets and the experimental results show the superior performance of three models. It is worth noting that for the proposed models, FMAWS2 is the generalization of FMAWS1 and FMAWS3 is the generalization of other two.},
  archive      = {J_TNNLS},
  author       = {Jingjing Xue and Feiping Nie and Rong Wang and Liang Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2023.3236686},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9800-9813},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast clustering via maximizing adaptively within-class similarity},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperspectral anomaly detection based on adaptive low-rank
transformed tensor. <em>TNNLS</em>, <em>35</em>(7), 9787–9799. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral anomaly detection, which is aimed at distinguishing anomaly pixels from the surroundings in spatial features and spectral characteristics, has attracted considerable attention due to its various applications. In this article, we propose a novel hyperspectral anomaly detection algorithm based on adaptive low-rank transform, in which the input hyperspectral image (HSI) is divided into a background tensor, an anomaly tensor, and a noise tensor. To take full advantage of the spatial–spectral information, the background tensor is represented as the product of a transformed tensor and a low-rank matrix. The low-rank constraint is imposed on frontal slices of the transformed tensor to depict the spatial–spectral correlation of the HSI background. Besides, we initialize a matrix with predefined size and then minimize its $l_{2.1}$ -norm to adaptively derive an appropriate low-rank matrix. The anomaly tensor is constrained with the $l_{2.1.1}$ -norm to depict the group sparsity of anomalous pixels. We integrate all regularization terms and a fidelity term into a non-convex problem and develop a proximal alternating minimization (PAM) algorithm to solve it. Interestingly, the sequence generated by the PAM algorithm is proven to converge to a critical point. Experimental results conducted on four widely used datasets demonstrate the superiority of the proposed anomaly detector over several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Siyu Sun and Jun Liu and Ziwei Zhang and Wei Li},
  doi          = {10.1109/TNNLS.2023.3236641},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9787-9799},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral anomaly detection based on adaptive low-rank transformed tensor},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LGGNet: Learning from local-global-graph representations for
brain–computer interface. <em>TNNLS</em>, <em>35</em>(7), 9773–9786. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuropsychological studies suggest that co-operative activities among different brain functional areas drive high-level cognitive processes. To learn the brain activities within and among different functional areas of the brain, we propose local-global-graph network (LGGNet), a novel neurologically inspired graph neural network (GNN), to learn local-global-graph (LGG) representations of electroencephalography (EEG) for brain–computer interface (BCI). The input layer of LGGNet comprises a series of temporal convolutions with multiscale 1-D convolutional kernels and kernel-level attentive fusion. It captures temporal dynamics of EEG which then serves as input to the proposed local- and global-graph-filtering layers. Using a defined neurophysiologically meaningful set of local and global graphs, LGGNet models the complex relations within and among functional areas of the brain. Under the robust nested cross-validation settings, the proposed method is evaluated on three publicly available datasets for four types of cognitive classification tasks, namely the attention, fatigue, emotion, and preference classification tasks. LGGNet is compared with state-of-the-art (SOTA) methods, such as DeepConvNet, EEGNet, R2G-STNN, TSception, regularized graph neural network (RGNN), attention-based multiscale convolutional neural network-dynamical graph convolutional network (AMCNN-DGCN), hierarchical recurrent neural network (HRNN), and GraphNet. The results show that LGGNet outperforms these methods, and the improvements are statistically significant ( $p&amp;lt; 0.05$ ) in most cases. The results show that bringing neuroscience prior knowledge into neural network design yields an improvement of classification performance. The source code can be found at https://github.com/yi-ding-cs/LGG .},
  archive      = {J_TNNLS},
  author       = {Yi Ding and Neethu Robinson and Chengxuan Tong and Qiuhao Zeng and Cuntai Guan},
  doi          = {10.1109/TNNLS.2023.3236635},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9773-9786},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LGGNet: Learning from local-global-graph representations for Brain–Computer interface},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMARL: An attention-based multiagent reinforcement learning
approach to the min-max multiple traveling salesmen problem.
<em>TNNLS</em>, <em>35</em>(7), 9758–9772. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the multiple traveling salesmen problem (MTSP or multiple TSP) has received increasing research interest and one of its main applications is coordinated multirobot mission planning, such as cooperative search and rescue tasks. However, it is still challenging to solve MTSP with improved inference efficiency as well as solution quality in varying situations, e.g., different city positions, different numbers of cities, or agents. In this article, we propose an attention-based multiagent reinforcement learning (AMARL) approach, which is based on the gated transformer feature representations for min-max multiple TSPs. The state feature extraction network in our proposed approach adopts the gated transformer architecture with reordering layer normalization (LN) and a new gate mechanism. It aggregates fixed-dimensional attention-based state features irrespective of the number of agents and cities. The action space of our proposed approach is designed to decouple the interaction of agents’ simultaneous decision-making. At each time step, only one agent is assigned to a non-zero action so that the action selection strategy can be transferred across tasks with different numbers of agents and cities. Extensive experiments on min-max multiple TSPs were conducted to illustrate the effectiveness and advantages of the proposed approach. Compared with six representative algorithms, our proposed approach achieves state-of-the-art performance in solution quality and inference efficiency. In particular, the proposed approach is suitable for tasks with different numbers of agents or cities without extra learning, and experimental results demonstrate that the proposed approach realizes powerful transfer capability across tasks.},
  archive      = {J_TNNLS},
  author       = {Hao Gao and Xing Zhou and Xin Xu and Yixing Lan and Yongqian Xiao},
  doi          = {10.1109/TNNLS.2023.3236629},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9758-9772},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AMARL: An attention-based multiagent reinforcement learning approach to the min-max multiple traveling salesmen problem},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A recurrent neural network approach for constrained
distributed fuzzy convex optimization. <em>TNNLS</em>, <em>35</em>(7),
9743–9757. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a class of constrained distributed fuzzy convex optimization problems, where the objective function is the sum of a set of local fuzzy convex objective functions, and the constraints include partial order relation and closed convex set constraints. In undirected connected node communication network, each node only knows its own objective function and constraints, and the local objective function and partial order relation functions may be nonsmooth. To solve this problem, a recurrent neural network approach based on differential inclusion framework is proposed. The network model is constructed with the help of the idea of penalty function, and the estimation of penalty parameters in advance is eliminated. Through theoretical analysis, it is proven that the state solution of the network enters the feasible region in finite time and does not escape again, and finally reaches consensus at an optimal solution of the distributed fuzzy optimization problem. Furthermore, the stability and global convergence of the network do not depend on the selection of the initial state. A numerical example and an intelligent ship output power optimization problem are given to illustrate the feasibility and effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Jingxin Liu and Xiaofeng Liao and Jin-Song Dong},
  doi          = {10.1109/TNNLS.2023.3236607},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9743-9757},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A recurrent neural network approach for constrained distributed fuzzy convex optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view knowledge ensemble with frequency consistency for
cross-domain face translation. <em>TNNLS</em>, <em>35</em>(7),
9728–9742. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain face translation aims to transfer face images from one domain to another. It can be widely used in practical applications, such as photos/sketches in law enforcement, photos/drawings in digital entertainment, and near-infrared (NIR)/visible (VIS) images in security access control. Restricted by limited cross-domain face image pairs, the existing methods usually yield structural deformation or identity ambiguity, which leads to poor perceptual appearance. To address this challenge, we propose a multi-view knowledge (structural knowledge and identity knowledge) ensemble framework with frequency consistency (MvKE-FC) for cross-domain face translation. Due to the structural consistency of facial components, the multi-view knowledge learned from large-scale data can be appropriately transferred to limited cross-domain image pairs and significantly improve the generative performance. To better fuse multi-view knowledge, we further design an attention-based knowledge aggregation module that integrates useful information, and we also develop a frequency-consistent (FC) loss that constrains the generated images in the frequency domain. The designed FC loss consists of a multidirection Prewitt (mPrewitt) loss for high-frequency consistency and a Gaussian blur loss for low-frequency consistency. Furthermore, our FC loss can be flexibly applied to other generative models to enhance their overall performance. Extensive experiments on multiple cross-domain face datasets demonstrate the superiority of our method over state-of-the-art methods both qualitatively and quantitatively.},
  archive      = {J_TNNLS},
  author       = {Bing Cao and Qinghe Wang and Pengfei Zhu and Qinghua Hu and Dongwei Ren and Wangmeng Zuo and Xinbo Gao},
  doi          = {10.1109/TNNLS.2023.3236486},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9728-9742},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-view knowledge ensemble with frequency consistency for cross-domain face translation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning with incremental instances and features.
<em>TNNLS</em>, <em>35</em>(7), 9713–9727. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world applications, data may dynamically expand over time in both volume and feature dimensions. Besides, they are often collected in batches (also called blocks). We refer this kind of data whose volume and features increase in blocks as blocky trapezoidal data streams. Current works either assume that the feature space of data streams is fixed or stipulate that the algorithm receives only one instance at a time, and none of them can effectively handle the blocky trapezoidal data streams. In this article, we propose a novel algorithm to learn a classification model from blocky trapezoidal data streams, called learning with incremental instances and features (IIF). We attempt to design highly dynamic model update strategies that can learn from increasing training data with an expanding feature space. Specifically, we first divide the data streams obtained on each round and construct the corresponding classifiers for these different divided parts. Then, to realize the effective interaction of information between each classifier, we utilize a single global loss function to capture their relationship. Finally, we use the idea of ensemble to achieve the final classification model. Furthermore, to make this method more applicable, we directly transform it into the kernel method. Both theoretical analysis and empirical analysis validate the effectiveness of our algorithm.},
  archive      = {J_TNNLS},
  author       = {Shilin Gu and Yuhua Qian and Chenping Hou},
  doi          = {10.1109/TNNLS.2023.3236479},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9713-9727},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning with incremental instances and features},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust tensor completion via capped frobenius norm.
<em>TNNLS</em>, <em>35</em>(7), 9700–9712. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor completion (TC) refers to restoring the missing entries in a given tensor by making use of the low-rank structure. Most existing algorithms have excellent performance in Gaussian noise or impulsive noise scenarios. Generally speaking, the Frobenius-norm-based methods achieve excellent performance in additive Gaussian noise, while their recovery severely degrades in impulsive noise. Although the algorithms using the $\ell _{p}$ -norm ( $0&amp;lt; p&amp;lt; 2$ ) or its variants can attain high restoration accuracy in the presence of gross errors, they are inferior to the Frobenius-norm-based methods when the noise is Gaussian-distributed. Therefore, an approach that is able to perform well in both Gaussian noise and impulsive noise is desired. In this work, we use a capped Frobenius norm to restrain outliers, which corresponds to a form of the truncated least-squares loss function. The upper bound of our capped Frobenius norm is automatically updated using normalized median absolute deviation during iterations. Therefore, it achieves better performance than the $\ell _{p}$ -norm with outlier-contaminated observations and attains comparable accuracy to the Frobenius norm without tuning parameter in Gaussian noise. We then adopt the half-quadratic theory to convert the nonconvex problem into a tractable multivariable problem, that is, convex optimization with respect to (w.r.t.) each individual variable. To address the resultant task, we exploit the proximal block coordinate descent (PBCD) method and then establish the convergence of the suggested algorithm. Specifically, the objective function value is guaranteed to be convergent while the variable sequence has a subsequence converging to a critical point. Experimental results based on real-world images and videos exhibit the superiority of the devised approach over several state-of-the-art algorithms in terms of recovery performance. MATLAB code is available at https://github.com/Li-X-P/Code-of-Robust-Tensor-Completion .},
  archive      = {J_TNNLS},
  author       = {Xiao Peng Li and Zhi-Yong Wang and Zhang-Lei Shi and Hing Cheung So and Nicholas D. Sidiropoulos},
  doi          = {10.1109/TNNLS.2023.3236415},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9700-9712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust tensor completion via capped frobenius norm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient learning with the mode-induced loss: Consistency
analysis and applications. <em>TNNLS</em>, <em>35</em>(7), 9686–9699.
(<a href="https://doi.org/10.1109/TNNLS.2023.3236345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection methods aim to select the key covariates related to the response variable for learning problems with high-dimensional data. Typical methods of variable selection are formulated in terms of sparse mean regression with a parametric hypothesis class, such as linear functions or additive functions. Despite rapid progress, the existing methods depend heavily on the chosen parametric function class and are incapable of handling variable selection for problems where the data noise is heavy-tailed or skewed. To circumvent these drawbacks, we propose sparse gradient learning with the mode-induced loss (SGLML) for robust model-free (MF) variable selection. The theoretical analysis is established for SGLML on the upper bound of excess risk and the consistency of variable selection, which guarantees its ability for gradient estimation from the lens of gradient risk and informative variable identification under mild conditions. Experimental analysis on the simulated and real data demonstrates the competitive performance of our method over the previous gradient learning (GL) methods.},
  archive      = {J_TNNLS},
  author       = {Hong Chen and Youcheng Fu and Xue Jiang and Yanhong Chen and Weifu Li and Yicong Zhou and Feng Zheng},
  doi          = {10.1109/TNNLS.2023.3236345},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9686-9699},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gradient learning with the mode-induced loss: Consistency analysis and applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast filter pruning via coarse-to-fine neural architecture
search and contrastive knowledge transfer. <em>TNNLS</em>,
<em>35</em>(7), 9674–9685. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Filter pruning is the most representative technique for lightweighting convolutional neural networks (CNNs). In general, filter pruning consists of the pruning and fine-tuning phases, and both still require a considerable computational cost. So, to increase the usability of CNNs, filter pruning itself needs to be lightweighted. For this purpose, we propose a coarse-to-fine neural architecture search (NAS) algorithm and a fine-tuning structure based on contrastive knowledge transfer (CKT). First, candidates of subnetworks are coarsely searched by a filter importance scoring (FIS) technique, and then the best subnetwork is obtained by a fine search based on NAS-based pruning. The proposed pruning algorithm does not require a supernet and adopts a computationally efficient search process, so it can create a pruned network with higher performance at a lower cost than the existing NAS-based search algorithms. Next, a memory bank is configured to store the information of interim subnetworks, i.e., by-products of the above-mentioned subnetwork search phase. Finally, the fine-tuning phase delivers the information of the memory bank through a CKT algorithm. Thanks to the proposed fine-tuning algorithm, the pruned network accomplishes high performance and fast convergence speed because it can take clear guidance from the memory bank. Experiments on various datasets and models prove that the proposed method has a significant speed efficiency with reasonable performance leakage over the state-of-the-art (SOTA) models. For example, the proposed method pruned the ResNet-50 trained on Imagenet-2012 up to 40.01% with no accuracy loss. Also, since the computational cost amounts to only 210 GPU hours, the proposed method is computationally more efficient than SOTA techniques. The source code is publicly available at https://github.com/sseung0703/FFP .},
  archive      = {J_TNNLS},
  author       = {Seunghyun Lee and Byung Cheol Song},
  doi          = {10.1109/TNNLS.2023.3236336},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9674-9685},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast filter pruning via coarse-to-fine neural architecture search and contrastive knowledge transfer},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Coupled multimodal emotional feature analysis based on
broad-deep fusion networks in human–robot interaction. <em>TNNLS</em>,
<em>35</em>(7), 9663–9673. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A coupled multimodal emotional feature analysis (CMEFA) method based on broad–deep fusion networks, which divide multimodal emotion recognition into two layers, is proposed. First, facial emotional features and gesture emotional features are extracted using the broad and deep learning fusion network (BDFN). Considering that the bi-modal emotion is not completely independent of each other, canonical correlation analysis (CCA) is used to analyze and extract the correlation between the emotion features, and a coupling network is established for emotion recognition of the extracted bi-modal features. Both simulation and application experiments are completed. According to the simulation experiments completed on the bimodal face and body gesture database (FABO), the recognition rate of the proposed method has increased by 1.15% compared to that of the support vector machine recursive feature elimination (SVMRFE) (without considering the unbalanced contribution of features). Moreover, by using the proposed method, the multimodal recognition rate is 21.22%, 2.65%, 1.61%, 1.54%, and 0.20% higher than those of the fuzzy deep neural network with sparse autoencoder (FDNNSA), ResNet-101 + GFK, C3D + MCB + DBN, the hierarchical classification fusion strategy (HCFS), and cross-channel convolutional neural network (CCCNN), respectively. In addition, preliminary application experiments are carried out on our developed emotional social robot system, where emotional robot recognizes the emotions of eight volunteers based on their facial expressions and body gestures.},
  archive      = {J_TNNLS},
  author       = {Luefeng Chen and Min Li and Min Wu and Witold Pedrycz and Kaoru Hirota},
  doi          = {10.1109/TNNLS.2023.3236320},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9663-9673},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coupled multimodal emotional feature analysis based on broad-deep fusion networks in Human–Robot interaction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical bidirected graph convolutions for large-scale
3-d point cloud place recognition. <em>TNNLS</em>, <em>35</em>(7),
9651–9662. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel hierarchical bidirected graph convolution network (HiBi-GCN) for large-scale 3-D point cloud place recognition. Unlike place recognition methods based on 2-D images, those based on 3-D point cloud data are typically robust to substantial changes in real-world environments. However, these methods have difficulty in defining convolution for point cloud data to extract informative features. To solve this problem, we propose a new hierarchical kernel defined as a hierarchical graph structure through unsupervised clustering from the data. In particular, we pool hierarchical graphs from the fine to coarse direction using pooling edges and fuse the pooled graphs from the coarse to fine direction using fusing edges. The proposed method can, thus, learn representative features hierarchically and probabilistically; moreover, it can extract discriminative and informative global descriptors for place recognition. Experimental results demonstrate that the proposed hierarchical graph structure is more suitable for point clouds to represent real-world 3-D scenes.},
  archive      = {J_TNNLS},
  author       = {Dong Wook Shu and Junseok Kwon},
  doi          = {10.1109/TNNLS.2023.3236313},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9651-9662},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical bidirected graph convolutions for large-scale 3-D point cloud place recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online learning with uncertain feedback graphs.
<em>TNNLS</em>, <em>35</em>(7), 9636–9650. (<a
href="https://doi.org/10.1109/TNNLS.2023.3235734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning with expert advice is widely used in various machine learning tasks. It considers the problem where a learner chooses one from a set of experts to take advice and make a decision. In many learning problems, experts may be related, henceforth the learner can observe the losses associated with a subset of experts that are related to the chosen one. In this context, the relationship among experts can be captured by a feedback graph, which can be used to assist the learner’s decision-making. However, in practice, the nominal feedback graph often entails uncertainties, which renders it impossible to reveal the actual relationship among experts. To cope with this challenge, the present work studies various cases of potential uncertainties and develops novel online learning algorithms to deal with uncertainties while making use of the uncertain feedback graph. The proposed algorithms are proved to enjoy sublinear regret under mild conditions. Experiments on real datasets are presented to demonstrate the effectiveness of the novel algorithms.},
  archive      = {J_TNNLS},
  author       = {Pouya M. Ghari and Yanning Shen},
  doi          = {10.1109/TNNLS.2023.3235734},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9636-9650},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online learning with uncertain feedback graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Achieving better category separability for hyperspectral
image classification: A spatial–spectral approach. <em>TNNLS</em>,
<em>35</em>(7), 9621–9635. (<a
href="https://doi.org/10.1109/TNNLS.2023.3235711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of hyperspectral image (HSI) classification has attracted extensive attention. The rich spectral information in HSIs not only provides more detailed information but also brings a lot of redundant information. Redundant information makes spectral curves of different categories have similar trends, which leads to poor category separability. In this article, we achieve better category separability from the perspective of increasing the difference between categories and reducing the variation within category, thus improving the classification accuracy. Specifically, we propose the template spectrum-based processing module from spectral perspective, which can effectively expose the unique characteristics of different categories and reduce the difficulty of model mining key features. Second, we design an adaptive dual attention network from spatial perspective, where the target pixel can adaptively aggregate high-level features by evaluating the confidence of effective information in different receptive fields. Compared with the single adjacency scheme, the adaptive dual attention mechanism makes the ability of target pixel to combine spatial information to reduce variation more stable. Finally, we designed a dispersion loss from the classifier’s perspective. By supervising the learnable parameters of the final classification layer, the loss makes the category standard eigenvectors learned by the model more dispersed, which improves the category separability and reduces the rate of misclassification. Experiments on three common datasets show that our proposed method is superior to the comparison method.},
  archive      = {J_TNNLS},
  author       = {Jing Bai and Wei Shi and Zhu Xiao and Talal Ahmed Ali Ali and Fawang Ye and Licheng Jiao},
  doi          = {10.1109/TNNLS.2023.3235711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9621-9635},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Achieving better category separability for hyperspectral image classification: A Spatial–Spectral approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced gradient for differentiable architecture search.
<em>TNNLS</em>, <em>35</em>(7), 9606–9620. (<a
href="https://doi.org/10.1109/TNNLS.2023.3235479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, neural architecture search (NAS) methods have been proposed for the automatic generation of task-oriented network architecture in image classification. However, the architectures obtained by existing NAS approaches are optimized only for classification performance and do not adapt to devices with limited computational resources. To address this challenge, we propose a neural network architecture search algorithm aiming to simultaneously improve the network performance and reduce the network complexity. The proposed framework automatically builds the network architecture at two stages: block-level search and network-level search. At the stage of block-level search, a gradient-based relaxation method is proposed, using an enhanced gradient to design high-performance and low-complexity blocks. At the stage of network-level search, an evolutionary multiobjective algorithm is utilized to complete the automatic design from blocks to the target network. The experimental results demonstrate that our method outperforms all evaluated hand-crafted networks in image classification, with an error rate of 3.18% on Canadian Institute for Advanced Research (CIFAR10) and an error rate of 19.16% on CIFAR100, both at network parameter size less than 1 M. Obviously, compared with other NAS methods, our method offers a tremendous reduction in designed network architecture parameters.},
  archive      = {J_TNNLS},
  author       = {Haichao Zhang and Kuangrong Hao and Lei Gao and Xue-Song Tang and Bing Wei},
  doi          = {10.1109/TNNLS.2023.3235479},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9606-9620},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhanced gradient for differentiable architecture search},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Surrogate-assisted and filter-based multiobjective
evolutionary feature selection for deep learning. <em>TNNLS</em>,
<em>35</em>(7), 9591–9605. (<a
href="https://doi.org/10.1109/TNNLS.2023.3234629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) for deep learning prediction models is a difficult topic for researchers to tackle. Most of the approaches proposed in the literature consist of embedded methods through the use of hidden layers added to the neural network architecture that modify the weights of the units associated with each input attribute so that the worst attributes have less weight in the learning process. Other approaches used for deep learning are filter methods, which are independent of the learning algorithm, which can limit the precision of the prediction model. Wrapper methods are impractical with deep learning due to their high computational cost. In this article, we propose new attribute subset evaluation FS methods for deep learning of the wrapper, filter and wrapper-filter hybrid types, where multiobjective and many-objective evolutionary algorithms are used as search strategies. A novel surrogate-assisted approach is used to reduce the high computational cost of the wrapper-type objective function, while the filter-type objective functions are based on correlation and an adaptation of the reliefF algorithm. The proposed techniques have been applied in a time series forecasting problem of air quality in the Spanish south-east and an indoor temperature forecasting problem in a domotic house, with promising results compared to other FS techniques used in the literature.},
  archive      = {J_TNNLS},
  author       = {Raquel Espinosa and Fernando Jiménez and José Palma},
  doi          = {10.1109/TNNLS.2023.3234629},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9591-9605},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Surrogate-assisted and filter-based multiobjective evolutionary feature selection for deep learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end multitask learning with vision transformer.
<em>TNNLS</em>, <em>35</em>(7), 9579–9590. (<a
href="https://doi.org/10.1109/TNNLS.2023.3234166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask learning (MTL) is a challenging puzzle, particularly in the realm of computer vision (CV). Setting up vanilla deep MTL requires either hard or soft parameter sharing schemes that employ greedy search to find the optimal network designs. Despite its widespread application, the performance of MTL models is vulnerable to under-constrained parameters. In this article, we draw on the recent success of vision transformer (ViT) to propose a multitask representation learning method called multitask ViT (MTViT), which proposes a multiple branch transformer to sequentially process the image patches (i.e., tokens in transformer) that are associated with various tasks. Through the proposed cross-task attention (CA) module, a task token from each task branch is regarded as a query for exchanging information with other task branches. In contrast to prior models, our proposed method extracts intrinsic features using the built-in self-attention mechanism of the ViT and requires just linear time on memory and computation complexity, rather than quadratic time. Comprehensive experiments are carried out on two benchmark datasets, including NYU-Depth V2 (NYUDv2) and CityScapes, after which it is found that our proposed MTViT outperforms or is on par with existing convolutional neural network (CNN)-based MTL methods. In addition, we apply our method to a synthetic dataset in which task relatedness is controlled. Surprisingly, experimental results reveal that the MTViT exhibits excellent performance when tasks are less related.},
  archive      = {J_TNNLS},
  author       = {Yingjie Tian and Kunlong Bai},
  doi          = {10.1109/TNNLS.2023.3234166},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9579-9590},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {End-to-end multitask learning with vision transformer},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stochastic recurrent encoder decoder network for multistep
probabilistic wind power predictions. <em>TNNLS</em>, <em>35</em>(7),
9565–9578. (<a
href="https://doi.org/10.1109/TNNLS.2023.3234130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a stochastic recurrent encoder decoder neural network (SREDNN), which considers latent random variables in its recurrent structures, is developed for the first time for the generative multistep probabilistic wind power predictions (MPWPPs). The SREDNN enables the stochastic recurrent model under the encoder-decoder framework to engage exogenous covariates to produce better MPWPP. The SREDNN consists of five components, the prior network, the inference network, the generative network, the encoder recurrent network, and the decoder recurrent network. The SREDNN is equipped with two critical advantages compared with conventional RNN-based methods. First, the integration over the latent random variable builds an infinite Gaussian mixture model (IGMM) as the observation model, which drastically increases the expressiveness of the wind power distribution. Secondly, hidden states of the SREDNN are updated in a stochastic way, which builds an infinite mixture of the IGMM for describing the ultimate wind power distribution and enables the SREDNN to model complex patterns across wind speed and wind power sequences. Computational experiments are conducted on a dataset of a commercial wind farm having 25 wind turbines (WTs) and two publicly assessable WT datasets to verify the advantages and effectiveness of the SREDNN for MPWPP. Experimental results show that the SREDNN achieves a lower negative form of the continuously ranked probability score (CRPS*) as well as a superior sharpness and comparable reliability of prediction intervals by comparing against considered benchmarking models. Results also show the clear benefit gained from considering latent random variables in SREDNN.},
  archive      = {J_TNNLS},
  author       = {Zhong Zheng and Zijun Zhang},
  doi          = {10.1109/TNNLS.2023.3234130},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9565-9578},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A stochastic recurrent encoder decoder network for multistep probabilistic wind power predictions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed multiagent reinforcement learning with action
networks for dynamic economic dispatch. <em>TNNLS</em>, <em>35</em>(7),
9553–9564. (<a
href="https://doi.org/10.1109/TNNLS.2023.3234049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new class of distributed multiagent reinforcement learning (MARL) algorithm suitable for problems with coupling constraints is proposed in this article to address the dynamic economic dispatch problem (DEDP) in smart grids. Specifically, the assumption made commonly in most existing results on the DEDP that the cost functions are known and/or convex is removed in this article. A distributed projection optimization algorithm is designed for the generation units to find the feasible power outputs satisfying the coupling constraints. By using a quadratic function to approximate the state-action value function of each generation unit, the approximate optimal solution of the original DEDP can be obtained by solving a convex optimization problem. Then, each action network utilizes a neural network (NN) to learn the relationship between the total power demand and the optimal power output of each generation unit, such that the algorithm obtains the generalization ability to predict the optimal power output distribution on an unseen total power demand. Furthermore, an improved experience replay mechanism is introduced into the action networks to improve the stability of the training process. Finally, the effectiveness and robustness of the proposed MARL algorithm are verified by simulation.},
  archive      = {J_TNNLS},
  author       = {Chengfang Hu and Guanghui Wen and Shuai Wang and Junjie Fu and Wenwu Yu},
  doi          = {10.1109/TNNLS.2023.3234049},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9553-9564},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed multiagent reinforcement learning with action networks for dynamic economic dispatch},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representation learning on heterostructures via
heterogeneous anonymous walks. <em>TNNLS</em>, <em>35</em>(7),
9538–9552. (<a
href="https://doi.org/10.1109/TNNLS.2023.3234005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing structural similarity has been a hot topic in the field of network embedding (NE) recently due to its great help in understanding node functions and behaviors. However, existing works have paid very much attention to learning structures on homogeneous networks, while the related study on heterogeneous networks is still void. In this article, we try to take the first step for representation learning on heterostructures, which is very challenging due to their highly diverse combinations of node types and underlying structures. To effectively distinguish diverse heterostructures, we first propose a theoretically guaranteed technique called heterogeneous anonymous walk (HAW) and give two more applicable variants. Then, we devise the HAW embedding (HAWE) and its variants in a data-driven manner to circumvent using an extremely large number of possible walks and train embeddings by predicting occurring walks in the neighborhood of each node. Finally, we design and apply extensive and illustrative experiments on synthetic and real-world networks to build a benchmark on heterostructure learning and evaluate the effectiveness of our methods. The results demonstrate our methods achieve outstanding performance compared with both homogeneous and heterogeneous classic methods and can be applied on large-scale networks.},
  archive      = {J_TNNLS},
  author       = {Xuan Guo and Pengfei Jiao and Wang Zhang and Ting Pan and Mengyu Jia and Danyang Shi and Wenjun Wang},
  doi          = {10.1109/TNNLS.2023.3234005},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9538-9552},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Representation learning on heterostructures via heterogeneous anonymous walks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interaction-and-response network for distantly supervised
relation extraction. <em>TNNLS</em>, <em>35</em>(7), 9523–9537. (<a
href="https://doi.org/10.1109/TNNLS.2023.3233971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distantly supervised relation extraction (DSRE) aims to identify semantic relations from massive plain texts. A broad range of the prior research has leveraged a series of selective attention mechanisms over sentences in a bag to extract relation features without considering dependencies among the relation features. As a result, potential discriminative information existed in the dependencies is ignored, causing a decline in the performance of extracting entity relations. In this article, we focus on going beyond the selective attention mechanisms and propose a new framework termed interaction-and-response network (IR-Net) that adaptively recalibrates the features of sentence, bag, and group levels by explicitly modeling interdependencies among the features on each level. The IR-Net consists of a series of interactive and responsive modules throughout feature hierarchy, seeking to strengthen its power of learning salient discriminative features for distinguishing entity relations. We conduct extensive experiments on three benchmark DSRE datasets, including NYT-10, NYT-16, and Wiki-20m. The experimental results demonstrate that the IR-Net brings obvious improvements in performance when comparing ten state-of-the-art DSRE methods for entity relation extraction.},
  archive      = {J_TNNLS},
  author       = {Wei Song and Weishuai Gu and Fuxin Zhu and Soon Cheol Park},
  doi          = {10.1109/TNNLS.2023.3233971},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9523-9537},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interaction-and-response network for distantly supervised relation extraction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural projection filter: Learning unknown dynamics driven
by noisy observations. <em>TNNLS</em>, <em>35</em>(7), 9508–9522. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose the novel neural stochastic differential equations (SDEs) driven by noisy sequential observations called neural projection filter (NPF) under the continuous state-space models (SSMs) framework. The contributions of this work are both theoretical and algorithmic. On the one hand, we investigate the approximation capacity of the NPF, i.e., the universal approximation theorem for NPF. More explicitly, under some natural assumptions, we prove that the solution of the SDE driven by the semimartingale can be well approximated by the solution of the NPF. In particular, the explicit estimation bound is given. On the other hand, as an important application of this result, we develop a novel data-driven filter based on NPF. Also, under certain condition, we prove the algorithm convergence; i.e., the dynamics of NPF converges to the target dynamics. At last, we systematically compare the NPF with the existing filters. We verify the convergence theorem in linear case and experimentally demonstrate that the NPF outperforms existing filters in nonlinear case with robustness and efficiency. Furthermore, NPF could handle high-dimensional systems in real-time manner, even for the 100-D cubic sensor, while the state-of-the-art (SOTA) filter fails to do it.},
  archive      = {J_TNNLS},
  author       = {Yangtianze Tao and Jiayi Kang and Stephen Shing-Toung Yau},
  doi          = {10.1109/TNNLS.2022.3233888},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9508-9522},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural projection filter: Learning unknown dynamics driven by noisy observations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-projection fusion and refinement network for salient
object detection in 360° omnidirectional image. <em>TNNLS</em>,
<em>35</em>(7), 9495–9507. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) aims to determine the most visually attractive objects in an image. With the development of virtual reality (VR) technology, 360° omnidirectional image has been widely used, but the SOD task in 360° omnidirectional image is seldom studied due to its severe distortions and complex scenes. In this article, we propose a multi-projection fusion and refinement network (MPFR-Net) to detect the salient objects in 360° omnidirectional image. Different from the existing methods, the equirectangular projection (EP) image and four corresponding cube-unfolding (CU) images are embedded into the network simultaneously as inputs, where the CU images not only provide supplementary information for EP image but also ensure the object integrity of cube-map projection. In order to make full use of these two projection modes, a dynamic weighting fusion (DWF) module is designed to adaptively integrate the features of different projections in a complementary and dynamic manner from the perspective of inter and intrafeatures. Furthermore, in order to fully explore the way of interaction between encoder and decoder features, a filtration and refinement (FR) module is designed to suppress the redundant information of the feature itself and between the features. Experimental results on two omnidirectional datasets demonstrate that the proposed approach outperforms the state-of-the-art methods both qualitatively and quantitatively. The code and results can be found from the link of https://rmcong.github.io/proj_MPFRNet.html .},
  archive      = {J_TNNLS},
  author       = {Runmin Cong and Ke Huang and Jianjun Lei and Yao Zhao and Qingming Huang and Sam Kwong},
  doi          = {10.1109/TNNLS.2022.3233883},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9495-9507},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-projection fusion and refinement network for salient object detection in 360° omnidirectional image},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized-type multistability of almost periodic solutions
for memristive cohen–grossberg neural networks. <em>TNNLS</em>,
<em>35</em>(7), 9483–9494. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a generalized type of multistability about almost periodic solutions for memristive Cohen–Grossberg neural networks (MCGNNs). As the inevitable disturbances in biological neurons, almost periodic solutions are more common in nature than equilibrium points (EPs). They are also generalizations of EPs in mathematics. According to the concepts of almost periodic solutions and $\Psi $ -type stability, this article presents a generalized-type multistability definition of almost periodic solutions. The results show that $(K+1)^{n}$ generalized stable almost periodic solutions can coexist in a MCGNN with $n$ neurons, where $K$ is a parameter of the activation functions. The enlarged attraction basins are also estimated based on the original state space partition method. Some comparisons and convincing simulations are given to verify the theoretical results at the end of this article.},
  archive      = {J_TNNLS},
  author       = {Song Zhu and Yuanchu Shen and Chaoxu Mu and Xiaoyang Liu and Shiping Wen},
  doi          = {10.1109/TNNLS.2022.3233719},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9483-9494},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized-type multistability of almost periodic solutions for memristive Cohen–Grossberg neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal contextual learning for single object
tracking on point clouds. <em>TNNLS</em>, <em>35</em>(7), 9470–9482. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single object tracking (SOT) is one of the most active research directions in the field of computer vision. Compared with the 2-D image-based SOT which has already been well-studied, SOT on 3-D point clouds is a relatively emerging research field. In this article, a novel approach, namely, the contextual-aware tracker (CAT), is investigated to achieve a superior 3-D SOT through spatially and temporally contextual learning from the LiDAR sequence. More precisely, in contrast to the previous 3-D SOT methods merely exploiting point clouds in the target bounding box as the template, CAT generates templates by adaptively including the surroundings outside the target box to use available ambient cues. This template generation strategy is more effective and rational than the previous area-fixed one, especially when the object has only a small number of points. Moreover, it is deduced that LiDAR point clouds in 3-D scenes are often incomplete and significantly vary from frame to another, which makes the learning process more difficult. To this end, a novel cross-frame aggregation (CFA) module is proposed to enhance the feature representation of the template by aggregating the features from a historical reference frame. Leveraging such schemes enables CAT to achieve a robust performance, even in the case of extremely sparse point clouds. The experiments confirm that the proposed CAT outperforms the state-of-the-art methods on both the KITTI and NuScenes benchmarks, achieving 3.9% and 5.6% improvements in terms of precision.},
  archive      = {J_TNNLS},
  author       = {Jiantao Gao and Xu Yan and Weibing Zhao and Zhen Lyu and Yinghong Liao and Chaoda Zheng},
  doi          = {10.1109/TNNLS.2022.3233562},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9470-9482},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatio-temporal contextual learning for single object tracking on point clouds},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-identical twins variational AutoEncoder for few-shot
learning. <em>TNNLS</em>, <em>35</em>(7), 9455–9469. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is a popular way for few-shot learning (FSL). It generates more samples as supplements and then transforms the FSL task into a common supervised learning problem for a solution. However, most data-augmentation-based FSL approaches only consider the prior visual knowledge for feature generation, thereby leading to low diversity and poor quality of generated data. In this study, we attempt to address this issue by incorporating both prior visual and prior semantic knowledge to condition the feature generation process. Inspired by some genetic characteristics of semi-identical twins, a novel multimodal generative FSL approach was developed named semi-identical twins variational autoencoder (STVAE) to better exploit the complementarity of these modality information by considering the multimodal conditional feature generation process as a process that semi-identical twins are born and collaborate to simulate their father. STVAE conducts feature synthesis by pairing two conditional variational autoencoders (CVAEs) with the same seed but different modality conditions. Subsequently, the generated features of two CVAEs are considered as semi-identical twins and adaptively combined to yield the final feature, which is considered as their fake father. STVAE requires that the final feature can be converted back into its paired conditions while ensuring these conditions remain consistent with the original in both representation and function. Moreover, STVAE is able to work in the partial modality-absence case due to the adaptive linear feature combination strategy. STVAE essentially provides a novel idea to exploit the complementarity of different modality prior information inspired by genetics in FSL. Extensive experimental results demonstrate that our work achieves promising performances in comparison to the recent state-of-the-art approaches, as well as validate its effectiveness on FSL under various modality settings.},
  archive      = {J_TNNLS},
  author       = {Yi Zhang and Sheng Huang and Xi Peng and Dan Yang},
  doi          = {10.1109/TNNLS.2022.3233553},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9455-9469},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semi-identical twins variational AutoEncoder for few-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating relational knowledge with text sequences for
script event prediction. <em>TNNLS</em>, <em>35</em>(7), 9443–9454. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Script event prediction aims to infer subsequent events given an incomplete script. It requires a deep understanding of events, and can provide support for a variety of tasks. Existing models rarely consider the relational knowledge between events, they regard scripts as sequences or graphs, which cannot capture the relational information between events and the semantic information of script sequences jointly. To address this issue, we propose a new script form, relational event chain, that combines event chains and relational graphs. We also introduce a new model, relational-transformer, to learn embeddings based on this new script form. In particular, we first extract the relationship between events from an event knowledge graph to formalize scripts as relational event chains, then use the relational-transformer to calculate the likelihood of different candidate events, where the model learns event embeddings that encode both semantic and relational knowledge by combining transformers and graph neural networks (GNNs). Experimental results on both one-step inference and multistep inference tasks show that our model can outperform existing baselines, indicating the validity of encoding relational knowledge into event embeddings. The influence of using different model structures and different types of relational knowledge is analyzed as well.},
  archive      = {J_TNNLS},
  author       = {Zikang Wang and Linjing Li and Daniel Zeng},
  doi          = {10.1109/TNNLS.2022.3233371},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9443-9454},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Integrating relational knowledge with text sequences for script event prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved frequent directions algorithm for low-rank
approximation via block krylov iteration. <em>TNNLS</em>,
<em>35</em>(7), 9428–9442. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent directions (FDs), as a deterministic matrix sketching technique, have been proposed for tackling low-rank approximation problems. This method has a high degree of accuracy and practicality but experiences a lot of computational cost for large-scale data. Several recent works on the randomized version of FDs greatly improve the computational efficiency but unfortunately sacrifice some precision. To remedy such an issue, this article aims to find a more accurate projection subspace to further improve the efficiency and effectiveness of the existing FDs’ techniques. Specifically, by utilizing the power of the block Krylov iteration and random projection technique, this article presents a fast and accurate FDs algorithm named r-BKIFD. The rigorous theoretical analysis shows that the proposed r-BKIFD has a comparable error bound with original FDs, and the approximation error can be arbitrarily small when the number of iterations is chosen appropriately. Extensive experimental results on both synthetic and real data further demonstrate the superiority of r-BKIFD over several popular FDs algorithms both in terms of computational efficiency and accuracy.},
  archive      = {J_TNNLS},
  author       = {Chenhao Wang and Qianxin Yi and Xiuwu Liao and Yao Wang},
  doi          = {10.1109/TNNLS.2022.3233243},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9428-9442},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An improved frequent directions algorithm for low-rank approximation via block krylov iteration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Late fusion multiview clustering via min-max optimization.
<em>TNNLS</em>, <em>35</em>(7), 9417–9427. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering (MVC) sufficiently exploits the diverse and complementary information among different views to improve the clustering performance. As a representative algorithm of MVC, the newly proposed simple multiple kernel ${k}$ -means (SimpleMKKM) algorithm takes a min-max formulation and applies a gradient descent algorithm to decrease the resultant objective function. It is empirically observed that its superiority is attributed to the novel min-max formulation and the new optimization. In this article, we propose to integrate the min-max learning paradigm adopted by SimpleMKKM into late fusion MVC (LF-MVC). This leads to a tri-level max-min-max optimization problem with respect to the perturbation matrices, weight coefficient, and clustering partition matrix. To solve this intractable max-min-max optimization problem, we design an efficient two-step alternative optimization strategy. Furthermore, we analyze the generalization clustering performance of the proposed algorithm from the theoretical perspective. Comprehensive experiments have been conducted to evaluate the proposed algorithm in terms of clustering accuracy (ACC), computation time, convergence, as well as the evolution of the learned consensus clustering matrix, clustering with different numbers of samples, and analysis of the learned kernel weight. The experimental results show that the proposed algorithm is able to significantly reduce the computation time and improve the clustering ACC when compared to several state-of-the-art LF-MVC algorithms. The code of this work is publicly released at: https://xinwangliu.github.io/Under-Review .},
  archive      = {J_TNNLS},
  author       = {Miaomiao Li and Xinwang Liu and Yi Zhang and Weixuan Liang},
  doi          = {10.1109/TNNLS.2022.3233179},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9417-9427},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Late fusion multiview clustering via min-max optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predefined-time zeroing neural networks with independent
prior parameter for solving time-varying plural lyapunov tensor
equation. <em>TNNLS</em>, <em>35</em>(7), 9408–9416. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extension of the Lyapunov equation, the time-varying plural Lyapunov tensor equation (TV-PLTE) can carry multidimensional data, which can be solved by zeroing neural network (ZNN) models effectively. However, existing ZNN models only focus on time-varying equations in field of real number. Besides, the upper bound of the settling time depends on the value of ZNN model parameters, which is a conservative estimation for existing ZNN models. Therefore, this article proposes a novel design formula for converting the upper bound of the settling time into an independent and directly modifiable prior parameter. On this basis, we design two new ZNN models called strong predefined-time convergence ZNN (SPTC-ZNN) and fast predefined (FP)-time convergence ZNN (FPTC-ZNN) models. The SPTC-ZNN model has a nonconservative upper bound of the settling time, and the FPTC-ZNN model has excellent convergence performance. The upper bound of the settling time and robustness of the SPTC-ZNN and FPTC-ZNN models are verified by theoretical analyses. Then, the effect of noise on the upper bound of settling time is discussed. The simulation results show that the SPTC-ZNN and FPTC-ZNN models have better comprehensive performance than existing ZNN models.},
  archive      = {J_TNNLS},
  author       = {Zhaohui Qi and Yingqiang Ning and Lin Xiao and Yongjun He and Jiajie Luo and Biao Luo},
  doi          = {10.1109/TNNLS.2022.3233050},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9408-9416},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Predefined-time zeroing neural networks with independent prior parameter for solving time-varying plural lyapunov tensor equation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balanced federated semisupervised learning with
fairness-aware pseudo-labeling. <em>TNNLS</em>, <em>35</em>(7),
9395–9407. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated semisupervised learning (FSSL) aims to train models with both labeled and unlabeled data in the federated settings, enabling performance improvement and easier deployment in realistic scenarios. However, the nonindependently identical distributed data in clients leads to imbalanced model training due to the unfair learning effects on different classes. As a result, the federated model exhibits inconsistent performance on not only different classes, but also different clients. This article presents a balanced FSSL method with the fairness-aware pseudo-labeling (FAPL) strategy to tackle the fairness issue. Specifically, this strategy globally balances the total number of unlabeled data samples which is capable to participate in model training. Then, the global numerical restrictions are further decomposed into personalized local restrictions for each client to assist the local pseudo-labeling. Consequently, this method derives a more fair federated model for all clients and gains better performance. Experiments on image classification datasets demonstrate the superiority of the proposed method over the state-of-the-art FSSL methods.},
  archive      = {J_TNNLS},
  author       = {Xiao-Xiang Wei and Hua Huang},
  doi          = {10.1109/TNNLS.2022.3233093},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9395-9407},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Balanced federated semisupervised learning with fairness-aware pseudo-labeling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Mitigating modality discrepancies for RGB-t semantic
segmentation. <em>TNNLS</em>, <em>35</em>(7), 9380–9394. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation models gain robustness against adverse illumination conditions by taking advantage of complementary information from visible and thermal infrared (RGB-T) images. Despite its importance, most existing RGB-T semantic segmentation models directly adopt primitive fusion strategies, such as elementwise summation, to integrate multimodal features. Such strategies, unfortunately, overlook the modality discrepancies caused by inconsistent unimodal features obtained by two independent feature extractors, thus hindering the exploitation of cross-modal complementary information within the multimodal data. For that, we propose a novel network for RGB-T semantic segmentation, i.e. MDRNet+, which is an improved version of our previous work ABMDRNet. The core of MDRNet+ is a brand new idea, termed the strategy of bridging-then-fusing, which mitigates modality discrepancies before cross-modal feature fusion. Concretely, an improved Modality Discrepancy Reduction (MDR+) subnetwork is designed, which first extracts unimodal features and reduces their modality discrepancies. Afterward, discriminative multimodal features for RGB-T semantic segmentation are adaptively selected and integrated via several channel-weighted fusion (CWF) modules. Furthermore, a multiscale spatial context (MSC) module and a multiscale channel context (MCC) module are presented to effectively capture the contextual information. Finally, we elaborately assemble a challenging RGB-T semantic segmentation dataset, i.e., RTSS, for urban scene understanding to mitigate the lack of well-annotated training data. Comprehensive experiments demonstrate that our proposed model surpasses other state-of-the-art models on the MFNet, PST900, and RTSS datasets remarkably.},
  archive      = {J_TNNLS},
  author       = {Shenlu Zhao and Yichen Liu and Qiang Jiao and Qiang Zhang and Jungong Han},
  doi          = {10.1109/TNNLS.2022.3233089},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9380-9394},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mitigating modality discrepancies for RGB-T semantic segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection of deepfake videos using long-distance attention.
<em>TNNLS</em>, <em>35</em>(7), 9366–9379. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid progress of deepfake techniques in recent years, facial video forgery can generate highly deceptive video content and bring severe security threats. And detection of such forgery videos is much more urgent and challenging. Most existing detection methods treat the problem as a vanilla binary classification problem. In this article, the problem is treated as a special fine-grained classification problem since the differences between fake and real faces are very subtle. It is observed that most existing face forgery methods left some common artifacts in the spatial domain and time domain, including generative defects in the spatial domain and interframe inconsistencies in the time domain. And a spatial-temporal model is proposed which has two components for capturing spatial and temporal forgery traces from a global perspective, respectively. The two components are designed using a novel long-distance attention mechanism. One component of the spatial domain is used to capture artifacts in a single frame, and the other component of the time domain is used to capture artifacts in consecutive frames. They generate attention maps in the form of patches. The attention method has a broader vision which contributes to better assembling global information and extracting local statistic information. Finally, the attention maps are used to guide the network to focus on pivotal parts of the face, just like other fine-grained classification methods. The experimental results on different public datasets demonstrate that the proposed method achieves state-of-the-art performance, and the proposed long-distance attention method can effectively capture pivotal parts for face forgery.},
  archive      = {J_TNNLS},
  author       = {Wei Lu and Lingyi Liu and Bolin Zhang and Junwei Luo and Xianfeng Zhao and Yicong Zhou and Jiwu Huang},
  doi          = {10.1109/TNNLS.2022.3233063},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9366-9379},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Detection of deepfake videos using long-distance attention},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PMSGAN: Parallel multistage GANs for face image translation.
<em>TNNLS</em>, <em>35</em>(7), 9352–9365. (<a
href="https://doi.org/10.1109/TNNLS.2022.3233025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address the face image translation task, which aims to translate a face image of a source domain to a target domain. Although significant progress has been made by recent studies, face image translation is still a challenging task because it has more strict requirements for texture details: even a few artifacts will greatly affect the impression of generated face images. Targeting to synthesize high-quality face images with admirable visual appearance, we revisit the coarse-to-fine strategy and propose a novel p arallel m ultistage architecture on the basis of g enerative a dversarial n etworks (PMSGAN). More specifically, PMSGAN progressively learns the translation function by disintegrating the general synthesis process into multiple parallel stages that take images with gradually decreasing spatial resolution as inputs. To prompt the information exchange between various stages, a cross-stage atrous spatial pyramid (CSASP) structure is specially designed to receive and fuse the contextual information from other stages. At the end of the parallel model, we introduce a novel attention-based module that leverages multistage decoded outputs as in situ supervised attention to refine the final activations and yield the target image. Extensive experiments on several face image translation benchmarks show that PMSGAN performs considerably better than state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Changcheng Liang and Mingrui Zhu and Nannan Wang and Heng Yang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2022.3233025},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9352-9365},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PMSGAN: Parallel multistage GANs for face image translation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). HGBER: Heterogeneous graph neural network with
bidirectional encoding representation. <em>TNNLS</em>, <em>35</em>(7),
9340–9351. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graphs with multiple types of nodes and link relationships are ubiquitous in many real-world applications. Heterogeneous graph neural networks (HGNNs) as an efficient technique have shown superior capacity of dealing with heterogeneous graphs. Existing HGNNs usually define multiple meta-paths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models only consider the simple relationships (i.e., concatenation or linear superposition) between different meta-paths, ignoring more general or complex relationships. In this article, we propose a novel unsupervised framework termed Heterogeneous Graph neural network with bidirectional encoding representation (HGBER) to learn comprehensive node representations. Specifically, the contrastive forward encoding is firstly performed to extract node representations on a set of meta-specific graphs corresponding to meta-paths. We then introduce the reversed encoding for the degradation process from the final node representations to each single meta-specific node representations. Moreover, to learn structure-preserving node representations, we further utilize a self-training module to discover the optimal node distribution through iterative optimization. Extensive experiments on five open public datasets show that the proposed HGBER model outperforms the state-of-the-art HGNNs baselines by 0.8%–8.4% in terms of accuracy on most datasets in various downstream tasks.},
  archive      = {J_TNNLS},
  author       = {Yanbei Liu and Lianxi Fan and Xiao Wang and Zhitao Xiao and Shuai Ma and Yanwei Pang and Jerry Chun-Wei Lin},
  doi          = {10.1109/TNNLS.2022.3232709},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9340-9351},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HGBER: Heterogeneous graph neural network with bidirectional encoding representation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuro-optimal event-triggered impulsive control for
stochastic systems via ADP. <em>TNNLS</em>, <em>35</em>(7), 9325–9339.
(<a href="https://doi.org/10.1109/TNNLS.2022.3232635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel neural-network-based optimal event-triggered impulsive control method. First, a novel general-event-based impulsive transition matrix (GITM) is constructed to represent the probability distribution evolving characteristics regarding all system states across the impulsive actions, rather than the prefixed timing sequence. On the foundation of this GITM, the event-triggered impulsive adaptive dynamic programming (ETIADP) algorithm and its high-efficiency version (HEIADP) are developed to deal with the optimization problems for stochastic systems with event-triggered impulsive controls. It is shown that the obtained controller design scheme can reduce the computational and communication burden caused by updating the controller periodically. By analyzing the admissibility, monotonicity, and optimality properties of ETIADP and HEIADP, we further establish the approximation error bound of the neural networks to address the connection between the ideal and neural-network-based realizations of the present methods. It is proven that the iterative value functions of both the ETIADP and HEIADP algorithms fall in a small neighborhood of the optimum as the iteration index increases to infinity. By adopting a novel task synchronization mechanism, the proposed HEIADP algorithm fully utilizes the computing resources of multiprocessor systems (MPSs), while significantly reducing the memory requirement compared to traditional ADP approaches. Finally, we carry out a numerical study to show that the proposed methods can fulfill the desired goals.},
  archive      = {J_TNNLS},
  author       = {Mingming Liang and Derong Liu},
  doi          = {10.1109/TNNLS.2022.3232635},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9325-9339},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuro-optimal event-triggered impulsive control for stochastic systems via ADP},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-rank tensor regularized views recovery for incomplete
multiview clustering. <em>TNNLS</em>, <em>35</em>(7), 9312–9324. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real applications, it is often that the collected multiview data contain missing views. Most existing incomplete multiview clustering (IMVC) methods cannot fully utilize the underlying information of missing data or sufficiently explore the consistent and complementary characteristics. In this article, we propose a novel Low-rAnk Tensor regularized viEws Recovery (LATER) method for IMVC, which jointly reconstructs and utilizes the missing views and learns multilevel graphs for comprehensive similarity discovery in a unified model. The missing views are recovered from a common latent representation, and the recovered views conversely improve the learning of shared patterns. Based on the shared subspace representations and recovered complete multiview data, the multilevel graphs are learned by self-representation to fully exploit the consistent and complementary information among views. Besides, a tensor nuclear norm regularizer is introduced to pursue the global low-rank property and explore the interview correlations. An alternating direction minimization algorithm is presented to optimize the proposed model. Moreover, a new initialization method is proposed to promote the effectiveness of our method for latent representation learning and missing data recovery. Extensive experiments demonstrate that our method outperforms the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Chao Zhang and Huaxiong Li and Caihua Chen and Xiuyi Jia and Chunlin Chen},
  doi          = {10.1109/TNNLS.2022.3232538},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9312-9324},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low-rank tensor regularized views recovery for incomplete multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composite neighbor-aware convolutional metric networks for
hyperspectral image classification. <em>TNNLS</em>, <em>35</em>(7),
9297–9311. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised classification of hyperspectral image (HSI) is generally required to obtain better performance in spectral–spatial feature learning by fully using complex pixel- and superpixel-level interdependencies with small labeled samples. Limited by the local regular convolutions, convolutional neural networks (CNNs) can only exploit information from the short-range Euclidean neighbors of a target, hindering the effectiveness of feature representation. In contrast, graph convolutional networks (GCNs) can learn long-range dependencies between non-Euclidean neighbors but usually require the input of a full graph constructed from a whole HSI, making GCNs must be trained in a full-batch manner with tremendous computational consumption. In this work, we propose a composite neighbor-aware convolutional metric network (CNCMN), aiming to learn each target’s representation from its composite neighbors (i.e., both Euclidean and non-Euclidean neighbors) in a batchwise manner. Specifically, for each target in an HSI, its Euclidean neighbors are the pixels in the local square region centered on itself, and its non-Euclidean neighbors are several related nodes selected from the constructed full graph. Correspondingly, a composite convolution (CoConv) is proposed by coupling an image convolution and a graph convolution, which can perform flexible convolutions on those composite neighbors and extract adaptively fused features from them. Besides, to further boost classification, we also propose a mini-batch metric classifier to dynamically optimize interclass and intraclass distances of samples batch by batch, which is then combined with the CoConv to form the mini-batch CNCMN. Extensive experiments on three real-world HSIs demonstrate the advantages of the proposed method over mini-batch deep learning algorithms and have obtained the state-of-the-art performance in these fields. The code is available at: https://github.com/qichaoliu/HSI-CNCMN .},
  archive      = {J_TNNLS},
  author       = {Qichao Liu and Liang Xiao and Nan Huang and Jinhui Tang},
  doi          = {10.1109/TNNLS.2022.3232532},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9297-9311},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Composite neighbor-aware convolutional metric networks for hyperspectral image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature consistency-based prototype network for open-set
hyperspectral image classification. <em>TNNLS</em>, <em>35</em>(7),
9286–9296. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) classification methods have made great progress in recent years. However, most of these methods are rooted in the closed-set assumption that the class distribution in the training and testing stages is consistent, which cannot handle the unknown class in open-world scenes. In this work, we propose a feature consistency-based prototype network (FCPN) for open-set HSI classification, which is composed of three steps. First, a three-layer convolutional network is designed to extract the discriminative features, where a contrastive clustering module is introduced to enhance the discrimination. Then, the extracted features are used to construct a scalable prototype set. Finally, a prototype-guided open-set module (POSM) is proposed to identify the known samples and unknown samples. Extensive experiments reveal that our method achieves remarkable classification performance over other state-of-the-art classification techniques.},
  archive      = {J_TNNLS},
  author       = {Zhuojun Xie and Puhong Duan and Wang Liu and Xudong Kang and Xiaohui Wei and Shutao Li},
  doi          = {10.1109/TNNLS.2022.3232225},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9286-9296},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature consistency-based prototype network for open-set hyperspectral image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical spiking-based model for efficient image
classification with enhanced feature extraction and encoding.
<em>TNNLS</em>, <em>35</em>(7), 9277–9285. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to their event-driven nature, spiking neural networks (SNNs) are surmised to be great computation-efficient models. The spiking neurons encode beneficial temporal facts and possess excessive anti-noise properties. However, the high-quality encoding of spatio-temporal complexity and also its training optimization of SNNs are restricted by means of the contemporary problem, this article proposes a novel hierarchical event-driven visual device to explore how information transmits and signifies in the retina the usage of biologically manageable mechanisms. This cognitive model is an augmented spiking-based framework consisting of the function learning capacity of convolutional neural networks (CNNs) with the cognition capability of SNNs. Furthermore, this visual device is modeled in a biological realism way with unsupervised learning rules and advanced spike firing rate encoding methods. We train and test them on some image datasets (Modified National Institute of Standards and Technology (MNIST), Canadian Institute for Advanced Research (CIFAR)10, and its noisy versions) to show that our mannequin can process greater vital data than present cognitive models. This article also proposes a novel quantization approach to make the proposed spiking-based model more efficient for neuromorphic hardware implementation. The outcomes show this joint CNN-SNN model can reap excessive focus accuracy and get more effective generalization ability.},
  archive      = {J_TNNLS},
  author       = {Qi Xu and Yaxin Li and Jiangrong Shen and Pingping Zhang and Jian K. Liu and Huajin Tang and Gang Pan},
  doi          = {10.1109/TNNLS.2022.3232106},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9277-9285},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical spiking-based model for efficient image classification with enhanced feature extraction and encoding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative deep q-learning framework for environments
providing image feedback. <em>TNNLS</em>, <em>35</em>(7), 9267–9276. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address two key challenges in deep reinforcement learning (DRL) setting, sample inefficiency and slow learning, with a dual-neural network (NN)-driven learning approach. In the proposed approach, we use two deep NNs with independent initialization to robustly approximate the action-value function in the presence of image inputs. In particular, we develop a temporal difference (TD) error-driven learning (EDL) approach, where we introduce a set of linear transformations of the TD error to directly update the parameters of each layer in the deep NN. We demonstrate theoretically that the cost minimized by the EDL regime is an approximation of the empirical cost, and the approximation error reduces as learning progresses, irrespective of the size of the network. Using simulation analysis, we show that the proposed methods enable faster learning and convergence and require reduced buffer size (thereby increasing the sample efficiency).},
  archive      = {J_TNNLS},
  author       = {Krishnan Raghavan and Vignesh Narayanan and Sarangapani Jagannathan},
  doi          = {10.1109/TNNLS.2022.3232069},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9267-9276},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cooperative deep Q-learning framework for environments providing image feedback},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network with a preference sampling paradigm for
imbalanced data classification. <em>TNNLS</em>, <em>35</em>(7),
9252–9266. (<a
href="https://doi.org/10.1109/TNNLS.2022.3231917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most data in real life are characterized by imbalance problems. One of the classic models for dealing with imbalanced data is neural networks. However, the data imbalance problem often causes the neural network to display negative class preference behavior. Using an undersampling strategy to reconstruct a balanced dataset is one of the methods to alleviate the data imbalance problem. However, most existing undersampling methods focus more on the data or aim to preserve the overall structural characteristics of the negative class through potential energy estimation, while the problems of gradient inundation and insufficient empirical representation of positive samples have not been well considered. Therefore, a new paradigm for solving the data imbalance problem is proposed. Specifically, to solve the problem of gradient inundation, an informative undersampling strategy is derived from the performance degradation and used to restore the ability of neural networks to work under imbalanced data. In addition, to alleviate the problem of insufficient empirical representation of positive samples, a boundary expansion strategy with linear interpolation and the prediction consistency constraint is considered. We tested the proposed paradigm on 34 imbalanced datasets with imbalance ratios ranging from 16.90 to 100.14. The test results show that our paradigm obtained the best area under the receiver operating characteristic curve (AUC) on 26 datasets.},
  archive      = {J_TNNLS},
  author       = {Zhan ao Huang and Yongsheng Sang and Yanan Sun and Jiancheng Lv},
  doi          = {10.1109/TNNLS.2022.3231917},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9252-9266},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network with a preference sampling paradigm for imbalanced data classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial kinetic prototype framework for open set
recognition. <em>TNNLS</em>, <em>35</em>(7), 9238–9251. (<a
href="https://doi.org/10.1109/TNNLS.2022.3231924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity of real-world applications, open set recognition is often more practical than closed set recognition. Compared with closed set recognition, open set recognition needs not only to recognize known classes but also to identify unknown classes. Different from most of the current methods, we proposed three novel frameworks with kinetic pattern to address the open set recognition problems, and they are kinetic prototype framework (KPF), adversarial KPF (AKPF), and an upgraded version of the AKPF, AKPF++. First, KPF introduces a novel kinetic margin constraint radius, which can improve the compactness of the known features to increase the robustness for the unknowns. Based on KPF, AKPF can generate adversarial samples and add these samples into the training phase, which can improve the performance with the adversarial motion of the margin constraint radius. Compared with AKPF, AKPF++ further improves the performance by adding more generated data into the training phase. Extensive experimental results on various benchmark datasets indicate that the proposed frameworks with kinetic pattern are superior to other existing approaches and achieve the state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Ziheng Xia and Penghui Wang and Ganggang Dong and Hongwei Liu},
  doi          = {10.1109/TNNLS.2022.3231924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9238-9251},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial kinetic prototype framework for open set recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep subdomain learning adaptation network: A sensor
fault-tolerant soft sensor for industrial processes. <em>TNNLS</em>,
<em>35</em>(7), 9226–9237. (<a
href="https://doi.org/10.1109/TNNLS.2022.3231849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor faults are non-negligible issues for soft sensor modeling. However, existing deep learning-based soft sensors are fragile and sensitive when considering sensor faults. To improve the robustness against sensor faults, this article proposes a deep subdomain learning adaptation network (DSLAN) to develop a sensor fault-tolerant soft sensor, which is capable of handling both sensor degradation and sensor failure simultaneously. Primarily, domain adaptation works for process data with sensor degradation in industrial processes. Being founded on the basic structure of deep domain adaptation, a novel subdomain learner is added to automatically learn the subdomain division, enabling DSLAN adaptable to multimode industrial processes. Notably, the subdomain structure of each sample follows a categorical distribution parameterized by output of the subdomain learner. Based on the designed subdomain learner, a new probabilistic local maximum mean discrepancy (PLMMD) is presented to measure the difference in distribution between source and target features. In addition, a generator for failure data imputation is integrated in the framework, making DSLAN handle sensor failure simultaneously. Finally, the Tennessee Eastman (TE) benchmark process and two real industrial processes are used to verify the effectiveness of the proposed method. With the fault tolerance ability, soft sensing technology will take a step toward practical applications.},
  archive      = {J_TNNLS},
  author       = {Xiangrui Zhang and Chunyue Song and Jun Zhao and Zuhua Xu and Xiaogang Deng},
  doi          = {10.1109/TNNLS.2022.3231849},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9226-9237},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep subdomain learning adaptation network: A sensor fault-tolerant soft sensor for industrial processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LPOT: Locality-preserving gromov–wasserstein discrepancy for
nonrigid point set registration. <em>TNNLS</em>, <em>35</em>(7),
9213–9225. (<a
href="https://doi.org/10.1109/TNNLS.2022.3231652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main problems in point registration involve recovering correspondences and estimating transformations, especially in a fully unsupervised way without any feature descriptors. In this work, we propose a robust point matching method using discrete optimal transport (OT), which is a natural and useful approach for assignment tasks, to recover the underlying correspondences and improve the nonrigid registration in the presence of unknown global transformations. Specifically, we cast the registration problem as a joint estimation over local transport couplings and global transformations, observing that the local neighborhood topology structures should be preserved strongly and stably for nonrigid transformations. By solving the Gromov–Wasserstein discrepancy, a smooth assignment matrix from one point set to another can be recovered in a fully unsupervised way. Registration performance can be improved by applying an unsupervised map to guide the transformation estimate under the alternating optimization. Experimental results on several datasets reveal how the presented method is superior to the state-of-the-art methods when facing large data degradations.},
  archive      = {J_TNNLS},
  author       = {Gang Wang},
  doi          = {10.1109/TNNLS.2022.3231652},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9213-9225},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LPOT: Locality-preserving Gromov–Wasserstein discrepancy for nonrigid point set registration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable graph reservoir computing with the temporal
pattern attention. <em>TNNLS</em>, <em>35</em>(7), 9198–9212. (<a
href="https://doi.org/10.1109/TNNLS.2022.3231620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph reservoir computing (GraphRC) gains increasing attention by virtue of its high training efficiency. However, since GraphRC is developed without knowledge of its internal mechanism, it cannot be fully trusted to deploy in practice. Although there are some existing approaches that can be extended to interpret GraphRC, the specific role played by each neuron (i.e., reservoir node) of GraphRC is far less explored. To address this issue, the latent short-term memory property of each reservoir node of GraphRC is qualitatively characterized to unravel its role in predicting the graph signal, thereby enabling an interpretable GraphRC. Specifically, we first deduce the equivalence between the GraphRC and conventional reservoir computing (RC). Then, the underlying memory properties of the GraphRC and its reservoir nodes can be characterized in theory by the multisource reachability among the reservoir nodes in the transformed RC. Moreover, the distinct temporal patterns hidden in reservoir nodes are identified, and then, an attention mechanism based on the identified temporal patterns is deployed in the GraphRC to improve its performance. In addition, the effectiveness of the interpretability for GraphRC and improved GraphRC is verified on the Lorenz-96 spatiotemporal dynamical system. The experimental results of the Lorenz-96 spatiotemporal chaotic system and three real-world traffic datasets demonstrate that the improved GraphRC is superior to original GraphRC and can achieve prediction performance comparable to the state-of-the-art baseline models, but with much less training cost.},
  archive      = {J_TNNLS},
  author       = {Xinyu Han and Yi Zhao},
  doi          = {10.1109/TNNLS.2022.3231620},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9198-9212},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interpretable graph reservoir computing with the temporal pattern attention},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised tracking via target-aware data synthesis.
<em>TNNLS</em>, <em>35</em>(7), 9186–9197. (<a
href="https://doi.org/10.1109/TNNLS.2022.3231537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep-learning-based tracking methods have achieved substantial progress, they entail large-scale and high-quality annotated data for sufficient training. To eliminate expensive and exhaustive annotation, we study self-supervised (SS) learning for visual tracking. In this work, we develop the crop-transform-paste operation, which is able to synthesize sufficient training data by simulating various appearance variations during tracking, including appearance variations of objects and background interference. Since the target state is known in all synthesized data, existing deep trackers can be trained in routine ways using the synthesized data without human annotation. The proposed target-aware data-synthesis method adapts existing tracking approaches within a SS learning framework without algorithmic changes. Thus, the proposed SS learning mechanism can be seamlessly integrated into existing tracking frameworks to perform training. Extensive experiments show that our method: 1) achieves favorable performance against supervised (Su) learning schemes under the cases with limited annotations; 2) helps deal with various tracking challenges such as object deformation, occlusion (OCC), or background clutter (BC) due to its manipulability; 3) performs favorably against the state-of-the-art unsupervised tracking methods; and 4) boosts the performance of various state-of-the-art Su learning frameworks, including SiamRPN++, DiMP, and TransT.},
  archive      = {J_TNNLS},
  author       = {Xin Li and Wenjie Pei and Yaowei Wang and Zhenyu He and Huchuan Lu and Ming-Hsuan Yang},
  doi          = {10.1109/TNNLS.2022.3231537},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9186-9197},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised tracking via target-aware data synthesis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast clustering by directly solving bipartite graph
clustering problem. <em>TNNLS</em>, <em>35</em>(7), 9174–9185. (<a
href="https://doi.org/10.1109/TNNLS.2022.3219131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering (SC) has been widely used in many applications and shows excellent performance. Its high computational cost limits its applications; many strategies including the anchor technique can partly alleviate the high computational cost problem. However, early methods ignore the fact that SC usually involves two stages: relaxation and postprocessing, i.e., it relaxes the discrete constraints to continuous constraints, and then conducts the postprocessing to get the discrete solution, which is time-consuming and deviates from directly solving the primal problem. In this article, we first adopt the bipartite graph strategy to reduce the time complexity of SC, and then an improved coordinate descent (CD) method is proposed to solve the primal problem directly without singular value decomposition (SVD) and postprocessing, i.e., directly solving the primal problem not approximately solving. Experiments on various real-world benchmark datasets show that the proposed method can get better solutions faster with better clustering performance than traditional optimization methods. Furthermore, it can jump out of local minima of traditional methods and continue to obtain better local solutions. Moreover, compared with other clustering methods, it also shows its superiority.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Jingjing Xue and Rong Wang and Liang Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3219131},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9174-9185},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast clustering by directly solving bipartite graph clustering problem},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Defocus blur detection attack via mutual-referenced feature
transfer. <em>TNNLS</em>, <em>35</em>(7), 9162–9173. (<a
href="https://doi.org/10.1109/TNNLS.2022.3219059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from deep learning, defocus blur detection (DBD) has made prominent progress. Existing DBD methods generally study multiscale and multilevel features to improve performance. In this article, from a different perspective, we explore to generate confrontational images to attack DBD network. Based on the observation that defocus area and focus region in an image can provide mutual feature reference to help improve the quality of the confrontational image, we propose a novel mutual-referenced attack framework. Firstly, we design a divide-and-conquer perturbation image generation model, where the focus region attack image and defocus area attack image are generated respectively. Then, we integrate mutual-referenced feature transfer (MRFT) models to improve attack performance. Comprehensive experiments are provided to verify the effectiveness of our method. Moreover, related applications of our study are presented, e.g., sample augmentation to improve DBD and paired sample generation to boost defocus deblurring.},
  archive      = {J_TNNLS},
  author       = {Wenda Zhao and Mingyue Wang and Fei Wei and Haipeng Wang and You He and Huchuan Lu},
  doi          = {10.1109/TNNLS.2022.3219059},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9162-9173},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Defocus blur detection attack via mutual-referenced feature transfer},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TGIN: Translation-based graph inference network for few-shot
relational triplet extraction. <em>TNNLS</em>, <em>35</em>(7),
9147–9161. (<a
href="https://doi.org/10.1109/TNNLS.2022.3218981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting relational triplets aims at detecting entity pairs and their semantic relations. Compared with pipeline models, joint models can reduce error propagation and achieve better performance. However, all of these models require large amounts of training data, therefore performing poorly on many long-tail relations in reality with insufficient data. In this article, we propose a novel end-to-end model, called TGIN, for few-shot triplet extraction. The core of TGIN is a multilayer heterogeneous graph with two types of nodes (entity node and relation node) and three types of edges (relation–entity edge, entity–entity edge, and relation–relation edge). On the one hand, this heterogeneous graph with entities and relations as nodes can intuitively extract relational triplets jointly, thereby reducing error propagation. On the other hand, it enables the triplet information of limited labeled data to interact better, thus maximizing the advantage of this information for few-shot triplet extraction. Moreover, we devise a graph aggregation and update method that utilizes translation algebraic operations to mine semantic features while retaining structure features between entities and relations, thereby improving the robustness of the TGIN in a few-shot setting. After updating the node and edge features through layers, TGIN propagates the label information from a few labeled examples to unlabeled examples, thus inferring triplets from these unlabeled examples. Extensive experiments on three reconstructed datasets demonstrate that TGIN can significantly improve the accuracy of triplet extraction by 2.34%~10.74% compared with the state-of-the-art baselines. To the best of our knowledge, we are the first to introduce a heterogeneous graph for few-shot relational triplet extraction.},
  archive      = {J_TNNLS},
  author       = {Jiaxin Wang and Lingling Zhang and Jun Liu and Kunming Ma and Wenjun Wu and Xiang Zhao and Yaqiang Wu and Yi Huang},
  doi          = {10.1109/TNNLS.2022.3218981},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9147-9161},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TGIN: Translation-based graph inference network for few-shot relational triplet extraction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph–graph similarity network. <em>TNNLS</em>,
<em>35</em>(7), 9136–9146. (<a
href="https://doi.org/10.1109/TNNLS.2022.3218936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph learning aims to predict the label for an entire graph. Recently, graph neural network (GNN)-based approaches become an essential strand to learning low-dimensional continuous embeddings of entire graphs for graph label prediction. While GNNs explicitly aggregate the neighborhood information and implicitly capture the topological structure for graph representation, they ignore the relationships among graphs. In this article, we propose a graph–graph (G2G) similarity network to tackle the graph learning problem by constructing a SuperGraph through learning the relationships among graphs. Each node in the SuperGraph represents an input graph, and the weights of edges denote the similarity between graphs. By this means, the graph learning task is then transformed into a classical node label propagation problem. Specifically, we use an adversarial autoencoder to align embeddings of all the graphs to a prior data distribution. After the alignment, we design the G2G similarity network to learn the similarity between graphs, which functions as the adjacency matrix of the SuperGraph. By running node label propagation algorithms on the SuperGraph, we can predict the labels of graphs. Experiments on five widely used classification benchmarks and four public regression benchmarks under a fair setting demonstrate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Han Yue and Pengyu Hong and Hongfu Liu},
  doi          = {10.1109/TNNLS.2022.3218936},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9136-9146},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph–Graph similarity network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Marginal subspace learning with group low-rank for
unsupervised domain adaptation. <em>TNNLS</em>, <em>35</em>(7),
9122–9135. (<a
href="https://doi.org/10.1109/TNNLS.2022.3218554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation is intended to construct a reliable model for the unlabeled target samples using the well-labeled but differently distributed source samples. To tackle the domain shift issue, learning domain-invariant feature representations across domains is important, and most of the existing methods have concentrated on this goal. However, these methods rarely take into consideration the group discriminability of the feature representation, which is detrimental to the final recognition. Therefore, this article proposes a novel unsupervised domain adaptation method, named marginal subspace learning with group low-rank (MSL-GLR), to extract both domain-invariant and discriminative feature representations. Specifically, MSL-GLR uses the retargeting strategy to relax the regression matrix, such that the regression values would be forced to satisfy a margin maximization criterion for the requirement of correct classification. Moreover, MSL-GLR imposes a class-induced low-rank constraint, which enables the samples of each class to be located in their respective subspace. In this way, the distance between samples from the same class can be decreased and the discriminant ability of the projection is greatly improved. Furthermore, with the help of alternating direction method of multipliers (ADMM), an efficient algorithm is presented to solve the resulting optimization problem. Finally, the effectiveness of the proposed MSL-GLR is demonstrated by comprehensive evaluations on multiple domain adaptation benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Liran Yang and Qinghua Zhou and Bin Lu},
  doi          = {10.1109/TNNLS.2022.3218554},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9122-9135},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Marginal subspace learning with group low-rank for unsupervised domain adaptation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TS-CAM: Token semantic coupled attention map for weakly
supervised object localization. <em>TNNLS</em>, <em>35</em>(7),
9109–9121. (<a
href="https://doi.org/10.1109/TNNLS.2022.3218471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object localization (WSOL), which trains object localization models using solely image category annotations, remains a challenging problem. Existing approaches based on convolutional neural networks (CNNs) tend to miss full object extent while activating discriminative object parts. Based on our analysis, this is caused by CNN’s intrinsic characteristics, which experiences difficulty to capture object semantics at long distances. In this article, we introduce the vision transformer to WSOL, with the aim to capture long-range semantic dependency of features by leveraging transformer’s cascaded self-attention mechanism. We propose the token semantic coupled attention map (TS-CAM) method, which first decomposes class-aware semantics and then couples the semantics with attention maps for semantic-aware activation. To capture object semantics at long distances and avoid partial activation, TS-CAM performs spatial embedding by partitioning an image to a set of patch tokens. To incorporate object category information to patch tokens, TS-CAM reallocates category-related semantics to each patch token. The patch tokens are finally coupled with attention maps which are semantic-agnostic to perform semantic-aware object localization. By introducing semantic tokens to produce semantic-aware attention maps, we further explore the capability of TS-CAM for multicategory object localization. Experiments show that TS-CAM outperforms its CNN-CAM counterpart by 11.6% and 28.9% on ILSVRC and CUB-200-2011 datasets, respectively, improving the state-of-the-art with large margins. TS-CAM also demonstrates superiority for multicategory object localization on the Pascal VOC dataset. The code is available at github.com/yuanyao366/ts-cam-extension.},
  archive      = {J_TNNLS},
  author       = {Yuan Yao and Fang Wan and Wei Gao and Xingjia Pan and Zhiliang Peng and Qi Tian and Qixiang Ye},
  doi          = {10.1109/TNNLS.2022.3218471},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9109-9121},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TS-CAM: Token semantic coupled attention map for weakly supervised object localization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast convergent antinoise dual neural network controller
with adaptive gain for flexible endoscope robots. <em>TNNLS</em>,
<em>35</em>(7), 9095–9108. (<a
href="https://doi.org/10.1109/TNNLS.2022.3218461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manual rigid endoscopes have defects such as a low efficiency, difficult operation, and safety risks, and the antinoise interference ability, convergence speed, and control accuracy of the neural network control technology for the existing autonomous endoscopes are often ignored. Solving these problems is important for the stable operation of endoscopes. Therefore, a new adaptive fast convergent antinoise dual neural network (AFA-DNN) controller for the visual servo control of ten-degree of freedom flexible endoscope robots (FERs) with physical constraints is proposed in this work. First, the control scheme of the FERs is formulated as a quadratic programming problem, and then, an AFA-DNN visual servo controller is designed for the FERs. The adaptive gains of the controller can accelerate the convergence, improve the antinoise ability, and increase the convergence accuracy of the controller. Then, according to the Lyapunov theory, the fast convergence of the AFA-DNN in finite time is proven for both noise-free and noisy conditions. The experimental results indicate that the FER controlled by the proposed AFA-DNN can accurately track various trajectories and that the AFA-DNN has a better antinoise interference ability, higher convergence accuracy, and faster convergence speed than conventional methods. The convergence speed of the AFA-DNN is increased by a factor of 4.22 by using the adaptive gains. Experiments also indicate that the AFA-DNN remains well functioning under various noise disturbances (such as constant, periodic, linear, and Gaussian noise).},
  archive      = {J_TNNLS},
  author       = {Zhiwei Cui and Jixiu Li and Weibing Li and Xue Zhang and Philip Wai Yan Chiu and Zheng Li},
  doi          = {10.1109/TNNLS.2022.3218461},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9095-9108},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast convergent antinoise dual neural network controller with adaptive gain for flexible endoscope robots},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multipattern mining using pattern-level contrastive learning
and multipattern activation map. <em>TNNLS</em>, <em>35</em>(7),
9080–9094. (<a
href="https://doi.org/10.1109/TNNLS.2022.3218073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual patterns are basic elements in images and represent the discernible regularity in the visual world. Thus, mining visual patterns is a fundamental task in computer vision. Most previous studies consider that only one visual pattern exists in a category, and then builds up a one-to-one mapping using category label. In reality, however, many categories include multiple patterns, which are many-to-one mappings. Without knowing the information of patterns, few existing pattern mining methods can discover and distinguish varied patterns in a category. To tackle this problem, we propose a novel framework, PaclMap, which learns medium-grained features to represent patterns. It includes an unsupervised pattern-level contrastive learning and a multipattern activation map. Their joint optimization encourages the network to mine both discriminative and frequent patterns in a category. Extensive experiments conducted on four benchmark datasets (Place-20, imagenet large scale visual recognition challenge (ILSVRC)-20, visual object classes (VOC), and Travel) demonstrate that PaclMap outperforms six state-of-the-art methods with average improvements of 2.9% on accuracy and 12.3% on frequency, respectively.},
  archive      = {J_TNNLS},
  author       = {Xuefeng Liang and Zhihui Liang and Huiwen Shi and Xiaosong Zhang and Ying Zhou and Yifan Ma},
  doi          = {10.1109/TNNLS.2022.3218073},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9080-9094},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multipattern mining using pattern-level contrastive learning and multipattern activation map},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel learning-based trajectory generation strategy for a
quadrotor. <em>TNNLS</em>, <em>35</em>(7), 9068–9079. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a learning-based trajectory generation framework is proposed for quadrotors, which guarantees real-time, efficient, and practice-reliable navigation by online making human-like decisions via reinforcement learning (RL) and imitation learning (IL). Specifically, inspired by human driving behavior and the perception range of sensors, a real-time local planner is designed by combining learning and optimization techniques, where the smooth and flexible trajectories are online planned efficiently in the observable area. In particular, the key problems in the framework, temporal optimality (time allocation), and spatial optimality (trajectory distribution) are solved by designing an RL policy, which provides human-like commands in real-time (e.g., slower or faster) to achieve better navigation, instead of generating traditional low-level motions. In this manner, real-time trajectories are calculated using convex optimization according to the efficient and accurate decisions of the RL policy. In addition, to improve generalization performance and to accelerate the training, an expert policy and IL are employed in the framework. Compared with existing works, the kernel contribution is to design a real-time practice-oriented intelligent trajectory generation framework for quadrotors, where human-like decision-making and model-based optimization are integrated to plan high-quality trajectories. The results of comparative experiments in known and unknown environments illustrate the superior performance of the proposed trajectory generation strategy in terms of efficiency, smoothness, and flexibility.},
  archive      = {J_TNNLS},
  author       = {Hean Hua and Yongchun Fang},
  doi          = {10.1109/TNNLS.2022.3217814},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9068-9079},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel learning-based trajectory generation strategy for a quadrotor},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking pretraining as a bridge from ANNs to SNNs.
<em>TNNLS</em>, <em>35</em>(7), 9054–9067. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are known as typical kinds of brain-inspired models with their unique features of rich neuronal dynamics, diverse coding schemes, and low power consumption properties. How to obtain a high-accuracy model has always been the main challenge in the field of SNN. Currently, there are two mainstream methods, i.e., obtaining a converted SNN through converting a well-trained artificial NN (ANN) to its SNN counterpart or training an SNN directly. However, the inference time of a converted SNN is too long, while SNN training is generally very costly and inefficient. In this work, a new SNN training paradigm is proposed by combining the concepts of the two different training methods with the help of the pretrain technique and BP-based deep SNN training mechanism. We believe that the proposed paradigm is a more efficient pipeline for training SNNs. The pipeline includes pipe-S for static data transfer tasks and pipe-D for dynamic data transfer tasks. State-of-the-art (SOTA) results are obtained in a large-scale event-driven dataset ES-ImageNet. For training acceleration, we achieve the same (or higher) best accuracy as similar leaky-integrate-and-fire (LIF)-SNNs using 1/8 training time on ImageNet-1K and 1/2 training time on ES-ImageNet and also provide a time-accuracy benchmark for a new dataset ES-UCF101. These experimental results reveal the similarity of the functions of parameters between ANNs and SNNs and also demonstrate various potential applications of this SNN training pipeline.},
  archive      = {J_TNNLS},
  author       = {Yihan Lin and Yifan Hu and Shijie Ma and Dongjie Yu and Guoqi Li},
  doi          = {10.1109/TNNLS.2022.3217796},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9054-9067},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rethinking pretraining as a bridge from ANNs to SNNs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Distributed fault tolerant consensus control of nonlinear
multiagent systems via adaptive dynamic programming. <em>TNNLS</em>,
<em>35</em>(7), 9041–9053. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a distributed fault-tolerant consensus control (DFTCC) approach for multiagent systems by using adaptive dynamic programming. By establishing a local fault observer, the potential actuator faults of each agent are estimated. Subsequently, the DFTCC problem is transformed into an optimal consensus control problem by designing a novel local value function for each agent which contains the estimated fault, the consensus errors, and the control laws of the local agent and its neighbors. In order to solve the coupled Hamilton–Jacobi–Bellman equation of each agent, a critic-only structure is established to obtain the approximate local optimal consensus control law of each agent. Moreover, by using Lyapunov’s direct method, it is proven that the approximate local optimal consensus control law guarantees the uniform ultimate boundedness of the consensus error of all agents, which means that all following agents with potential actuator faults synchronize to the leader. Finally, two simulation examples are provided to validate the effectiveness of the present DFTCC scheme.},
  archive      = {J_TNNLS},
  author       = {Yongwei Zhang and Bo Zhao and Derong Liu and Shunchao Zhang},
  doi          = {10.1109/TNNLS.2022.3217774},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9041-9053},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed fault tolerant consensus control of nonlinear multiagent systems via adaptive dynamic programming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view multi-label fine-grained emotion decoding from
human brain activity. <em>TNNLS</em>, <em>35</em>(7), 9026–9040. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoding emotional states from human brain activity play an important role in the brain–computer interfaces. Existing emotion decoding methods still have two main limitations: one is only decoding a single emotion category from a brain activity pattern and the decoded emotion categories are coarse-grained, which is inconsistent with the complex emotional expression of humans; the other is ignoring the discrepancy of emotion expression between the left and right hemispheres of the human brain. In this article, we propose a novel multi-view multi-label hybrid model for fine-grained emotion decoding (up to 80 emotion categories) which can learn the expressive neural representations and predict multiple emotional states simultaneously. Specifically, the generative component of our hybrid model is parameterized by a multi-view variational autoencoder, in which we regard the brain activity of left and right hemispheres and their difference as three distinct views and use the product of expert mechanism in its inference network. The discriminative component of our hybrid model is implemented by a multi-label classification network with an asymmetric focal loss. For more accurate emotion decoding, we first adopt a label-aware module for emotion-specific neural representation learning and then model the dependency of emotional states by a masked self-attention mechanism. Extensive experiments on two visually evoked emotional datasets show the superiority of our method.},
  archive      = {J_TNNLS},
  author       = {Kaicheng Fu and Changde Du and Shengpei Wang and Huiguang He},
  doi          = {10.1109/TNNLS.2022.3217767},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9026-9040},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-view multi-label fine-grained emotion decoding from human brain activity},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble conformalized quantile regression for probabilistic
time series forecasting. <em>TNNLS</em>, <em>35</em>(7), 9014–9025. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel probabilistic forecasting method called ensemble conformalized quantile regression (EnCQR). EnCQR constructs distribution-free and approximately marginally valid prediction intervals (PIs), which are suitable for nonstationary and heteroscedastic time series data. EnCQR can be applied on top of a generic forecasting model, including deep learning architectures. EnCQR exploits a bootstrap ensemble estimator, which enables the use of conformal predictors for time series by removing the requirement of data exchangeability. The ensemble learners are implemented as generic machine learning algorithms performing quantile regression (QR), which allow the length of the PIs to adapt to local variability in the data. In the experiments, we predict time series characterized by a different amount of heteroscedasticity. The results demonstrate that EnCQR outperforms models based only on QR or conformal prediction (CP), and it provides sharper, more informative, and valid PIs.},
  archive      = {J_TNNLS},
  author       = {Vilde Jensen and Filippo Maria Bianchi and Stian Normann Anfinsen},
  doi          = {10.1109/TNNLS.2022.3217694},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {9014-9025},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ensemble conformalized quantile regression for probabilistic time series forecasting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Power law in deep neural networks: Sparse network generation
and continual learning with preferential attachment. <em>TNNLS</em>,
<em>35</em>(7), 8999–9013. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep neural networks (DNNs) typically requires massive computational power. Existing DNNs exhibit low time and storage efficiency due to the high degree of redundancy. In contrast to most existing DNNs, biological and social networks with vast numbers of connections are highly efficient and exhibit scale-free properties indicative of the power law distribution, which can be originated by preferential attachment in growing networks. In this work, we ask whether the topology of the best performing DNNs shows the power law similar to biological and social networks and how to use the power law topology to construct well-performing and compact DNNs. We first find that the connectivities of sparse DNNs can be modeled by truncated power law distribution, which is one of the variations of the power law. The comparison of different DNNs reveals that the best performing networks correlated highly with the power law distribution. We further model the preferential attachment in DNNs evolution and find that continual learning in networks with growth in tasks correlates with the process of preferential attachment. These identified power law dynamics in DNNs can lead to the construction of highly accurate and compact DNNs based on preferential attachment. Inspired by the discovered findings, two novel applications have been proposed, including evolving optimal DNNs in sparse network generation and continual learning tasks with efficient network growth using power law dynamics. Experimental results indicate that the proposed applications can speed up training, save storage, and learn with fewer samples than other well-established baselines. Our demonstration of preferential attachment and power law in well-performing DNNs offers insight into designing and constructing more efficient deep learning.},
  archive      = {J_TNNLS},
  author       = {Fan Feng and Lu Hou and Qi She and Rosa H. M. Chan and James T. Kwok},
  doi          = {10.1109/TNNLS.2022.3217403},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8999-9013},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Power law in deep neural networks: Sparse network generation and continual learning with preferential attachment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning lightweight dynamic kernels with attention inside
via local–global context fusion. <em>TNNLS</em>, <em>35</em>(7),
8984–8998. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional convolutional neural networks (CNNs) share their kernels among all positions of the input, which may constrain the representation ability in feature extraction. Dynamic convolution proposes to generate different kernels for different inputs to improve the model capacity. However, the total parameters of the dynamic network can be significantly huge. In this article, we propose a lightweight dynamic convolution method to strengthen traditional CNNs with an affordable increase of total parameters and multiply–adds. Instead of generating the whole kernels directly or combining several static kernels, we choose to “look inside,” learning the attention within convolutional kernels. An extra network is used to adjust the weights of kernels for every feature aggregation operation. By combining local and global contexts, the proposed approach can capture the variance among different samples, the variance in different positions of the feature maps, and the variance in different positions inside sliding windows. With a minor increase in the number of model parameters, remarkable improvements in image classification on CIFAR and ImageNet with multiple backbones have been obtained. Experiments on object detection also verify the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Yonglin Tian and Yu Shen and Xiao Wang and Jiangong Wang and Kunfeng Wang and Weiping Ding and Zilei Wang and Fei-Yue Wang},
  doi          = {10.1109/TNNLS.2022.3217301},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8984-8998},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning lightweight dynamic kernels with attention inside via Local–Global context fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoNoT: Coupled nonlinear transform-based low-rank tensor
representation for multidimensional image completion. <em>TNNLS</em>,
<em>35</em>(7), 8969–8983. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the transform-based tensor nuclear norm (TNN) methods have shown promising performance and drawn increasing attention in tensor completion (TC) problems. The main idea of these methods is to exploit the low-rank structure of frontal slices of the tensor under the transform. However, the transforms in TNN methods usually treat all modes equally and do not consider the different traits of different modes (i.e., spatial and spectral/temporal modes). To address this problem, we suggest a new low-rank tensor representation based on the coupled nonlinear transform (called CoNoT) for a better low-rank approximation. Concretely, spatial and spectral/temporal transforms in the CoNoT, respectively, exploit the different traits of different modes and are coupled together to boost the implicit low-rank structure. Here, we use the convolutional neural network (CNN) as the CoNoT, which can be learned solely from an observed multidimensional image in an unsupervised manner. Based on this low-rank tensor representation, we build a new multidimensional image completion model. Moreover, we also propose an enhanced version (called Ms-CoNoT) to further exploit the spatial multiscale nature of real-world data. Extensive experiments on real-world data substantiate the superiority of the proposed models against many state-of-the-art methods both qualitatively and quantitatively.},
  archive      = {J_TNNLS},
  author       = {Jian-Li Wang and Ting-Zhu Huang and Xi-Le Zhao and Yi-Si Luo and Tai-Xiang Jiang},
  doi          = {10.1109/TNNLS.2022.3217198},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8969-8983},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CoNoT: Coupled nonlinear transform-based low-rank tensor representation for multidimensional image completion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monotonic quantile network for worst-case offline
reinforcement learning. <em>TNNLS</em>, <em>35</em>(7), 8954–8968. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in offline reinforcement learning (RL) is how to ensure the learned offline policy is safe, especially in safety-critical domains. In this article, we focus on learning a distributional value function in offline RL and optimizing a worst-case criterion of returns. However, optimizing a distributional value function in offline RL can be hard, since the crossing quantile issue is serious, and the distribution shift problem needs to be addressed. To this end, we propose monotonic quantile network (MQN) with conservative quantile regression (CQR) for risk-averse policy learning. First, we propose an MQN to learn the distribution over returns with non-crossing guarantees of the quantiles. Then, we perform CQR by penalizing the quantile estimation for out-of-distribution (OOD) actions to address the distribution shift in offline RL. Finally, we learn a worst-case policy by optimizing the conditional value-at-risk (CVaR) of the distributional value function. Furthermore, we provide theoretical analysis of the fixed-point convergence in our method. We conduct experiments in both risk-neutral and risk-sensitive offline settings, and the results show that our method obtains safe and conservative behaviors in robotic locomotion tasks.},
  archive      = {J_TNNLS},
  author       = {Chenjia Bai and Ting Xiao and Zhoufan Zhu and Lingxiao Wang and Fan Zhou and Animesh Garg and Bin He and Peng Liu and Zhaoran Wang},
  doi          = {10.1109/TNNLS.2022.3217189},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8954-8968},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Monotonic quantile network for worst-case offline reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional probabilistic subspaces approximation for
multiview clustering. <em>TNNLS</em>, <em>35</em>(7), 8939–8953. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing multiview clustering models learn a consistent low-dimensional embedding either from multiple feature matrices or multiple similarity matrices, which ignores the interaction between the two procedures and limits the improvement of clustering performance on multiview data. To address this issue, a bidirectional probabilistic subspaces approximation (BPSA) model is developed in this article to learn a consistently orthogonal embedding from multiple feature matrices and multiple similarity matrices simultaneously via the disturbed probabilistic subspace modeling and approximation. A skillful bidirectional fusion strategy is designed to guarantee the parameter-free property of the BPSA model. Two adaptively weighted learning mechanisms are introduced to ensure the inconsistencies among multiple views and the inconsistencies between bidirectional learning processes. To solve the optimization problem involved in the BPSA model, an iterative solver is derived, and a rigorous convergence guarantee is provided. Extensive experimental results on both toy and real-world datasets demonstrate that our BPSA model achieves state-of-the-art performance even if it is parameter-free.},
  archive      = {J_TNNLS},
  author       = {Danyang Wu and Xia Dong and Jianfu Cao and Rong Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3217032},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8939-8953},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bidirectional probabilistic subspaces approximation for multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). SPD manifold deep metric learning for image set
classification. <em>TNNLS</em>, <em>35</em>(7), 8924–8938. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By characterizing each image set as a nonsingular covariance matrix on the symmetric positive definite (SPD) manifold, the approaches of visual content classification with image sets have made impressive progress. However, the key challenge of unhelpfully large intraclass variability and interclass similarity of representations remains open to date. Although, several recent studies have mitigated the two problems by jointly learning the embedding mapping and the similarity metric on the original SPD manifold, their inherent shallow and linear feature transformation mechanism are not powerful enough to capture useful geometric features, especially in complex scenarios. To this end, this article explores a novel approach, termed SPD manifold deep metric learning (SMDML), for image set classification. Specifically, SMDML first selects a prevailing SPD manifold neural network (SPDNet) as the backbone (encoder) to derive an SPD matrix nonlinear representation. To counteract the degradation of structural information during multistage feature embedding, we construct a Riemannian decoder at the end of the encoder, trained by a reconstruction error term (RT), to induce the generated low-dimensional feature manifold of the hidden layer to capture the pivotal information about the visual data describing the imaged scene. We demonstrate through theory and experiments that it is feasible to replace the Riemannian metric with Euclidean distance in RT. Then, the ReCov layer is introduced into the established Riemannian network to regularize the local statistical information within each input feature matrix, which enhances the effectiveness of the learning process. The theoretical analysis of the activation function used in the ReCov layer in terms of continuity and conditions for generating positive definite matrices is beneficial for network design. Inspired by the fact that the single cross-entropy loss used for training is unable to effectively parse the geometric distribution of the deep representations, we finally endow the suggested model with a novel metric learning regularization term. By explicitly incorporating the encoding and processing of the data variations into the network learning process, this term can not only derive a powerful Riemannian representation but also train an effective classifier. The experimental results show the superiority of the proposed approach on three typical visual classification tasks.},
  archive      = {J_TNNLS},
  author       = {Rui Wang and Xiao-Jun Wu and Ziheng Chen and Cong Hu and Josef Kittler},
  doi          = {10.1109/TNNLS.2022.3216811},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8924-8938},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SPD manifold deep metric learning for image set classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tree broad learning system for small data modeling.
<em>TNNLS</em>, <em>35</em>(7), 8909–8923. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broad learning system based on neural network (BLS-NN) has poor efficiency for small data modeling with various dimensions. Tree-based BLS (TBLS) is designed for small data modeling by introducing nondifferentiable modules and an ensemble strategy to the traditional broad learning system (BLS). TBLS replaces the neurons of BLS with the tree modules to map the input data. Moreover, we present three new TBLS variant methods and their incremental learning implementations, which are motivated by deep, broad, and ensemble learning. Their major distinction is reflected in the incremental learning strategies based on: 1) mean square error (mse); 2) pseudo-inverse; and 3) pseudo-inverse theory and stack representation. Therefore, this study further explores the domain of BLS based on the nondifferentiable modules. The simulations are compared with some state-of-the-art (SOTA) BLS-NN and tree methods under high-, medium-, and low-dimensional benchmark datasets. Results show that the proposed method outperforms the BLS-NN, and the modeling accuracy is remarkably improved with the small training data of the proposed TBLS.},
  archive      = {J_TNNLS},
  author       = {Heng Xia and Jian Tang and Wen Yu and Junfei Qiao},
  doi          = {10.1109/TNNLS.2022.3216788},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8909-8923},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tree broad learning system for small data modeling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single finger trajectory prediction from intracranial brain
activity using block-term tensor regression with fast and automatic
component extraction. <em>TNNLS</em>, <em>35</em>(7), 8897–8908. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiway- or tensor-based decoding techniques for brain–computer interfaces (BCIs) are believed to better account for the multilinear structure of brain signals than conventional vector- or matrix-based ones. However, despite their outlook on significant performance gains, the used parameter optimization approach is often too computationally demanding so that conventional techniques are still preferred. We propose two novel tensor factorizations which we integrate into our block-term tensor regression (BTTR) algorithm and further introduce a marginalization procedure that guarantees robust predictions while reducing the risk of overfitting (generalized regression). BTTR accounts for the underlying (hidden) data structure in a fully automatic and computationally efficient manner, leading to a significant performance gain over conventional vector- or matrix-based techniques in a challenging real-world application. As a challenging real-world application, we apply BTTR to accurately predict single finger movement trajectories from intracranial recordings in human subjects. We compare the obtained performance with that of the state-of-the-art.},
  archive      = {J_TNNLS},
  author       = {Axel Faes and Flavio Camarrone and Marc M. Van Hulle},
  doi          = {10.1109/TNNLS.2022.3216589},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8897-8908},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Single finger trajectory prediction from intracranial brain activity using block-term tensor regression with fast and automatic component extraction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward graph self-supervised learning with contrastive
adjusted zooming. <em>TNNLS</em>, <em>35</em>(7), 8882–8896. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning (GRL) is critical for graph-structured data analysis. However, most of the existing graph neural networks (GNNs) heavily rely on labeling information, which is normally expensive to obtain in the real world. Although some existing works aim to effectively learn graph representations in an unsupervised manner, they suffer from certain limitations, such as the heavy reliance on monotone contrastiveness and limited scalability. To overcome the aforementioned problems, in light of the recent advancements in graph contrastive learning, we introduce a novel self-supervised GRL algorithm via graph contrastive adjusted zooming, namely, G-Zoom, to learn node representations by leveraging the proposed adjusted zooming scheme. Specifically, this mechanism enables G-Zoom to explore and extract self-supervision signals from a graph from multiple scales: micro (i.e., node level), meso (i.e., neighborhood level), and macro (i.e., subgraph level). First, we generate two augmented views of the input graph via two different graph augmentations. Then, we establish three different contrastiveness on the above three scales progressively, from node, neighboring, to subgraph level, where we maximize the agreement between graph representations across scales. While we can extract valuable clues from a given graph on the micro and macro perspectives, the neighboring-level contrastiveness offers G-Zoom the capability of a customizable option based on our adjusted zooming scheme to manually choose an optimal viewpoint that lies between the micro and macro perspectives to better understand the graph data. In addition, to make our model scalable to large graphs, we use a parallel graph diffusion approach to decouple model training from the graph size. We have conducted extensive experiments on real-world datasets, and the results demonstrate that our proposed model outperforms the state-of-the-art methods consistently.},
  archive      = {J_TNNLS},
  author       = {Yizhen Zheng and Ming Jin and Shirui Pan and Yuan-Fang Li and Hao Peng and Ming Li and Zhao Li},
  doi          = {10.1109/TNNLS.2022.3216630},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8882-8896},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward graph self-supervised learning with contrastive adjusted zooming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Versatile graph neural networks toward intuitive human
activity understanding. <em>TNNLS</em>, <em>35</em>(7), 8869–8881. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the advanced human visual system, humans naturally classify activities and predict motions in a short time. However, most existing computer vision studies consider those two tasks separately, resulting in an insufficient understanding of human actions. Moreover, the effects of view variations remain challenging for most existing skeleton-based methods, and the existing graph operators cannot fully explore multiscale relationship. In this article, a versatile graph-based model (Vers-GNN) is proposed to deal with those two tasks simultaneously. First, a skeleton representation self-regulated scheme is proposed. It is among the first trials that successfully integrate the idea of view adaptation into a graph-based human activity analysis system. Next, several novel graph operators are proposed to model the positional relationships and learn the abstract dynamics between different human joints and parts. Finally, a practical multitask learning framework and a multiobjective self-supervised learning scheme are proposed to promote both the tasks. The comparative experimental results show that Vers-GNN outperforms the recent state-of-the-art methods for both the tasks, with the to date highest recognition accuracies on the datasets of NTU RGB + D (CV: 97.2%), UWA3D (88.7%), and CMU (1000 ms: 1.13).},
  archive      = {J_TNNLS},
  author       = {Jiahui Yu and Yingke Xu and Hang Chen and Zhaojie Ju},
  doi          = {10.1109/TNNLS.2022.3216084},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8869-8881},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Versatile graph neural networks toward intuitive human activity understanding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully deformable network for multiview face image synthesis.
<em>TNNLS</em>, <em>35</em>(7), 8854–8868. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photorealistic multiview face synthesis from a single image is a challenging problem. Existing works mainly learn a texture mapping model from the source to the target faces. However, they rarely consider the geometric constraints on the internal deformation arising from pose variations, which causes a high level of uncertainty in face pose modeling, and hence, produces inferior results for large pose variations. Moreover, current methods typically suffer from undesired facial details loss due to the adoption of the de-facto standard encoder–decoder architecture without any skip connections (SCs). In this article, we directly learn and exploit geometric constraints and propose a fully deformable network to simultaneously model the deformations of both landmarks and faces for face synthesis. Specifically, our model consists of two parts: a deformable landmark learning network (DLLN) and a gated deformable face synthesis network (GDFSN). The DLLN converts an initial reference landmark to an individual-specific target landmark as delicate pose guidance for face rotation. The GDFSN adopts a dual-stream structure, with one stream estimating the deformation of two views in the form of convolution offsets according to the source pose and the converted target pose, and the other leveraging the predicted deformation offsets to create the target face. In this way, individual-aware pose changes are explicitly modeled in the face generator to cope with geometric transformation, by adaptively focusing on pertinent regions of the source face. To compensate for offset estimation errors, we introduce a soft-gating mechanism for adaptive fusion between deformable features and primitive features. Additionally, a pose-aligned SC (PASC) is tailored to propagate low-level input features to the appropriate positions in the output features for further enhancing the facial details and identity preservation. Extensive experiments on six benchmarks show that our approach performs favorably against the state-of-the-arts, especially with large pose changes. Code is available at https://github.com/cschengxu/FDFace .},
  archive      = {J_TNNLS},
  author       = {Cheng Xu and Keke Li and Xuandi Luo and Xuemiao Xu and Shengfeng He and Kun Zhang},
  doi          = {10.1109/TNNLS.2022.3216018},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8854-8868},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully deformable network for multiview face image synthesis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust corrupted data recovery and clustering via
generalized transformed tensor low-rank representation. <em>TNNLS</em>,
<em>35</em>(7), 8839–8853. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor analysis has received widespread attention in high-dimensional data learning. Unfortunately, the tensor data are often accompanied by arbitrary signal corruptions, including missing entries and sparse noise. How to recover the characteristics of the corrupted tensor data and make it compatible with the downstream clustering task remains a challenging problem. In this article, we study a generalized transformed tensor low-rank representation (TTLRR) model for simultaneously recovering and clustering the corrupted tensor data. The core idea is to find the latent low-rank tensor structure from the corrupted measurements using the transformed tensor singular value decomposition (SVD). Theoretically, we prove that TTLRR can recover the clean tensor data with a high probability guarantee under mild conditions. Furthermore, by using the transform adaptively learning from the data itself, the proposed TTLRR model can approximately represent and exploit the intrinsic subspace and seek out the cluster structure of the tensor data precisely. An effective algorithm is designed to solve the proposed model under the alternating direction method of multipliers (ADMMs) algorithm framework. The effectiveness and superiority of the proposed method against the compared methods are showcased over different tasks, including video/face data recovery and face/object/scene data clustering.},
  archive      = {J_TNNLS},
  author       = {Jing-Hua Yang and Chuan Chen and Hong-Ning Dai and Meng Ding and Zhe-Bin Wu and Zibin Zheng},
  doi          = {10.1109/TNNLS.2022.3215983},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8839-8853},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust corrupted data recovery and clustering via generalized transformed tensor low-rank representation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-rank tensor completion based on self-adaptive learnable
transforms. <em>TNNLS</em>, <em>35</em>(7), 8826–8838. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tensor nuclear norm (TNN), defined as the sum of nuclear norms of frontal slices of the tensor in a frequency domain, has been found useful in solving low-rank tensor recovery problems. Existing TNN-based methods use either fixed or data-independent transformations, which may not be the optimal choices for the given tensors. As the consequence, these methods cannot exploit the potential low-rank structure of tensor data adaptively. In this article, we propose a framework called self-adaptive learnable transform (SALT) to learn a transformation matrix from the given tensor. Specifically, SALT aims to learn a lossless transformation that induces a lower average-rank tensor, where the Schatten- $p$ quasi-norm is used as the rank proxy. Then, because SALT is less sensitive to the orientation, we generalize SALT to other dimensions of tensor (SALTS), namely, learning three self-adaptive transformation matrices simultaneously from given tensor. SALTS is able to adaptively exploit the potential low-rank structures in all directions. We provide a unified optimization framework based on alternating direction multiplier method for SALTS model and theoretically prove the weak convergence property of the proposed algorithm. Experimental results in hyperspectral image (HSI), color video, magnetic resonance imaging (MRI), and COIL-20 datasets show that SALTS is much more accurate in tensor completion than existing methods. The demo code can be found at https://faculty.uestc.edu.cn/gaobin/zh_CN/lwcg/153392/list/index.htm .},
  archive      = {J_TNNLS},
  author       = {Tongle Wu and Bin Gao and Jicong Fan and Jize Xue and W. L. Woo},
  doi          = {10.1109/TNNLS.2022.3215974},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8826-8838},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low-rank tensor completion based on self-adaptive learnable transforms},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic-horizon model-based value estimation with latent
imagination. <em>TNNLS</em>, <em>35</em>(7), 8812–8825. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing model-based value expansion (MVE) methods typically leverage a world model for value estimation with a fixed rollout horizon to assist policy learning. However, a proper horizon setting is essential to world-model-based policy learning. Meanwhile, choosing an appropriate horizon value is time-consuming, especially for visual control tasks. In this article, we investigate the idea of adaptively using the model knowledge for value expansion. We propose a novel world-model-based method called dynamic-horizon MVE (DMVE) to adjust the use of the world model with adaptive rollout horizon selection. Based on the reconstruction-based technique, the raw and reconstructed images are both used to obtain multihorizon rollouts by utilizing latent imagination. Then, a horizon reliability degree detection approach is given to select appropriate horizons and obtain more accurate value estimation by the reconstructed value expansion errors. Experimental results on the mainstream benchmark visual control tasks show that DMVE outperforms all baselines in sample efficiency and final performance. In addition, experiments on the autonomous driving lane-changing task further demonstrate the scalability of our method. The codes of DMVE are available at https://github.com/JunjieWang95/dmve .},
  archive      = {J_TNNLS},
  author       = {Junjie Wang and Qichao Zhang and Dongbin Zhao},
  doi          = {10.1109/TNNLS.2022.3215788},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8812-8825},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic-horizon model-based value estimation with latent imagination},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Hider: A hyperspectral image denoising transformer with
spatial–spectral constraints for hybrid noise removal. <em>TNNLS</em>,
<em>35</em>(7), 8797–8811. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) qualities are limited by a mixture of Gaussian noise, impulse noise, stripes, and deadlines during the sensor imaging process, resulting in weak application performance. To enhance HSI qualities, methods based on convolutional neural networks have been successively applied to restore clean data from the observed data. However, the architecture of these methods lacks spectral and spatial constraints, and the convolution operators have limited receptive fields and inflexible model inferences. Thus, in this study, we propose an efficient end-to-end transformer, named HSI denoising transformer (Hider), for mixed HSI noise removal. First, a U-shaped 3-D transformer architecture is built for multiscale feature aggregation. Second, a multihead global spectral attention module within the spectral transformer block is designed to excavate information in different spectral patterns. Finally, an additional locally enhanced cross-spatial attention module within the spatial–spectral transformer block is constructed to build the long-range spatial relationship to avoid the high computational complexity of global spatial self-attention. Through the imposition of global correlations along spectrum and spatial self-similarity constraints on the transformer, our proposed Hider aims to capture long-range spatial contextual information and cluster objects with the same spectral pattern for HSI denoising. To verify the effectiveness and efficiency of Hider, we conducted extensive simulated and real experiments. The denoising results on both simulated and real-world datasets show that Hider achieves superior evaluation metrics and visual assessments compared with other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Hongyu Chen and Guangyi Yang and Hongyan Zhang},
  doi          = {10.1109/TNNLS.2022.3215751},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8797-8811},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hider: A hyperspectral image denoising transformer with Spatial–Spectral constraints for hybrid noise removal},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving exploration in actor–critic with weakly
pessimistic value estimation and optimistic policy optimization.
<em>TNNLS</em>, <em>35</em>(7), 8783–8796. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep off-policy actor–critic algorithms have been successfully applied to challenging tasks in continuous control. However, these methods typically suffer from the poor sample efficiency problem, limiting their widespread adoption in real-world domains. To mitigate this issue, we propose a novel actor–critic algorithm with weakly pessimistic value estimation and optimistic policy optimization (WPVOP) for continuous control. WPVOP integrates two key ingredients: 1) a weakly pessimistic value estimation, which compensates the pessimism of lower confidence bound in conventional value function (i.e., clipped double $Q$ -learning) to trigger exploration in low-value state-action regions and 2) an optimistic policy optimization algorithm by sampling actions that could benefit the policy learning most toward optimal $Q$ -values for efficient exploration. We theoretically analyze that the proposed weakly pessimistic value estimation method is lower and upper bounded, and empirically show that it could avoid extremely over-optimistic value estimates. We show that these two ideas are largely complementary, and can be fruitfully integrated to improve performance and promote sample efficiency of exploration. We evaluate WPVOP on the suite of continuous control tasks from MuJoCo, achieving state-of-the-art sample efficiency and performance.},
  archive      = {J_TNNLS},
  author       = {Fan Li and Mingsheng Fu and Wenyu Chen and Fan Zhang and Haixian Zhang and Hong Qu and Zhang Yi},
  doi          = {10.1109/TNNLS.2022.3215596},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8783-8796},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving exploration in Actor–Critic with weakly pessimistic value estimation and optimistic policy optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploration in deep reinforcement learning: From
single-agent to multiagent domain. <em>TNNLS</em>, <em>35</em>(7),
8762–8782. (<a
href="https://doi.org/10.1109/TNNLS.2023.3236361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.},
  archive      = {J_TNNLS},
  author       = {Jianye Hao and Tianpei Yang and Hongyao Tang and Chenjia Bai and Jinyi Liu and Zhaopeng Meng and Peng Liu and Zhen Wang},
  doi          = {10.1109/TNNLS.2023.3236361},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8762-8782},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploration in deep reinforcement learning: From single-agent to multiagent domain},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring disentanglement: A review of metrics.
<em>TNNLS</em>, <em>35</em>(7), 8747–8761. (<a
href="https://doi.org/10.1109/TNNLS.2022.3218982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to disentangle and represent factors of variation in data is an important problem in artificial intelligence. While many advances have been made to learn these representations, it is still unclear how to quantify disentanglement. While several metrics exist, little is known on their implicit assumptions, what they truly measure, and their limits. In consequence, it is difficult to interpret results when comparing different representations. In this work, we survey supervised disentanglement metrics and thoroughly analyze them. We propose a new taxonomy in which all metrics fall into one of the three families: intervention-based, predictor-based, and information-based. We conduct extensive experiments in which we isolate properties of disentangled representations, allowing stratified comparison along several axes. From our experiment results and analysis, we provide insights on relations between disentangled representation properties. Finally, we share guidelines on how to measure disentanglement.},
  archive      = {J_TNNLS},
  author       = {Marc-André Carbonneau and Julian Zaïdi and Jonathan Boilard and Ghyslain Gagnon},
  doi          = {10.1109/TNNLS.2022.3218982},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8747-8761},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Measuring disentanglement: A review of metrics},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy and robustness in federated learning: Attacks and
defenses. <em>TNNLS</em>, <em>35</em>(7), 8726–8746. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data are increasingly being stored in different silos and societies becoming more aware of data privacy issues, the traditional centralized training of artificial intelligence (AI) models is facing efficiency and privacy challenges. Recently, federated learning (FL) has emerged as an alternative solution and continues to thrive in this new reality. Existing FL protocol designs have been shown to be vulnerable to adversaries within or outside of the system, compromising data privacy and system robustness. Besides training powerful global models, it is of paramount importance to design FL systems that have privacy guarantees and are resistant to different types of adversaries. In this article, we conduct a comprehensive survey on privacy and robustness in FL over the past five years. Through a concise introduction to the concept of FL and a unique taxonomy covering: 1) threat models; 2) privacy attacks and defenses; and 3) poisoning attacks and defenses, we provide an accessible review of this important topic. We highlight the intuitions, key techniques, and fundamental assumptions adopted by various attacks and defenses. Finally, we discuss promising future research directions toward robust and privacy-preserving FL, and their interplays with the multidisciplinary goals of FL.},
  archive      = {J_TNNLS},
  author       = {Lingjuan Lyu and Han Yu and Xingjun Ma and Chen Chen and Lichao Sun and Jun Zhao and Qiang Yang and Philip S. Yu},
  doi          = {10.1109/TNNLS.2022.3216981},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {7},
  number       = {7},
  pages        = {8726-8746},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Privacy and robustness in federated learning: Attacks and defenses},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonlinear decoupling control with PIλ dμ neural network for
MIMO systems. <em>TNNLS</em>, <em>35</em>(6), 8715–8722. (<a
href="https://doi.org/10.1109/TNNLS.2022.3225636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, a fractional order proportional-integral-differential neural network (PIDNN) controller based on the beetle swarm optimization algorithm (BSO-PI $^{\lambda }\text{D}^{\mu }$ NN) is proposed for multi-input multi-output (MIMO) systems with strong coupling. First, the fractional order PID operator is introduced to the hidden layer neurons of the neural network, where long memory characteristics of the fractional order neurons can improve the control accuracy and convergence speed. Second, a sufficient condition on the learning rate is established to ensure the stability of the controller by the Lyapunov theory. Third, the PI $^{\lambda }\text{D}^{\mu }$ NN is initialized by the BSO algorithm to prevent weights from falling into local optima. The proposed fractional order PIDNN controller can eliminate the coupling between variables and achieve desirable control performance without specific system models. To the authors&#39; best knowledge, this is the first work that the fractional order PI $^{\lambda }\text{D}^{\mu }$ neurons are employed in neural network. Two simulation examples verify the effectiveness and superiority of the proposed controller.},
  archive      = {J_TNNLS},
  author       = {Jie Ding and Min Wu and Min Xiao},
  doi          = {10.1109/TNNLS.2022.3225636},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8715-8722},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonlinear decoupling control with PIλ dμ neural network for MIMO systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Positive-incentive noise. <em>TNNLS</em>, <em>35</em>(6),
8708–8714. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise is conventionally viewed as a severe problem in diverse fields, e.g., engineering and learning systems. However, this brief aims to investigate whether the conventional proposition always holds. It begins with the definition of task entropy, which extends from the information entropy and measures the complexity of the task. After introducing the task entropy, the noise can be classified into two kinds, positive-incentive noise (Pi-noise or $\pi $ -noise) and pure noise, according to whether the noise can reduce the complexity of the task. Interestingly, as shown theoretically and empirically, even the simple random noise can be the $\pi $ -noise that simplifies the task. $\pi $ -noise offers new explanations for some models and provides a new principle for some fields, such as multitask learning, adversarial training, and so on. Moreover, it reminds us to rethink the investigation of noises.},
  archive      = {J_TNNLS},
  author       = {Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3224577},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8708-8714},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Positive-incentive noise},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered adaptive neural network tracking control for
uncertain systems with unknown input saturation based on command
filters. <em>TNNLS</em>, <em>35</em>(6), 8702–8707. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief presents a modified event-triggered command filter backstepping tracking control scheme for a class of uncertain nonlinear systems with unknown input saturation based on the adaptive neural network (NN) technique. First, the virtual control functions are reconstructed to address the uncertainties in subsystems by using command filters. A piecewise continuous function is employed to deal with the unknown input saturation problem. Next, an event-triggered tracking controller is developed by utilizing the adaptive NN technique. Compared with standard NN control schemes based on multiple-function-approximators, our controller only requires a single NN. The closed-loop system stability is analyzed based on the Lyapunov stability theorem, and it is shown that the Zeno behavior is also avoided under the designed event-triggering mechanism. Simulation studies are performed to validate the effectiveness of our controller.},
  archive      = {J_TNNLS},
  author       = {Jiapeng Liu and Qing-Guo Wang and Jinpeng Yu},
  doi          = {10.1109/TNNLS.2022.3224065},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8702-8707},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive neural network tracking control for uncertain systems with unknown input saturation based on command filters},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online identification of nonlinear systems with separable
structure. <em>TNNLS</em>, <em>35</em>(6), 8695–8701. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Separable nonlinear models (SNLMs) are of great importance in system modeling, signal processing, and machine learning because of their flexible structure and excellent description of nonlinear behaviors. The online identification of such models is quite challenging, and previous related work usually ignores the special structure where the estimated parameters can be partitioned into a linear and a nonlinear part. In this brief, we propose an efficient first-order recursive algorithm for SNLMs by introducing the variable projection (VP) step. The proposed algorithm utilizes the recursive least-squares method to eliminate the linear parameters, resulting in a reduced function. Then, the stochastic gradient descent (SGD) algorithm is employed to update the parameters of the reduced function. By considering the tight coupling relationship between linear parameters and nonlinear parameters, the proposed first-order VP algorithm is more efficient and robust than the traditional SGD algorithm and alternating optimization algorithm. More importantly, since the proposed algorithm just uses the first-order information, it is easier to apply it to large-scale models. Numerical results on examples of different sizes confirm the effectiveness and efficiency of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Guang-Yong Chen and Min Gan and Long Chen and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3215756},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8695-8701},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online identification of nonlinear systems with separable structure},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asynchronous parallel large-scale gaussian process
regression. <em>TNNLS</em>, <em>35</em>(6), 8683–8694. (<a
href="https://doi.org/10.1109/TNNLS.2022.3200602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process regression (GPR) is an important nonparametric learning method in machine learning research with many real-world applications. It is well known that training large-scale GPR is a challenging task due to the required heavy computational cost and large volume memory. To address this challenging problem, in this article, we propose an asynchronous doubly stochastic gradient algorithm to handle the large-scale training of GPR. We formulate the GPR to a convex optimization problem, i.e., kernel ridge regression. After that, in order to efficiently solve this convex kernel problem, we first use the random feature mapping method to approximate the kernel model and then utilize two unbiased stochastic approximations, i.e., stochastic variance reduced gradient and stochastic coordinate descent, to update the solution asynchronously and in parallel. In this way, our algorithm scales well in both sample size and dimensionality, and speeds up the training computation. More importantly, we prove that our algorithm has a global linear convergence rate. Our experimental results on eight large-scale benchmark datasets with both regression and classification tasks show that the proposed algorithm outperforms the existing state-of-the-art GPR methods.},
  archive      = {J_TNNLS},
  author       = {Zhiyuan Dang and Bin Gu and Cheng Deng and Heng Huang},
  doi          = {10.1109/TNNLS.2022.3200602},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8683-8694},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asynchronous parallel large-scale gaussian process regression},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). RCDNet: An interpretable rain convolutional dictionary
network for single image deraining. <em>TNNLS</em>, <em>35</em>(6),
8668–8682. (<a
href="https://doi.org/10.1109/TNNLS.2022.3231453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As common weather, rain streaks adversely degrade the image quality and tend to negatively affect the performance of outdoor computer vision systems. Hence, removing rains from an image has become an important issue in the field. To handle such an ill-posed single image deraining task, in this article, we specifically build a novel deep architecture, called rain convolutional dictionary network (RCDNet), which embeds the intrinsic priors of rain streaks and has clear interpretability. In specific, we first establish a rain convolutional dictionary (RCD) model for representing rain streaks and utilize the proximal gradient descent technique to design an iterative algorithm only containing simple operators for solving the model. By unfolding it, we then build the RCDNet in which every network module has clear physical meanings and corresponds to each operation involved in the algorithm. This good interpretability greatly facilitates an easy visualization and analysis of what happens inside the network and why it works well in the inference process. Moreover, taking into account the domain gap issue in real scenarios, we further design a novel dynamic RCDNet, where the rain kernels can be dynamically inferred corresponding to input rainy images and then help shrink the space for rain layer estimation with few rain maps, so as to ensure a fine generalization performance in the inconsistent scenarios of rain types between training and testing data. By end-to-end training such an interpretable network, all involved rain kernels and proximal operators can be automatically extracted, faithfully characterizing the features of both rain and clean background layers and, thus, naturally leading to better deraining performance. Comprehensive experiments implemented on a series of representative synthetic and real datasets substantiate the superiority of our method, especially on its well generality to diverse testing scenarios and good interpretability for all its modules, compared with state-of-the-art single image derainers both visually and quantitatively. Code is available at https://github.com/hongwang01/DRCDNet .},
  archive      = {J_TNNLS},
  author       = {Hong Wang and Qi Xie and Qian Zhao and Yuexiang Li and Yong Liang and Yefeng Zheng and Deyu Meng},
  doi          = {10.1109/TNNLS.2022.3231453},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8668-8682},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RCDNet: An interpretable rain convolutional dictionary network for single image deraining},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autobalanced multitask node embedding framework for
intelligent education. <em>TNNLS</em>, <em>35</em>(6), 8653–8667. (<a
href="https://doi.org/10.1109/TNNLS.2022.3231421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, online education has become popular. Many e-learning platforms have been launched with various intelligent services aimed at improving the learning efficiency and effectiveness of learners. Graphs are used to describe the pairwise relations between entities, and the node embedding technique is the foundation of many intelligent services, which have received increasing attention from researchers. However, the graph in the intelligent education scenario has three noteworthy properties, namely, heterogeneity, evolution, and lopsidedness, which makes it challenging to implement ecumenical node embedding methods on it. In this article, an autobalanced multitask node embedding model is proposed, named MNE, and applied to the interaction graph, settling a few actual tasks in intelligent education. More specifically, MNE builds two purpose-built self-supervised node embedding learning tasks for heterogeneous evolutive graphs. Edge-specific reconstruction tasks are built according to the semantic information and properties of the heterogeneous edges, and an evolutive weight regression task is designed, aiding the model to perceive the evolution of learners’ implicit cognitive states. Then, both aleatoric and epistemic uncertainty quantification techniques are introduced, achieving both task- and node-level weight estimation and instructing subtask autobalancing. Experimental results on real-world datasets indicate that the proposed model outperforms the state-of-the-art graph embedding methods on two assessment tasks and demonstrates the validity of the proposed multitask framework and subtask balancing mechanism. Our implementations are available at https://github.com/ccnu-mathits/MNE4HEN .},
  archive      = {J_TNNLS},
  author       = {Xiaoxuan Shen and Jianwei Yu and Ruxia Liang and Qing Li and Shengyingjie Liu and Shangheng Du and Jianwen Sun and Sannyuya Liu},
  doi          = {10.1109/TNNLS.2022.3231421},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8653-8667},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Autobalanced multitask node embedding framework for intelligent education},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coupled memristor oscillators for neuromorphic locomotion
control: Modeling and analysis. <em>TNNLS</em>, <em>35</em>(6),
8638–8652. (<a
href="https://doi.org/10.1109/TNNLS.2022.3231298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent surge of interest in brain-inspired architectures along with the development of nonlinear dynamical electronic devices and circuits has enabled energy-efficient hardware realizations of several important neurobiological systems and features. Central pattern generator (CPG) is one such neural system underlying the control of various rhythmic motor behaviors in animals. A CPG can produce spontaneous coordinated rhythmic output signals without any feedback mechanism, ideally realizable by a system of coupled oscillators. Bio-inspired robotics aims to use this approach to control the limb movement for synchronized locomotion. Hence, devising a compact and energy-efficient hardware platform to implement neuromorphic CPGs would be of great benefit for bio-inspired robotics. In this work, we demonstrate that four capacitively coupled vanadium dioxide (VO2) memristor-based oscillators can produce spatiotemporal patterns corresponding to the primary quadruped gaits. The phase relationships underlying the gait patterns are governed by four tunable bias voltages (or four coupling strengths) making the network programmable, reducing the complex problem of gait selection and dynamic interleg coordination to the choice of four control parameters. To this end, we first introduce a dynamical model for the VO2 memristive nanodevice, then perform analytical and bifurcation analysis of a single oscillator, and finally demonstrate the dynamics of coupled oscillators through extensive numerical simulations. We also show that adopting the presented model for a VO2 memristor reveals a striking resemblance between VO2 memristor oscillators and conductance-based biological neuron models such as the Morris–Lecar (ML) model. This can inspire and guide further research on implementation of neuromorphic memristor circuits that emulate neurobiological phenomena.},
  archive      = {J_TNNLS},
  author       = {Akhil Bonagiri and Dipayan Biswas and Srinivasa Chakravarthy},
  doi          = {10.1109/TNNLS.2022.3231298},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8638-8652},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coupled memristor oscillators for neuromorphic locomotion control: Modeling and analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed state estimation for mixed delays system over
sensor networks with multichannel random attacks and markov switching
topology. <em>TNNLS</em>, <em>35</em>(6), 8623–8637. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the distributed state estimation for mixed delays system under unknown attacks. A new multichannel random attack model is established for the first time, where network attacks are considered to exist in three channels: the target-to-sensor channel, the senor-to-sensor channel, and the sensor-to-estimator channel. In the above model, transmitted packets are allowed to be attacked multiple times simultaneously, and when they are successfully attacked, the transmitted information is modified. Besides, the topology of the sensor network is considered to change dynamically according to the Markov chain. Based on the newly established distributed estimation model, the estimation error system is proven to be asymptotically mean-square stable under a given ${H_{\infty} }$ antidisturbance index by using a Lyapunov theory and a stochastic analysis technique; then, the estimator parameter matrices are solved utilizing a linearization method. Finally, several simulation examples are listed to testify the effectiveness of the designed algorithm.},
  archive      = {J_TNNLS},
  author       = {Wei Qian and Di Lu and Simeng Guo and Yunji Zhao},
  doi          = {10.1109/TNNLS.2022.3230978},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8623-8637},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed state estimation for mixed delays system over sensor networks with multichannel random attacks and markov switching topology},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRLC: Graph representation learning with constraints.
<em>TNNLS</em>, <em>35</em>(6), 8609–8622. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has been successfully applied in unsupervised representation learning. However, the generalization ability of representation learning is limited by the fact that the loss of downstream tasks (e.g., classification) is rarely taken into account while designing contrastive methods. In this article, we propose a new contrastive-based unsupervised graph representation learning (UGRL) framework by 1) maximizing the mutual information (MI) between the semantic information and the structural information of the data and 2) designing three constraints to simultaneously consider the downstream tasks and the representation learning. As a result, our proposed method outputs robust low-dimensional representations. Experimental results on 11 public datasets demonstrate that our proposed method is superior over recent state-of-the-art methods in terms of different downstream tasks. Our code is available at https://github.com/LarryUESTC/GRLC .},
  archive      = {J_TNNLS},
  author       = {Liang Peng and Yujie Mo and Jie Xu and Jialie Shen and Xiaoshuang Shi and Xiaoxiao Li and Heng Tao Shen and Xiaofeng Zhu},
  doi          = {10.1109/TNNLS.2022.3230979},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8609-8622},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GRLC: Graph representation learning with constraints},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse-free DZNN models for solving time-dependent linear
system via high-precision linear six-step method. <em>TNNLS</em>,
<em>35</em>(6), 8597–8608. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-dependent linear system (TDLS) is usually encountered in scientific research, which is the mathematical formulation of many practical applications. Different from conventional inverse-need models, by utilizing zeroing neural network (ZNN) method twice, an inverse-free continuous ZNN (CZNN) model is developed for solving TDLS. For conveniently practical use, a discrete model is naturally desired. Superior to conventional discretization methods, a general linear six-step (LSS) method with the seventh-order precision and five variable parameters is proposed for the first time. Constraints about five variable parameters are theoretically analyzed to guarantee the efficacy of the general LSS method. Within constraints, 12 specific LSS methods are further developed. Aided with the general LSS method, an inverse-free discrete ZNN (DZNN) is proposed and termed DZNN-LSS model, and its precision is greatly improved compared with conventional discrete models. For comparison, three conventional discretization methods are also utilized to generate DZNN models. Detailed theoretical analyses are provided to prove the efficacy of relevant models. In addition, a specific TDLS example is considered to show the effectiveness and superiority of the DZNN-LSS model. More than that, applications to manipulator control and sound source localization are conducted to illustrate the applicability of the DZNN-LSS model.},
  archive      = {J_TNNLS},
  author       = {Min Yang and Yunong Zhang and Haifeng Hu},
  doi          = {10.1109/TNNLS.2022.3230898},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8597-8608},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Inverse-free DZNN models for solving time-dependent linear system via high-precision linear six-step method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Signal propagation: The framework for learning and inference
in a forward pass. <em>TNNLS</em>, <em>35</em>(6), 8585–8596. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new learning framework, signal propagation (sigprop), for propagating a learning signal and updating neural network parameters via a forward pass, as an alternative to backpropagation (BP). In sigprop, there is only the forward path for inference and learning. So, there are no structural or computational constraints necessary for learning to take place, beyond the inference model itself, such as feedback connectivity, weight transport, or a backward pass, which exist under BP-based approaches. That is, sigprop enables global supervised learning with only a forward path. This is ideal for parallel training of layers or modules. In biology, this explains how neurons without feedback connections can still receive a global learning signal. In hardware, this provides an approach for global supervised learning without backward connectivity. Sigprop by construction has compatibility with models of learning in the brain and in hardware than BP, including alternative approaches relaxing learning constraints. We also demonstrate that sigprop is more efficient in time and memory than they are. To further explain the behavior of sigprop, we provide evidence that sigprop provides useful learning signals in context to BP. To further support relevance to biological and hardware learning, we use sigprop to train continuous time neural networks with the Hebbian updates and train spiking neural networks (SNNs) with only the voltage or with biologically and hardware-compatible surrogate functions.},
  archive      = {J_TNNLS},
  author       = {Adam Kohan and Edward A. Rietman and Hava T. Siegelmann},
  doi          = {10.1109/TNNLS.2022.3230914},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8585-8596},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Signal propagation: The framework for learning and inference in a forward pass},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MiniSeg: An extremely minimum network based on lightweight
multiscale learning for efficient COVID-19 segmentation. <em>TNNLS</em>,
<em>35</em>(6), 8570–8584. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid spread of the new pandemic, i.e., coronavirus disease 2019 (COVID-19), has severely threatened global health. Deep-learning-based computer-aided screening, e.g., COVID-19 infected area segmentation from computed tomography (CT) image, has attracted much attention by serving as an adjunct to increase the accuracy of COVID-19 screening and clinical diagnosis. Although lesion segmentation is a hot topic, traditional deep learning methods are usually data-hungry with millions of parameters, easy to overfit under limited available COVID-19 training data. On the other hand, fast training/testing and low computational cost are also necessary for quick deployment and development of COVID-19 screening systems, but traditional methods are usually computationally intensive. To address the above two problems, we propose MiniSeg, a lightweight model for efficient COVID-19 segmentation from CT images. Our efforts start with the design of an attentive hierarchical spatial pyramid (AHSP) module for lightweight, efficient, effective multiscale learning that is essential for image segmentation. Then, we build a two-path (TP) encoder for deep feature extraction, where one path uses AHSP modules for learning multiscale contextual features and the other is a shallow convolutional path for capturing fine details. The two paths interact with each other for learning effective representations. Based on the extracted features, a simple decoder is added for COVID-19 segmentation. For comparing MiniSeg to previous methods, we build a comprehensive COVID-19 segmentation benchmark. Extensive experiments demonstrate that the proposed MiniSeg achieves better accuracy because its only 83k parameters make it less prone to overfitting. Its high efficiency also makes it easy to deploy and develop. The code has been released at https://github.com/yun-liu/MiniSeg .},
  archive      = {J_TNNLS},
  author       = {Yu Qiu and Yun Liu and Shijie Li and Jing Xu},
  doi          = {10.1109/TNNLS.2022.3230821},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8570-8584},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MiniSeg: An extremely minimum network based on lightweight multiscale learning for efficient COVID-19 segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depthwise convolution for multi-agent communication with
enhanced mean-field approximation. <em>TNNLS</em>, <em>35</em>(6),
8557–8569. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent settings remain a fundamental challenge in the reinforcement learning (RL) domain due to the partial observability and the lack of accurate real-time interactions across agents. In this article, we propose a new method based on local communication learning to tackle the multi-agent RL (MARL) challenge within a large number of agents coexisting. First, we design a new communication protocol that exploits the ability of depthwise convolution to efficiently extract local relations and learn local communication between neighboring agents. To facilitate multi-agent coordination, we explicitly learn the effect of joint actions by taking the policies of neighboring agents as inputs. Second, we introduce the mean-field approximation into our method to reduce the scale of agent interactions. To more effectively coordinate behaviors of neighboring agents, we enhance the mean-field approximation by a supervised policy rectification network (PRN) for rectifying real-time agent interactions and by a learnable compensation term for correcting the approximation bias. The proposed method enables efficient coordination as well as outperforms several baseline approaches on the adaptive traffic signal control (ATSC) task and the StarCraft II multi-agent challenge (SMAC).},
  archive      = {J_TNNLS},
  author       = {Donghan Xie and Zhi Wang and Chunlin Chen and Daoyi Dong},
  doi          = {10.1109/TNNLS.2022.3230701},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8557-8569},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Depthwise convolution for multi-agent communication with enhanced mean-field approximation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effects of two-channel noise and packet loss on performance
of information time delay systems. <em>TNNLS</em>, <em>35</em>(6),
8549–8556. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance limitations of multiple-input multiple-output (MIMO) information time delay system (ITDS) with packet loss, codec and white Gaussian noise (WGN) are investigated in this article. By using the spectrum decomposition technique, inner-outer factorization, and partial factorization techniques, the expression of performance limitations is obtained under the two-degree-of-freedom (2DOF) compensator. The theoretical analysis results demonstrate that the system performance is related to the time delay, non-minimum phase (NMP) zeros, unstable zeros and their directions in a given device. In addition, WGN, packet loss and codec also impact the performance. Finally, the theoretical results are verified by simulation examples. Simulation results show that packet loss rate and encoding and decoding have a greater impact on system performance.},
  archive      = {J_TNNLS},
  author       = {Xiangchen Du and Xisheng Zhan and Jie Wu and Huaicheng Yan},
  doi          = {10.1109/TNNLS.2022.3230648},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8549-8556},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effects of two-channel noise and packet loss on performance of information time delay systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable graph wavelet denoising network for intelligent
fault diagnosis. <em>TNNLS</em>, <em>35</em>(6), 8535–8548. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL)-based intelligent fault diagnosis methods have greatly promoted the development of the field of fault diagnosis due to their powerful feature extraction ability for handling massive monitoring data. However, most of them still suffer from the following three limitations. First, many existing DL-based intelligent diagnosis methods cannot extract proper discriminative features from signals with strong noise. Second, the interactions or relationships between signals are ignored, while they mainly focus on extracting temporal features from the signal. Third, owing to their black-box nature, the learned features lack interpretability, which hinders their application in the industry. To tackle these issues, an explainable graph wavelet denoising network (GWDN) is proposed to achieve intelligent fault diagnosis under noisy working conditions in this article. In GWDN, the collected signals are first transformed into graph-structured data to consider the interactions among signals. Then, the graph wavelet denoising convolution (GWDConv) is proposed based on the discrete graph wavelet frame, which allows GWDN to achieve multiscale feature extraction for graph-structured data and realize signal denoising. Extensive experiments are implemented to verify the efficacy of the proposed GWDN, and the experimental results show that GWDN can achieve state-of-the-art performance among the comparison methods. Besides, by using the square envelope spectrum to analyze the extracted features of GWDConv, we find that it can well retain the fault-related components of the signal and realize signal denoising, which further proves that GWDN is explainable.},
  archive      = {J_TNNLS},
  author       = {Tianfu Li and Chuang Sun and Sinan Li and Zhiying Wang and Xuefeng Chen and Ruqiang Yan},
  doi          = {10.1109/TNNLS.2022.3230458},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8535-8548},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Explainable graph wavelet denoising network for intelligent fault diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Event-triggered adaptive containment control for
heterogeneous stochastic nonlinear multiagent systems. <em>TNNLS</em>,
<em>35</em>(6), 8524–8534. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the event-triggered adaptive containment control problem for a class of stochastic nonlinear multiagent systems with unmeasurable states. A stochastic system with unknown heterogeneous dynamics is established to describe the agents in a random vibration environment. Besides, the uncertain nonlinear dynamics are approximated by radial basis function neural networks (NNs), and the unmeasured states are estimated by constructing the NN-based observer. In addition, the switching-threshold-based event-triggered control method is adopted with the hope of reducing communication consumption and balancing system performance and network constraints. Moreover, we develop the novel distributed containment controller by utilizing the adaptive backstepping control strategy and the dynamic surface control (DSC) approach such that the output of each follower converges to the convex hull spanned by multiple leaders, and all signals of the closed-loop system are cooperatively semi-globally uniformly ultimately bounded in mean square. Finally, we verify the efficiency of the proposed controller by the simulation examples.},
  archive      = {J_TNNLS},
  author       = {Xin Wang and Rui Xu and Tingwen Huang and Jürgen Kurths},
  doi          = {10.1109/TNNLS.2022.3230508},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8524-8534},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive containment control for heterogeneous stochastic nonlinear multiagent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive image segmentation network for surface defect
detection. <em>TNNLS</em>, <em>35</em>(6), 8510–8523. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface defect detection plays an essential role in industry, and it is challenging due to the following problems: 1) the similarity between defect and nondefect texture is very high, which eventually leads to recognition or classification errors and 2) the size of defects is tiny, which are much more difficult to be detected than larger ones. To address such problems, this article proposes an adaptive image segmentation network (AIS-Net) for pixelwise segmentation of surface defects. It consists of three main parts: multishuffle-block dilated convolution (MSDC), dual attention context guidance (DACG), and adaptive category prediction (ACP) modules, where MSDC is designed to merge the multiscale defect features for avoiding the loss of tiny defect feature caused by model depth, DACG is designed to capture more contextual information from the defect feature map for locating defect regions and obtaining clear segmentation boundaries, and ACP is used to make classification and regression for predicting defect categories. Experimental results show that the proposed AIS-Net is superior to the state-of-the-art approaches on four actual surface defect datasets (NEU-DET: 98.38% ± 0.03%, DAGM: 99.25% ± 0.02%, Magnetic-tile: 98.73% ± 0.13%, and MVTec: 99.72% ± 0.02%).},
  archive      = {J_TNNLS},
  author       = {Taiheng Liu and Zhaoshui He and Zhijie Lin and Guang-Zhong Cao and Wenqing Su and Shengli Xie},
  doi          = {10.1109/TNNLS.2022.3230426},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8510-8523},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An adaptive image segmentation network for surface defect detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond homophily and homogeneity assumption: Relation-based
frequency adaptive graph neural networks. <em>TNNLS</em>,
<em>35</em>(6), 8497–8509. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been playing important roles in various graph-related tasks. However, most existing GNNs are based on the assumption of homophily, so they cannot be directly generalized to heterophily settings where connected nodes may have different features and class labels. Moreover, real-world graphs often arise from highly entangled latent factors, but the existing GNNs tend to ignore this and simply denote the heterogeneous relations between nodes as binary-valued homogeneous edges. In this article, we propose a novel relation-based frequency adaptive GNN (RFA-GNN) to handle both heterophily and heterogeneity in a unified framework. RFA-GNN first decomposes an input graph into multiple relation graphs, each representing a latent relation. More importantly, we provide detailed theoretical analysis from the perspective of spectral signal processing. Based on this, we propose a relation-based frequency adaptive mechanism that adaptively picks up signals of different frequencies in each corresponding relation space in the message-passing process. Extensive experiments on synthetic and real-world datasets show qualitatively and quantitatively that RFA-GNN yields truly encouraging results for both the heterophily and heterogeneity settings. Codes are publicly available at: https://github.com/LirongWu/RFA-GNN .},
  archive      = {J_TNNLS},
  author       = {Lirong Wu and Haitao Lin and Bozhen Hu and Cheng Tan and Zhangyang Gao and Zicheng Liu and Stan Z. Li},
  doi          = {10.1109/TNNLS.2022.3230417},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8497-8509},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Beyond homophily and homogeneity assumption: Relation-based frequency adaptive graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the temporal consistency of arbitrary style
transfer: A channelwise perspective. <em>TNNLS</em>, <em>35</em>(6),
8482–8496. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary image stylization by neural networks has become a popular topic, and video stylization is attracting more attention as an extension of image stylization. However, when image stylization methods are applied to videos, unsatisfactory results that suffer from severe flickering effects appear. In this article, we conducted a detailed and comprehensive analysis of the cause of such flickering effects. Systematic comparisons among typical neural style transfer approaches show that the feature migration modules for state-of-the-art (SOTA) learning systems are ill-conditioned and could lead to a channelwise misalignment between the input content representations and the generated frames. Unlike traditional methods that relieve the misalignment via additional optical flow constraints or regularization modules, we focus on keeping the temporal consistency by aligning each output frame with the input frame. To this end, we propose a simple yet efficient multichannel correlation network (MCCNet), to ensure that output frames are directly aligned with inputs in the hidden feature space while maintaining the desired style patterns. An inner channel similarity loss is adopted to eliminate side effects caused by the absence of nonlinear operations such as softmax for strict alignment. Furthermore, to improve the performance of MCCNet under complex light conditions, we introduce an illumination loss during training. Qualitative and quantitative evaluations demonstrate that MCCNet performs well in arbitrary video and image style transfer tasks. Code is available at https://github.com/kongxiuxiu/MCCNetV2 .},
  archive      = {J_TNNLS},
  author       = {Xiaoyu Kong and Yingying Deng and Fan Tang and Weiming Dong and Chongyang Ma and Yongyong Chen and Zhenyu He and Changsheng Xu},
  doi          = {10.1109/TNNLS.2022.3230084},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8482-8496},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploring the temporal consistency of arbitrary style transfer: A channelwise perspective},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tiny machine learning for concept drift. <em>TNNLS</em>,
<em>35</em>(6), 8470–8481. (<a
href="https://doi.org/10.1109/TNNLS.2022.3229897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tiny machine learning (TML) is a new research area whose goal is to design machine and deep learning (DL) techniques able to operate in embedded systems and the Internet-of-Things (IoT) units, hence satisfying the severe technological constraints on memory, computation, and energy characterizing these pervasive devices. Interestingly, the related literature mainly focused on reducing the computational and memory demand of the inference phase of machine and deep learning models. At the same time, the training is typically assumed to be carried out in cloud or edge computing systems (due to the larger memory and computational requirements). This assumption results in TML solutions that might become obsolete when the process generating the data is affected by concept drift (e.g., due to periodicity or seasonality effect, faults or malfunctioning affecting sensors or actuators, or changes in the users’ behavior), a common situation in real-world application scenarios. For the first time in the literature, this article introduces a TML for concept drift (TML-CD) solution based on deep learning feature extractors and a $k$ -nearest neighbors ( $k$ -NNs) classifier integrating a hybrid adaptation module able to deal with concept drift affecting the data-generating process. This adaptation module continuously updates (in a passive way) the knowledge base of TML-CD and, at the same time, employs a change detection test (CDT) to inspect for changes (in an active way) to quickly adapt to concept drift by removing obsolete knowledge. Experimental results on both image and audio benchmarks show the effectiveness of the proposed solution, whilst the porting of TML-CD on three off-the-shelf micro-controller units (MCUs) shows the feasibility of what is proposed in real-world pervasive systems.},
  archive      = {J_TNNLS},
  author       = {Simone Disabato and Manuel Roveri},
  doi          = {10.1109/TNNLS.2022.3229897},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8470-8481},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tiny machine learning for concept drift},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neighborhood pattern is crucial for graph convolutional
networks performing node classification. <em>TNNLS</em>, <em>35</em>(6),
8456–8469. (<a
href="https://doi.org/10.1109/TNNLS.2022.3229721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) are widely believed to perform well in the graph node classification task, and homophily assumption plays a core rule in the design of previous GCNs. However, some recent advances on this area have pointed out that homophily may not be a necessity for GCNs. For deeper analysis of the critical factor affecting the performance of GCNs, we first propose a metric, namely, neighborhood class consistency (NCC), to quantitatively characterize the neighborhood patterns of graph datasets. Experiments surprisingly illustrate that our NCC is a better indicator, in comparison to the widely used homophily metrics, to estimate GCN performance for node classification. Furthermore, we propose a topology augmentation graph convolutional network (TA-GCN) framework under the guidance of the NCC metric, which simultaneously learns an augmented graph topology with higher NCC score and a node classifier based on the augmented graph topology. Extensive experiments on six public benchmarks clearly show that the proposed TA-GCN derives ideal topology with higher NCC score given the original graph topology and raw features, and it achieves excellent performance for semi-supervised node classification in comparison to several state-of-the-art (SOTA) baseline algorithms.},
  archive      = {J_TNNLS},
  author       = {Gongpei Zhao and Tao Wang and Yidong Li and Yi Jin and Congyan Lang and Songhe Feng},
  doi          = {10.1109/TNNLS.2022.3229721},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8456-8469},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neighborhood pattern is crucial for graph convolutional networks performing node classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MutexMatch: Semi-supervised learning with mutex-based
consistency regularization. <em>TNNLS</em>, <em>35</em>(6), 8441–8455.
(<a href="https://doi.org/10.1109/TNNLS.2022.3228380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core issue in semi-supervised learning (SSL) lies in how to effectively leverage unlabeled data, whereas most existing methods tend to put a great emphasis on the utilization of high-confidence samples yet seldom fully explore the usage of low-confidence samples. In this article, we aim to utilize low-confidence samples in a novel way with our proposed mutex-based consistency regularization, namely MutexMatch. Specifically, the high-confidence samples are required to exactly predict “what it is” by the conventional true-positive classifier (TPC), while low-confidence samples are employed to achieve a simpler goal—to predict with ease “what it is not” by the true-negative classifier (TNC). In this sense, we not only mitigate the pseudo-labeling errors but also make full use of the low-confidence unlabeled data by the consistency of dissimilarity degree. MutexMatch achieves superior performance on multiple benchmark datasets, i.e., Canadian Institute for Advanced Research (CIFAR)-10, CIFAR-100, street view house numbers (SVHN), self-taught learning 10 (STL-10), and mini-ImageNet. More importantly, our method further shows superiority when the amount of labeled data is scarce, e.g., 92.23% accuracy with only 20 labeled data on CIFAR-10. Code has been released at https://github.com/NJUyued/MutexMatch4SSL .},
  archive      = {J_TNNLS},
  author       = {Yue Duan and Zhen Zhao and Lei Qi and Lei Wang and Luping Zhou and Yinghuan Shi and Yang Gao},
  doi          = {10.1109/TNNLS.2022.3228380},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8441-8455},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MutexMatch: Semi-supervised learning with mutex-based consistency regularization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning graph representations through learning and
propagating edge features. <em>TNNLS</em>, <em>35</em>(6), 8429–8440.
(<a href="https://doi.org/10.1109/TNNLS.2022.3228102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks have achieved considerable success in various graph domain tasks. Recently, numerous types of graph convolutional networks have been developed. A typical rule for learning a node’s feature in these graph convolutional networks is to aggregate node features from the node’s local neighborhood. However, in these models, the interrelation information between adjacent nodes is not well-considered. This information could be helpful to learn improved node embeddings. In this article, we present a graph representation learning framework that generates node embeddings through learning and propagating edge features. Instead of aggregating node features from a local neighborhood, we learn a feature for each edge and update a node’s representation by aggregating local edge features. The edge feature is learned from the concatenation of the edge’s starting node feature, the input edge feature, and the edge’s end node feature. Unlike node feature propagation-based graph networks, our model propagates different features from a node to its neighbors. In addition, we learn an attention vector for each edge in aggregation, enabling the model to focus on important information in each feature dimension. By learning and aggregating edge features, the interrelation between a node and its neighboring nodes is integrated in the aggregated feature, which helps learn improved node embeddings in graph representation learning. Our model is evaluated on graph classification, node classification, graph regression, and multitask binary graph classification on eight popular datasets. The experimental results demonstrate that our model achieves improved performance compared with a wide variety of baseline models.},
  archive      = {J_TNNLS},
  author       = {Haimin Zhang and Jiahao Xia and Guoqiang Zhang and Min Xu},
  doi          = {10.1109/TNNLS.2022.3228102},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8429-8440},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning graph representations through learning and propagating edge features},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direction and residual awareness curriculum learning network
for rain streaks removal. <em>TNNLS</em>, <em>35</em>(6), 8414–8428. (<a
href="https://doi.org/10.1109/TNNLS.2022.3227730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image rain streaks’ removal has attracted great attention in recent years. However, due to the highly visual similarity between the rain streaks and the line pattern image edges, the over-smoothing of image edges or residual rain streaks’ phenomenon may unexpectedly occur in the deraining results. To overcome this problem, we propose a direction and residual awareness network within the curriculum learning paradigm for the rain streaks’ removal. Specifically, we present a statistical analysis of the rain streaks on large-scale real rainy images and figure out that rain streaks in local patches possess principal directionality. This motivates us to design a direction-aware network for rain streaks’ modeling, in which the principal directionality property endows us with the discriminative representation ability of better differing rain streaks from image edges. On the other hand, for image modeling, we are motivated by the iterative regularization in classical image processing and unfold it into a novel residual-aware block (RAB) to explicitly model the relationship between the image and the residual. The RAB adaptively learns balance parameters to selectively emphasize informative image features and better suppress the rain streaks. Finally, we formulate the rain streaks’ removal problem into the curriculum learning paradigm which progressively learns the directionality of the rain streaks, rain streaks’ appearance, and the image layer in a coarse-to-fine, easy-to-hard guidance manner. Solid experiments on extensive simulated and real benchmarks demonstrate the visual and quantitative improvement of the proposed method over the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yi Chang and Meiya Chen and Changfeng Yu and Yi Li and Liqun Chen and Luxin Yan},
  doi          = {10.1109/TNNLS.2022.3227730},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8414-8428},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Direction and residual awareness curriculum learning network for rain streaks removal},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure suture learning-based robust multiview palmprint
recognition. <em>TNNLS</em>, <em>35</em>(6), 8401–8413. (<a
href="https://doi.org/10.1109/TNNLS.2022.3227473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-quality palmprint images will degrade the recognition performance, when they are captured under the open, unconstraint, and low-illumination conditions. Moreover, the traditional single-view palmprint representation methods have been difficult to express the characteristics of each palm strongly, where the palmprint characteristics become weak. To tackle these issues, in this article, we propose a structure suture learning-based robust multiview palmprint recognition method (SSL_RMPR), which comprehensively presents the salient palmprint features from multiple views. Unlike the existing multiview palmprint representation methods, SSL_RMPR introduces a structure suture learning strategy to produce an elastic nearest neighbor graph (ENNG) on the reconstruction errors that simultaneously exploit the label information and the latent consensus structure of the multiview data, such that the discriminant palmprint representation can be adaptively enhanced. Meanwhile, a low-rank reconstruction term integrating with the projection matrix learning is proposed, in such a manner that the robustness of the projection matrix can be improved. Particularly, since no extra structure capture term is imposed into the proposed model, the complexity of the model can be greatly reduced. Experimental results have proven the superiority of the proposed SSL_RMPR by achieving the best recognition performances on a number of real-world palmprint databases.},
  archive      = {J_TNNLS},
  author       = {Shuping Zhao and Lunke Fei and Jie Wen and Bob Zhang and Pengyang Zhao and Shuyi Li},
  doi          = {10.1109/TNNLS.2022.3227473},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8401-8413},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Structure suture learning-based robust multiview palmprint recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geodesic basis function neural network. <em>TNNLS</em>,
<em>35</em>(6), 8386–8400. (<a
href="https://doi.org/10.1109/TNNLS.2022.3227296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the learning of existing radial basis function neural networks-based methods, it is difficult to propagate errors back. This leads to an inconsistency between the learning and recognition task. This article proposes a geodesic basis function neural network with subclass extension learning (GBFNN-ScE). The geodesic basis function (GBF), which is defined here for the first time, uses the geodetic distance in the manifold as a measure to obtain the response of the sample with respect to the local center. To learn network parameters by back-propagating errors for the purpose of correct classification, a specific GBF based on a pruned gamma encoding cosine function is constructed. This function has a concise and explicit expression on the hyperspherical manifold, which is conducive to the realization of error back propagation. In the preprocessing layer, a sample unitization method with no loss of information, nonnegative unit hyperspherical crown (NUHC) mapping, is proposed. The sample can be mapped to the support set of the GBF. To alleviate the problem that one-hot encoding is not effective enough in the differential expression of data labels within a class, a subclass extension (ScE) learning strategy is proposed. The ScE learning strategy can help the learned network be more robust. For the working of GBFNN-ScE, the original sample is projected onto the support set of GBF through the NUHC mapping. Then the mapped samples are sent to the nonlinear computation units composed of GBFs in the hidden layer. Finally, the response obtained in the hidden layer is weighted by the learned weight to obtain the network output. This article theoretically proves that the separability of the data with ScE learning is stronger. The experimental results show that the proposed GBFNN-ScE has a better performance in recognition tasks than existing methods. The ablation experiments show that the ideas of the GBFNN-ScE contribute to the algorithm performance.},
  archive      = {J_TNNLS},
  author       = {Yang Zhao and Daokun Si and Jihong Pei and Xuan Yang},
  doi          = {10.1109/TNNLS.2022.3227296},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8386-8400},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Geodesic basis function neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bionic swarm control based on second-order communication
topology. <em>TNNLS</em>, <em>35</em>(6), 8373–8385. (<a
href="https://doi.org/10.1109/TNNLS.2022.3227292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose bionic swarm control based on second-order communication topology (SOCT) inspired by the migration of birds, which solves the difficulty in constructing communication topologies and high-computational complexity in controlling large-scale swarm systems. To realize bionic swarm control, there are three problems supposed to be solved. First, the adjacency matrix and the Laplacian matrix in traditional methods cannot be applied to SOCT directly, which should be redesigned. Second, sub-swarm systems formed based on 2-order communication topology (2-OCT) and independently distributed with each other also need to be put forward to reduce computational complexity. At last, the followers in 1-order communication topology (1-OCT) are set as the leaders of sub-swarm systems in 2-OCT. As a result, coupling in large-scale swarm systems would be reduced. The bionic swarm controller is designed through the backstepping method. In this case, the stability of bionic swarm controller is proven by the designed Lyapunov function. The simulations show the efficiency of the designed bionic swarm controller. And the tracking-containment control based on SOCT with 42 swarm members is realized.},
  archive      = {J_TNNLS},
  author       = {Dengxiu Yu and Hao Xu and Xiaoyue Jin and Qiang Yin and Zhen Wang and C. L. Philip Chen and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3227292},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8373-8385},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bionic swarm control based on second-order communication topology},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperspectral anomaly detection using reconstruction fusion
of quaternion frequency domain analysis. <em>TNNLS</em>, <em>35</em>(6),
8358–8372. (<a
href="https://doi.org/10.1109/TNNLS.2022.3227167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing techniques consider hyperspectral anomaly detection (HAD) as background modeling and anomaly search problems in the spatial domain. In this article, we model the background in the frequency domain and treat anomaly detection as a frequency-domain analysis problem. We illustrate that spikes in the amplitude spectrum correspond to the background, and a Gaussian low-pass filter performing on the amplitude spectrum is equivalent to an anomaly detector. The initial anomaly detection map is obtained by the reconstruction with the filtered amplitude and the raw phase spectrum. To further suppress the nonanomaly high-frequency detailed information, we illustrate that the phase spectrum is critical information to perceive the spatial saliency of anomalies. The saliency-aware map obtained by phase-only reconstruction (POR) is used to enhance the initial anomaly map, which realizes a significant improvement in background suppression. In addition to the standard Fourier transform (FT), we adopt the quaternion FT (QFT) for conducting multiscale and multifeature processing in a parallel way, to obtain the frequency domain representation of the hyperspectral images (HSIs). This helps with robust detection performance. Experimental results on four real HSIs validate the remarkable detection performance and excellent time efficiency of our proposed approach when compared to some state-of-the-art anomaly detection methods.},
  archive      = {J_TNNLS},
  author       = {Bing Tu and Xianchang Yang and Wei He and Jun Li and Antonio Plaza},
  doi          = {10.1109/TNNLS.2022.3227167},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8358-8372},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral anomaly detection using reconstruction fusion of quaternion frequency domain analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observer-based boundary stabilization of coupled semilinear
reaction–diffusion neural networks with spatially varying coefficients
via event-triggered controller. <em>TNNLS</em>, <em>35</em>(6),
8348–8357. (<a
href="https://doi.org/10.1109/TNNLS.2022.3227109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article is to propose an observer-based event-triggered Robin boundary control strategy for the exponential stabilization of the coupled semilinear reaction–diffusion neural networks with spatially varying coefficients. Toward this aim, we design an observer to estimate the value of system states by using some of these system values as the available measurement. An observer-based event-triggered boundary stabilizer is then presented to exponentially stabilize the considered systems with the Zeno behavior being excluded. Throughout this article, the main used method is backstepping, which yields an explicit expression of the control formulae. Moreover, we see that the proposed event-triggered boundary control scheme can ensure the desired level of control performance with fewer control law updates. A numerical example is finally given to illustrate the effectiveness of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Fudong Ge and YangQuan Chen},
  doi          = {10.1109/TNNLS.2022.3227109},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8348-8357},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based boundary stabilization of coupled semilinear Reaction–Diffusion neural networks with spatially varying coefficients via event-triggered controller},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-based intermittent formation control of multi-UAV
systems under deception attacks. <em>TNNLS</em>, <em>35</em>(6),
8336–8347. (<a
href="https://doi.org/10.1109/TNNLS.2022.3227101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of event-based intermittent formation control for multi-UAV systems subject to deception attacks. Compared to the available research studies on multi-UAV systems with continuous control strategy, the proposed intermittent control strategy saves a large amount of computation resources. An average method is introduced in developing the event-triggered mechanism (ETM) such that the amount of unexpected triggering events induced by uncertain disturbances is greatly reduced. Moreover, such a mechanism can further decrease the average data-releasing rate, thereby alleviating the burden of network bandwidth. Sufficient conditions for multi-UAV systems with deception attacks to achieve the predefined formation are obtained with the aid of Lyapunov stability theory. Finally, the validity of the proposed theoretical results is demonstrated via a simulation example.},
  archive      = {J_TNNLS},
  author       = {Tingting Yin and Zhou Gu and Ju H. Park},
  doi          = {10.1109/TNNLS.2022.3227101},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8336-8347},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-based intermittent formation control of multi-UAV systems under deception attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Complex relation embedding for scene graph generation.
<em>TNNLS</em>, <em>35</em>(6), 8321–8335. (<a
href="https://doi.org/10.1109/TNNLS.2022.3226871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an input image, scene graph generation (SGG) aims to generate comprehensive visual relationships between objects in the form of graphs. Recently, more attention to the design of complex networks and complicated strategies has been paid to the long tail issue caused by the imbalanced class distribution. However, most existing methods adopt the concatenated features of two objects in real space as the final relation representation for a given triplet. We mainly argue that such a simple concatenation may neglect the importance of complex interactions between objects, which results in the diversity of visual relations. In addition, the representation learning in real space is also inadequate to express this property. To alleviate these issues, we seamlessly incorporate Hermitian inner product into existing models to facilitate the generation of scene graphs by learning Relation Embedding in Complex space (CoRE). More specifically, we first introduce the concept of complex-valued representations for entities and then formulate the relation triplets with Hermitian inner product in complex space. Finally, we investigate the effect of utilizing only real component or both of Hermitian inner product on inferring more reasonable interaction between objects for scene graphs. Comprehensive experiments on two widely used benchmark datasets, Visual Genome (VG) and Open Image, demonstrate our effectiveness, superiority, and generalization on various metrics for biased or unbiased inference.},
  archive      = {J_TNNLS},
  author       = {Zheng Wang and Xing Xu and Yin Zhang and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2022.3226871},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8321-8335},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Complex relation embedding for scene graph generation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication-efficient and collision-free motion planning
of underwater vehicles via integral reinforcement learning.
<em>TNNLS</em>, <em>35</em>(6), 8306–8320. (<a
href="https://doi.org/10.1109/TNNLS.2022.3226776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion planning of underwater vehicles is regarded as a promising technique to make up the flexibility deficiency of underwater sensor networks (USNs). Nonetheless, the unique characteristics of underwater channel and environment make it challenging to achieve the above mission. This article is concerned with a communication-efficient and collision-free motion planning issue for underwater vehicles in fading channel and obstacle environment. We first develop a model-based integral reinforcement learning (IRL) estimator to predict the stochastic signal-to-noise ratio (SNR). With the estimated SNR, an integrated optimization problem for the codesign of communication efficiency and motion planning is constructed, in which the underwater vehicle dynamics, communication capacity, collision avoidance, and position control are all considered. In order to tackle this problem, a model-free IRL algorithm is designed to drive underwater vehicles to the desired position points while maximizing the communication capacity and avoiding the collision. It is worth mentioning that, the proposed motion planning solution in this article considers a realistic underwater communication channel, as well as a realistic dynamic model for underwater vehicles. Finally, simulation and experimental results are demonstrated to verify the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Jing Yan and Wenqiang Cao and Xian Yang and Cailian Chen and Xinping Guan},
  doi          = {10.1109/TNNLS.2022.3226776},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8306-8320},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Communication-efficient and collision-free motion planning of underwater vehicles via integral reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PHNNs: Lightweight neural networks via parameterized
hypercomplex convolutions. <em>TNNLS</em>, <em>35</em>(6), 8293–8305.
(<a href="https://doi.org/10.1109/TNNLS.2022.3226772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypercomplex neural networks have proven to reduce the overall number of parameters while ensuring valuable performance by leveraging the properties of Clifford algebras. Recently, hypercomplex linear layers have been further improved by involving efficient parameterized Kronecker products. In this article, we define the parameterization of hypercomplex convolutional layers and introduce the family of parameterized hypercomplex neural networks (PHNNs) that are lightweight and efficient large-scale models. Our method grasps the convolution rules and the filter organization directly from data without requiring a rigidly predefined domain structure to follow. PHNNs are flexible to operate in any user-defined or tuned domain, from 1-D to $n\text{D}$ regardless of whether the algebra rules are preset. Such a malleability allows processing multidimensional inputs in their natural domain without annexing further dimensions, as done, instead, in quaternion neural networks (QNNs) for 3-D inputs like color images. As a result, the proposed family of PHNNs operates with $1/n$ free parameters as regards its analog in the real domain. We demonstrate the versatility of this approach to multiple domains of application by performing experiments on various image datasets and audio datasets in which our method outperforms real and quaternion-valued counterparts. Full code is available at: https://github.com/eleGAN23/HyperNets .},
  archive      = {J_TNNLS},
  author       = {Eleonora Grassucci and Aston Zhang and Danilo Comminiello},
  doi          = {10.1109/TNNLS.2022.3226772},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8293-8305},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PHNNs: Lightweight neural networks via parameterized hypercomplex convolutions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The hierarchical discrete pursuit learning automaton: A
novel scheme with fast convergence and epsilon-optimality.
<em>TNNLS</em>, <em>35</em>(6), 8278–8292. (<a
href="https://doi.org/10.1109/TNNLS.2022.3226538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the early 1960s, the paradigm of learning automata (LA) has experienced abundant interest. Arguably, it has also served as the foundation for the phenomenon and field of reinforcement learning (RL). Over the decades, new concepts and fundamental principles have been introduced to increase the LA’s speed and accuracy. These include using probability updating functions, discretizing the probability space, and using the “Pursuit” concept. Very recently, the concept of incorporating “structure” into the ordering of the LA’s actions has improved both the speed and accuracy of the corresponding hierarchical machines, when the number of actions is large. This has led to the $\epsilon $ -optimal hierarchical continuous pursuit LA (HCPA). This article pioneers the inclusion of all the above-mentioned phenomena into a new single LA, leading to the novel hierarchical discretized pursuit LA (HDPA). Indeed, although the previously proposed HCPA is powerful, its speed has an impediment when any action probability is close to unity, because the updates of the components of the probability vector are correspondingly smaller when any action probability becomes closer to unity. We propose here, the novel HDPA, where we infuse the phenomenon of discretization into the action probability vector’s updating functionality, and which is invoked recursively at every stage of the machine’s hierarchical structure. This discretized functionality does not possess the same impediment, because discretization prohibits it. We demonstrate the HDPA’s robustness and validity by formally proving the $\epsilon $ -optimality by utilizing the moderation property. We also invoke the submartingale characteristic at every level, to prove that the action probability of the optimal action converges to unity as time goes to infinity. Apart from the new machine being $\epsilon $ -optimal, the numerical results demonstrate that the number of iterations required for convergence is significantly reduced for the HDPA, when compared to the state-of-the-art HCPA scheme.},
  archive      = {J_TNNLS},
  author       = {Rebekka Olsson Omslandseter and Lei Jiao and Xuan Zhang and Anis Yazidi and B. John Oommen},
  doi          = {10.1109/TNNLS.2022.3226538},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8278-8292},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The hierarchical discrete pursuit learning automaton: A novel scheme with fast convergence and epsilon-optimality},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new framework of collaborative learning for adaptive
metric distillation. <em>TNNLS</em>, <em>35</em>(6), 8266–8277. (<a
href="https://doi.org/10.1109/TNNLS.2022.3226569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new adaptive metric distillation approach that can significantly improve the student networks’ backbone features, along with better classification results. Previous knowledge distillation (KD) methods usually focus on transferring the knowledge across the classifier logits or feature structure, ignoring the excessive sample relations in the feature space. We demonstrated that such a design greatly limits performance, especially for the retrieval task. The proposed collaborative adaptive metric distillation (CAMD) has three main advantages: 1) the optimization focuses on optimizing the relationship between key pairs by introducing the hard mining strategy into the distillation framework; 2) it provides an adaptive metric distillation that can explicitly optimize the student feature embeddings by applying the relation in the teacher embeddings as supervision; and 3) it employs a collaborative scheme for effective knowledge aggregation. Extensive experiments demonstrated that our approach sets a new state-of-the-art in both the classification and retrieval tasks, outperforming other cutting-edge distillers under various settings.},
  archive      = {J_TNNLS},
  author       = {Hao Liu and Mang Ye and Yan Wang and Sanyuan Zhao and Ping Li and Jianbing Shen},
  doi          = {10.1109/TNNLS.2022.3226569},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8266-8277},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A new framework of collaborative learning for adaptive metric distillation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advanced optimal tracking control with stability guarantee
via novel value learning formulation. <em>TNNLS</em>, <em>35</em>(6),
8254–8265. (<a
href="https://doi.org/10.1109/TNNLS.2022.3226518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, to solve the optimal tracking control problem (OTCP) for discrete-time (DT) nonlinear systems, general value iteration (GVI) scheme and online value iteration (VI) algorithms with novel value function are discussed. First, the disadvantage of the traditional value function for the OTCP is presented and the novel value function is introduced. Second, we analyze the monotonicity and convergence of GVI and establish the admissibility condition of GVI to evaluate the admissibility of the current iterative control. Note that a novel approach is introduced to analyze the admissibility. Third, based on the attraction domain, improved control policies with online VI can be obtained by judging the location of the current tracking error and reference point. Finally, the stability of the online VI-based control system is guaranteed. Besides, we provide two simulation examples to show the performance of the proposed methods.},
  archive      = {J_TNNLS},
  author       = {Ding Wang and Junlong Wu and Mingming Ha and Mingming Zhao and Menghua Li and Junfei Qiao},
  doi          = {10.1109/TNNLS.2022.3226518},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8254-8265},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Advanced optimal tracking control with stability guarantee via novel value learning formulation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware poly(a) signal prediction model via deep
spatial–temporal neural networks. <em>TNNLS</em>, <em>35</em>(6),
8241–8253. (<a
href="https://doi.org/10.1109/TNNLS.2022.3226301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polyadenylation [Poly(A)] is an essential process during messenger RNA (mRNA) maturation in biological eukaryote systems. Identifying Poly(A) signals (PASs) from the genome level is the key to understanding the mechanism of translation regulation and mRNA metabolism. In this work, we propose a deep dual-dynamic context-aware Poly(A) signal prediction model, called multiscale convolution with self-attention networks (MCANet), to adaptively uncover the spatial–temporal contextual dependence information. Specifically, the model automatically learns and strengthens informative features from the temporalwise and the spatialwise dimension. The identity connectivity performs contextual feature maps of Poly(A) data by direct connections from previous layers to subsequent layers. Then, a fully parametric rectified linear unit (FP-RELU) with dual-dynamic coefficients is devised to make the training of the model easier and enhance the generalization ability. A cross-entropy loss (CL) function is designed to make the model focus on samples that are easy to misclassify. Experiments on different Poly(A) signals demonstrate the superior performance of the proposed MCANet, and an ablation study shows the effectiveness of the network design for the feature learning and prediction of Poly(A) signals.},
  archive      = {J_TNNLS},
  author       = {Yanbu Guo and Dongming Zhou and Pu Li and Chaoyang Li and Jinde Cao},
  doi          = {10.1109/TNNLS.2022.3226301},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8241-8253},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Context-aware Poly(A) signal prediction model via deep Spatial–Temporal neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep fuzzy min–max neural network: Analysis and design.
<em>TNNLS</em>, <em>35</em>(6), 8229–8240. (<a
href="https://doi.org/10.1109/TNNLS.2022.3226040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy min-max neural network (FMNN) is one kind of three-layer models based on hyperboxes that are constructed in a sequential way. Such a sequential mechanism inevitably leads to the input order and overlap region problem. In this study, we propose a deep FMNN (DFMNN) based on initialization and optimization operation to overcome these limitations. Initialization operation that can solve the input order problem is to design hyperboxes in a simultaneous way, and side parameters have been proposed to control the size of hyperboxes. Optimization operation that can eliminate overlap region problem is realized by means of deep layers, where the number of layers is immediately determined when the overlap among hyperboxes is eliminated. In the optimization process, each layer consists of three sections, namely, the partition section, combination section, and union section. The partition section aims to divide the hyperboxes into a nonoverlapping hyperbox set and an overlapping hyperbox set. The combination section eliminates the overlap problem of overlapping hyperbox set. The union section obtains the optimized hyperbox set in the current layer. DFMNN is evaluated based on a series of benchmark datasets. A comparative analysis illustrates that the proposed DFMNN model outperforms several models previously reported in the literature.},
  archive      = {J_TNNLS},
  author       = {Wei Huang and Mingxi Sun and Liehuang Zhu and Sung-Kwun Oh and Witold Pedrycz},
  doi          = {10.1109/TNNLS.2022.3226040},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8229-8240},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep fuzzy Min–Max neural network: Analysis and design},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning via plurality vote. <em>TNNLS</em>,
<em>35</em>(6), 8215–8228. (<a
href="https://doi.org/10.1109/TNNLS.2022.3225715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows collaborative clients to solve a machine-learning problem while preserving data privacy. Recent studies have tackled various challenges in federated learning, but the joint optimization of communication overhead, learning reliability, and deployment efficiency is still an open problem. To this end, we propose a new scheme named federated learning via plurality vote (FedVote). In each communication round of FedVote, clients transmit binary or ternary weights to the server with low communication overhead. The model parameters are aggregated via weighted voting to enhance the resilience against Byzantine attacks. When deployed for inference, the model with binary or ternary weights is resource-friendly to edge devices. Our results demonstrate that the proposed method can reduce quantization error and converges faster compared to the methods directly quantizing the model updates.},
  archive      = {J_TNNLS},
  author       = {Kai Yue and Richeng Jin and Chau-Wai Wong and Huaiyu Dai},
  doi          = {10.1109/TNNLS.2022.3225715},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8215-8228},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Federated learning via plurality vote},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic parameter noise-tolerant zeroing neural network
for time-varying quaternion matrix equation with applications.
<em>TNNLS</em>, <em>35</em>(6), 8205–8214. (<a
href="https://doi.org/10.1109/TNNLS.2022.3225309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a common and significant problem in the field of industrial information, the time-varying quaternion matrix equation (TV-QME) is considered in this article and addressed by an improved zeroing neural network (ZNN) method based on the real representation of the quaternion. In the light of an improved dynamic parameter (IDP) and an innovative activation function (IAF), a dynamic parameter noise-tolerant ZNN (DPNTZNN) model is put forward for solving the TV-QME. The presented IDP with the character of changing with the residual error and the proposed IAF with the remarkable performance can strongly enhance the convergence and robustness of the DPNTZNN model. Therefore, the DPNTZNN model possesses fast predefined-time convergence and superior robustness under different noise environments, which are theoretically analyzed in detail. Besides, the provided simulative experiments verify the advantages of the DPNTZNN model for solving the TV-QME, especially compared with other ZNN models. Finally, the DPNTZNN model is applied to image restoration, which further illustrates the practicality of the DPNTZNN model.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Yuanfang Zhang and Wenqian Huang and Lei Jia and Xieping Gao},
  doi          = {10.1109/TNNLS.2022.3225309},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8205-8214},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A dynamic parameter noise-tolerant zeroing neural network for time-varying quaternion matrix equation with applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed cooperative quantum learning for discrete-time
multiagent source exploration with information prompts. <em>TNNLS</em>,
<em>35</em>(6), 8190–8204. (<a
href="https://doi.org/10.1109/TNNLS.2022.3225184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In light of optimization theory and swarm evolutionary schemes, under multiple single-integrator mobile agents equipped with sensors and prompters, this article addresses a discrete-time multiagent source exploration problem with information prompts. Regarding information prompts as constraints on the unknown target, by virtue of penalty function skills (PFSs) and sequential unconstrained minimization techniques (SUMTs), the agents are driven toward the source under the guidance of the control strategy. In two cases of available and unavailable gradient information, a quantum potential well, an average optimal position estimator (AOPE), and a global optimal position estimator (GOPE) are introduced into swarm evolutionary schemes with a periodically oscillating weight, such that distributed cooperative quantum learning (DCQL) policy is proposed as a control strategy under communication restrictions, where AOPE and GOPE are developed relying on distributed consensus theory. In particular, when the gradient is unavailable, we put forth an adaptive generalized Bernstein neural network (AGBNN) to replace it based on excellent properties of Bernstein polynomials and adaptive approaches. Further, a performance analysis for the proposed policy is executed on the convergence and computational complexity, which ensures the accuracy and efficiency of the source exploration in theory. Ultimately, a simulation test is carried out, and the results validate the practicability and effectiveness of the offered method.},
  archive      = {J_TNNLS},
  author       = {Rui-Guo Li and Huai-Ning Wu},
  doi          = {10.1109/TNNLS.2022.3225184},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8190-8204},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed cooperative quantum learning for discrete-time multiagent source exploration with information prompts},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonsingular gradient descent algorithm for interval type-2
fuzzy neural network. <em>TNNLS</em>, <em>35</em>(6), 8176–8189. (<a
href="https://doi.org/10.1109/TNNLS.2022.3225181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval type-2 fuzzy neural network (IT2FNN) is widely used to model nonlinear systems. Unfortunately, the gradient descent-based IT2FNN with uncertain variances always suffers from low convergence speed due to its inherent singularity. To cope with this problem, a nonsingular gradient descent algorithm (NSGDA) is developed to update IT2FNN in this article. First, the widths of type-2 fuzzy rules are transformed into root inverse variances (RIVs) that always satisfy the sufficient condition of differentiability. Second, the singular RIVs are reformulated by the nonsingular Shapley-based matrices associated with type-2 fuzzy rules. It averts the convergence stagnation caused by zero derivatives of singular RIVs, thereby sustaining the gradient convergence. Third, an integrated-form update strategy (IUS) is designed to obtain the derivatives of parameters, including RIVs, centers, weight coefficients, deviations, and proportionality coefficient of IT2FNN. These parameters are packed into multiple subvariable matrices, which are capable to accelerate gradient convergence using parallel calculation instead of sequence iteration. Finally, the experiments showcase that the proposed NSGDA-based IT2FNN can improve the convergence speed through the improved learning algorithm.},
  archive      = {J_TNNLS},
  author       = {Honggui Han and Chenxuan Sun and Xiaolong Wu and Hongyan Yang and Junfei Qiao},
  doi          = {10.1109/TNNLS.2022.3225181},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8176-8189},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonsingular gradient descent algorithm for interval type-2 fuzzy neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multitask learning for joint diagnosis of multiple mental
disorders in resting-state fMRI. <em>TNNLS</em>, <em>35</em>(6),
8161–8175. (<a
href="https://doi.org/10.1109/TNNLS.2022.3225179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facing the increasing worldwide prevalence of mental disorders, the symptom-based diagnostic criteria struggle to address the urgent public health concern due to the global shortfall in well-qualified professionals. Thanks to the recent advances in neuroimaging techniques, functional magnetic resonance imaging (fMRI) has surfaced as a new solution to characterize neuropathological biomarkers for detecting functional connectivity (FC) anomalies in mental disorders. However, the existing computer-aided diagnosis models for fMRI analysis suffer from unstable performance on large datasets. To address this issue, we propose an efficient multitask learning (MTL) framework for joint diagnosis of multiple mental disorders using resting-state fMRI data. A novel multiobjective evolutionary clustering algorithm is presented to group regions of interests (ROIs) into different clusters for FC pattern analysis. On the optimal clustering solution, the multicluster multigate mixture-of-expert model is used for the final classification by capturing the highly consistent feature patterns among related diagnostic tasks. Extensive simulation experiments demonstrate that the performance of the proposed framework is superior to that of the other state-of-the-art methods. Moreover, the potential for practical application of the framework is also validated in terms of limited computational resources, real-time analysis, and insufficient training data. The proposed model can identify the remarkable interpretative biomarkers associated with specific mental disorders for clinical interpretation analysis.},
  archive      = {J_TNNLS},
  author       = {Zhi-An Huang and Rui Liu and Zexuan Zhu and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2022.3225179},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8161-8175},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitask learning for joint diagnosis of multiple mental disorders in resting-state fMRI},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite-time synchronization of neural networks with
proportional delays for RGB-d image protection. <em>TNNLS</em>,
<em>35</em>(6), 8149–8160. (<a
href="https://doi.org/10.1109/TNNLS.2022.3225164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the depth information of images facilitates the analysis of the spatial distance of objects in computer vision applications, it is necessary to protect the image depth information. Thus this article proposes a novel red-green-blue-depth (RGB-D) image protection algorithm, which is implemented with the finite-time synchronization (FTS) of neural networks (NNs) with proportional delays via the quantized intermittent control to derive the system synchronization criterion based on Lyapunov stability theory. The performance of RGB-D image protection depends on the synchronization error of the system by driving the system sequence to encrypt the RGB-D image and responding to the system sequence to decrypt the encrypted image. Subsequently, the validity of the proposed criteria is verified by simulation examples, and the practical application of RGB-D image protection is verified.},
  archive      = {J_TNNLS},
  author       = {Wenqiang Yang and Junjian Huang and Xing He and Shiping Wen and Tingwen Huang},
  doi          = {10.1109/TNNLS.2022.3225164},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8149-8160},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time synchronization of neural networks with proportional delays for RGB-D image protection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal image classification by multiview latent pattern
extraction, selection, and correlation. <em>TNNLS</em>, <em>35</em>(6),
8134–8148. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large amount of data available in the modern big data era opens new opportunities to expand our knowledge by integrating information from heterogeneous sources. Multiview learning has recently achieved tremendous success in deriving complementary information from multiple data modalities. This article proposes a framework called multiview latent space projection (MVLSP) to integrate features extracted from multiple sources in a discriminative way to facilitate binary and multiclass classifications. Our approach is associated with three innovations. First, most existing multiview learning algorithms promote pairwise consistency between two views and do not have a natural extension to applications with more than two views. MVLSP finds optimum mappings from a common latent space to match the feature space in each of the views. As the matching is performed on a view-by-view basis, the framework can be readily extended to multiview applications. Second, feature selection in the common latent space can be readily achieved by adding a class view, which matches the latent space representations of training samples with their corresponding labels. Then, high-order view correlations are extracted by considering feature-label correlations. Third, a technique is proposed to optimize the integration of different latent patterns based on their correlations. The experimental results on the prostate image dataset demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jianghong Ma and Weixuan Kou and Mingquan Lin and Carmen C. M. Cho and Bernard Chiu},
  doi          = {10.1109/TNNLS.2022.3224946},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8134-8148},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multimodal image classification by multiview latent pattern extraction, selection, and correlation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fault detection of urban wastewater treatment process based
on combination of deep information and transformer network.
<em>TNNLS</em>, <em>35</em>(6), 8124–8133. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the hot issues of concerns during modern social development, the wastewater treatment process is acknowledged to be a process with complex biochemical reactions and susceptible to an external environment, featuring strong nonlinear and time correlation characteristics, which are difficult for traditional mechanism-based models to tackle. For many classical data-driven fault detection methods, a complete retraining process is necessary to monitor every new fault, and most of the current neural network-based strategies rarely achieve satisfactory monitoring accuracy or robustness either. Giving full consideration to the aforementioned problems, this article takes advantage of position encoding, residual connection, and multihead attention mechanism embedded in the Transformer structure to establish an effective and efficient wastewater treatment process fault detection model, where offline modeling and online monitoring are performed successively to achieve accurate detection of the faults. In the experimental part, the advantages of the proposed method are strongly verified through the simulation monitoring of 27 faults on the benchmark simulation model 1 (BSM1), where the false alarm rate (FAR) and miss alarm rate (MAR) of the established method are proved to be significantly lower than those of the compared state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Chang Peng and Meng FanChao},
  doi          = {10.1109/TNNLS.2022.3224804},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8124-8133},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fault detection of urban wastewater treatment process based on combination of deep information and transformer network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Design of fully spectral CNNs for efficient FPGA-based
acceleration. <em>TNNLS</em>, <em>35</em>(6), 8111–8123. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing convolutional layers in the frequency domain using fast Fourier transformation (FFT) has been demonstrated to be effective in reducing the computational complexity of convolutional neural networks (CNNs). Nevertheless, the main challenge of this approach lies in the frequent and repeated transformations between the spatial and frequency domains due to the absence of nonlinear functions in the spectral domain, as such it makes the benefit less attractive for low-latency inference, especially on embedded platforms. To overcome the drawbacks in the existing FFT-based convolution, we propose a fully spectral CNN using a novel spectral-domain adaptive rectified linear unit (ReLU) layer, which completely removes the compute-intensive transformations between the spatial and frequency domains within the network. The proposed fully spectral CNNs maintain the nonlinearity of the spatial CNNs while taking into account the hardware efficiency. We then propose a deeply customized and compute-efficient hardware architecture to accelerate the fully spectral CNN inference on field programmable gate array (FPGA). Different hardware optimizations, such as spectral-domain intralayer and interlayer pipeline techniques, are introduced to further improve the performance of throughput. To achieve a load-balanced pipeline, a design space exploration (DSE) framework is proposed to optimize the resource allocation between hardware modules according to the resource constraints. On an Intel’s Arria 10 SX160 FPGA, our optimized accelerator achieves a throughput of 204 Gop/s with 80% of compute efficiency. Compared with the state-of-the-art spatial and FFT-based implementations on the same device, our accelerator is $4\times \sim \,\,6.6\times $ and $3.0\times \sim \,\,4.4\times $ faster while maintaining a similar level of accuracy across different benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Shuanglong Liu and Hongxiang Fan and Wayne Luk},
  doi          = {10.1109/TNNLS.2022.3224779},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8111-8123},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design of fully spectral CNNs for efficient FPGA-based acceleration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incomplete data meets uncoupled case: A challenging task of
multiview clustering. <em>TNNLS</em>, <em>35</em>(6), 8097–8110. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multiview clustering (IMC) methods have achieved remarkable progress by exploring the complementary information and consensus representation of incomplete multiview data. However, to our best knowledge, none of the existing methods attempts to handle the uncoupled and incomplete data simultaneously, which affects their generalization ability in real-world scenarios. For uncoupled incomplete data, the unclear and partial cross-view correlation introduces the difficulty to explore the complementary information between views, which results in the unpromising clustering performance for the existing multiview clustering methods. Besides, the presence of hyperparameters limits their applications. To fill these gaps, a novel uncoupled IMC (UIMC) method is proposed in this article. Specifically, UIMC develops a joint framework for feature inferring and recoupling. The high-order correlations of all views are explored by performing a tensor singular value decomposition (t-SVD)-based tensor nuclear norm (TNN) on recoupled and inferred self-representation matrices. Moreover, all hyperparameters of the UIMC method are updated in an exploratory manner. Extensive experiments on six widely used real-world datasets have confirmed the superiority of the proposed method in handling the uncoupled incomplete multiview data compared with the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Jia-Qi Lin and Xiang-Long Li and Man-Sheng Chen and Chang-Dong Wang and Haizhang Zhang},
  doi          = {10.1109/TNNLS.2022.3224748},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8097-8110},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incomplete data meets uncoupled case: A challenging task of multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronization of neural networks involving
distributed-delay coupling: A distributed-delay differential
inequalities approach. <em>TNNLS</em>, <em>35</em>(6), 8086–8096. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address the synchronization issue for coupled neural networks (CNNs) with mixed couplings by way of the delayed impulsive control, where the delay is distributed. Particularly, mixed couplings comprise the current-state coupling and the distributed-delay coupling, where influences on network connections caused by the past information of CNNs over a certain period are considered. First, we propose a novel array of delayed impulsive differential inequalities involving distributed-delay-dependent impulses, where distributed delays can be relatively larger. Second, we apply such delayed inequalities to analyze the problem of synchronization for CNNs with two different topologies. Sufficient criteria and distributed-delay-dependent impulsive controller are derived thereby. Furthermore, using techniques of matrix decomposition, several low-dimensional criteria are set out, which are appropriate for applications of large scale CNNs. Finally, a numerical example of CNNs with both the current-state coupling and the distributed-delay coupling involving three cases, are exhibited to exemplify the validity and the efficiency of the obtained theoretical results.},
  archive      = {J_TNNLS},
  author       = {Xiaoyu Zhang and Chuandong Li and Hongfei Li and Jing Xu},
  doi          = {10.1109/TNNLS.2022.3224393},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8086-8096},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of neural networks involving distributed-delay coupling: A distributed-delay differential inequalities approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new look and convergence rate of federated multitask
learning with laplacian regularization. <em>TNNLS</em>, <em>35</em>(6),
8075–8085. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-independent and identically distributed (non-IID) data distribution among clients is considered as the key factor that degrades the performance of federated learning (FL). Several approaches to handle non-IID data, such as personalized FL and federated multitask learning (FMTL), are of great interest to research communities. In this work, first, we formulate the FMTL problem using Laplacian regularization to explicitly leverage the relationships among the models of clients for multitask learning. Then, we introduce a new view of the FMTL problem, which, for the first time, shows that the formulated FMTL problem can be used for conventional FL and personalized FL. We also propose two algorithms $\textsf {FedU}$ and decentrali- zed $\textsf {FedU}$ ( $\textsf {dFedU}$ ) to solve the formulated FMTL problem in communication-centralized and decentralized schemes, respectively. Theoretically, we prove that the convergence rates of both algorithms achieve linear speedup for strongly convex and sublinear speedup of order $1/2$ for nonconvex objectives. Experimentally, we show that our algorithms outperform the conventional algorithm FedAvg, FedProx, SCAFFOLD, and AFL in FL settings, MOCHA in FMTL settings, as well as pFedMe and Per-FedAvg in personalized FL settings.},
  archive      = {J_TNNLS},
  author       = {Canh T. Dinh and Tung T. Vu and Nguyen H. Tran and Minh N. Dao and Hongyu Zhang},
  doi          = {10.1109/TNNLS.2022.3224252},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8075-8085},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A new look and convergence rate of federated multitask learning with laplacian regularization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-rank graph completion-based incomplete multiview
clustering. <em>TNNLS</em>, <em>35</em>(6), 8064–8074. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to reduce the negative effect of missing data on clustering, incomplete multiview clustering (IMVC) has become an important research content in machine learning. At present, graph-based methods are widely used in IMVC, but these methods still have some defects. First, some of the methods overlook potential relationships across views. Second, most of the methods depend on local structure information and ignore the global structure information. Third, most of the methods cannot use both global structure information and potential information across views to adaptively recover the incomplete relationship structure. To address the above issues, we propose a unified optimization framework to learn reasonable affinity relationships, called low-rank graph completion-based IMVC (LRGR_IMVC). 1) Our method introduces adaptive graph embedding to effectively explore the potential relationship among views; 2) we append a low-rank constraint to adequately exploit the global structure information among views; and 3) this method unites related information within views, potential information across views, and global structure information to adaptively recover the incomplete graph structure and obtain complete affinity relationships. Experimental results on several commonly used datasets show that the proposed method achieves better clustering performance significantly than some of the most advanced methods.},
  archive      = {J_TNNLS},
  author       = {Jinrong Cui and Yulu Fu and Cheng Huang and Jie Wen},
  doi          = {10.1109/TNNLS.2022.3224058},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8064-8074},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low-rank graph completion-based incomplete multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed optimal attitude synchronization control of
multiple QUAVs via adaptive dynamic programming. <em>TNNLS</em>,
<em>35</em>(6), 8053–8063. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a distributed optimal attitude synchronization control strategy for multiple quadrotor unmanned aerial vehicles (QUAVs) through the adaptive dynamic programming (ADP) algorithm. The attitude systems of QUAVs are modeled as affine nominal systems subject to parameter uncertainties and external disturbances. Considering attitude constraints in complex flying environments, a one-to-one mapping technique is utilized to transform the constrained systems into equivalent unconstrained systems. An improved nonquadratic cost function is constructed for each QUAV, which reflects the requirements of robustness and the constraints of control input simultaneously. To overcome the issue that the persistence of excitation (PE) condition is difficult to meet, a novel tuning rule of critic neural network (NN) weights is developed via the concurrent learning (CL) technique. In terms of the Lyapunov stability theorem, the stability of the closed-loop system and the convergence of critic NN weights are proved. Finally, simulation results on multiple QUAVs show the effectiveness of the proposed control strategy.},
  archive      = {J_TNNLS},
  author       = {Zijie Guo and Hongyi Li and Hui Ma and Wei Meng},
  doi          = {10.1109/TNNLS.2022.3224029},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8053-8063},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed optimal attitude synchronization control of multiple QUAVs via adaptive dynamic programming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous interaction modeling with reduced accumulated
error for multiagent trajectory prediction. <em>TNNLS</em>,
<em>35</em>(6), 8040–8052. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamical complex systems composed of interactive heterogeneous agents are prevalent in the world, including urban traffic systems and social networks. Modeling the interactions among agents is the key to understanding and predicting the dynamics of the complex system, e.g., predicting the trajectories of traffic participants in the city. Compared with interaction modeling in homogeneous systems such as pedestrians in a crowded scene, heterogeneous interaction modeling is less explored. Worse still, the error accumulation problem becomes more severe since the interactions are more complex. To tackle the two problems, this article proposes heterogeneous interaction modeling with reduced accumulated error (HIMRAE) for multiagent trajectory prediction. Based on the historical trajectories, our method infers the dynamic interaction graphs among agents, featured by directed interacting relations and interacting effects. A heterogeneous attention mechanism (HAM) is defined on the interaction graphs for aggregating the influence from heterogeneous neighbors to the target agent. To alleviate the error accumulation problem, this article analyzes the error sources from the spatial and temporal perspectives, and proposes to introduce the graph entropy and the mixup training strategy for reducing the two types of errors, respectively. Our method is examined on three real-world datasets containing heterogeneous agents, and the experimental results validate the superiority of our method.},
  archive      = {J_TNNLS},
  author       = {Siyuan Chen and Jiahai Wang},
  doi          = {10.1109/TNNLS.2022.3224007},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8040-8052},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heterogeneous interaction modeling with reduced accumulated error for multiagent trajectory prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network control of underactuated surface vehicles
with prescribed trajectory tracking performance. <em>TNNLS</em>,
<em>35</em>(6), 8026–8039. (<a
href="https://doi.org/10.1109/TNNLS.2022.3223666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the fast and accurate trajectory tracking control problem for a sort of underactuated surface vehicle under model uncertainties and environmental disturbances. A novel neural networks (NNs)-based prescribed performance control strategy is proposed to solve the problem. In the control design, a new type of performance function is constructed which provides a way to predefine the settling time and accuracy, straightforward. Then, a pair of barrier functions are employed to combat not only the position error but also the virtual control input. This evades the possible singularity or discontinuity of the control solution. Next, an initialization technique is exploited, removing the requirement for the initial condition of the control system. Finally, two NNs are employed to deal with the unknown ship nonlinearities. The performance analysis not only demonstrates the effectiveness of the proposed approach but also reveals its robustness against disturbances and unknown reference trajectory derivatives. There is, thus, no need to acquire such knowledge or employ specialized tools to handle disturbances. The theoretical findings are illustrated by a simulation study.},
  archive      = {J_TNNLS},
  author       = {Jin-Xi Zhang and Tao Yang and Tianyou Chai},
  doi          = {10.1109/TNNLS.2022.3223666},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8026-8039},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network control of underactuated surface vehicles with prescribed trajectory tracking performance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPiForest: An anomaly detecting algorithm using space
partition constructed by probability density-based inverse sampling.
<em>TNNLS</em>, <em>35</em>(6), 8013–8025. (<a
href="https://doi.org/10.1109/TNNLS.2022.3223342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SPiForest, a new isolation-based approach to outlier detection, constructs iTrees on the space containing all attributes by probability density-based inverse sampling. Most existing iForest (iF)-based approaches can precisely and quickly detect outliers scattering around one or more normal clusters. However, the performance of these methods seriously decreases when facing outliers whose nature “few and different” disappears in subspace (e.g., anomalies surrounded by normal samples). To solve this problem, SPiForest is proposed, which is different from existing approaches. First, SPiForest uses the principal component analysis (PCA) to find principal components and estimate each component’s probability density function (pdf). Second, SPiForest utilizes the inv-pdf, which is inversely proportional to the pdf estimated from the given dataset, to generate support points in the space containing all attributes. Third, the hyperplane decided by these support points is used to isolate the outliers in the space. Next, these steps are repeated to build an iTree. Finally, many iTrees construct a forest for outlier detection. SPiForest provides two benefits: 1) it isolates outliers with fewer hyperplanes, which significantly improves the accuracy and 2) it effectively detects the outliers whose nature “few and different” disappears in subspace. Comparative analyses and experiments show that the SPiForest achieves a significant improvement in terms of area under the curve (AUC) when compared with the state-of-the-art methods. Specifically, our method improves by at most 17.7% on AUC when compared to iF-based algorithms.},
  archive      = {J_TNNLS},
  author       = {Xiansheng Yang and Yuan Zhuang and Min Shi and Xiaoxiang Cao and Dong Chen and Yufei Tang},
  doi          = {10.1109/TNNLS.2022.3223342},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {8013-8025},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SPiForest: An anomaly detecting algorithm using space partition constructed by probability density-based inverse sampling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale dynamic curvelet scattering network.
<em>TNNLS</em>, <em>35</em>(6), 7999–8012. (<a
href="https://doi.org/10.1109/TNNLS.2022.3223212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The feature representation learning process greatly determines the performance of networks in classification tasks. By combining multiscale geometric tools and networks, better representation and learning can be achieved. However, relatively fixed geometric features and multiscale structures are always used. In this article, we propose a more flexible framework called the multiscale dynamic curvelet scattering network (MSDCCN). This data-driven dynamic network is based on multiscale geometric prior knowledge. First, multiresolution scattering and multiscale curvelet features are efficiently aggregated in different levels. Then, these features can be reused in networks flexibly and dynamically, depending on the multiscale intervention flag. The initial value of this flag is based on the complexity assessment, and it is updated according to feature sparsity statistics on the pretrained model. With the multiscale dynamic reuse structure, the feature representation learning process can be improved in the following training process. Also, multistage fine-tuning can be performed to further improve the classification accuracy. Furthermore, a novel multiscale dynamic curvelet scattering module, which is more flexible, is developed to be further embedded into other networks. Extensive experimental results show that better classification accuracies can be achieved by MSDCCN. In addition, necessary evaluation experiments have been performed, including convergence analysis, insight analysis, and adaptability analysis.},
  archive      = {J_TNNLS},
  author       = {Jie Gao and Licheng Jiao and Xu Liu and Lingling Li and Puhua Chen and Fang Liu and Shuyuan Yang},
  doi          = {10.1109/TNNLS.2022.3223212},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7999-8012},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiscale dynamic curvelet scattering network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weather translation via weather-cue transferring.
<em>TNNLS</em>, <em>35</em>(6), 7988–7998. (<a
href="https://doi.org/10.1109/TNNLS.2022.3223081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the weather translation task is proposed, which aims to transfer the weather type of the image from one category to another. Weather translation is a complicated image weather editing task that changes the weather cue of an image across multiple weather types, and it is related to image restoration, image editing, and photographic style transfer tasks. Although lots of approaches have been developed for traditional image translation and restoration tasks, only few of them are capable of handling the multicategory weather types problem with a single network due to the rich categories and highly complicated semantic structures of weather images. Especially, it is difficult to change the weather cue while preserving the weather-invariant area. To solve these issues, we developed a weather-cue guided multidomain translation approach based on StarGAN v2, termed WeatherGAN. In the proposed model, the core generator is redesigned to transfer the weather cue according to the target weather type. The weather segmentation module is first introduced to acquire the weather semantic structure of images in a weakly supervised multitask manner. In addition, a weather clues module is presented to reprocess the weather segmentation into a weather-specific clues map, which identifies the weather-invariant and weather-cue areas clearly. Extensive studies and evaluations show that our approach outperforms the state of the art. The data and source code will be publicly available soon after the manuscript is accepted.},
  archive      = {J_TNNLS},
  author       = {Xuelong Li and Chen Li and Kai Kou and Bin Zhao},
  doi          = {10.1109/TNNLS.2022.3223081},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7988-7998},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weather translation via weather-cue transferring},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Joint anchor graph embedding and discrete feature scoring
for unsupervised feature selection. <em>TNNLS</em>, <em>35</em>(6),
7974–7987. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of existing unsupervised feature selection (UFS) methods heavily relies on the assumption that the intrinsic relationships among original high-dimensional (HD) data samples exist in the discriminative low-dimension (LD) subspace. However, previous UFS methods commonly construct pairwise graphs and employ $\ell _{2,1}$ -norm regularization to severally preserve the local structure and calculate the score of features, which is computationally complex and easy to get stuck into local optimum, so that those approaches cannot be applied in dealing with large-scale datasets in practice. To overcome this challenge, we propose a novel UFS method, in which a novel anchor graph embedding paradigm is designed to extract the local neighborhood relationships among data samples by reducing the computational complexity of graph construction to be linear in the number of data. Moreover, to improve the optimality of selected features as well as the performance of downstream tasks, we propose a discrete feature scoring mechanism, which imposes orthogonal $\ell _{2,0}$ -norm constraints on learned projections, in order to enhance the distinction of feature scores as well as reduce the probability of falling into local optimum. In addition, solving the proposed nonconvex and nonsmooth NP-hard problem is challenging, and we present an efficient optimization algorithm to address it and acquire a closed-form solution of the transformation matrix. Extensive experiments demonstrate the effectiveness and efficiency of the proposed UFS by comparison with several state-of-the-art approaches to clustering and image segmentation tasks.},
  archive      = {J_TNNLS},
  author       = {Zheng Wang and Dongming Wu and Rong Wang and Feiping Nie and Fei Wang},
  doi          = {10.1109/TNNLS.2022.3222466},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7974-7987},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint anchor graph embedding and discrete feature scoring for unsupervised feature selection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Command filtered neuroadaptive fault-tolerant control for
nonlinear systems with input saturation and unknown control direction.
<em>TNNLS</em>, <em>35</em>(6), 7963–7973. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the tracking control of a class of nonlinear systems with input saturation, subject to nonaffine faults and unknown control direction. A fault-tolerant command filtered control (CFC) method based on adaptive neural networks (NNs) is proposed for this kind of nonlinear system. First, the combination of CFC and error compensation overcomes the “explosion of complexity” issue and alleviates the impact of filter errors. Then, a set of radial basis function NNs is constructed to approximate the unknown nonlinear items containing the nonaffine fault function. Additionally, the issue of unknown control direction in the system is effectively resolved by using Nussbaum gain technology. It is proven that the designed controller can ensure that all signals in the closed-loop system are bounded and convergent, and the upper bound of the absolute value of system tracking error is given. Finally, three comparative simulation results are illustrated to show the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Shuai Cheng and Bin Xin and Qing Wang and Jie Chen and Fang Deng},
  doi          = {10.1109/TNNLS.2022.3222464},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7963-7973},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Command filtered neuroadaptive fault-tolerant control for nonlinear systems with input saturation and unknown control direction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive robust control of uncertain euler–lagrange systems
using gaussian processes. <em>TNNLS</em>, <em>35</em>(6), 7949–7962. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel adaptive robust control approach based on Gaussian processes (GPs) for the high-precision tracking problem of uncertain Euler–Lagrange (EL) systems with time-varying external disturbances. Given a prior dynamic model, the GP regression (GPR) technique is employed to obtain a nonparametric data-based uncertainty model, including its probabilistic confidence intervals. Based on the adaptive sliding mode control (ASMC) framework, the posterior means of GPs are utilized for dynamic compensation, whereas the posterior variances are applied to adjust the feedback gains. This proposed control strategy is robust against significant system uncertainty with low feedback gains. A novel adaptive law for updating hyperparameters based on tracking error feedback is presented, thereby improving the performance of both tracking control and GP modeling simultaneously. Compared to existing likelihood-based optimization methods, this hyperparameter adaptive law enables data-efficient and fast uncertainty learning for control applications. The proposed control strategy guarantees the semiglobal asymptotic convergence to zero tracking error with a specified probability. Simulations using an underwater robot model demonstrate that the utilization of GPs and hyperparameter adaptive law significantly improves the performance of tracking control and uncertainty learning.},
  archive      = {J_TNNLS},
  author       = {Yongxu He and Yuxin Zhao},
  doi          = {10.1109/TNNLS.2022.3222405},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7949-7962},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive robust control of uncertain Euler–Lagrange systems using gaussian processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eliminating negative word similarities for measuring
document distances: A thoroughly empirical study on word mover’s
distance. <em>TNNLS</em>, <em>35</em>(6), 7936–7948. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document distance is a fundamental yet significant research topic in the information retrieval community, and its accuracy dominates the performance of many text retrieval applications. Beyond the Bag-of-Words (BoW) model, the Word Mover’s Distance (WMD) semantically defines the distance between documents as the minimum cost (i.e., measured by word similarities of embeddings) required to transport the words from one document to another, and it has been proven to be superior by ${k}$ -nearest neighbor classification. In this article, we thoroughly study the characteristics of WMD and its relaxed versions, e.g., Relaxed WMD (RWMD) and Iterative Constrained Transfers (ICT), in various scenarios. Specifically, we concentrate on the problem of negative word similarity: the WMD family leverages all word similarities, however, most of them are meaningless, resulting in negative effects for measuring document distances. To remedy this problem, we propose Informative Similarity Filter (ISF), which retains a very small percentage of top word similarities and fixes the others as the same lower similarity. Built on it, we propose a greedy optimization (GOM) for WMD, an accurate approximation to WMD. We theoretically analyze that ISF-GOM is more applicable for relatively longer documents. Extensive experiments have been conducted to validate: 1) the problem of RWMD; 2) the effectiveness of ISF-GOM; and 3) the consistence of our analysis of ISF-GOM. Our codes and datasets are available at https://github.com/BoCheng-96/ISF-GOM .},
  archive      = {J_TNNLS},
  author       = {Bo Cheng and Ximing Li and Yi Chang},
  doi          = {10.1109/TNNLS.2022.3222336},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7936-7948},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Eliminating negative word similarities for measuring document distances: A thoroughly empirical study on word mover’s distance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse adversarial diversity learning for network ensemble.
<em>TNNLS</em>, <em>35</em>(6), 7923–7935. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network ensemble aims to obtain better results by aggregating the predictions of multiple weak networks, in which how to keep the diversity of different networks plays a critical role in the training process. Many existing approaches keep this kind of diversity either by simply using different network initializations or data partitions, which often requires repeated attempts to pursue a relatively high performance. In this article, we propose a novel inverse adversarial diversity learning (IADL) method to learn a simple yet effective ensemble regime, which can be easily implemented in the following two steps. First, we take each weak network as a generator and design a discriminator to judge the difference between the features extracted by different weak networks. Second, we present an inverse adversarial diversity constraint to push the discriminator to cheat generators that all the resulting features of the same image are too similar to distinguish each other. As a result, diverse features will be extracted by these weak networks through a min–max optimization. What is more, our method can be applied to a variety of tasks, such as image classification and image retrieval, by applying a multitask learning objective function to train all these weak networks in an end-to-end manner. We conduct extensive experiments on the CIFAR-10, CIFAR-100, CUB200-2011, and CARS196 datasets, in which the results show that our method significantly outperforms most of the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Sanping Zhou and Jinjun Wang and Le Wang and Xingyu Wan and Siqi Hui and Nanning Zheng},
  doi          = {10.1109/TNNLS.2022.3222263},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7923-7935},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Inverse adversarial diversity learning for network ensemble},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-attention-based deep reinforcement learning approach
for AGV dispatching systems. <em>TNNLS</em>, <em>35</em>(6), 7911–7922.
(<a href="https://doi.org/10.1109/TNNLS.2022.3222206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated guided vehicle (AGV) dispatching problem is to develop a rule to assign transportation tasks to certain vehicles. This article proposes a new deep reinforcement learning approach with a self-attention mechanism to dynamically dispatch the tasks to AGV. The AGV dispatching system is modeled as a less complicated Markov decision process (MDP) using vehicle-initiated rules to dispatch a workcenter to an idle AGV. In order to deal with the highly dynamical environment, the self-attention mechanism is introduced to calculate the importance of different information. The invalid action masking technique is performed to alleviate false actions. A multimodal structure is employed to mix the features of various sources. Comparative experiments are performed to show the effectiveness of the proposed method. The properties of the learned policies are also investigated under different environment settings. It is discovered that the policies explore and learn the properties of different systems, and also smooth the traffic congestion. Under certain environment settings, the policy converges to a heuristic rule that assigns the idle AGV to the workcenter with the shortest queue length, which shows the adaptiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Qinglai Wei and Yutian Yan and Jie Zhang and Jun Xiao and Cong Wang},
  doi          = {10.1109/TNNLS.2022.3222206},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7911-7922},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A self-attention-based deep reinforcement learning approach for AGV dispatching systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReSmooth: Detecting and utilizing OOD samples when training
with data augmentation. <em>TNNLS</em>, <em>35</em>(6), 7899–7910. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation (DA) is a widely used technique for enhancing the training of deep neural networks. Recent DA techniques which achieve state-of-the-art performance always meet the need for diversity in augmented training samples. However, an augmentation strategy that has a high diversity usually introduces out-of-distribution (OOD) augmented samples and these samples consequently impair the performance. To alleviate this issue, we propose ReSmooth, a framework that first detects OOD samples in augmented samples and then leverages them. To be specific, we first use a Gaussian mixture model (GMM) to fit the loss distribution of both the original and augmented samples and accordingly split these samples into in-distribution (ID) samples and OOD samples. Then we start a new training where ID and OOD samples are incorporated with different smooth labels. By treating ID samples and OOD samples unequally, we can make better use of the diverse augmented data. Furthermore, we incorporate our ReSmooth framework with negative DA (NDA) strategies. By properly handling their intentionally created OOD samples, the classification performance of NDAs is largely ameliorated. Experiments on several classification benchmarks show that ReSmooth can be easily extended to the existing augmentation strategies [such as RandAugment (RA), rotate, and jigsaw] and improve on them. Our code is available at https://github.com/Chenyang4/ReSmooth .},
  archive      = {J_TNNLS},
  author       = {Chenyang Wang and Junjun Jiang and Xiong Zhou and Xianming Liu},
  doi          = {10.1109/TNNLS.2022.3222044},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7899-7910},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ReSmooth: Detecting and utilizing OOD samples when training with data augmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NDNet: Spacewise multiscale representation learning via
neighbor decoupling for real-time driving scene parsing. <em>TNNLS</em>,
<em>35</em>(6), 7884–7898. (<a
href="https://doi.org/10.1109/TNNLS.2022.3221745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a safety-critical application, autonomous driving requires high-quality semantic segmentation and real-time performance for deployment. Existing method commonly suffers from information loss and massive computational burden due to high-resolution input-output and multiscale learning scheme, which runs counter to the real-time requirements. In contrast to channelwise information modeling commonly adopted by modern networks, in this article, we propose a novel real-time driving scene parsing framework named NDNet from a novel perspective of spacewise neighbor decoupling (ND) and neighbor coupling (NC). We first define and implement the reversible operations called ND and NC, which realize lossless resolution conversion for complementary thumbnails sampling and collation to facilitate spatial modeling. Based on ND and NC, we further propose three modules, namely, local capturer and global dependence builder (LCGB), spacewise multiscale feature extractor (SMFE), and high-resolution semantic generator (HSG), which form the whole pipeline of NDNet. The LCGB serves as a stem block to preprocess the large-scale input for fast but lossless resolution reduction and extract initial features with global context. Then the SMFE is used for dense feature extraction and can obtain rich multiscale features in spatial dimension with less computational overhead. As for high-resolution semantic output, the HSG is designed for fast resolution reconstruction and adaptive semantic confusion amending. Experiments show the superiority of the proposed method. NDNet achieves the state-of-the-art performance on the Cityscapes dataset which reports 76.47% mIoU at 240+ frames/s and 78.8% mIoU at 150+ frames/s on the benchmark. Codes are available at https://github.com/LiShuTJ/NDNet .},
  archive      = {J_TNNLS},
  author       = {Shu Li and Qingqing Yan and Xun Zhou and Deming Wang and Chengju Liu and Qijun Chen},
  doi          = {10.1109/TNNLS.2022.3221745},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7884-7898},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {NDNet: Spacewise multiscale representation learning via neighbor decoupling for real-time driving scene parsing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring brain effective connectivity networks through
spatiotemporal graph convolutional models. <em>TNNLS</em>,
<em>35</em>(6), 7871–7883. (<a
href="https://doi.org/10.1109/TNNLS.2022.3221617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning brain effective connectivity networks (ECN) from functional magnetic resonance imaging (fMRI) data has gained much attention in recent years. With the successful applications of deep learning in numerous fields, several brain ECN learning methods based on deep learning have been reported in the literature. However, current methods ignore the deep temporal features of fMRI data and fail to fully employ the spatial topological relationship between brain regions. In this article, we propose a novel method for learning brain ECN based on spatiotemporal graph convolutional models (STGCM), named STGCMEC, in which we first adopt the temporal convolutional network to extract the deep temporal features of fMRI data and utilize the graph convolutional network to update the spatial features of each brain region by aggregating information from neighborhoods, which makes the features of brain regions more discriminative. Then, based on such features of brain regions, we design a joint loss function to guide STGCMEC to learn the brain ECN, which includes a task prediction loss and a graph regularization loss. The experimental results on a simulated dataset and a real Alzheimer’s disease neuroimaging initiative (ADNI) dataset show that the proposed STGCMEC is able to better learn brain ECN compared with some state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Aixiao Zou and Junzhong Ji and Minglong Lei and Jinduo Liu and Yongduan Song},
  doi          = {10.1109/TNNLS.2022.3221617},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7871-7883},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploring brain effective connectivity networks through spatiotemporal graph convolutional models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Unsupervised monocular depth estimation with channel and
spatial attention. <em>TNNLS</em>, <em>35</em>(6), 7860–7870. (<a
href="https://doi.org/10.1109/TNNLS.2022.3221416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding 3-D scene geometry from videos is a fundamental topic in visual perception. In this article, we propose an unsupervised monocular depth and camera motion estimation framework using unlabeled monocular videos to overcome the limitation of acquiring per-pixel ground-truth depth at scale. The photometric loss couples the depth network and pose network together and is essential to the unsupervised method, which is based on warping nearby views to target using the estimated depth and pose. We introduce the channelwise attention mechanism to dig into the relationship between channels and introduce the spatialwise attention mechanism to utilize the inner-spatial relationship of features. Both of them applied in depth networks can better activate the feature information between different convolutional layers and extract more discriminative features. In addition, we apply the Sobel boundary to our edge-aware smoothness for more reasonable accuracy, and clearer boundaries and structures. All of these help to close the gap with fully supervised methods and show high-quality state-of-the-art results on the KITTI benchmark and great generalization performance on the Make3D dataset.},
  archive      = {J_TNNLS},
  author       = {Zhuping Wang and Xinke Dai and Zhanyu Guo and Chao Huang and Hao Zhang},
  doi          = {10.1109/TNNLS.2022.3221416},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7860-7870},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised monocular depth estimation with channel and spatial attention},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining spatial co-location patterns with a mixed prevalence
measure. <em>TNNLS</em>, <em>35</em>(6), 7845–7859. (<a
href="https://doi.org/10.1109/TNNLS.2022.3221112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A co-location pattern indicates a subset of spatial features whose instances are frequently located together in proximate geographical space. Most previous studies of spatial co-location pattern mining concern what percentage of instances per feature are involved in the table instance of a pattern, but neglect the heterogeneity in the number of feature instances and the distribution of instances. As a result, the deviation may be occurred in the interest measure of co-locations. In this article, we propose a novel mixed prevalence index (MPI) incorporating the effect of feature-level and instance-level heterogeneity on the prevalence measure, which can address some dilemmas in existing interest measures. Luckily, MPI possesses the partial antimonotone property. In virtue of this property, a branch-based search algorithm equipped with some optimizing strategies of MPI calculation is proposed, namely, Branch-Opt-MPI. Comprehensive experiments are conducted on both real and synthetic spatial datasets. Experimental results reveal the superiority of MPI compared to other interest measures and also validate the efficiency and scalability of the Branch-Opt-MPI. Particularly, the Branch-Opt-MPI performs more efficiently than baselines for several times or even orders of magnitude in dense data.},
  archive      = {J_TNNLS},
  author       = {Peizhong Yang and Lizhen Wang and Lihua Zhou and Hongmei Chen},
  doi          = {10.1109/TNNLS.2022.3221112},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7845-7859},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mining spatial co-location patterns with a mixed prevalence measure},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Event-triggered guarantee cost control for partially
unknown stochastic systems via explorized integral reinforcement
learning strategy. <em>TNNLS</em>, <em>35</em>(6), 7830–7844. (<a
href="https://doi.org/10.1109/TNNLS.2022.3221105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an integral reinforcement learning (IRL)-based event-triggered guarantee cost control (GCC) approach is proposed for stochastic systems which are modulated by randomly time-varying parameters. First, with the aid of the RL algorithm, the optimal GCC (OGCC) problem is converted into an optimal zero-sum game by solving a modified Hamilton–Jacobin–Isaac (HJI) equation of the auxiliary system. Moreover, in order to address the stochastic zero-sum game, we propose an on-policy IRL-based control approach involved by the multivariate probabilistic collocation method (MPCM), which can accurately predict the mean value of uncertain functions with randomly time-varying parameters. Furthermore, a novel GCC method, which combines the explorized IRL algorithm and MPCM, is designed to relax the restriction of knowing the system dynamics for the class of stochastic systems. On this foundation, for the purpose of reducing computation cost and avoiding the waste of resources, we propose an event-triggered GCC approach involved with explorized IRL and MPCM by utilizing critic-actor-disturbance neural networks (NNs). Meanwhile, the weight vectors of three NNs are updated simultaneously and aperiodically according to the designed triggering condition. The ultimate boundedness (UB) properties of the controlled systems have been proved by means of the Lyapunov theorem. Finally, the effectiveness of the developed GCC algorithms is illustrated via two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Yuling Liang and Huaguang Zhang and Juan Zhang and Zhongyang Ming},
  doi          = {10.1109/TNNLS.2022.3221105},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7830-7844},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered guarantee cost control for partially unknown stochastic systems via explorized integral reinforcement learning strategy},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Hierarchical neighbors embedding. <em>TNNLS</em>,
<em>35</em>(6), 7816–7829. (<a
href="https://doi.org/10.1109/TNNLS.2022.3221103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manifold learning now plays an important role in machine learning and many relevant applications. In spite of the superior performance of manifold learning techniques in dealing with nonlinear data distribution, their performance would drop when facing the problem of data sparsity. It is hard to obtain satisfactory embeddings when sparsely sampled high-dimensional data are mapped into the observation space. To address this issue, in this article, we propose hierarchical neighbors embedding (HNE), which enhances the local connections through hierarchical combination of neighbors. And three different HNE-based implementations are derived by further analyzing the topological connection and reconstruction performance. The experimental results on both the synthetic and real-world datasets illustrate that our HNE-based methods could obtain more faithful embeddings with better topological and geometrical properties. From the view of embedding quality, HNE develops the outstanding advantages in dealing with data of general distributions. Furthermore, comparing with other state-of-the-art manifold learning methods, HNE shows its superiority in dealing with sparsely sampled data and weak-connected manifolds.},
  archive      = {J_TNNLS},
  author       = {Shenglan Liu and Yang Yu and Kaiyuan Liu and Feilong Wang and Wujun Wen and Hong Qiao},
  doi          = {10.1109/TNNLS.2022.3221103},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7816-7829},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical neighbors embedding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangled retrieval and reasoning for implicit question
answering. <em>TNNLS</em>, <em>35</em>(6), 7804–7815. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date, most of the existing open-domain question answering (QA) methods focus on explicit questions where the reasoning steps are mentioned explicitly in the question. In this article, we study implicit QA where the reasoning steps are not evident in the question. Implicit QA is challenging in two aspects. First, evidence retrieval is difficult since there is little overlap between a question and its required evidence. Second, answer inference is difficult since the reasoning strategy is latent in the question. To tackle implicit QA, we propose a systematic solution denoted as DisentangledQA, which disentangles topic, attribute, and reasoning strategy from the implicit question to guide the retrieval and reasoning. Specifically, we disentangle the topic and attribute information from the implicit question to guide evidence retrieval. For answer reasoning, we propose a disentangled reasoning model for answer prediction based on retrieved evidence as well as the latent representation of the reasoning strategy. The disentangled framework empowers each module to focus on a specific latent element in the question, and thus, leads to effective representation learning for them. Experiments on the StrategyQA dataset demonstrate the effectiveness of our method in answering implicit questions, improving performance in evidence retrieval and answering inference by 31.7% and 4.5%, respectively, and achieving the best performance on the official leaderboard. In addition, our method achieved the best performance on the challenging EntityQuestions dataset, indicating the effectiveness in improving general open-domain QA tasks.},
  archive      = {J_TNNLS},
  author       = {Qian Liu and Xiubo Geng and Yu Wang and Erik Cambria and Daxin Jiang},
  doi          = {10.1109/TNNLS.2022.3220933},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7804-7815},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disentangled retrieval and reasoning for implicit question answering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep fusion clustering network with reliable structure
preservation. <em>TNNLS</em>, <em>35</em>(6), 7792–7803. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering, which can elegantly exploit data representation to seek a partition of the samples, has attracted intensive attention. Recently, combining auto-encoder (AE) with graph neural networks (GNNs) has accomplished excellent performance by introducing structural information implied among data in clustering tasks. However, we observe that there are some limitations of most existing works: 1) in practical graph datasets, there exist some noisy or inaccurate connections among nodes, which would confuse network learning and cause biased representations, thus leading to unsatisfied clustering performance; 2) lacking dynamic information fusion module to carefully combine and refine the node attributes and the graph structural information to learn more consistent representations; and 3) failing to exploit the two separated views’ information for generating a more robust target distribution. To solve these problems, we propose a novel method termed deep fusion clustering network with reliable structure preservation (DFCN-RSP). Specifically, the random walk mechanism is introduced to boost the reliability of the original graph structure by measuring localized structure similarities among nodes. It can simultaneously filter out noisy connections and supplement reliable connections in the original graph. Moreover, we provide a transformer-based graph auto-encoder (TGAE) that can use a self-attention mechanism with the localized structure similarity information to fine-tune the fused topology structure among nodes layer by layer. Furthermore, we provide a dynamic cross-modality fusion strategy to combine the representations learned from both TGAE and AE. Also, we design a triplet self-supervision strategy and a target distribution generation measure to explore the cross-modality information. The experimental results on five public benchmark datasets reflect that DFCN-RSP is more competitive than the state-of-the-art deep clustering algorithms. The corresponding code is available at https://github.com/gongleii/DFCN-RSP .},
  archive      = {J_TNNLS},
  author       = {Lei Gong and Wenxuan Tu and Sihang Zhou and Long Zhao and Zhe Liu and Xinwang Liu},
  doi          = {10.1109/TNNLS.2022.3220914},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7792-7803},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep fusion clustering network with reliable structure preservation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust coevolutionary neural-based optimization algorithm
for constrained nonconvex optimization. <em>TNNLS</em>, <em>35</em>(6),
7778–7791. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For nonconvex optimization problems, a routine is to assume that there is no perturbation when executing the solution task. Nevertheless, dealing with the perturbation in advance may increase the burden on the system and take up extra time. To remedy this weakness, we propose a robust coevolutionary neural-based optimization algorithm with inherent robustness based on the hybridization between the particle swarm optimization and a class of robust neural dynamics (RND). In this framework, every neural agent guided by the RND supersedes the place of the particle, mutually searches for the optimal solution, and stabilizes itself from different perturbations. The theoretical analysis ensures that the proposed algorithm is globally convergent with probability one. Besides, the effectiveness and robustness of the proposed approach are illustrated by illustrative examples compared with the existing methods. We further apply this proposed algorithm to the source localization and manipulability optimization of the redundant manipulator, simultaneously disposing of perturbation from the internal and exogenous system with satisfactory performance.},
  archive      = {J_TNNLS},
  author       = {Lin Wei and Long Jin and Xin Luo},
  doi          = {10.1109/TNNLS.2022.3220806},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7778-7791},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A robust coevolutionary neural-based optimization algorithm for constrained nonconvex optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A local-and-global attention reinforcement learning
algorithm for multiagent cooperative navigation. <em>TNNLS</em>,
<em>35</em>(6), 7767–7777. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cooperative navigation algorithm is the crucial technology for multirobot systems to accomplish autonomous collaborative operations, and it is still a challenge for researchers. In this work, we propose a new multiagent reinforcement learning algorithm called multiagent local-and-global attention actor–critic (MLGA2C) for multiagent cooperative navigation. Inspired by the attention mechanism, we design the local-and-global attention module to dynamically extract and encode critical environmental features. Meanwhile, based on the centralized training and decentralized execution (CTDE) paradigm, we extend a new actor–critic method to handle feature encoding and make navigation decisions. We also evaluate the proposed algorithm in two cooperative navigation scenarios: static target navigation and dynamic pedestrian target tracking. The multiple experimental results show that our algorithm performs well in cooperative navigation tasks with increasing agents.},
  archive      = {J_TNNLS},
  author       = {Chunwei Song and Zichen He and Lu Dong},
  doi          = {10.1109/TNNLS.2022.3220798},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7767-7777},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A local-and-global attention reinforcement learning algorithm for multiagent cooperative navigation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). E2SCNet: Efficient multiobjective evolutionary automatic
search for remote sensing image scene classification network
architecture. <em>TNNLS</em>, <em>35</em>(6), 7752–7766. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image scene classification methods based on deep learning have been widely studied and discussed. However, most of the network architectures are directly reliant on natural image processing methods and are fixed. A few studies have focused on automatic search mechanisms, but they cannot weigh the interpretation accuracy and the parameter quantity for practical application. As a result, automatic global search methods based on multiobjective evolutionary computation have more advantages. However, in the ranking process, the network individuals with large parameter quantities are easy to eliminate, but a higher accuracy may be obtained after full training. In addition, evolutionary neural architecture search methods often take several days. In this article, in order to solve the above concerns, we propose an efficient multiobjective evolutionary automatic search framework for remote sensing image scene classification deep learning network architectures (E2SCNet). In E2SCNet, eight kinds of lightweight operators are used to build a diversified search space, and the coding connection mode is flexible. In the search process, a large model retention mechanism is implemented through two-step multiobjective modeling and evolutionary search, where one step involves the “parameter quantity and accuracy,” and the other step involves the “parameter quantity and accuracy growth quantity.” Moreover, a super network is constructed to share the weight in the process of individual network evaluation and promote the search speed. The effectiveness of E2SCNet is proven by comparison with several networks designed by human experts and networks obtained by gradient and evolutionary computing-based search methods.},
  archive      = {J_TNNLS},
  author       = {Yuting Wan and Yanfei Zhong and Ailong Ma and Junjue Wang and Liangpei Zhang},
  doi          = {10.1109/TNNLS.2022.3220699},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7752-7766},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {E2SCNet: Efficient multiobjective evolutionary automatic search for remote sensing image scene classification network architecture},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast incomplete multi-view clustering with view-independent
anchors. <em>TNNLS</em>, <em>35</em>(6), 7740–7751. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) methods aim to exploit consistent and complementary information among each view and achieve encouraging performance improvement than single-view counterparts. In practical applications, it is common to obtain instances with partially available information, raising researches of incomplete multi-view clustering (IMC) issues. Recently, several fast IMC methods have been proposed to process the large-scale partial data. Though with considerable acceleration, these methods seek view-shared anchors and ignore specific information among single views. To tackle the above issue, we propose a fast IMC with view-independent anchors (FIMVC-VIA) method in this article. Specifically, we learn individual anchors based on the diversity of distribution among each incomplete view and construct a unified anchor graph following the principle of consistent clustering structure. By constructing an anchor graph instead of pairwise full graph, the time and space complexities of our proposed FIMVC-VIA are proven to be linearly related to the number of samples, which can efficiently solve the large-scale task. The experiment performed on benchmarks with different missing rate illustrates the improvement in complexity and effectiveness of our method compared with other IMC methods. Our code is publicly available at https://github.com/Tracesource/ FIMVC-VIA.},
  archive      = {J_TNNLS},
  author       = {Suyuan Liu and Xinwang Liu and Siwei Wang and Xin Niu and En Zhu},
  doi          = {10.1109/TNNLS.2022.3220486},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7740-7751},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast incomplete multi-view clustering with view-independent anchors},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive multimodel knowledge transfer matrix machine for
EEG classification. <em>TNNLS</em>, <em>35</em>(6), 7726–7739. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging matrix learning methods have achieved promising performances in electroencephalogram (EEG) classification by exploiting the structural information between the columns or rows of feature matrices. Due to the intersubject variability of EEG data, these methods generally need to collect a large amount of labeled individual EEG data, which would cause fatigue and inconvenience to the subjects. Insufficient subject-specific EEG data will weaken the generalization capability of the matrix learning methods in neural pattern decoding. To overcome this dilemma, we propose an adaptive multimodel knowledge transfer matrix machine (AMK-TMM), which can selectively leverage model knowledge from multiple source subjects and capture the structural information of the corresponding EEG feature matrices. Specifically, by incorporating least-squares (LS) loss with spectral elastic net regularization, we first present an LS support matrix machine (LS-SMM) to model the EEG feature matrices. To boost the generalization capability of LS-SMM in scenarios with limited EEG data, we then propose a multimodel adaption method, which can adaptively choose multiple correlated source model knowledge with a leave-one-out cross-validation strategy on the available target training data. We extensively evaluate our method on three independent EEG datasets. Experimental results demonstrate that our method achieves promising performances on EEG classification.},
  archive      = {J_TNNLS},
  author       = {Shuang Liang and Wenlong Hang and Baiying Lei and Jun Wang and Jing Qin and Kup-Sze Choi and Yu Zhang},
  doi          = {10.1109/TNNLS.2022.3220551},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7726-7739},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive multimodel knowledge transfer matrix machine for EEG classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Master memory function for delay-based reservoir computers
with single-variable dynamics. <em>TNNLS</em>, <em>35</em>(6),
7712–7725. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that many delay-based reservoir computers considered in the literature can be characterized by a universal master memory function (MMF). Once computed for two independent parameters, this function provides linear memory capacity for any delay-based single-variable reservoir with small inputs. Moreover, we propose an analytical description of the MMF that enables its efficient and fast computation. Our approach can be applied not only to single-variable delay-based reservoirs governed by known dynamical rules, such as the Mackey–Glass or Stuart–Landau-like systems, but also to reservoirs whose dynamical model is not available.},
  archive      = {J_TNNLS},
  author       = {Felix Köster and Serhiy Yanchuk and Kathy Lüdge},
  doi          = {10.1109/TNNLS.2022.3220532},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7712-7725},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Master memory function for delay-based reservoir computers with single-variable dynamics},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mode-mixed effects based intralayer-dependent impulsive
synchronization for multiple mismatched multilayer neural networks.
<em>TNNLS</em>, <em>35</em>(6), 7697–7711. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the intralayer-dependent impulsive synchronization of multiple mismatched multilayer neural networks (NNs) with mode-mixed effects. Initially, a novel multilayer NN model that removes the one-to-one interlayer coupling constraint and introduces nonidentical model parameters is first established to meet diverse modeling requirements in complex applications. To help the multilayer target NNs with mismatched connection coefficients and time delays achieve synchronization, the hybrid controller is designed using intralayer-dependent impulsive control and switched feedback control approaches. Furthermore, the mode-mixed effects caused by the intralayer coupling delays and switched intralayer topologies are incorporated into the novel model and analysis method to ensure that the subsystems operating within the current switching interval can effectively use the topology information of the previous switching intervals. Then, a novel analysis framework including super-Laplacian matrix, augmented matrix, and mode-mixed methods is developed to derive the synchronization results. Finally, the main results are verified via the numerical simulation with secure communication.},
  archive      = {J_TNNLS},
  author       = {Xiangxiang Wang and Yongbin Yu and Shuzhi Sam Ge and Kaibo Shi and Shouming Zhong and Jingye Cai},
  doi          = {10.1109/TNNLS.2022.3220193},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7697-7711},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mode-mixed effects based intralayer-dependent impulsive synchronization for multiple mismatched multilayer neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Channel-aware decoupling network for multiturn dialog
comprehension. <em>TNNLS</em>, <em>35</em>(6), 7685–7696. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training machines to understand natural language and interact with humans is one of the major goals of artificial intelligence. Recent years have witnessed an evolution from matching networks to pretrained language models (PrLMs). In contrast to the plain-text modeling as the focus of the PrLMs, dialog texts involve multiple speakers and reflect special characteristics, such as topic transitions and structure dependencies, between distant utterances. However, the related PrLM models commonly represent dialogs sequentially by processing the pairwise dialog history as a whole. Thus, the hierarchical information on either utterance interrelation or speaker roles coupled in such representations is not well addressed. In this work, we propose compositional learning for holistic interaction across the utterances beyond the sequential contextualization from PrLMs, in order to capture the utterance-aware and speaker-aware representations entailed in a dialog history. We decouple the contextualized word representations by masking mechanisms in transformer-based PrLM, making each word only focus on the words in the current utterance, other utterances, and two speaker roles (i.e., utterances of the sender and utterances of the receiver), respectively. In addition, we employ domain-adaptive training strategies to help the model adapt to the dialog domains. Experimental results show that our method substantially boosts the strong PrLM baselines in four public benchmark datasets, achieving new state-of-the-art performance over previous methods.},
  archive      = {J_TNNLS},
  author       = {Zhuosheng Zhang and Hai Zhao and Longxiang Liu},
  doi          = {10.1109/TNNLS.2022.3220047},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7685-7696},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Channel-aware decoupling network for multiturn dialog comprehension},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self supervised progressive network for high performance
video object segmentation. <em>TNNLS</em>, <em>35</em>(6), 7671–7684.
(<a href="https://doi.org/10.1109/TNNLS.2022.3219936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, self-supervised video object segmentation (VOS) has attracted much interest. However, most proxy tasks are proposed to train only a single backbone, which relies on a point-to-point correspondence strategy to propagate masks through a video sequence. Due to its simple pipeline, the performance of the single backbone paradigm is still unsatisfactory. Instead of following the previous literature, we propose our self-supervised progressive network (SSPNet) which consists of a memory retrieval module (MRM) and collaborative refinement module (CRM). The MRM can perform point-to-point correspondence and produce a propagated coarse mask for a query frame through self-supervised pixel-level and frame-level similarity learning. The CRM, which is trained via cycle consistency region tracking, aggregates the reference &amp; query information and learns the collaborative relationship among them implicitly to refine the coarse mask. Furthermore, to learn semantic knowledge from unlabeled data, we also design two novel mask-generation strategies to provide the training data with meaningful semantic information for the CRM. Extensive experiments conducted on DAVIS-17, YouTube- VOS and SegTrack v2 demonstrate that our method surpasses the state-of-the-art self-supervised methods and narrows the gap with the fully supervised methods.},
  archive      = {J_TNNLS},
  author       = {Guorong Li and Dexiang Hong and Kai Xu and Bineng Zhong and Li Su and Zhenjun Han and Qingming Huang},
  doi          = {10.1109/TNNLS.2022.3219936},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7671-7684},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self supervised progressive network for high performance video object segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain adaptation with self-supervised learning and feature
clustering for intelligent fault diagnosis. <em>TNNLS</em>,
<em>35</em>(6), 7657–7670. (<a
href="https://doi.org/10.1109/TNNLS.2022.3219896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation indeed promotes the progress of intelligent fault diagnosis in industrial scenarios. The abundant labeled samples are not necessary. The identical distribution between the training and testing datasets is not any more the prerequisite for intelligent fault diagnosis working. However, two issues arise subsequently: Feature learning in domain adaptation framework tends to be biased to the source domain, and unreliable pseudolabeling seriously impacts on the conditional domain adaptation. In this article, a new domain adaptation approach with self-supervised learning and feature clustering (DASSL-FC) is proposed, trying to alleviate the issues by unbiased feature learning and pseudolabels updating strategy. Taking different transformation methods as pretext, the transformed data and its pretext train a neural network in an SSL way. As to pseudolabeling, clusters are taken as the auxiliary information to correct the network predicted labels in terms of the “strong cluster” rule. Then, the updated pseudolabels and their confidence are enforced to further estimate the conditional distribution discrepancy and its confidence weight. To verify the effectiveness of the proposed method, the experiments are implemented on intraplatform and interplatforms for simulating the practical scenarios.},
  archive      = {J_TNNLS},
  author       = {Nannan Lu and Hanhan Xiao and Zhanguo Ma and Tong Yan and Min Han},
  doi          = {10.1109/TNNLS.2022.3219896},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7657-7670},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain adaptation with self-supervised learning and feature clustering for intelligent fault diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). DualGCN: Exploring syntactic and semantic information for
aspect-based sentiment analysis. <em>TNNLS</em>, <em>35</em>(6),
7642–7656. (<a
href="https://doi.org/10.1109/TNNLS.2022.3219615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of aspect-based sentiment analysis aims to identify sentiment polarities of given aspects in a sentence. Recent advances have demonstrated the advantage of incorporating the syntactic dependency structure with graph convolutional networks (GCNs). However, their performance of these GCN-based methods largely depends on the dependency parsers, which would produce diverse parsing results for a sentence. In this article, we propose a dual GCN (DualGCN) that jointly considers the syntax structures and semantic correlations. Our DualGCN model mainly comprises four modules: 1) SynGCN: instead of explicitly encoding syntactic structure, the SynGCN module uses the dependency probability matrix as a graph structure to implicitly integrate the syntactic information; 2) SemGCN: we design the SemGCN module with multihead attention to enhance the performance of the syntactic structure with the semantic information; 3) Regularizers: we propose orthogonal and differential regularizers to precisely capture semantic correlations between words by constraining attention scores in the SemGCN module; and 4) Mutual BiAffine: we use the BiAffine module to bridge relevant information between the SynGCN and SemGCN modules. Extensive experiments are conducted compared with up-to-date pretrained language encoders on two groups of datasets, one including Restaurant14, Laptop14, and Twitter and the other including Restaurant15 and Restaurant16. The experimental results demonstrate that the parsing results of various dependency parsers affect their performance of the GCN-based models. Our DualGCN model achieves superior performance compared with the state-of-the-art approaches. The source code and preprocessed datasets are provided and publicly available on GitHub (see https://github.com/CCChenhao997/DualGCN-ABSA ).},
  archive      = {J_TNNLS},
  author       = {Ruifan Li and Hao Chen and Fangxiang Feng and Zhanyu Ma and Xiaojie Wang and Eduard Hovy},
  doi          = {10.1109/TNNLS.2022.3219615},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7642-7656},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DualGCN: Exploring syntactic and semantic information for aspect-based sentiment analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Attention-like multimodality fusion with data augmentation
for diagnosis of mental disorders using MRI. <em>TNNLS</em>,
<em>35</em>(6), 7627–7641. (<a
href="https://doi.org/10.1109/TNNLS.2022.3219551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The globally rising prevalence of mental disorders leads to shortfalls in timely diagnosis and therapy to reduce patients’ suffering. Facing such an urgent public health problem, professional efforts based on symptom criteria are seriously overstretched. Recently, the successful applications of computer-aided diagnosis approaches have provided timely opportunities to relieve the tension in healthcare services. Particularly, multimodal representation learning gains increasing attention thanks to the high temporal and spatial resolution information extracted from neuroimaging fusion. In this work, we propose an efficient multimodality fusion framework to identify multiple mental disorders based on the combination of functional and structural magnetic resonance imaging. A multioutput conditional generative adversarial network (GAN) is developed to address the scarcity of multimodal data for augmentation. Based on the augmented training data, the multiheaded gating fusion model is proposed for classification by extracting the complementary features across different modalities. The experiments demonstrate that the proposed model can achieve robust accuracies of $75.1~\pm ~1.5$ %, $72.9~\pm ~1.1$ %, and $87.2~\pm ~1.5$ % for autism spectrum disorder (ASD), attention deficit/hyperactivity disorder, and schizophrenia, respectively. In addition, the interpretability of our model is expected to enable the identification of remarkable neuropathology diagnostic biomarkers, leading to well-informed therapeutic decisions.},
  archive      = {J_TNNLS},
  author       = {Rui Liu and Zhi-An Huang and Yao Hu and Zexuan Zhu and Ka-Chun Wong and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2022.3219551},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7627-7641},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention-like multimodality fusion with data augmentation for diagnosis of mental disorders using MRI},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). DAGCN: Dynamic and adaptive graph convolutional network for
salient object detection. <em>TNNLS</em>, <em>35</em>(6), 7612–7626. (<a
href="https://doi.org/10.1109/TNNLS.2022.3219245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning-based salient object detection (SOD) has achieved significant success in recent years. The SOD focuses on the context modeling of the scene information, and how to effectively model the context relationship in the scene is the key. However, it is difficult to build an effective context structure and model it. In this article, we propose a novel SOD method called dynamic and adaptive graph convolutional network (DAGCN) that is composed of two parts, adaptive neighborhood-wise graph convolutional network (AnwGCN) and spatially restricted K-nearest neighbors (SRKNN). The AnwGCN is novel adaptive neighborhood-wise graph convolution, which is used to model and analyze the saliency context. The SRKNN constructs the topological relationship of the saliency context by measuring the non-Euclidean spatial distance within a limited range. The proposed method constructs the context relationship as a topological graph by measuring the distance of the features in the non-Euclidean space, and conducts comparative modeling of context information through AnwGCN. The model has the ability to learn the metrics from features and can adapt to the hidden space distribution of the data. The description of the feature relationship is more accurate. Through the convolutional kernel adapted to the neighborhood, the model obtains the structure learning ability. Therefore, the graph convolution process can adapt to different graph data. Experimental results demonstrate that our solution achieves satisfactory performance on six widely used datasets and can also effectively detect camouflaged objects. Our code will be available at: https://github.com/CSIM-LUT/DAGCN.git .},
  archive      = {J_TNNLS},
  author       = {Ce Li and Fenghua Liu and Zhiqiang Tian and Shaoyi Du and Yang Wu},
  doi          = {10.1109/TNNLS.2022.3219245},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7612-7626},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DAGCN: Dynamic and adaptive graph convolutional network for salient object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic event-triggered control for GSES of memristive
neural networks under multiple cyber-attacks. <em>TNNLS</em>,
<em>35</em>(6), 7602–7611. (<a
href="https://doi.org/10.1109/TNNLS.2022.3217461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the dynamic event-triggered control problem of memristive neural networks (MNNs) under multiple cyber-attacks is considered. A novel dynamic event-triggering scheme (DETS) and the corresponding event-triggered controller are proposed by taking into consideration both denial-of-service and deception attacks (DoS-DAs). Then, a key lemma is established to show that the dynamic event-triggered controller can be used to solve the globally stochastically exponential stability (GSES) issue of concerned MNN under multiple cyber-attacks. Meanwhile, a novel Lyapunov functional is proposed based on the actual sampling pattern. It is shown that under our proposed dynamic event-triggered controller and Lyapunov functional, the concerned MNN can achieve GSES in the presence of DoS-DAs. In addition, our results include relevant results on event-triggered control of MNN with static event-triggering scheme (SETS) or without cyber-attacks as special cases. The effectiveness of the proposed event-triggered controller under multiple cyber-attacks is illustrated by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Xin Wang and Ju H. Park and Zongcheng Liu and Huilan Yang},
  doi          = {10.1109/TNNLS.2022.3217461},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7602-7611},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic event-triggered control for GSES of memristive neural networks under multiple cyber-attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tactile transfer learning and object recognition with a
multifingered hand using morphology specific convolutional neural
networks. <em>TNNLS</em>, <em>35</em>(6), 7587–7601. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifingered robot hands can be extremely effective in physically exploring and recognizing objects, especially if they are extensively covered with distributed tactile sensors. Convolutional neural networks (CNNs) have been proven successful in processing high dimensional data, such as camera images, and are, therefore, very well suited to analyze distributed tactile information as well. However, a major challenge is to organize tactile inputs coming from different locations on the hand in a coherent structure that could leverage the computational properties of the CNN. Therefore, we introduce a morphology-specific CNN (MS-CNN), in which hierarchical convolutional layers are formed following the physical configuration of the tactile sensors on the robot. We equipped a four-fingered Allegro robot hand with several uSkin tactile sensors; overall, the hand is covered with 240 sensitive elements, each one measuring three-axis contact force. The MS-CNN layers process the tactile data hierarchically: at the level of small local clusters first, then each finger, and then the entire hand. We show experimentally that, after training, the robot hand can successfully recognize objects by a single touch, with a recognition rate of over 95%. Interestingly, the learned MS-CNN representation transfers well to novel tasks: by adding a limited amount of data about new objects, the network can recognize nine types of physical properties.},
  archive      = {J_TNNLS},
  author       = {Satoshi Funabashi and Gang Yan and Fei Hongyi and Alexander Schmitz and Lorenzo Jamone and Tetsuya Ogata and Shigeki Sugano},
  doi          = {10.1109/TNNLS.2022.3215723},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7587-7601},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tactile transfer learning and object recognition with a multifingered hand using morphology specific convolutional neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive multigranularity information propagation for
coupled aspect-opinion extraction. <em>TNNLS</em>, <em>35</em>(6),
7577–7586. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coupled aspect-opinion extraction aims to identify aspect-opinion pairs in the form of (aspect term, opinion term) or triplets in the form of (aspect term, opinion term, sentiment polarity) from user-generated texts. Compared to the traditional aspect-based sentiment prediction or extraction tasks, coupled aspect-opinion extraction needs to associate aspects with their corresponding opinions and organize opinion-related information into structured outputs. The existing works either divide this task into subproblems (i.e., term extraction and relation prediction) or utilize a unified tagging scheme. However, these methods only focus on atomic word-level interactions and ignore the intensive information propagation among different granularities (e.g., words and word pairs). To address this limitation, we propose a progressive multigranularity information propagation network that progressively explores three types of correlations with different granularities. Specifically, our model starts with the most basic word-level correlations by composing all possible word pairs. In the second stage, the pairwise relation information is used to update the word features. The last stage propagates information among word pairs to produce the relation scores. We treat the task as a unified relation prediction problem and construct an end-to-end framework that iteratively conducts the three-stage information propagation to refine the textual representations. Comprehensive experiments on different aspect-based sentiment analysis benchmarks clearly demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Fengmao Lv and Tao Liang and Zhihui Fei and Wenya Wang},
  doi          = {10.1109/TNNLS.2022.3215190},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7577-7586},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Progressive multigranularity information propagation for coupled aspect-opinion extraction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diversity-learning block: Conquer feature homogenization of
multibranch. <em>TNNLS</em>, <em>35</em>(6), 7563–7576. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Geometry Group (VGG)-style ConvNet is an neural-network process units (NPU)-friendly network; however, the accuracy of this architecture cannot keep up with other well-designed network structures. Although some reparameterization methods are proposed to remedy this weakness, their performance suffers from the homogenization issue of parallel branches, and the preset shape of convolution kernels also influences spatial perception. To address this problem, we propose a diversity-learning (DL) block to build the DLNet, which could adaptively learn various features to enrich the feature space. To balance floating point of operations (FLOPs) and accuracy, groupwise operation is introduced and finally, a lightweight DL ConvNet DLGNet is obtained. Extensive evaluations have been conducted on different computer vision tasks, e.g., image classification [Canadian Institute For Advanced Research (CIFAR) and ImageNet], object detection [PASCAL visual object classes (VOC) and Microsoft Common Objects in Context (MS COCO)], and semantic segmentation (Cityscapes). The experimental results show that our proposed DLGNet can achieve comparable performance with the state-of-the-art networks while the speed is 183% faster than GhostNet and even over 600% faster than MobileNetV3 with similar accuracy when running on NPU.},
  archive      = {J_TNNLS},
  author       = {Junjie Yang and Shenqi Lai and Xuan Wang and Yaxiong Wang and Xueming Qian},
  doi          = {10.1109/TNNLS.2022.3214993},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7563-7576},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Diversity-learning block: Conquer feature homogenization of multibranch},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware aggregation for federated open set domain
adaptation. <em>TNNLS</em>, <em>35</em>(6), 7548–7562. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open set domain adaptation (OSDA) methods have been proposed to leverage the difference between the source and target domains, as well as to recognize the known and unknown classes in the target domain. Such methods typically require the entire source and target data simultaneously to train the target model. However, in real scenarios, data are distributed and stored in various clients. They cannot be exchanged among clients because of privacy protection. Federated learning (FL) is a decentralized approach for training an effective global model with the training data distributed among the clients. Despite its potential in addressing the privacy concerns of data sharing, FL methods for OSDA that can handle unknown classes is not yet available. To tackle this problem, we have developed a novel federated OSDA (FOSDA) algorithm. More specifically, FOSDA adopts an uncertainty-aware mechanism to generate a global model from all client models. It reduces the uncertainty of the federated aggregation by focusing on the contribution of source clients with high uncertainty while retaining those with high consistency. Moreover, a federated class-based weighted strategy is also implemented in FOSDA to maintain the category information of the source clients. We have conducted comprehensive experiments on three benchmark datasets to evaluate the performance of the proposed method, and the results demonstrate the effectiveness of FOSDA.},
  archive      = {J_TNNLS},
  author       = {Zixuan Qin and Liu Yang and Fei Gao and Qinghua Hu and Chenyang Shen},
  doi          = {10.1109/TNNLS.2022.3214930},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7548-7562},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Uncertainty-aware aggregation for federated open set domain adaptation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter-efficient person re-identification in the 3D
space. <em>TNNLS</em>, <em>35</em>(6), 7534–7547. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People live in a 3D world. However, existing works on person re-identification (re-id) mostly consider the semantic representation learning in a 2D space, intrinsically limiting the understanding of people. In this work, we address this limitation by exploring the prior knowledge of the 3D body structure. Specifically, we project 2D images to a 3D space and introduce a novel parameter-efficient omni-scale graph network (OG-Net) to learn the pedestrian representation directly from 3D point clouds. OG-Net effectively exploits the local information provided by sparse 3D points and takes advantage of the structure and appearance information in a coherent manner. With the help of 3D geometry information, we can learn a new type of deep re-id feature free from noisy variants, such as scale and viewpoint. To our knowledge, we are among the first attempts to conduct person re-id in the 3D space. We demonstrate through extensive experiments that the proposed method: (1) eases the matching difficulty in the traditional 2D space; 2) exploits the complementary information of 2D appearance and 3D structure; 3) achieves competitive results with limited parameters on four large-scale person re-id datasets; and 4) has good scalability to unseen datasets. Our code, models, and generated 3D human data are publicly available at https://github.com/layumi/person-reid-3d .},
  archive      = {J_TNNLS},
  author       = {Zhedong Zheng and Xiaohan Wang and Nenggan Zheng and Yi Yang},
  doi          = {10.1109/TNNLS.2022.3214834},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7534-7547},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter-efficient person re-identification in the 3D space},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive optimal tracking control of an underactuated
surface vessel using actor–critic reinforcement learning.
<em>TNNLS</em>, <em>35</em>(6), 7520–7533. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present an adaptive reinforcement learning optimal tracking control (RLOTC) algorithm for an underactuated surface vessel subject to modeling uncertainties and time-varying external disturbances. By integrating backstepping technique with the optimized control design, we show that the desired optimal tracking performance of vessel control is guaranteed due to the fact that the virtual and actual control inputs are designed as optimized solutions of every subsystem. To enhance the robustness of vessel control systems, we employ neural network (NN) approximators to approximate uncertain vessel dynamics and present adaptive control technique to estimate the upper boundedness of external disturbances. Under the reinforcement learning framework, we construct actor–critic networks to solve the Hamilton–Jacobi–Bellman equations corresponding to subsystems of surface vessel to achieve the optimized control. The optimized control algorithm can synchronously train the adaptive parameters not only for actor–critic networks but also for NN approximators and adaptive control. By Lyapunov stability theorem, we show that the RLOTC algorithm can ensure the semiglobal uniform ultimate boundedness of the closed-loop systems. Compared with the existing reinforcement learning control results, the presented RLOTC algorithm can compensate for uncertain vessel dynamics and unknown disturbances, and obtain the optimized control performance by considering optimization in every backstepping design. Simulation studies on an underactuated surface vessel are given to illustrate the effectiveness of the RLOTC algorithm.},
  archive      = {J_TNNLS},
  author       = {Lin Chen and Shi-Lu Dai and Chao Dong},
  doi          = {10.1109/TNNLS.2022.3214681},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7520-7533},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive optimal tracking control of an underactuated surface vessel using Actor–Critic reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep neural networks and tabular data: A survey.
<em>TNNLS</em>, <em>35</em>(6), 7499–7519. (<a
href="https://doi.org/10.1109/TNNLS.2022.3229161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous tabular data are the most commonly used form of data and are essential for numerous critical and computationally demanding applications. On homogeneous datasets, deep neural networks have repeatedly shown excellent performance and have therefore been widely adopted. However, their adaptation to tabular data for inference or data generation tasks remains highly challenging. To facilitate further progress in the field, this work provides an overview of state-of-the-art deep learning methods for tabular data. We categorize these methods into three groups: data transformations, specialized architectures, and regularization models. For each of these groups, our work offers a comprehensive overview of the main approaches. Moreover, we discuss deep learning approaches for generating tabular data and also provide an overview over strategies for explaining deep models on tabular data. Thus, our first contribution is to address the main research streams and existing methodologies in the mentioned areas while highlighting relevant challenges and open research questions. Our second contribution is to provide an empirical comparison of traditional machine learning methods with 11 deep learning approaches across five popular real-world tabular datasets of different sizes and with different learning objectives. Our results, which we have made publicly available as competitive benchmarks, indicate that algorithms based on gradient-boosted tree ensembles still mostly outperform deep learning models on supervised learning tasks, suggesting that the research progress on competitive deep learning models for tabular data is stagnating. To the best of our knowledge, this is the first in-depth overview of deep learning approaches for tabular data; as such, this work can serve as a valuable starting point to guide researchers and practitioners interested in deep learning with tabular data.},
  archive      = {J_TNNLS},
  author       = {Vadim Borisov and Tobias Leemann and Kathrin Seßler and Johannes Haug and Martin Pawelczyk and Gjergji Kasneci},
  doi          = {10.1109/TNNLS.2022.3229161},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7499-7519},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep neural networks and tabular data: A survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A survey of visual transformers. <em>TNNLS</em>,
<em>35</em>(6), 7478–7498. (<a
href="https://doi.org/10.1109/TNNLS.2022.3227717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer, an attention-based encoder–decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern convolution neural networks (CNNs). In this survey, we have reviewed over 100 of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, two promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang-ict/awesome-visual-transformers .},
  archive      = {J_TNNLS},
  author       = {Yang Liu and Yao Zhang and Yixin Wang and Feng Hou and Jin Yuan and Jiang Tian and Yang Zhang and Zhongchao Shi and Jianping Fan and Zhiqiang He},
  doi          = {10.1109/TNNLS.2022.3227717},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7478-7498},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of visual transformers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of nuclei detection and segmentation on microscopy
images using deep learning with applications to unbiased stereology
counting. <em>TNNLS</em>, <em>35</em>(6), 7458–7477. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection and segmentation of stained cells and nuclei are essential prerequisites for subsequent quantitative research for many diseases. Recently, deep learning has shown strong performance in many computer vision problems, including solutions for medical image analysis. Furthermore, accurate stereological quantification of microscopic structures in stained tissue sections plays a critical role in understanding human diseases and developing safe and effective treatments. In this article, we review the most recent deep learning approaches for cell (nuclei) detection and segmentation in cancer and Alzheimer’s disease with an emphasis on deep learning approaches combined with unbiased stereology. Major challenges include accurate and reproducible cell detection and segmentation of microscopic images from stained sections. Finally, we discuss potential improvements and future trends in deep learning applied to cell detection and segmentation.},
  archive      = {J_TNNLS},
  author       = {Saeed S. Alahmari and Dmitry Goldgof and Lawrence O. Hall and Peter R. Mouton},
  doi          = {10.1109/TNNLS.2022.3213407},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7458-7477},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A review of nuclei detection and segmentation on microscopy images using deep learning with applications to unbiased stereology counting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BAI-net: Individualized anatomical cerebral cartography
using graph neural network. <em>TNNLS</em>, <em>35</em>(6), 7446–7457.
(<a href="https://doi.org/10.1109/TNNLS.2022.3213581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain atlas is an important tool in the diagnosis and treatment of neurological disorders. However, due to large variations in the organizational principles of individual brains, many challenges remain in clinical applications. Brain atlas individualization network (BAI-Net) is an algorithm that subdivides individual cerebral cortex into segregated areas using brain morphology and connectomes. The presented method integrates group priors derived from a population atlas, adjusts areal probabilities using the context of connectivity fingerprints derived from the fiber-tract embedding of tractography, and provides reliable and explainable individualized brain areas across multiple sessions and scanners. We demonstrate that BAI-Net outperforms the conventional iterative clustering approach by capturing significantly heritable topographic variations in individualized cartographies. The topographic variability of BAI-Net cartographies has shown strong associations with individual variability in brain morphology, connectivity as well as higher relationship on individual cognitive behaviors and genetics. This study provides an explainable framework for individualized brain cartography that may be useful in the precise localization of neuromodulation and treatments on individual brains.},
  archive      = {J_TNNLS},
  author       = {Liang Ma and Yu Zhang and Hantian Zhang and Luqi Cheng and Zhengyi Yang and Yuheng Lu and Weiyang Shi and Wen Li and Junjie Zhuo and Jiaojian Wang and Lingzhong Fan and Tianzi Jiang},
  doi          = {10.1109/TNNLS.2022.3213581},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7446-7457},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BAI-net: Individualized anatomical cerebral cartography using graph neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anatomy-guided spatio-temporal graph convolutional networks
(AG-STGCNs) for modeling functional connectivity between gyri and sulci
across multiple task domains. <em>TNNLS</em>, <em>35</em>(6), 7435–7445.
(<a href="https://doi.org/10.1109/TNNLS.2022.3194733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cerebral cortex is folded as gyri and sulci, which provide the foundation to unveil anatomo-functional relationship of brain. Previous studies have extensively demonstrated that gyri and sulci exhibit intrinsic functional difference, which is further supported by morphological, genetic, and structural evidences. Therefore, systematically investigating the gyro-sulcal (G-S) functional difference can help deeply understand the functional mechanism of brain. By integrating functional magnetic resonance imaging (fMRI) with advanced deep learning models, recent studies have unveiled the temporal difference in functional activity between gyri and sulci. However, the potential difference of functional connectivity, which represents functional dependency between gyri and sulci, is much unknown. Moreover, the regularity and variability of the G-S functional connectivity difference across multiple task domains remains to be explored. To address the two concerns, this study developed new anatomy-guided spatio-temporal graph convolutional networks (AG-STGCNs) to investigate the regularity and variability of functional connectivity differences between gyri and sulci across multiple task domains. Based on 830 subjects with seven different task-based and one resting state fMRI (rs-fMRI) datasets from the public Human Connectome Project (HCP), we consistently found that there are significant differences of functional connectivity between gyral and sulcal regions within task domains compared with resting state (RS). Furthermore, there is considerable variability of such functional connectivity and information flow between gyri and sulci across different task domains, which are correlated with individual cognitive behaviors. Our study helps better understand the functional segregation of gyri and sulci within task domains as well as the anatomo-functional-behavioral relationship of the human brain.},
  archive      = {J_TNNLS},
  author       = {Mingxin Jiang and Yuzhong Chen and Jiadong Yan and Zhenxiang Xiao and Wei Mao and Boyu Zhao and Shimin Yang and Zhongbo Zhao and Tuo Zhang and Lei Guo and Benjamin Becker and Dezhong Yao and Keith M. Kendrick and Xi Jiang},
  doi          = {10.1109/TNNLS.2022.3194733},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7435-7445},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Anatomy-guided spatio-temporal graph convolutional networks (AG-STGCNs) for modeling functional connectivity between gyri and sulci across multiple task domains},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hypergraph structural information aggregation generative
adversarial networks for diagnosis and pathogenetic factors
identification of alzheimer’s disease with imaging genetic data.
<em>TNNLS</em>, <em>35</em>(6), 7420–7434. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a neurodegenerative disease with profound pathogenetic causes. Imaging genetic data analysis can provide comprehensive insights into its causes. To fully utilize the multi-level information in the data, this article proposes a hypergraph structural information aggregation model, and constructs a novel deep learning method named hypergraph structural information aggregation generative adversarial networks (HSIA-GANs) for the automatic sample classification and accurate feature extraction. Specifically, HSIA-GAN is composed of generator and discriminator. The generator has three main functions. First, vertex graph and edge graph are constructed based on the input hypergraph to present the low-order relations. Second, the low-order structural information of hypergraph is extracted by the designed vertex convolution layers and edge convolution layers. Finally, the synthetic hypergraph is generated as the input of the discriminator. The discriminator can extract the high-order structural information directly from hypergraph through vertex-edge convolution, fuse the high and low-order structural information, and finalize the results through the full connection (FC) layers. Based on the data acquired from AD neuroimaging initiative, HSIA-GAN shows significant advantages in three classification tasks, and extracts discriminant features conducive to better disease classification.},
  archive      = {J_TNNLS},
  author       = {Xia-An Bi and Yu Wang and Sheng Luo and Ke Chen and Zhaoxu Xing and Luyun Xu},
  doi          = {10.1109/TNNLS.2022.3212700},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7420-7434},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hypergraph structural information aggregation generative adversarial networks for diagnosis and pathogenetic factors identification of alzheimer’s disease with imaging genetic data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient matching federated domain adaptation for brain
image classification. <em>TNNLS</em>, <em>35</em>(6), 7405–7419. (<a
href="https://doi.org/10.1109/TNNLS.2022.3223144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning has shown its unique advantages in many different tasks, including brain image analysis. It provides a new way to train deep learning models while protecting the privacy of medical image data from multiple sites. However, previous studies suggest that domain shift across different sites may influence the performance of federated models. As a solution, we propose a gradient matching federated domain adaptation (GM-FedDA) method for brain image classification, aiming to reduce domain discrepancy with the assistance of a public image dataset and train robust local federated models for target sites. It mainly includes two stages: 1) pretraining stage; we propose a one-common-source adversarial domain adaptation (OCS-ADA) strategy, i.e., adopting ADA with gradient matching loss to pretrain encoders for reducing domain shift at each target site (private data) with the assistance of a common source domain (public data) and 2) fine-tuning stage; we develop a gradient matching federated (GM-Fed) fine-tuning method for updating local federated models pretrained with the OCS-ADA strategy, i.e., pushing the optimization direction of a local federated model toward its specific local minimum by minimizing gradient matching loss between sites. Using fully connected networks as local models, we validate our method with the diagnostic classification tasks of schizophrenia and major depressive disorder based on multisite resting-state functional MRI (fMRI), respectively. Results show that the proposed GM-FedDA method outperforms other commonly used methods, suggesting the potential of our method in brain imaging analysis and other fields, which need to utilize multisite data while preserving data privacy.},
  archive      = {J_TNNLS},
  author       = {Ling-Li Zeng and Zhipeng Fan and Jianpo Su and Min Gan and Limin Peng and Hui Shen and Dewen Hu},
  doi          = {10.1109/TNNLS.2022.3223144},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7405-7419},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gradient matching federated domain adaptation for brain image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy attention neural network to tackle discontinuity in
airway segmentation. <em>TNNLS</em>, <em>35</em>(6), 7391–7404. (<a
href="https://doi.org/10.1109/TNNLS.2023.3269223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airway segmentation is crucial for the examination, diagnosis, and prognosis of lung diseases, while its manual delineation is unduly burdensome. To alleviate this time-consuming and potentially subjective manual procedure, researchers have proposed methods to automatically segment airways from computerized tomography (CT) images. However, some small-sized airway branches (e.g., bronchus and terminal bronchioles) significantly aggravate the difficulty of automatic segmentation by machine learning models. In particular, the variance of voxel values and the severe data imbalance in airway branches make the computational module prone to discontinuous and false-negative predictions, especially for cohorts with different lung diseases. The attention mechanism has shown the capacity to segment complex structures, while fuzzy logic can reduce the uncertainty in feature representations. Therefore, the integration of deep attention networks and fuzzy theory, given by the fuzzy attention layer, should be an escalated solution for better generalization and robustness. This article presents an efficient method for airway segmentation, comprising a novel fuzzy attention neural network (FANN) and a comprehensive loss function to enhance the spatial continuity of airway segmentation. The deep fuzzy set is formulated by a set of voxels in the feature map and a learnable Gaussian membership function. Different from the existing attention mechanism, the proposed channel-specific fuzzy attention addresses the issue of heterogeneous features in different channels. Furthermore, a novel evaluation metric is proposed to assess both the continuity and completeness of airway structures. The efficiency, generalization, and robustness of the proposed method have been proved by training on normal lung disease while testing on datasets of lung cancer, COVID-19, and pulmonary fibrosis.},
  archive      = {J_TNNLS},
  author       = {Yang Nan and Javier Del Ser and Zeyu Tang and Peng Tang and Xiaodan Xing and Yingying Fang and Francisco Herrera and Witold Pedrycz and Simon Walsh and Guang Yang},
  doi          = {10.1109/TNNLS.2023.3269223},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7391-7404},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fuzzy attention neural network to tackle discontinuity in airway segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MVCNet: Multiview contrastive network for unsupervised
representation learning for 3-d CT lesions. <em>TNNLS</em>,
<em>35</em>(6), 7376–7390. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the renaissance of deep learning, automatic diagnostic algorithms for computed tomography (CT) have achieved many successful applications. However, they heavily rely on lesion-level annotations, which are often scarce due to the high cost of collecting pathological labels. On the other hand, the annotated CT data, especially the 3-D spatial information, may be underutilized by approaches that model a 3-D lesion with its 2-D slices, although such approaches have been proven effective and computationally efficient. This study presents a multiview contrastive network (MVCNet), which enhances the representations of 2-D views contrastively against other views of different spatial orientations. Specifically, MVCNet views each 3-D lesion from different orientations to collect multiple 2-D views; it learns to minimize a contrastive loss so that the 2-D views of the same 3-D lesion are aggregated, whereas those of different lesions are separated. To alleviate the issue of false negative examples, the uninformative negative samples are filtered out, which results in more discriminative features for downstream tasks. By linear evaluation, MVCNet achieves state-of-the-art accuracies on the lung image database consortium and image database resource initiative (LIDC-IDRI) (88.62%), lung nodule database (LNDb) (76.69%), and TianChi (84.33%) datasets for unsupervised representation learning. When fine-tuned on 10% of the labeled data, the accuracies are comparable to the supervised learning models (89.46% versus 85.03%, 73.85% versus 73.44%, 83.56% versus 83.34% on the three datasets, respectively), indicating the superiority of MVCNet in learning representations with limited annotations. Our findings suggest that contrasting multiple 2-D views is an effective approach to capturing the original 3-D information, which notably improves the utilization of the scarce and valuable annotated CT data.},
  archive      = {J_TNNLS},
  author       = {Penghua Zhai and Huaiwei Cong and Enwei Zhu and Gangming Zhao and Yizhou Yu and Jinpeng Li},
  doi          = {10.1109/TNNLS.2022.3203412},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7376-7390},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MVCNet: Multiview contrastive network for unsupervised representation learning for 3-D CT lesions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive brain network learning via hierarchical signed
graph pooling model. <em>TNNLS</em>, <em>35</em>(6), 7363–7375. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, brain networks have been widely adopted to study brain dynamics, brain development, and brain diseases. Graph representation learning techniques on brain functional networks can facilitate the discovery of novel biomarkers for clinical phenotypes and neurodegenerative diseases. However, current graph learning techniques have several issues on brain network mining. First, most current graph learning models are designed for unsigned graph, which hinders the analysis of many signed network data (e.g., brain functional networks). Meanwhile, the insufficiency of brain network data limits the model performance on clinical phenotypes’ predictions. Moreover, few of the current graph learning models are interpretable, which may not be capable of providing biological insights for model outcomes. Here, we propose an interpretable hierarchical signed graph representation learning (HSGPL) model to extract graph-level representations from brain functional networks, which can be used for different prediction tasks. To further improve the model performance, we also propose a new strategy to augment functional brain network data for contrastive learning. We evaluate this framework on different classification and regression tasks using data from human connectome project (HCP) and open access series of imaging studies (OASIS). Our results from extensive experiments demonstrate the superiority of the proposed model compared with several state-of-the-art techniques. In addition, we use graph saliency maps, derived from these prediction tasks, to demonstrate detection and interpretation of phenotypic biomarkers.},
  archive      = {J_TNNLS},
  author       = {Haoteng Tang and Guixiang Ma and Lei Guo and Xiyao Fu and Heng Huang and Liang Zhan},
  doi          = {10.1109/TNNLS.2022.3220220},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7363-7375},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Contrastive brain network learning via hierarchical signed graph pooling model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multibranch CNN with MLP-mixer-based feature exploration for
high-performance disease diagnosis. <em>TNNLS</em>, <em>35</em>(6),
7351–7362. (<a
href="https://doi.org/10.1109/TNNLS.2023.3250490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based diagnosis is becoming an indispensable part of modern healthcare. For high-performance diagnosis, the optimal design of deep neural networks (DNNs) is a prerequisite. Despite its success in image analysis, existing supervised DNNs based on convolutional layers often suffer from their rudimentary feature exploration ability caused by the limited receptive field and biased feature extraction of conventional convolutional neural networks (CNNs), which compromises the network performance. Here, we propose a novel feature exploration network named manifold embedded multilayer perceptron (MLP) mixer (ME-Mixer), which utilizes both supervised and unsupervised features for disease diagnosis. In the proposed approach, a manifold embedding network is employed to extract class-discriminative features; then, two MLP-Mixer-based feature projectors are adopted to encode the extracted features with the global reception field. Our ME-Mixer network is quite general and can be added as a plugin to any existing CNN. Comprehensive evaluations on two medical datasets are performed. The results demonstrate that their approach greatly enhances the classification accuracy in comparison with different configurations of DNNs with acceptable computational complexity.},
  archive      = {J_TNNLS},
  author       = {Zixia Zhou and Md Tauhidul Islam and Lei Xing},
  doi          = {10.1109/TNNLS.2023.3250490},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7351-7362},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multibranch CNN with MLP-mixer-based feature exploration for high-performance disease diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An explainable and generalizable recurrent neural network
approach for differentiating human brain states on EEG dataset.
<em>TNNLS</em>, <em>35</em>(6), 7339–7350. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) is one of the most widely used brain computer interface (BCI) approaches. Despite the success of existing EEG approaches in brain state recognition studies, it is still challenging to differentiate brain states via explainable and generalizable deep learning approaches. In other words, how to explore meaningful and distinguishing features and how to overcome the huge variability and overfitting problem still need to be further studied. To alleviate these challenges, in this work, a multiple random fragment search-based multilayer recurrent neural network (MRFS-MRNN) is proposed to improve the differentiating performance and explore meaningful patterns. Specifically, an explainable MRNN module is proposed to capture the temporal dependences preserved in EEG time series. Besides, a MRFS module is designed to cut multiple random fragments from the entire EEG signal time course to improve the effectiveness of brain state differentiating ability. MRFS-MRNN is concatenatedto effectively overcome the huge variabilities and overfitting problems. Experiment results demonstrate that the proposed MRFS-MRNN model not only has excellent differentiating performance, but also has good explanation and generalization ability. The classification accuracies reach as high as 95.18% for binary classification and 89.19% for four-category classification on the individual level. Similarly, 95.53% and 85.84% classification accuracies are obtained for the binary and four-category classification on the group level. What’s more, 94.28% and 85.43% classification accuracies of binary and four-category classifications are achieved for predicting brand new subjects. The experiment results showed that the proposed method outperformed other state-of-the-art (SOTA) models on the same underlying data and improved the explanation and generalization ability.},
  archive      = {J_TNNLS},
  author       = {Shu Zhang and Lin Wu and Sigang Yu and Enze Shi and Ning Qiang and Huan Gao and Jingyi Zhao and Shijie Zhao},
  doi          = {10.1109/TNNLS.2022.3214225},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7339-7350},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An explainable and generalizable recurrent neural network approach for differentiating human brain states on EEG dataset},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GMILT: A novel transformer network that can noninvasively
predict EGFR mutation status. <em>TNNLS</em>, <em>35</em>(6), 7324–7338.
(<a href="https://doi.org/10.1109/TNNLS.2022.3190671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noninvasively and accurately predicting the epidermal growth factor receptor (EGFR) mutation status is a clinically vital problem. Moreover, further identifying the most suspicious area related to the EGFR mutation status can guide the biopsy to avoid false negatives. Deep learning methods based on computed tomography (CT) images may improve the noninvasive prediction of EGFR mutation status and potentially help clinicians guide biopsies by visual methods. Inspired by the potential inherent links between EGFR mutation status and invasiveness information, we hypothesized that the predictive performance of a deep learning network can be improved through extra utilization of the invasiveness information. Here, we created a novel explainable transformer network for EGFR classification named gated multiple instance learning transformer (GMILT) by integrating multi-instance learning and discriminative weakly supervised feature learning. Pathological invasiveness information was first introduced into the multitask model as embeddings. GMILT was trained and validated on a total of 512 patients with adenocarcinoma and tested on three datasets (the internal test dataset, the external test dataset, and The Cancer Imaging Archive (TCIA) public dataset). The performance (area under the curve (AUC) $=0.772$ on the internal test dataset) of GMILT exceeded that of previously published methods and radiomics-based methods (i.e., random forest and support vector machine) and attained a preferable generalization ability (AUC $=0.856$ in the TCIA test dataset and AUC $=0.756$ in the external dataset). A diameter-based subgroup analysis further verified the efficiency of our model (most of the AUCs exceeded 0.772) to noninvasively predict EGFR mutation status from computed tomography (CT) images. In addition, because our method also identified the “core area” of the most suspicious area related to the EGFR mutation status, it has the potential ability to guide biopsies.},
  archive      = {J_TNNLS},
  author       = {Wei Zhao and Weidao Chen and Ge Li and Du Lei and Jiancheng Yang and Yanjing Chen and Yingjia Jiang and Jiangfen Wu and Bingbing Ni and Yeqi Sun and Shaokang Wang and Yingli Sun and Ming Li and Jun Liu},
  doi          = {10.1109/TNNLS.2022.3190671},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7324-7338},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GMILT: A novel transformer network that can noninvasively predict EGFR mutation status},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GCNs-net: A graph convolutional neural network approach for
decoding time-resolved EEG motor imagery signals. <em>TNNLS</em>,
<em>35</em>(6), 7312–7323. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Toward the development of effective and efficient brain–computer interface (BCI) systems, precise decoding of brain activity measured by an electroencephalogram (EEG) is highly demanded. Traditional works classify EEG signals without considering the topological relationship among electrodes. However, neuroscience research has increasingly emphasized network patterns of brain dynamics. Thus, the Euclidean structure of electrodes might not adequately reflect the interaction between signals. To fill the gap, a novel deep learning (DL) framework based on the graph convolutional neural networks (GCNs) is presented to enhance the decoding performance of raw EEG signals during different types of motor imagery (MI) tasks while cooperating with the functional topological relationship of electrodes. Based on the absolute Pearson’s matrix of overall signals, the graph Laplacian of EEG electrodes is built up. The GCNs-Net constructed by graph convolutional layers learns the generalized features. The followed pooling layers reduce dimensionality, and the fully-connected (FC) softmax layer derives the final prediction. The introduced approach has been shown to converge for both personalized and groupwise predictions. It has achieved the highest averaged accuracy, 93.06% and 88.57% (PhysioNet dataset), 96.24% and 80.89% (high gamma dataset), at the subject and group level, respectively, compared with existing studies, which suggests adaptability and robustness to individual variability. Moreover, the performance is stably reproducible among repetitive experiments for cross-validation. The excellent performance of our method has shown that it is an important step toward better BCI approaches. To conclude, the GCNs-Net filters EEG signals based on the functional topological relationship, which manages to decode relevant features for brain MI. A DL library for EEG task classification including the code for this study is open source at https://github.com/SuperBruceJia/ EEG-DL for scientific research.},
  archive      = {J_TNNLS},
  author       = {Yimin Hou and Shuyue Jia and Xiangmin Lun and Ziqian Hao and Yan Shi and Yang Li and Rui Zeng and Jinglei Lv},
  doi          = {10.1109/TNNLS.2022.3202569},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7312-7323},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GCNs-net: A graph convolutional neural network approach for decoding time-resolved EEG motor imagery signals},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fourier domain robust denoising decomposition and adaptive
patch MRI reconstruction. <em>TNNLS</em>, <em>35</em>(6), 7299–7311. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparsity of the Fourier transform domain has been applied to magnetic resonance imaging (MRI) reconstruction in $k$ -space. Although unsupervised adaptive patch optimization methods have shown promise compared to data-driven-based supervised methods, the following challenges exist in MRI reconstruction: 1) in previous $k$ -space MRI reconstruction tasks, MRI with noise interference in the acquisition process is rarely considered. 2) Differences in transform domains should be resolved to achieve the high-quality reconstruction of low undersampled MRI data. 3) Robust patch dictionary learning problems are usually nonconvex and NP-hard, and alternate minimization methods are often computationally expensive. In this article, we propose a method for Fourier domain robust denoising decomposition and adaptive patch MRI reconstruction (DDAPR). DDAPR is a two-step optimization method for MRI reconstruction in the presence of noise and low undersampled data. It includes the low-rank and sparse denoising reconstruction model (LSDRM) and the robust dictionary learning reconstruction model (RDLRM). In the first step, we propose LSDRM for different domains. For the optimization solution, the proximal gradient method is used to optimize LSDRM by singular value decomposition and soft threshold algorithms. In the second step, we propose RDLRM, which is an effective adaptive patch method by introducing a low-rank and sparse penalty adaptive patch dictionary and using a sparse rank-one matrix to approximate the undersampled data. Then, the block coordinate descent (BCD) method is used to optimize the variables. The BCD optimization process involves valid closed-form solutions. Extensive numerical experiments show that the proposed method has a better performance than previous methods in image reconstruction based on compressed sensing or deep learning.},
  archive      = {J_TNNLS},
  author       = {Junpeng Tan and Xin Zhang and Chunmei Qing and Xiangmin Xu},
  doi          = {10.1109/TNNLS.2022.3222394},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7299-7311},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fourier domain robust denoising decomposition and adaptive patch MRI reconstruction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoupled unbiased teacher for source-free domain adaptive
medical object detection. <em>TNNLS</em>, <em>35</em>(6), 7287–7298. (<a
href="https://doi.org/10.1109/TNNLS.2023.3272389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) aims to adapt a lightweight pretrained source model to unlabeled new domains without the original labeled source data. Due to the privacy of patients and storage consumption concerns, SFDA is a more practical setting for building a generalized model in medical object detection. Existing methods usually apply the vanilla pseudo-labeling technique, while neglecting the bias issues in SFDA, leading to limited adaptation performance. To this end, we systematically analyze the biases in SFDA medical object detection by constructing a structural causal model (SCM) and propose an unbiased SFDA framework dubbed decoupled unbiased teacher (DUT). Based on the SCM, we derive that the confounding effect causes biases in the SFDA medical object detection task at the sample level, feature level, and prediction level. To prevent the model from emphasizing easy object patterns in the biased dataset, a dual invariance assessment (DIA) strategy is devised to generate counterfactual synthetics. The synthetics are based on unbiased invariant samples in both discrimination and semantic perspectives. To alleviate overfitting to domain-specific features in SFDA, we design a cross-domain feature intervention (CFI) module to explicitly deconfound the domain-specific prior with feature intervention and obtain unbiased features. Besides, we establish a correspondence supervision prioritization (CSP) strategy for addressing the prediction bias caused by coarse pseudo-labels by sample prioritizing and robust box supervision. Through extensive experiments on multiple SFDA medical object detection scenarios, DUT yields superior performance over previous state-of-the-art unsupervised domain adaptation (UDA) and SFDA counterparts, demonstrating the significance of addressing the bias issues in this challenging task. The code is available at https://github.com/CUHK-AIM-Group/Decoupled-Unbiased-Teacher .},
  archive      = {J_TNNLS},
  author       = {Xinyu Liu and Wuyang Li and Yixuan Yuan},
  doi          = {10.1109/TNNLS.2023.3272389},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7287-7298},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decoupled unbiased teacher for source-free domain adaptive medical object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial learning based node-edge graph attention
networks for autism spectrum disorder identification. <em>TNNLS</em>,
<em>35</em>(6), 7275–7286. (<a
href="https://doi.org/10.1109/TNNLS.2022.3154755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have received increasing interest in the medical imaging field given their powerful graph embedding ability to characterize the non-Euclidean structure of brain networks based on magnetic resonance imaging (MRI) data. However, previous studies are largely node-centralized and ignore edge features for graph classification tasks, resulting in moderate performance of graph classification accuracy. Moreover, the generalizability of GNN model is still far from satisfactory in brain disorder [e.g., autism spectrum disorder (ASD)] identification due to considerable individual differences in symptoms among patients as well as data heterogeneity among different sites. In order to address the above limitations, this study proposes a novel adversarial learning-based node–edge graph attention network (AL-NEGAT) for ASD identification based on multimodal MRI data. First, both node and edge features are modeled based on structural and functional MRI data to leverage complementary brain information and preserved in the constructed weighted adjacent matrix for individuals through the attention mechanism in the proposed NEGAT. Second, two AL methods are employed to improve the generalizability of NEGAT. Finally, a gradient-based saliency map strategy is utilized for model interpretation to identify important brain regions and connections contributing to the classification. Experimental results based on the public Autism Brain Imaging Data Exchange I (ABIDE I) data demonstrate that the proposed framework achieves a classification accuracy of 74.7% between ASD and typical developing (TD) groups based on 1007 subjects across 17 different sites and outperforms the state-of-the-art methods, indicating satisfying classification ability and generalizability of the proposed AL-NEGAT model. Our work provides a powerful tool for brain disorder identification.},
  archive      = {J_TNNLS},
  author       = {Yuzhong Chen and Jiadong Yan and Mingxin Jiang and Tuo Zhang and Zhongbo Zhao and Weihua Zhao and Jian Zheng and Dezhong Yao and Rong Zhang and Keith M. Kendrick and Xi Jiang},
  doi          = {10.1109/TNNLS.2022.3154755},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7275-7286},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial learning based node-edge graph attention networks for autism spectrum disorder identification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Editorial special issue on explainable and generalizable
deep learning for medical imaging. <em>TNNLS</em>, <em>35</em>(6),
7271–7274. (<a
href="https://doi.org/10.1109/TNNLS.2024.3395937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancements in deep learning technologies have profoundly influenced the field of medical image analysis, yet their full integration into clinical radiology practices has not progressed as quickly as expected. A significant hurdle to their widespread adoption among radiologists and clinicians is the prevailing lack of trust and confidence in the outcomes produced by these technologies. This concern primarily stems from concerns regarding the explainability and generalizability of deep learning models within the realm of medical imaging. As part of the responses from the Medical Image Analysis Community to address these critical issues, we organized the IEEE Transactions on Neural Networks and Learning Systems (TNNLS) Special Issue on explainable and generalizable deep learning for medical imaging. This IEEE TNNLS Special Issue calls for original and innovative methodological contributions that aim to address the key challenges on explainability and generalizability of deep learning for medical imaging. This IEEE TNNLS Special Issue emphasizes the research and advanced development of the technical aspects of new image analysis methodologies, and all the developed new methods should also be evaluated or validated on real and large-scale medical imaging data.},
  archive      = {J_TNNLS},
  author       = {Tianming Liu and Dajiang Zhu and Fei Wang and Islem Rekik and Xia Hu and Dinggang Shen},
  doi          = {10.1109/TNNLS.2024.3395937},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {6},
  number       = {6},
  pages        = {7271-7274},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Editorial special issue on explainable and generalizable deep learning for medical imaging},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aspect-aware graph attention network for heterogeneous
information networks. <em>TNNLS</em>, <em>35</em>(5), 7259–7266. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) derive inspiration from recent advances in computer vision, by stacking layers of first-order filters followed by a nonlinear activation function to learn entity or graph embeddings. Although GCNs have been shown to boost the performance of many network analysis tasks, they still face tremendous challenges in learning from Heterogeneous Information Networks (HINs), where relations play a decisive role in knowledge reasoning. What’s more, there are multiaspect representations of entities in HINs, and a filter learned in one aspect do not necessarily apply to another. We address these challenges by proposing the Aspect-Aware Graph Attention Network (AGAT), a model that extends GCNs with alternative learnable filters to incorporate entity and relational information. Instead of focusing on learning the general entity embeddings, AGAT learns the adaptive entity embeddings based on prediction scenario. Experiments of link prediction and semi-supervised classification verify the effectiveness of our algorithm.},
  archive      = {J_TNNLS},
  author       = {Qidong Liu and Cheng Long and Jie Zhang and Mingliang Xu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2022.3213799},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7259-7266},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Aspect-aware graph attention network for heterogeneous information networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composing synergistic macro actions for reinforcement
learning agents. <em>TNNLS</em>, <em>35</em>(5), 7251–7258. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Macro actions have been demonstrated to be beneficial for the learning processes of an agent and have encouraged a variety of techniques to be developed for constructing more effective ones. However, previous techniques usually do not further consider combining macro actions to form a synergistic macro action ensemble, in which synergism exhibits when the constituent macro actions are favorable to be jointly used by an agent during evaluation. Such a synergistic macro action ensemble may potentially allow an agent to perform even better than the individual macro actions within it. Motivated by the recent advances of neural architecture search (NAS), in this brief, we formulate the construction of a synergistic macro action ensemble as a Markov decision process (MDP) and evaluate the constructed macro action ensemble as a whole. Such a problem formulation enables synergism to be taken into account by the proposed evaluation procedure. Our experimental results demonstrate that the proposed framework is able to discover the synergistic macro action ensembles. Furthermore, we also highlight the benefits of these macro action ensembles through a set of analytical cases.},
  archive      = {J_TNNLS},
  author       = {Yu-Ming Chen and Kaun-Yu Chang and Chien Liu and Tsu-Ching Hsiao and Zhang-Wei Hong and Chun-Yi Lee},
  doi          = {10.1109/TNNLS.2022.3213606},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7251-7258},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Composing synergistic macro actions for reinforcement learning agents},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view subspace clustering via structured multi-pathway
network. <em>TNNLS</em>, <em>35</em>(5), 7244–7250. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep multi-view clustering (MVC) has attracted increasing attention in multi-view learning owing to its promising performance. However, most existing deep multi-view methods use single-pathway neural networks to extract features of each view, which cannot explore comprehensive complementary information and multilevel features. To tackle this problem, we propose a deep structured multi-pathway network (SMpNet) for multi-view subspace clustering task in this brief. The proposed SMpNet leverages structured multi-pathway convolutional neural networks to explicitly learn the subspace representations of each view in a layer-wise way. By this means, both low-level and high-level structured features are integrated through a common connection matrix to explore the comprehensive complementary structure among multiple views. Moreover, we impose a low-rank constraint on the connection matrix to decrease the impact of noise and further highlight the consensus information of all the views. Experimental results on five public datasets show the effectiveness of the proposed SMpNet compared with several state-of-the-art deep MVC methods.},
  archive      = {J_TNNLS},
  author       = {Qianqian Wang and Zhiqiang Tao and Quanxue Gao and Licheng Jiao},
  doi          = {10.1109/TNNLS.2022.3213374},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7244-7250},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-view subspace clustering via structured multi-pathway network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network boundary approximation for uncertain
nonlinear spatiotemporal systems and its application of tracking
control. <em>TNNLS</em>, <em>35</em>(5), 7238–7243. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief addresses the neural network (NN) approximation problem for uncertain nonlinear systems with time-varying parameters (that is, unknown nonlinear spatiotemporal systems). Due to the fact that the unknown spatiotemporal functions cannot be directly approximated by NNs, a so-called time-varying parameter extraction is given to separate time-varying parameters from uncertain nonlinear spatiotemporal functions. By using the supremum of Euler norm of the extracted time-varying parameters, the nonlinear spatiotemporal function is mapped to an unknown state-based boundary function, which can be approximated by NNs. Based on the time-varying parameter extraction, an adaptive neural tracking control law is designed for uncertain strict-feedback nonlinear spatiotemporal systems, which guarantees the convergence of the tracking error with a trajectory performance. The effectiveness of the designed method is verified by simulations.},
  archive      = {J_TNNLS},
  author       = {Faxiang Zhang and Yang-Yang Chen and Ya Zhang},
  doi          = {10.1109/TNNLS.2022.3212696},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7238-7243},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network boundary approximation for uncertain nonlinear spatiotemporal systems and its application of tracking control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Seeing all from a few: Nodes selection using graph pooling
for graph clustering. <em>TNNLS</em>, <em>35</em>(5), 7231–7237. (<a
href="https://doi.org/10.1109/TNNLS.2022.3210370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been considerable research interest in graph clustering aimed at data partition using graph information. However, one limitation of most graph-based methods is that they assume that the graph structure to operate is reliable. However, there are inevitably some edges in the graph that are not conducive to graph clustering, which we call spurious edges. This brief is the first attempt to employ the graph pooling technique for node clustering to the best of our knowledge. In this brief, we propose a novel dual graph embedding network (DGEN), which is designed as a two-step graph encoder connected by a graph pooling layer to learn the graph embedding. In DGEN, we assume that if a node and its nearest neighboring node are close to the same clustering center, this node is informative, and this edge can be considered as a cluster-friendly edge. Based on this assumption, the neighbor cluster pooling (NCPool) is devised to select the most informative subset of nodes and the corresponding edges based on the distance of nodes and their nearest neighbors to the cluster centers. This can effectively alleviate the impact of the spurious edges on the clustering. Finally, to obtain the clustering assignment of all nodes, a classifier is trained using the clustering results of the selected nodes. Experiments on five benchmark graph datasets demonstrate the superiority of the proposed method over state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Yiming Wang and Dongxia Chang and Zhiqiang Fu and Yao Zhao},
  doi          = {10.1109/TNNLS.2022.3210370},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7231-7237},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Seeing all from a few: Nodes selection using graph pooling for graph clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A joint intensity-neuromorphic event imaging system with
bandwidth-limited communication channel. <em>TNNLS</em>, <em>35</em>(5),
7216–7230. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel adaptive multimodal intensity-event algorithm to optimize an overall objective of object tracking under bit rate constraints for a host-chip architecture. The chip is a computationally resource-constrained device acquiring high-resolution intensity frames and events, while the host is capable of performing computationally expensive tasks. We develop a joint intensity-neuromorphic event rate-distortion compression framework with a quadtree (QT)-based compression of intensity and events scheme. The goal of this compression framework is to optimally allocate bits to the intensity frames and neuromorphic events based on the minimum distortion at a given communication channel capacity. The data acquisition on the chip is driven by the presence of objects of interest in the scene as detected by an object detector. The most informative intensity and event data are communicated to the host under rate constraints so that the best possible tracking performance is obtained. The detection and tracking of objects in the scene are done on the distorted data at the host. Intensity and events are jointly used in a fusion framework to enhance the quality of the distorted images, in order to improve the object detection and tracking performance. The performance assessment of the overall system is done in terms of the multiple object tracking accuracy (MOTA) score. Compared with using intensity modality only, there is an improvement in MOTA using both these modalities in different scenarios.},
  archive      = {J_TNNLS},
  author       = {Srutarshi Banerjee and Henry H. Chopp and Jianping Zhang and Zihao W. Wang and Peng Kang and Oliver Cossairt and Aggelos Katsaggelos},
  doi          = {10.1109/TNNLS.2022.3214779},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7216-7230},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A joint intensity-neuromorphic event imaging system with bandwidth-limited communication channel},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential label enhancement. <em>TNNLS</em>,
<em>35</em>(5), 7204–7215. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution learning (LDL) is a novel machine learning paradigm for solving ambiguous tasks, where the degree to which each label describing the instance is ambiguous. However, obtaining the label distribution is high cost and the description degree is difficult to quantify. Most existing research works focus on designing an objective function to obtain the whole description degrees at once but seldom care about the sequentiality in the process of recovering the label distribution. In this article, we formulate the label distribution recovering task as a sequential decision process called sequential label enhancement (Seq_LE), which is more consistent with the process of annotating the label distribution in human brains. Specifically, the discrete label and its description degree are serially mapped by the reinforcement learning (RL) agent. Besides, we carefully design a joint reward function to drive the agent to fully learn the optimal decision policy. Extensive experiments on 16 LDL datasets are conducted under various evaluation metrics. The experimental results demonstrate convincingly that the proposed sequential label enhancement (LE) leads to better performance over the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yongbiao Gao and Ke Wang and Xin Geng},
  doi          = {10.1109/TNNLS.2022.3214610},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7204-7215},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sequential label enhancement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep class-incremental learning from decentralized data.
<em>TNNLS</em>, <em>35</em>(5), 7190–7203. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we focus on a new and challenging decentralized machine learning paradigm in which there are continuous inflows of data to be addressed and the data are stored in multiple repositories. We initiate the study of data-decentralized class-incremental learning (DCIL) by making the following contributions. First, we formulate the DCIL problem and develop the experimental protocol. Second, we introduce a paradigm to create a basic decentralized counterpart of typical (centralized) CIL approaches, and as a result, establish a benchmark for the DCIL study. Third, we further propose a decentralized composite knowledge incremental distillation (DCID) framework to transfer knowledge from historical models and multiple local sites to the general model continually. DCID consists of three main components, namely, local CIL, collaborated knowledge distillation (KD) among local models, and aggregated KD from local models to the general one. We comprehensively investigate our DCID framework by using a different implementation of the three components. Extensive experimental results demonstrate the effectiveness of our DCID framework. The source code of the baseline methods and the proposed DCIL is available at https://github.com/Vision-Intelligence-and-Robots-Group/DCIL .},
  archive      = {J_TNNLS},
  author       = {Xiaohan Zhang and Songlin Dong and Jinjie Chen and Qi Tian and Yihong Gong and Xiaopeng Hong},
  doi          = {10.1109/TNNLS.2022.3214573},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7190-7203},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep class-incremental learning from decentralized data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Robust principal component analysis via joint
reconstruction and projection. <em>TNNLS</em>, <em>35</em>(5),
7175–7189. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is one of the most widely used unsupervised dimensionality reduction algorithms, but it is very sensitive to outliers because the squared $\ell _{2}$ -norm is used as distance metric. Recently, many scholars have devoted themselves to solving this difficulty. They learn the projection matrix from minimum reconstruction error or maximum projection variance as the starting point, which leads them to ignore a serious problem, that is, the original PCA learns the projection matrix by minimizing the reconstruction error and maximizing the projection variance simultaneously, but they only consider one of them, which imposes various limitations on the performance of model. To solve this problem, we propose a novel robust principal component analysis via joint reconstruction and projection, namely, RPCA-RP, which combines reconstruction error and projection variance to fully mine the potential information of data. Furthermore, we carefully design a discrete weight for model to implicitly distinguish between normal data and outliers, so as to easily remove outliers and improve the robustness of method. In addition, we also unexpectedly discovered that our method has anomaly detection capabilities. Subsequently, an effective iterative algorithm is explored to solve this problem and perform related theoretical analysis. Extensive experimental results on several real-world datasets and RGB large-scale dataset demonstrate the superiority of our method.},
  archive      = {J_TNNLS},
  author       = {Sisi Wang and Feiping Nie and Zheng Wang and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3214307},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7175-7189},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust principal component analysis via joint reconstruction and projection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Denoised non-local neural network for semantic segmentation.
<em>TNNLS</em>, <em>35</em>(5), 7162–7174. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-local (NL) network has become a widely used technique for semantic segmentation, which computes an attention map to measure the relationships of each pixel pair. However, most of the current popular NL models tend to ignore the phenomenon that the calculated attention map appears to be very noisy, containing interclass and intraclass inconsistencies, which lowers the accuracy and reliability of the NL methods. In this article, we figuratively denote these inconsistencies as attention noises and explore the solutions to denoise them. Specifically, we inventively propose a denoised NL network, which consists of two primary modules, i.e., the global rectifying (GR) block and the local retention (LR) block, to eliminate the interclass and intraclass noises, respectively. First, GR adopts the class-level predictions to capture a binary map to distinguish whether the selected two pixels belong to the same category. Second, LR captures the ignored local dependencies and further uses them to rectify the unwanted hollows in the attention map. The experimental results on two challenging semantic segmentation datasets demonstrate the superior performance of our model. Without any external training data, our proposed denoised NL can achieve the state-of-the-art performance of 83.5% and 46.69% mean of classwise intersection over union (mIoU) on Cityscapes and ADE20K, respectively.},
  archive      = {J_TNNLS},
  author       = {Qi Song and Jie Li and Hao Guo and Rui Huang},
  doi          = {10.1109/TNNLS.2022.3214216},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7162-7174},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Denoised non-local neural network for semantic segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Query-adaptive late fusion for hierarchical fine-grained
video-text retrieval. <em>TNNLS</em>, <em>35</em>(5), 7150–7161. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a hierarchical fine-grained fusion mechanism has been proved effective in cross-modal retrieval between videos and texts. Generally, the hierarchical fine-grained semantic representations (video-text semantic matching is decomposed into three levels including global-event representation matching, action–relation representation matching, and local-entity representation matching) to be fused can work well by themselves for the query. However, in real-world scenarios and applications, existing methods failed to adaptively estimate the effectiveness of multiple levels of the semantic representations for a given query in advance of multilevel fusion, resulting in a worse performance than expected. As a result, it is extremely essential to identify the effectiveness of hierarchical semantic representations in a query-adaptive manner. To this end, this article proposes an effective query-adaptive multilevel fusion (QAMF) model based on manipulating multiple similarity scores between the hierarchical visual and text representations. First, we decompose video-side and text-side representations into hierarchical semantic representations consisting of global-event level, action-relation level, and local-entity level, respectively. Then, the multilevel representation of the video-text pair is aligned to calculate the similarity score for each level. Meanwhile, the sorted similarity score curves of the good semantic representation are different from the inferior ones, which exhibit a “cliff” shape and gradually decline (see Fig. 1 as an example). Finally, we leverage the Gaussian decay function to fit the tail of the score curve and calculate the area under the normalized sorted similarity curve as the indicator of semantic representation effectiveness, namely, the area of good semantic representation is small, and vice versa. Extensive experiments on three public benchmark video-text datasets have demonstrated that our method consistently outperforms the state-of-the-art (SoTA). A simple demo of QAMF will soon be publicly available on our homepage: https://github.com/Lab-ANT .},
  archive      = {J_TNNLS},
  author       = {Wentao Ma and Qingchao Chen and Fang Liu and Tongqing Zhou and Zhiping Cai},
  doi          = {10.1109/TNNLS.2022.3214208},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7150-7161},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Query-adaptive late fusion for hierarchical fine-grained video-text retrieval},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamical modeling and qualitative analysis of a delayed
model for CD8 t cells in response to viral antigens. <em>TNNLS</em>,
<em>35</em>(5), 7138–7149. (<a
href="https://doi.org/10.1109/TNNLS.2022.3214076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the immune effector CD8 T cells play a crucial role in clearance of viruses, the mechanisms underlying the dynamics of how CD8 T cells respond to viral infection remain largely unexplored. Here, we develop a delayed model that incorporates CD8 T cells and infected cells to investigate the functional role of CD8 T cells in persistent virus infection. Bifurcation analysis reveals that the model has four steady states that can finely divide the progressions of viral infection into four states, and endows the model with bistability that has ability to achieve the switch from one state to another. Furthermore, analytical and numerical methods find that the time delay resulting from incubation period of virus can induce a stable low-infection steady state to be oscillatory, coexisting with a stable high-infection steady state in phase space. In particular, a novel mechanism to achieve the switch between two stable steady states, time-delay-based switch, is proposed, where the initial conditions and other parameters of the model remain unchanged. Moreover, our model predicts that, for a certain range of initial antigen load: 1) under a longer incubation period, the lower the initial antigen load, the easier the virus infection will evolve into severe state; while the higher the initial antigen load, the easier it is for the virus infection to be effectively controlled and 2) only when the incubation period is small, the lower the initial antigen load, the easier it is to effectively control the infection progression. Our results are consistent with multiple experimental observations, which may facilitate the understanding of the dynamical and physiological mechanisms of CD8 T cells in response to viral infections.},
  archive      = {J_TNNLS},
  author       = {Yuan Zhang and Yijia Chen and Jinde Cao and Haihong Liu and Zhouhong Li},
  doi          = {10.1109/TNNLS.2022.3214076},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7138-7149},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamical modeling and qualitative analysis of a delayed model for CD8 t cells in response to viral antigens},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Event-triggered constrained optimal control for organic
rankine cycle systems via safe reinforcement learning. <em>TNNLS</em>,
<em>35</em>(5), 7126–7137. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The organic Rankine cycle (ORC) is an effective application for converting low-grade heat sources into power and is crucial for environmentally friendly production and energy recovery. However, the inherent complexity of the mechanism, its strong and unidentified nonlinearity, and the presence of control constraints severely impair the design of its optimal controller. To solve these issues, this study provides a novel event-triggered (ET) constrained optimal control approach for the ORC systems based on a safe reinforcement learning technique to find the optimal control law. Instead of employing the usual non-quadratic integral form to solve the control-limited optimal control problems, a constraint handling strategy based on a relaxed weighted barrier function (BF) technique is proposed. By adding the BF terms to the original value function, a modified value iteration algorithm is developed to make the control input solutions that tend to violate the constraints be pushed back and maintained in their safe sets. In addition, the ET mechanism proposed in this article is critically required for the ORC systems, and it can significantly reduce the computational load. The combination of these two techniques allows the ORC systems to achieve set-point tracking control and satisfy the control restrictions. The proposed approach is conducted based on a heuristic dynamic programming framework with three neural networks (NNs) involved. The safety and convergence of the proposed approach and the stability of the closed-loop system are analyzed. Simulation results and comparisons are presented to demonstrate its effectiveness.},
  archive      = {J_TNNLS},
  author       = {Lingzhi Zhang and Runze Lin and Lei Xie and Wei Dai and Hongye Su},
  doi          = {10.1109/TNNLS.2022.3213825},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7126-7137},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered constrained optimal control for organic rankine cycle systems via safe reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Memory and communication efficient federated kernel
k-means. <em>TNNLS</em>, <em>35</em>(5), 7114–7125. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A federated kernel $k$ -means (FedKKM) algorithm is developed in this article to conduct distributed clustering with low memory consumption on user devices. In FedKKM, a federated eigenvector approximation (FEA) algorithm is designed to iteratively determine the low-dimensional approximate vectors of the transformed feature vectors, using only low-dimensional random feature vectors. To maintain high communication efficiency in each iteration of FEA, a communication-efficient Lanczos algorithm (CELA) is further designed in FEA to reduce the communication cost. Based on the low-dimensional approximate vectors, the clustering result is obtained by leveraging a distributed linear $k$ -means algorithm. A theoretical analysis shows that: 1) FEA has a convergence rate of $O(1/T)$ , where $T$ is the number of iterations; 2) the scalability of FedKKM is not affected by the dataset size since the communication cost of FedKKM is independent of the number of users’ data; and 3) FedKKM is a $(1+\epsilon)$ approximation algorithm. The experimental results show that FedKKM achieves the comparable clustering quality to that of a centralized kernel $k$ -means. Compared with state-of-the-art schemes, FedKKM reduces the memory consumption on user devices by up to 94% and also reduces the communication cost by more than 40%.},
  archive      = {J_TNNLS},
  author       = {Xiaochen Zhou and Xudong Wang},
  doi          = {10.1109/TNNLS.2022.3213777},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7114-7125},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memory and communication efficient federated kernel k-means},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep spectral clustering with constrained laplacian rank.
<em>TNNLS</em>, <em>35</em>(5), 7102–7113. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering (SC) is a well-performed and prevalent technique for data processing and analysis, which has attracted significant attention in the field of clustering. While the scalability and generalization ability of this method make it prohibitive for the large-scale dataset and the out-of-sample-extension problem. In this work, we propose a new efficient deep clustering architecture based on SC, named deep SC (DSC) with constrained Laplacian rank (DSCCLR). DSCCLR develops a self-adaptive affinity matrix with a clustering-friendly structure by constraining the Laplacian rank, which greatly mines the intrinsic relationships. Meanwhile, by introducing a simple fully connected network with an orthogonality constraint on the last layer, DSCCLR learns discriminative representations in a short training time. The proposed method has the following salient properties: 1) it overcomes limited generalization ability and scalability of the existing DSC methods; 2) it explores the intrinsic relationship between samples in the affinity matrix, which maintains the latent manifold of data as much as possible; and 3) it alleviates the complexity of eigendecomposition via a simple but effective fully connected network. The extensive empirical results demonstrate the superiorities of DSCCLR over other 17 clustering methods.},
  archive      = {J_TNNLS},
  author       = {Xuelong Li and Tengfei Wei and Yang Zhao},
  doi          = {10.1109/TNNLS.2022.3213756},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7102-7113},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep spectral clustering with constrained laplacian rank},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convergence of the distributed SG algorithm under
cooperative excitation condition. <em>TNNLS</em>, <em>35</em>(5),
7087–7101. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a distributed stochastic gradient (SG) algorithm is proposed where the estimators are aimed to collectively estimate an unknown time-invariant parameter from a set of noisy measurements obtained by distributed sensors. The proposed distributed SG algorithm combines the consensus strategy of the estimation of neighbors with the diffusion of regression vectors. For the theoretical investigation of the proposed algorithm, the main challenge lies in analyzing the influence of the Laplacian matrix on the state transition matrix and the properties of the product of nonindependent and nonstationary random matrices. Some analysis techniques such as graph theory and martingale theory are used to deal with the above issues. A cooperative excitation condition is introduced, under which the convergence of the distributed SG algorithm can be obtained without relying on the independency or stationarity assumptions of regression vectors which are commonly used in the existing literature. Furthermore, the convergence rate of the algorithm can be established. Finally, we show that all the sensors can cooperate to fulfill the estimation task even though any individual sensor cannot by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Die Gan and Zhixin Liu},
  doi          = {10.1109/TNNLS.2022.3213715},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7087-7101},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convergence of the distributed SG algorithm under cooperative excitation condition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Minicolumn-based episodic memory model with spiking
neurons, dendrites and delays. <em>TNNLS</em>, <em>35</em>(5),
7072–7086. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Episodic memory is fundamental to the brain’s cognitive function, but how neuronal activity is temporally organized during its encoding and retrieval is still unknown. In this article, combining hippocampus structure with a spiking neural network (SNN), a new bionic spiking temporal memory (BSTM) model is proposed to explore the encoding, formation, and retrieval of episodic memory. For encoding episodic memory, the spike-timing-dependent-plasticity (STDP) learning algorithm and a proposed minicolumn selection algorithm are used to encode each input item into several active minicolumns. For the formation of episodic memory, a sequential memory algorithm is proposed to store the contexts between items. For retrieval of episodic memory, the local retrieval algorithm and the global retrieval algorithm are proposed to retrieve sequence information, achieving multisentence prediction and multitime step prediction. All functions of BSTM are based on bionic spiking neurons, which have biological characteristics including columnar and dendritic structures, firing and receiving spikes, and delaying transmission. To test the performance of the BSTM model, the Children’s Book Test (CBT) data set was used to conduct a series of experiments under different settings, including changing the number of minicolumns, neurons and sequences, modifying sequence items, etc. Compared to other sequence memory algorithms, the experimental results show that the proposed BSTM achieves higher accuracy and better robustness.},
  archive      = {J_TNNLS},
  author       = {Yun Zhang and Yi Chen and Jilun Zhang and Xiaoling Luo and Malu Zhang and Hong Qu and Zhang Yi},
  doi          = {10.1109/TNNLS.2022.3213688},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7072-7086},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Minicolumn-based episodic memory model with spiking neurons, dendrites and delays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Incremental PID controller-based learning rate scheduler
for stochastic gradient descent. <em>TNNLS</em>, <em>35</em>(5),
7060–7071. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As we all know, the learning rate plays a vital role in deep neural network (DNN) training. This study introduces an incremental proportional-integral-derivative (PID) controller widely used in automatic control as a learning rate scheduler for stochastic gradient descent (SGD). To automatically calculate the current learning rate, we utilize feedback control to determine the relationship between training losses and learning rates, named incremental PID learning rates, which include PID-Base and PID-Warmup. The new schedulers reduce the dependence on the initial learning rate and achieve higher accuracy. Compared with multistep learning rates (MSLR), cyclical learning rates (CLR), and SGD with warm restarts (SGDR), incremental PID learning rates based on feedback control obtain higher accuracy on CIFAR-10, CIFAR-100, and Tiny-ImageNet-200. We believe that our methods can improve the performance of SGD.},
  archive      = {J_TNNLS},
  author       = {Zenghui Wang and Jun Zhang},
  doi          = {10.1109/TNNLS.2022.3213677},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7060-7071},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incremental PID controller-based learning rate scheduler for stochastic gradient descent},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Penalizing the hard example but not too much: A strong
baseline for fine-grained visual classification. <em>TNNLS</em>,
<em>35</em>(5), 7048–7059. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though significant progress has been achieved on fine-grained visual classification (FGVC), severe overfitting still hinders model generalization. A recent study shows that hard samples in the training set can be easily fit, but most existing FGVC methods fail to classify some hard examples in the test set. The reason is that the model overfits those hard examples in the training set, but does not learn to generalize to unseen examples in the test set. In this article, we propose a moderate hard example modulation (MHEM) strategy to properly modulate the hard examples. MHEM encourages the model to not overfit hard examples and offers better generalization and discrimination. First, we introduce three conditions and formulate a general form of a modulated loss function. Second, we instantiate the loss function and provide a strong baseline for FGVC, where the performance of a naive backbone can be boosted and be comparable with recent methods. Moreover, we demonstrate that our baseline can be readily incorporated into the existing methods and empower these methods to be more discriminative. Equipped with our strong baseline, we achieve consistent improvements on three typical FGVC datasets, i.e., CUB-200-2011, Stanford Cars, and FGVC-Aircraft. We hope the idea of moderate hard example modulation will inspire future research work toward more effective fine-grained visual recognition.},
  archive      = {J_TNNLS},
  author       = {Yuanzhi Liang and Linchao Zhu and Xiaohan Wang and Yi Yang},
  doi          = {10.1109/TNNLS.2022.3213563},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7048-7059},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Penalizing the hard example but not too much: A strong baseline for fine-grained visual classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online stochastic DCA with applications to principal
component analysis. <em>TNNLS</em>, <em>35</em>(5), 7035–7047. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic algorithms are well-known for their performance in the era of big data. In this article, we study nonsmooth stochastic Difference-of-Convex functions (DC) programs—the major class of nonconvex stochastic optimization, which have a variety of applications in divers domains, in particular, machine learning. We propose new online stochastic algorithms based on the state-of-the-art DC Algorithm (DCA)—a powerful approach in nonconvex programming framework, in the online context of streaming data continuously generated by some (unknown) source distribution. The new schemes use the stochastic approximations (SAs) principle: deterministic quantities of the standard DCA are replaced by their noisy estimators constructed using newly arriving samples. The convergence analysis of the proposed algorithms is studied intensively with the help of tools from modern convex analysis and martingale theory. Finally, we study several aspects of the proposed algorithms on an important problem in machine learning: the expected problem in principal component analysis (PCA).},
  archive      = {J_TNNLS},
  author       = {Hoai An Le Thi and Hoang Phuc Hau Luu and Tao Pham Dinh},
  doi          = {10.1109/TNNLS.2022.3213558},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7035-7047},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online stochastic DCA with applications to principal component analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning framework for start–end frame pair-driven
motion synthesis. <em>TNNLS</em>, <em>35</em>(5), 7021–7034. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A start–end frame pair and a motion pattern-based motion synthesis scheme can provide more control to the synthesis process and produce content-various motion sequences. However, the data preparation for the motion training is intractable, and concatenating feature spaces of the start–end frame pair and the motion pattern lacks theoretical rationality in previous works. In this article, we propose a deep learning framework that completes automatic data preparation and learns the nonlinear mapping from start–end frame pairs to motion patterns. The proposed model consists of three modules: action detection, motion extraction, and motion synthesis networks. The action detection network extends the deep subspace learning framework to a supervised version, i.e., uses the local self-expression (LSE) of the motion data to supervise feature learning and complement the classification error. A long short-term memory (LSTM)-based network is used to efficiently extract the motion patterns to address the speed deficiency reflected in the previous optimization-based method. A motion synthesis network consists of a group of LSTM-based blocks, where each of them is to learn the nonlinear relation between the start–end frame pairs and the motion patterns of a certain joint. The superior performances in action detection accuracy, motion pattern extraction efficiency, and motion synthesis quality show the effectiveness of each module in the proposed framework.},
  archive      = {J_TNNLS},
  author       = {Guiyu Xia and Peng Xue and Du Zhang and Qingshan Liu and Yubao Sun},
  doi          = {10.1109/TNNLS.2022.3213596},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7021-7034},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deep learning framework for Start–End frame pair-driven motion synthesis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Generalized and robust least squares regression.
<em>TNNLS</em>, <em>35</em>(5), 7006–7020. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a simple yet effective method, least squares regression (LSR) is extensively applied for data regression and classification. Combined with sparse representation, LSR can be extended to feature selection (FS) as well, in which $\ell _{1}$ regularization is often applied in embedded FS algorithms. However, because the loss function is in the form of squared error, LSR and its variants are sensitive to noises, which significantly degrades the effectiveness and performance of classification and FS. To cope with the problem, we propose a generalized and robust LSR (GRLSR) for classification and FS, which is made up of arbitrary concave loss function and the $\ell _{2,p}$ -norm regularization term. Meanwhile, an iterative algorithm is applied to efficiently deal with the nonconvex minimization problem, in which an additional weight to suppress the effect of noises is added to each data point. The weights can be automatically assigned according to the error of the samples. When the error is large, the value of the corresponding weight is small. It is this mechanism that allows GRLSR to reduce the impact of noises and outliers. According to the different formulations of the concave loss function, four specific methods are proposed to clarify the essence of the framework. Comprehensive experiments on corrupted datasets have proven the advantage of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jingyu Wang and Fangyuan Xie and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3213594},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {7006-7020},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized and robust least squares regression},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric multimodal deep learning with multiscaled graph
wavelet convolutional network. <em>TNNLS</em>, <em>35</em>(5),
6991–7005. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal data provide complementary information of a natural phenomenon by integrating data from various domains with very different statistical properties. Capturing the intramodality and cross-modality information of multimodal data is the essential capability of multimodal learning methods. The geometry-aware data analysis approaches provide these capabilities by implicitly representing data in various modalities based on their geometric underlying structures. Also, in many applications, data are explicitly defined on an intrinsic geometric structure. Generalizing deep learning methods to the non-Euclidean domains is an emerging research field, which has recently been investigated in many studies. Most of those popular methods are developed for unimodal data. In this article, a multimodal graph wavelet convolutional network (M-GWCN) is proposed as an end-to-end network. M-GWCN simultaneously finds intramodality representation by applying the multiscale graph wavelet transform to provide helpful localization properties in the graph domain of each modality and cross-modality representation by learning permutations that encode correlations among various modalities. M-GWCN is not limited to either the homogeneous modalities with the same number of data or any prior knowledge indicating correspondences between modalities. Several semisupervised node classification experiments have been conducted on three popular unimodal explicit graph-based datasets and five multimodal implicit ones. The experimental results indicate the superiority and effectiveness of the proposed methods compared with both spectral graph domain convolutional neural networks and state-of-the-art multimodal methods.},
  archive      = {J_TNNLS},
  author       = {Maysam Behmanesh and Peyman Adibi and Sayyed Mohammad Saeed Ehsani and Jocelyn Chanussot},
  doi          = {10.1109/TNNLS.2022.3213589},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6991-7005},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Geometric multimodal deep learning with multiscaled graph wavelet convolutional network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A multitask latent feature augmentation method for few-shot
learning. <em>TNNLS</em>, <em>35</em>(5), 6976–6990. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to learn novel concepts quickly from a few novel labeled samples with the transferable knowledge learned from base dataset. The existing FSL methods usually treat each sample as a single feature point in embedding space and classify through one single comparison task. However, the few-shot single feature points on the novel meta-testing episode are still vulnerable to noise easily although with the good transferable knowledge, because the novel categories are never seen on base dataset. Besides, the existing FSL models are trained by only one single comparison task and ignore that different semantic feature maps have different weights on different comparison objects and tasks, which cannot take full advantage of the valuable information from different multiple comparison tasks and objects to make the latent features (LFs) more robust based on only few-shot samples. In this article, we propose a novel multitask LF augmentation (MTLFA) framework to learn the meta-knowledge of generalizing key intraclass and distinguishable interclass sample features from only few-shot samples through an LF augmentation (LFA) module and a multitask (MT) framework. Our MTLFA treats the support features as sampling from the class-specific LF distribution, enhancing the diversity of support features and reducing the impact of noise based on few-shot support samples. Furthermore, an MT framework is introduced to obtain more valuable comparison-task-related and episode-related comparison information from multiple different comparison tasks in which different semantic feature maps have different weights, adjusting the prior LFs and generating the more robust and effective episode-related classifier. Besides, we analyze the feasibility and effectiveness of MTLFA from theoretical views based on the Hoeffding’s inequality and the Chernoff’s bounding method. Extensive experiments conducted on three benchmark datasets demonstrate that the MTLFA achieves the state-of-the-art performance in FSL. The experimental results verify our theoretical analysis and the effectiveness and robustness of MTLFA framework in FSL.},
  archive      = {J_TNNLS},
  author       = {Jian Xu and Bo Liu and Yanshan Xiao},
  doi          = {10.1109/TNNLS.2022.3213576},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6976-6990},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A multitask latent feature augmentation method for few-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Label-aware distribution calibration for long-tailed
classification. <em>TNNLS</em>, <em>35</em>(5), 6963–6975. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world data usually present long-tailed distributions. Training on imbalanced data tends to render neural networks perform well on head classes while much worse on tail classes. The severe sparseness of training instances for the tail classes is the main challenge, which results in biased distribution estimation during training. Plenty of efforts have been devoted to ameliorating the challenge, including data resampling and synthesizing new training instances for tail classes. However, no prior research has exploited the transferable knowledge from head classes to tail classes for calibrating the distribution of tail classes. In this article, we suppose that tail classes can be enriched by similar head classes and propose a novel distribution calibration (DC) approach named as label-aware DC (LADC). LADC transfers the statistics from relevant head classes to infer the distribution of tail classes. Sampling from calibrated distribution further facilitates rebalancing the classifier. Experiments on both image and text long-tailed datasets demonstrate that LADC significantly outperforms existing methods. The visualization also shows that LADC provides a more accurate distribution estimation.},
  archive      = {J_TNNLS},
  author       = {Chaozheng Wang and Shuzheng Gao and Pengyun Wang and Cuiyun Gao and Wenjie Pei and Lujia Pan and Zenglin Xu},
  doi          = {10.1109/TNNLS.2022.3213522},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6963-6975},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Label-aware distribution calibration for long-tailed classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian estimation of inverted beta mixture models with
extended stochastic variational inference for positive vector
classification. <em>TNNLS</em>, <em>35</em>(5), 6948–6962. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The finite inverted beta mixture model (IBMM) has been proven to be efficient in modeling positive vectors. Under the traditional variational inference framework, the critical challenge in Bayesian estimation of the IBMM is that the computational cost of performing inference with large datasets is prohibitively expensive, which often limits the use of Bayesian approaches to small datasets. An efficient alternative provided by the recently proposed stochastic variational inference (SVI) framework allows for efficient inference on large datasets. Nevertheless, when using the SVI framework to address the non-Gaussian statistical models, the evidence lower bound (ELBO) cannot be explicitly calculated due to the intractable moment computation. Therefore, the algorithm under the SVI framework cannot directly use stochastic optimization to optimize the ELBO, and an analytically tractable solution cannot be derived. To address this problem, we propose an extended version of the SVI framework with more flexibility, namely, the extended SVI (ESVI) framework. This framework can be used in many non-Gaussian statistical models. First, some approximation strategies are applied to further lower the ELBO to avoid intractable moment calculations. Then, stochastic optimization with noisy natural gradients is used to optimize the lower bound. The excellent performance and effectiveness of the proposed method are verified in real data evaluation.},
  archive      = {J_TNNLS},
  author       = {Yuping Lai and Wenbo Guan and Lijuan Luo and Yanhui Guo and Heping Song and Hongying Meng},
  doi          = {10.1109/TNNLS.2022.3213518},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6948-6962},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bayesian estimation of inverted beta mixture models with extended stochastic variational inference for positive vector classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weight decay with tailored adam on scale-invariant weights
for better generalization. <em>TNNLS</em>, <em>35</em>(5), 6936–6947.
(<a href="https://doi.org/10.1109/TNNLS.2022.3213536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weight decay (WD) is a fundamental and practical regularization technique in improving generalization of current deep learning models. However, it is observed that the WD does not work effectively for an adaptive optimization algorithm (such as Adam), as it works for SGD. Specifically, the solution found by Adam with the WD often generalizes unsatisfactorily. Though efforts have been made to mitigate this issue, the reason for such deficiency is still vague. In this article, we first show that when using the Adam optimizer, the weight norm increases very fast along with the training procedure, which is in contrast to SGD where the weight norm increases relatively slower and tends to converge. The fast increase of weight norm is adverse to WD; in consequence, the Adam optimizer will lose efficacy in finding solution that generalizes well. To resolve this problem, we propose to tailor Adam by introducing a regularization term on the adaptive learning rate, such that it is friendly to WD. Meanwhile, we introduce first moment on the WD to further enhance the regularization effect. We show that the proposed method is able to find solution with small norm and generalizes better than SGD. We test the proposed method on general image classification and fine-grained image classification tasks with different networks. Experimental results on all these cases substantiate the effectiveness of the proposed method in help improving the generalization. Specifically, the proposed method improves the test accuracy of Adam by a large margin and even improves the performance of SGD by 0.84% on CIFAR 10 and 1.03 % on CIFAR 100 with ResNet-50. The code of this article is public available at xxx.},
  archive      = {J_TNNLS},
  author       = {Xixi Jia and Xiangchu Feng and Hongwei Yong and Deyu Meng},
  doi          = {10.1109/TNNLS.2022.3213536},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6936-6947},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weight decay with tailored adam on scale-invariant weights for better generalization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimized uncertainty-aware training framework for neural
networks. <em>TNNLS</em>, <em>35</em>(5), 6928–6935. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification (UQ) for predictions generated by neural networks (NNs) is of vital importance in safety-critical applications. An ideal model is supposed to generate low uncertainty for correct predictions and high uncertainty for incorrect predictions. The main focus of state-of-the-art training algorithms is to optimize the NN parameters to improve the accuracy-related metrics. Training based on uncertainty metrics has been fully ignored or overlooked in the literature. This article introduces a novel uncertainty-aware training algorithm for classification tasks. A novel predictive uncertainty estimate-based objective function is defined and optimized using the stochastic gradient descent method. This new multiobjective loss function covers both accuracy and uncertainty accuracy (UA) simultaneously during training. The performance of the proposed training framework is compared from different aspects with other UQ techniques for different benchmarks. The obtained results demonstrate the effectiveness of the proposed framework for developing the NN models capable of generating reliable uncertainty estimates.},
  archive      = {J_TNNLS},
  author       = {Pegah Tabarisaadi and Abbas Khosravi and Saeid Nahavandi and Miadreza Shafie-Khah and João P. S. Catalão},
  doi          = {10.1109/TNNLS.2022.3213315},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6928-6935},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An optimized uncertainty-aware training framework for neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LFT: Neural ordinary differential equations with learnable
final-time. <em>TNNLS</em>, <em>35</em>(5), 6918–6927. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the last decade, deep neural networks have shown remarkable capability in learning representations. The recently proposed neural ordinary differential equations (NODEs) can be viewed as the continuous-time equivalence of residual neural networks. It has been shown that NODEs have a tremendous advantage over the conventional counterparts in terms of spatial complexity for modeling continuous-time processes. However, existing NODEs methods entail their final time to be specified in advance, precluding the models from choosing a desirable final time and limiting their expressive capabilities. In this article, we propose learnable final-time (LFT) NODEs to overcome this limitation. LFT rebuilds the NODEs learning process as a final-time-free optimal control problem and employs the calculus of variations to derive the learning algorithm of NODEs. In contrast to existing NODEs methods, the new approach empowers the NODEs models to choose their suitable final time, thus being more flexible in adjusting the model depth for given tasks. Additionally, we analyze the gradient estimation errors caused by numerical ordinary differential equations (ODEs) solvers and employ checkpoint-based methods to obtain accurate gradients. We demonstrate the effectiveness of the proposed method with experimental results on continuous normalizing flows (CNFs) and feedforward models.},
  archive      = {J_TNNLS},
  author       = {Dong Pang and Xinyi Le and Xinping Guan and Jun Wang},
  doi          = {10.1109/TNNLS.2022.3213308},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6918-6927},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LFT: Neural ordinary differential equations with learnable final-time},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-IID federated learning with sharper risk bound.
<em>TNNLS</em>, <em>35</em>(5), 6906–6917. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In federated learning (FL), the not independently or identically distributed (non-IID) data partitioning impairs the performance of the global model, which is a severe problem to be solved. Despite the extensive literature related to the algorithmic novelties and optimization analysis of FL, there has been relatively little theoretical research devoted to studying the generalization performance of non-IID FL. The generalization research of non-IID FL still lack effective tools and analytical approach. In this article, we propose weighted local Rademacher complexity to pertinently analyze the generalization properties of non-IID FL and derive a sharper excess risk bound based on weighted local Rademacher complexity, where the convergence rate is much faster than the existing bounds. Based on the theoretical results, we present a general framework federated averaging with local rademacher complexity (FedALRC) to lower the excess risk without additional communication costs compared to some famous methods, such as FedAvg. Through extensive experiments, we show that FedALRC outperforms FedAvg, FedProx and FedNova, and those experimental results coincide with our theoretical findings.},
  archive      = {J_TNNLS},
  author       = {Bojian Wei and Jian Li and Yong Liu and Weiping Wang},
  doi          = {10.1109/TNNLS.2022.3213187},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6906-6917},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Non-IID federated learning with sharper risk bound},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified approach to coreset learning. <em>TNNLS</em>,
<em>35</em>(5), 6893–6905. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coreset of a given dataset and loss function is usually a small weighed set that approximates this loss for every query from a given set of queries. Coresets have shown to be very useful in many applications. However, coresets’ construction is done in a problem-dependent manner and it could take years to design and prove the correctness of a coreset for a specific family of queries. This could limit coresets’ use in practical applications. Moreover, small coresets provably do not exist for many problems. To address these limitations, we propose a generic, learning-based algorithm for construction of coresets. Our approach offers a new definition of coreset, which is a natural relaxation of the standard definition and aims at approximating the average loss of the original data over the queries. This allows us to use a learning paradigm to compute a small coreset of a given set of inputs with respect to a given loss function using a training set of queries. We derive formal guarantees for the proposed approach. Experimental evaluation on deep networks and classic machine learning problems show that our learned coresets yield comparable or even better results than the existing algorithms with worst case theoretical guarantees (that may be too pessimistic in practice). Furthermore, our approach applied to deep network pruning provides the first coreset for a full deep network, i.e., compresses all the networks at once, and not layer by layer or similar divide-and-conquer methods.},
  archive      = {J_TNNLS},
  author       = {Alaa Maalouf and Gilad Eini and Ben Mussay and Dan Feldman and Margarita Osadchy},
  doi          = {10.1109/TNNLS.2022.3213169},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6893-6905},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A unified approach to coreset learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Supervised feature selection via collaborative neurodynamic
optimization. <em>TNNLS</em>, <em>35</em>(5), 6878–6892. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial part of machine learning and pattern recognition, feature selection aims at selecting a subset of the most informative features from the set of all available features. In this article, supervised feature selection is at first formulated as a mixed-integer optimization problem with an objective function of weighted feature redundancy and relevancy subject to a cardinality constraint on the number of selected features. It is equivalently reformulated as a bound-constrained mixed-integer optimization problem by augmenting the objective function with a penalty function for realizing the cardinality constraint. With additional bilinear and linear equality constraints for realizing the integrality constraints, it is further reformulated as a bound-constrained biconvex optimization problem with two more penalty terms. Two collaborative neurodynamic optimization (CNO) approaches are proposed for solving the formulated and reformulated feature selection problems. One of the proposed CNO approaches uses a population of discrete-time recurrent neural networks (RNNs), and the other use a pair of continuous-time projection networks operating concurrently on two timescales. Experimental results on 13 benchmark datasets are elaborated to substantiate the superiority of the CNO approaches to several mainstream methods in terms of average classification accuracy with three commonly used classifiers.},
  archive      = {J_TNNLS},
  author       = {Yadi Wang and Jun Wang and Nikhil R. Pal},
  doi          = {10.1109/TNNLS.2022.3213167},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6878-6892},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supervised feature selection via collaborative neurodynamic optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A co-training framework for heterogeneous heuristic domain
adaptation. <em>TNNLS</em>, <em>35</em>(5), 6863–6877. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this article is to address unsupervised domain adaptation (UDA) where a labeled source domain and an unlabeled target domain are given. Recent advanced UDA methods attempt to remove domain-specific properties by separating domain-specific information from domain-invariant representations, which heavily rely on the designed neural network structures. Meanwhile, they do not consider class discriminate representations when learning domain-invariant representations. To this end, this article proposes a co-training framework for heterogeneous heuristic domain adaptation (CO-HHDA) to address the above issues. First, a heterogeneous heuristic network is introduced to model domain-specific characters. It allows structures of heuristic network to be different between domains to avoid underfitting or overfitting. Specially, we initialize a small structure that is shared between domains and increase a subnetwork for the domain which preserves rich specific information. Second, we propose a co-training scheme to train two classifiers, a source classifier and a target classifier, to enhance class discriminate representations. The two classifiers are designed based on domain-invariant representations, where the source classifier learns from the labeled source data, and the target classifier is trained from the generated target pseudolabeled data. The two classifiers teach each other in the training process with high-quality pseudolabeled data. Meanwhile, an adaptive threshold is presented to select reliable pseudolabels in each classifier. Empirical results on three commonly used benchmark datasets demonstrate that the proposed CO-HHDA outperforms the state-of-the-art domain adaptation methods.},
  archive      = {J_TNNLS},
  author       = {Cuie Yang and Bing Xue and Kay Chen Tan and Mengjie Zhang},
  doi          = {10.1109/TNNLS.2022.3212924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6863-6877},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A co-training framework for heterogeneous heuristic domain adaptation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Manifold peaks nonnegative matrix factorization.
<em>TNNLS</em>, <em>35</em>(5), 6850–6862. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) has attracted increasing interest for its high interpretability in recent years. It is shown that the NMF is closely related to fuzzy $k$ -means clustering, where the basis matrix represents the cluster centroids. However, most of the existing NMF-based clustering algorithms often have their decomposed centroids deviate away from the data manifold, which potentially undermines the clustering results, especially when the datasets lie on complicated geometric structures. In this article, we present a manifold peaks NMF (MPNMF) for data clustering. The proposed approach has the following advantages: 1) it selects a number of MPs to characterize the backbone of the data manifold; 2) it enforces the centroids to lie on the original data manifold, by restricting each centroid to be a conic combination of a small number of nearby MPs; 3) it generalizes the graph smoothness regularization to guide the local graph construction; and 4) it solves a general problem of quadratic regularized nonnegative least squares (NNLSs) with group $l_{0}$ -norm constraint and further develops an efficient optimization algorithm to solve the objective function of the MPNMF. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Xiaohua Xu and Ping He},
  doi          = {10.1109/TNNLS.2022.3212922},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6850-6862},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Manifold peaks nonnegative matrix factorization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AUD-net: A unified deep detector for multiple hyperspectral
image anomaly detection via relation and few-shot learning.
<em>TNNLS</em>, <em>35</em>(5), 6835–6849. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of the building an out-of-the-box deep detector, motivated by the need to perform anomaly detection across multiple hyperspectral images (HSIs) without repeated training. To solve this challenging task, we propose a unified detector [anomaly detection network (AUD-Net)] inspired by few-shot learning. The crucial issues solved by AUD-Net include: how to improve the generalization of the model on various HSIs that contain different categories of land cover; and how to unify the different spectral sizes between HSIs. To achieve this, we first build a series of subtasks to classify the relations between the center and its surroundings in the dual window. Through relation learning, AUD-Net can be more easily generalized to unseen HSIs, as the relations of the pixel pairs are shared among different HSIs. Secondly, to handle different HSIs with various spectral sizes, we propose a pooling layer based on the vector of local aggregated descriptors, which maps the variable-sized features to the same space and acquires the fixed-sized relation embeddings. To determine whether the center of the dual window is an anomaly, we build a memory model by the transformer, which integrates the contextual relation embeddings in the dual window and estimates the relation embeddings of the center. By computing the feature difference between the estimated relation embeddings of the centers and the corresponding real ones, the centers with large differences will be detected as anomalies, as they are more difficult to be estimated by the corresponding surroundings. Extensive experiments on both the simulation dataset and 13 real HSIs demonstrate that this proposed AUD-Net has strong generalization for various HSIs and achieves significant advantages over the specific-trained detectors for each HSI.},
  archive      = {J_TNNLS},
  author       = {Ning Huyan and Xiangrong Zhang and Dou Quan and Jocelyn Chanussot and Licheng Jiao},
  doi          = {10.1109/TNNLS.2022.3213023},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6835-6849},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AUD-net: A unified deep detector for multiple hyperspectral image anomaly detection via relation and few-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ordinal regression for direction-related anomaly detection.
<em>TNNLS</em>, <em>35</em>(5), 6821–6834. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is widely used in many fields to reveal the abnormal process of a system. Typical model-based anomaly detection methods work well in general anomaly detection problems. However, in some application-specific scenarios, the anomalies of interest are “direction-related,” that is, only deviation in certain directions of the data space is abnormal. Most existing anomaly detection methods do not work well in these scenarios, especially when there is no abnormal data information during training. Considering that in many real anomaly detection applications such as medical disease detection and industrial device faults diagnosis, the normal data have several ordinal levels, and the anomalies can be regarded as an unseen level distributed roughly along the ordinal direction outside the normal levels. Notice that the ordinal information is inherently “direction-related,” and we can use the ordinal information to assist in finding a “direction-related” boundary for the normal data to detect anomalies of interest. A typical type of methods utilizing the ordinal information is ordinal regression. However, to the best of our knowledge, the existing ordinal regression methods are unable to be directly applied to anomaly detection. In this article, to detect the aforementioned “direction-related” anomalies, we propose an ordinal regression algorithm for direction-related anomaly detection (ORAD). Specifically, we first formulate ORAD as an optimization problem. Then, we apply the difference of convex functions (DC) programming to solve the problem to obtain a “direction-related” boundary. After that, we calculate the outlier scores based on the deviation from the boundary. Theoretically, we analyze the ordinal properties and the convergence of ORAD. We carry out experiments on both synthetic data and real datasets to demonstrate the effectiveness of the proposed ORAD.},
  archive      = {J_TNNLS},
  author       = {Jiankai Tu and Huan Liu and Chunguang Li},
  doi          = {10.1109/TNNLS.2022.3212991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6821-6834},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ordinal regression for direction-related anomaly detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised multiscale dynamic graph convolution network
for hyperspectral image classification. <em>TNNLS</em>, <em>35</em>(5),
6806–6820. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional neural networks (CNNs)-based methods achieve cracking performance on hyperspectral image (HSI) classification tasks, due to its hierarchical structure and strong nonlinear fitting capacity. Most of them, however, are supervised approaches that need a large number of labeled data to train them. Conventional convolution kernels are fixed shape of rectangular with fixed sizes, which are good at capturing short-range relations between pixels within HSIs but ignore the long-range context within HSIs, limiting their performance. To overcome the limitations mentioned above, we present a dynamic multiscale graph convolutional network (GCN) classifier (DMSGer). DMSGer first constructs a relatively small graph at region-level based on a superpixel segmentation algorithm and metric-learning. A dynamic pixel-level feature update strategy is then applied to the region-level adjacency matrix, which can help DMSGer learn the pixel representation dynamically. Finally, to deeply understand the complex contents within HSIs, our model is expanded into a multiscale version. On the one hand, by introducing graph learning theory, DMSGer accomplishes HSI classification tasks in a semi-supervised manner, relieving the pressure of collecting abundant labeled samples. Superpixels are generally in irregular shapes and sizes which can group only similar pixels in a neighborhood. On the other hand, based on the proposed dynamic-GCN, the pixel-level and region-level information can be captured simultaneously in one graph convolution layer such that the classification results can be improved. Also, due to the proper multiscale expansion, more helpful information can be captured from HSIs. Extensive experiments were conducted on four public HSIs, and the promising results illustrate that our DMSGer is robust in classifying HSIs. Our source codes are available at https://github.com/TangXu-Group/DMSGer .},
  archive      = {J_TNNLS},
  author       = {Yuqun Yang and Xu Tang and Xiangrong Zhang and Jingjing Ma and Fang Liu and Xiuping Jia and Licheng Jiao},
  doi          = {10.1109/TNNLS.2022.3212985},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6806-6820},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semi-supervised multiscale dynamic graph convolution network for hyperspectral image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered cluster consensus of multi-agent systems via
a modified genetic algorithm. <em>TNNLS</em>, <em>35</em>(5), 6792–6805.
(<a href="https://doi.org/10.1109/TNNLS.2022.3212967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the event-triggered output feedback cluster consensus of leader–following multi-agent systems (MASs) under limited communication resources. Specifically, the distributed agents are divided into several clusters to accomplish different collective tasks under diverse intracluster and intercluster communications. First, to alleviate excessive communication resource consumption, two sampled-data-based event-triggered schemes are developed to distinguish agent-to-agent communications within clusters and between clusters. Based on these schemes, an event-based cluster consensus control protocol is proposed to solve the problem. Then, sufficient criteria on asymptotic stability of the resulting closed-loop system are derived and expressed in terms of matrix inequalities. It is noteworthy that the derived criteria for controller design are nonlinear and nonconvex with respect to the output feedback control gains and triggering parameters. To handle this issue, a modified genetic algorithm (MGA) with multiple subpopulations is proposed, where the subpopulations are independent of each other. The key feature of the designed MGA lies in that the fitness value is described as an accumulation of initial value and weighing value of each matrix inequality. Finally, an application of satellite formation flying is exemplified to demonstrate the effectiveness of the derived theoretical results.},
  archive      = {J_TNNLS},
  author       = {Yanping Yang and Shenrong Li and Xiaohua Ge and Qing-Long Han},
  doi          = {10.1109/TNNLS.2022.3212967},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6792-6805},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered cluster consensus of multi-agent systems via a modified genetic algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video moment retrieval with noisy labels. <em>TNNLS</em>,
<em>35</em>(5), 6779–6791. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video moment retrieval (VMR) aims to localize the target moment in an untrimmed video according to the given nature language query. The existing algorithms typically rely on clean annotations to train their models. However, making annotations by human labors may introduce much noise. Thus, the video moment retrieval models will not be well trained in practice. In this article, we present a simple yet effective video moment retrieval framework via bottom-up schema, which is in end-to-end manners and robust to noisy label training. Specifically, we extract the multimodal features by syntactic graph convolutional networks and multihead attention layers, which are fused by the cross gates and the bilinear approach. Then, the feature pyramid networks are constructed to encode plentiful scene relationships and capture high semantics. Furthermore, to mitigate the effects of noisy annotations, we devise the multilevel losses characterized by two levels: a frame-level loss that improves noise tolerance and an instance-level loss that reduces adverse effects of negative instances. For the frame level, we adopt the Gaussian smoothing to regard noisy labels as soft labels through the partial fitting. For the instance level, we exploit a pair of structurally identical models to let them teach each other during iterations. This leads to our proposed robust video moment retrieval model, which experimentally and significantly outperforms the state-of-the-art approaches on standard public datasets ActivityCaption and textually annotated cooking scene (TACoS). We also evaluate the proposed approach on the different manual annotation noises to further demonstrate the effectiveness of our model.},
  archive      = {J_TNNLS},
  author       = {Wenwen Pan and Zhou Zhao and Wencan Huang and Zhu Zhang and Liyong Fu and Zhigeng Pan and Jun Yu and Fei Wu},
  doi          = {10.1109/TNNLS.2022.3212900},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6779-6791},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Video moment retrieval with noisy labels},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aligning correlation information for domain adaptation in
action recognition. <em>TNNLS</em>, <em>35</em>(5), 6767–6778. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) approaches address domain shift and enable networks to be applied to different scenarios. Although various image DA approaches have been proposed in recent years, there is limited research toward video DA. This is partly due to the complexity in adapting the different modalities of features in videos, which includes the correlation features extracted as long-range dependencies of pixels across spatiotemporal dimensions. The correlation features are highly associated with action classes and proven their effectiveness in accurate video feature extraction through the supervised action recognition task. Yet correlation features of the same action would differ across domains due to domain shift. Therefore, we propose a novel adversarial correlation adaptation network (ACAN) to align action videos by aligning pixel correlations. ACAN aims to minimize the distribution of correlation information, termed as pixel correlation discrepancy (PCD). Additionally, video DA research is also limited by the lack of cross-domain video datasets with larger domain shifts. We, therefore, introduce a novel HMDB-ARID dataset with a larger domain shift caused by a larger statistical difference between domains. This dataset is built in an effort to leverage current datasets for dark video classification. Empirical results demonstrate the state-of-the-art performance of our proposed ACAN for both existing and the new video DA datasets.},
  archive      = {J_TNNLS},
  author       = {Yuecong Xu and Haozhi Cao and Kezhi Mao and Zhenghua Chen and Lihua Xie and Jianfei Yang},
  doi          = {10.1109/TNNLS.2022.3212909},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6767-6778},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Aligning correlation information for domain adaptation in action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zeroing neural network with coefficient functions and
adjustable parameters for solving time-variant sylvester equation.
<em>TNNLS</em>, <em>35</em>(5), 6757–6766. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the time-variant Sylvester equation, in 2013, Li et al. proposed the zeroing neural network with sign-bi-power function (ZNN-SBPF) model via constructing a nonlinear activation function. In this article, to further improve the convergence rate, the zeroing neural network with coefficient functions and adjustable parameters (ZNN-CFAP) model as a variation in zeroing neural network (ZNN) model is proposed. On the basis of the introduced coefficient functions, an appropriate ZNN-CFAP model can be chosen according to the error function. The high convergence rate of the ZNN-CFAP model can be achieved by choosing appropriate adjustable parameters. Moreover, the finite-time convergence property and convergence time upper bound of the ZNN-CFAP model are proved in theory. Computer simulations and numerical experiments are performed to illustrate the efficacy and validity of the ZNN-CFAP model in time-variant Sylvester equation solving. Comparative experiments among the ZNN-CFAP, ZNN-SBPF, and ZNN with linear function (ZNN-LF) models further substantiate the superiority of the ZNN-CFAP model in view of the convergence rate. Finally, the proposed ZNN-CFAP model is successfully applied to the tracking control of robot manipulator to verify its practicability.},
  archive      = {J_TNNLS},
  author       = {Wenqi Wu and Yunong Zhang},
  doi          = {10.1109/TNNLS.2022.3212869},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6757-6766},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Zeroing neural network with coefficient functions and adjustable parameters for solving time-variant sylvester equation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Local means binary networks for image super-resolution.
<em>TNNLS</em>, <em>35</em>(5), 6746–6756. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of modern single image super-resolution (SISR) algorithms is inspired by the development of deep convolutional neural networks (CNNs). However, these CNN-based methods require considerable computation and complexity, making it impossible for these methods to perform real-time calculations in edge devices. Thus, lightweight model design has become a development trend in the super-resolution field, including pruning, quantization, and other methods. The 1-bit quantization is an extreme lightweight method which can reduce the calculation amount of the model in an extreme manner and is friendly to hardware such as edge devices. Most existing binary quantization approaches lead to a large information loss during forward propagation, especially in detailed color information (e.g., edge, texture, and contrast). The loss of color information makes modern binary methods unsuitable for SISR tasks. We think the loss occurs because these methods typically utilize a uniform threshold to quantize the weights and activations. Thus, in this article, we thoroughly analyze the difference between normal classification tasks and SISR tasks, and present a binarization scheme based on local means. The proposed method can maintain more detailed information in feature maps using dynamic thresholds during quantization. Specifically, each value in the full precision activations has a corresponding threshold during the quantization process, and those thresholds are determined by the full precision values of the surroundings. In addition, a gradient approximator is introduced to adaptively optimize the gradient for updating binary weights. We then verify the effectiveness of our method for training binary networks on several SISR benchmarks including VDSR and SRResNet. Experimental results show that the proposed method can outperform the state-of-the-art algorithms to obtain binary networks for image super-resolution with better peak signal-to-noise ratio (PSNR) values and visual quality.},
  archive      = {J_TNNLS},
  author       = {Keyu Li and Nannan Wang and Jingwei Xin and Xinrui Jiang and Jie Li and Xinbo Gao and Kai Han and Yunhe Wang},
  doi          = {10.1109/TNNLS.2022.3212827},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6746-6756},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local means binary networks for image super-resolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint metric learning-based class-specific representation
for image set classification. <em>TNNLS</em>, <em>35</em>(5), 6731–6745.
(<a href="https://doi.org/10.1109/TNNLS.2022.3212703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advances in digital imaging and communication technologies, recently image set classification has attracted significant attention and has been widely used in many real-world scenarios. As an effective technology, the class-specific representation theory-based methods have demonstrated their superior performances. However, this type of methods either only uses one gallery set to measure the gallery-to-probe set distance or ignores the inner connection between different metrics, leading to the learned distance metric lacking robustness, and is sensitive to the size of image sets. In this article, we propose a novel joint metric learning-based class-specific representation framework (JMLC), which can jointly learn the related and unrelated metrics. By iteratively modeling probe set and related or unrelated gallery sets as affine hull, we reconstruct this hull sparsely or collaboratively over another image set. With the obtained representation coefficients, the combined metric between the query set and the gallery set can then be calculated. In addition, we also derive the kernel extension of JMLC and propose two new unrelated set constituting strategies. Specifically, kernelized JMLC (KJMLC) embeds the gallery sets and probe sets into the high-dimensional Hilbert space, and in the kernel space, the data become approximately linear separable. Extensive experiments on seven benchmark databases show the superiority of the proposed methods to the state-of-the-art image set classifiers.},
  archive      = {J_TNNLS},
  author       = {Xizhan Gao and Sijie Niu and Dong Wei and Xingrui Liu and Tingwei Wang and Fa Zhu and Jiwen Dong and Quansen Sun},
  doi          = {10.1109/TNNLS.2022.3212703},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6731-6745},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint metric learning-based class-specific representation for image set classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative knowledge distillation via multiknowledge
transfer. <em>TNNLS</em>, <em>35</em>(5), 6718–6730. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD), as an efficient and effective model compression technique, has received considerable attention in deep learning. The key to its success is about transferring knowledge from a large teacher network to a small student network. However, most existing KD methods consider only one type of knowledge learned from either instance features or relations via a specific distillation strategy, failing to explore the idea of transferring different types of knowledge with different distillation strategies. Moreover, the widely used offline distillation also suffers from a limited learning capacity due to the fixed large-to-small teacher–student architecture. In this article, we devise a collaborative KD via multiknowledge transfer (CKD-MKT) that prompts both self-learning and collaborative learning in a unified framework. Specifically, CKD-MKT utilizes a multiple knowledge transfer framework that assembles self and online distillation strategies to effectively: 1) fuse different kinds of knowledge, which allows multiple students to learn knowledge from both individual instances and instance relations, and 2) guide each other by learning from themselves using collaborative and self-learning. Experiments and ablation studies on six image datasets demonstrate that the proposed CKD-MKT significantly outperforms recent state-of-the-art methods for KD.},
  archive      = {J_TNNLS},
  author       = {Jianping Gou and Liyuan Sun and Baosheng Yu and Lan Du and Kotagiri Ramamohanarao and Dacheng Tao},
  doi          = {10.1109/TNNLS.2022.3212733},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6718-6730},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collaborative knowledge distillation via multiknowledge transfer},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced security and privacy via fragmented federated
learning. <em>TNNLS</em>, <em>35</em>(5), 6703–6717. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In federated learning (FL), a set of participants share updates computed on their local data with an aggregator server that combines updates into a global model. However, reconciling accuracy with privacy and security is a challenge to FL. On the one hand, good updates sent by honest participants may reveal their private local information, whereas poisoned updates sent by malicious participants may compromise the model’s availability and/or integrity. On the other hand, enhancing privacy via update distortion damages accuracy, whereas doing so via update aggregation damages security because it does not allow the server to filter out individual poisoned updates. To tackle the accuracy-privacy-security conflict, we propose fragmented FL (FFL), in which participants randomly exchange and mix fragments of their updates before sending them to the server. To achieve privacy, we design a lightweight protocol that allows participants to privately exchange and mix encrypted fragments of their updates so that the server can neither obtain individual updates nor link them to their originators. To achieve security, we design a reputation-based defense tailored for FFL that builds trust in participants and their mixed updates based on the quality of the fragments they exchange and the mixed updates they send. Since the exchanged fragments’ parameters keep their original coordinates and attackers can be neutralized, the server can correctly reconstruct a global model from the received mixed updates without accuracy loss. Experiments on four real data sets show that FFL can prevent semi-honest servers from mounting privacy attacks, can effectively counter-poisoning attacks, and can keep the accuracy of the global model.},
  archive      = {J_TNNLS},
  author       = {Najeeb Moharram Jebreel and Josep Domingo-Ferrer and Alberto Blanco-Justicia and David Sánchez},
  doi          = {10.1109/TNNLS.2022.3212627},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6703-6717},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhanced security and privacy via fragmented federated learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentanglement by cyclic reconstruction. <em>TNNLS</em>,
<em>35</em>(5), 6693–6702. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have demonstrated their ability to automatically extract meaningful features from data. However, in supervised learning, information specific to the dataset used for training, but irrelevant to the task at hand, may remain encoded in the extracted representations. This remaining information introduces a domain-specific bias, weakening the generalization performance. In this work, we propose splitting the information into a task-related representation and its complementary context representation. We propose an original method, combining adversarial feature predictors and cyclic reconstruction, to disentangle these two representations in the single-domain supervised case. We then adapt this method to the unsupervised domain adaptation (UDA) problem, consisting of training a model capable of performing on both a source and a target domain. In particular, our method promotes disentanglement in the target domain, despite the absence of training labels. This enables the isolation of task-specific information from both domains and a projection into a common representation. The task-specific representation allows the efficient transfer of knowledge acquired from the source domain to the target domain. In the single-domain case, we demonstrate the quality of our representations on information retrieval tasks and the generalization benefits induced by sharpened task-specific representations. We then validate the proposed method on several classical domain adaptation (DA) benchmarks and illustrate the benefits of disentanglement for DA.},
  archive      = {J_TNNLS},
  author       = {David Bertoin and Emmanuel Rachelson},
  doi          = {10.1109/TNNLS.2022.3212620},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6693-6702},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disentanglement by cyclic reconstruction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Capacitated clustering via majorization-minimization and
collaborative neurodynamic optimization. <em>TNNLS</em>, <em>35</em>(5),
6679–6692. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses capacitated clustering based on majorization-minimization and collaborative neurodynamic optimization (CNO). Capacitated clustering is formulated as a combinatorial optimization problem. Its objective function consists of fractional terms with intra-cluster similarities in their numerators and cluster cardinalities in their denominators as normalized cluster compactness measures. To obviate the difficulty in optimizing the objective function with factional terms, the combinatorial optimization problem is reformulated as an iteratively reweighted quadratic unconstrained binary optimization problem with a surrogate function and a penalty function in a majorization-minimization framework. A clustering algorithm is developed based on CNO for solving the reformulated problem. It employs multiple Boltzmann machines operating concurrently for local searches and a particle swarm optimization rule for repositioning neuronal states upon their local convergence. Experimental results on ten benchmark datasets are elaborated to demonstrate the superior clustering performance of the proposed approaches against seven baseline algorithms in terms of 21 internal cluster validity criteria.},
  archive      = {J_TNNLS},
  author       = {Hongzong Li and Jun Wang},
  doi          = {10.1109/TNNLS.2022.3212593},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6679-6692},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Capacitated clustering via majorization-minimization and collaborative neurodynamic optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hidden order of boolean networks. <em>TNNLS</em>,
<em>35</em>(5), 6667–6678. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a common belief that the order of a Boolean network is mainly determined by its attractors, including fixed points and cycles. Using the semi-tensor product (STP) of matrices and the algebraic state-space representation (ASSR) of the Boolean networks, this article reveals that in addition to this explicit order, there is a certain implicit or hidden order, which is determined by the fixed points and limit cycles of their dual networks. The structure and certain properties of dual networks are investigated. Instead of a trajectory, which describes the evolution of a state, the hidden order provides a global horizon to describe the evolution of the overall network. We conjecture that the order of networks is mainly determined by the dual attractors via their corresponding hidden orders. Then these results about the Boolean networks are further extended to the $k$ -valued case.},
  archive      = {J_TNNLS},
  author       = {Xiao Zhang and Zhengping Ji and Daizhan Cheng},
  doi          = {10.1109/TNNLS.2022.3212274},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6667-6678},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hidden order of boolean networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic integrated actor–critic for deep reinforcement
learning. <em>TNNLS</em>, <em>35</em>(5), 6654–6666. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a deep stochastic actor–critic algorithm with an integrated network architecture and fewer parameters. We address stabilization of the learning procedure via an adaptive objective to the critic’s loss and a smaller learning rate for the shared parameters between the actor and the critic. Moreover, we propose a mixed on–off policy exploration strategy to speed up learning. Experiments illustrate that our algorithm reduces the sample complexity by 50%–93% compared with the state-of-the-art deep reinforcement learning (RL) algorithms twin delayed deep deterministic policy gradient (TD3), soft actor–critic (SAC), proximal policy optimization (PPO), advantage actor–critic (A2C), and interpolated policy gradient (IPG) over continuous control tasks LunarLander, BipedalWalker, BipedalWalkerHardCore, Ant, and Minitaur in the OpenAI Gym.},
  archive      = {J_TNNLS},
  author       = {Jiaohao Zheng and Mehmet Necip Kurt and Xiaodong Wang},
  doi          = {10.1109/TNNLS.2022.3212273},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6654-6666},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stochastic integrated Actor–Critic for deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Distributed estimation of support vector machines for
matrix data. <em>TNNLS</em>, <em>35</em>(5), 6643–6653. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrimination problems are of significant interest in the machine learning literature. There has been growing interest in extending traditional vector-based machine learning techniques to their matrix forms. In this article, we investigate the statistical properties of the nuclear-norm-based regularized linear support vector machines (SVMs), in particular establishing the convergence rate of the estimator in the high-dimensional setting. Furthermore, within the distributed estimation paradigm, we propose a communication-efficient estimator that can achieve the same convergence rate. We illustrate the performances of the estimators via some simulation examples and an empirical data analysis.},
  archive      = {J_TNNLS},
  author       = {Wangli Xu and Jiamin Liu and Heng Lian},
  doi          = {10.1109/TNNLS.2022.3212390},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6643-6653},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed estimation of support vector machines for matrix data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal graph representation learning for fraudster
group detection. <em>TNNLS</em>, <em>35</em>(5), 6628–6642. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by potential financial gain, companies may hire fraudster groups to write fake reviews to either demote competitors or promote their own businesses. Such groups are considerably more successful in misleading customers, as people are more likely to be influenced by the opinion of a large group. To detect such groups, a common model is to represent fraudster groups’ static networks, consequently overlooking the longitudinal behavior of a reviewer, thus, the dynamics of coreview relations among reviewers in a group. Hence, these approaches are incapable of excluding outlier reviewers, which are fraudsters intentionally camouflaging themselves in a group and genuine reviewers happen to coreview in fraudster groups. To address this issue, we propose “FGDT,” a framework for “fraudster group detection through temporal relations.” FGDT first capitalizes on the effectiveness of the HIN-recurrent neural network (RNN) in both reviewers’ representation learning while capturing the collaboration between reviewers. The HIN-RNN models the coreview relations of reviewers in a group in a fixed time window of 28 days. We refer to this as spatial relation learning representation to signify the generalizability of this work to other networked scenarios. Then, we use an RNN on the spatial relations to predict the spatio-temporal relations of reviewers in the group. In the third step, a graph convolution network (GCN) refines the reviewers’ vector representations using these predicted relations. These refined representations are then used to remove outlier reviewers. The average of the remaining reviewers’ representation is then fed to a simple fully connected layer to predict if the group is a fraudster group or not. Exhaustive experiments of FGDT showed a 5% (4%), 12% (5%), and 12% (5%) improvement over three of the most recent approaches on precision, recall, and F1-value over the Yelp (Amazon) dataset, respectively.},
  archive      = {J_TNNLS},
  author       = {Saeedreza Shehnepoor and Roberto Togneri and Wei Liu and Mohammed Bennamoun},
  doi          = {10.1109/TNNLS.2022.3212001},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6628-6642},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatio-temporal graph representation learning for fraudster group detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-image 3-d reconstruction: Rethinking point cloud
deformation. <em>TNNLS</em>, <em>35</em>(5), 6613–6627. (<a
href="https://doi.org/10.1109/TNNLS.2022.3211929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image 3-D reconstruction has long been a challenging problem. Recent deep learning approaches have been introduced to this 3-D area, but the ability to generate point clouds still remains limited due to inefficient and expensive 3-D representations, the dependency between the output and the number of model parameters, or the lack of a suitable computing operation. In this article, we present a novel deep-learning-based method to reconstruct a point cloud of an object from a single still image. The proposed method can be decomposed into two steps: feature fusion and deformation. The first step extracts both global and point-specific shape features from a 2-D object image, and then injects them into a randomly generated point cloud. In the second step, which is deformation, we introduce a new layer termed as GraphX that considers the interrelationship between points like common graph convolutions but operates on unordered sets. The framework can be applicable to realistic image data with background as we optionally learn a mask branch to segment objects from input images. To complement the quality of point clouds, we further propose an objective function to control the point uniformity. In addition, we introduce different variants of GraphX that cover from best performance to best memory budget. Moreover, the proposed model can generate an arbitrary-sized point cloud, which is the first deep method to do so. Extensive experiments demonstrate that we outperform the existing models and set a new height for different performance metrics in single-image 3-D reconstruction.},
  archive      = {J_TNNLS},
  author       = {Anh-Duc Nguyen and Seonghwa Choi and Woojae Kim and Jongyoo Kim and Heeseok Oh and Jiwoo Kang and Sanghoon Lee},
  doi          = {10.1109/TNNLS.2022.3211929},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6613-6627},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Single-image 3-D reconstruction: Rethinking point cloud deformation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDN: Semantic decoupling network for temporal language
grounding. <em>TNNLS</em>, <em>35</em>(5), 6598–6612. (<a
href="https://doi.org/10.1109/TNNLS.2022.3211850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal language grounding (TLG) is one of the most challenging cross-modal video understanding tasks, which aims at retrieving the most relevant video segment from an untrimmed video according to a natural language sentence. The existing methods can be separated into two dominant types: 1) proposal-based and 2) proposal-free methods, where the former conduct contextual interactions and the latter localizes timestamps flexibly. However, the constant-scale candidates in proposal-based methods limit the localization precision and bring extra computational costs. In contrast, the proposal-free methods perform well on high-precision metrics-based on the fine-grained features but suffer from a lack of coarse-grained interactions, which cause degeneration when the video becomes complex. In this article, we propose a novel framework termed semantic decoupling network (SDN) that combines the advantages of proposal-based and proposal-free methods and overcomes their defects. It contains three key components: 1) semantic decoupling module (SDM); 2) context modeling block (CMB); and 3) semantic cross-level aggregation module (SCAM). By capturing the video-text contexts in multilevel semantics, the SDM and CMB effectively utilize the benefits of proposal-based methods. Meanwhile, the SCAM maintains the merit of proposal-free methods in that it localizes timestamps precisely. The experiments on three challenge datasets, i.e., Charades-STA, TACoS, and ActivityNet-Caption, show that our proposed SDN method significantly outperforms recent state-of-the-art methods, especially the proposal-free methods. Extensive analyses, as well as the implementation code of the proposed SDN method, are provided at https://github.com/CFM-MSG/Code_SDN .},
  archive      = {J_TNNLS},
  author       = {Xun Jiang and Xing Xu and Jingran Zhang and Fumin Shen and Zuo Cao and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2022.3211850},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6598-6612},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SDN: Semantic decoupling network for temporal language grounding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperseed: Unsupervised learning with vector symbolic
architectures. <em>TNNLS</em>, <em>35</em>(5), 6583–6597. (<a
href="https://doi.org/10.1109/TNNLS.2022.3211274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by recent innovations in biologically inspired neuromorphic hardware, this article presents a novel unsupervised machine learning algorithm named Hyperseed that draws on the principles of vector symbolic architectures (VSAs) for fast learning of a topology preserving feature map of unlabeled data. It relies on two major operations of VSA, binding and bundling. The algorithmic part of Hyperseed is expressed within the Fourier holographic reduced representations (FHRR) model, which is specifically suited for implementation on spiking neuromorphic hardware. The two primary contributions of the Hyperseed algorithm are few-shot learning and a learning rule based on single vector operation. These properties are empirically evaluated on synthetic datasets and on illustrative benchmark use cases, IRIS classification, and a language identification task using the $n$ -gram statistics. The results of these experiments confirm the capabilities of Hyperseed and its applications in neuromorphic hardware.},
  archive      = {J_TNNLS},
  author       = {Evgeny Osipov and Sachin Kahawala and Dilantha Haputhanthri and Thimal Kempitiya and Daswin De Silva and Damminda Alahakoon and Denis Kleyko},
  doi          = {10.1109/TNNLS.2022.3211274},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6583-6597},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperseed: Unsupervised learning with vector symbolic architectures},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal moore–penrose inverse-based recomputation
framework for big data analysis. <em>TNNLS</em>, <em>35</em>(5),
6570–6582. (<a
href="https://doi.org/10.1109/TNNLS.2022.3211149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most multilayer Moore–Penrose inverse (MPI)-based neural networks, such as deep random vector functional link (RVFL), are structured with two separate stages: unsupervised feature encoding and supervised pattern classification. Once the unsupervised learning is finished, the latent encoding is fixed without supervised fine-tuning. However, in complex tasks such as handling the ImageNet dataset, there are often many more clues that can be directly encoded, while unsupervised learning, by definition, cannot know exactly what is useful for a certain task. There is a need to retrain the latent space representations in the supervised pattern classification stage to learn some clues that unsupervised learning has not yet been learned. In particular, the residual error in the output layer is pulled back to each hidden layer, and the parameters of the hidden layers are recalculated with MPI for more robust representations. In this article, a recomputation-based multilayer network using Moore–Penrose inverse (RML-MP) is developed. A sparse RML-MP (SRML-MP) model to boost the performance of RML-MP is then proposed. The experimental results with varying training samples (from 3k to 1.8 million) show that the proposed models provide higher Top-1 testing accuracy than most representation learning algorithms. For reproducibility, the source codes are available at https://github.com/W1AE/Retraining .},
  archive      = {J_TNNLS},
  author       = {Wandong Zhang and Yimin Yang and Q. M. Jonathan Wu and Tianlei Wang and Hui Zhang},
  doi          = {10.1109/TNNLS.2022.3211149},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6570-6582},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multimodal Moore–Penrose inverse-based recomputation framework for big data analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text feature adversarial learning for text generation with
knowledge transfer from GPT2. <em>TNNLS</em>, <em>35</em>(5), 6558–6569.
(<a href="https://doi.org/10.1109/TNNLS.2022.3210975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text generation is a key component of many natural language tasks. Motivated by the success of generative adversarial networks (GANs) for image generation, many text-specific GANs have been proposed. However, due to the discrete nature of text, these text GANs often use reinforcement learning (RL) or continuous relaxations to calculate gradients during learning, leading to high-variance or biased estimation. Furthermore, the existing text GANs often suffer from mode collapse (i.e., they have limited generative diversity). To tackle these problems, we propose a new text GAN model named text feature GAN (TFGAN), where adversarial learning is performed in a continuous text feature space. In the adversarial game, GPT2 provides the “true” features, while the generator of TFGAN learns from them. TFGAN is trained by maximum likelihood estimation on text space and adversarial learning on text feature space, effectively combining them into a single objective, while alleviating mode collapse. TFGAN achieves appealing performance in text generation tasks, and it can also be used as a flexible framework for learning text representations.},
  archive      = {J_TNNLS},
  author       = {Hao Zhang and Yulai Cong and Zhengjue Wang and Lei Zhang and Miaoyun Zhao and Liqun Chen and Shijing Si and Ricardo Henao and Lawrence Carin},
  doi          = {10.1109/TNNLS.2022.3210975},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6558-6569},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Text feature adversarial learning for text generation with knowledge transfer from GPT2},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diminishing batch normalization. <em>TNNLS</em>,
<em>35</em>(5), 6544–6557. (<a
href="https://doi.org/10.1109/TNNLS.2022.3210840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a generalization of the batch normalization (BN) algorithm, diminishing BN (DBN), where we update the BN parameters in a diminishing moving average way. BN is very effective in accelerating the convergence of a neural network training phase that it has become a common practice. Our proposed DBN algorithm retains the overall structure of the original BN algorithm while introducing a weighted averaging update to some trainable parameters. We provide an analysis of the convergence of the DBN algorithm that converges to a stationary point with respect to the trainable parameters. Our analysis can be easily generalized to the original BN algorithm by setting some parameters to constant. To the best of our knowledge, this analysis is the first of its kind for convergence with BN. We analyze a two-layer model with arbitrary activation functions. Common activation functions, such as ReLU and any smooth activation functions, meet our assumptions. In the numerical experiments, we test the proposed algorithm on complex modern CNN models with stochastic gradients (SGs) and ReLU activation on regression, classification, and image reconstruction tasks. We observe that DBN outperforms the original BN algorithm and benchmark layer normalization (LN) on the MNIST, NI, CIFAR-10, CIFAR-100, and Caltech-UCSD Birds-200-2011 datasets with modern complex CNN models such as Resnet-18 and typical FNN models.},
  archive      = {J_TNNLS},
  author       = {Yintai Ma and Diego Klabjan},
  doi          = {10.1109/TNNLS.2022.3210840},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6544-6557},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Diminishing batch normalization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Almost sure stability of complex-valued complex networks: A
noise-based delayed coupling under random denial-of-service attacks.
<em>TNNLS</em>, <em>35</em>(5), 6520–6530. (<a
href="https://doi.org/10.1109/TNNLS.2022.3210551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with stability for stochastic complex-valued delayed complex networks under random denial-of-service (RDoS) attacks. Different from the existing literature on the stability of stochastic complex-valued systems that concentrate on moment stability, we investigate almost sure stability (ASS), where noise plays a stabilizing role. It is noted that, besides the vertex systems influenced by noise, the interactions among vertices are also at the mercy of noise. As a consequence, an innovative noise-based delayed coupling (NDC) in the presence of RDoS attacks is proposed first to accomplish the stability of complex-valued networks, where the RDoS attacks have a certain probability of triumphantly interfering with communications at active intervals of attackers. Namely, RDoS attacks considered are randomly launched at active periods, which is more realistic. In terms of the Lyapunov method and stochastic analysis theory, an almost sure exponential stability (ASES) criterion of the system discussed straightforwardly is developed by constructing a delay-free auxiliary system, while removing the traditional assumption of moment stability. The criterion strongly linked with topological structure, RDoS frequency, attack successful probability, and noise intensity reveals that the higher the noise intensity, the faster the convergence rate is for the stability of the network. In light of the criterion established, we present an algorithm that can be employed to analyze the tolerable attack parameters and the upper bound of the coupling delays, under the prerequisite of guaranteeing the stability of the network. Eventually, the theoretical results are applied to inertial complex-valued neural networks (ICNNs) and an illustrative example is presented to substantiate the efficiency of the theoretical works.},
  archive      = {J_TNNLS},
  author       = {Sen Li and Xueqing Gao and Xiaohua Ding},
  doi          = {10.1109/TNNLS.2022.3210551},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6520-6530},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Almost sure stability of complex-valued complex networks: A noise-based delayed coupling under random denial-of-service attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A heterogeneous group CNN for image super-resolution.
<em>TNNLS</em>, <em>35</em>(5), 6507–6519. (<a
href="https://doi.org/10.1109/TNNLS.2022.3210433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have obtained remarkable performance via deep architectures. However, these CNNs often achieve poor robustness for image super-resolution (SR) under complex scenes. In this article, we present a heterogeneous group SR CNN (HGSRCNN) via leveraging structure information of different types to obtain a high-quality image. Specifically, each heterogeneous group block (HGB) of HGSRCNN uses a heterogeneous architecture containing a symmetric group convolutional block and a complementary convolutional block in a parallel way to enhance the internal and external relations of different channels for facilitating richer low-frequency structure information of different types. To prevent the appearance of obtained redundant features, a refinement block (RB) with signal enhancements in a serial way is designed to filter useless information. To prevent the loss of original information, a multilevel enhancement mechanism guides a CNN to achieve a symmetric architecture for promoting expressive ability of HGSRCNN. Besides, a parallel upsampling mechanism is developed to train a blind SR model. Extensive experiments illustrate that the proposed HGSRCNN has obtained excellent SR performance in terms of both quantitative and qualitative analysis. Codes can be accessed at https://github.com/hellloxiaotian/HGSRCNN .},
  archive      = {J_TNNLS},
  author       = {Chunwei Tian and Yanning Zhang and Wangmeng Zuo and Chia-Wen Lin and David Zhang and Yixuan Yuan},
  doi          = {10.1109/TNNLS.2022.3210433},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6507-6519},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A heterogeneous group CNN for image super-resolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Amplitude–time dual-view fused EEG temporal feature learning
for automatic sleep staging. <em>TNNLS</em>, <em>35</em>(5), 6492–6506.
(<a href="https://doi.org/10.1109/TNNLS.2022.3210384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) plays an important role in studying brain function and human cognitive performance, and the recognition of EEG signals is vital to develop an automatic sleep staging system. However, due to the complex nonstationary characteristics and the individual difference between subjects, how to obtain the effective signal features of the EEG for practical application is still a challenging task. In this article, we investigate the EEG feature learning problem and propose a novel temporal feature learning method based on amplitude–time dual-view fusion for automatic sleep staging. First, we explore the feature extraction ability of convolutional neural networks for the EEG signal from the perspective of interpretability and construct two new representation signals for the raw EEG from the views of amplitude and time. Then, we extract the amplitude–time signal features that reflect the transformation between different sleep stages from the obtained representation signals by using conventional 1-D CNNs. Furthermore, a hybrid dilation convolution module is used to learn the long-term temporal dependency features of EEG signals, which can overcome the shortcoming that the small-scale convolution kernel can only learn the local signal variation information. Finally, we conduct attention-based feature fusion for the learned dual-view signal features to further improve sleep staging performance. To evaluate the performance of the proposed method, we test 30-s-epoch EEG signal samples for healthy subjects and subjects with mild sleep disorders. The experimental results from the most commonly used datasets show that the proposed method has better sleep staging performance and has the potential for the development and application of an EEG-based automatic sleep staging system.},
  archive      = {J_TNNLS},
  author       = {Panfeng An and Jianhui Zhao and Bo Du and Wenyuan Zhao and Tingbao Zhang and Zhiyong Yuan},
  doi          = {10.1109/TNNLS.2022.3210384},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6492-6506},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Amplitude–Time dual-view fused EEG temporal feature learning for automatic sleep staging},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental trainable parameter selection in deep neural
networks. <em>TNNLS</em>, <em>35</em>(5), 6478–6491. (<a
href="https://doi.org/10.1109/TNNLS.2022.3210297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores the utilization of the effective degree-of-freedom (DoF) of a deep learning model to regularize its stochastic gradient descent (SGD)-based training. The effective DoF of a deep learning model is defined only by a subset of its total parameters. This subset is highly responsive or sensitive toward the training loss, and its cardinality can be used to govern the effective DoF of a model during training. To this aim, the incremental trainable parameter selection (ITPS) algorithm is introduced in this article. The proposed ITPS algorithm acts as a wrapper over SGD and incrementally selects the parameters for updation that exhibit the maximum sensitivity toward the training loss. Hence, it gradually increases the DoF of the model during training. In ideal cases, the proposed algorithm arrives at a model configuration (i.e., DoF) optimum for the task at hand. This whole process results in a regularization-like behavior induced by a gradual increment of the DoF. Since the selection and updation of parameters is a function of the training loss, the proposed algorithm can be seen as a task and data-dependent regularization mechanism. This article exhibits the general utility of ITPS by evaluating it on various prominent neural network architectures such as CNNs, transformers, recurrent neural networks (RNNs), and multilayer perceptrons. These models are trained for image classification and healthcare tasks using the publicly available CIFAR-10, SLT-10, and MIMIC-III datasets.},
  archive      = {J_TNNLS},
  author       = {Anshul Thakur and Vinayak Abrol and Pulkit Sharma and Tingting Zhu and David A. Clifton},
  doi          = {10.1109/TNNLS.2022.3210297},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6478-6491},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incremental trainable parameter selection in deep neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-based adaptive NN fixed-time cooperative formation for
multiagent systems. <em>TNNLS</em>, <em>35</em>(5), 6467–6477. (<a
href="https://doi.org/10.1109/TNNLS.2022.3210269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the fixed-time formation control problem for nonlinear multiagent systems (MASs) with dynamic uncertainties and limited communication resources. Under the framework of the backstepping method, a time-varying formation function is introduced in the controller design. To attain the prescribed transient and steady-state performance of MASs, a fixed-time prescribed performance function (FTPPF) is designed and the further coordinate transformation addressing the zero equilibrium point problem is removed. To achieve better approximating performance, a neural network (NN)-based composite dynamic surface control (CDSC) strategy is proposed, where the CDSC scheme is consisted of prediction errors and serial–parallel estimation models. According to the signals generated by the estimation models, disturbance observers are established to overcome the difficulty from approximating errors and mismatched disturbances. Moreover, an improved dynamic event-triggered mechanism and varying threshold parameters are constructed to reduce the signal transmission frequency. Via the Lyapunov stability theory, all the signals in the closed-loop system are semi-globally uniformly ultimately bounded. Finally, the simulation results verify the effectiveness of the developed CDSC strategy.},
  archive      = {J_TNNLS},
  author       = {Liang Cao and Zhijian Cheng and Yang Liu and Hongyi Li},
  doi          = {10.1109/TNNLS.2022.3210269},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6467-6477},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-based adaptive NN fixed-time cooperative formation for multiagent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularizing scale-adaptive central moment sharpness for
neural networks. <em>TNNLS</em>, <em>35</em>(5), 6452–6466. (<a
href="https://doi.org/10.1109/TNNLS.2022.3210045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep learning, finding flat minima of loss function is a hot research topic in improving generalization. The existing methods usually find flat minima by sharpness minimization algorithms. However, these methods suffer from insufficient flexibility for optimization and generalization due to their ignorance of loss value. This article theoretically and experimentally explores the sharpness minimization algorithms for neural networks. First, a novel scale-invariant sharpness which is called scale-adaptive central moment sharpness (SA-CMS) is proposed. This sharpness is not only scale-invariant but can characterize the nature of loss surface clearly. Based on the proposed sharpness, this article further derives a new regularization term by integrating the different orders of the sharpness. Particularly, a host of sharpness minimization functions such as local entropy can be covered by this regularization term. Then the central moment sharpness generating function is introduced as a new objective function. Moreover, theoretical analyses indicate that the new objective function has a smoother landscape and prefer converging to flat local minima. Furthermore, a computationally efficient two-stage algorithm is developed to minimize the objective function. Compared with other algorithms, the two-stage loss-sharpness minimization (TSLSM) algorithm offers a more flexible optimization target for different training stages. On a variety of learning tasks with both small and large batch sizes, this algorithm is more universal and effective, and meanwhile achieves or surpasses the generalization performance of the state-of-the-art sharpness minimization algorithms.},
  archive      = {J_TNNLS},
  author       = {Junhong Chen and Ziyang Guo and Hong Li and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3210045},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6452-6466},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Regularizing scale-adaptive central moment sharpness for neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label noise robust collaborative learning for remote
sensing image classification. <em>TNNLS</em>, <em>35</em>(5), 6438–6451.
(<a href="https://doi.org/10.1109/TNNLS.2022.3209992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of accurate methods for multi-label classification (MLC) of remote sensing (RS) images is one of the most important research topics in RS. The MLC methods based on convolutional neural networks (CNNs) have shown strong performance gains in RS. However, they usually require a high number of reliable training images annotated with multiple land-cover class labels. Collecting such data is time-consuming and costly. To address this problem, the publicly available thematic products, which can include noisy labels, can be used to annotate RS images with zero-labeling cost. However, multi-label noise (which can be associated with wrong and missing label annotations) can distort the learning process of the MLC methods. To address this problem, we propose a novel multi-label noise robust collaborative learning (RCML) method to alleviate the negative effects of multi-label noise during the training phase of a CNN model. RCML identifies, ranks, and excludes noisy multi-labels in RS images based on three main modules: 1) the discrepancy module; 2) the group lasso module; and 3) the swap module. The discrepancy module ensures that the two networks learn diverse features, while producing the same predictions. The task of the group lasso module is to detect the potentially noisy labels assigned to multi-labeled training images, while the swap module is devoted to exchange the ranking information between two networks. Unlike the existing methods that make assumptions about noise distribution, our proposed RCML does not make any prior assumption about the type of noise in the training set. The experiments conducted on two multi-label RS image archives confirm the robustness of the proposed RCML under extreme multi-label noise rates. Our code is publicly available at: https://www.noisy-labels-in-rs.org .},
  archive      = {J_TNNLS},
  author       = {Ahmet Kerem Aksoy and Mahdyar Ravanbakhsh and Begüm Demir},
  doi          = {10.1109/TNNLS.2022.3209992},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6438-6451},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-label noise robust collaborative learning for remote sensing image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Electromagnetic source imaging via a data-synthesis-based
convolutional encoder–decoder network. <em>TNNLS</em>, <em>35</em>(5),
6423–6437. (<a
href="https://doi.org/10.1109/TNNLS.2022.3209925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electromagnetic source imaging (ESI) requires solving a highly ill-posed inverse problem. To seek a unique solution, traditional ESI methods impose various forms of priors that may not accurately reflect the actual source properties, which may hinder their broad applications. To overcome this limitation, in this article, a novel data-synthesized spatiotemporally convolutional encoder–decoder network (DST-CedNet) method is proposed for ESI. The DST-CedNet recasts ESI as a machine learning problem, where discriminative learning and latent-space representations are integrated in a CedNet to learn a robust mapping from the measured electroencephalography/magnetoencephalography (E/MEG) signals to the brain activity. In particular, by incorporating prior knowledge regarding dynamical brain activities, a novel data synthesis strategy is devised to generate large-scale samples for effectively training CedNet. This stands in contrast to traditional ESI methods where the prior information is often enforced via constraints primarily aimed for mathematical convenience. Extensive numerical experiments as well as analysis of a real MEG and epilepsy EEG dataset demonstrate that the DST-CedNet outperforms several state-of-the-art ESI methods in robustly estimating source signals under a variety of source configurations.},
  archive      = {J_TNNLS},
  author       = {Gexin Huang and Ke Liu and Jiawen Liang and Chang Cai and Zheng Hui Gu and Feifei Qi and Yuanqing Li and Zhu Liang Yu and Wei Wu},
  doi          = {10.1109/TNNLS.2022.3209925},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6423-6437},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Electromagnetic source imaging via a data-synthesis-based convolutional Encoder–Decoder network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLRNet: A cross locality relation network for crowd counting
in videos. <em>TNNLS</em>, <em>35</em>(5), 6408–6422. (<a
href="https://doi.org/10.1109/TNNLS.2022.3209918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new cross locality relation network (CLRNet) to generate high-quality crowd density maps for crowd counting in videos. Specifically, a cross locality relation module (CLRM) is proposed to enhance feature representations by modeling local dependencies of pixels between adjacent frames with an adapted local self-attention mechanism. First, different from the existing methods which measure similarity between pixels by dot product, a new adaptive cosine similarity is advanced to measure the relationship between two positions. Second, the traditional self-attention modules usually integrate the reconstructed features with the same weights for all the positions. However, crowd movement and background changes in a video sequence are uneven in real-life applications. As a consequence, it is inappropriate to treat all the positions in reconstructed features equally. To address this issue, a scene consistency attention map (SCAM) is developed to make CLRM pay more attention to the positions with strong correlations in adjacent frames. Furthermore, CLRM is incorporated into the network in a coarse-to-fine way to further enhance the representational capability of features. Experimental results demonstrate the effectiveness of our proposed CLRNet in comparison to the state-of-the-art methods on four public video datasets. The codes are available at: https://github.com/Amelie01/CLRNet .},
  archive      = {J_TNNLS},
  author       = {Li Dong and Haijun Zhang and Jianghong Ma and Xiaofei Xu and Yimin Yang and Q. M. Jonathan Wu},
  doi          = {10.1109/TNNLS.2022.3209918},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6408-6422},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CLRNet: A cross locality relation network for crowd counting in videos},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Partial-neurons-based proportional-integral observer design
for artificial neural networks: A multiple description encoding scheme.
<em>TNNLS</em>, <em>35</em>(5), 6393–6407. (<a
href="https://doi.org/10.1109/TNNLS.2022.3209632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with a new partial-neurons-based proportional-integral observer (PIO) design problem for a class of artificial neural networks (ANNs) subject to bounded disturbances. For the purpose of improving the reliability of the data transmission, the multiple description encoding mechanisms are exploited to encode the measurement data into two identically important descriptions, and the encoded data are then transmitted to the decoders via two individual communication channels susceptible to packet dropouts, where Bernoulli-distributed stochastic variables are utilized to characterize the random occurrence of the packet dropouts. An explicit relationship is discovered that quantifies the influences of the packet dropouts on the decoding accuracy, and a sufficient condition is provided to assess the boundedness of the estimation error dynamics. Furthermore, the desired PIO parameters are calculated by solving two optimization problems based on two metrics (i.e., the smallest ultimate bound and the fastest decay rate) characterizing the estimation performance. Finally, the applicability and advantage of the proposed PIO design strategy are verified by means of an illustrative example.},
  archive      = {J_TNNLS},
  author       = {Di Zhao and Zidong Wang and Yun Chen and Guoliang Wei and Weiguo Sheng},
  doi          = {10.1109/TNNLS.2022.3209632},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6393-6407},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Partial-neurons-based proportional-integral observer design for artificial neural networks: A multiple description encoding scheme},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised simple siamese framework for fault diagnosis
of rotating machinery with unlabeled samples. <em>TNNLS</em>,
<em>35</em>(5), 6380–6392. (<a
href="https://doi.org/10.1109/TNNLS.2022.3209332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis is vital to ensuring the security of rotating machinery operations. While fault data obtained from mechanical equipment for this issue are often insufficient and of no labels. In this case, supervised algorithms cannot come into play. Hence, this article proposes a self-supervised simple Siamese framework (SSF) for bearing fault diagnosis based on the contrastive learning algorithm SimSiam which uses a simplified Siamese network to find the distinguishable features of different fault categories. SSF consists of a weight-sharing encoder applied on two inputs, a nonlinear predictor and a linear classifier. SSF learns invariant characteristics of fault samples via maximizing the similarity between two views of each inputted sample. Several data augmentation (DA) methods for vibration signals, which provide different sample views for the model, are also studied, for it is crucial for contrastive learning. After fine-tuning the learned encoder and a linear layer classifier with a small subset of labeled data (1%–5% of the total samples), the network achieves satisfactory performance for bearing fault diagnosis. A series of experiments based on the data from three different scenarios are used to verify the proposed methods, getting 100%, 99.38%, and 98.87% accuracy separately.},
  archive      = {J_TNNLS},
  author       = {Wenqing Wan and Jinglong Chen and Zitong Zhou and Zhen Shi},
  doi          = {10.1109/TNNLS.2022.3209332},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6380-6392},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised simple siamese framework for fault diagnosis of rotating machinery with unlabeled samples},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attack-resilient distributed nash equilibrium seeking of
uncertain multiagent systems over unreliable communication networks.
<em>TNNLS</em>, <em>35</em>(5), 6365–6379. (<a
href="https://doi.org/10.1109/TNNLS.2022.3209313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the distributed Nash equilibrium (NE) seeking problem of uncertain multiagent systems in unreliable communication networks. In this problem, the action of each agent is subject to a class of nonlinear systems with uncertain dynamics, and the communication network among agents will be affected by the nonperiodic denial of service (DoS) attacks. Note that, in this insecure network environment, the existence of DoS attacks will directly destroy the connectivity of the network, which leads to performance degradation or even failure of the most existing distributed NE seeking algorithms. To address this problem, we propose a two-stage distributed NE seeking strategy, including the attack-resilient distributed NE estimator and the neuroadaptive tracking controller. The estimator based on the projection subgradient method and the consensus protocol can converge exponentially to virtual NE against DoS attacks. Then, the neuroadaptive tracking controller is designed for uncertain multiagent systems with the output of the estimator as the reference signal such that the actual action of all agents can reach NE. Based on the Lyapunov stability theory and improved average dwell time automaton, the stability of the estimator and the controller is proven, and all signals in the closed-loop system are uniformly bounded. Numerical examples are presented to verify the effectiveness of the proposed strategy.},
  archive      = {J_TNNLS},
  author       = {Qing Meng and Xiaohong Nian and Yong Chen and Zhao Chen},
  doi          = {10.1109/TNNLS.2022.3209313},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6365-6379},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attack-resilient distributed nash equilibrium seeking of uncertain multiagent systems over unreliable communication networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parareal neural networks emulating a parallel-in-time
algorithm. <em>TNNLS</em>, <em>35</em>(5), 6353–6364. (<a
href="https://doi.org/10.1109/TNNLS.2022.3206797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep neural networks (DNNs) become deeper, the training time increases. In this perspective, multi-GPU parallel computing has become a key tool in accelerating the training of DNNs. In this article, we introduce a novel methodology to construct a parallel neural network that can utilize multiple GPUs simultaneously from a given DNN. We observe that layers of DNN can be interpreted as the time steps of a time-dependent problem and can be parallelized by emulating a parallel-in-time algorithm called parareal. The parareal algorithm consists of fine structures which can be implemented in parallel and a coarse structure that gives suitable approximations to the fine structures. By emulating it, the layers of DNN are torn to form a parallel structure, which is connected using a suitable coarse network. We report accelerated and accuracy-preserved results of the proposed methodology applied to VGG-16 and ResNet-1001 on several datasets.},
  archive      = {J_TNNLS},
  author       = {Youngkyu Lee and Jongho Park and Chang-Ock Lee},
  doi          = {10.1109/TNNLS.2022.3206797},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6353-6364},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parareal neural networks emulating a parallel-in-time algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel accelerated multistage learning control mechanism
via virtual performance reduction. <em>TNNLS</em>, <em>35</em>(5),
6338–6352. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study uses a multistage learning mechanism concept to investigate the accelerated learning control for stochastic systems. In this mechanism, the learning iterations are divided into successive stages, with each stage comprising several iterations. The learning gain is constant in each stage to accelerate the learning process and decreases it from one stage to another to eliminate the noise effect asymptotically. The critical issue is determining the switching iteration when a new stage starts. This study resolves this issue by calculating a virtual performance index of the mean-squared input error and its estimated upper bound. Specifically, the ideal, practical, and improved multistage learning control schemes are proposed to determine the switching iteration and generate the learning gain sequence. The ideal scheme achieves the best performance at the cost of a large computation burden, and the practical scheme saves computation cost, but the performance is not excellent. The improved scheme significantly approximates the best performance by introducing additional stretching parameters to the performance index. Illustrative simulations are provided to verify the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Xiang Cheng and Hao Jiang and Dong Shen},
  doi          = {10.1109/TNNLS.2022.3212766},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6338-6352},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel accelerated multistage learning control mechanism via virtual performance reduction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imitation learning: Progress, taxonomies and challenges.
<em>TNNLS</em>, <em>35</em>(5), 6322–6337. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imitation learning (IL) aims to extract knowledge from human experts’ demonstrations or artificially created agents to replicate their behaviors. It promotes interdisciplinary communication and real-world automation applications. However, the process of replicating behaviors still exhibits various problems, such as the performance is highly dependent on the demonstration quality, and most trained agents are limited to perform well in task-specific environments. In this survey, we provide an insightful review on IL. We first introduce the background knowledge from development history and preliminaries, followed by presenting different taxonomies within IL and key milestones of the field. We then detail challenges in learning strategies and present research opportunities with learning policy from suboptimal demonstration, voice instructions, and other associated optimization schemes.},
  archive      = {J_TNNLS},
  author       = {Boyuan Zheng and Sunny Verma and Jianlong Zhou and Ivor W. Tsang and Fang Chen},
  doi          = {10.1109/TNNLS.2022.3213246},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6322-6337},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Imitation learning: Progress, taxonomies and challenges},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on deep learning event extraction: Approaches and
applications. <em>TNNLS</em>, <em>35</em>(5), 6301–6321. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event extraction (EE) is a crucial research task for promptly apprehending event information from massive textual data. With the rapid development of deep learning, EE based on deep learning technology has become a research hotspot. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This article fills the research gap by reviewing the state-of-the-art approaches, especially focusing on the general domain EE based on deep learning models. We introduce a new literature classification of current general domain EE research according to the task definition. Afterward, we summarize the paradigm and models of EE approaches, and then discuss each of them in detail. As an important aspect, we summarize the benchmarks that support tests of predictions and evaluation metrics. A comprehensive comparison among different approaches is also provided in this survey. Finally, we conclude by summarizing future research directions facing the research area.},
  archive      = {J_TNNLS},
  author       = {Qian Li and Jianxin Li and Jiawei Sheng and Shiyao Cui and Jia Wu and Yiming Hei and Hao Peng and Shu Guo and Lihong Wang and Amin Beheshti and Philip S. Yu},
  doi          = {10.1109/TNNLS.2022.3213168},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6301-6321},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on deep learning event extraction: Approaches and applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Concurrent learning robust adaptive fault tolerant boundary
regulation of hyperbolic distributed parameter systems. <em>TNNLS</em>,
<em>35</em>(5), 6286–6300. (<a
href="https://doi.org/10.1109/TNNLS.2022.3224245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a robust adaptive boundary output regulation approach for a class of complex anticollocated hyperbolic partial differential equations subjected to multiplicative unknown faults in both the boundary sensor and actuator. The regulator design is based on the internal model principle, which amounts to stabilize a coupled cascade system, which consists of a finite-dimensional internal model driven by a hyperbolic distributed parameter system (DPS). To this end, a systematic sliding mode equipped with a backstepping approach is developed such that the robust state feedback control can be realized. Moreover, since the available information is a faulty boundary measurement at the right side point, state estimation is required. However, due to the presence of boundary unknown faults, we need to solve an issue of joint fault-state estimation. Restrictive persistent excitation conditions are usually required to guarantee the exact estimation of faults but are unrealistic in practice. To this end, a novel concurrent learning (CL) adaptive observer is proposed so that exponential convergence is obtained. It is the first time that the spirit of CL is introduced to the field of DPSs. Consequently, the observer-based adaptive boundary fault tolerant control scheme is developed, and rigorous theoretical analysis is given such that the exponential output regulation can be achieved. Finally, the effectiveness of the proposed methodology is demonstrated via comparative simulations.},
  archive      = {J_TNNLS},
  author       = {Yuan Yuan and Xiaodong Xu and Chunhua Yang and Biao Luo and Stevan Dubljevic},
  doi          = {10.1109/TNNLS.2022.3224245},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6286-6300},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Concurrent learning robust adaptive fault tolerant boundary regulation of hyperbolic distributed parameter systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Neural-network-based adaptive fault-tolerant cooperative
control of heterogeneous multiagent systems with multiple faults and DoS
attacks. <em>TNNLS</em>, <em>35</em>(5), 6273–6285. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the issue of adaptive fault-tolerant cooperative control is addressed for heterogeneous multiple unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) with actuator faults and sensor faults under denial-of-service (DoS) attacks. First, a unified control model with actuator faults and sensor faults is developed based on the dynamic models of the UAVs and UGVs. To handle the difficulty introduced by the nonlinear term, a neural-network-based switching-type observer is established to obtain the unmeasured state variables when DoS attacks are active. Then, the fault-tolerant cooperative control scheme is presented by utilizing an adaptive backstepping control algorithm under DoS attacks. According to Lyapunov stability theory and improved average dwell time method by integrating the duration and frequency characteristics of DoS attacks, the stability of the closed-loop system is proved. In addition, all vehicles can track their individual references, while the synchronized tracking errors among vehicles are uniformly ultimately bounded. Finally, simulation studies are given to demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Shangkun Liu and Bin Jiang and Zehui Mao and Youmin Zhang},
  doi          = {10.1109/TNNLS.2023.3282234},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6273-6285},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive fault-tolerant cooperative control of heterogeneous multiagent systems with multiple faults and DoS attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-global bipartite fault-tolerant containment control for
heterogeneous multiagent systems with antagonistic communication
networks and input saturation. <em>TNNLS</em>, <em>35</em>(5),
6265–6272. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-global bipartite fault-tolerant containment control framework on antagonistic communication networks is proposed in this article for heterogeneous multiagent systems (MASs) under the influence of input saturation and actuator faults. An observer is constructed to estimate the leaders’ states on signed digraph, where the communication networks are antagonistic. A fully distributed virtual control approach is developed to acquire the containment trajectory. Based on the observer, a semi-global containment control method is developed to compensate for the detrimental impacts of both input saturation and actuator faults. Besides, the dynamics and state-space dimensions of the agents can be different. The proposed framework overcomes two drawbacks of the conventional containment control: 1) the containment trajectory is obtained under general antagonistic communication networks, which is more general in engineering applications and 2) both actuator faults and input saturation are solved for heterogeneous agents, which relaxes the limitation of homogeneous dynamics. Finally, a simulation example is conducted to test and verify the feasibility of the proposed method framework.},
  archive      = {J_TNNLS},
  author       = {Malika Sader and Wenyu Li and Haijun Jiang and Zengqiang Chen and Zhongxin Liu},
  doi          = {10.1109/TNNLS.2022.3208449},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6265-6272},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semi-global bipartite fault-tolerant containment control for heterogeneous multiagent systems with antagonistic communication networks and input saturation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuroadaptive fault-tolerant control embedded with
diversified activating functions with application to auto-driving
vehicles under fading actuation. <em>TNNLS</em>, <em>35</em>(5),
6255–6264. (<a
href="https://doi.org/10.1109/TNNLS.2023.3248100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a neuroadaptive fault-tolerant control method for path tracking of multiinput multioutput (MIMO) systems in the presence of modeling uncertainties and external disturbances. In dealing with modeling uncertainties, neural networks (NNs) with diversified activation/basis functions are considered, with which we establish a set of control algorithms that are robust against uncertainties, adaptive to unknown parameters, and tolerant to actuation faults. This is the first work that explicitly takes into account the neural weights uncertainties and activating function uncertainties in multiple layered neural networks in control design. In addition, we apply the developed control algorithms to unmanned ground vehicles (UGVs) with actuator failures. With the aid of Lyapunov stability theory, it is shown that the proposed control is able to drive the vehicle along the desired path with high precision and all the internal signals are uniformly ultimately bounded (UUB) and continuous. Both theoretical analysis and numerical simulation confirm the effectiveness of the designed strategy.},
  archive      = {J_TNNLS},
  author       = {Zhen Gao and Wei Yu and Jun Yan},
  doi          = {10.1109/TNNLS.2023.3248100},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6255-6264},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuroadaptive fault-tolerant control embedded with diversified activating functions with application to auto-driving vehicles under fading actuation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). JITL-MBN: A real-time causality representation learning for
sensor fault diagnosis of traction drive system in high-speed trains.
<em>TNNLS</em>, <em>35</em>(5), 6243–6254. (<a
href="https://doi.org/10.1109/TNNLS.2022.3173337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A traction drive system (TDS) in high-speed trains is composed of various modules including rectifier, intermediate dc link, inverter, and others; the sensor fault of one module will lead to abnormal measurement of sensor in other modules. At the same time, the fault diagnosis methods based on single-operating condition are unsuitable to the TDS under multi-operating conditions, because a fault appears various in different conditions. To this end, a real-time causality representation learning based on just-in-time learning (JITL) and modular Bayesian network (MBN) is proposed to diagnose its sensor faults. In specific, the proposed method tracks the change of operating conditions and learns potential features in real time by JITL. Then, the MBN learns causality representation between faults and features to diagnose sensor faults. Due to the reduction of the nodes number, the MBN alleviates the problem of slow real-time modeling speed. To verity the effectiveness of the proposed method, experiments are carried out. The results show that the proposed method has the best performance than several traditional methods in the term of fault diagnosis accuracy.},
  archive      = {J_TNNLS},
  author       = {Zhiwen Chen and Wenying Chen and Xinyu Fan and Tao Peng and Chunhua Yang},
  doi          = {10.1109/TNNLS.2022.3173337},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6243-6254},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {JITL-MBN: A real-time causality representation learning for sensor fault diagnosis of traction drive system in high-speed trains},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated multitasking intelligent bearing fault
diagnosis scheme based on representation learning under imbalanced
sample condition. <em>TNNLS</em>, <em>35</em>(5), 6231–6242. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate bearing fault diagnosis is of great significance of the safety and reliability of rotary mechanical system. In practice, the sample proportion between faulty data and healthy data in rotating mechanical system is imbalanced. Furthermore, there are commonalities between the bearing fault detection, classification, and identification tasks. Based on these observations, this article proposes a novel integrated multitasking intelligent bearing fault diagnosis scheme with the aid of representation learning under imbalanced sample condition, which realizes bearing fault detection, classification, and unknown fault identification. Specifically, in the unsupervised condition, a bearing fault detection approach based on modified denoising autoencoder (DAE) with self-attention mechanism for bottleneck layer (MDAE-SAMB) is proposed in the integrated scheme, which only uses the healthy data for training. The self-attention mechanism is introduced into the neurons in the bottleneck layer, which can assign different weights to the neurons in the bottleneck layer. Moreover, the transfer learning based on representation learning is proposed for few-shot fault classification. Only a few fault samples are used for offline training, and high-accuracy online bearing fault classification is achieved. Finally, according to the known fault data, the unknown bearing faults can be effectively identified. A bearing dataset generated by rotor dynamics experiment rig (RDER) and a public bearing dataset demonstrates the applicability of the proposed integrated fault diagnosis scheme.},
  archive      = {J_TNNLS},
  author       = {Jiusi Zhang and Ke Zhang and Yiyao An and Hao Luo and Shen Yin},
  doi          = {10.1109/TNNLS.2022.3232147},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6231-6242},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An integrated multitasking intelligent bearing fault diagnosis scheme based on representation learning under imbalanced sample condition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial cross mapping based on sparse variable selection for
direct fault root cause diagnosis for industrial processes.
<em>TNNLS</em>, <em>35</em>(5), 6218–6230. (<a
href="https://doi.org/10.1109/TNNLS.2023.3242361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Root cause diagnosis of process industry is of significance to ensure safe production and improve production efficiency. Conventional contribution plot methods have challenges in root cause diagnosis due to the smearing effect. Other traditional root cause diagnosis methods, such as Granger causality (GC) and transfer entropy, have unsatisfactory performance in root cause diagnosis for complex industrial processes due to the existence of indirect causality. In this work, a regularization and partial cross mapping (PCM)-based root cause diagnosis framework is proposed for efficient direct causality inference and fault propagation path tracing. First, generalized Lasso-based variable selection is performed. The Hotelling $T^{2}$ statistic is formulated and the Lasso-based fault reconstruction is applied to select candidate root cause variables. Second, the root cause is diagnosed through the PCM and the propagation path is drawn out according to the diagnosis result. The proposed framework is studied in four cases to verify its rationality and effectiveness, including a numerical example, the Tennessee Eastman benchmark process, the wastewater treatment process (WWTP), and the decarburization process of high-speed wire rod spring steel.},
  archive      = {J_TNNLS},
  author       = {Qingchao Jiang and Jiashi Jiang and Wenjing Wang and Chunjian Pan and Weimin Zhong},
  doi          = {10.1109/TNNLS.2023.3242361},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6218-6230},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Partial cross mapping based on sparse variable selection for direct fault root cause diagnosis for industrial processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reweighted regularized prototypical network for few-shot
fault diagnosis. <em>TNNLS</em>, <em>35</em>(5), 6206–6217. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the challenging few-shot fault diagnosis (FSFD) problem where limited faulty samples are available. Metric-based meta-learning methods have been a prevalent approach toward FSFD; however, most of them rely on learning a generalized distance metric and fall short of leveraging intraclass and interclass distribution information. To this end, we develop a novel reweighted regularized prototypical network to improve the performance of FSFD, where an intraclass reweighting strategy is proposed to reduce the influence of noise and outliers and obtain stable estimations of fault prototypes. In addition, a novel balance-enforcing regularization (BER) is proposed to hedge against the between-class imbalance and improve the discrimination capability. These two remedies help to reduce the intraclass difference and enlarge the interclass difference via episodic training. In this way, an improved metric space and a better diagnostic performance can be attained in a few-shot learning context. Case studies on the Tennessee Eastman benchmark process and a real-world railway turnout dataset demonstrate that the proposed FSFD approach compares favorably against state-of-the-art methodologies with desirable diagnostic performance.},
  archive      = {J_TNNLS},
  author       = {Kang Li and Chao Shang and Hao Ye},
  doi          = {10.1109/TNNLS.2022.3232394},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6206-6217},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reweighted regularized prototypical network for few-shot fault diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCCAM: Supervised contrastive convolutional attention
mechanism for ante-hoc interpretable fault diagnosis with limited fault
samples. <em>TNNLS</em>, <em>35</em>(5), 6194–6205. (<a
href="https://doi.org/10.1109/TNNLS.2023.3313728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real industrial processes, fault diagnosis methods are required to learn from limited fault samples since the procedures are mainly under normal conditions and the faults rarely occur. Although attention mechanisms have become increasingly popular for the task of fault diagnosis, the existing attention-based methods are still unsatisfying for the above practical applications. First, pure attention-based architectures like transformers need a substantial quantity of fault samples to offset the lack of inductive biases thus performing poorly under limited fault samples. Moreover, the poor fault classification dilemma further leads to the failure of the existing attention-based methods to identify the root causes. To develop a solution to the aforementioned problems, we innovatively propose a supervised contrastive convolutional attention mechanism (SCCAM) with ante-hoc interpretability, which solves the root cause analysis problem under limited fault samples for the first time. First, accurate classification results are obtained under limited fault samples. More specifically, we integrate the convolutional neural network (CNN) with attention mechanisms to provide strong intrinsic inductive biases of locality and spatial invariance, thereby strengthening the representational power under limited fault samples. In addition, we ulteriorly enhance the classification capability of the SCCAM method under limited fault samples by employing the supervised contrastive learning (SCL) loss. Second, a novel ante-hoc interpretable attention-based architecture is designed to directly obtain the root causes without expert knowledge. The convolutional block attention module (CBAM) is utilized to directly provide feature contributions behind each prediction thus achieving feature-level explanations. The proposed SCCAM method is testified on a continuous stirred tank heater (CSTH) and the Tennessee Eastman (TE) industrial process benchmark. Three common fault diagnosis scenarios are covered, including a balanced scenario for additional verification and two scenarios with limited fault samples (i.e., imbalanced scenario and long-tail scenario). The effectiveness of the presented SCCAM method is evidenced by the comprehensive results that show our method outperforms the state-of-the-art methods in terms of fault classification and root cause analysis.},
  archive      = {J_TNNLS},
  author       = {Mengxuan Li and Peng Peng and Jingxin Zhang and Hongwei Wang and Weiming Shen},
  doi          = {10.1109/TNNLS.2023.3313728},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6194-6205},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SCCAM: Supervised contrastive convolutional attention mechanism for ante-hoc interpretable fault diagnosis with limited fault samples},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational attention-based interpretable transformer
network for rotary machine fault diagnosis. <em>TNNLS</em>,
<em>35</em>(5), 6180–6193. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning technology provides a promising approach for rotary machine fault diagnosis (RMFD), where vibration signals are commonly utilized as input of a deep network model to reveal the internal state of machinery. However, most existing methods fail to mine association relationships within signals. Unlike deep neural networks, transformer networks are capable of capturing association relationships through the global self-attention mechanism to enhance feature representations from vibration signals. Despite this, transformer networks cannot explicitly establish the causal association between signal patterns and fault types, resulting in poor interpretability. To tackle these problems, an interpretable deep learning model named the variational attention-based transformer network (VATN) is proposed for RMFD. VATN is improved from transformer encoder to mine the association relationships within signals. To embed the prior knowledge of the fault type, which can be recognized based on several key features of vibration signals, a sparse constraint is designed for attention weights. Variational inference is employed to force attention weights to samples from Dirichlet distributions, and Laplace approximation is applied to realize reparameterization. Finally, two experimental studies conducted on bevel gear and bearing datasets demonstrate the effectiveness of VATN to other comparison methods, and the heat map of attention weights illustrates the causal association between fault types and signal patterns.},
  archive      = {J_TNNLS},
  author       = {Yasong Li and Zheng Zhou and Chuang Sun and Xuefeng Chen and Ruqiang Yan},
  doi          = {10.1109/TNNLS.2022.3202234},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6180-6193},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Variational attention-based interpretable transformer network for rotary machine fault diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable intelligent fault diagnosis for nonlinear
dynamic systems: From unsupervised to supervised learning.
<em>TNNLS</em>, <em>35</em>(5), 6166–6179. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increased complexity and intelligence of automation systems require the development of intelligent fault diagnosis (IFD) methodologies. By relying on the concept of a suspected space, this study develops explainable data-driven IFD approaches for nonlinear dynamic systems. More specifically, we parameterize nonlinear systems through a generalized kernel representation for system modeling and the associated fault diagnosis. An important result obtained is a unified form of kernel representations, applicable to both unsupervised and supervised learning. More importantly, through a rigorous theoretical analysis, we discover the existence of a bridge (i.e., a bijective mapping) between some supervised and unsupervised learning-based entities. Notably, the designed IFD approaches achieve the same performance with the use of this bridge. In order to have a better understanding of the results obtained, both unsupervised and supervised neural networks are chosen as the learning tools to identify the generalized kernel representations and design the IFD schemes; an invertible neural network is then employed to build the bridge between them. This article is a perspective article, whose contribution lies in proposing and formalizing the fundamental concepts for explainable intelligent learning methods, contributing to system modeling and data-driven IFD designs for nonlinear dynamic systems.},
  archive      = {J_TNNLS},
  author       = {Hongtian Chen and Zhigang Liu and Cesare Alippi and Biao Huang and Derong Liu},
  doi          = {10.1109/TNNLS.2022.3201511},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6166-6179},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Explainable intelligent fault diagnosis for nonlinear dynamic systems: From unsupervised to supervised learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Codesign of FDI attacks detection, isolation, and mitigation
for complex microgrid systems: An HBF-NN-based approach. <em>TNNLS</em>,
<em>35</em>(5), 6156–6165. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary purpose of this article is to design an intelligent false data injection (FDI) attacks detection, isolation, and mitigation scheme for a class of complex microgrid systems with electric vehicles (EVs). First, a networked microgrid with an EV model is well established, which takes load disturbance, wind generation fluctuation, and FDI attacks into account so as to truly reflect the operation process of the complex system. Then, an intelligent hyper basis function neural network (HBF-NN) observer is designed to accurately estimate the state of the microgrids, learn, and reconstruct the possible attack signal online. Subsequently, a novel HBF-NN-based ${\mathcal {H}}_{\infty }$ controller is skillfully designed to mitigate the negative impact of FDI attacks online, so as to ensure the normal operation of the complex systems in an unreliable network environment. Finally, a two-stage integrated intelligent detection and maintenance algorithm is summarized and one simulation is presented to provide tangible evidence of the feasibility and superiority of the proposed FDI attacks detection, isolation, and mitigation methodology.},
  archive      = {J_TNNLS},
  author       = {Engang Tian and Zhihua Wu and Xiangpeng Xie},
  doi          = {10.1109/TNNLS.2022.3230056},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6156-6165},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Codesign of FDI attacks detection, isolation, and mitigation for complex microgrid systems: An HBF-NN-based approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representation-learning-based CNN for intelligent attack
localization and recovery of cyber-physical power systems.
<em>TNNLS</em>, <em>35</em>(5), 6145–6155. (<a
href="https://doi.org/10.1109/TNNLS.2023.3257225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabled by the advances in communication networks, computational units, and control systems, cyber-physical power systems (CPPSs) are anticipated to be complex and smart systems in which a large amount of data are generated, exchanged, and processed for various purposes. Due to these strong interactions, CPPSs will introduce new security vulnerabilities. To ensure secure operation and control of CPPSs, it is essential to detect the locations of the attacked measurements and remove the state bias caused by malicious cyber-attacks such as false data inject attack, jamming attack, denial of service attack, or hybrid attack. Accordingly, this article makes the first contribution concerning the representation-learning-based convolutional neural network (RL-CNN) for intelligent attack localization and system recovery of CPPSs. In the proposed method, the cyber-attacks’ locational detection problem is formulated as a multilabel classification problem for CPPSs. An RL-CNN is originally adopted as the multilabel classifier to explore and exploit the implicit information of measurements. By comparing with previous multilabel classifiers, the RL-CNN improves the performance of attack localization for complex CPPSs. Then, to automatically filter out the cyber-attacks for system recovery, a mean-squared estimator is used to handle the difficulty in state estimation with the removal of contaminated measurements. In this scheme, prior knowledge of the system state is obtained based on the outputs of the stochastic power flow or historical measurements. The extensive simulation results in three IEEE bus systems show that the proposed method is able to provide high accuracy for attack localization and perform automatic attack filtering for system recovery under various cyber-attacks.},
  archive      = {J_TNNLS},
  author       = {Kang-Di Lu and Le Zhou and Zheng-Guang Wu},
  doi          = {10.1109/TNNLS.2023.3257225},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6145-6155},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Representation-learning-based CNN for intelligent attack localization and recovery of cyber-physical power systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving robustness of intent detection under adversarial
attacks: A geometric constraint perspective. <em>TNNLS</em>,
<em>35</em>(5), 6133–6144. (<a
href="https://doi.org/10.1109/TNNLS.2023.3267460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs)-based natural language processing (NLP) systems are vulnerable to being fooled by adversarial examples presented in recent studies. Intent detection tasks in dialog systems are no exception, however, relatively few works have been attempted on the defense side. The combination of linear classifier and softmax is widely used in most defense methods for other NLP tasks. Unfortunately, it does not encourage the model to learn well-separated feature representations. Thus, it is easy to induce adversarial examples. In this article, we propose a simple, yet efficient defense method from the geometric constraint perspective. Specifically, we first propose an M-similarity metric to shrink variances of intraclass features. Intuitively, better geometric conditions of feature space can bring lower misclassification probability (MP). Therefore, we derive the optimal geometric constraints of anchors within each category from the overall MP (OMP) with theoretical guarantees. Due to the nonconvex characteristic of the optimal geometric condition, it is hard to satisfy the traditional optimization process. To this end, we regard such geometric constraints as manifold optimization processes in the Stiefel manifold, thus naturally avoiding the above challenges. Experimental results demonstrate that our method can significantly improve robustness compared with baselines, while retaining the excellent performance on normal examples.},
  archive      = {J_TNNLS},
  author       = {Biqing Qi and Bowen Zhou and Weinan Zhang and Jianxing Liu and Ligang Wu},
  doi          = {10.1109/TNNLS.2023.3267460},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6133-6144},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving robustness of intent detection under adversarial attacks: A geometric constraint perspective},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perturbed progressive learning for semisupervised defect
segmentation. <em>TNNLS</em>, <em>35</em>(5), 6118–6132. (<a
href="https://doi.org/10.1109/TNNLS.2023.3324188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the development of intelligent manufacturing, the demand for surface defect inspection has been increasing. Deep learning has achieved promising results in defect inspection. However, due to the rareness of defect data and the difficulties of pixelwise annotation, the existing supervised defect inspection methods are too inferior to be implemented in practice. To solve the problem of defect segmentation with few labeled data, we propose a simple and efficient method for semisupervised defect segmentation (SSDS), named perturbed progressive learning (PPL). On the one hand, PPL decouples the predictions of student and teacher networks as well as alleviates overfitting on noisy pseudo-labels. On the other hand, PPL encourages consistency across various perturbations in a broader stagewise scope, alleviating drift caused by the noisy pseudo-labels. Specifically, PPL contains two training stages. In the first stage, the teacher network gives the unlabeled data with pseudo-labels that are divided into the easy and hard groups. The labeled data and the unlabeled data in the easy group with their perturbation are both used to train for a better-performing student network. In the second stage, the unlabeled data in the hard group are predicted by the obtained student network, so the refined pseudo-labeled data are enlarged. All the pseudo-labeling data and labeled data with their perturbation are used to retrain the student network, progressively improving the defect feature representation. We build a mobile screen defect dataset (MSDD-3) with three classes of defects. PPL is implemented on MSDD-3 as well as other public datasets. Extensive experimental results demonstrate that PPL significantly surpasses the state-of-the-art methods across all evaluation partition protocols.},
  archive      = {J_TNNLS},
  author       = {Yao Wu and Mingwei Xing and Yachao Zhang and Yuan Xie and Zongze Wu and Yanyun Qu},
  doi          = {10.1109/TNNLS.2023.3324188},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6118-6132},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Perturbed progressive learning for semisupervised defect segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep adaptive fuzzy clustering for evolutionary unsupervised
representation learning. <em>TNNLS</em>, <em>35</em>(5), 6103–6117. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster assignment of large and complex datasets is a crucial but challenging task in pattern recognition and computer vision. In this study, we explore the possibility of employing fuzzy clustering in a deep neural network framework. Thus, we present a novel evolutionary unsupervised learning representation model with iterative optimization. It implements the deep adaptive fuzzy clustering (DAFC) strategy that learns a convolutional neural network classifier from given only unlabeled data samples. DAFC consists of a deep feature quality-verifying model and a fuzzy clustering model, where deep feature representation learning loss function and embedded fuzzy clustering with the weighted adaptive entropy is implemented. We joint fuzzy clustering to the deep reconstruction model, in which fuzzy membership is utilized to represent a clear structure of deep cluster assignments and jointly optimize for the deep representation learning and clustering. Also, the joint model evaluates current clustering performance by inspecting whether the resampled data from estimated bottleneck space have consistent clustering properties to improve the deep clustering model progressively. Experiments on various datasets show that the proposed method obtains a substantially better performance for both reconstruction and clustering quality compared to the other state-of-the-art deep clustering methods, as demonstrated with the in-depth analysis in the extensive experiments.},
  archive      = {J_TNNLS},
  author       = {Dayu Tan and Zheng Huang and Xin Peng and Weimin Zhong and Vladimir Mahalec},
  doi          = {10.1109/TNNLS.2023.3243666},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6103-6117},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep adaptive fuzzy clustering for evolutionary unsupervised representation learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multitask-guided deep clustering with boundary adaptation.
<em>TNNLS</em>, <em>35</em>(5), 6089–6102. (<a
href="https://doi.org/10.1109/TNNLS.2023.3307126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask learning uses external knowledge to improve internal clustering and single-task learning. Existing multitask learning algorithms mostly use shallow-level correlation to aid judgment, and the boundary factors on high-dimensional datasets often lead algorithms to poor performance. The initial parameters of these algorithms cause the border samples to fall into a local optimal solution. In this study, a multitask-guided deep clustering (DC) with boundary adaptation (MTDC-BA) based on a convolutional neural network autoencoder (CNN-AE) is proposed. In the first stage, dubbed multitask pretraining (M-train), we construct an autoencoder (AE) named CNN-AE using the DenseNet-like structure, which performs deep feature extraction and stores captured multitask knowledge into model parameters. In the second phase, the parameters of the M-train are shared for CNN-AE, and clustering results are obtained by deep features, which is termed as single-task fitting (S-fit). To eliminate the boundary effect, we use data augmentation and improved self-paced learning to construct the boundary adaptation. We integrate boundary adaptors into the M-train and S-fit stages appropriately. The interpretability of MTDC-BA is accomplished by data transformation. The model relies on the principle that features become important as the reconfiguration loss decreases. Experiments on a series of typical datasets confirm the performance of the proposed MTDC-BA. Compared with other traditional clustering methods, including single-task DC algorithms and the latest multitask clustering algorithms, our MTDC-BA achieves better clustering performance with higher computational efficiency. Deep features clustering results demonstrate the stability of MTDC-BA by visualization and convergence verification. Through the visualization experiment, we explain and analyze the whole model data input and the middle characteristic layer. Further understanding of the principle of MTDC-BA. Through additional experiments, we know that the proposed MTDC-BA is efficient in the use of multitask knowledge. Finally, we carry out sensitivity experiments on the hyper-parameters to verify their optimal performance.},
  archive      = {J_TNNLS},
  author       = {Xiaobo Zhang and Tao Wang and Xiaole Zhao and Dengmin Wen and Donghai Zhai},
  doi          = {10.1109/TNNLS.2023.3307126},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6089-6102},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitask-guided deep clustering with boundary adaptation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VAE-based interpretable latent variable model for process
monitoring. <em>TNNLS</em>, <em>35</em>(5), 6075–6088. (<a
href="https://doi.org/10.1109/TNNLS.2023.3282047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent variable-based process monitoring (PM) models have been generously developed by shallow learning approaches, such as multivariate statistical analysis and kernel techniques. Owing to their explicit projection objectives, the extracted latent variables are usually meaningful and easily interpretable in mathematical terms. Recently, deep learning (DL) has been introduced to PM and has exhibited excellent performance because of its powerful presentation capability. However, its complex nonlinearity prevents it from being interpreted as human-friendly. It is a mystery how to design a proper network structure to achieve satisfactory PM performance for DL-based latent variable models (LVMs). In this article, a variational autoencoder-based interpretable LVM (VAE-ILVM) is developed for PM. Based on Taylor expansions, two propositions are proposed to guide the design of appropriate activation functions for VAE-ILVM, allowing nondisappearing fault impact terms contained in the generated monitoring metrics (MMs). During threshold learning, the sequence of counting that test statistics exceed the threshold is considered a martingale, a representative of weakly dependent stochastic processes. A de la Peña inequality is then adopted to learn a suitable threshold. Finally, two chemical examples verify the effectiveness of the proposed method. The use of de la Peña inequality significantly reduces the minimum required sample size for modeling.},
  archive      = {J_TNNLS},
  author       = {Zhuofu Pan and Yalin Wang and Yue Cao and Weihua Gui},
  doi          = {10.1109/TNNLS.2023.3282047},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6075-6088},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {VAE-based interpretable latent variable model for process monitoring},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer dynamic latent variable modeling for quality
prediction of multimode processes. <em>TNNLS</em>, <em>35</em>(5),
6061–6074. (<a
href="https://doi.org/10.1109/TNNLS.2023.3265762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality prediction is beneficial to intelligent inspection, advanced process control, operation optimization, and product quality improvements of complex industrial processes. Most of the existing work obeys the assumption that training samples and testing samples follow similar data distributions. The assumption is, however, not true for practical multimode processes with dynamics. In practice, traditional approaches mostly establish a prediction model using the samples from the principal operating mode (POM) with abundant samples. The model is inapplicable to other modes with a few samples. In view of this, this article will propose a novel dynamic latent variable (DLV)-based transfer learning approach, called transfer DLV regression (TDLVR), for quality prediction of multimode processes with dynamics. The proposed TDLVR can not only derive the dynamics between process variables and quality variables in the POM but also extract the co-dynamic variations among process variables between the POM and the new mode. This can effectively overcome data marginal distribution discrepancy and enrich the information of the new mode. To make full use of the available labeled samples from the new mode, an error compensation mechanism is incorporated into the established TDLVR, termed compensated TDLVR (CTDLVR), to adapt to the conditional distribution discrepancy. Empirical studies show the efficacy of the proposed TDLVR and CTDLVR methods in several case studies, including numerical simulation examples and two real-industrial process examples.},
  archive      = {J_TNNLS},
  author       = {Chao Yang and Qiang Liu and Yi Liu and Yiu-Ming Cheung},
  doi          = {10.1109/TNNLS.2023.3265762},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6061-6074},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transfer dynamic latent variable modeling for quality prediction of multimode processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online piecewise convex-optimization interpretable weight
learning for machine life cycle performance assessment. <em>TNNLS</em>,
<em>35</em>(5), 6048–6060. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine life cycle performance assessment is of great significance to use a health index to inform the time of incipient fault initiation in a normal stage and realize fault identification and fault trending in a performance degradation stage. However, most existing works consider using unexplainable model parameters and historical data to build models and infer their off-line parameters for machine life cycle performance assessment. To overcome these limitations, an online piecewise convex-optimization interpretable weight learning framework without needing any historical abnormal and faulty data is proposed in this article to generate a piecewise health index to practically implement machine life cycle performance assessment. Firstly, based on a separation criterion, the first submodel in the proposed framework is built to detect the time of incipient fault initiation. Here, the piecewise health index generated by the first submodel is continuously updated by on-line monitoring data to timely detect the occurrence of any abnormal health conditions. Secondly, once the time of incipient fault initiation is informed, online updated model weights are highly correlated with fault characteristic frequencies and informative frequency bands for immediate fault identification. Simultaneously, the second submodel integrated with monotonicity and fitness properties in the proposed framework is triggered to generate the piecewise health index to realize overall monotonic fault trending. The significance of this article is that only online monitoring data are used to continuously update interpretable model weights as fault frequencies and informative frequency bands to generate the proposed piecewise health index so as to practically realize machine life cycle performance assessment. Two run-to-failure cases are studied to show the effectiveness and superiority of the proposed framework.},
  archive      = {J_TNNLS},
  author       = {Tongtong Yan and Dong Wang and Tangbin Xia and Ershun Pan and Zhike Peng and Lifeng Xi},
  doi          = {10.1109/TNNLS.2022.3183123},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6048-6060},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online piecewise convex-optimization interpretable weight learning for machine life cycle performance assessment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust representation learning for power system short-term
voltage stability assessment under diverse data loss conditions.
<em>TNNLS</em>, <em>35</em>(5), 6035–6047. (<a
href="https://doi.org/10.1109/TNNLS.2023.3325542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the help of neural network-based representation learning, significant progress has been recently made in data-driven online dynamic stability assessment (DSA) of complex electric power systems. However, without sufficient attention to diverse data loss conditions in practice, the existing data-driven DSA solutions’ performance could be largely degraded due to practical defective input data. To address this problem, this work develops a robust representation learning approach to enhance DSA performance against multiple input data loss conditions in practice. Specifically, focusing on the short-term voltage stability (SVS) issue, an ensemble representation learning scheme (ERLS) is carefully designed to achieve data loss-tolerant online SVS assessment: 1) based on an efficient data masking technique, various missing data conditions are handled and augmented in a unified manner for lossy learning dataset preparation; 2) the emerging spatial–temporal graph convolutional network (STGCN) is leveraged to derive multiple diversified base learners with strong capability in SVS feature learning and representation; and 3) with massive SVS scenarios deeply grouped into a number of clusters, these STGCN-enabled base learners are distinctly assembled for each cluster via multilinear regression (MLR) to realize ensemble SVS assessment. Such a divide-and-conquer ensemble strategy results in highly robust SVS assessment performance when faced with various severe data loss conditions. Numerical tests on the benchmark Nordic test system illustrate the efficacy of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Lipeng Zhu and Weijia Wen and Yinpeng Qu and Feifan Shen and Jiayong Li and Yue Song and Tao Liu},
  doi          = {10.1109/TNNLS.2023.3325542},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6035-6047},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust representation learning for power system short-term voltage stability assessment under diverse data loss conditions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regional evaluation study of VFTO interference to secondary
side cables based on cloud model and MARCOS. <em>TNNLS</em>,
<em>35</em>(5), 6021–6034. (<a
href="https://doi.org/10.1109/TNNLS.2023.3325537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the data era, most power secondary side equipment tends to be digitized. The power system needs more accurate numerical results to further improve its operating efficiency. Therefore, it is important to study the electromagnetic interferences of very fast transient overvoltage (VFTO) generated by gas-insulated switchgear (GIS). To protect the secondary side cable from interferences, the secondary side cable is wrapped with an outer shield and the shield is grounded. When the interference of VFTO comes, it will couple the interference current and interference voltage on the shield of the cable. By grounding, the interference is greatly discharged. However, due to the grounding resistance, there will be a potential difference between the grounding points at the two ends of the shield of the cable. This causes a corresponding interference current to flow through the shield, which will affect the transmission of signals inside the cable. In the actual substation, the resistivity of the soil, the ambient temperature and humidity of the area, and so on will have impacts on the grounding resistance. In addition, the irregularity of the cable arrangement and the time of the use of the cable will have impacts on the signal transmission of the cable. Based on the abovementioned issues, this article proposed a comprehensive assessment method based on the combination of the cloud model and measurement of alternatives and ranking according to compromise solution (MARCOS). The method brings the cloud model into MARCOS by the algorithm of the contribution of the cloud droplets. It overcomes the difficulty of cloud model quantification. By comparing the results of the proposed method with the actual conditions at the substation and the results of the common MARCOS assessment method, the validity of the method is verified, and a reference scheme is provided for substation optimization.},
  archive      = {J_TNNLS},
  author       = {Yongji Wu and Wei Yan and Puliang Du and Xiaomin Gong and Mengxia Zhou},
  doi          = {10.1109/TNNLS.2023.3325537},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6021-6034},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Regional evaluation study of VFTO interference to secondary side cables based on cloud model and MARCOS},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial algorithm unrolling network for interpretable
mechanical anomaly detection. <em>TNNLS</em>, <em>35</em>(5), 6007–6020.
(<a href="https://doi.org/10.1109/TNNLS.2023.3250664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mechanical anomaly detection, algorithms with higher accuracy, such as those based on artificial neural networks, are frequently constructed as black boxes, resulting in opaque interpretability in architecture and low credibility in results. This article proposes an adversarial algorithm unrolling network (AAU-Net) for interpretable mechanical anomaly detection. AAU-Net is a generative adversarial network (GAN). Its generator, composed of an encoder and a decoder, is mainly produced by algorithm unrolling of a sparse coding model, which is specially designed for feature encoding and decoding of vibration signals. Thus, AAU-Net has a mechanism-driven and interpretable network architecture. In other words, it is ad hoc interpretable. Moreover, a multiscale feature visualization approach for AAU-Net is introduced to verify that meaningful features are encoded by AAU-Net, helping users to trust the detection results. The feature visualization approach enables the results of AAU-Net to be interpretable, i.e., post hoc interpretable. To verify AAU-Net’s capability of feature encoding and anomaly detection, we designed and performed simulations and experiments. The results show that AAU-Net can learn signal features that match the dynamic mechanism of the mechanical system. Considering the excellent feature learning ability, unsurprisingly, AAU-Net achieves the best overall anomaly detection performance compared with other algorithms.},
  archive      = {J_TNNLS},
  author       = {Botao An and Shibin Wang and Fuhua Qin and Zhibin Zhao and Ruqiang Yan and Xuefeng Chen},
  doi          = {10.1109/TNNLS.2023.3250664},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {6007-6020},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial algorithm unrolling network for interpretable mechanical anomaly detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-constraint variational neural network for wear state
assessment of external gear pump. <em>TNNLS</em>, <em>35</em>(5),
5996–6006. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current data-driven prognosis approaches suffer from their uncontrollable and unexplainable properties. To address this issue, this article proposes a physics-constraint variational neural network (PCVNN) for wear state assessment of the external gear pump. First, a response model of the pressure pulsation of the gear pump is constructed via a spectral method, and a compound neural network is utilized to extract features from the pressure pulsation signal. Then, the response model is formulated into an objective function to softly constrain the learning process of the neural network, forcing the learned features to have explicit physics meaning. Meanwhile, to characterize the system uncertainty, the variational inference is utilized to extend a Kullback–Leibler (KL) divergence into the objective function. Finally, the wear state is evaluated based on the distance of learned physics features. Experimental results on an external gear pump validate the merits of the proposed method in explainable representation learning and system uncertainty estimation. It also offers a controllable and explainable perspective to understand the dynamic behavior of the system.},
  archive      = {J_TNNLS},
  author       = {Wengang Xu and Zheng Zhou and Tianfu Li and Chuang Sun and Xuefeng Chen and Ruqiang Yan},
  doi          = {10.1109/TNNLS.2022.3213009},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5996-6006},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Physics-constraint variational neural network for wear state assessment of external gear pump},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neural networks for solving forward and
inverse problems in complex beam systems. <em>TNNLS</em>,
<em>35</em>(5), 5981–5995. (<a
href="https://doi.org/10.1109/TNNLS.2023.3310585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new framework using physics-informed neural networks (PINNs) to simulate complex structural systems that consist of single and double beams based on Euler–Bernoulli and Timoshenko theories, where the double beams are connected with a Winkler foundation. In particular, forward and inverse problems for the Euler–Bernoulli and Timoshenko partial differential equations (PDEs) are solved using nondimensional equations with the physics-informed loss function. Higher order complex beam PDEs are efficiently solved for forward problems to compute the transverse displacements and cross-sectional rotations with less than $1e-3$ % error. Furthermore, inverse problems are robustly solved to determine the unknown dimensionless model parameters and applied force in the entire space–time domain, even in the case of noisy data. The results suggest that PINNs are a promising strategy for solving problems in engineering structures and machines involving beam systems.},
  archive      = {J_TNNLS},
  author       = {Taniya Kapoor and Hongrui Wang and Alfredo Núnez and Rolf Dollevoet},
  doi          = {10.1109/TNNLS.2023.3310585},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5981-5995},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Physics-informed neural networks for solving forward and inverse problems in complex beam systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network design for impedance modeling of power
electronic systems based on latent features. <em>TNNLS</em>,
<em>35</em>(5), 5968–5980. (<a
href="https://doi.org/10.1109/TNNLS.2023.3235806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven approaches are promising to address the modeling issues of modern power electronics-based power systems, due to the black-box feature. Frequency-domain analysis has been applied to address the emerging small-signal oscillation issues caused by converter control interactions. However, the frequency-domain model of a power electronic system is linearized around a specific operating condition. It thus requires measurement or identification of frequency-domain models repeatedly at many operating points (OPs) due to the wide operation range of the power systems, which brings significant computation and data burden. This article addresses this challenge by developing a deep learning approach using multilayer feedforward neural networks (FNNs) to train the frequency-domain impedance model of power electronic systems that is continuous of OP. Distinguished from the prior neural network designs relying on trial-and-error and sufficient data size, this article proposes to design the FNN based on latent features of power electronic systems, i.e., the number of system poles and zeros. To further investigate the impacts of data quantity and quality, learning procedures from a small dataset are developed, and K-medoids clustering based on dynamic time warping is used to reveal insights into multivariable sensitivity, which helps improve the data quality. The proposed approaches for the FNN design and learning have been proven simple, effective, and optimal based on case studies on a power electronic converter, and future prospects in its industrial applications are also discussed.},
  archive      = {J_TNNLS},
  author       = {Yicheng Liao and Yufei Li and Minjie Chen and Lars Nordström and Xiongfei Wang and Prateek Mittal and H. Vincent Poor},
  doi          = {10.1109/TNNLS.2023.3235806},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5968-5980},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network design for impedance modeling of power electronic systems based on latent features},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive multiview representation learning via deep
autoencoder-like nonnegative matrix factorization. <em>TNNLS</em>,
<em>35</em>(5), 5953–5967. (<a
href="https://doi.org/10.1109/TNNLS.2023.3304626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a comprehensive representation from multiview data is crucial in many real-world applications. Multiview representation learning (MRL) based on nonnegative matrix factorization (NMF) has been widely adopted by projecting high-dimensional space into a lower order dimensional space with great interpretability. However, most prior NMF-based MRL techniques are shallow models that ignore hierarchical information. Although deep matrix factorization (DMF)-based methods have been proposed recently, most of them only focus on the consistency of multiple views and have cumbersome clustering steps. To address the above issues, in this article, we propose a novel model termed deep autoencoder-like NMF for MRL (DANMF-MRL), which obtains the representation matrix through the deep encoding stage and decodes it back to the original data. In this way, through a DANMF-based framework, we can simultaneously consider the multiview consistency and complementarity, allowing for a more comprehensive representation. We further propose a one-step DANMF-MRL, which learns the latent representation and final clustering labels matrix in a unified framework. In this approach, the two steps can negotiate with each other to fully exploit the latent clustering structure, avoid previous tedious clustering steps, and achieve optimal clustering performance. Furthermore, two efficient iterative optimization algorithms are developed to solve the proposed models both with theoretical convergence analysis. Extensive experiments on five benchmark datasets demonstrate the superiority of our approaches against other state-of-the-art MRL methods.},
  archive      = {J_TNNLS},
  author       = {Haonan Huang and Guoxu Zhou and Qibin Zhao and Lifang He and Shengli Xie},
  doi          = {10.1109/TNNLS.2023.3304626},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5953-5967},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Comprehensive multiview representation learning via deep autoencoder-like nonnegative matrix factorization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DLformer: A dynamic length transformer-based network for
efficient feature representation in remaining useful life prediction.
<em>TNNLS</em>, <em>35</em>(5), 5942–5952. (<a
href="https://doi.org/10.1109/TNNLS.2023.3257038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning-based remaining useful life (RUL) prediction plays a crucial role in improving the security and reducing the maintenance cost of complex systems. Despite the superior performance, the high computational cost of deep networks hinders deploying the models on low-compute platforms. A significant reason for the high cost is the computation of representing long sequences. In contrast to most RUL prediction methods that learn features of the same sequence length, we consider that each time series has its characteristics and the sequence length should be adjusted adaptively. Our motivation is that an “easy” sample with representative characteristics can be correctly predicted even when short feature representation is provided, while “hard” samples need complete feature representation. Therefore, we focus on sequence length and propose a dynamic length transformer (DLformer) that can adaptively learn sequence representation of different lengths. Then, a feature reuse mechanism is developed to utilize previously learned features to reduce redundant computation. Finally, in order to achieve dynamic feature representation, a particular confidence strategy is designed to calculate the confidence level for the prediction results. Regarding interpretability, the dynamic architecture can help human understand which part of the model is activated. Experiments on multiple datasets show that DLformer can increase up to 90% inference speed, with less than 5% degradation in model accuracy.},
  archive      = {J_TNNLS},
  author       = {Lei Ren and Haiteng Wang and Gao Huang},
  doi          = {10.1109/TNNLS.2023.3257038},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5942-5952},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DLformer: A dynamic length transformer-based network for efficient feature representation in remaining useful life prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Integrated optimal control for electrolyte temperature with
temporal causal network and reinforcement learning. <em>TNNLS</em>,
<em>35</em>(5), 5929–5941. (<a
href="https://doi.org/10.1109/TNNLS.2023.3278729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electrowinning process is a critical operation in nonferrous hydrometallurgy and consumes large quantities of power consumption. Current efficiency is an important process index related to power consumption, and it is vital to operate the electrolyte temperature close to the optimum point to ensure high current efficiency. However, the optimal control of electrolyte temperature faces the following challenges. First, the temporal causal relationship between process variables and current efficiency makes it difficult to estimate the current efficiency accurately and set the optimal electrolyte temperature. Second, the substantial fluctuation of influencing variables of electrolyte temperature leads to difficulty in maintaining the electrolyte temperature close to the optimum point. Third, due to the complex mechanism, building a dynamic electrowinning process model is intractable. Hence, it is a problem of index optimal control in the multivariable fluctuation scenario without process modeling. To get around this issue, an integrated optimal control method based on temporal causal network and reinforcement learning (RL) is proposed. First, the working conditions are divided and the temporal causal network is used to estimate current efficiency accurately to solve the optimal electrolyte temperature under multiple working conditions. Then, an RL controller is established under each working condition, and the optimal electrolyte temperature is placed into the controller’s reward function to assist in control strategy learning. An experiment case study of the zinc electrowinning process is provided to verify the effectiveness of the proposed method and to show that it can stabilize the electrolyte temperature within the optimal range without modeling.},
  archive      = {J_TNNLS},
  author       = {Tianhao Liu and Chunhua Yang and Can Zhou and Yonggang Li and Bei Sun},
  doi          = {10.1109/TNNLS.2023.3278729},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5929-5941},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Integrated optimal control for electrolyte temperature with temporal causal network and reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reinforcement learning-based pantograph control strategy
for improving current collection quality in high-speed railways.
<em>TNNLS</em>, <em>35</em>(5), 5915–5928. (<a
href="https://doi.org/10.1109/TNNLS.2022.3219814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-speed railways, the pantograph-catenary system (PCS) is a critical subsystem of the train power supply system. In particular, when the double-PCS (DPCS) is in operation, the passing of the leading pantograph (LP) causes the contact force of the trailing pantograph (TP) to fluctuate violently, affecting the power collection quality of the electric multiple units (EMUs). The actively controlled pantograph is the most promising technique for reducing the pantograph-catenary contact force (PCCF) fluctuation and improving the current collection quality. Based on the Nash equilibrium framework, this study proposes a multiagent reinforcement learning (MARL) algorithm for active pantograph control called cooperative proximity policy optimization (Coo-PPO). In the algorithm implementation, the heterogeneous agents play a unique role in a cooperative environment guided by the global value function. Then, a novel reward propagation channel is proposed to reveal implicit associations between agents. Furthermore, a curriculum learning approach is adopted to strike a balance between reward maximization and rational movement patterns. An existing MARL algorithm and a traditional control strategy are compared in the same scenario to validate the proposed control strategy’s performance. The experimental results show that the Coo-PPO algorithm obtains more rewards, significantly suppresses the fluctuation in PCCF (up to 41.55%), and dramatically decreases the TP’s offline rate (up to 10.77%). This study adopts MARL technology for the first time to address the coordinated control of double pantographs in DPCS.},
  archive      = {J_TNNLS},
  author       = {Hui Wang and Zhiwei Han and Wenqiang Liu and Yanbo Wu},
  doi          = {10.1109/TNNLS.2022.3219814},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5915-5928},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A reinforcement learning-based pantograph control strategy for improving current collection quality in high-speed railways},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated multiagent deep reinforcement learning approach
via physics-informed reward for multimicrogrid energy management.
<em>TNNLS</em>, <em>35</em>(5), 5902–5914. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of large-scale distributed renewable energy (RE) promotes the development of the multimicrogrid (MMG), which raises the need of developing an effective energy management method to minimize economic costs and keep self energy sufficiency. The multiagent deep reinforcement learning (MADRL) has been widely used for the energy management problem because of its real-time scheduling ability. However, its training requires massive energy operation data of microgrids (MGs), while gathering these data from different MGs would threaten their privacy and data security. Therefore, this article tackles this practical yet challenging issue by proposing a federated MADRL (F-MADRL) algorithm via the physics-informed reward. In this algorithm, the federated learning (FL) mechanism is introduced to train the F-MADRL algorithm, thus ensures the privacy and the security of data. In addition, a decentralized MMG model is built, and the energy of each participated MG is managed by an agent, which aims to minimize economic costs and keep self energy sufficiency according to the physics-informed reward. At first, MGs individually execute the self-training based on local energy operation data to train their local agent models. Then, these local models are periodically uploaded to a server and their parameters are aggregated to build a global agent, which will be broadcasted to MGs and replace their local agents. In this way, the experience of each MG agent can be shared and the energy operation data are not explicitly transmitted, thus protecting the privacy and ensuring data security. Finally, experiments are conducted on Oak Ridge National Laboratory distributed energy control communication laboratory MG (ORNL-MG) test system, and the comparisons are carried out to verify the effectiveness of introducing the FL mechanism and the outperformance of our proposed F-MADRL.},
  archive      = {J_TNNLS},
  author       = {Yuanzheng Li and Shangyang He and Yang Li and Yang Shi and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2022.3232630},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5902-5914},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Federated multiagent deep reinforcement learning approach via physics-informed reward for multimicrogrid energy management},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Submission to special issue to explainable representation
learning-based intelligent inspection and maintenance of complex
systems: Synchronization of inertial neural networks with unbounded
delays via sampled-data control. <em>TNNLS</em>, <em>35</em>(5),
5891–5901. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the synchronization issue for inertial neural networks (INNs) with heterogeneous time-varying delays and unbounded distributed delays, in which the state quantization is considered. First, by fully considering the delay and sampling time point information, a modified looped-functional is proposed for the synchronization error system. Compared with the existing Lyapunov–Krasovskii functional (LKF), the proposed functional contains the sawtooth structure term $\mathcal {V}_{8}(t)$ and the time-varying terms $e_{\mathit {x}}(t-\beta \hbar (t))$ and $e_{\mathit {y}}({t-\beta \hbar (t))}$ . Then, the obtained constraints may be further relaxed. Based on the functional and integral inequality, less conservative synchronization criteria are derived as the basis of controller design. In addition, the required quantized sampled-data controller is proposed by solving a set of linear matrix inequalities. Finally, two numerical examples are given to show the effectiveness and superiority of the proposed scheme in this article.},
  archive      = {J_TNNLS},
  author       = {Chao Ge and Xiaodong Liu and Yajuan Liu and Changchun Hua},
  doi          = {10.1109/TNNLS.2022.3222861},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5891-5901},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Submission to special issue to explainable representation learning-based intelligent inspection and maintenance of complex systems: Synchronization of inertial neural networks with unbounded delays via sampled-data control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new grey wolf optimizer tuned extended generalized
predictive control for distillation process. <em>TNNLS</em>,
<em>35</em>(5), 5880–5890. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distillation process plays an essential role in the petrochemical industry. However, the high-purity distillation column has complicated dynamic characteristics such as strong coupling and large time delay. To control the distillation column accurately, we proposed an extended generalized predictive control (EGPC) method inspired by the principles of extended state observer and proportional–integral-type generalized predictive control method; the proposed EGPC can adaptively compensate the system for the effects of coupling and model mismatch online and performs well in controlling time-delay systems. The strong coupling of the distillation column needs fast control, and the large time delay requires soft control. To balance the requirement for fast and soft control at the same time, a grey wolf optimizer with reverse learning and adaptive leaders number strategies (RAGWO) was proposed to tune the parameters of EGPC, and these strategies enable RAGWO to have a better initial population and improve its exploitation and exploration ability. The benchmark test results indicate that the RAGWO outperforms the existing optimizers for most of the selected benchmark functions. Extensive simulations show that the proposed method in terms of fluctuation and response time is superior to other methods for controlling the distillation process.},
  archive      = {J_TNNLS},
  author       = {Jia Ren and Zengqiang Chen and Yikang Yang and Zenghui Wang and Mingwei Sun and Qinglin Sun},
  doi          = {10.1109/TNNLS.2023.3262556},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5880-5890},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A new grey wolf optimizer tuned extended generalized predictive control for distillation process},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Disturbance observer-based adaptive intelligent control of
marine vessel with position and heading constraint condition related to
desired output. <em>TNNLS</em>, <em>35</em>(5), 5870–5879. (<a
href="https://doi.org/10.1109/TNNLS.2022.3141419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the adaptive control about the geodetic fixed positions and heading of three-degree-of-freedom dual-propeller vessel. During the navigation of a vessel at sea, due to the unpredictable sea, on the one hand, it is important to ensure that the vessel can smoothly follow the desired geodesic fixed position and heading; on the other hand, when the sailing environment is harsh, it is even more important that the vessel can adapt to the desired geodesic fixed position and heading that change at any time for safe driving. Therefore, this article selects the time-varying function related to the desired geodesic fixed position and heading as the constraint condition, and the constraint condition will change in real time as the expected position and heading change. The design of the control strategy is difficult, and the designed control strategy will be more suitable for complex maritime navigation conditions. First, the article constructs a log-type barrier Lyapunov function. Second, by introducing an unknown external disturbance observer, the external disturbances caused by the environment that may be encountered during the vessel’s voyage can be observed. Then, combined with the backstepping algorithm, a neural network (NN) control strategy and adaptive law are designed. Among them, for the uncertain function in the process of designing the control strategy, the NN is used to approximate it. Furthermore, through the Lyapunov stability analysis, it is shown that applying the designed control strategy to the vessel system in this article can ensure that the system is closed-loop stable. The final simulation experiment shows the effectiveness of the designed control strategy.},
  archive      = {J_TNNLS},
  author       = {Lei Liu and Zheng Li and Yang Chen and Rui Wang},
  doi          = {10.1109/TNNLS.2022.3141419},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5870-5879},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disturbance observer-based adaptive intelligent control of marine vessel with position and heading constraint condition related to desired output},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered fractional-order tracking control for an
uncertain nonlinear system with output saturation and disturbances.
<em>TNNLS</em>, <em>35</em>(5), 5857–5869. (<a
href="https://doi.org/10.1109/TNNLS.2022.3212281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an event-triggered (ET) fractional-order adaptive tracking control scheme (ATCS) is studied for the uncertain nonlinear system with the output saturation and the external disturbances by using the nonlinear disturbance observer (NDO) and the neural networks (NNs). Based on NNs, the system uncertainties are approximated. An NN-based NDO is designed to estimate the bounded disturbances. Combining the NNs, the output of the designed NDO, the fractional-order theory, and the ET mechanism, an ATCS is proposed under the output saturation. According to the stability analysis, all the closed-loop signals are semiglobally uniformly ultimately bounded based on the investigative ATCS. The simulation results and the comparative experiment verifications are shown to indicate the viability of the developed control scheme.},
  archive      = {J_TNNLS},
  author       = {Shuyi Shao and Mou Chen and Sijia Zheng and Shumin Lu and Qijun Zhao},
  doi          = {10.1109/TNNLS.2022.3212281},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5857-5869},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered fractional-order tracking control for an uncertain nonlinear system with output saturation and disturbances},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal spin polarization control for the spin-exchange
relaxation-free system using adaptive dynamic programming.
<em>TNNLS</em>, <em>35</em>(5), 5835–5847. (<a
href="https://doi.org/10.1109/TNNLS.2022.3230200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is the first to solve the 3-D spin polarization control (3DSPC) problem of atomic ensembles, which controls the spin polarization to achieve arbitrary states with the cooperation of multiphysics fields. First, a novel adaptive dynamic programming (ADP) structure is proposed based on the developed multicritic multiaction neural network (MCMANN) structure with nonquadratic performance functions, as a way to solve the multiplayer nonzero-sum game (MP-NZSG) problem in 3DSPC under the constraints of asymmetric saturation inputs. Then, we utilize the MCMANNs to implement the multicritic multiaction ADP (MCMA-ADP) algorithm, whose convergence is proven by the compression mapping principle. Finally, the MCMA-ADP is deployed in the spin-exchange relaxation-free (SERF) system to provide a set of control laws in 3DSPC that fully exploits the multiphysics fields to achieve arbitrary spin polarization states. Numerical simulations support the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Ruigang Wang and Zhuo Wang and Sixun Liu and Tao Li and Feng Li and Bodong Qin and Qinglai Wei},
  doi          = {10.1109/TNNLS.2022.3230200},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5835-5847},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal spin polarization control for the spin-exchange relaxation-free system using adaptive dynamic programming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel automatic generation control method based on the
large-scale electric vehicles and wind power integration into the grid.
<em>TNNLS</em>, <em>35</em>(5), 5824–5834. (<a
href="https://doi.org/10.1109/TNNLS.2022.3194247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem of frequency instability of power system due to strong random disturbance caused by large-scale electric vehicles and wind power grid connection, an improved reinforcement learning algorithm, namely, optimistic initialized double Q, is proposed in this article from the perspective of automatic generation control. The proposed algorithm uses the optimistic initialization principle to expand the agent action exploration space, so as to prevent Q-learning from falling into local optimum by greedy strategy; meanwhile, it integrates double Q-learning to solve the problem of overestimation of action value in traditional reinforcement learning based on Q-learning. In the algorithm, the hyperparameter $\alpha _{\tau }$ is introduced to improve the learning efficiency, and the reward $b_{\tau }$ based on exploration times is introduced to increase the $Q$ value estimation to drive the exploration of the algorithm, so as to obtain the optimal solution. By simulating the two-area load frequency control model integrated with large-scale electric vehicles and the four-area interconnected power grid model integrated with large-scale wind power generation, it is verified that the proposed algorithm can obtain the global optimal solution, thus effectively solvinng the frequency instability caused by strong random disturbance in the grid-connected mode of large-scale wind power generation, and compared with many reinforcement learning algorithms, the proposed algorithm has better control performance.},
  archive      = {J_TNNLS},
  author       = {Lei Xi and Haokai Li and Jizhong Zhu and Yanying Li and Shouxiang Wang},
  doi          = {10.1109/TNNLS.2022.3194247},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5824-5834},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel automatic generation control method based on the large-scale electric vehicles and wind power integration into the grid},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Special issue on explainable representation
learning-based intelligent inspection and maintenance of complex
systems. <em>TNNLS</em>, <em>35</em>(5), 5819–5823. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, representation learning has received particular attention in the intelligent inspection and maintenance of complex systems thanks to its overwhelming advantages in discovering and mining hidden knowledge representations. The room for in-depth investigations of representation learning-related topics remains open, especially explainable approaches for intelligent inspection and maintenance of complex systems. The primary objective of this special issue, entitled “Explainable Representation Learning-based Intelligent Inspection and Maintenance of Complex Systems,” of IEEE Transactions on Neural Networks and Learning Systems is to provide the related latest achievements made by researchers and practitioners on the one hand and to identify critical issues and challenges for future investigation on the other hand.},
  archive      = {J_TNNLS},
  author       = {Zhigang Liu and Cesare Alippi and Hongtian Chen and Derong Liu},
  doi          = {10.1109/TNNLS.2024.3371593},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {5},
  number       = {5},
  pages        = {5819-5823},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial: Special issue on explainable representation learning-based intelligent inspection and maintenance of complex systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Balancing transferability and discriminability for
unsupervised domain adaptation. <em>TNNLS</em>, <em>35</em>(4),
5807–5814. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to leverage a sufficiently labeled source domain to classify or represent the fully unlabeled target domain with a different distribution. Generally, the existing approaches try to learn a domain-invariant representation for feature transferability and add class discriminability constraints for feature discriminability. However, the feature transferability and discriminability are usually not synchronized, and there are even some contradictions between them, which is often ignored and, thus, reduces the accuracy of recognition. In this brief, we propose a deep multirepresentations adversarial learning (DMAL) method to explore and mitigate the inconsistency between feature transferability and discriminability in UDA task. Specifically, we consider feature representation learning at both the domain level and class level and explore four types of feature representations: domain-invariant, domain-specific, class-invariant, and class-specific. The first two types indicate the transferability of features, and the last two indicate the discriminability. We develop an adversarial learning strategy between the four representations to make the feature transferability and discriminability to be gradually synchronized. A series of experimental results verify that the proposed DMAL achieves comparable and promising results on six UDA datasets.},
  archive      = {J_TNNLS},
  author       = {Jingke Huang and Ni Xiao and Lei Zhang},
  doi          = {10.1109/TNNLS.2022.3201623},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5807-5814},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Balancing transferability and discriminability for unsupervised domain adaptation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A jump-gain integral recurrent neural network for solving
noise-disturbed time-variant nonlinear inequality problems.
<em>TNNLS</em>, <em>35</em>(4), 5793–5806. (<a
href="https://doi.org/10.1109/TNNLS.2023.3241207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear inequalities are widely used in science and engineering areas, attracting the attention of many researchers. In this article, a novel jump-gain integral recurrent (JGIR) neural network is proposed to solve noise-disturbed time-variant nonlinear inequality problems. To do so, an integral error function is first designed. Then, a neural dynamic method is adopted and the corresponding dynamic differential equation is obtained. Third, a jump gain is exploited and applied to the dynamic differential equation. Fourth, the derivatives of errors are substituted into the jump-gain dynamic differential equation, and the corresponding JGIR neural network is set up. Global convergence and robustness theorems are proposed and proved theoretically. Computer simulations verify that the proposed JGIR neural network can solve noise-disturbed time-variant nonlinear inequality problems effectively. Compared with some advanced methods, such as modified zeroing neural network (ZNN), noise-tolerant ZNN, and varying-parameter convergent-differential neural network, the proposed JGIR method has smaller computational errors, faster convergence speed, and no overshoot when disturbance exists. In addition, physical experiments on manipulator control have verified the effectiveness and superiority of the proposed JGIR neural network.},
  archive      = {J_TNNLS},
  author       = {Zhijun Zhang and Yating Song and Lunan Zheng and Yamei Luo},
  doi          = {10.1109/TNNLS.2023.3241207},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5793-5806},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A jump-gain integral recurrent neural network for solving noise-disturbed time-variant nonlinear inequality problems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and experimental validation of deep reinforcement
learning-based fast trajectory planning and control for mobile robot in
unknown environment. <em>TNNLS</em>, <em>35</em>(4), 5778–5792. (<a
href="https://doi.org/10.1109/TNNLS.2022.3209154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the problem of planning optimal maneuver trajectories and guiding the mobile robot toward target positions in uncertain environments for exploration purposes. A hierarchical deep learning-based control framework is proposed which consists of an upper level motion planning layer and a lower level waypoint tracking layer. In the motion planning phase, a recurrent deep neural network (RDNN)-based algorithm is adopted to predict the optimal maneuver profiles for the mobile robot. This approach is built upon a recently proposed idea of using deep neural networks (DNNs) to approximate the optimal motion trajectories, which has been validated that a fast approximation performance can be achieved. To further enhance the network prediction performance, a recurrent network model capable of fully exploiting the inherent relationship between preoptimized system state and control pairs is advocated. In the lower level, a deep reinforcement learning (DRL)-based collision-free control algorithm is established to achieve the waypoint tracking task in an uncertain environment (e.g., the existence of unexpected obstacles). Since this approach allows the control policy to directly learn from human demonstration data, the time required by the training process can be significantly reduced. Moreover, a noisy prioritized experience replay (PER) algorithm is proposed to improve the exploring rate of control policy. The effectiveness of applying the proposed deep learning-based control is validated by executing a number of simulation and experimental case studies. The simulation result shows that the proposed DRL method outperforms the vanilla PER algorithm in terms of training speed. Experimental videos are also uploaded, and the corresponding results confirm that the proposed strategy is able to fulfill the autonomous exploration mission with improved motion planning performance, enhanced collision avoidance ability, and less training time.},
  archive      = {J_TNNLS},
  author       = {Runqi Chai and Hanlin Niu and Joaquin Carrasco and Farshad Arvin and Hujun Yin and Barry Lennox},
  doi          = {10.1109/TNNLS.2022.3209154},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5778-5792},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design and experimental validation of deep reinforcement learning-based fast trajectory planning and control for mobile robot in unknown environment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential fusion estimation for multirate complex networks
with uniform quantization: A zonotopic set-membership approach.
<em>TNNLS</em>, <em>35</em>(4), 5764–5777. (<a
href="https://doi.org/10.1109/TNNLS.2022.3209135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the sequential fusion estimation problem is investigated for multirate complex networks (MRCNs) with uniformly quantized measurements. The process and measurement noises, which are unknown-yet-bounded (UYB), are restrained into a family of zonotopes, and the multiple sensors are allowed to have different sampling periods. To facilitate digital transmissions, the sensor measurements are uniformly quantized before being sent to the remote estimator. The purpose of this article is to design a sequential set-membership estimator such that, in the simultaneous presence of UYB noises, multirate samplings, and uniform quantization effects, the estimation error (after each measurement update) is confined to a zonotope with minimum $F$ -radius at each time instant. By introducing certain virtual measurements, the MRCNs are first transformed into single-rate ones exhibiting a switching phenomenon. Then, by utilizing the properties of zonotopes, the desired zonotopes are derived, which contain the estimation error dynamics after each measurement update. Subsequently, the gain matrices of the sequential estimator are derived by minimizing the $F$ -radii of these zonotopes, and the uniform boundedness is analyzed for the $F$ -radius of the zonotope containing the estimation error after all measurement updates. Furthermore, sufficient conditions are derived to ensure the existence of the desired uniform upper/lower bounds. Finally, an illustrated example is proposed to show the effectiveness of the proposed sequential fusion estimation method.},
  archive      = {J_TNNLS},
  author       = {Zhongyi Zhao and Zidong Wang and Lei Zou},
  doi          = {10.1109/TNNLS.2022.3209135},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5764-5777},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sequential fusion estimation for multirate complex networks with uniform quantization: A zonotopic set-membership approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised fusion feature matching for data bias in
uncertainty active learning. <em>TNNLS</em>, <em>35</em>(4), 5749–5763.
(<a href="https://doi.org/10.1109/TNNLS.2022.3209085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning (AL) aims to sample the most valuable data for model improvement from the unlabeled pool. Traditional works, especially uncertainty-based methods, are prone to suffer from a data bias issue, which means that selected data cannot cover the entire unlabeled pool well. Although there have been lots of literature works focusing on this issue recently, they mainly benefit from the huge additional training costs and the artificially designed complex loss. The latter causes these methods to be redesigned when facing new models or tasks, which is very time-consuming and laborious. This article proposes a feature-matching-based uncertainty that resamples selected uncertainty data by feature matching, thus removing similar data to alleviate the data bias issue. To ensure that our proposed method does not introduce a lot of additional costs, we specially design a unsupervised fusion feature matching (UFFM), which does not require any training in our novel AL framework. Besides, we also redesign several classic uncertainty methods to be applied to more complex visual tasks. We conduct rigorous experiments on lots of standard benchmark datasets to validate our work. The experimental results show that our UFFM is better than the similar unsupervised feature matching technologies, and our proposed uncertainty calculation method outperforms random sampling, classic uncertainty approaches, and recent state-of-the-art (SOTA) uncertainty approaches.},
  archive      = {J_TNNLS},
  author       = {Wei Huang and Shuzhou Sun and Xiao Lin and Ping Li and Lei Zhu and Jihong Wang and C. L. Philip Chen and Bin Sheng},
  doi          = {10.1109/TNNLS.2022.3209085},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5749-5763},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised fusion feature matching for data bias in uncertainty active learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning transactional behavioral representations for credit
card fraud detection. <em>TNNLS</em>, <em>35</em>(4), 5735–5748. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit card fraud detection is a challenging task since fraudulent actions are hidden in massive legitimate behaviors. This work aims to learn a new representation for each transaction record based on the historical transactions of users in order to capture fraudulent patterns accurately and, thus, automatically detect a fraudulent transaction. We propose a novel model by improving long short-term memory with a time-aware gate that can capture the behavioral changes caused by consecutive transactions of users. A current-historical attention module is designed to build up connections between current and historical transactional behaviors, which enables the model to capture behavioral periodicity. An interaction module is designed to learn comprehensive and rational behavioral representations. To validate the effectiveness of the learned behavioral representations, experiments are conducted on a large real-world transaction dataset provided to us by a financial company in China, as well as a public dataset. Experimental results and the visualization of the learned representations illustrate that our method delivers a clear distinction between legitimate behaviors and fraudulent ones, and achieves better fraud detection performance compared with the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yu Xie and Guanjun Liu and Chungang Yan and Changjun Jiang and Mengchu Zhou and Maozhen Li},
  doi          = {10.1109/TNNLS.2022.3208967},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5735-5748},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning transactional behavioral representations for credit card fraud detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Fast multilabel feature selection via global relevance and
redundancy optimization. <em>TNNLS</em>, <em>35</em>(4), 5721–5734. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information theoretical-based methods have attracted a great attention in recent years and gained promising results for multilabel feature selection (MLFS). Nevertheless, most of the existing methods consider a heuristic way to the grid search of important features, and they may also suffer from the issue of fully utilizing labeling information. Thus, they are probable to deliver a suboptimal result with heavy computational burden. In this article, we propose a general optimization framework global relevance and redundancy optimization (GRRO) to solve the learning problem. The main technical contribution in GRRO is a formulation for MLFS while feature relevance, label relevance (i.e., label correlation), and feature redundancy are taken into account, which can avoid repetitive entropy calculations to obtain a global optimal solution efficiently. To further improve the efficiency, we extend GRRO to filter out inessential labels and features, thus facilitating fast MLFS. We call the extension as GRROfast, in which the key insights are twofold: 1) promising labels and related relevant features are investigated to reduce ineffective calculations in terms of features, even labels and 2) the framework of GRRO is reconstructed to generate the optimal result with an ensemble. Moreover, our proposed algorithms have an excellent mechanism for exploiting the inherent properties of multilabel data; specifically, we provide a formulation to enhance the proposal with label-specific features. Extensive experiments clearly reveal the effectiveness and efficiency of our proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Jia Zhang and Yidong Lin and Min Jiang and Shaozi Li and Yong Tang and Jinyi Long and Jian Weng and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2022.3208956},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5721-5734},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast multilabel feature selection via global relevance and redundancy optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the equivalence of linear discriminant analysis and least
squares regression. <em>TNNLS</em>, <em>35</em>(4), 5710–5720. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying the relationship between linear discriminant analysis (LDA) and least squares regression (LSR) is of great theoretical and practical significance. It is well-known that the two-class LDA is equivalent to an LSR problem, and directly casting multiclass LDA as an LSR problem, however, becomes more challenging. Recent study reveals that the equivalence between multiclass LDA and LSR can be established based on a special class indicator matrix, but under a mild condition which may not hold under the scenarios with low-dimensional or oversampled data. In this article, we show that the equivalence between multiclass LDA and LSR can be established based on arbitrary linearly independent class indicator vectors and without any condition. In addition, we show that LDA is also equivalent to a constrained LSR based on the data-dependent indicator vectors. It can be concluded that under exactly the same mild condition, such two regressions are both equivalent to the null space LDA method. Illuminated by the equivalence of LDA and LSR, we propose a direct LDA classifier to replace the conventional framework of LDA plus extra classifier. Extensive experiments well validate the above theoretic analysis.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Hong Chen and Shiming Xiang and Changshui Zhang and Shuicheng Yan and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3208944},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5710-5720},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the equivalence of linear discriminant analysis and least squares regression},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reinforcement learning approach for flexible job shop
scheduling problem with crane transportation and setup times.
<em>TNNLS</em>, <em>35</em>(4), 5695–5709. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flexible job shop scheduling problem (FJSP) has attracted research interests as it can significantly improve the energy, cost, and time efficiency of production. As one type of reinforcement learning, deep Q-network (DQN) has been applied to solve numerous realistic optimization problems. In this study, a DQN model is proposed to solve a multiobjective FJSP with crane transportation and setup times (FJSP-CS). Two objectives, i.e., makespan and total energy consumption, are optimized simultaneously based on weighting approach. To better reflect the problem realities, eight different crane transportation stages and three typical machine states including processing, setup, and standby are investigated. Considering the complexity of FJSP-CS, an identification rule is designed to organize the crane transportation in solution decoding. As for the DQN model, 12 state features and seven actions are designed to describe the features in the scheduling process. A novel structure is applied in the DQN topology, saving the calculation resources and improving the performance. In DQN training, double deep Q-network technique and soft target weight update strategy are used. In addition, three reported improvement strategies are adopted to enhance the solution qualities by adjusting scheduling assignments. Extensive computational tests and comparisons demonstrate the effectiveness and advantages of the proposed method in solving FJSP-CS, where the DQN can choose appropriate dispatching rules at various scheduling situations.},
  archive      = {J_TNNLS},
  author       = {Yu Du and Junqing Li and Chengdong Li and Peiyong Duan},
  doi          = {10.1109/TNNLS.2022.3208942},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5695-5709},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A reinforcement learning approach for flexible job shop scheduling problem with crane transportation and setup times},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WalkGAN: Network representation learning with sequence-based
generative adversarial networks. <em>TNNLS</em>, <em>35</em>(4),
5684–5694. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network representation learning, also known as network embedding, aims to learn the low-dimensional representations of vertices while capturing and preserving the network structure. For real-world networks, the edges that represent some important relationships between the vertices of a network may be missed and may result in degenerated performance. The existing methods usually treat missing edges as negative samples, thereby ignoring the true connections between two vertices in a network. To capture the true network structure effectively, we propose a novel network representation learning method called WalkGAN, where random walk scheme and generative adversarial networks (GAN) are incorporated into a network embedding framework. Specifically, WalkGAN leverages GAN to generate the synthetic sequences of the vertices that sufficiently simulate random walk on a network and further learn vertex representations from these vertex sequences. Thus, the unobserved links between the vertices are inferred with high probability instead of treating them as nonexistence. Experimental results on the benchmark network datasets demonstrate that WalkGAN achieves significant performance improvements for vertex classification, link prediction, and visualization tasks.},
  archive      = {J_TNNLS},
  author       = {Taisong Jin and Xixi Yang and Zhengtao Yu and Han Luo and Yongmei Zhang and Feiran Jie and Xiangxiang Zeng and Min Jiang},
  doi          = {10.1109/TNNLS.2022.3208914},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5684-5694},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {WalkGAN: Network representation learning with sequence-based generative adversarial networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymmetric constrained optimal tracking control with critic
learning of nonlinear multiplayer zero-sum games. <em>TNNLS</em>,
<em>35</em>(4), 5671–5683. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By utilizing a neural-network-based adaptive critic mechanism, the optimal tracking control problem is investigated for nonlinear continuous-time (CT) multiplayer zero-sum games (ZSGs) with asymmetric constraints. Initially, we build an augmented system with the tracking error system and the reference system. Moreover, a novel nonquadratic function is introduced to address asymmetric constraints. Then, we derive the tracking Hamilton–Jacobi–Isaacs (HJI) equation of the constrained nonlinear multiplayer ZSG. However, it is extremely hard to get the analytical solution to the HJI equation. Hence, an adaptive critic mechanism based on neural networks is established to estimate the optimal cost function, so as to obtain the near-optimal control policy set and the near worst disturbance policy set. In the process of neural critic learning, we only utilize one critic neural network and develop a new weight updating rule. After that, by using the Lyapunov approach, the uniform ultimate boundedness stability of the tracking error in the augmented system and the weight estimation error of the critic network is verified. Finally, two simulation examples are provided to demonstrate the efficacy of the established mechanism.},
  archive      = {J_TNNLS},
  author       = {Junfei Qiao and Menghua Li and Ding Wang},
  doi          = {10.1109/TNNLS.2022.3208611},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5671-5683},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asymmetric constrained optimal tracking control with critic learning of nonlinear multiplayer zero-sum games},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning dynamic compact memory embedding for deformable
visual object tracking. <em>TNNLS</em>, <em>35</em>(4), 5656–5670. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, template-based trackers have become the leading tracking algorithms with promising performance in terms of efficiency and accuracy. However, the correlation operation between query feature and the given template only achieves accurate target localization, but is prone to state estimation error, especially when the target suffers from severe deformation. To address this issue, segmentation-based trackers are proposed that use per-pixel matching to improve the tracking performance of deformable objects effectively. However, most of the existing trackers only match with the target features of the initial frame, thereby lacking the discrimination for handling a variety of challenging factors, e.g., similar distractors, background clutter, and appearance change. To this end, we propose a dynamic compact memory embedding technique to enhance the discrimination of the segmentation-based visual tracking method that can well tell the target from the background. Specifically, we initialize a memory embedding with the target features in the first frame. During the tracking process, the current target features that have certain correlation with the existing memory are updated to the memory embedding online. To further improve the tracking accuracy for deformable objects, we use a weighted point-to-global matching strategy to measure the correlation between the pixelwise query feature and the whole template, so as to capture more detailed deformation information. Extensive evaluations on six challenging tracking benchmarks including VOT2016, VOT2018, VOT2019, GOT-10K, TrackingNet, and LaSOT demonstrate the superiority of our method over recent remarkable trackers. Besides, our tracker outperforms the excellent segmentation-based trackers, i.e., D3S and SiamMask on the DAVIS2017 benchmark. The code is available at https://github.com/peace-love243/CMEDFL .},
  archive      = {J_TNNLS},
  author       = {Hongtao Yu and Pengfei Zhu and Kaihua Zhang and Yu Wang and Shuai Zhao and Lei Wang and Tianzhu Zhang and Qinghua Hu},
  doi          = {10.1109/TNNLS.2022.3208605},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5656-5670},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning dynamic compact memory embedding for deformable visual object tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy graph subspace convolutional network. <em>TNNLS</em>,
<em>35</em>(4), 5641–5655. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) are a popular approach to learn the feature embedding of graph-structured data, which has shown to be highly effective as well as efficient in performing node classification in an inductive way. However, with massive nongraph-organized data existing in application scenarios nowadays, it is critical to exploit the relationships behind the given groups of data, which makes better use of GCN and broadens the application field. In this article, we propose the f uzzy g raph s ubspace c onvolutional n etwork (FGSCN) to provide a brand-new paradigm for feature embedding and node classification with graph convolution (GC) when given an arbitrary collection of data. The FGSCN performs GC on the f uzzy s ubspace ( $\mathcal {F}$ -space), which simultaneously learns from the underlying subspace information in the low-dimensional space as well as its neighborliness information in the high-dimensional space. In particular, we construct the fuzzy homogenous graph $\mathcal {G}_{\mathcal {F}}$ on the $\mathcal {F}$ -space by fusing the homogenous graph of neighborliness $\mathcal {G}_{\mathcal {N}}$ and homogenous graph of subspace $\mathcal {G}_{\mathcal {S}}$ (defined by the affinity matrix of the low-rank representation). Here, it is proven that the GC on $\mathcal {F}$ -space will propagate both the local and global information through fuzzy set theory. We evaluated FGSCN on 15 unique datasets with different tasks (e.g., feature embedding, visual recognition, etc.). The experimental results showed that the proposed FGSCN has significant superiority compared with current state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Jianhang Zhou and Qi Zhang and Shaoning Zeng and Bob Zhang},
  doi          = {10.1109/TNNLS.2022.3208557},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5641-5655},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fuzzy graph subspace convolutional network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M3W: Multistep three-way clustering. <em>TNNLS</em>,
<em>35</em>(4), 5627–5640. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way clustering has been an active research topic in the field of cluster analysis in recent years. Some efforts are focused on the technique due to its feasibility and rationality. We observe, however, that the existing three-way clustering algorithms struggle to obtain more information and limit the fault tolerance excessively. Moreover, although the one-step three-way allocation based on a pair of fixed, global thresholds is the most straightforward way to generate the three-way cluster representations, the clusters derived from a pair of global thresholds cannot exactly reveal the inherent clustering structure of the dataset, and the threshold values are often difficult to determine beforehand. Inspired by sequential three-way decisions, we propose an algorithm, called multistep three-way clustering (M3W), to address these issues. Specifically, we first use a progressive erosion strategy to construct a multilevel structure of data, so that lower levels (or external layers) can gather more available information from higher levels (or internal layers). Then, we further propose a multistep three-way allocation strategy, which sufficiently considers the neighborhood information of every eroded instance. We use the allocation strategy in combination with the multilevel structure to ensure that more information is gradually obtained to increase the probability of being assigned correctly, capturing adaptively the inherent clustering structure of the dataset. The proposed algorithm is compared with eight competitors using 18 benchmark datasets. Experimental results show that M3W achieves superior performance, verifying its advantages and effectiveness.},
  archive      = {J_TNNLS},
  author       = {Mingjing Du and Jingqi Zhao and Jiarui Sun and Yongquan Dong},
  doi          = {10.1109/TNNLS.2022.3208418},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5627-5640},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {M3W: Multistep three-way clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Learning complementary correlations for depth
super-resolution with incomplete data in real world. <em>TNNLS</em>,
<em>35</em>(4), 5616–5626. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth information is a significant ingredient to visually perceive the physical world. However, mainstream depth sensors, e.g., time-of-flight (ToF) cameras, often measure incomplete and low-resolution depth data, resulting in low-quality visual perception. In this article, we try to address a potentially valuable task, i.e., depth super-resolution (DSR) with incomplete data, which recovers dense and high-resolution depth map from incomplete and low-resolution one. To tackle this task, we introduce a novel incomplete DSR (IDSR) framework, including a primary branch for DSR to recover high-frequency details, and an auxiliary branch for depth completion (DC) to fill missing pixels. More importantly, we propose two modules, joint correlation learning (JCL) and iterative-cross (IC), to enhance the learning of complementary information flows between the two branches. The former module aims to learn the correlative relationships of the two branches, whilst the latter module adequately fuses higher level representations for more precise predictions. Extensive experiments show that our framework is effective and achieves the state-of-the-art performance on the real-world RGB-D-D and the synthetic NYUv2 datasets.},
  archive      = {J_TNNLS},
  author       = {Zhiqiang Yan and Kun Wang and Xiang Li and Zhenyu Zhang and Guangyu Li and Jun Li and Jian Yang},
  doi          = {10.1109/TNNLS.2022.3208330},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5616-5626},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning complementary correlations for depth super-resolution with incomplete data in real world},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The dilemma of quantum neural networks. <em>TNNLS</em>,
<em>35</em>(4), 5603–5615. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core of quantum machine learning is to devise quantum models with good trainability and low generalization error bounds than their classical counterparts to ensure better reliability and interpretability. Recent studies confirmed that quantum neural networks (QNNs) have the ability to achieve this goal on specific datasets. In this regard, it is of great importance to understand whether these advantages are still preserved on real-world tasks. Through systematic numerical experiments, we empirically observe that current QNNs fail to provide any benefit over classical learning models. Concretely, our results deliver two key messages. First, QNNs suffer from the severely limited effective model capacity, which incurs poor generalization on real-world datasets. Second, the trainability of QNNs is insensitive to regularization techniques, which sharply contrasts with the classical scenario. These empirical results force us to rethink the role of current QNNs and to design novel protocols for solving real-world problems with quantum advantages.},
  archive      = {J_TNNLS},
  author       = {Yang Qian and Xinbiao Wang and Yuxuan Du and Xingyao Wu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2022.3208313},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5603-5615},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The dilemma of quantum neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CTV-net: Complex-valued TV-driven network with nested
topology for 3-d SAR imaging. <em>TNNLS</em>, <em>35</em>(4), 5588–5602.
(<a href="https://doi.org/10.1109/TNNLS.2022.3208252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The regularization-based approaches offer promise in improving synthetic aperture radar (SAR) imaging quality while reducing system complexity. However, the widely applied $\ell _{1}$ regularization model is hindered by their hypothesis of inherent sparsity, causing unreal estimations of surface-like targets. Inspired by the edge-preserving property of total variation (TV), we propose a new complex-valued TV (CTV)-driven interpretable neural network with nested topology, i.e., CTV-Net, for 3-D SAR imaging. In our scheme, based on the 2-D holography imaging operator, the CTV-driven optimization model is constructed to pursue precise estimations in weakly sparse scenarios. Subsequently, a nested algorithmic framework, i.e., complex-valued TV-driven fast iterative shrinkage thresholding (CTV-FIST), is derived from the theory of proximal gradient descent (PGD) and FIST algorithm, theoretically supporting the design of CTV-Net. In CTV-Net, the trainable weights are layer-varied and functionally relevant to the hyperparameters of CTV-FIST, which aims to constrain the algorithmic parameters to update in a well-conditioned tendency. All weights are learned by end-to-end training based on a two-term cost function, which bounds the measurement fidelity and TV norm simultaneously. Under the guidance of the SAR signal model, a reasonably sized training set is generated, by randomly selecting reference images from the MNIST set and consequently synthesizing complex-valued label signals. Finally, the methodology is validated, numerically and visually, by extensive SAR simulations and real-measured experiments, and the results demonstrate the viability and efficiency of the proposed CTV-Net in the cases of recovering 3-D SAR images from incomplete echoes.},
  archive      = {J_TNNLS},
  author       = {Mou Wang and Shunjun Wei and Zichen Zhou and Jun Shi and Xiaoling Zhang and Yongxin Guo},
  doi          = {10.1109/TNNLS.2022.3208252},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5588-5602},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CTV-net: Complex-valued TV-driven network with nested topology for 3-D SAR imaging},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal tracking control of heterogeneous MASs using
event-driven adaptive observer and reinforcement learning.
<em>TNNLS</em>, <em>35</em>(4), 5577–5587. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the output tracking control problem of nonidentical linear multiagent systems (MASs) using a model-free reinforcement learning (RL) algorithm, where partial followers have no prior knowledge of the leader’s information. To lower the communication and computing burden among agents, an event-driven adaptive distributed observer is proposed to predict the leader’s system matrix and state, which consists of the estimated value of relative states governed by an edge-based predictor. Meanwhile, the integral input-based triggering condition is exploited to decide whether to transmit its private control input to its neighbors. Then, an RL-based state feedback controller for each agent is developed to solve the output tracking control problem, which is further converted into the optimal control problem by introducing a discounted performance function. Inhomogeneous algebraic Riccati equations (AREs) are derived to obtain the optimal solution of AREs. An off-policy RL algorithm is used to learn the solution of inhomogeneous AREs online without requiring any knowledge of the system dynamics. Rigorous analysis shows that under the proposed event-driven adaptive observer mechanism and RL algorithm, all followers are able to synchronize the leader’s output asymptotically. Finally, a numerical simulation is demonstrated to verify the proposed approach in theory.},
  archive      = {J_TNNLS},
  author       = {Yong Xu and Jian Sun and Ya-Jun Pan and Zheng-Guang Wu},
  doi          = {10.1109/TNNLS.2022.3208237},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5577-5587},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal tracking control of heterogeneous MASs using event-driven adaptive observer and reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grouped spherical data modeling through hierarchical
nonparametric bayesian models and its application to fMRI data analysis.
<em>TNNLS</em>, <em>35</em>(4), 5566–5576. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, spherical data (i.e., $L_{2}$ normalized vectors) modeling has become a promising research topic in various real-world applications (such as gene expression data analysis, document categorization, and gesture recognition). In this work, we propose a hierarchical nonparametric Bayesian model based on von Mises–Fisher (VMF) distributions for modeling spherical data that involve multiple groups, where each observation within a group is sampled from a VMF mixture model with an infinite number of components allowing them to be shared across groups. Our model is formulated by employing a hierarchical nonparametric Bayesian framework known as the hierarchical Pitman–Yor (HPY) process mixture model, which possesses a power-law nature over the distribution of the components and is particularly useful for data distributions with heavy tails and skewness. To learn the proposed HPY process mixture model with VMF distributions, we systematically develop a closed-form optimization algorithm based on variational Bayes (VB). The merits of the proposed hierarchical Bayesian nonparametric model for modeling grouped spherical data are demonstrated through experiments on both synthetic data and a real-world application about resting-state functional magnetic resonance imaging (fMRI) data analysis.},
  archive      = {J_TNNLS},
  author       = {Wentao Fan and Lin Yang and Nizar Bouguila},
  doi          = {10.1109/TNNLS.2022.3208202},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5566-5576},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Grouped spherical data modeling through hierarchical nonparametric bayesian models and its application to fMRI data analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exponential convergence of primal–dual dynamics under
general conditions and its application to distributed optimization.
<em>TNNLS</em>, <em>35</em>(4), 5551–5565. (<a
href="https://doi.org/10.1109/TNNLS.2022.3208086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we establish the local and global exponential convergence of a primal–dual dynamics (PDD) for solving equality-constrained optimization problems without strong convexity and full row rank assumption on the equality constraint matrix. Under the metric subregularity of Karush-Kuhn-Tucker (KKT) mapping, we prove the local exponential convergence of the dynamics. Moreover, we establish the global exponential convergence of the dynamics in an invariant subspace under a technically designed condition which is weaker than strong convexity. As an application, the obtained theoretical results are used to show the exponential convergence of several existing state-of-the-art primal–dual algorithms for solving distributed optimization without strong convexity. Finally, we provide some experiments to demonstrate the effectiveness of our results.},
  archive      = {J_TNNLS},
  author       = {Luyao Guo and Xinli Shi and Jinde Cao and Zihao Wang},
  doi          = {10.1109/TNNLS.2022.3208086},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5551-5565},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential convergence of Primal–Dual dynamics under general conditions and its application to distributed optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the distributions of aggregation layers in
deep neural networks. <em>TNNLS</em>, <em>35</em>(4), 5536–5550. (<a
href="https://doi.org/10.1109/TNNLS.2022.3207790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of aggregation is ubiquitous in almost all the deep nets’ models. It functions as an important mechanism for consolidating deep features into a more compact representation while increasing the robustness to overfitting and providing spatial invariance in deep nets. In particular, the proximity of global aggregation layers to the output layers of DNNs means that aggregated features directly influence the performance of a deep net. A better understanding of this relationship can be obtained using information theoretic methods. However, this requires knowledge of the distributions of the activations of aggregation layers. To achieve this, we propose a novel mathematical formulation for analytically modeling the probability distributions of output values of layers involved with deep feature aggregation. An important outcome is our ability to analytically predict the Kullback–Leibler (KL)-divergence of output nodes in a DNN. We also experimentally verify our theoretical predictions against empirical observations across a broad range of different classification tasks and datasets.},
  archive      = {J_TNNLS},
  author       = {Eng-Jon Ong and Sameed Husain and Miroslaw Bober},
  doi          = {10.1109/TNNLS.2022.3207790},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5536-5550},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding the distributions of aggregation layers in deep neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Differentially private consensus for second-order
multiagent systems with quantized communication. <em>TNNLS</em>,
<em>35</em>(4), 5523–5535. (<a
href="https://doi.org/10.1109/TNNLS.2022.3207470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the differentially private consensus problem of discrete-time second-order multiagent systems with partially measurable states and limited communication channel capacity, where only the integer-value information of agents can be transmitted. To reduce the potential risk of state information disclosure in digital communication, a differentially private consensus algorithm via dynamic encoding–decoding is proposed for the second-order multiagent system to make agents achieve mean-square consensus by transmitting quantized integer values with privacy protection. To deal with the uncertainty of the quantizer saturation, the statistical analysis is given for the boundedness of the input of quantizers. It is shown that the expectation of the minimum memory capacity of quantizers is 2 bits. Finally, some simulation results are given to visualize our conclusions.},
  archive      = {J_TNNLS},
  author       = {Wenjun Zhang and Bing-Chang Wang and Yong Liang},
  doi          = {10.1109/TNNLS.2022.3207470},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5523-5535},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Differentially private consensus for second-order multiagent systems with quantized communication},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Output feedback-based consensus for nonlinear multiagent
systems: The event-triggered communication strategy. <em>TNNLS</em>,
<em>35</em>(4), 5512–5522. (<a
href="https://doi.org/10.1109/TNNLS.2022.3207168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current investigation explores the leader-following consensus problem for nonlinear multiagent systems under the output feedback control mechanism and the event-triggered communication mechanism. Owing to the physical instrument constraints, a significant portion of the state variables is not readily available. Therefore, this article put forward a distributed event-based leader-following consensus protocol only using agents’ relative output measurements and underlying neighbors. Furthermore, this article develops two event-triggered mechanisms simultaneously, one is the event-triggered communication mechanism in the sensor-to-controller channel, and another is the event-triggered controller update in the controller-to-actuator track. Besides that, it is proven that the developed event-triggered control protocol can settle the leader-following consensus problem of the nonlinear multiagent systems, and the Zeno behavior is excluded in both the channels. Finally, we perform two simulation examples to illustrate the efficacy of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Lihua Tan and Xin Wang and Chuandong Li and Xing He},
  doi          = {10.1109/TNNLS.2022.3207168},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5512-5522},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Output feedback-based consensus for nonlinear multiagent systems: The event-triggered communication strategy},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive neural network-based event-triggered SOC observer
with application to a stochastic battery model. <em>TNNLS</em>,
<em>35</em>(4), 5501–5511. (<a
href="https://doi.org/10.1109/TNNLS.2022.3205040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate state of charge (SOC) is crucial to achieving safe, reliable, and efficient use of batteries. This article proposes an adaptive neural network (NN)-based event-triggered observer to estimate SOC. First, a stochastic battery equivalent circuit model (ECM) is established, where an adaptive NN is employed to approximate the unknown nonlinear part. The learning process of network weight is conducted online to observe the variations of model parameters and avoid time-consuming processes for parameter extraction. Besides, for the purpose of saving computational cost, an event-triggered mechanism (ETM) is employed in the weight updating law, which means the weights only update when it is necessary. Then, an adaptive radial basis function (RBF) NN-based SOC observer is designed, and its stability is proven by the Lyapunov theory. Moreover, the strictly positive lower bound of interevent time is derived, and undesirable Zeno behavior can be excluded. Finally, the accuracy and robustness of the proposed observer are evaluated by experiments and simulations. Results show that the proposed method can estimate SOC accurately in the presence of initial deviation and sensor noises.},
  archive      = {J_TNNLS},
  author       = {Chenyang Pan and Zhaoxia Peng and Shichun Yang and Guoguang Wen and Biao Luo and Tingwen Huang},
  doi          = {10.1109/TNNLS.2022.3205040},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5501-5511},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network-based event-triggered SOC observer with application to a stochastic battery model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural joint entropy estimation. <em>TNNLS</em>,
<em>35</em>(4), 5488–5500. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the entropy of a discrete random variable is a fundamental problem in information theory and related fields. This problem has many applications in various domains, including machine learning, statistics, and data compression. Over the years, a variety of estimation schemes have been suggested. However, despite significant progress, most methods still struggle when the sample is small, compared to the variable’s alphabet size. In this work, we introduce a practical solution to this problem, which extends the work of McAllester and Statos. The proposed scheme uses the generalization abilities of cross-entropy estimation in deep neural networks (DNNs) to introduce improved entropy estimation accuracy. Furthermore, we introduce a family of estimators for related information-theoretic measures, such as conditional entropy and mutual information (MI). We show that these estimators are strongly consistent and demonstrate their performance in a variety of use cases. First, we consider large alphabet entropy estimation. Then, we extend the scope to MI estimation. Next, we apply the proposed scheme to conditional MI estimation, as we focus on independence testing tasks. Finally, we study a transfer entropy (TE) estimation problem. The proposed estimators demonstrate improved performance compared to existing methods in all of these setups.},
  archive      = {J_TNNLS},
  author       = {Yuval Shalev and Amichai Painsky and Irad Ben-Gal},
  doi          = {10.1109/TNNLS.2022.3204919},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5488-5500},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural joint entropy estimation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-behavior graph neural networks for recommender system.
<em>TNNLS</em>, <em>35</em>(4), 5473–5487. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have been demonstrated to be effective to meet user’s personalized interests for many online services (e.g., E-commerce and online advertising platforms). Recent years have witnessed the emerging success of many deep-learning-based recommendation models for augmenting collaborative filtering (CF) architectures with various neural network architectures, such as multilayer perceptron and autoencoder. However, the majority of them model the user–item relationship with single type of interaction, while overlooking the diversity of user behaviors on interacting with items, which can be click, add-to-cart, tag-as-favorite, and purchase. Such various types of interaction behaviors have great potential in providing rich information for understanding the user preferences. In this article, we pay special attention on user–item relationships with the exploration of multityped user behaviors. Technically, we contribute a new multi-behavior graph neural network (MBRec), which specially accounts for diverse interaction patterns and the underlying cross-type behavior interdependencies. In the MBRec framework, we develop a graph-structured learning framework to perform expressive modeling of high-order connectivity in behavior-aware user–item interaction graph. After that, a mutual relationship encoder is proposed to adaptively uncover complex relational structures and make aggregations across layer-specific behavior representations. Through comprehensive evaluation on real-world datasets, the advantages of our MBRec method have been validated under different experimental settings. Further analysis verifies the positive effects of incorporating the multi-behavioral context into the recommendation paradigm. In addition, the conducted case studies offer insights into the interpretability of user multi-behavior representations. We release our model implementation at https://github.com/akaxlh/MBRec .},
  archive      = {J_TNNLS},
  author       = {Lianghao Xia and Chao Huang and Yong Xu and Peng Dai and Liefeng Bo},
  doi          = {10.1109/TNNLS.2022.3204775},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5473-5487},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-behavior graph neural networks for recommender system},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph representation learning for large-scale neuronal
morphological analysis. <em>TNNLS</em>, <em>35</em>(4), 5461–5472. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of neuronal morphological data is essential to investigate the neuronal properties and brain mechanisms. The complex morphologies, absence of annotations, and sheer volume of these data pose significant challenges in neuronal morphological analysis, such as identifying neuron types and large-scale neuron retrieval, all of which require accurate measuring and efficient matching algorithms. Recently, many studies have been conducted to describe neuronal morphologies quantitatively using predefined measurements. However, hand-crafted features are usually inadequate for distinguishing fine-grained differences among massive neurons. In this article, we propose a novel morphology-aware contrastive graph neural network (MACGNN) for unsupervised neuronal morphological representation learning. To improve the retrieval efficiency in large-scale neuronal morphological datasets, we further propose Hash-MACGNN by introducing an improved deep hash algorithm to train the network end-to-end to learn binary hash representations of neurons. We conduct extensive experiments on the largest dataset, NeuroMorpho, which contains more than $100\,000$ neurons. The experimental results demonstrate the effectiveness and superiority of our MACGNN and Hash-MACGNN for large-scale neuronal morphological analysis.},
  archive      = {J_TNNLS},
  author       = {Jie Zhao and Xuejin Chen and Zhiwei Xiong and Zheng-Jun Zha and Feng Wu},
  doi          = {10.1109/TNNLS.2022.3204686},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5461-5472},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph representation learning for large-scale neuronal morphological analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Not all instances contribute equally: Instance-adaptive
class representation learning for few-shot visual recognition.
<em>TNNLS</em>, <em>35</em>(4), 5447–5460. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot visual recognition refers to recognize novel visual concepts from a few labeled instances. Many few-shot visual recognition methods adopt the metric-based meta-learning paradigm by comparing the query representation with class representations to predict the category of query instance. However, the current metric-based methods generally treat all instances equally and consequently often obtain biased class representation, considering not all instances are equally significant when summarizing the instance-level representations for the class-level representation. For example, some instances may contain unrepresentative information, such as too much background and information of unrelated concepts, which skew the results. To address the above issues, we propose a novel metric-based meta-learning framework termed instance-adaptive class representation learning network (ICRL-Net) for few-shot visual recognition. Specifically, we develop an adaptive instance revaluing network (AIRN) with the capability to address the biased representation issue when generating the class representation, by learning and assigning adaptive weights for different instances according to their relative significance in the support set of corresponding class. In addition, we design an improved bilinear instance representation and incorporate two novel structural losses, i.e., intraclass instance clustering loss and interclass representation distinguishing loss, to further regulate the instance revaluation process and refine the class representation. We conduct extensive experiments on four commonly adopted few-shot benchmarks: miniImageNet, tieredImageNet, CIFAR-FS, and FC100 datasets. The experimental results compared with the state-of-the-art approaches demonstrate the superiority of our ICRL-Net.},
  archive      = {J_TNNLS},
  author       = {Mengya Han and Yibing Zhan and Yong Luo and Bo Du and Han Hu and Yonggang Wen and Dacheng Tao},
  doi          = {10.1109/TNNLS.2022.3204684},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5447-5460},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Not all instances contribute equally: Instance-adaptive class representation learning for few-shot visual recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot object detection with self-supervising and
cooperative classifier. <em>TNNLS</em>, <em>35</em>(4), 5435–5446. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot object detection (FSOD), which detects novel objects with only a few training instances, has recently attracted more attention. Previous works focus on making the most use of label information of objects. Still, they fail to consider the structural and semantic information of the image itself and solve the misclassification between data-abundant base classes and data-scarce novel classes efficiently. In this article, we propose FSOD with Self-Supervising and Cooperative Classifier ( $\text {F}\text {S}^{3}\text {C}$ ) approach to deal with those concerns. Specifically, we analyze the underlying performance degradation of novel classes in FSOD and discover that false-positive samples are the main reason. By looking into these false-positive samples, we further notice that misclassifying novel classes as base classes are the main cause. Thus, we introduce double RoI heads into the existing Fast-RCNN to learn more specific features for novel classes. We also consider using self-supervised learning (SSL) to learn more structural and semantic information. Finally, we propose a cooperative classifier (CC) with the base–novel regularization to maximize the interclass variance between base and novel classes. In the experiment, $\text {F}\text {S}^{3}\text {C}$ outperforms all the latest baselines in most cases on PASCAL VOC and COCO.},
  archive      = {J_TNNLS},
  author       = {Di Qi and Jilin Hu and Jianbing Shen},
  doi          = {10.1109/TNNLS.2022.3204597},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5435-5446},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Few-shot object detection with self-supervising and cooperative classifier},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A learning-based approach for diagnosis and diagnosability
of unknown discrete event systems. <em>TNNLS</em>, <em>35</em>(4),
5421–5434. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a novel active-learning technique for fault diagnosis of an initially unknown finite-state discrete event system (DES). The proposed method constructs a diagnosis tool (termed diagnoser), which is able to detect and identify occurred faults by tracking the observable behaviors of the system under diagnosis. The proposed algorithm utilizes an active-learning mechanism to incrementally collect the information about the system to construct the diagnoser. This is achieved by completing a series of observation tables in a systematic way, resulting in the construction of the diagnoser. It is proven that the proposed algorithm terminates after a finite number of iterations and returns a correctly conjectured diagnoser. The developed diagnoser is a deterministic finite-state automaton. Furthermore, we have proven that the developed diagnoser consists of a minimum number of states. A sufficient condition for diagnosability of the system under diagnosis is derived, which guarantees the diagnosis of faults within a bounded number of observations. The developed method is applied to two case-studies, illustrating the steps of the proposed algorithm and its capability of diagnosing multiple faults.},
  archive      = {J_TNNLS},
  author       = {Ira Wendell Bates and Ali Karimoddini and Mohammad Karimadini},
  doi          = {10.1109/TNNLS.2022.3204557},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5421-5434},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A learning-based approach for diagnosis and diagnosability of unknown discrete event systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A conformable moments-based deep learning system for forged
handwriting detection. <em>TNNLS</em>, <em>35</em>(4), 5407–5420. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting forged handwriting is important in a wide variety of machine learning applications, and it is challenging when the input images are degraded with noise and blur. This article presents a new model based on conformable moments (CMs) and deep ensemble neural networks (DENNs) for forged handwriting detection in noisy and blurry environments. Since CMs involve fractional calculus with the ability to model nonlinearities and geometrical moments as well as preserving spatial relationships between pixels, fine details in images are preserved. This motivates us to introduce a DENN classifier, which integrates stenographic kernels and spatial features to classify input images as normal (original, clean images), altered (handwriting changed through copy-paste and insertion operations), noisy (added noise to original image), blurred (added blur to original image), altered-noise (noise is added to the altered image), and altered-blurred (blur is added to the altered image). To evaluate our model, we use a newly introduced dataset, which comprises handwritten words altered at the character level, as well as several standard datasets, namely ACPR 2019, ICPR 2018-FDC, and the IMEI dataset. The first two of these datasets include handwriting samples that are altered at the character and word levels, and the third dataset comprises forged International Mobile Equipment Identity (IMEI) numbers. Experimental results demonstrate that the proposed method outperforms the existing methods in terms of classification rate.},
  archive      = {J_TNNLS},
  author       = {Lokesh Nandanwar and Palaiahnakote Shivakumara and Hamid A. Jalab and Rabha W. Ibrahim and Ramachandra Raghavendra and Umapada Pal and Tong Lu and Michael Blumenstein},
  doi          = {10.1109/TNNLS.2022.3204390},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5407-5420},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A conformable moments-based deep learning system for forged handwriting detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized weakly supervised object localization.
<em>TNNLS</em>, <em>35</em>(4), 5395–5406. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the goal of learning to localize specific object semantics using the low-cost image-level annotation, weakly supervised object localization (WSOL) has been receiving increasing attention in recent years. Although existing literatures have studied a number of major issues in this field, one important yet challenging scenario, where the test object semantics may appear in the training phase (seen categories) or never been observed before (unseen categories), is still beyond the exploration of the existing works. We define this scenario as the generalized WSOL (GWSOL) and make a pioneering effort to study it in this article. By leveraging attribute vectors to associate seen and unseen categories, we involve threefold modeling components, i.e., the class-sensitive modeling, semantic-agnostic modeling, and content-aware modeling, into a unified end-to-end learning framework. Such design enables our model to recognize and localize unconstrained object semantics, learn compact and discriminative features that could represent the potential unseen categories, and customize content-aware attribute weights to avoid localizing on misleading attribute elements. To advance this research direction, we contribute the bounding-box manual annotations to the widely used AwA2 dataset and benchmark the GWSOL methods. Comprehensive experiments demonstrate the effectiveness of our proposed learning framework and each of the considered modeling components.},
  archive      = {J_TNNLS},
  author       = {Dingwen Zhang and Guangyu Guo and Wenyuan Zeng and Lei Li and Junwei Han},
  doi          = {10.1109/TNNLS.2022.3204337},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5395-5406},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized weakly supervised object localization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spurious local minima are common for deep neural networks
with piecewise linear activations. <em>TNNLS</em>, <em>35</em>(4),
5382–5394. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, theoretically, it is shown that spurious local minima are common for deep fully connected networks and average-pooling convolutional neural networks (CNNs) with piecewise linear activations and datasets that cannot be fit by linear models. Motivating examples are given to explain why spurious local minima exist: each output neuron of deep fully connected networks and CNNs with piecewise linear activations produces a continuous piecewise linear (CPWL) function, and different pieces of the CPWL output can optimally fit disjoint groups of data samples when minimizing the empirical risk. Fitting data samples with different CPWL functions usually results in different levels of empirical risk, leading to the prevalence of spurious local minima. The results are proved in general settings with arbitrary continuous loss functions and general piecewise linear activations. The main proof technique is to represent a CPWL function as maximization over minimization of linear pieces. Deep networks with piecewise linear activations are then constructed to produce these linear pieces and implement the maximization over minimization operation.},
  archive      = {J_TNNLS},
  author       = {Bo Liu},
  doi          = {10.1109/TNNLS.2022.3204319},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5382-5394},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spurious local minima are common for deep neural networks with piecewise linear activations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive black-box defense against trojan attacks
(TrojDef). <em>TNNLS</em>, <em>35</em>(4), 5367–5381. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trojan backdoor is a poisoning attack against neural network (NN) classifiers in which adversaries try to exploit the (highly desirable) model reuse property to implant Trojans into model parameters for backdoor breaches through a poisoned training process. To misclassify an input to a target class, the attacker activates the backdoor by augmenting the input with a predefined trigger that is only known to her/him. Most of the proposed defenses against Trojan attacks assume a white-box setup, in which the defender either has access to the inner state of NN or is able to run backpropagation through it. In this work, we propose a more practical black-box defense, dubbed TrojDef. In a black-box setup, the defender can only run forward-pass of the NN. TrojDef is motivated by the Trojan poisoned training, in which the model is trained on both benign and Trojan inputs. TrojDef tries to identify and filter out Trojan inputs (i.e., inputs augmented with the Trojan trigger) by monitoring the changes in the prediction confidence when the input is repeatedly perturbed by random noise. We derive a function based on the prediction outputs which is called the prediction confidence bound to decide whether the input example is Trojan or not. The intuition is that Trojan inputs are more stable as the misclassification only depends on the trigger, while benign inputs will suffer when augmented with noise due to the perturbation of the classification features. Through mathematical analysis, we show that if the attacker is perfect in injecting the backdoor, the Trojan infected model will be trained to learn the appropriate prediction confidence bound, which is used to distinguish Trojan and benign inputs under arbitrary perturbations. However, because the attacker might not be perfect in injecting the backdoor, we introduce a nonlinear transform to the prediction confidence bound to improve the detection accuracy in practical settings. Extensive empirical evaluations show that TrojDef significantly outperforms the-state-of-the-art defenses and is highly stable under different settings, even when the classifier architecture, the training process, or the hyperparameters change.},
  archive      = {J_TNNLS},
  author       = {Guanxiong Liu and Abdallah Khreishah and Fatima Sharadgah and Issa Khalil},
  doi          = {10.1109/TNNLS.2022.3204283},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5367-5381},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An adaptive black-box defense against trojan attacks (TrojDef)},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SwinPA-net: Swin transformer-based multiscale feature
pyramid aggregation network for medical image segmentation.
<em>TNNLS</em>, <em>35</em>(4), 5355–5366. (<a
href="https://doi.org/10.1109/TNNLS.2022.3204090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precise segmentation of medical images is one of the key challenges in pathology research and clinical practice. However, many medical image segmentation tasks have problems such as large differences between different types of lesions and similar shapes as well as colors between lesions and surrounding tissues, which seriously affects the improvement of segmentation accuracy. In this article, a novel method called Swin Pyramid Aggregation network (SwinPA-Net) is proposed by combining two designed modules with Swin Transformer to learn more powerful and robust features. The two modules, named dense multiplicative connection (DMC) module and local pyramid attention (LPA) module, are proposed to aggregate the multiscale context information of medical images. The DMC module cascades the multiscale semantic feature information through dense multiplicative feature fusion, which minimizes the interference of shallow background noise to improve the feature expression and solves the problem of excessive variation in lesion size and type. Moreover, the LPA module guides the network to focus on the region of interest by merging the global attention and the local attention, which helps to solve similar problems. The proposed network is evaluated on two public benchmark datasets for polyp segmentation task and skin lesion segmentation task as well as a clinical private dataset for laparoscopic image segmentation task. Compared with existing state-of-the-art (SOTA) methods, the SwinPA-Net achieves the most advanced performance and can outperform the second-best method on the mean Dice score by 1.68%, 0.8%, and 1.2% on the three tasks, respectively.},
  archive      = {J_TNNLS},
  author       = {Hao Du and Jiazheng Wang and Min Liu and Yaonan Wang and Erik Meijering},
  doi          = {10.1109/TNNLS.2022.3204090},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5355-5366},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SwinPA-net: Swin transformer-based multiscale feature pyramid aggregation network for medical image segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered adaptive fuzzy neural network output
feedback control for constrained stochastic nonlinear systems.
<em>TNNLS</em>, <em>35</em>(4), 5345–5354. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of command-filtered event-triggered adaptive fuzzy neural network (FNN) output feedback control for stochastic nonlinear systems (SNSs) with time-varying asymmetric constraints and input saturation. By constructing quartic asymmetric time-varying barrier Lyapunov functions (TVBLFs), all the state variables are not to transgress the prescribed dynamic constraints. The command-filtered backstepping method and the error compensation mechanism are combined to eliminate the issue of “computational explosion” and compensate the filtering errors. An FNN observer is developed to estimate the unmeasured states. The event-triggered mechanism is introduced to improve the efficiency in resource utilization. It is shown that the tracking error can converge to a small neighborhood of the origin, and all signals in the closed-loop systems are bounded. Finally, a physical example is used to verify the feasibility of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Chenyi Si and Qing-Guo Wang and Jinpeng Yu},
  doi          = {10.1109/TNNLS.2022.3203419},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5345-5354},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive fuzzy neural network output feedback control for constrained stochastic nonlinear systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sampled-data-based secure synchronization control for
chaotic lur’e systems subject to denial-of-service attacks.
<em>TNNLS</em>, <em>35</em>(4), 5332–5344. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the sampled-data-based secure synchronization control problem for chaotic Lur’e systems subject to power-constrained denial-of-service (DoS) attacks, which can block data packets’ transmission in communication channels. To eliminate the adverse effects, a resilient sampled data control scheme consisting of a secure controller and communication protocol is designed by considering the attack signals and periodic sampling mechanism simultaneously. Then, a novel index, i.e., the maximum anti-attack ratio, is proposed to measure the secure level. On this basis, a multi-interval-dependent functional is established for the resulting closed-loop system model. The main feature of the developed functional lies in that it can fully use the information of resilient sampling intervals and DoS attacks. In combination with the convex combination method, discrete-time Lyapunov theory, and some inequality estimate techniques, two sufficient conditions are, respectively, derived to achieve sampled-data-based secure synchronization of drive–response systems against DoS attacks. Compared with the existing Lyapunov functionals, the advantages of the proposed multi-interval-dependent functional are analyzed in detail. Finally, a synchronization example and an application to secure communication are provided to display the effectiveness and validity of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Yingjie Fan and Xia Huang and Yuxia Li and Hao Shen},
  doi          = {10.1109/TNNLS.2022.3203382},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5332-5344},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sampled-data-based secure synchronization control for chaotic lur’e systems subject to denial-of-service attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient and adaptive granular-ball generation method in
classification problem. <em>TNNLS</em>, <em>35</em>(4), 5319–5331. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Granular-ball computing (GBC) is an efficient, robust, and scalable learning method for granular computing. The granular ball (GB) generation method is based on GB computing. This article proposes a method for accelerating GB generation using division to replace $k$ -means. It can significantly improve the efficiency of GB generation while ensuring an accuracy similar to that of the existing methods. In addition, a new adaptive method for GB generation is proposed by considering the elimination of the GB overlap and other factors. This makes the GB generation process parameter-free and completely adaptive in the true sense. In addition, this study first provides mathematical models for the GB covering. The experimental results on some real datasets demonstrate that the two proposed GB generation methods have accuracies similar to those of the existing method in most cases, while adaptiveness or acceleration is realized. All the codes were released in the open-source GBC library at https://www.cquptshuyinxia.com/GBC.html or https://github.com/syxiaa/gbc},
  archive      = {J_TNNLS},
  author       = {Shuyin Xia and Xiaochuan Dai and Guoyin Wang and Xinbo Gao and Elisabeth Giem},
  doi          = {10.1109/TNNLS.2022.3203381},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5319-5331},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient and adaptive granular-ball generation method in classification problem},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LaplaceNet: A hybrid graph-energy neural network for deep
semisupervised classification. <em>TNNLS</em>, <em>35</em>(4),
5306–5318. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised learning (SSL) has received a lot of recent attention as it alleviates the need for large amounts of labeled data which can often be expensive, requires expert knowledge, and be time consuming to collect. Recent developments in deep semisupervised classification have reached unprecedented performance and the gap between supervised and SSL is ever-decreasing. This improvement in performance has been based on the inclusion of numerous technical tricks, strong augmentation techniques, and costly optimization schemes with multiterm loss functions. We propose a new framework, LaplaceNet, for deep semisupervised classification that has a greatly reduced model complexity. We utilize a hybrid approach where pseudolabels are produced by minimizing the Laplacian energy on a graph. These pseudolabels are then used to iteratively train a neural-network backbone. Our model outperforms state-of-the-art methods for deep semisupervised classification, over several benchmark datasets. Furthermore, we consider the application of strong augmentations to neural networks theoretically and justify the use of a multisampling approach for SSL. We demonstrate, through rigorous experimentation, that a multisampling augmentation approach improves generalization and reduces the sensitivity of the network to augmentation.},
  archive      = {J_TNNLS},
  author       = {Philip Sellars and Angelica I. Aviles-Rivero and Carola-Bibiane Schönlieb},
  doi          = {10.1109/TNNLS.2022.3203315},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5306-5318},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LaplaceNet: A hybrid graph-energy neural network for deep semisupervised classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observer-based neural control of n-link flexible-joint
robots. <em>TNNLS</em>, <em>35</em>(4), 5295–5305. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concentrates on the adaptive neural control approach of $n$ -link flexible-joint electrically driven robots. The presented control method only needs to know the position and armature current information of the flexible-joint manipulator. An adaptive observer is designed to estimate the velocities of links and motors, and radial basis function neural networks are applied to approximate the unknown nonlinearities. Based on the backstepping technique and the Lyapunov stability theory, the observer-based neural control issue is addressed by relying on uplink-event-triggered states only. It is demonstrated that all signals are semi-globally ultimately uniformly bounded and the tracking errors can converge to a small neighborhood of zero. Finally, simulation results are shown to validate the designed event-triggered control strategy.},
  archive      = {J_TNNLS},
  author       = {Hui Ma and Hongru Ren and Qi Zhou and Hongyi Li and Zhenyou Wang},
  doi          = {10.1109/TNNLS.2022.3203074},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5295-5305},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based neural control of N-link flexible-joint robots},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimax optimal bandits for heavy tail rewards.
<em>TNNLS</em>, <em>35</em>(4), 5280–5294. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic multiarmed bandits (stochastic MABs) are a problem of sequential decision-making with noisy rewards, where an agent sequentially chooses actions under unknown reward distributions to minimize cumulative regret. The majority of prior works on stochastic MABs assume that the reward distribution of each action has bounded supports or follows light-tailed distribution, i.e., sub-Gaussian distribution. However, in a variety of decision-making problems, the reward distributions follow a heavy-tailed distribution. In this regard, we consider stochastic MABs with heavy-tailed rewards, whose $p$ th moment is bounded by a constant $\nu _{p}$ for $1 &amp;lt; p\leq 2$ . First, we provide theoretical analysis on sub-optimality of the existing exploration methods for heavy-tailed rewards where it has been proven that existing exploration methods do not guarantee a minimax optimal regret bound. Second, to achieve the minimax optimality under heavy-tailed rewards, we propose a minimax optimal robust upper confidence bound (MR-UCB) by providing tight confidence bound of a $p$ -robust estimator. Furthermore, we also propose a minimax optimal robust adaptively perturbed exploration (MR-APE) which is a randomized version of MR-UCB. In particular, unlike the existing robust exploration methods, both proposed methods have no dependence on $\nu _{p}$ . Third, we provide the gap-dependent and independent regret bounds of proposed methods and prove that both methods guarantee the minimax optimal regret bound for a heavy-tailed stochastic MAB problem. The proposed methods are the first algorithm that theoretically guarantees the minimax optimality under heavy-tailed reward settings to the best of our knowledge. Finally, we demonstrate the superiority of the proposed methods in simulation with Pareto and Fréchet noises with respect to regrets.},
  archive      = {J_TNNLS},
  author       = {Kyungjae Lee and Sungbin Lim},
  doi          = {10.1109/TNNLS.2022.3203035},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5280-5294},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Minimax optimal bandits for heavy tail rewards},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Action candidate driven clipped double q-learning for
discrete and continuous action tasks. <em>TNNLS</em>, <em>35</em>(4),
5269–5279. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Double Q-learning is a popular reinforcement learning algorithm in Markov decision process (MDP) problems. Clipped double Q-learning, as an effective variant of double Q-learning, employs the clipped double estimator to approximate the maximum expected action value. Due to the underestimation bias of the clipped double estimator, the performance of clipped Double Q-learning may be degraded in some stochastic environments. In this article, in order to reduce the underestimation bias, we propose an action candidate-based clipped double estimator (AC-CDE) for Double Q-learning. Specifically, we first select a set of elite action candidates with high action values from one set of estimators. Then, among these candidates, we choose the highest valued action from the other set of estimators. Finally, we use the maximum value in the second set of estimators to clip the action value of the chosen action in the first set of estimators and the clipped value is used for approximating the maximum expected action value. Theoretically, the underestimation bias in our clipped Double Q-learning decays monotonically as the number of action candidates decreases. Moreover, the number of action candidates controls the tradeoff between the overestimation and underestimation biases. In addition, we also extend our clipped Double Q-learning to continuous action tasks via approximating the elite continuous action candidates. We empirically verify that our algorithm can more accurately estimate the maximum expected action value on some toy environments and yield good performance on several benchmark problems. Code is available at https://github.com/Jiang-HB/ac_CDQ .},
  archive      = {J_TNNLS},
  author       = {Haobo Jiang and Guangyu Li and Jin Xie and Jian Yang},
  doi          = {10.1109/TNNLS.2022.3203024},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5269-5279},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Action candidate driven clipped double Q-learning for discrete and continuous action tasks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Error loss networks. <em>TNNLS</em>, <em>35</em>(4),
5256–5268. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel model called error loss network (ELN) is proposed to build an error loss function for supervised learning. The ELN is similar in structure to a radial basis function (RBF) neural network, but its input is an error sample and output is a loss corresponding to that error sample. That means the nonlinear input–output mapper of the ELN creates an error loss function. The proposed ELN provides a unified model for a large class of error loss functions, which includes some information-theoretic learning (ITL) loss functions as special cases. The activation function, weight parameters, and network size of the ELN can be predetermined or learned from the error samples. On this basis, we propose a new machine learning paradigm where the learning process is divided into two stages: first, learning a loss function using an ELN; second, using the learned loss function to continue to perform the learning. Experimental results are presented to demonstrate the desirable performance of the new method.},
  archive      = {J_TNNLS},
  author       = {Badong Chen and Yunfei Zheng and Pengju Ren},
  doi          = {10.1109/TNNLS.2022.3202989},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5256-5268},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Error loss networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed adaptive forwarding finite-time output consensus
of high-order multiagent systems via immersion and invariance-based
approximator. <em>TNNLS</em>, <em>35</em>(4), 5241–5255. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A finite-time output consensus control problem is investigated in this article for an uncertain nonlinear high-order multiagent systems (MASs). For this class of MASs, the order of individual follower is reduced gradually by implementing the immersion and invariance (I&amp;I) control theory repeatedly, and a requirement of solving partial differential equations (PDEs) in I&amp;I control theory is obviated. Furthermore, an I&amp;I-based radial basis function neural network (RBFNN) approximator is developed, where an extra cross term is added in the approximation mechanism, and the form of an update law for weights is transformed into a proportional and integral one. This I&amp;I-based RBFNN approximator does not rely on a cancellation of the perturbation term, and these uncertainties are reconstructed by the I&amp;I manifold adaptively, which is for improvement of approximation behaviors of traditional RBFNNs. On this basis, a distributed adaptive forwarding finite-time output consensus control strategy is proposed by combining a sign function, and the convergence time of the MAS can be adjusted with appropriate finite-time parameters. Finally, two illustrative examples verify the effectiveness of the theoretical claims.},
  archive      = {J_TNNLS},
  author       = {Yang Yang and Shuang Song and Sergey Gorbachev and Dong Yue and Jianchao He},
  doi          = {10.1109/TNNLS.2022.3203011},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5241-5255},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed adaptive forwarding finite-time output consensus of high-order multiagent systems via immersion and invariance-based approximator},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to synthesize compatible fashion items using
semantic alignment and collocation classification: An outfit generation
framework. <em>TNNLS</em>, <em>35</em>(4), 5226–5240. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of fashion compatibility learning has attracted great attention from both the academic and industrial communities in recent years. Many studies have been carried out for fashion compatibility prediction, collocated outfit recommendation, artificial intelligence (AI)-enabled compatible fashion design, and related topics. In particular, AI-enabled compatible fashion design can be used to synthesize compatible fashion items or outfits to improve the design experience for designers or the efficacy of recommendations for customers. However, previous generative models for collocated fashion synthesis have generally focused on the image-to-image translation between fashion items of upper and lower clothing. In this article, we propose a novel outfit generation framework, i.e., OutfitGAN, with the aim of synthesizing a set of complementary items to compose an entire outfit, given one extant fashion item and reference masks of target synthesized items. OutfitGAN includes a semantic alignment module (SAM), which is responsible for characterizing the mapping correspondence between the existing fashion items and the synthesized ones, to improve the quality of the synthesized images, and a collocation classification module (CCM), which is used to improve the compatibility of a synthesized outfit. To evaluate the performance of our proposed models, we built a large-scale dataset consisting of 20 000 fashion outfits. Extensive experimental results on this dataset show that our OutfitGAN can synthesize photo-realistic outfits and outperform the state-of-the-art methods in terms of similarity, authenticity, and compatibility measurements.},
  archive      = {J_TNNLS},
  author       = {Dongliang Zhou and Haijun Zhang and Kai Yang and Linlin Liu and Han Yan and Xiaofei Xu and Zhao Zhang and Shuicheng Yan},
  doi          = {10.1109/TNNLS.2022.3202842},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5226-5240},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning to synthesize compatible fashion items using semantic alignment and collocation classification: An outfit generation framework},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Holistic-guided disentangled learning with cross-video
semantics mining for concurrent first-person and third-person activity
recognition. <em>TNNLS</em>, <em>35</em>(4), 5211–5225. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of wearable devices has increased the demands for the research on first-person activity recognition. However, most of the current first-person activity datasets are built based on the assumption that only the human–object interaction (HOI) activities, performed by the camera-wearer, are captured in the field of view. Since humans live in complicated scenarios, in addition to the first-person activities, it is likely that third-person activities performed by other people also appear. Analyzing and recognizing these two types of activities simultaneously occurring in a scene is important for the camera-wearer to understand the surrounding environments. To facilitate the research on concurrent first- and third-person activity recognition (CFT-AR), we first created a new activity dataset, namely PolyU concurrent first- and third-person (CFT) Daily, which exhibits distinct properties and challenges, compared with previous activity datasets. Since temporal asynchronism and appearance gap usually exist between the first- and third-person activities, it is crucial to learn robust representations from all the activity-related spatio-temporal positions. Thus, we explore both holistic scene-level and local instance-level (person-level) features to provide comprehensive and discriminative patterns for recognizing both first- and third-person activities. On the one hand, the holistic scene-level features are extracted by a 3-D convolutional neural network, which is trained to mine shared and sample-unique semantics between video pairs, via two well-designed attention-based modules and a self-knowledge distillation (SKD) strategy. On the other hand, we further leverage the extracted holistic features to guide the learning of instance-level features in a disentangled fashion, which aims to discover both spatially conspicuous patterns and temporally varied, yet critical, cues. Experimental results on the PolyU CFT Daily dataset validate that our method achieves the state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Tianshan Liu and Rui Zhao and Wenqi Jia and Kin-Man Lam and Jun Kong},
  doi          = {10.1109/TNNLS.2022.3202835},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5211-5225},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Holistic-guided disentangled learning with cross-video semantics mining for concurrent first-person and third-person activity recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronization of stochastic neural networks using
looped-lyapunov functional and its application to secure communication.
<em>TNNLS</em>, <em>35</em>(4), 5198–5210. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to investigate the synchronization of user-controlled and uncontrolled neural networks (NNs) that exhibit chaotic solutions. The idea behind focusing on synchronization problems is to design the user-desired NNs by emulating the dynamical properties of traditional NNs rather than redefining them. Besides, instead of conventional NNs, this study considers NNs with significant factors such as time-dependent delays and uncertainties in the neural coefficients. In addition, information transmission over transmission may experience stochastic disturbances and network transmission. These factors will result in a stochastic differential NN model. Analyzing the NNs without these factors may be incompatible during the implementation. Theoretically, the model with stochastic disturbances can be considered a stochastic differential model, and the stability conditions are derived by employing Itô’s formula and appropriate integral inequalities. To achieve synchronization, the sampled-data-based control scheme is proposed because it is more effective while information is being transmitted over networks. In contrast to the existing studies, this study contributes in terms of handling stochastic disturbances, effects of time-varying delays, and uncertainties in the system parameters via looped-type Lyapunov functional. Besides this, in the application view, delayed NNs are employed as a cryptosystem that helps to secure the transmission between the sender and the receiver, which is explored by illustrating the statistical measures evaluated for the standard images. From the simulation results, the proposed control and derived sufficient conditions can provide better synchronization and the proposed delayed NNs give a better cryptosystem.},
  archive      = {J_TNNLS},
  author       = {Bhuvaneshwari Ganesan and Prakash Mani and Lakshmanan Shanmugam and Manivannan Annamalai},
  doi          = {10.1109/TNNLS.2022.3202799},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5198-5210},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of stochastic neural networks using looped-lyapunov functional and its application to secure communication},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explicit and implicit pattern relation analysis for
discovering actionable negative sequences. <em>TNNLS</em>,
<em>35</em>(4), 5183–5197. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-life events, behaviors, and interactions produce sequential data. An important but rarely explored problem is to analyze those nonoccurring (also called negative) yet important sequences, forming negative sequence analysis (NSA). A typical NSA area is to discover negative sequential patterns (NSPs) consisting of important nonoccurring and occurring elements and patterns. The limited existing work on NSP mining relies on frequentist and downward closure property-based pattern selection, producing large and highly redundant NSPs, nonactionable for business decision-making. This work makes the first attempt for actionable NSP discovery. It builds an NSP graph representation, quantifies both explicit occurrence and implicit nonoccurrence-based element and pattern relations, and then discovers significant, diverse, and informative NSPs in the NSP graph to represent the entire NSP set for discovering actionable NSPs. A DPP-based NSP representation and actionable NSP discovery method, EINSP, introduces novel and significant contributions to NSA and sequence analysis: 1) it represents NSPs by a determinantal point process (DPP)-based graph; 2) it quantifies actionable NSPs in terms of their statistical significance, diversity, and strength of explicit/implicit element/pattern relations; and 3) it models and measures both explicit and implicit element/pattern relations in the DPP-based NSP graph to represent direct and indirect couplings between NSP items, elements, and patterns. We substantially analyze the effectiveness of EINSP in terms of various theoretical and empirical aspects, including complexity, item/pattern coverage, pattern size and diversity, implicit pattern relation strength, and data factors.},
  archive      = {J_TNNLS},
  author       = {Wei Wang and Longbing Cao},
  doi          = {10.1109/TNNLS.2022.3202791},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5183-5197},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Explicit and implicit pattern relation analysis for discovering actionable negative sequences},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A convergence path to deep learning on noisy labels.
<em>TNNLS</em>, <em>35</em>(4), 5170–5182. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world machine learning classification applications, the model performance based on deep neural networks (DNNs) oftentimes suffers from label noise. Various methods have been proposed in the literature to address this issue, primarily by focusing on designing noise-tolerant loss functions, cleaning label noise, and correcting the objective loss. However, the noise-tolerant loss functions face challenges when the noise level increases. This article aims to reveal a convergence path of a trained model in the presence of label noise, and here, the convergence path depicts the evolution of a trained model over epochs. We first propose a theorem to demonstrate that any surrogate loss function can be used to learn DNNs from noisy labels. Next, theories on the general convergence path for the deep models under label noise are presented and verified through a series of experiments. In addition, we design an algorithm based on the proposed theorems that make efficient corrections on the noisy labels and achieve strong robustness in the DNN models. We designed several experiments using benchmark datasets to assess noise tolerance and verify the theorems presented in this article. The comprehensive experimental results firmly confirm our theoretical results and also clearly validate the effectiveness of our method under various levels of label noise.},
  archive      = {J_TNNLS},
  author       = {Defu Liu and Ivor W. Tsang and Guowu Yang},
  doi          = {10.1109/TNNLS.2022.3202752},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5170-5182},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A convergence path to deep learning on noisy labels},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive unfolding deraining network. <em>TNNLS</em>,
<em>35</em>(4), 5155–5169. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the fact that the degradation of image quality caused by rain usually affects outdoor vision tasks, image deraining becomes more and more important. Focusing on the single image deraining (SID) task, in this article, we propose a novel Contrastive Unfolding DEraining Network (CUDEN), which combines the traditional iterative algorithm and deep network, exhibiting excellent performance and nice interpretability. CUDEN transforms the challenge of locating rain streaks into discovering rain features and defines the relationship between the image and feature domains in terms of mapping pairs. To obtain the mapping pairs efficiently, we propose a dynamic multidomain translation (DMT) module for decomposing the original mapping into sub-mappings. To enhance the feature extraction capability of networks, we also propose a new serial multireceptive field fusion (SMF) block, which extracts complex and variable rain features with convolution kernels of different receptive fields. Moreover, we are the first to introduce contrastive learning to the SID task and combine it with perceptual loss to propose a new contrastive perceptual loss (CPL), which is quite generalized and greatly helpful in identifying the appropriate gradient descent direction during training. Extensive experiments on synthetic and real-world datasets demonstrate that our proposed CUDEN outperforms the state-of-the-art (SOTA) deraining networks.},
  archive      = {J_TNNLS},
  author       = {Zihong Huang and Jian Zhang},
  doi          = {10.1109/TNNLS.2022.3202724},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5155-5169},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Contrastive unfolding deraining network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counting crowd by weighing counts: A sequential
decision-making perspective. <em>TNNLS</em>, <em>35</em>(4), 5141–5154.
(<a href="https://doi.org/10.1109/TNNLS.2022.3202652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that crowd counting can be formulated as a sequential decision-making (SDM) problem. Inspired by human counting, we evade one-step estimation mostly executed in existing counting models and decompose counting into sequential sub-decision problems. During implementation, a key insight is to interpret sequential counting as a physical process in reality—scale weighing. This analogy allows us to implement a novel “counting scale” termed LibraNet. Our idea is that, by placing a crowd image on the scale, LibraNet (agent) learns to place appropriate weights to match the count: at each step, one weight (action) is chosen from the weight box (the predefined action pool) conditioned on the image features and the placed weights (state) until the pointer (the agent output) informs balance. We investigate two forms of state definition and explore four types of LibraNet implementations under different learning paradigms, including deep Q-network (DQN), actor-critic (AC), imitation learning (IL), and mixed AC+IL. Experiments show that LibraNet indeed mimics scale weighing, that it outperforms or performs comparably against state-of-the-art approaches on five crowd counting benchmarks, that it can be used as a plug-in to improve off-the-shelf counting models, and particularly that it demonstrates remarkable cross-dataset generalization. Code and models are available at https://git.io/libranet .},
  archive      = {J_TNNLS},
  author       = {Hao Lu and Liang Liu and Hu Wang and Zhiguo Cao},
  doi          = {10.1109/TNNLS.2022.3202652},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5141-5154},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Counting crowd by weighing counts: A sequential decision-making perspective},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eye-LRCN: A long-term recurrent convolutional network for
eye blink completeness detection. <em>TNNLS</em>, <em>35</em>(4),
5130–5140. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision syndrome causes vision problems and discomfort mainly due to dry eye. Several studies show that dry eye in computer users is caused by a reduction in the blink rate and an increase in the prevalence of incomplete blinks. In this context, this article introduces Eye-LRCN, a new eye blink detection method that also evaluates the completeness of the blink. The method is based on a long-term recurrent convolutional network (LRCN), which combines a convolutional neural network (CNN) for feature extraction with a bidirectional recurrent neural network that performs sequence learning and classifies the blinks. A Siamese architecture is used during CNN training to overcome the high-class imbalance present in blink detection and the limited amount of data available to train blink detection models. The method was evaluated on three different tasks: blink detection, blink completeness detection, and eye state detection. We report superior performance to the state-of-the-art methods in blink detection and blink completeness detection, and remarkable results in eye state detection.},
  archive      = {J_TNNLS},
  author       = {Gonzalo de la Cruz and Madalena Lira and Oscar Luaces and Beatriz Remeseiro},
  doi          = {10.1109/TNNLS.2022.3202643},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5130-5140},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Eye-LRCN: A long-term recurrent convolutional network for eye blink completeness detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid CMOS-memristor spiking neural network supporting
multiple learning rules. <em>TNNLS</em>, <em>35</em>(4), 5117–5129. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is changing the way computing is performed to cope with real-world, ill-defined tasks for which traditional algorithms fail. AI requires significant memory access, thus running into the von Neumann bottleneck when implemented in standard computing platforms. In this respect, low-latency energy-efficient in-memory computing can be achieved by exploiting emerging memristive devices, given their ability to emulate synaptic plasticity, which provides a path to design large-scale brain-inspired spiking neural networks (SNNs). Several plasticity rules have been described in the brain and their coexistence in the same network largely expands the computational capabilities of a given circuit. In this work, starting from the electrical characterization and modeling of the memristor device, we propose a neuro-synaptic architecture that co-integrates in a unique platform with a single type of synaptic device to implement two distinct learning rules, namely, the spike-timing-dependent plasticity (STDP) and the Bienenstock–Cooper–Munro (BCM). This architecture, by exploiting the aforementioned learning rules, successfully addressed two different tasks of unsupervised learning.},
  archive      = {J_TNNLS},
  author       = {Davide Florini and Daniela Gandolfi and Jonathan Mapelli and Lorenzo Benatti and Paolo Pavan and Francesco Maria Puglisi},
  doi          = {10.1109/TNNLS.2022.3202501},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5117-5129},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid CMOS-memristor spiking neural network supporting multiple learning rules},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Deep metric learning based on meta-mining strategy with
semiglobal information. <em>TNNLS</em>, <em>35</em>(4), 5103–5116. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep metric learning (DML) has achieved great success. Some existing DML methods propose adaptive sample mining strategies, which learn to weight the samples, leading to interesting performance. However, these methods suffer from a small memory (e.g., one training batch), limiting their efficacy. In this work, we introduce a data-driven method, meta-mining strategy with semiglobal information (MMSI), to apply meta-learning to learn to weight samples during the whole training, leading to an adaptive mining strategy. To introduce richer information than one training batch only, we elaborately take advantage of the validation set of meta-learning by implicitly adding additional validation sample information to training. Furthermore, motivated by the latest self-supervised learning, we introduce a dictionary (memory) that maintains very large and diverse information. Together with the validation set, this dictionary presents much richer information to the training, leading to promising performance. In addition, we propose a new theoretical framework that can formulate pairwise and tripletwise metric learning loss functions in a unified framework. This framework brings new insights to society and facilitates us to generalize our MMSI to many existing DML methods. We conduct extensive experiments on three public datasets, CUB200-2011, Cars-196, and Stanford Online Products (SOP). Results show that our method can achieve the state of the art or very competitive performance. Our source codes have been made available at https://github.com/NUST-Machine-Intelligence-Laboratory/MMSI .},
  archive      = {J_TNNLS},
  author       = {Xiruo Jiang and Sheng Liu and Xili Dai and Guosheng Hu and Xingguo Huang and Yazhou Yao and Guo-Sen Xie and Ling Shao},
  doi          = {10.1109/TNNLS.2022.3202571},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5103-5116},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep metric learning based on meta-mining strategy with semiglobal information},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative suprasphere embedding for fine-grained visual
categorization. <em>TNNLS</em>, <em>35</em>(4), 5092–5102. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great success of the existing work in fine-grained visual categorization (FGVC), there are still several unsolved challenges, e.g., poor interpretation and vagueness contribution. To circumvent this drawback, motivated by the hypersphere embedding method, we propose a discriminative suprasphere embedding (DSE) framework, which can provide intuitive geometric interpretation and effectively extract discriminative features. Specifically, DSE consists of three modules. The first module is a suprasphere embedding (SE) block, which learns discriminative information by emphasizing weight and phase. The second module is a phase activation map (PAM) used to analyze the contribution of local descriptors to the suprasphere feature representation, which uniformly highlights the object region and exhibits remarkable object localization capability. The last module is a class contribution map (CCM), which quantitatively analyzes the network classification decision and provides insight into the domain knowledge about classified objects. Comprehensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed method in comparison with state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Shuo Ye and Qinmu Peng and Wenju Sun and Jiamiao Xu and Yu Wang and Xinge You and Yiu-Ming Cheung},
  doi          = {10.1109/TNNLS.2022.3202534},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5092-5102},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative suprasphere embedding for fine-grained visual categorization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sampled-data control for exponential synchronization of
delayed inertial neural networks with aperiodic sampling and state
quantization. <em>TNNLS</em>, <em>35</em>(4), 5079–5091. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is devoted to dealing with exponential synchronization for inertial neural networks (INNs) with heterogeneous time-varying delays (HTVDs) under the framework of aperiodic sampling and state quantization. First, by taking the effect of aperiodic sampling and state quantization into consideration, a novel quantized sampled-data (QSD) controller with time-varying control gain is designed to tackle the exponential synchronization of INNs. Second, considering the available information of the lower and upper bounds of each HTVD, a refined Lyapunov–Krasovskii functional (LKF) is proposed. Meanwhile, an improved looped-functional method is utilized to fully capture the characteristic of practical sampling patterns and further relax the positive definiteness requirement for LKF. Consequently, less conservative exponential synchronization conditions with extra flexibility are derived. Finally, a numerical example is employed to demonstrate the effectiveness and advantages of the proposed synchronization method.},
  archive      = {J_TNNLS},
  author       = {Zheng You and Huaicheng Yan and Hao Zhang and Meng Wang and Kaibo Shi},
  doi          = {10.1109/TNNLS.2022.3202343},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5079-5091},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sampled-data control for exponential synchronization of delayed inertial neural networks with aperiodic sampling and state quantization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Deep reinforcement learning: A survey. <em>TNNLS</em>,
<em>35</em>(4), 5064–5078. (<a
href="https://doi.org/10.1109/TNNLS.2022.3207346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) integrates the feature representation ability of deep learning with the decision-making ability of reinforcement learning so that it can achieve powerful end-to-end learning control capabilities. In the past decade, DRL has made substantial advances in many tasks that require perceiving high-dimensional input and making optimal or near-optimal decisions. However, there are still many challenging problems in the theory and applications of DRL, especially in learning control tasks with limited samples, sparse rewards, and multiple agents. Researchers have proposed various solutions and new theories to solve these problems and promote the development of DRL. In addition, deep learning has stimulated the further development of many subfields of reinforcement learning, such as hierarchical reinforcement learning (HRL), multiagent reinforcement learning, and imitation learning. This article gives a comprehensive overview of the fundamental theories, key algorithms, and primary research domains of DRL. In addition to value-based and policy-based DRL algorithms, the advances in maximum entropy-based DRL are summarized. The future research topics of DRL are also analyzed and discussed.},
  archive      = {J_TNNLS},
  author       = {Xu Wang and Sen Wang and Xingxing Liang and Dawei Zhao and Jincai Huang and Xin Xu and Bin Dai and Qiguang Miao},
  doi          = {10.1109/TNNLS.2022.3207346},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5064-5078},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning: A survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to double-check model prediction from a causal
perspective. <em>TNNLS</em>, <em>35</em>(4), 5054–5063. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present machine learning schema typically uses a one-pass model inference (e.g., forward propagation) to make predictions in the testing phase. It is inherently different from human students who double-check the answer during examinations especially when the confidence is low. To bridge this gap, we propose a learning to double-check (L2D) framework, which formulates double check as a learnable procedure with two core operations: recognizing unreliable predictions and revising predictions. To judge the correctness of a prediction, we resort to counterfactual faithfulness in causal theory and design a contrastive faithfulness measure. In particular, L2D generates counterfactual features by imagining: “what would the sample features be if its label was the predicted class” and judges the prediction by the faithfulness of the counterfactual features. Furthermore, we design a simple and effective revision module to revise the original model prediction according to the faithfulness. We apply the L2D framework to three classification models and conduct experiments on two public datasets for image classification, validating the effectiveness of L2D in prediction correctness judgment and revision.},
  archive      = {J_TNNLS},
  author       = {Xun Deng and Fuli Feng and Xiang Wang and Xiangnan He and Hanwang Zhang and Tat-Seng Chua},
  doi          = {10.1109/TNNLS.2023.3264712},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5054-5063},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning to double-check model prediction from a causal perspective},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causality-aware predictions in static anticausal machine
learning tasks. <em>TNNLS</em>, <em>35</em>(4), 5039–5053. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a counterfactual approach to train “causality-aware” predictive models that are able to leverage causal information in static anticausal machine learning tasks (i.e., prediction tasks where the outcome influences the inputs). In applications plagued by confounding, the approach can be used to generate predictions that are free from the influence of observed confounders. In applications involving observed mediators, the approach can be used to generate predictions that only capture the direct or the indirect causal influences. Mechanistically, we train supervised learners on (counterfactually) simulated inputs that retain only the associations generated by the causal relations of interest. We focus on linear models, where analytical results connecting covariances, causal effects, and prediction mean square errors are readily available. Quite importantly, we show that our approach does not require knowledge of the full causal graph. It suffices to know which variables represent potential confounders and/or mediators. We investigate the stability of the method with respect to dataset shifts generated by selection biases and also relax the linearity assumption by extending the approach to additive models better able to account for nonlinearities in the data. We validate our approach in a series of synthetic data experiments and illustrate its application to a real dataset.},
  archive      = {J_TNNLS},
  author       = {Elias Chaibub Neto},
  doi          = {10.1109/TNNLS.2022.3202151},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5039-5053},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Causality-aware predictions in static anticausal machine learning tasks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Targeted-BEHRT: Deep learning for observational causal
inference on longitudinal electronic health records. <em>TNNLS</em>,
<em>35</em>(4), 5027–5038. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational causal inference is useful for decision-making in medicine when randomized clinical trials (RCTs) are infeasible or nongeneralizable. However, traditional approaches do not always deliver unconfounded causal conclusions in practice. The rise of “doubly robust” nonparametric tools coupled with the growth of deep learning for capturing rich representations of multimodal data offers a unique opportunity to develop and test such models for causal inference on comprehensive electronic health records (EHRs). In this article, we investigate causal modeling of an RCT-established causal association: the effect of classes of antihypertensive on incident cancer risk. We develop a transformer-based model, targeted bidirectional EHR transformer (T-BEHRT) coupled with doubly robust estimation to estimate average risk ratio (RR). We compare our model to benchmark statistical and deep learning models for causal inference in multiple experiments on semi-synthetic derivations of our dataset with various types and intensities of confounding. In order to further test the reliability of our approach, we test our model on situations of limited data. We find that our model provides more accurate estimates of relative risk [least sum absolute error (SAE) from ground truth] compared with benchmark estimations. Finally, our model provides an estimate of class-wise antihypertensive effect on cancer risk that is consistent with results derived from RCTs.},
  archive      = {J_TNNLS},
  author       = {Shishir Rao and Mohammad Mamouei and Gholamreza Salimi-Khorshidi and Yikuan Li and Rema Ramakrishnan and Abdelaali Hassaine and Dexter Canoy and Kazem Rahimi},
  doi          = {10.1109/TNNLS.2022.3183864},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5027-5038},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Targeted-BEHRT: Deep learning for observational causal inference on longitudinal electronic health records},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariate balancing methods for randomized controlled trials
are not adversarially robust. <em>TNNLS</em>, <em>35</em>(4), 5014–5026.
(<a href="https://doi.org/10.1109/TNNLS.2023.3266429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first step toward investigating the effectiveness of a treatment via a randomized trial is to split the population into control and treatment groups then compare the average response of the treatment group receiving the treatment to the control group receiving the placebo. To ensure that the difference between the two groups is caused only by the treatment, it is crucial that the control and the treatment groups have similar statistics. Indeed, the validity and reliability of a trial are determined by the similarity of two groups’ statistics. Covariate balancing methods increase the similarity between the distributions of the two groups’ covariates. However, often in practice, there are not enough samples to accurately estimate the groups’ covariate distributions. In this article, we empirically show that covariate balancing with the standardized means difference (SMD) covariate balancing measure, as well as Pocock and Simon’s sequential treatment assignment method, are susceptible to worst case treatment assignments. Worst case treatment assignments are those admitted by the covariate balance measure, but result in highest possible ATE estimation errors. We developed an adversarial attack to find adversarial treatment assignment for any given trial. Then, we provide an index to measure how close the given trial is to the worst case. To this end, we provide an optimization-based algorithm, namely adversarial treatment assignment in treatment effect trials (ATASTREET), to find the adversarial treatment assignments.},
  archive      = {J_TNNLS},
  author       = {Hossein Babaei and Sina Alemohammad and Richard G. Baraniuk},
  doi          = {10.1109/TNNLS.2023.3266429},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {5014-5026},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Covariate balancing methods for randomized controlled trials are not adversarially robust},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic causal effect estimation with global neural
network forecasting models. <em>TNNLS</em>, <em>35</em>(4), 4999–5013.
(<a href="https://doi.org/10.1109/TNNLS.2022.3190984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel method to estimate the causal effects of an intervention over multiple treated units by combining the techniques of probabilistic forecasting with global forecasting methods using deep learning (DL) models. Considering the counterfactual and synthetic approach for policy evaluation, we recast the causal effect estimation problem as a counterfactual prediction outcome of the treated units in the absence of the treatment. Nevertheless, in contrast to estimating only the counterfactual time series outcome, our work differs from conventional methods by proposing to estimate the counterfactual time series probability distribution based on the past preintervention set of treated and untreated time series. We rely on time series properties and forecasting methods, with shared parameters, applied to stacked univariate time series for causal identification. This article presents DeepProbCP, a framework for producing accurate quantile probabilistic forecasts for the counterfactual outcome, based on training a global autoregressive recurrent neural network model with conditional quantile functions on a large set of related time series. The output of the proposed method is the counterfactual outcome as the spline-based representation of the counterfactual distribution. We demonstrate how this probabilistic methodology added to the global DL technique to forecast the counterfactual trend and distribution outcomes overcomes many challenges faced by the baseline approaches to the policy evaluation problem. Oftentimes, some target interventions affect only the tails or the variance of the treated units’ distribution rather than the mean or median, which is usual for skewed or heavy-tailed distributions. Under this scenario, the classical causal effect models based on counterfactual predictions are not capable of accurately capturing or even seeing policy effects. By means of empirical evaluations of synthetic and real-world datasets, we show that our framework delivers more accurate forecasts than the state-of-the-art models, depicting, in which quantiles, the intervention most affected the treated units, unlike the conventional counterfactual inference methods based on nonprobabilistic approaches.},
  archive      = {J_TNNLS},
  author       = {Priscila Grecov and Ankitha Nandipura Prasanna and Klaus Ackermann and Sam Campbell and Debbie Scott and Dan I. Lubman and Christoph Bergmeir},
  doi          = {10.1109/TNNLS.2022.3190984},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4999-5013},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic causal effect estimation with global neural network forecasting models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is single enough? A joint spatiotemporal feature learning
framework for multivariate time series prediction. <em>TNNLS</em>,
<em>35</em>(4), 4985–4998. (<a
href="https://doi.org/10.1109/TNNLS.2022.3216107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fuzzy cognitive map (FCM) is a simple but effective tool for modeling and predicting time series. This article focuses on the problem of multivariate time series prediction (TSP), which is essential and challenging in data mining. Although several FCM-based approaches have been designed to solve this problem, their feature extraction module designed for single mode falls short in capturing the nonlinear spatiotemporal dependencies among variates, thereby resulting in low prediction accuracy in forecasting multivariate time series, which shows that the single mode learning is not enough. Therefore, in this article, we propose a joint spatiotemporal feature learning framework for multivariate TSP, where a mix-resolution spatial module consisting of multiple sparse autoencoders (SAEs) is designed to extract the feature series with different spatial resolutions, and a mix-order spatiotemporal module concluding multiple high-order FCMs (HFCMs) is designed to model the spatiotemporal dynamics of these feature series. Finally, the outputs of the two modules are concatenated to predict future values. We refer to this framework as the spatiotemporal FCM (STFCM). Especially, an efficient learning algorithm is designed to update the integral weights of STFCM based on the batch gradient descent algorithm when it deems necessary. We validate the performance of the STFCM on four real-world datasets. Compared with the existing state-of-the-art (SOTA) methods, the experimental results not only show the advantages of the two designed modules in the STFCM but also show the excellent performance of the STFCM.},
  archive      = {J_TNNLS},
  author       = {Kaixin Yuan and Kai Wu and Jing Liu},
  doi          = {10.1109/TNNLS.2022.3216107},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4985-4998},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Is single enough? a joint spatiotemporal feature learning framework for multivariate time series prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stepwise multivariate granger causality method for
constructing hierarchical directed brain functional network.
<em>TNNLS</em>, <em>35</em>(4), 4974–4984. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The directed brain functional network construction gives us the new insights into the relationships between brain regions from the causality point of view. The Granger causality analysis is one of the powerful methods to model the directed network. The complex brain network is also hierarchically constructed, which is particularly suited to facilitate segregated functions and the global integration of the segregated functions. Therefore, it is of great interest to explore new approach to model the hierarchical architecture of the directed network. In the present study, we proposed a new approach, namely, stepwise multivariate Granger causality (SMGC), considering both the directed and hierarchical features of brain functional network to explore the stepwise causal relationship in the network. The simulation study demonstrated that the diverse and complex hierarchical organization could be embedded in the apparently simple directed network. The proposed SMGC method could capture the multiple hierarchy of the directed network. When applying to the real functional magnetic resonance imaging (fMRI) datasets, the core triple resting-state networks in human brain showed within-network directed connections in the first-level directed network and rich and diverse between-network pathways in the second-level hierarchical network. The default mode network (DMN) had a prominent role in the resting-state acting as both the causal source and the important relay station. Further exploratory research on the adaption of directed hierarchical network in athletes suggested the enhanced bidirectional communication between the DMN and the central executive network (CEN) and the enhanced directed connections from the salience network (SN) to the CEN in the athlete group. The SMGC approach is capable of capturing the hierarchical architecture of the brain directed functional network, which refreshes the new stepwise causal relationship in the directed network. This might shed light on the potential application for exploring the altered hierarchical organization of brain directed network in neuropsychiatric disorders.},
  archive      = {J_TNNLS},
  author       = {Qing Gao and Ning Luo and Minfeng Liang and Weiqi Zhou and Yan Li and Rong Li and Xiaofei Hu and Ting Zou and Xuyang Wang and Jiali Yu and Jinsong Leng and Huafu Chen},
  doi          = {10.1109/TNNLS.2022.3202535},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4974-4984},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A stepwise multivariate granger causality method for constructing hierarchical directed brain functional network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Out-of-sample tuning for causal discovery. <em>TNNLS</em>,
<em>35</em>(4), 4963–4973. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery is continually being enriched with new algorithms for learning causal graphical probabilistic models. Each one of them requires a set of hyperparameters, creating a great number of combinations. Given that the true graph is unknown and the learning task is unsupervised, the challenge to a practitioner is how to tune these choices. We propose out-of-sample causal tuning (OCT) that aims to select an optimal combination. The method treats a causal model as a set of predictive models and uses out-of-sample protocols for supervised methods. This approach can handle general settings like latent confounders and nonlinear relationships. The method uses an information-theoretic approach to be able to generalize to mixed data types and a penalty for dense graphs to penalize for complexity. To evaluate OCT, we introduce a causal-based simulation method to create datasets that mimic the properties of real-world problems. We evaluate OCT against two other tuning approaches, based on stability and in-sample fitting. We show that OCT performs well in many experimental settings and it is an effective tuning method for causal discovery.},
  archive      = {J_TNNLS},
  author       = {Konstantina Biza and Ioannis Tsamardinos and Sofia Triantafillou},
  doi          = {10.1109/TNNLS.2022.3185842},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4963-4973},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Out-of-sample tuning for causal discovery},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linear deconfounded score method: Scoring DAGs with dense
unobserved confounding. <em>TNNLS</em>, <em>35</em>(4), 4948–4962. (<a
href="https://doi.org/10.1109/TNNLS.2024.3352657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the discovery of causal relations from a combination of observational data and qualitative assumptions about the nature of causality in the presence of unmeasured confounding. We focus on applications where unobserved variables are known to have a widespread effect on many of the observed ones, which makes the problem particularly difficult for constraint-based methods, because most pairs of variables are conditionally dependent given any other subset, rendering the causal effect unidentifiable. In this article, we show that under the principle of independent mechanisms, unobserved confounding in this setting leaves a statistical footprint in the observed data distribution that allows for disentangling spurious and causal effects. Using this insight, we demonstrate that a sparse linear Gaussian directed acyclic graph (DAG) among observed variables may be recovered approximately and propose a simple adjusted score-based causal discovery algorithm that may be implemented with general-purpose solvers and scales to high-dimensional problems. We find, in addition, that despite the conditions we pose to guarantee causal recovery, performance in practice is robust to large deviations in model assumptions, and extensions to nonlinear structural models are possible.},
  archive      = {J_TNNLS},
  author       = {Alexis Bellot and Mihaela van der Schaar},
  doi          = {10.1109/TNNLS.2024.3352657},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4948-4962},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Linear deconfounded score method: Scoring DAGs with dense unobserved confounding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging directed causal discovery to detect latent common
causes in cause-effect pairs. <em>TNNLS</em>, <em>35</em>(4), 4938–4947.
(<a href="https://doi.org/10.1109/TNNLS.2022.3205128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of causal relationships is a fundamental problem in science and medicine. In recent years, many elegant approaches to discovering causal relationships between two variables from observational data have been proposed. However, most of these deal only with purely directed causal relationships and cannot detect latent common causes. Here, we devise a general heuristic which takes a causal discovery algorithm that can only distinguish purely directed causal relations and modifies it to also detect latent common causes. We apply our method to two directed causal discovery algorithms, the information geometric causal inference (IGCI) of (Daniusis et al., 2010) and the kernel conditional deviance for causal inference of (Mitrovic et al., 2018), and extensively test on synthetic data—detecting latent common causes in additive, multiplicative and complex noise regimes—and on real data, where we are able to detect known common causes. In addition to detecting latent common causes, our experiments demonstrate that both the modified algorithms preserve the performance of the original in distinguishing directed causal relations.},
  archive      = {J_TNNLS},
  author       = {Ciarán M. Gilligan-Lee and Christopher Hart and Jonathan Richens and Saurabh Johri},
  doi          = {10.1109/TNNLS.2022.3205128},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4938-4947},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Leveraging directed causal discovery to detect latent common causes in cause-effect pairs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On low-rank directed acyclic graphs and causal structure
learning. <em>TNNLS</em>, <em>35</em>(4), 4924–4937. (<a
href="https://doi.org/10.1109/TNNLS.2023.3273353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite several advances in recent years, learning causal structures represented by directed acyclic graphs (DAGs) remains a challenging task in high-dimensional settings when the graphs to be learned are not sparse. In this article, we propose to exploit a low-rank assumption regarding the (weighted) adjacency matrix of a DAG causal model to help address this problem. We utilize existing low-rank techniques to adapt causal structure learning methods to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low-rank assumption. Specifically, we show that the maximum rank is highly related to hubs, suggesting that scale-free (SF) networks, which are frequently encountered in practice, tend to be low rank. Our experiments demonstrate the utility of the low-rank adaptations for a variety of data models, especially with relatively large and dense graphs. Moreover, with a validation procedure, the adaptations maintain a superior or comparable performance even when graphs are not restricted to be low rank.},
  archive      = {J_TNNLS},
  author       = {Zhuangyan Fang and Shengyu Zhu and Jiji Zhang and Yue Liu and Zhitang Chen and Yangbo He},
  doi          = {10.1109/TNNLS.2023.3273353},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4924-4937},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On low-rank directed acyclic graphs and causal structure learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal discovery on discrete data via weighted normalized
wasserstein distance. <em>TNNLS</em>, <em>35</em>(4), 4911–4923. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of causal discovery from observational data $(X,Y)$ is defined as the task of deciding whether $X$ causes $Y$ , or $Y$ causes $X$ or if there is no causal relationship between $X$ and $Y$ . Causal discovery from observational data is an important problem in many areas of science. In this study, we propose a method to address this problem when the cause-and-effect relationship is represented by a discrete additive noise model (ANM). First, assuming that $X$ causes $Y$ , we estimate the conditional distributions of the noise given $X$ using regression. Similarly, assuming that $Y$ causes $X$ , we also estimate the conditional distributions of noise given $Y$ . Based on the structural characteristics of the discrete ANM, we find that the dissimilarity of the conditional distributions of noise in the causal direction is smaller than that in the anticausal direction. Then, we propose a weighted normalized Wasserstein distance to measure the dissimilarity of the conditional distributions of noise. Finally, we propose a decision rule for casual discovery by comparing two computed weighted normalized Wasserstein distances. An empirical investigation demonstrates that our method performs well on synthetic data and outperforms state-of-the-art methods on real data.},
  archive      = {J_TNNLS},
  author       = {Yi Wei and Xiaofei Li and Lihui Lin and Dengming Zhu and Qingyong Li},
  doi          = {10.1109/TNNLS.2022.3213641},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4911-4923},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Causal discovery on discrete data via weighted normalized wasserstein distance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local independence testing for point processes.
<em>TNNLS</em>, <em>35</em>(4), 4902–4910. (<a
href="https://doi.org/10.1109/TNNLS.2023.3335265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constraint-based causal structure learning for point processes require empirical tests of local independence. Existing tests require strong model assumptions, e.g., that the true data generating model is a Hawkes process with no latent confounders. Even when restricting attention to Hawkes processes, latent confounders are a major technical difficulty because a marginalized process will generally not be a Hawkes process itself. We introduce an expansion similar to Volterra expansions as a tool to represent marginalized intensities. Our main theoretical result is that such expansions can approximate the true marginalized intensity arbitrarily well. Based on this, we propose a test of local independence and investigate its properties in real and simulated data.},
  archive      = {J_TNNLS},
  author       = {Nikolaj Thams and Niels Richard Hansen},
  doi          = {10.1109/TNNLS.2023.3335265},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4902-4910},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local independence testing for point processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IEEE transactions on neural networks and learning systems
special issue on causal discovery and causality-inspired machine
learning. <em>TNNLS</em>, <em>35</em>(4), 4899–4901. (<a
href="https://doi.org/10.1109/TNNLS.2024.3365968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality is a fundamental notion in science and engineering. It has attracted much interest across research communities in statistics, machine learning (ML), healthcare, and artificial intelligence (AI), and is becoming increasingly recognized as a vital research area. One of the fundamental problems in causality is how to find the causal structure or the underlying causal model. Accordingly, one focus of this Special Issue is on causal discovery , i.e., how can we discover causal structure over a set of variables from observational data with automated procedures? Besides learning causality, another focus is on using causality to help understand and advance ML, that is, causality-inspired ML.},
  archive      = {J_TNNLS},
  author       = {Kun Zhang and Ilya Shpitser and Sara Magliacane and Davide Bacciu and Fei Wu and Changshui Zhang and Peter Spirtes},
  doi          = {10.1109/TNNLS.2024.3365968},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4899-4901},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE transactions on neural networks and learning systems special issue on causal discovery and causality-inspired machine learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nowhere to hide: Online rumor detection based on retweeting
graph neural networks. <em>TNNLS</em>, <em>35</em>(4), 4887–4898. (<a
href="https://doi.org/10.1109/TNNLS.2022.3161697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online rumor detection is crucial for a healthier online environment. Traditional methods mainly rely on content understanding. However, these contents can be easily adjusted to avoid such supervision and are insufficient to improve the detection result. Compared with the content, information propagation patterns are more informative to support further performance promotion. Unfortunately, learning the propagation patterns is difficult, since the retweeting tree is more topologically complicated than linear sequences or binary trees. In light of this, we propose a novel rumor detection framework based on structure-aware retweeting graph neural networks. To capture the propagation patterns, we first design a novel conversion method to transform the complex retweeting tree as more tractable binary tree without losing the reconstruction information. Then, we serialize the retweeting tree as a corpus of meta-tree paths, where each meta-tree can preserve a basic substructure. A deep neural network is then designed to integrate all meta-trees and to generate the global structural embeddings. Furthermore, we propose to integrate content, users, and propagation patterns to enhance more reliable performance. To this end, we propose a novel self-attention-based retweeting neural network to learn individual features from both content and users. We then fuse the node-level features with our global structural embeddings via a mutual attention unit. In this way, we can generate more comprehensive representations for rumor detection. Extensive evaluations on two real-world datasets show remarkable superiorities of our model compared with existing methods.},
  archive      = {J_TNNLS},
  author       = {Bo Liu and Xiangguo Sun and Qing Meng and Xinyan Yang and Yang Lee and Jiuxin Cao and Junzhou Luo and Roy Ka-Wei Lee},
  doi          = {10.1109/TNNLS.2022.3161697},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4887-4898},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nowhere to hide: Online rumor detection based on retweeting graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survival analysis of high-dimensional data with graph
convolutional networks and geometric graphs. <em>TNNLS</em>,
<em>35</em>(4), 4876–4886. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a survival model based on graph convolutional networks (GCNs) with geometric graphs directly constructed from high-dimensional features. First, we clarify that the graphs used in GCNs play an important role in processing the relational information of samples, and the graphs that align well with the underlying data structure could be beneficial for survival analysis. Second, we show that sparse geometric graphs derived from high-dimensional data are more favorable compared with dense graphs when used in GCNs for survival analysis. Third, from this insight, we propose a model for survival analysis based on GCNs. By using multiple sparse geometric graphs and a proposed sequential forward floating selection algorithm, the new model is able to simultaneously perform survival analysis and unveil the local neighborhoods of samples. The experimental results on real-world datasets show that the proposed survival analysis approach based on GCNs outperforms a variety of existing methods and indicate that geometric graphs can aid survival analysis of high-dimensional data.},
  archive      = {J_TNNLS},
  author       = {Yurong Ling and Zijing Liu and Jing-Hao Xue},
  doi          = {10.1109/TNNLS.2022.3190321},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4876-4886},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Survival analysis of high-dimensional data with graph convolutional networks and geometric graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional connectivity prediction with deep learning for
graph transformation. <em>TNNLS</em>, <em>35</em>(4), 4862–4875. (<a
href="https://doi.org/10.1109/TNNLS.2022.3197337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring resting-state functional connectivity (FC) from anatomical brain wiring, known as structural connectivity (SC), is of enormous significance in neuroscience for understanding biological neuronal networks and treating mental diseases. Both SC and FC are networks where the nodes are brain regions, and in SC, the edges are the physical fiber nerves among the nodes, while in FC, the edges are the nodes’ coactivation relations. Despite the importance of SC and FC, until very recently, the rapidly growing research body on this topic has generally focused on either linear models or computational models that rely heavily on heuristics and simple assumptions regarding the mapping between FC and SC. However, the relationship between FC and SC is actually highly nonlinear and complex and contains considerable randomness; additional factors, such as the subject’s age and health, can also significantly impact the SC-FC relationship and hence cannot be ignored. To address these challenges, here, we develop a novel SC-to-FC generative adversarial network (SF-GAN) framework for mapping SC to FC, along with additional metafeatures based on a newly proposed graph neural network-based generative model that is capable of learning the stochasticity. Specifically, a new graph-based conditional generative adversarial nets model is proposed, where edge convolution layers are leveraged to encode the graph patterns in the SC in the form of a graph representation. New edge deconvolution layers are then utilized to decode the representation back to FC. Additional metafeatures of subjects’ profile information are integrated into the graph representation with newly designed sparse-regularized layers that can automatically select features that impact FC. Finally, we have also proposed new post hoc explainer of our SF-GAN, which can identify which subgraphs in SC strongly influence which subgraphs in FC by a new multilevel edge-correlation-guided graph clustering problem. The results of experiments conducted to test the new model confirm that it significantly outperforms existing state-of-the-art methods, with additional interpretability for identifying important metafeatures and subgraphs.},
  archive      = {J_TNNLS},
  author       = {Negar Etemadyrad and Yuyang Gao and Qingzhe Li and Xiaojie Guo and Frank Krueger and Qixiang Lin and Deqiang Qiu and Liang Zhao},
  doi          = {10.1109/TNNLS.2022.3197337},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4862-4875},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Functional connectivity prediction with deep learning for graph transformation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometry-based molecular generation with deep constrained
variational autoencoder. <em>TNNLS</em>, <em>35</em>(4), 4852–4861. (<a
href="https://doi.org/10.1109/TNNLS.2022.3147790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding target molecules with specific chemical properties plays a decisive role in drug development. We proposed GEOM-CVAE, a constrained variational autoencoder based on geometric representation for molecular generation with specific properties, which is protein-context-dependent. In terms of machine learning, it includes continuous feature embedding encoder and molecular generation decoder. Our key contribution is to propose an efficient geometric embedding method, including the spatial structure representations of drug molecule (converting the 3-D coordinates into image) and the geometric graph representations of protein target (modeling the protein surface as a mesh). The 3-D geometric information is vital to successful molecular generation, which is different from previous molecular generative methods based on 1-D or 2-D. Our model framework generates specific molecules in two phases, by first generating special image with molecular 3-D information to learn latent representations and generating molecules with constrained condition based on geometric graph convolution for specific protein and then inputting the generated structural molecules into a parser network for obtaining Simplified Molecular Input Line Entry System (SMILES) strings. Our model achieves competitive performance that implies its potential effectiveness to enable the exploration of the vast chemical space for drug discovery.},
  archive      = {J_TNNLS},
  author       = {Chunyan Li and Junfeng Yao and Wei Wei and Zhangming Niu and Xiangxiang Zeng and Jin Li and Jianmin Wang},
  doi          = {10.1109/TNNLS.2022.3147790},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4852-4861},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Geometry-based molecular generation with deep constrained variational autoencoder},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Graph-based contrastive learning for description and
detection of local features. <em>TNNLS</em>, <em>35</em>(4), 4839–4851.
(<a href="https://doi.org/10.1109/TNNLS.2022.3208837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confronted with the task environment full of repetitive textures, the state-of-art description and detection methods for local features greatly suffer from the “pseudo-negatives,” bringing inconsistent optimization objectives during training. To address this problem, this article develops a self-supervised graph-based contrastive learning framework to train the model for local features, GCLFeat. The proposed approach learns to alleviate the pseudo-negatives specifically from three aspects: 1) designing a graph neural network (GNN), which focuses on mining the local transformational invariance across different views and global textual knowledge within individual images; 2) generating the dense correspondence annotations from a diverse natural dataset with a self-supervised paradigm; and 3) adopting a keypoints-aware sampling strategy to compute the loss across the whole dataset. The experimental results show that the unsupervised framework outperforms the state-of-the-art supervised baselines on diverse downstream benchmarks including image matching, 3-D reconstruction and visual localization. The code will be made public and available at https://github.com/RealZihaoWang/GCLFeat .},
  archive      = {J_TNNLS},
  author       = {Zihao Wang and Zhen Li and Xueyi Li and Wenjie Chen and Xiangdong Liu},
  doi          = {10.1109/TNNLS.2022.3208837},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4839-4851},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-based contrastive learning for description and detection of local features},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mutually supervised graph attention network for few-shot
segmentation: The perspective of fully utilizing limited samples.
<em>TNNLS</em>, <em>35</em>(4), 4826–4838. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully supervised semantic segmentation has performed well in many computer vision tasks. However, it is time-consuming because training a model requires a large number of pixel-level annotated samples. Few-shot segmentation has recently become a popular approach to addressing this problem, as it requires only a handful of annotated samples to generalize to new categories. However, the full utilization of limited samples remains an open problem. Thus, in this article, a mutually supervised few-shot segmentation network is proposed. First, the feature maps from intermediate convolution layers are fused to enrich the capacity of feature representation. Second, the support image and query image are combined into a bipartite graph, and the graph attention network is adopted to avoid losing spatial information and increase the number of pixels in the support image to guide the query image segmentation. Third, the attention map of the query image is used as prior information to enhance the support image segmentation, which forms a mutually supervised regime. Finally, the attention maps of the intermediate layers are fused and sent into the graph reasoning layer to infer the pixel categories. Experiments are conducted on the PASCAL VOC- $5^{i}$ dataset and FSS-1000 dataset, and the results demonstrate the effectiveness and superior performance of our method compared with other baseline methods.},
  archive      = {J_TNNLS},
  author       = {Honghao Gao and Junsheng Xiao and Yuyu Yin and Tong Liu and Jiangang Shi},
  doi          = {10.1109/TNNLS.2022.3155486},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4826-4838},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A mutually supervised graph attention network for few-shot segmentation: The perspective of fully utilizing limited samples},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-graph attention convolution network for 3-d point cloud
classification. <em>TNNLS</em>, <em>35</em>(4), 4813–4825. (<a
href="https://doi.org/10.1109/TNNLS.2022.3162301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional point cloud classification is fundamental but still challenging in 3-D vision. Existing graph-based deep learning methods fail to learn both low-level extrinsic and high-level intrinsic features together. These two levels of features are critical to improving classification accuracy. To this end, we propose a dual-graph attention convolution network (DGACN). The idea of DGACN is to use two types of graph attention convolution operations with a feedback graph feature fusion mechanism. Specifically, we exploit graph geometric attention convolution to capture low-level extrinsic features in 3-D space. Furthermore, we apply graph embedding attention convolution to learn multiscale low-level extrinsic and high-level intrinsic fused graph features together. Moreover, the points belonging to different parts in real-world 3-D point cloud objects are distinguished, which results in more robust performance for 3-D point cloud classification tasks than other competitive methods, in practice. Our extensive experimental results show that the proposed network achieves state-of-the-art performance on both the synthetic ModelNet40 and real-world ScanObjectNN datasets.},
  archive      = {J_TNNLS},
  author       = {Chang-Qin Huang and Fan Jiang and Qiong-Hao Huang and Xi-Zhe Wang and Zhong-Mei Han and Wei-Yu Huang},
  doi          = {10.1109/TNNLS.2022.3162301},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4813-4825},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual-graph attention convolution network for 3-D point cloud classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel local–global graph convolutional method for point
cloud semantic segmentation. <em>TNNLS</em>, <em>35</em>(4), 4798–4812.
(<a href="https://doi.org/10.1109/TNNLS.2022.3155282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although convolutional neural networks (CNNs) have shown good performance on grid data, they are limited in the semantic segmentation of irregular point clouds. This article proposes a novel and effective graph CNN framework, referred to as the local–global graph convolutional method (LGGCM), which can achieve short- and long-range dependencies on point clouds. The key to this framework is the design of local spatial attention convolution (LSA-Conv). The design includes two parts: generating a weighted adjacency matrix of the local graph composed of neighborhood points, and updating and aggregating the features of nodes to obtain the spatial geometric features of the local point cloud. In addition, a smooth module for central points is incorporated into the process of LSA-Conv to enhance the robustness of the convolution against noise interference by adjusting the position coordinates of the points adaptively. The learned robust LSA-Conv features are then fed into a global spatial attention module with the gated unit to extract long-range contextual information and dynamically adjust the weights of features from different stages. The proposed framework, consisting of both encoding and decoding branches, is an end-to-end trainable network for semantic segmentation of 3-D point clouds. The theoretical analysis of the approximation capabilities of LSA-Conv is discussed to determine whether the features of the point cloud can be accurately represented. Experimental results on challenging benchmarks of the 3-D point cloud demonstrate that the proposed framework achieves excellent performance.},
  archive      = {J_TNNLS},
  author       = {Zijin Du and Hailiang Ye and Feilong Cao},
  doi          = {10.1109/TNNLS.2022.3155282},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4798-4812},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel Local–Global graph convolutional method for point cloud semantic segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusing higher-order features in graph neural networks for
skeleton-based action recognition. <em>TNNLS</em>, <em>35</em>(4),
4783–4797. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton sequences are lightweight and compact and thus are ideal candidates for action recognition on edge devices. Recent skeleton-based action recognition methods extract features from 3-D joint coordinates as spatial–temporal cues, using these representations in a graph neural network for feature fusion to boost recognition performance. The use of first- and second-order features, that is, joint and bone representations, has led to high accuracy. Nonetheless, many models are still confused by actions that have similar motion trajectories. To address these issues, we propose fusing higher-order features in the form of angular encoding (AGE) into modern architectures to robustly capture the relationships between joints and body parts. This simple fusion with popular spatial–temporal graph neural networks achieves new state-of-the-art accuracy in two large benchmarks, including NTU60 and NTU120, while employing fewer parameters and reduced run time. Our source code is publicly available at: https://github.com/ZhenyueQin/Angular-Skeleton-Encoding .},
  archive      = {J_TNNLS},
  author       = {Zhenyue Qin and Yang Liu and Pan Ji and Dongwoo Kim and Lei Wang and R. I. McKay and Saeed Anwar and Tom Gedeon},
  doi          = {10.1109/TNNLS.2022.3201518},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4783-4797},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fusing higher-order features in graph neural networks for skeleton-based action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Online multi-agent forecasting with interpretable
collaborative graph neural networks. <em>TNNLS</em>, <em>35</em>(4),
4768–4782. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers predicting future statuses of multiple agents in an online fashion by exploiting dynamic interactions in the system. We propose a novel collaborative prediction unit (CoPU), which aggregates the predictions from multiple collaborative predictors according to a collaborative graph. Each collaborative predictor is trained to predict the agent status by integrating the impact of another agent. The edge weights of the collaborative graph reflect the importance of each predictor. The collaborative graph is adjusted online by multiplicative update, which can be motivated by minimizing an explicit objective. With this objective, we also conduct regret analysis to indicate that, along with training, our CoPU achieves similar performance with the best individual collaborative predictor in hindsight. This theoretical interpretability distinguishes our method from many other graph networks. To progressively refine predictions, multiple CoPUs are stacked to form a collaborative graph neural network. Extensive experiments are conducted on three tasks: online simulated trajectory prediction, online human motion prediction, and online traffic speed prediction, and our methods outperform state-of-the-art works on the three tasks by 28.6%, 17.4%, and 21.0% on average, respectively; in addition, the proposed CoGNNs have lower average time costs in one online training/testing iteration than most previous methods.},
  archive      = {J_TNNLS},
  author       = {Maosen Li and Siheng Chen and Yanning Shen and Genjia Liu and Ivor W. Tsang and Ya Zhang},
  doi          = {10.1109/TNNLS.2022.3152251},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4768-4782},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online multi-agent forecasting with interpretable collaborative graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph convolutional network discrete hashing for cross-modal
retrieval. <em>TNNLS</em>, <em>35</em>(4), 4756–4767. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of deep neural networks, cross-modal hashing has made great progress. However, the information of different types of data is asymmetrical, that is to say, if the resolution of an image is high enough, it can reproduce almost 100% of the real-world scenes. However, text usually carries personal emotion and it is not objective enough, so we generally think that the information of image will be much richer than text. Although most of the existing methods unify the semantic feature extraction and hash function learning modules for end-to-end learning, they ignore this issue and do not use information-rich modalities to support information-poor modalities, leading to suboptimal results, although they unify the semantic feature extraction and hash function learning modules for end-to-end learning. Furthermore, previous methods learn hash functions in a relaxed way that causes nontrivial quantization losses. To address these issues, we propose a new method called graph convolutional network (GCN) discrete hashing. This method uses a GCN to bridge the information gap between different types of data. The GCN can represent each label as word embedding, with the embedding regarded as a set of interdependent object classifiers. From these classifiers, we can obtain predicted labels to enhance feature representations across modalities. In addition, we use an efficient discrete optimization strategy to learn the discrete binary codes without relaxation. Extensive experiments conducted on three commonly used datasets demonstrate that our proposed method graph convolutional network-based discrete hashing (GCDH) outperforms the current state-of-the-art cross-modal hashing methods.},
  archive      = {J_TNNLS},
  author       = {Cong Bai and Chao Zeng and Qing Ma and Jinglin Zhang},
  doi          = {10.1109/TNNLS.2022.3174970},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4756-4767},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph convolutional network discrete hashing for cross-modal retrieval},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial danger identification on temporally dynamic
graphs. <em>TNNLS</em>, <em>35</em>(4), 4744–4755. (<a
href="https://doi.org/10.1109/TNNLS.2023.3252175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting plays an increasingly critical role in various applications, such as power management, smart cities, finance, and healthcare. Recent advances in temporal graph neural networks (GNNs) have shown promising results in multivariate time series forecasting due to their ability to characterize high-dimensional nonlinear correlations and temporal patterns. However, the vulnerability of deep neural networks (DNNs) constitutes serious concerns about using these models to make decisions in real-world applications. Currently, how to defend multivariate forecasting models, especially temporal GNNs, is overlooked. The existing adversarial defense studies are mostly in static and single-instance classification domains, which cannot apply to forecasting due to the generalization challenge and the contradiction issue. To bridge this gap, we propose an adversarial danger identification method for temporally dynamic graphs to effectively protect GNN-based forecasting models. Our method consists of three steps: 1) a hybrid GNN-based classifier to identify dangerous times; 2) approximate linear error propagation to identify the dangerous variates based on the high-dimensional linearity of DNNs; and 3) a scatter filter controlled by the two identification processes to reform time series with reduced feature erasure. Our experiments, including four adversarial attack methods and four state-of-the-art forecasting models, demonstrate the effectiveness of the proposed method in defending forecasting models against adversarial attacks.},
  archive      = {J_TNNLS},
  author       = {Fuqiang Liu and Jingbo Tian and Luis Miranda-Moreno and Lijun Sun},
  doi          = {10.1109/TNNLS.2023.3252175},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4744-4755},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial danger identification on temporally dynamic graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SiReN: Sign-aware recommendation using graph neural
networks. <em>TNNLS</em>, <em>35</em>(4), 4729–4743. (<a
href="https://doi.org/10.1109/TNNLS.2022.3175772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many recommender systems using network embedding (NE) such as graph neural networks (GNNs) have been extensively studied in the sense of improving recommendation accuracy. However, such attempts have focused mostly on utilizing only the information of positive user–item interactions with high ratings. Thus, there is a challenge on how to make use of low rating scores for representing users’ preferences since low ratings can be still informative in designing NE-based recommender systems. In this study, we present Si ReN, a new Si gn-aware Re commender system based on GN N models. Specifically, SiReN has three key components: 1) constructing a signed bipartite graph for more precisely representing users’ preferences, which is split into two edge-disjoint graphs with positive and negative edges each; 2) generating two embeddings for the partitioned graphs with positive and negative edges via a GNN model and a multilayer perceptron (MLP), respectively, and then using an attention model to obtain the final embeddings; and 3) establishing a sign-aware Bayesian personalized ranking (BPR) loss function in the process of optimization. Through comprehensive experiments, we empirically demonstrate that SiReN consistently outperforms state-of-the-art NE-aided recommendation methods.},
  archive      = {J_TNNLS},
  author       = {Changwon Seo and Kyeong-Joong Jeong and Sungsu Lim and Won-Yong Shin},
  doi          = {10.1109/TNNLS.2022.3175772},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4729-4743},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SiReN: Sign-aware recommendation using graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal incremental graph convolution for recommender system
retraining. <em>TNNLS</em>, <em>35</em>(4), 4718–4728. (<a
href="https://doi.org/10.1109/TNNLS.2022.3156066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-world recommender system needs to be regularly retrained to keep with the new data. In this work, we consider how to efficiently retrain graph convolution network (GCN)-based recommender models that are state-of-the-art techniques for the collaborative recommendation. To pursue high efficiency, we set the target as using only new data for model updating, meanwhile not sacrificing the recommendation accuracy compared with full model retraining. This is nontrivial to achieve since the interaction data participates in both the graph structure for model construction and the loss function for model learning, whereas the old graph structure is not allowed to use in model updating. Toward the goal, we propose a causal incremental graph convolution (IGC) approach, which consists of two new operators named IGC and colliding effect distillation (CED) to estimate the output of full graph convolution. In particular, we devise simple and effective modules for IGC to ingeniously combine the old representations and the incremental graph and effectively fuse the long- and short-term preference signals. CED aims to avoid the out-of-date issue of inactive nodes that are not in the incremental graph, which connects the new data with inactive nodes through causal inference. In particular, CED estimates the causal effect of new data on the representation of inactive nodes through the control of their collider. Extensive experiments on three real-world datasets demonstrate both accuracy gains and significant speed-ups over the existing retraining mechanism.},
  archive      = {J_TNNLS},
  author       = {Sihao Ding and Fuli Feng and Xiangnan He and Yong Liao and Jun Shi and Yongdong Zhang},
  doi          = {10.1109/TNNLS.2022.3156066},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4718-4728},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Causal incremental graph convolution for recommender system retraining},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hawk: Rapid android malware detection through heterogeneous
graph attention networks. <em>TNNLS</em>, <em>35</em>(4), 4703–4717. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Android is undergoing unprecedented malicious threats daily, but the existing methods for malware detection often fail to cope with evolving camouflage in malware. To address this issue, we present Hawk, a new malware detection framework for evolutionary Android applications. We model Android entities and behavioral relationships as a heterogeneous information network (HIN), exploiting its rich semantic meta-structures for specifying implicit higher order relationships. An incremental learning model is created to handle the applications that manifest dynamically, without the need for reconstructing the whole HIN and the subsequent embedding model. The model can pinpoint rapidly the proximity between a new application and existing in-sample applications and aggregate their numerical embeddings under various semantics. Our experiments examine more than 80 860 malicious and 100 375 benign applications developed over a period of seven years, showing that Hawk achieves the highest detection accuracy against baselines and takes only 3.5 ms on average to detect an out-of-sample application, with the accelerated training time of $50\times $ faster than the existing approach.},
  archive      = {J_TNNLS},
  author       = {Yiming Hei and Renyu Yang and Hao Peng and Lihong Wang and Xiaolin Xu and Jianwei Liu and Hong Liu and Jie Xu and Lichao Sun},
  doi          = {10.1109/TNNLS.2021.3105617},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4703-4717},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hawk: Rapid android malware detection through heterogeneous graph attention networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey on community detection with deep
learning. <em>TNNLS</em>, <em>35</em>(4), 4682–4702. (<a
href="https://doi.org/10.1109/TNNLS.2021.3137396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting a community in a network is a matter of discerning the distinct features and connections of a group of members that are different from those in other communities. The ability to do this is of great significance in network analysis. However, beyond the classic spectral clustering and statistical inference methods, there have been significant developments with deep learning techniques for community detection in recent years—particularly when it comes to handling high-dimensional network data. Hence, a comprehensive review of the latest progress in community detection through deep learning is timely. To frame the survey, we have devised a new taxonomy covering different state-of-the-art methods, including deep learning models based on deep neural networks (DNNs), deep nonnegative matrix factorization, and deep sparse filtering. The main category, i.e., DNNs, is further divided into convolutional networks, graph attention networks, generative adversarial networks, and autoencoders. The popular benchmark datasets, evaluation metrics, and open-source implementations to address experimentation settings are also summarized. This is followed by a discussion on the practical applications of community detection in various domains. The survey concludes with suggestions of challenging topics that would make for fruitful future research directions in this fast-growing deep learning field.},
  archive      = {J_TNNLS},
  author       = {Xing Su and Shan Xue and Fanzhen Liu and Jia Wu and Jian Yang and Chuan Zhou and Wenbin Hu and Cecile Paris and Surya Nepal and Di Jin and Quan Z. Sheng and Philip S. Yu},
  doi          = {10.1109/TNNLS.2021.3137396},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4682-4702},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A comprehensive survey on community detection with deep learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural networks for graph drawing. <em>TNNLS</em>,
<em>35</em>(4), 4668–4681. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph drawing techniques have been developed in the last few years with the purpose of producing esthetically pleasing node-link layouts. Recently, the employment of differentiable loss functions has paved the road to the massive usage of gradient descent and related optimization algorithms. In this article, we propose a novel framework for the development of Graph Neural Drawers (GNDs), machines that rely on neural computation for constructing efficient and complex maps. GND is Graph Neural Networks (GNNs) whose learning process can be driven by any provided loss function, such as the ones commonly employed in Graph Drawing. Moreover, we prove that this mechanism can be guided by loss functions computed by means of feedforward neural networks, on the basis of supervision hints that express beauty properties, like the minimization of crossing edges. In this context, we show that GNNs can nicely be enriched by positional features to deal also with unlabeled vertexes. We provide a proof-of-concept by constructing a loss function for the edge crossing and provide quantitative and qualitative comparisons among different GNN models working under the proposed framework.},
  archive      = {J_TNNLS},
  author       = {Matteo Tiezzi and Gabriele Ciravegna and Marco Gori},
  doi          = {10.1109/TNNLS.2022.3184967},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4668-4681},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph neural networks for graph drawing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On representation knowledge distillation for graph neural
networks. <em>TNNLS</em>, <em>35</em>(4), 4656–4667. (<a
href="https://doi.org/10.1109/TNNLS.2022.3223018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) is a learning paradigm for boosting resource-efficient graph neural networks (GNNs) using more expressive yet cumbersome teacher models. Past work on distillation for GNNs proposed the local structure preserving (LSP) loss, which matches local structural relationships defined over edges across the student and teacher’s node embeddings. This article studies whether preserving the global topology of how the teacher embeds graph data can be a more effective distillation objective for GNNs, as real-world graphs often contain latent interactions and noisy edges. We propose graph contrastive representation distillation (G-CRD), which uses contrastive learning to implicitly preserve global topology by aligning the student node embeddings to those of the teacher in a shared representation space. Additionally, we introduce an expanded set of benchmarks on large-scale real-world datasets where the performance gap between teacher and student GNNs is non-negligible. Experiments across four datasets and 14 heterogeneous GNN architectures show that G-CRD consistently boosts the performance and robustness of lightweight GNNs, outperforming LSP (and a global structure preserving (GSP) variant of LSP) as well as baselines from 2-D computer vision. An analysis of the representational similarity among teacher and student embedding spaces reveals that G-CRD balances preserving local and global relationships, while structure preserving approaches are best at preserving one or the other.},
  archive      = {J_TNNLS},
  author       = {Chaitanya K. Joshi and Fayao Liu and Xu Xun and Jie Lin and Chuan Sheng Foo},
  doi          = {10.1109/TNNLS.2022.3223018},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4656-4667},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On representation knowledge distillation for graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A meta-learning approach for training explainable graph
neural networks. <em>TNNLS</em>, <em>35</em>(4), 4647–4655. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the degree of explainability of graph neural networks (GNNs). The existing explainers work by finding global/local subgraphs to explain a prediction, but they are applied after a GNN has already been trained. Here, we propose a meta-explainer for improving the level of explainability of a GNN directly at training time, by steering the optimization procedure toward minima that allow post hoc explainers to achieve better results, without sacrificing the overall accuracy of GNN. Our framework (called MATE, MetA-Train to Explain) jointly trains a model to solve the original task, e.g., node classification, and to provide easily processable outputs for downstream algorithms that explain the model’s decisions in a human-friendly way. In particular, we meta-train the model’s parameters to quickly minimize the error of an instance-level GNNExplainer trained on-the-fly on randomly sampled nodes. The final internal representation relies on a set of features that can be “better” understood by an explanation algorithm, e.g., another instance of GNNExplainer. Our model-agnostic approach can improve the explanations produced for different GNN architectures and use any instance-based explainer to drive this process. Experiments on synthetic and real-world datasets for node and graph classification show that we can produce models that are consistently easier to explain by different algorithms. Furthermore, this increase in explainability comes at no cost to the accuracy of the model.},
  archive      = {J_TNNLS},
  author       = {Indro Spinelli and Simone Scardapane and Aurelio Uncini},
  doi          = {10.1109/TNNLS.2022.3171398},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4647-4655},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A meta-learning approach for training explainable graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DyGCN: Efficient dynamic graph embedding with graph
convolutional network. <em>TNNLS</em>, <em>35</em>(4), 4635–4646. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding, aiming to learn low-dimensional representations (aka. embeddings) of nodes in graphs, has received significant attention. In recent years, there has been a surge of efforts, among which graph convolutional networks (GCNs) have emerged as an effective class of models. However, these methods mainly focus on the static graph embedding. In the present work, an efficient dynamic graph embedding approach is proposed, called dynamic GCN (DyGCN), which is an extension of the GCN-based methods. The embedding propagation scheme of GCN is naturally generalized to a dynamic setting in an efficient manner, which propagates the change in topological structure and neighborhood embeddings along the graph to update the node embeddings. The most affected nodes are updated first, and then their changes are propagated to further nodes, which in turn are updated. Extensive experiments on various dynamic graphs showed that the proposed model can update the node embeddings in a time-saving and performance-preserving way.},
  archive      = {J_TNNLS},
  author       = {Zeyu Cui and Zekun Li and Shu Wu and Xiaoyu Zhang and Qiang Liu and Liang Wang and Mengmeng Ai},
  doi          = {10.1109/TNNLS.2022.3185527},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4635-4646},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DyGCN: Efficient dynamic graph embedding with graph convolutional network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coarse-to-fine contrastive learning on graphs.
<em>TNNLS</em>, <em>35</em>(4), 4622–4634. (<a
href="https://doi.org/10.1109/TNNLS.2022.3228556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the impressive success of contrastive learning (CL), a variety of graph augmentation strategies have been employed to learn node representations in a self-supervised manner. Existing methods construct the contrastive samples by adding perturbations to the graph structure or node attributes. Although impressive results are achieved, it is rather blind to the wealth of prior information assumed: with the increase of the perturbation degree applied on the original graph: 1) the similarity between the original graph and the generated augmented graph gradually decreases and 2) the discrimination between all nodes within each augmented view gradually increases. In this article, we argue that both such prior information can be incorporated (differently) into the CL paradigm following our general ranking framework. In particular, we first interpret CL as a special case of learning to rank (L2R), which inspires us to leverage the ranking order among positive augmented views. Meanwhile, we introduce a self-ranking paradigm to ensure that the discriminative information among different nodes can be maintained and also be less altered to the perturbations of different degrees. Experiment results on various benchmark datasets verify the effectiveness of our algorithm compared with the supervised and unsupervised models.},
  archive      = {J_TNNLS},
  author       = {Peiyao Zhao and Yuangang Pan and Xin Li and Xu Chen and Ivor W. Tsang and Lejian Liao},
  doi          = {10.1109/TNNLS.2022.3228556},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4622-4634},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coarse-to-fine contrastive learning on graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-aware prototypical neural process for few-shot
graph classification. <em>TNNLS</em>, <em>35</em>(4), 4607–4621. (<a
href="https://doi.org/10.1109/TNNLS.2022.3173318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph classification plays an important role in a wide range of applications from biological prediction to social analysis. Traditional graph classification models built on graph kernels are hampered by the challenge of poor generalization as they are heavily dependent on the dedicated design of handcrafted features. Recently, graph neural networks (GNNs) become a new class of tools for analyzing graph data and have achieved promising performance. However, it is necessary to collect a large number of labeled graph data for training an accurate GNN, which is often unaffordable in real-world applications. Therefore, it is an open question to build GNNs under the condition of few-shot learning where only a few labeled graphs are available. In this article, we introduce a new Structure-aware Prototypical Neural Process (SPNP for short) for a few-shot graph classification. Specifically, at the encoding stage, SPNP first employs GNNs to capture graph structure information. Then, SPNP incorporates such structural priors into the latent path and the deterministic path for representing stochastic processes. At the decoding stage, SPNP uses a new prototypical decoder to define a metric space where unseen graphs can be predicted effectively. The proposed decoder, which contains a self-attention mechanism to learn the intraclass dependence between graphs, can enhance the class-level representations, especially for new classes. Furthermore, benefited from such a flexible encoding-decoding architecture, SPNP can directly map the context samples to a predictive distribution without any complicated operations used in previous methods. Extensive experiments demonstrate that SPNP achieves consistent and significant improvements over state-of-the-art methods. Further discussions are provided toward model efficiency and more detailed analysis.},
  archive      = {J_TNNLS},
  author       = {Xixun Lin and Zhao Li and Peng Zhang and Luchen Liu and Chuan Zhou and Bin Wang and Zhihong Tian},
  doi          = {10.1109/TNNLS.2022.3173318},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4607-4621},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Structure-aware prototypical neural process for few-shot graph classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-level graph neural network. <em>TNNLS</em>,
<em>35</em>(4), 4593–4606. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are recently proposed neural network structures for the processing of graph-structured data. Due to their employed neighbor aggregation strategy, existing GNNs focus on capturing node-level information and neglect high-level information. Existing GNNs, therefore, suffer from representational limitations caused by the local permutation invariance (LPI) problem. To overcome these limitations and enrich the features captured by GNNs, we propose a novel GNN framework, referred to as the two-level GNN (TL-GNN). This merges subgraph-level information with node-level information. Moreover, we provide a mathematical analysis of the LPI problem, which demonstrates that subgraph-level information is beneficial to overcoming the problems associated with LPI. A subgraph counting method based on the dynamic programming algorithm is also proposed, and this has the time complexity of $O(n^{3})$ , where $n$ is the number of nodes of a graph. Experiments show that TL-GNN outperforms existing GNNs and achieves state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Xing Ai and Chengyu Sun and Zhihong Zhang and Edwin R. Hancock},
  doi          = {10.1109/TNNLS.2022.3144343},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4593-4606},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two-level graph neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Status-aware signed heterogeneous network embedding with
graph neural networks. <em>TNNLS</em>, <em>35</em>(4), 4580–4592. (<a
href="https://doi.org/10.1109/TNNLS.2022.3151046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world applications are inherently modeled as signed heterogeneous networks or graphs with positive and negative links. Signed graph embedding embeds rich structural and semantic information of a signed graph into low-dimensional node representations. Existing methods usually exploit social structural balance theory to capture the semantics of the complex structure in a signed graph. These methods either omit the node features or may discard the direction information of the links. To address these issues, we propose a new framework, called a status-aware graph neural network (S-GNN), to boost the representation learning performance. S-GNN is equipped with a loss function designed based on status theory, a social-psychological theory specifically developed for directed signed graphs. Extensive experimental results on benchmarking datasets verified that S-GNN can distill comprehensive information ingrained in a signed graph in the embedding space. Specifically, S-GNN achieves state-of-the-art accuracy, robustness, and scalability: it speeds up the processing time of link sign prediction by up to $6.5 \times $ and increases accuracy by up to 18.8% as compared with the alternatives. We also show that S-GNN can obtain effective status scores of nodes for link sign prediction and node ranking tasks, both of which yield state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Wanyu Lin and Baochun Li},
  doi          = {10.1109/TNNLS.2022.3151046},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4580-4592},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Status-aware signed heterogeneous network embedding with graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transition propagation graph neural networks for temporal
networks. <em>TNNLS</em>, <em>35</em>(4), 4567–4579. (<a
href="https://doi.org/10.1109/TNNLS.2022.3220548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers of temporal networks (e.g., social networks and transaction networks) have been interested in mining dynamic patterns of nodes from their diverse interactions. Inspired by recently powerful graph mining methods like skip-gram models and graph neural networks (GNNs), existing approaches focus on generating temporal node embeddings sequentially with nodes’ sequential interactions. However, the sequential modeling of previous approaches cannot handles the transition structure between nodes’ neighbors with limited memorization capacity. In detail, an effective method for the transition structures is required to both model nodes’ personalized patterns adaptively and capture node dynamics accordingly. In this article, we propose a method, namely t ransition p ropagation g raph n eural n etworks (TIP-GNN), to tackle the challenges of encoding nodes’ transition structures. The proposed TIP-GNN focuses on the bilevel graph structure in temporal networks: besides the explicit interaction graph, a node’s sequential interactions can also be constructed as a transition graph. Based on the bilevel graph, TIP-GNN further encodes transition structures by multistep transition propagation and distills information from neighborhoods by a bilevel graph convolution. Experimental results over various temporal networks reveal the efficiency of our TIP-GNN, with at most 7.2% improvements of accuracy on temporal link prediction. Extensive ablation studies further verify the effectiveness and limitations of the transition propagation module. Our code is available at https://github.com/doujiang-zheng/TIP-GNN .},
  archive      = {J_TNNLS},
  author       = {Tongya Zheng and Zunlei Feng and Tianli Zhang and Yunzhi Hao and Mingli Song and Xingen Wang and Xinyu Wang and Ji Zhao and Chun Chen},
  doi          = {10.1109/TNNLS.2022.3220548},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4567-4579},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transition propagation graph neural networks for temporal networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CapMax: A framework for dynamic network representation
learning from the view of multiuser communication. <em>TNNLS</em>,
<em>35</em>(4), 4554–4566. (<a
href="https://doi.org/10.1109/TNNLS.2022.3222165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a modified mutual information maximization (InfoMax) framework, named channel capacity maximization (CapMax), is proposed and applied to learn informative representations for dynamic networks with time-varying topology and/or time-evolving node attributes. The CapMax is based on the network information theory for multiuser communication, where the representation model is treated as a multiaccess communication channel with memory and feedback. Without requirements of the backbone structure, the learning objective of our CapMax is maximizing the channel capacity, which is measured by directed information (DI) rather than mutual information. For efficient implementation, we design an estimator of the channel capacity through the combination of graph neural networks (GNNs) and recurrent neural networks (RNNs). Under some mild conditions, we theoretically prove that DI is a better measure than mutual information in capturing useful information. The experiments are conducted on multiple real-world dynamic network datasets, and the outperformance of our CapMax on different backbone models on link detection and prediction validates the effectiveness of modeling the representation model as a communication channel.},
  archive      = {J_TNNLS},
  author       = {Chenming Yang and Hui Wen and Bryan Hooi and Liang Zhou},
  doi          = {10.1109/TNNLS.2022.3222165},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4554-4566},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CapMax: A framework for dynamic network representation learning from the view of multiuser communication},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Haar wavelet feature compression for quantized graph
convolutional networks. <em>TNNLS</em>, <em>35</em>(4), 4542–4553. (<a
href="https://doi.org/10.1109/TNNLS.2023.3285874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) are widely used in a variety of applications and can be seen as an unstructured version of standard convolutional neural networks (CNNs). As in CNNs, the computational cost of GCNs for large input graphs (such as large point clouds or meshes) can be high and inhibit the use of these networks, especially in environments with low computational resources. To ease these costs, quantization can be applied to GCNs. However, aggressive quantization of the feature maps can lead to a significant degradation in performance. On a different note, the Haar wavelet transforms are known to be one of the most effective and efficient approaches to compress signals. Therefore, instead of applying aggressive quantization to feature maps, we propose to use Haar wavelet compression and light quantization to reduce the computations involved with the network. We demonstrate that this approach surpasses aggressive feature quantization by a significant margin, for a variety of problems ranging from node classification to point cloud classification and both part and semantic segmentation.},
  archive      = {J_TNNLS},
  author       = {Moshe Eliasof and Benjamin J. Bodner and Eran Treister},
  doi          = {10.1109/TNNLS.2023.3285874},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4542-4553},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Haar wavelet feature compression for quantized graph convolutional networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reverse graph learning for graph neural network.
<em>TNNLS</em>, <em>35</em>(4), 4530–4541. (<a
href="https://doi.org/10.1109/TNNLS.2022.3161030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) conduct feature learning by taking into account the local structure preservation of the data to produce discriminative features, but need to address the following issues, i.e., 1) the initial graph containing faulty and missing edges often affect feature learning and 2) most GNN methods suffer from the issue of out-of-example since their training processes do not directly generate a prediction model to predict unseen data points. In this work, we propose a reverse GNN model to learn the graph from the intrinsic space of the original data points as well as to investigate a new out-of-sample extension method. As a result, the proposed method can output a high-quality graph to improve the quality of feature learning, while the new method of out-of-sample extension makes our reverse GNN method available for conducting supervised learning and semi-supervised learning. Experimental results on real-world datasets show that our method outputs competitive classification performance, compared to state-of-the-art methods, in terms of semi-supervised node classification, out-of-sample extension, random edge attack, link prediction, and image retrieval.},
  archive      = {J_TNNLS},
  author       = {Liang Peng and Rongyao Hu and Fei Kong and Jiangzhang Gan and Yujie Mo and Xiaoshuang Shi and Xiaofeng Zhu},
  doi          = {10.1109/TNNLS.2022.3161030},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4530-4541},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reverse graph learning for graph neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). GNDAN: Graph navigated dual attention network for zero-shot
learning. <em>TNNLS</em>, <em>35</em>(4), 4516–4529. (<a
href="https://doi.org/10.1109/TNNLS.2022.3155602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) tackles the unseen class recognition problem by transferring semantic knowledge from seen classes to unseen ones. Typically, to guarantee desirable knowledge transfer, a direct embedding is adopted for associating the visual and semantic domains in ZSL. However, most existing ZSL methods focus on learning the embedding from implicit global features or image regions to the semantic space. Thus, they fail to: 1) exploit the appearance relationship priors between various local regions in a single image, which corresponds to the semantic information and 2) learn cooperative global and local features jointly for discriminative feature representations. In this article, we propose the novel graph navigated dual attention network (GNDAN) for ZSL to address these drawbacks. GNDAN employs a region-guided attention network (RAN) and a region-guided graph attention network (RGAT) to jointly learn a discriminative local embedding and incorporate global context for exploiting explicit global embeddings under the guidance of a graph. Specifically, RAN uses soft spatial attention to discover discriminative regions for generating local embeddings. Meanwhile, RGAT employs an attribute-based attention to obtain attribute-based region features, where each attribute focuses on the most relevant image regions. Motivated by the graph neural network (GNN), which is beneficial for structural relationship representations, RGAT further leverages a graph attention network to exploit the relationships between the attribute-based region features for explicit global embedding representations. Based on the self-calibration mechanism, the joint visual embedding learned is matched with the semantic embedding to form the final prediction. Extensive experiments on three benchmark datasets demonstrate that the proposed GNDAN achieves superior performances to the state-of-the-art methods. Our code and trained models are available at https://github.com/shiming-chen/GNDAN .},
  archive      = {J_TNNLS},
  author       = {Shiming Chen and Ziming Hong and Guosen Xie and Qinmu Peng and Xinge You and Weiping Ding and Ling Shao},
  doi          = {10.1109/TNNLS.2022.3155602},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4516-4529},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GNDAN: Graph navigated dual attention network for zero-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attributed graph force learning. <em>TNNLS</em>,
<em>35</em>(4), 4502–4515. (<a
href="https://doi.org/10.1109/TNNLS.2022.3221100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In numerous network analysis tasks, feature representation plays an imperative role. Due to the intrinsic nature of networks being discrete, enormous challenges are imposed on their effective usage. There has been a significant amount of attention on network feature learning in recent times that has the potential of mapping discrete features into a continuous feature space. The methods, however, lack preserving the structural information owing to the utilization of random negative sampling during the training phase. The ability to effectively join attribute information to embedding feature space is also compromised. To address the shortcomings identified, a novel attribute force-based graph (AGForce) learning model is proposed that keeps the structural information intact along with adaptively joining attribute information to the node’s features. To demonstrate the effectiveness of the proposed framework, comprehensive experiments on benchmark datasets are performed. AGForce based on the spring-electrical model extends opportunities to simulate node interaction for graph learning.},
  archive      = {J_TNNLS},
  author       = {Ke Sun and Feng Xia and Jiaying Liu and Bo Xu and Vidya Saikrishna and Charu C. Aggarwal},
  doi          = {10.1109/TNNLS.2022.3221100},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4502-4515},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attributed graph force learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward quantized model parallelism for graph-augmented MLPs
based on gradient-free ADMM framework. <em>TNNLS</em>, <em>35</em>(4),
4491–4501. (<a
href="https://doi.org/10.1109/TNNLS.2022.3223879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While graph neural networks (GNNs) are popular in the deep learning community, they suffer from several challenges including over-smoothing, over-squashing, and gradient vanishing. Recently, a series of models have attempted to relieve these issues by first augmenting the node features and then imposing node-wise functions based on multilayer perceptron (MLP), which are widely referred to as graph-augmented MLP (GA-MLP) models. However, while GA-MLP models enjoy deeper architectures for better accuracy, their efficiency largely deteriorates. Moreover, popular acceleration techniques such as stochastic-version or data-parallelism cannot be effectively applied due to the dependency among samples (i.e., nodes) in graphs. To address these issues, in this article, instead of data parallelism, we propose a parallel graph deep learning Alternating Direction Method of Multipliers (pdADMM-G) framework to achieve model parallelism: parameters in each layer of GA-MLP models can be updated in parallel. The extended pdADMM-G-Q algorithm reduces communication costs by introducing the quantization technique. Theoretical convergence to a (quantized) stationary point of the pdADMM-G algorithm and the pdADMM-G-Q algorithm is provided with a sublinear convergence rate $o(1/k)$ , where $k$ is the number of iterations. Extensive experiments demonstrate the convergence of two proposed algorithms. Moreover, they lead to a more massive speedup and better performance than all state-of-the-art comparison methods on nine benchmark datasets. Last but not least, the proposed pdADMM-G-Q algorithm reduces communication overheads by up to 45% without loss of performance. Our code is available at https://github.com/xianggebenben/pdADMM-G .},
  archive      = {J_TNNLS},
  author       = {Junxiang Wang and Hongyi Li and Zheng Chai and Yongchao Wang and Yue Cheng and Liang Zhao},
  doi          = {10.1109/TNNLS.2022.3223879},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4491-4501},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward quantized model parallelism for graph-augmented MLPs based on gradient-free ADMM framework},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). ScaleGCN: Efficient and effective graph convolution via
channel-wise scale transformation. <em>TNNLS</em>, <em>35</em>(4),
4478–4490. (<a
href="https://doi.org/10.1109/TNNLS.2022.3199390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have shown success in many graph-based applications as they can combine node features and graph topology to obtain expressive embeddings. While there exist numerous GCN variants, a typical graph convolution layer uses neighborhood aggregation and fully-connected (FC) layers to extract topological and node-wise features, respectively. However, when the receptive field of GCNs becomes larger, the tight coupling between the number of neighborhood aggregation and FC layers can increase the risk of over-fitting. Also, the FC layer between two successive aggregation operations will mix and pollute features in different channels, bringing noise and making node features hard to converge at each channel. In this article, we explore graph convolution without FC layers. We propose scale graph convolution, a new graph convolution using channel-wise scale transformation to extract node features. We provide empirical evidence that our new method has lower over-fitting risk and needs fewer layers to converge. We show from both theoretical and empirical perspectives that models with scale graph convolution have lower computational and memory costs than traditional GCN models. Experimental results on various datasets show that our method can achieve state-of-the-art results, in a cost-effective fashion.},
  archive      = {J_TNNLS},
  author       = {Tianqi Zhang and Qitian Wu and Junchi Yan and Yunan Zhao and Bing Han},
  doi          = {10.1109/TNNLS.2022.3199390},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4478-4490},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ScaleGCN: Efficient and effective graph convolution via channel-wise scale transformation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GUIDE: Training deep graph neural networks via guided
dropout over edges. <em>TNNLS</em>, <em>35</em>(4), 4465–4477. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have made great progress in graph-based semi-supervised learning (GSSL). However, most existing GNNs are confronted with the oversmoothing issue that limits their expressive ability. A key factor that leads to this problem is the excessive aggregation of information from other classes when updating the node representation. To alleviate this limitation, we propose an effective method called GUIded Dropout over Edges (GUIDE) for training deep GNNs. The core of the method is to reduce the influence of nodes from other classes by removing a certain number of inter-class edges. In GUIDE, we drop edges according to the edge strength, which is defined as the time an edge acts as a bridge along the shortest path between node pairs. We find that the stronger the edge strength, the more likely it is to be an inter-class edge. In this way, GUIDE can drop more inter-class edges and keep more intra-class edges. Therefore, nodes in the same community or class are more similar, whereas different classes are more separated in the embedded space. In addition, we perform some theoretical analysis of the proposed method, which explains why it is effective in alleviating the oversmoothing problem. To validate its rationality and effectiveness, we conduct experiments on six public benchmarks with different GNNs backbones. Experimental results demonstrate that GUIDE consistently outperforms state-of-the-art methods in both shallow and deep GNNs.},
  archive      = {J_TNNLS},
  author       = {Jie Wang and Jianqing Liang and Jiye Liang and Kaixuan Yao},
  doi          = {10.1109/TNNLS.2022.3172879},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4465-4477},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GUIDE: Training deep graph neural networks via guided dropout over edges},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallelly adaptive graph convolutional clustering model.
<em>TNNLS</em>, <em>35</em>(4), 4451–4464. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from exploiting the data topological structure, graph convolutional network (GCN) has made considerable improvements in processing clustering tasks. The performance of GCN significantly relies on the quality of the pretrained graph, while the graph structures are often corrupted by noise or outliers. To overcome this problem, we replace the pre-trained and fixed graph in GCN by the adaptive graph learned from the data. In this article, we propose a novel end-to-end parallelly adaptive graph convolutional clustering (AGCC) model with two pathway networks. In the first pathway, an adaptive graph convolutional (AGC) module alternatively updates the graph structure and the data representation layer by layer. The updated graph can better reflect the data relationship than the fixed graph. In the second pathway, the auto-encoder (AE) module aims to extract the latent data features. To effectively connect the AGC and AE modules, we creatively propose an attention-mechanism-based fusion (AMF) module to weight and fuse the data representations of the two modules, and transfer them to the AGC module. This simultaneously avoids the over-smoothing problem of GCN. Experimental results on six public datasets show that the effectiveness of the proposed AGCC compared with multiple state-of-the-art deep clustering methods. The code is available at https://github.com/HeXiax/AGCC .},
  archive      = {J_TNNLS},
  author       = {Xiaxia He and Boyue Wang and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin},
  doi          = {10.1109/TNNLS.2022.3176411},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4451-4464},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parallelly adaptive graph convolutional clustering model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing heterogeneous networks with missing attributes by
unsupervised contrastive learning. <em>TNNLS</em>, <em>35</em>(4),
4438–4450. (<a
href="https://doi.org/10.1109/TNNLS.2022.3149997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous information networks (HINs) are potent models of complex systems. In practice, many nodes in an HIN have their attributes unspecified, resulting in significant performance degradation for supervised and unsupervised representation learning. We developed an unsupervised heterogeneous graph contrastive learning approach for analyzing HINs with missing attributes (HGCA). HGCA adopts a contrastive learning strategy to unify attribute completion and representation learning in an unsupervised heterogeneous framework. To deal with a large number of missing attributes and the absence of labels in unsupervised scenarios, we proposed an augmented network to capture the semantic relations between nodes and attributes to achieve a fine-grained attribute completion. Extensive experiments on three large real-world HINs demonstrated the superiority of HGCA over several state-of-the-art methods. The results also showed that the complemented attributes by HGCA can improve the performance of existing HIN models.},
  archive      = {J_TNNLS},
  author       = {Dongxiao He and Chundong Liang and Cuiying Huo and Zhiyong Feng and Di Jin and Liang Yang and Weixiong Zhang},
  doi          = {10.1109/TNNLS.2022.3149997},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4438-4450},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Analyzing heterogeneous networks with missing attributes by unsupervised contrastive learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning aligned vertex convolutional networks for graph
classification. <em>TNNLS</em>, <em>35</em>(4), 4423–4437. (<a
href="https://doi.org/10.1109/TNNLS.2021.3129649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) are powerful tools for graph structure data analysis. One main drawback arising in most existing GCN models is that of the oversmoothing problem, i.e., the vertex features abstracted from the existing graph convolution operation have previously tended to be indistinguishable if the GCN model has many convolutional layers (e.g., more than two layers). To address this problem, in this article, we propose a family of aligned vertex convolutional network (AVCN) models that focus on learning multiscale features from local-level vertices for graph classification. This is done by adopting a transitive vertex alignment algorithm to transform arbitrary-sized graphs into fixed-size grid structures. Furthermore, we define a new aligned vertex convolution operation that can effectively learn multiscale vertex characteristics by gradually aggregating local-level neighboring aligned vertices residing on the original grid structures into a new packed aligned vertex. With the new vertex convolution operation to hand, we propose two architectures for the AVCN models to extract different hierarchical multiscale vertex feature representations for graph classification. We show that the proposed models can avoid iteratively propagating redundant information between specific neighboring vertices, restricting the notorious oversmoothing problem arising in most spatial-based GCN models. Experimental evaluations on benchmark datasets demonstrate the effectiveness.},
  archive      = {J_TNNLS},
  author       = {Lixin Cui and Lu Bai and Xiao Bai and Yue Wang and Edwin R. Hancock},
  doi          = {10.1109/TNNLS.2021.3129649},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4423-4437},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning aligned vertex convolutional networks for graph classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Debiased graph neural networks with agnostic label selection
bias. <em>TNNLS</em>, <em>35</em>(4), 4411–4422. (<a
href="https://doi.org/10.1109/TNNLS.2022.3141260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing graph neural networks (GNNs) are proposed without considering the selection bias in data, i.e., the inconsistent distribution between the training set with the test set. In reality, the test data are not even available during the training process, making selection bias agnostic. Training GNNs with biased selected nodes leads to significant parameter estimation bias and greatly impacts the generalization ability on test nodes. In this article, we first present an experimental investigation, which clearly shows that the selection bias drastically hinders the generalization ability of GNNs, and theoretically proves that the selection bias will cause the biased estimation on GNN parameters. Then to remove the bias in GNN estimation, we propose a novel debiased GNNs (DGNN) with a differentiated decorrelation regularizer. The differentiated decorrelation regularizer estimates a sample weight for each labeled node such that the spurious correlation of learned embeddings could be eliminated. We analyze the regularizer in causal view and it motivates us to differentiate the weights of the variables based on their contribution to the confounding bias. Then, these sample weights are used for reweighting GNNs to eliminate the estimation bias, and thus, help to improve the stability of prediction on unknown test nodes. Comprehensive experiments are conducted on several challenging graph datasets with two kinds of label selection biases. The results well verify that our proposed model outperforms the state-of-the-art methods and DGNN is a flexible framework to enhance existing GNNs.},
  archive      = {J_TNNLS},
  author       = {Shaohua Fan and Xiao Wang and Chuan Shi and Kun Kuang and Nian Liu and Bai Wang},
  doi          = {10.1109/TNNLS.2022.3141260},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4411-4422},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Debiased graph neural networks with agnostic label selection bias},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convolutional neural networks for spherical signal
processing via area-regular spherical haar tight framelets.
<em>TNNLS</em>, <em>35</em>(4), 4400–4410. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a general theoretical framework for constructing Haar-type tight framelets on any compact set with a hierarchical partition. In particular, we construct a novel area-regular hierarchical partition on the two spheres and establish its corresponding spherical Haar tight framelets with directionality. We conclude by evaluating and illustrate the effectiveness of our area-regular spherical Haar tight framelets in several denoising experiments. Furthermore, we propose a convolutional neural network (CNN) model for spherical signal denoising, which employs fast framelet decomposition and reconstruction algorithms. Experiment results show that our proposed CNN model outperforms threshold methods and processes strong generalization and robustness.},
  archive      = {J_TNNLS},
  author       = {Jianfei Li and Han Feng and Xiaosheng Zhuang},
  doi          = {10.1109/TNNLS.2022.3160169},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4400-4410},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convolutional neural networks for spherical signal processing via area-regular spherical haar tight framelets},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empowering simple graph convolutional networks.
<em>TNNLS</em>, <em>35</em>(4), 4385–4399. (<a
href="https://doi.org/10.1109/TNNLS.2022.3232291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many neural networks for graphs are based on the graph convolution (GC) operator, proposed more than a decade ago. Since then, many alternative definitions have been proposed, which tend to add complexity (and nonlinearity) to the model. Recently, however, a simplified GC operator, dubbed simple graph convolution (SGC), which aims to remove nonlinearities was proposed. Motivated by the good results reached by this simpler model, in this article we propose, analyze, and compare simple graph convolution operators of increasing complexity that rely on linear transformations or controlled nonlinearities, and that can be implemented in single-layer graph convolutional networks (GCNs). Their computational expressiveness is characterized as well. We show that the predictive performance of the proposed GC operators is competitive with the ones of other widely adopted models on the considered node classification benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Luca Pasa and Nicolò Navarin and Wolfgang Erb and Alessandro Sperduti},
  doi          = {10.1109/TNNLS.2022.3232291},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4385-4399},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Empowering simple graph convolutional networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral graph convolutional neural networks in the context
of regularization theory. <em>TNNLS</em>, <em>35</em>(4), 4373–4384. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional neural networks (GCNN) have been widely used in graph learning and related applications. It has been identified that the filters in the state-of-the-art spectral graph convolutional networks (SGCNs) are essentially low-pass filters that enforce smoothness across the graph and use the functions of graph Laplacian as a tool that injects graph structure into the learning algorithm. There had been research findings that connect the smoothness functional in graphs, graph Laplacian, and regularization operators. We review the existing SGCNs in this context and propose a framework where the state-of-the-art filter designs can be deduced as special cases. We designed new filters that are associated with well-defined low-pass behavior and tested their performance on semisupervised node classification tasks. Their performance was found to be superior to that of the other state-of-the-art techniques. We further investigate the representation capability of low-pass features and make useful observations. In this context, we discuss a few points to further optimize the network, new strategies for designing SGCNs, their challenges, and some latest related developments. Based on our framework, we also deduce the connection between support vector kernels and SGCN filters.},
  archive      = {J_TNNLS},
  author       = {Asif Salim and S. Sumitra},
  doi          = {10.1109/TNNLS.2022.3177742},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4373-4384},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spectral graph convolutional neural networks in the context of regularization theory},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Deep neural networks for graphs: Theory,
models, algorithms, and applications. <em>TNNLS</em>, <em>35</em>(4),
4367–4372. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks for graphs (DNNGs) represent an emerging field that studies how the deep learning method can be generalized to graph-structured data. Since graphs are a powerful and flexible tool to represent complex information in the form of patterns and their relationships, ranging from molecules to protein-to-protein interaction networks, to social or transportation networks, or up to knowledge graphs, potentially modeling systems at very different scales, these methods have been exploited for many application domains.},
  archive      = {J_TNNLS},
  author       = {Ming Li and Alessio Micheli and Yu Guang Wang and Shirui Pan and Pietro Lió and Giorgio Stefano Gnecco and Marcello Sanguineti},
  doi          = {10.1109/TNNLS.2024.3371592},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {4},
  number       = {4},
  pages        = {4367-4372},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial: deep neural networks for graphs: theory, models, algorithms, and applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Particle-filter-based state estimation for delayed
artificial neural networks: When probabilistic saturation constraints
meet redundant channels. <em>TNNLS</em>, <em>35</em>(3), 4354–4362. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, the state estimation problem is investigated for a class of randomly delayed artificial neural networks (ANNs) subject to probabilistic saturation constraints (PSCs) and non-Gaussian noises under the redundant communication channels. A series of mutually independent Bernoulli distributed white sequences are introduced to govern the random occurrence of the time delays, the saturation constraints, and the transmission channel failures. A comprehensive redundant-channel-based communication mechanism is constructed to attenuate the phenomenon of packet dropouts so as to enhance the quality of data transmission. To compensate for the influence of randomly occurring time delays, the corresponding occurrence probability is exploited in the process of particle generation. In addition, an explicit expression of the likelihood function is established based on the statistical information to account for the impact of PSCs and redundant channels. By virtue of the modified operations of particle propagation and weight update, a particle-filter-based state estimation algorithm is proposed with mild restriction on the system type. Finally, an illustrative example with Monte Carlo simulations is provided to demonstrate the effectiveness of the developed state estimation scheme.},
  archive      = {J_TNNLS},
  author       = {Weihao Song and Zidong Wang and Zhongkui Li and Qing-Long Han},
  doi          = {10.1109/TNNLS.2022.3201160},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4354-4362},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Particle-filter-based state estimation for delayed artificial neural networks: When probabilistic saturation constraints meet redundant channels},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed nash equilibrium seeking dynamics with discrete
communication. <em>TNNLS</em>, <em>35</em>(3), 4347–4353. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we aim to provide a distributed Nash equilibrium seeking algorithm in continuous time with discrete communications. A group of agents are considered playing a continuous-kernel noncooperative game over a network. The agents need to seek the Nash equilibrium when each player cannot get the overall action profiles in real time, rather are only able to get information from its networked neighbors. Meanwhile, a continuous-time dynamics is discussed for the players to update their variables, but the communications over the network are only assumed to allow at discrete-time instants, since continuous-time communications are prohibitive and cumbersome in practice. First, the periodic communication is considered at a fixed interval, and the solvability of Nash equilibrium seeking is shown with discrete communications. Then, an event-trigger communication scheme is proposed to further reduce the communication rounds. Nevertheless, the event-trigger communication scheme requires each player continuously monitoring its local states. To alleviate the monitoring burden, a periodic event detection mechanism is further developed. The exponential convergence of the dynamics with the three discrete communication schemes is proven. Finally, the comparative simulation studies are designed to illustrate the algorithm performance with different communication schemes and parameter settings.},
  archive      = {J_TNNLS},
  author       = {Rui Yu and Yutao Tang and Peng Yi and Li Li},
  doi          = {10.1109/TNNLS.2022.3201133},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4347-4353},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed nash equilibrium seeking dynamics with discrete communication},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). Kernel adaptive filtering over complex networks.
<em>TNNLS</em>, <em>35</em>(3), 4339–4346. (<a
href="https://doi.org/10.1109/TNNLS.2022.3199679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief is concerned with the problem of kernel adaptive filtering for a complex network. First, a coupled kernel least mean square (KLMS) algorithm is developed for each node to uncover its nonlinear measurement function by using a series of input–output data. Subsequently, an upper bound is derived for the step-size of the coupled KLMS algorithm to guarantee the mean square convergence. It is shown that the upper bound is dependent on the coupling weights of the complex network. Especially, an optimal step size is obtained to achieve the fastest convergence speed and a suboptimal step size is presented for the purpose of practical implementations. Besides, a coupled kernel recursive least square (KRLS) algorithm is further proposed to improve the filtering performance. Finally, simulations are provided to verify the validity of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Wenling Li and Zidong Wang and Jun Hu and Junping Du and Weiguo Sheng},
  doi          = {10.1109/TNNLS.2022.3199679},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4339-4346},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Kernel adaptive filtering over complex networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge verification from data. <em>TNNLS</em>,
<em>35</em>(3), 4324–4338. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge verification is an important task in the quality management of knowledge graphs (KGs). Knowledge is a summary of facts and events based on human cognition and experience. Due to the nature of knowledge, most knowledge quality (KQ) management methods are designed by human experts or the characteristics of existing knowledge, which may be limited by human cognition and the quality of existing knowledge. Numerical data contain a wealth of potential information that may be helpful in verifying knowledge, which is rarely explored. However, due to the implicit representation of numerical data to facts as well as the noise in the data, it is challenging to use data to verify the knowledge. Therefore, this article proposes a knowledge verification method, which discovers the correlation and causality from numerical data to validate knowledge and then evaluate the quality of knowledge. Moreover, to address the impact of noise, the method integrates multisource knowledge to jointly evaluate the KQ. Specifically, an iterative update method is designed to update KQ by utilizing the consistency between multisource knowledge while designing knowledge verification factors based on data causality and correlation to manage update process. The method is validated with multiple datasets, and the results demonstrate that the proposed method could evaluate KQ more accurately and has strong robustness to noise in the data.},
  archive      = {J_TNNLS},
  author       = {Xiangyu Wang and Taiyu Ban and Lyuzhou Chen and Xingyu Wu and Derui Lyu and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2022.3202244},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4324-4338},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge verification from data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). 3-d convolutional neural networks for RGB-d salient object
detection and beyond. <em>TNNLS</em>, <em>35</em>(3), 4309–4323. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-depth (RGB-D) salient object detection (SOD) recently has attracted increasing research interest, and many deep learning methods based on encoder–decoder architectures have emerged. However, most existing RGB-D SOD models conduct explicit and controllable cross-modal feature fusion either in the single encoder or decoder stage, which hardly guarantees sufficient cross-modal fusion ability. To this end, we make the first attempt in addressing RGB-D SOD through 3-D convolutional neural networks. The proposed model, named RD3D, aims at prefusion in the encoder stage and in-depth fusion in the decoder stage to effectively promote the full integration of RGB and depth streams. Specifically, RD3D first conducts prefusion across RGB and depth modalities through a 3-D encoder obtained by inflating 2-D ResNet and later provides in-depth feature fusion by designing a 3-D decoder equipped with rich back-projection paths (RBPPs) for leveraging the extensive aggregation ability of 3-D convolutions. Toward an improved model RD3D+, we propose to disentangle the conventional 3-D convolution into successive spatial and temporal convolutions and, meanwhile, discard unnecessary zero padding. This eventually results in a 2-D convolutional equivalence that facilitates optimization and reduces parameters and computation costs. Thanks to such a progressive-fusion strategy involving both the encoder and the decoder, effective and thorough interactions between the two modalities can be exploited and boost detection accuracy. As an additional boost, we also introduce channel-modality attention and its variant after each path of RBPP to attend to important features. Extensive experiments on seven widely used benchmark datasets demonstrate that RD3D and RD3D+ perform favorably against 14 state-of-the-art RGB-D SOD approaches in terms of five key evaluation metrics. Our code will be made publicly available at https://github.com/PPOLYpubki/RD3D .},
  archive      = {J_TNNLS},
  author       = {Qian Chen and Zhenxi Zhang and Yanye Lu and Keren Fu and Qijun Zhao},
  doi          = {10.1109/TNNLS.2022.3202241},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4309-4323},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {3-D convolutional neural networks for RGB-D salient object detection and beyond},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised deep multiview spectral clustering.
<em>TNNLS</em>, <em>35</em>(3), 4299–4308. (<a
href="https://doi.org/10.1109/TNNLS.2022.3195780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview spectral clustering has received considerable attention in the past decades and still has great potential due to its unsupervised integration manner. It is well known that pairwise constraints boost the clustering process to a great extent. Nevertheless, the constraints are usually marked by human beings. To ameliorate the performance of multiview spectral clustering and alleviate the consumption of human resources, we propose self-supervised multiview spectral clustering with a small number of automatically retrieved pairwise constraints. First, the fused multiple autoencoders are used to extract the latent consistent feature of multiple views. Second, the pairwise constraints are achieved based on the commonality among multiple views. Then, the pairwise constraints are propagated through the neural network with historical memory. Finally, the propagated constraints are used to optimize the fused affinity matrix of spectral clustering. Our experiments on four benchmark datasets show the effectiveness of our proposed approach.},
  archive      = {J_TNNLS},
  author       = {Linlin Zong and Faqiang Miao and Xianchao Zhang and Wenxin Liang and Bo Xu},
  doi          = {10.1109/TNNLS.2022.3195780},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4299-4308},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised deep multiview spectral clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual parallel policy iteration with coupled policy
improvement. <em>TNNLS</em>, <em>35</em>(3), 4286–4298. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel coupled policy improvement mechanism is developed for improving policy iteration (PI) algorithms. In contrast to the common PI, the developed dual parallel policy iteration (DPPI) with coupled policy improvement mechanism consists of two parallel PIs. At each PI step, the performances of the two parallel policies are evaluated and the better one is defined as the dominant policy. Then, the dominant policy is used to guide the parallel policy improvement in a soft manner by constraining the Kullback–Liebler (KL) divergence between the dominant policy and the policy to be updated. It is proven that the convergence of DPPI can be guaranteed under the designed coupled policy improvement mechanism. Moreover, it is clearly shown that under certain conditions, the $Q$ -functions of the two new policies obtained in each parallel policy improvement are larger than those of all the previous dominant policies, which is conductive to accelerate the PI process and improve the policy learning efficiency to some extent. Furthermore, by combining DPPI with the twin delay deep deterministic (TD3) policy gradient, we propose a reinforcement learning (RL) algorithm: parallel TD3 (PTD3). Experimental results on continuous-action control tasks in the MuJoCo and OpenAI Gym platforms show that the proposed PTD3 outperforms the state-of-the-art RL algorithms.},
  archive      = {J_TNNLS},
  author       = {Yuhu Cheng and Longyang Huang and C. L. Philip Chen and Xuesong Wang},
  doi          = {10.1109/TNNLS.2022.3202192},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4286-4298},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual parallel policy iteration with coupled policy improvement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). User identification across multiple social networks based on
naive bayes model. <em>TNNLS</em>, <em>35</em>(3), 4274–4285. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the problem of user identification across multiple social networks (UIAMSNs) has attracted considerable attention since it is a prerequisite for many downstream tasks and applications. Although substantial network feature-based approaches have been proposed to solve the UIAMSNs’ problem, the matching degree in most of the current works is given by experience, which lacks a solid theoretical basis. To alleviate the above predicament, we propose a user identification algorithm based on naive Bayes model (UI-NBM) within the network feature-based framework. First, a matching degree index is designed based on the naive Bayes model, which can accurately measure the contributions of different common matched node pairs (MNPs) to the connection probability of unmatched node pairs (UMNPs). Second, the matching degrees of all UMNPs are formulated as the product of matrices, giving rise to the great reduction of the time complexity and the compact expression; Finally, with the idea of recursion process, more UMNPs can be iteratively predicted even when only a small amount of prior information (i.e., a few number of MNPs) is known. The experimental results on the synthetic and real cross platforms demonstrate that the method outperforms the baseline methods within the feature-based framework.},
  archive      = {J_TNNLS},
  author       = {Xiao Ding and Haifeng Zhang and Chuang Ma and Xingyi Zhang and Kai Zhong},
  doi          = {10.1109/TNNLS.2022.3202709},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4274-4285},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {User identification across multiple social networks based on naive bayes model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Learning all-in collaborative multiview binary
representation for clustering. <em>TNNLS</em>, <em>35</em>(3),
4260–4273. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering via binary representation has attracted intensive attention due to its effectiveness in handling large-scale multiple view data. However, these kind of clustering approaches usually ignore a very important potential high-order correlation in discrete representation learning. In this article, we propose a novel all-in collaborative multiview binary representation for clustering (AC-MVBC) framework, where multiview collaborative binary representation and clustering structure are learned in a joint manner. Specifically, using a new type of tensor low-rank constraint, the high-order collaborations, i.e., cross-view and inner view collaborations, can be effectively captured in our model. Moreover, by incorporating the Bregman discrepancy, the projective consistency among different views can be guaranteed to achieve a more powerful binary representation. An efficient optimization algorithm is also proposed to solve the objective function with fast convergence empirically. Experimental results on several challenge datasets demonstrate that the proposed method has achieved highly competent performance compared with the state-of-the-art multiview clustering (MVC) methods while maintaining low computational and memory requirements.},
  archive      = {J_TNNLS},
  author       = {Yachao Zhang and Yuan Xie and Cuihua Li and Zongze Wu and Yanyun Qu},
  doi          = {10.1109/TNNLS.2022.3202102},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4260-4273},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning all-in collaborative multiview binary representation for clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributional perspective on multiagent cooperation with
deep reinforcement learning. <em>TNNLS</em>, <em>35</em>(3), 4246–4259.
(<a href="https://doi.org/10.1109/TNNLS.2022.3202097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among various value decomposition-based multiagent reinforcement learning (MARL) algorithms, the overall performance of the multiagent system is represented by a scalar global $Q$ value and optimized by minimizing the temporal difference (TD) error with respect to that global $Q$ value. However, the global $Q$ value cannot accurately model the distributed dynamics of the multiagent system, since it is only a simplified representation for different individual $Q$ values of agents. To explicitly consider the correlations between different cooperative agents, in this article, we propose a distributional framework and construct a practical model called distributional multiagent cooperation (DMAC) from a novel distributional perspective. Specifically, in DMAC, we view the individual $Q$ value for the executed action of a random agent as a value distribution, whose expectation can further represent the overall performance. Then, we employ distributional RL to minimize the difference between the estimated distribution and its target for the optimization. The advantage of DMAC is that the distributed dynamics of agents can be explicitly modeled, and this results in better performance. To verify the effectiveness of DMAC, we conduct extensive experiments under nine different scenarios of the StarCraft Multiagent Challenge (SMAC). Experimental results show that the DMAC can significantly outperform the baselines with respect to the average median test win rate.},
  archive      = {J_TNNLS},
  author       = {Liwei Huang and Mingsheng Fu and Ananya Rao and Athirai A. Irissappane and Jie Zhang and Chengzhong Xu},
  doi          = {10.1109/TNNLS.2022.3202097},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4246-4259},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A distributional perspective on multiagent cooperation with deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Leveraging imitation learning on pose regulation problem of
a robotic fish. <em>TNNLS</em>, <em>35</em>(3), 4232–4245. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the pose regulation control problem of a robotic fish is investigated by formulating it as a Markov decision process (MDP). Such a typical task that requires the robot to arrive at the desired position with the desired orientation remains a challenge, since two objectives (position and orientation) may be conflicted during optimization. To handle the challenge, we adopt the sparse reward scheme, i.e., the robot will be rewarded if and only if it completes the pose regulation task. Although deep reinforcement learning (DRL) can achieve such an MDP with sparse rewards, the absence of immediate reward hinders the robot from efficient learning. To this end, we propose a novel imitation learning (IL) method that learns DRL-based policies from demonstrations with inverse reward shaping to overcome the challenge raised by extremely sparse rewards. Moreover, we design a demonstrator to generate various trajectory demonstrations based on one simple example from a nonexpert helper, which greatly reduces the time consumption of collecting robot samples. The simulation results evaluate the effectiveness of our proposed demonstrator and the state-of-the-art (SOTA) performance of our proposed IL method. Furthermore, we deploy the trained IL policy on a physical robotic fish to perform pose regulation in a swimming tank without/with external disturbances. The experimental results verify the effectiveness and robustness of our proposed methods in real world. Therefore, we believe this article is a step forward in the field of biomimetic underwater robot learning.},
  archive      = {J_TNNLS},
  author       = {Tianhao Zhang and Lu Yue and Chen Wang and Jinan Sun and Shikun Zhang and Airong Wei and Guangming Xie},
  doi          = {10.1109/TNNLS.2022.3202075},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4232-4245},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Leveraging imitation learning on pose regulation problem of a robotic fish},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero-shot learning with attentive region embedding and
enhanced semantics. <em>TNNLS</em>, <em>35</em>(3), 4220–4231. (<a
href="https://doi.org/10.1109/TNNLS.2022.3202014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of zero-shot learning (ZSL) can be improved progressively by learning better features and generating pseudosamples for unseen classes. Existing ZSL works typically learn feature extractors and generators independently, which may shift the unseen samples away from their real distribution and suffers from the domain bias problem. In this article, to tackle this challenge, we propose a variational autoencoder (VAE)-based framework, that is, joint Attentive Region Embedding with Enhanced Semantics (AREES), which is tailored to advance the zero-shot recognition. Specifically, AREES is end-to-end trainable and consists of three network branches: 1) attentive region embedding is used to learn the semantic-guided visual features by the attention mechanism (AM); 2) a decomposition structure and a semantic pivot regularization are used to extract enhanced semantics; and 3) a multimodal VAE (mVAE) with the cross-reconstruction loss and the distribution alignment loss is used to obtain a shared latent embedding space of visual features and semantics. Finally, features’ extraction and features’ generation are optimized together in AREES to address the domain shift problem to a large extent. The comprehensive evaluations on six benchmarks, including the ImageNet, demonstrate the superiority of the proposed model over its state-of-the-art counterparts.},
  archive      = {J_TNNLS},
  author       = {Yang Liu and Yuhao Dang and Xinbo Gao and Jungong Han and Ling Shao},
  doi          = {10.1109/TNNLS.2022.3202014},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4220-4231},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Zero-shot learning with attentive region embedding and enhanced semantics},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CGDD: Multiview graph clustering via cross-graph diversity
detection. <em>TNNLS</em>, <em>35</em>(3), 4206–4219. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview graph clustering has emerged as an important yet challenging technique due to the difficulty of exploiting the similarity relationships among multiple views. Typically, the similarity graph for each view learned by these methods is easily corrupted because of the unavoidable noise or diversity among views. To recover a clean graph, existing methods mainly focus on the diverse part within each graph yet overlook the diversity across multiple graphs. In this article, instead of merely considering the sparsity of diversity within a graph as previous methods do, we incline to a more suitable consideration that the diversity should be sparse across graphs. It is intuitive that the divergent parts are supposed to be inconsistent with each other, otherwise it would contradict the definition of diversity. By simultaneously and explicitly detecting the multiview consistency and cross-graph diversity, a pure graph for each view can be expected. The multiple pure graphs are further fused to the structured consensus graph with exactly $r$ connected components where $r$ is the number of clusters. Once the consensus graph is obtained, the cluster label to each instance can be directly allocated as each connected component precisely corresponds to an individual cluster. An alternating iterative algorithm is designed to optimize the subtasks of learning the similarity graphs adaptively, detecting the consistency as well as cross-graph diversity, fusing the multiple pure graphs, and assigning cluster label to each instance in a mutual reinforcement manner. Extensive experimental results on several benchmark multiview datasets demonstrate the effectiveness of our model, in comparison to several state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Shudong Huang and Ivor W. Tsang and Zenglin Xu and Jiancheng Lv},
  doi          = {10.1109/TNNLS.2022.3201964},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4206-4219},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CGDD: Multiview graph clustering via cross-graph diversity detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consensus clustering with co-association matrix
optimization. <em>TNNLS</em>, <em>35</em>(3), 4192–4205. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus clustering can derive a more promising and robust clustering result by integrating multiple partitions strategically. However, there are several limitations in the existing approaches: 1) most of the methods compute the ensemble-information matrix heuristically and lack of sufficient optimization; 2) the information from the original dataset is rarely considered; and 3) the noise in both label space and feature space is ignored. To address these issues, we proposed a novel consensus clustering method with co-association matrix optimization (CC-CMO), which aims at improving the co-association matrix by taking abundant information from both label space and feature space into consideration. In label space, CC-CMO derives a weighted partition matrix capturing the intercluster correlation and further designs a least squares regression (LSR) model to explore the global structure of data. In feature space, CC-CMO minimizes the reconstruction error with doubly stochastic normalization in the projective subspace to eliminate noise features as well as learn the local affinity of data. To improve the co-association matrix by jointly considering the subspace representation, global structure, and local affinity of data, we explicitly propose a unified optimization framework and design an alternating optimization algorithm for the optimal co-association matrix. Extensive experiments on a variety of real-world datasets demonstrate the superior performance of CC-CMO to the state-of-the-art consensus clustering approaches.},
  archive      = {J_TNNLS},
  author       = {Yifan Shi and Zhiwen Yu and C. L. Philip Chen and Huanqiang Zeng},
  doi          = {10.1109/TNNLS.2022.3201975},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4192-4205},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consensus clustering with co-association matrix optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Community detection via autoencoder-like nonnegative tensor
decomposition. <em>TNNLS</em>, <em>35</em>(3), 4179–4191. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection aims at partitioning a network into several densely connected subgraphs. Recently, nonnegative matrix factorization (NMF) has been widely adopted in many successful community detection applications. However, most existing NMF-based community detection algorithms neglect the multihop network topology and the extreme sparsity of adjacency matrices. To resolve them, we propose a novel conception of adjacency tensor, which extends adjacency matrix to multihop cases. Then, we develop a novel tensor Tucker decomposition-based community detection method—autoencoder-like nonnegative tensor decomposition (ANTD), leveraging the constructed adjacency tensor. Distinct from simply applying tensor decomposition on the constructed adjacency tensor, which only works as a decoder, ANTD also introduces an encoder component to constitute an autoencoder-like architecture, which can further enhance the quality of the detected communities. We also develop an efficient alternative updating algorithm with convergence guarantee to optimize ANTD, and theoretically analyze the algorithm complexity. Moreover, we also study a graph regularized variant of ANTD. Extensive experiments on real-world benchmark networks by comparing 27 state-of-the-art methods, validate the effectiveness, efficiency, and robustness of our proposed methods.},
  archive      = {J_TNNLS},
  author       = {Jiewen Guan and Bilian Chen and Xin Huang},
  doi          = {10.1109/TNNLS.2022.3201906},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4179-4191},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Community detection via autoencoder-like nonnegative tensor decomposition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Differential refinement network for zero-shot learning.
<em>TNNLS</em>, <em>35</em>(3), 4164–4178. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize novel categories by merely utilizing disjoint seen samples. It is a challenging task as the knowledge of unseen objects is forbidden in the training stage, which easily leads to unseen samples degrading to mismatched categories. In order to alleviate the biased recognition problem, in this article, we propose a differential refinement network (DRNet) for ZSL, which aims to explore robust semantic-to-visual embedding. Our DRNet model consists of two subnetworks: basic network and differential network. The basic network targets to generate initial class-specific visual centers conditioned on corresponding semantic prototypes. The differential network is designed to predict class-unrelated differences between visual centers of arbitrary semantic prototype pairs, which are applied to further polish the initial visual centers. The motivation is that, by comparing different prototypes, interactions between various categories will be characterized, benefiting the generation of authentic and discriminative visual centers. Moreover, a modified episode-based training paradigm is explored to optimize the two subnetworks actively. In the training stage, we form a collection of episodes, each of which is an imitated ZSL task. Our DRNet is optimized by those sampled tasks rather than individual samples, which progressively learns skills to adapt and generalize to novel classes. Experiments on four challenging datasets demonstrate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Yi Tian and Yilei Zhang and Yaping Huang and Wanru Xu and Zhengming Ding},
  doi          = {10.1109/TNNLS.2022.3201883},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4164-4178},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Differential refinement network for zero-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and fast joint sparse constrained canonical
correlation analysis for fault detection. <em>TNNLS</em>,
<em>35</em>(3), 4153–4163. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The canonical correlation analysis (CCA) has attracted wide attention in fault detection (FD). To improve the detection performance, we propose a new joint sparse constrained CCA (JSCCCA) model that integrates the $\ell _{2,0}$ -norm joint sparse constraints into classical CCA. The key idea is that JSCCCA can fully exploit the joint sparse structure to determine the number of extracted variables. We then develop an efficient alternating minimization algorithm using the improved iterative hard thresholding and manifold constrained gradient descent method. More importantly, we establish the convergence guarantee with detailed analysis. Finally, we provide extensive numerical studies on the simulated dataset, the benchmark Tennessee Eastman process, and a practical cylinder-piston process. In some cases, the computing time is reduced by 600 times, and the FD rate is increased by 12.62% compared with classical CCA. The results suggest that the proposed approach is efficient and fast.},
  archive      = {J_TNNLS},
  author       = {Xianchao Xiu and Lili Pan and Ying Yang and Wanquan Liu},
  doi          = {10.1109/TNNLS.2022.3201881},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4153-4163},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient and fast joint sparse constrained canonical correlation analysis for fault detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context enhancing representation for semantic segmentation
in remote sensing images. <em>TNNLS</em>, <em>35</em>(3), 4138–4152. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the foundation of image interpretation, semantic segmentation is an active topic in the field of remote sensing. Facing the complex combination of multiscale objects existing in remote sensing images (RSIs), the exploration and modeling of contextual information have become the key to accurately identifying the objects at different scales. Although several methods have been proposed in the past decade, insufficient context modeling of global or local information, which easily results in the fragmentation of large-scale objects, the ignorance of small-scale objects, and blurred boundaries. To address the above issues, we propose a contextual representation enhancement network (CRENet) to strengthen the global context (GC) and local context (LC) modeling in high-level features. The core components of the CRENet are the local feature alignment enhancement module (LFAEM) and the superpixel affinity loss (SAL). The LFAEM aligns and enhances the LC in low-level features by constructing contextual contrast through multilayer cascaded deformable convolution and is then supplemented with high-level features to refine the segmentation map. The SAL assists the network to accurately capture the GC by supervising semantic information and relationship learned from superpixels. The proposed method is plug-and-play and can be embedded in any FCN-based network. Experiments on two popular RSI datasets demonstrate the effectiveness of our proposed network with competitive performance in qualitative and quantitative aspects.},
  archive      = {J_TNNLS},
  author       = {Leyuan Fang and Peng Zhou and Xinxin Liu and Pedram Ghamisi and Siwei Chen},
  doi          = {10.1109/TNNLS.2022.3201820},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4138-4152},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Context enhancing representation for semantic segmentation in remote sensing images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compression of convolutional neural networks with divergent
representation of filters. <em>TNNLS</em>, <em>35</em>(3), 4125–4137.
(<a href="https://doi.org/10.1109/TNNLS.2022.3201846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have made remarkable achievements in many tasks. However, most of them are hardly applied to embedded systems directly because of the requirement of huge memory space and computing power. In this article, we propose a pruning framework, namely, FiltDivNet, to accelerate and compress CNN models for their applicability to small or portable devices. The correlations among filters are taken into account and measured by the goodness of fit. On this basis, a hybrid-cluster pruning strategy is designed with dynamic pruning ratios for different clusters in CNN models. It aims at representing its filters in their diversity by removing redundant ones cluster by cluster. In addition, a new loss function with adaptive sparsity constraints is introduced for the retraining and fine-tuning in the FiltDivNet. Finally, some comparative experiments based on classical CNN models are carried out to demonstrate its effectiveness in compression performance and its adaptability with different CNN architectures.},
  archive      = {J_TNNLS},
  author       = {Peng Lei and Jiawei Liang and Tong Zheng and Jun Wang},
  doi          = {10.1109/TNNLS.2022.3201846},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4125-4137},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Compression of convolutional neural networks with divergent representation of filters},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuromorphic camera denoising using graph neural
network-driven transformers. <em>TNNLS</em>, <em>35</em>(3), 4110–4124.
(<a href="https://doi.org/10.1109/TNNLS.2022.3201830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic vision is a bio-inspired technology that has triggered a paradigm shift in the computer vision community and is serving as a key enabler for a wide range of applications. This technology has offered significant advantages, including reduced power consumption, reduced processing needs, and communication speedups. However, neuromorphic cameras suffer from significant amounts of measurement noise. This noise deteriorates the performance of neuromorphic event-based perception and navigation algorithms. In this article, we propose a novel noise filtration algorithm to eliminate events that do not represent real log-intensity variations in the observed scene. We employ a graph neural network (GNN)-driven transformer algorithm, called GNN-Transformer, to classify every active event pixel in the raw stream into real log-intensity variation or noise. Within the GNN, a message-passing framework, referred to as EventConv, is carried out to reflect the spatiotemporal correlation among the events while preserving their asynchronous nature. We also introduce the known-object ground-truth labeling (KoGTL) approach for generating approximate ground-truth labels of event streams under various illumination conditions. KoGTL is used to generate labeled datasets, from experiments recorded in challenging lighting conditions, including moon light. These datasets are used to train and extensively test our proposed algorithm. When tested on unseen datasets, the proposed algorithm outperforms state-of-the-art methods by at least 8.8% in terms of filtration accuracy. Additional tests are also conducted on publicly available datasets (ETH Zürich Color-DAVIS346 datasets) to demonstrate the generalization capabilities of the proposed algorithm in the presence of illumination variations and different motion dynamics. Compared to state-of-the-art solutions, qualitative results verified the superior capability of the proposed algorithm to eliminate noise while preserving meaningful events in the scene.},
  archive      = {J_TNNLS},
  author       = {Yusra Alkendi and Rana Azzam and Abdulla Ayyad and Sajid Javed and Lakmal Seneviratne and Yahya Zweiri},
  doi          = {10.1109/TNNLS.2022.3201830},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4110-4124},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuromorphic camera denoising using graph neural network-driven transformers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A noniterative supervised on-chip training circuitry for
reservoir computing systems. <em>TNNLS</em>, <em>35</em>(3), 4097–4109.
(<a href="https://doi.org/10.1109/TNNLS.2022.3201828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs) is an exponentially growing field, mainly because of its wide range of applications to everyday life such as pattern recognition or time series forecasting. In particular, reservoir computing (RC) arises as an optimal computational framework suited for temporal/sequential data analysis. The direct on-silicon implementation of RCs may help to minimize power and maximize processing speed, that is especially relevant in edge intelligence applications where energy storage is considerably restricted. Nevertheless, most of the RC hardware solutions present in the literature perform the training process off-chip at the server level, thus increasing processing time and overall power dissipation. Some studies integrate both learning and inference on the same chip, although these works are normally oriented to implement unsupervised learning (UL) [with a lower expected accuracy than supervised learning (SL)], or propose iterative solutions (with a subsequent higher power consumption). Therefore, the integration of RC systems including both inference and a fast noniterative SL method is still an incipient field. In this article, we propose a noniterative SL methodology for RC systems that can be implemented on hardware either sequentially or fully parallel. The proposal presents a considerable advantage in terms of energy efficiency (EE) and processing speed if compared to traditional off-chip methods. In order to prove the validity of the model, a cyclic echo state NN with on-chip learning capabilities for time series prediction has been implemented and tested in a field-programmable gate array (FPGA). Also, a low-cost audio processing method is proposed that may be used to optimize the sound preprocessing steps.},
  archive      = {J_TNNLS},
  author       = {Fabio Galán-Prado and Josep L. Rosselló},
  doi          = {10.1109/TNNLS.2022.3201828},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4097-4109},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A noniterative supervised on-chip training circuitry for reservoir computing systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed stochastic proximal algorithm with random
reshuffling for nonsmooth finite-sum optimization. <em>TNNLS</em>,
<em>35</em>(3), 4082–4096. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nonsmooth finite-sum minimization is a fundamental problem in machine learning. This article develops a distributed stochastic proximal-gradient algorithm with random reshuffling to solve the finite-sum minimization over time-varying multiagent networks. The objective function is a sum of differentiable convex functions and nonsmooth regularization. Each agent in the network updates local variables by local information exchange and cooperates to seek an optimal solution. We prove that local variable estimates generated by the proposed algorithm achieve consensus and are attracted to a neighborhood of the optimal solution with an $\mathcal {O}(({1}/{T})+({1}/{\sqrt {T}}))$ convergence rate, where $T$ is the total number of iterations. Finally, some comparative simulations are provided to verify the convergence performance of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Xia Jiang and Xianlin Zeng and Jian Sun and Jie Chen and Lihua Xie},
  doi          = {10.1109/TNNLS.2022.3201711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4082-4096},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed stochastic proximal algorithm with random reshuffling for nonsmooth finite-sum optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Learning of long-horizon sparse-reward robotic manipulator
tasks with base controllers. <em>TNNLS</em>, <em>35</em>(3), 4072–4081.
(<a href="https://doi.org/10.1109/TNNLS.2022.3201705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) enables robots to perform some intelligent tasks end-to-end. However, there are still many challenges for long-horizon sparse-reward robotic manipulator tasks. On the one hand, a sparse-reward setting causes exploration inefficient. On the other hand, exploration using physical robots is of high cost and unsafe. In this article, we propose a method of learning long-horizon sparse-reward tasks utilizing one or more existing traditional controllers named base controllers in this article. Built upon deep deterministic policy gradients (DDPGs), our algorithm incorporates the existing base controllers into stages of exploration, value learning, and policy update. Furthermore, we present a straightforward way of synthesizing different base controllers to integrate their strengths. Through experiments ranging from stacking blocks to cups, it is demonstrated that the learned state-based or image-based policies steadily outperform base controllers. Compared to previous works of learning from demonstrations, our method improves sample efficiency by orders of magnitude and improves performance. Overall, our method bears the potential of leveraging existing industrial robot manipulation systems to build more flexible and intelligent controllers.},
  archive      = {J_TNNLS},
  author       = {Guangming Wang and Minjian Xin and Wenhua Wu and Zhe Liu and Hesheng Wang},
  doi          = {10.1109/TNNLS.2022.3201705},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4072-4081},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning of long-horizon sparse-reward robotic manipulator tasks with base controllers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Augmented sparse representation for incomplete multiview
clustering. <em>TNNLS</em>, <em>35</em>(3), 4058–4071. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multiview data are collected from multiple sources or characterized by multiple modalities, where the features of some samples or some views may be missing. Incomplete multiview clustering (IMVC) aims to partition the data into different groups by taking full advantage of the complementary information from multiple incomplete views. Most existing methods based on matrix factorization or subspace learning attempt to recover the missing views or perform imputation of the missing features to improve clustering performance. However, this problem is intractable due to a lack of prior knowledge, e.g., label information or data distribution, especially when the missing views or features are completely damaged. In this article, we proposed an augmented sparse representation (ASR) method for IMVC. We first introduce a discriminative sparse representation learning (DSRL) model, which learns the sparse representations of multiple views as applied to measure the similarity of the existing features. The DSRL model explores complementary and consistent information by integrating the sparse regularization item and a consensus regularization item, respectively. Simultaneously, it learns a discriminative dictionary from the original samples. The sparsity constrained optimization problem in the DSRL model can be efficiently solved by the alternating direction method of multipliers (ADMM). Then, we present a similarity fusion scheme, namely, a sparsity augmented fusion of sparse representations, to obtain a sparsity augmented similarity matrix across different views for spectral clustering. Experimental results on several datasets demonstrate the effectiveness of the proposed ASR method for IMVC.},
  archive      = {J_TNNLS},
  author       = {Jie Chen and Shengxiang Yang and Xi Peng and Dezhong Peng and Zhu Wang},
  doi          = {10.1109/TNNLS.2022.3201699},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4058-4071},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Augmented sparse representation for incomplete multiview clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic event-triggered-based adaptive finite-time neural
control for active suspension systems with displacement constraint.
<em>TNNLS</em>, <em>35</em>(3), 4047–4057. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we give an event-based control algorithm for nonlinear active suspension systems (ASSs) with the vertical displacement constraint. For in-vehicle communication networks, the controller area network (CAN) is a widely used standard for interconnecting electronic control units (ECUs). The main task in this work is, thus, to reduce the communication burden on the CAN. To this end, we develop a dynamic event-triggered communication mechanism in ASSs. Meanwhile, in practice, the vehicle safety is degraded when the body vibration exceeds the allowable maximum. Thus, the vertical displacement of ASSs should be constrained within a reliable range. For this purpose, we present a novel finite-time integral barrier Lyapunov function (FTIBLF), which not only guarantees that the vertical displacement constraint bounds are not violated, but also enables the position of the suspension to be stabilized in the neighborhood of a desired position in finite settling time. Furthermore, neural networks (NNs) are utilized to identify the unknown nonlinear characteristics in ASSs subjected to the modeling error and unknown mass. As a result, we propose an adaptive neural control technique based on the dynamic event-triggered condition, which helps to improve the ride comfort while ensuring the driving safety and handling stability. Finally, simulation results are provided to verify the validity of the presented methodology.},
  archive      = {J_TNNLS},
  author       = {Qiang Zeng and Jun Zhao},
  doi          = {10.1109/TNNLS.2022.3201695},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4047-4057},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic event-triggered-based adaptive finite-time neural control for active suspension systems with displacement constraint},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Critical path-based backdoor detection for deep neural
networks. <em>TNNLS</em>, <em>35</em>(3), 4032–4046. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backdoor attack to deep neural networks (DNNs) is among the predominant approaches to bring great threats into artificial intelligence. The existing methods to detect backdoor attacks focus on the perspective of distributions in DNNs, however, limited by its ability of generalization across DNN models. In this article, a critical-path-based backdoor detector (CPBD) is proposed, which approaches to detect backdoor attacks via DNN’s interpretability. CPBD is designed to efficiently discover the characteristics of backdoors, which distinguish the critical paths in the attacked DNNs. To deal with the intractably large number of neurons, we propose to simplify the neurons, and the preserved key nodes are integrated into a set of critical paths. Thus, a DNN model can be formulated as a combination of several critical paths. Afterward, the detection of backdoors is performed based on the analysis of critical paths corresponding to different classes. Then, combining all the above steps, the CPBD algorithm is integrated to present the results in a standard and systematic manner. In addition, CPBD is able to locate neurons associated with malicious triggers, the combination of which is named as trigger propagation path. Extensive experiments are conducted, which testify the efficiency of the proposed method on multiple DNNs and different trigger sizes.},
  archive      = {J_TNNLS},
  author       = {Wei Jiang and Xiangyu Wen and Jinyu Zhan and Xupeng Wang and Ziwei Song and Chen Bian},
  doi          = {10.1109/TNNLS.2022.3201586},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4032-4046},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Critical path-based backdoor detection for deep neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incomplete multiview nonnegative representation learning
with graph completion and adaptive neighbors. <em>TNNLS</em>,
<em>35</em>(3), 4017–4031. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite incomplete multiview clustering (IMC) being widely studied in the past decade, it is still difficult to model the correlation among multiple views due to the absence of partial views. Most existing works for IMC only mine the correlation among multiple views from available views and ignore the importance of missing views. To address this issue, we propose a novel Incomplete Multiview Nonnegative representation learning model with Graph completion and Adaptive neighbors (IMNGA), which performs common graph learning, missing graph completion, and consensus nonnegative representation learning simultaneously. In IMNGA, the common graph on all views and the incomplete graph of each view are used to reconstruct the completed graph of the corresponding view, where the common graph satisfies the neighbor constraints of incomplete multiview data and consensus representation. IMNGA gets consensus representation by factorizing completed and incomplete graphs, where consensus representation satisfies the common graph constraint. IMNGA shows its effectiveness by outperforming other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Shiliang Sun and Nan Zhang},
  doi          = {10.1109/TNNLS.2022.3201562},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4017-4031},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incomplete multiview nonnegative representation learning with graph completion and adaptive neighbors},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time interval-enhanced graph neural network for
shared-account cross-domain sequential recommendation. <em>TNNLS</em>,
<em>35</em>(3), 4002–4016. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared-account cross-domain sequential recommendation (SCSR) task aims to recommend the next item via leveraging the mixed user behaviors in multiple domains. It is gaining immense research attention as more and more users tend to sign up on different platforms and share accounts with others to access domain-specific services. Existing works on SCSR mainly rely on mining sequential patterns via recurrent neural network (RNN)-based models, which suffer from the following limitations: 1) RNN-based methods overwhelmingly target discovering sequential dependencies in single-user behaviors and they are not expressive enough to capture the relationships among multiple entities in SCSR; 2) all existing methods bridge two domains via knowledge transfer in the latent space and ignore the explicit cross-domain graph structure; and 3) none existing studies consider the time interval information among items, which is essential in the sequential recommendation for characterizing different items and learning discriminative representations for them. In this work, we propose a new graph-based solution, namely, time interval-enhanced domain-aware graph convolutional network (TiDA-GCN), to address the above challenges. Specifically, we first link users and items in each domain as a graph. Then, we devise a domain-aware graph convolution network to learn user-specific node representations. To fully account for users’ domain-specific preferences on items, two effective attention mechanisms are further developed to selectively guide the message-passing process. Moreover, to further enhance item- and account-level representation learning, we incorporate the time interval into the message passing and design an account-aware self-attention module for learning items’ interactive characteristics. Experiments demonstrate the superiority of our proposed method from various aspects.},
  archive      = {J_TNNLS},
  author       = {Lei Guo and Jinyu Zhang and Li Tang and Tong Chen and Lei Zhu and Hongzhi Yin},
  doi          = {10.1109/TNNLS.2022.3201533},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4002-4016},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Time interval-enhanced graph neural network for shared-account cross-domain sequential recommendation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Toward pixel-level precision for binary super-resolution
with mixed binary representation. <em>TNNLS</em>, <em>35</em>(3),
3989–4001. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary neural network (BNN) is an effective method for reducing model computational and memory cost, which has achieved much progress in the super-resolution (SR) field. However, there is still a noticeable performance gap between a binary SR network and its full-precision counterpart. Considering that the information density in quantization features is far lower than full-precision features, we aim to improve the precision of quantization features to produce rich-enough output activations for SR task. First, we make several observations that a multibit value could be approximated by multiple 1-bit values, and the computation power of binary convolution could be improved by approximating the multibit convolution process. Then, we propose a mixed binary representation set to approximate multibit activations, which is effective in compensating the quantization precision loss. Finally, we present a new precision-driven binary convolution (PDBC) module, which increases the convolution precision and protects image detail information without extra computation. Compared with normal binary convolution, our method could largely reduce the information loss caused by binarization. In experiments, our methods consistently show superior performance over the baseline models and can surpass state-of-the-art methods in terms of peak signal to noise ratio (PSNR) and visual quality.},
  archive      = {J_TNNLS},
  author       = {Xinrui Jiang and Nannan Wang and Jingwei Xin and Keyu Li and Xi Yang and Jie Li and Xinbo Gao},
  doi          = {10.1109/TNNLS.2022.3201528},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3989-4001},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward pixel-level precision for binary super-resolution with mixed binary representation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network-based fixed-time tracking control for
input-quantized nonlinear systems with actuator faults. <em>TNNLS</em>,
<em>35</em>(3), 3978–3988. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study reports a fixed-time tracking control problem for strict-feedback nonlinear systems with quantized inputs and actuator faults where the total number of faults is allowed to be infinite. By taking advantage of radial basis function neural networks (RBFNNs), unknown nonlinear function terms in the system dynamic model can be effectively approached. In addition, based on the sector property of quantization nonlinearities and the structure of the actuator fault model, novel adaptive estimations and innovative auxiliary design signals are constructed to compensate for the influence caused by actuator faults and quantized inputs properly in the fixed-time convergence settings. Then, rigorous theoretical analysis manifests that the proposed control scheme can make the output tracking error converge to a small neighborhood of the origin within a fixed time, and the upper bound of the setting time not only does not depend on initial states of the system but also can be preassigned by selecting parameters appropriately. Meanwhile, all the signals in the closed-loop system remain bounded. Finally, a numerical example and a practical example of a single-link manipulator are presented to demonstrate the effectiveness of the proposed control algorithm.},
  archive      = {J_TNNLS},
  author       = {Wei Sun and Jing Wu and Shun-Feng Su and Xudong Zhao},
  doi          = {10.1109/TNNLS.2022.3201504},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3978-3988},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based fixed-time tracking control for input-quantized nonlinear systems with actuator faults},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified framework based on graph consensus term for
multiview learning. <em>TNNLS</em>, <em>35</em>(3), 3964–3977. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multiview learning technologies have attracted a surge of interest in the machine learning domain. However, when facing complex and diverse applications, most multiview learning methods mainly focus on specific fields rather than provide a scalable and robust proposal for different tasks. Moreover, most conventional methods used in these tasks are based on single view, which cannot be readily extended into the multiview scenario. Therefore, how to provide an efficient and scalable multiview framework is very necessary yet full of challenges. Inspired by the fact that most of the existing single view algorithms are graph-based ones to learn the complex structures within given data, this article aims at leveraging most existing graph embedding works into one formula via introducing the graph consensus term and proposes a unified and scalable multiview learning framework, termed graph consensus multiview framework (GCMF). GCMF attempts to make full advantage of graph-based works and rich information in the multiview data at the same time. On one hand, the proposed method explores the graph structure in each view independently to preserve the diversity property of graph embedding methods; on the other hand, learned graphs can be flexibly chosen to construct the graph consensus term, which can more stably explore the correlations among multiple views. To this end, GCMF can simultaneously take the diversity and complementary information among different views into consideration. To further facilitate related research, we provide an implementation of the multiview extension for locality linear embedding (LLE), named GCMF-LLE, which can be efficiently solved by applying the alternating optimization strategy. Empirical validations conducted on six benchmark datasets can show the effectiveness of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Xiangzhu Meng and Lin Feng and Chonghui Guo and Huibing Wang and Shu Wu},
  doi          = {10.1109/TNNLS.2022.3201498},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3964-3977},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A unified framework based on graph consensus term for multiview learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). N-level hierarchy-based optimal control to develop
therapeutic strategies for ecological evolutionary dynamics systems.
<em>TNNLS</em>, <em>35</em>(3), 3953–3963. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article mainly proposes an evolutionary algorithm and its first application to develop therapeutic strategies for ecological evolutionary dynamics systems (EEDS), obtaining the balance between tumor cells and immune cells by rationally arranging chemotherapeutic drugs and immune drugs. First, an EEDS nonlinear kinetic model is constructed to describe the relationship between tumor cells, immune cells, dose, and drug concentration. Second, the N-level hierarchy optimization (NLHO) algorithm is designed and compared with five algorithms on 20 benchmark functions, which proves the feasibility and effectiveness of NLHO. Finally, we apply NLHO into EEDS to give a dynamic adaptive optimal control policy and develop therapeutic strategies to reduce tumor cells, while minimizing the harm of chemotherapy drugs and immune drugs to the human body. The experimental results prove the validity of the research method.},
  archive      = {J_TNNLS},
  author       = {Jinze Liu and Jiayue Sun and Huaguang Zhang and Shun Xu and Zifang Zou},
  doi          = {10.1109/TNNLS.2022.3201517},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3953-3963},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {N-level hierarchy-based optimal control to develop therapeutic strategies for ecological evolutionary dynamics systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking prior-guided face super-resolution: A new
paradigm with facial component prior. <em>TNNLS</em>, <em>35</em>(3),
3938–3952. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, facial priors (e.g., facial parsing maps and facial landmarks) have been widely employed in prior-guided face super-resolution (FSR) because it provides the location of facial components and facial structure information, and helps predict the missing high-frequency (HF) information. However, most existing approaches suffer from two shortcomings: 1) the extracted facial priors are inaccurate since they are extracted from low-resolution (LR) or low-quality super-resolved (SR) face images and 2) they only consider embedding facial priors into the reconstruction process from LR to SR face images, thus failing to explore facial priors to generate LR face image. In this article, we propose a novel pre-prior guided approach that extracts facial prior information from original high-resolution (HR) face images and embeds them into LR ones to obtain HF information-rich LR face images, thereby improving the performance of face reconstruction. Specifically, a novel component hybrid method is proposed, which fuses HR facial components and LR facial background to generate new LR face images (namely, LRmix) via facial parsing maps extracted from HR face images. Furthermore, we design a component hybrid network (CHNet) that learns the LR to LRmix mapping function to ensure that the LRmix can be obtained from LR face images in testing and real-world datasets. Experimental results show that our proposed scheme significantly improves the reconstruction performance for FSR.},
  archive      = {J_TNNLS},
  author       = {Tao Lu and Yuanzhi Wang and Yanduo Zhang and Junjun Jiang and Zhongyuan Wang and Zixiang Xiong},
  doi          = {10.1109/TNNLS.2022.3201448},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3938-3952},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rethinking prior-guided face super-resolution: A new paradigm with facial component prior},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning from box annotations for referring image
segmentation. <em>TNNLS</em>, <em>35</em>(3), 3927–3937. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring image segmentation (RIS) has obtained an impressive achievement by fully convolutional networks (FCNs). However, previous RIS methods require a large number of pixel-level annotations. In this article, we present a weakly supervised RIS method by using bounding box (BB) annotations. In the first stage, we introduce an adversarial boundary loss to extract the object contour from the BB, which is then used to select appropriate region proposals for pseudoground-truth (PGT) generation. In the second stage, we design a co-training (Co-T) strategy to purify the pseudolabels. Specifically, we train two networks and interactively guide them to pick clean labels for each other’s networks, which can weaken the effect of noisy labels on model training. Experiment results on four benchmark datasets demonstrate that the proposed method can produce high-quality masks with a speed of 63 frames/s.},
  archive      = {J_TNNLS},
  author       = {Guang Feng and Lihe Zhang and Zhiwei Hu and Huchuan Lu},
  doi          = {10.1109/TNNLS.2022.3201372},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3927-3937},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning from box annotations for referring image segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep ensemble dynamic learning network for corona virus
disease 2019 diagnosis. <em>TNNLS</em>, <em>35</em>(3), 3912–3926. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corona virus disease 2019 is an extremely fatal pandemic around the world. Intelligently recognizing X-ray chest radiography images for automatically identifying corona virus disease 2019 from other types of pneumonia and normal cases provides clinicians with tremendous conveniences in diagnosis process. In this article, a deep ensemble dynamic learning network is proposed. After a chain of image preprocessing steps and the division of image dataset, convolution blocks and the final average pooling layer are pretrained as a feature extractor. For classifying the extracted feature samples, two-stage bagging dynamic learning network is trained based on neural dynamic learning and bagging algorithms, which diagnoses the presence and types of pneumonia successively. Experimental results manifest that using the proposed deep ensemble dynamic learning network obtains 98.7179% diagnosis accuracy, which indicates more excellent diagnosis effect than existing state-of-the-art models on the open image dataset. Such accurate diagnosis effects provide convincing evidences for further detections and treatments.},
  archive      = {J_TNNLS},
  author       = {Zhijun Zhang and Bozhao Chen and Yamei Luo},
  doi          = {10.1109/TNNLS.2022.3201198},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3912-3926},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deep ensemble dynamic learning network for corona virus disease 2019 diagnosis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JORA: Weakly supervised user identity linkage via jointly
learning to represent and align. <em>TNNLS</em>, <em>35</em>(3),
3900–3911. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The user identity linkage that establishes correspondence between users across networks is a fundamental issue in various social network applications. Efforts have recently been devoted to introducing network embedding techniques that map the different network users into the common representation space, thereby inferring user correspondence based on the similarities of their representations. However, existing studies that separately train the network embedding and space alignment in two stages may lead to conflict between the objectives of the two stages. Besides, the similarities between unlabeled cross-network user pairs are difficult to define and largely impact the result. Moreover, many previous methods still need plenty of labeled aligned user pairs to ensure performance, which may not be available. To address the above problems, we propose to solve the weakly-supervised user identity linkage problem via JOintly learning to Represent and Align, i.e., the JORA model. The architecture of JORA adopts the inductive graph convolutional network (GCN) that learns representations for each network. The model is jointly optimized by the representation learning component and alignment learning component. The former one aims to preserve the similarities between intranetwork users. The latter one aligns the different spaces by a projection function and aims to preserve the similarities between cross-network users. A specific attention mechanism is proposed to learn self-adaptive similarities for unlabeled user pairs during alignment learning and it reduces the error propagation caused by predefined similarities. The joint optimization helps perceive network characteristics during alignment and reduces the number of labeled users required. Experiments conducted on real social networks show that the proposed model achieves significantly better performance than the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Conghui Zheng and Li Pan and Peng Wu},
  doi          = {10.1109/TNNLS.2022.3201102},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3900-3911},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {JORA: Weakly supervised user identity linkage via jointly learning to represent and align},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global search and analysis for the nonconvex two-level ℓ₁
penalty. <em>TNNLS</em>, <em>35</em>(3), 3886–3899. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imposing suitably designed nonconvex regularization is effective to enhance sparsity, but the corresponding global search algorithm has not been well established. In this article, we propose a global search algorithm for the nonconvex two-level $\ell _{1}$ penalty based on its piecewise linear property and apply it to machine learning tasks. With the search capability, the optimization performance of the proposed algorithm could be improved, resulting in better sparsity and accuracy than most state-of-the-art global and local algorithms. Besides, we also provide an approximation analysis to demonstrate the effectiveness of our global search algorithm in sparse quantile regression.},
  archive      = {J_TNNLS},
  author       = {Fan He and Mingzhen He and Lei Shi and Xiaolin Huang},
  doi          = {10.1109/TNNLS.2022.3201052},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3886-3899},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global search and analysis for the nonconvex two-level ℓ₁ penalty},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-stage selective fusion framework for joint intent
detection and slot filling. <em>TNNLS</em>, <em>35</em>(3), 3874–3885.
(<a href="https://doi.org/10.1109/TNNLS.2022.3202562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spoken language understanding (SLU) is the core of the speech-centric human–robot interaction system, which mainly involves intent detection and slot filling. The recent SLU research focuses on the joint modeling of the two tasks due to their correlation. Furthermore, the slot information consists of slot position and slot type. Although the slot types are semantically related to the intent, the slot positions of the same intent may vary a lot in different utterances due to the diversity of spoken language. Thus, the conventional one-stage slot filling task may introduce unrelated information for slot position prediction in the slot–intent interaction of the joint modeling. Therefore, we propose a novel two-stage selective fusion framework for joint intent detection and slot filling. Unlike the previous one-stage framework, the proposed framework decomposes the slot filling into two stages, i.e., the slot proposal and slot classification. The slot proposal network consisting of BERT and bidirectional long short-term memory (Bi-LSTM)-conditional random field (CRF) predicts the slot positions. Instead of the tokenwise fusion in the existing methods, the slot–intent feature fusion is only performed in the slot classification. A selective fusion mechanism is designed to facilitate the slot–intent interaction within each slot candidate for more accurate slot-type classification. Experiments on five standard benchmarks (i.e., ATIS, SNIPS, MixATIS, MixSNIPS, and DSTC4) show that the proposed framework achieves the best performance in comparison with several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Ziyu Ma and Bin Sun and Shutao Li},
  doi          = {10.1109/TNNLS.2022.3202562},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3874-3885},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A two-stage selective fusion framework for joint intent detection and slot filling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A hierarchical attention network for cross-domain group
recommendation. <em>TNNLS</em>, <em>35</em>(3), 3859–3873. (<a
href="https://doi.org/10.1109/TNNLS.2022.3200480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many online services allow users to participate in various group activities such as online meeting or group buying, and thus need to provide user groups with services that they are interested. The group recommender systems (GRSs) emerge as required and provide personalized services for various online user groups. Data sparsity is an important issue in GRSs, since even fewer group–item interactions are observed. Moreover, the group and the group members have complex and mutual relationships with each other, which exacerbates the difficulty in modeling the preferences of both a group and its members for recommendation. The cross-domain recommender system (CDRS) is a solution to alleviate data sparsity and assist preference modeling by transferring knowledge from a source domain which has relatively dense data to another. The existing CDRSs are usually developed for individual users and cannot be directly applied for group recommendation. To alleviate the data sparsity issue in GRSs, we first study the cross-domain group recommendation problem and propose a hierarchical attention network-based cross-domain group recommendation method, called HAN-CDGR. HAN-CDGR takes the advantage of data from a source domain to benefit recommendation generation for both the individual users and groups in the target domain which has data sparsity and cannot generate accurate recommendation. In HAN-CDGR, a hierarchical attention network is constructed to learn and model individual and group preferences, with consideration of both group members’ interactions and dynamic weights and the complex relationships between individuals and groups. Adversarial learning is used to effectively transfer knowledge from a source domain to the target domain. Extensive experiments, which demonstrate the effectiveness and superiority of our proposal, providing accurate recommendation for both individual users and groups, are conducted on three tasks.},
  archive      = {J_TNNLS},
  author       = {Ruxia Liang and Qian Zhang and Jianqiang Wang and Jie Lu},
  doi          = {10.1109/TNNLS.2022.3200480},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3859-3873},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hierarchical attention network for cross-domain group recommendation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A prediction-sampling-based multilayer-structured latent
factor model for accurate representation to high-dimensional and sparse
data. <em>TNNLS</em>, <em>35</em>(3), 3845–3858. (<a
href="https://doi.org/10.1109/TNNLS.2022.3200009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing highly accurate representation learning on a high-dimensional and sparse (HiDS) matrix is of great significance in a big data-related application such as a recommender system. A latent factor (LF) model is one of the most efficient approaches to the HiDS matrix representation. However, an LF model’s representation learning ability relies heavily on an HiDS matrix’s known data density, which is extremely low due to numerous missing data entities. To address this issue, this work proposes a prediction-sampling-based multilayer-structured LF (PMLF) model with twofold ideas: 1) constructing a loosely connected multilayered LF architecture to increase the known data density of an input HiDS matrix by generating synthetic data layer by layer and 2) constraining this synthetic data generating process through a random prediction-sampling strategy and nonlinear activations to avoid overfitting. In the experiments, PMLF is compared with six state-of-the-art LF-and deep neural network (DNN)-based models on four HiDS matrices from industrial applications. The results demonstrate that PMLF outperforms its peers in well-balancing prediction accuracy and computational efficiency.},
  archive      = {J_TNNLS},
  author       = {Di Wu and Xin Luo and Yi He and Mengchu Zhou},
  doi          = {10.1109/TNNLS.2022.3200009},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3845-3858},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A prediction-sampling-based multilayer-structured latent factor model for accurate representation to high-dimensional and sparse data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward certified robustness of distance metric learning.
<em>TNNLS</em>, <em>35</em>(3), 3834–3844. (<a
href="https://doi.org/10.1109/TNNLS.2022.3199902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric learning aims to learn a distance metric such that semantically similar instances are pulled together while dissimilar instances are pushed away. Many existing methods consider maximizing or at least constraining a distance margin in the feature space that separates similar and dissimilar pairs of instances to guarantee their generalization ability. In this article, we advocate imposing an adversarial margin in the input space so as to improve the generalization and robustness of metric learning algorithms. We first show that the adversarial margin, defined as the distance between training instances and their closest adversarial examples in the input space, takes account of both the distance margin in the feature space and the correlation between the metric and triplet constraints. Next, to enhance robustness to instance perturbation, we propose to enlarge the adversarial margin through minimizing a derived novel loss function termed the perturbation loss. The proposed loss can be viewed as a data-dependent regularizer and easily plugged into any existing metric learning methods. Finally, we show that the enlarged margin is beneficial to the generalization ability by using the theoretical technique of algorithmic robustness. Experimental results on 16 datasets demonstrate the superiority of the proposed method over existing state-of-the-art methods in both discrimination accuracy and robustness against possible noise.},
  archive      = {J_TNNLS},
  author       = {Xiaochen Yang and Yiwen Guo and Mingzhi Dong and Jing-Hao Xue},
  doi          = {10.1109/TNNLS.2022.3199902},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3834-3844},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward certified robustness of distance metric learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental embedding learning with disentangled
representation translation. <em>TNNLS</em>, <em>35</em>(3), 3821–3833.
(<a href="https://doi.org/10.1109/TNNLS.2022.3199816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans are capable of accumulating knowledge by sequentially learning different tasks, while neural networks fail to achieve this due to catastrophic forgetting problems. Most current incremental learning methods focus more on tackling catastrophic forgetting for traditional classification networks. Notably, however, embedding networks that are basic architectures for many metric learning applications also suffer from this problem. Moreover, the most significant difficulty for continual embedding networks is that the relationships between the latent features and prototypes of previous tasks will be destroyed once new tasks have been learned. Accordingly, we propose a novel incremental method for embedding networks, called the disentangled representation translation (DRT) method, to obtain the discriminative class-disentangled features without reusing any samples of previous tasks and while avoiding the perturbation of task-related information. Next, a mask-guided module is specifically explored to adaptively change or retain the valuable information of latent features. This module enables us to effectively preserve the discriminative yet representative features in the disentangled translation process. In addition, DRT can easily be equipped with a regularization item of incremental learning to further improve performance. We conduct extensive experiments on four popular datasets; as the experimental results clearly demonstrate, our method can effectively alleviate the catastrophic forgetting problem for embedding networks.},
  archive      = {J_TNNLS},
  author       = {Kun Wei and Da Chen and Yuhong Li and Xu Yang and Cheng Deng and Dacheng Tao},
  doi          = {10.1109/TNNLS.2022.3199816},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3821-3833},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incremental embedding learning with disentangled representation translation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A progressive subnetwork searching framework for dynamic
inference. <em>TNNLS</em>, <em>35</em>(3), 3809–3820. (<a
href="https://doi.org/10.1109/TNNLS.2022.3199703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) model compression is a popular and important optimization method for efficient and fast hardware acceleration. However, the compressed model is usually fixed, without the capability to tune the computing complexity (i.e., latency in hardware) on-the-fly, depending on dynamic latency requirements, workloads, and computing hardware resource allocation. To address this challenge, dynamic DNN with run-time adaption of computing structures has been constructed through training with a cross-entropy objective function consisting of multiple subnets sampled from the supernet. Our investigations in this work show that the performance of dynamic inference highly relies on the quality of subnet sampling. To construct a dynamic DNN with multiple high-quality subnets, we propose a progressive subnetwork searching framework, which is embedded with several proposed new techniques, including trainable noise ranking, channel-group sampling, selective fine-tuning, and subnet filtering. Our proposed framework empowers the target dynamic DNN with higher accuracy for all the subnets compared with prior works on both the Canadian Institute for Advanced Research dataset with 10 classes (CIFAR-10) and ImageNet datasets. Specifically, compared with United States-Neural Network (US-NN), our method achieves 0.9% average accuracy gain for Alexnet, 2.5% for ResNet18, 1.1% for Visual Geometry Group (VGG)11, and 0.58% for MobileNetv1, on the ImageNet dataset, respectively. Moreover, to demonstrate run-time tuning of computing latency of dynamic DNN in real computing system, we have deployed our constructed dynamic networks into Nvidia Titan graphics processing unit (GPU) and Intel Xeon central processing unit (CPU), showing great improvement over prior works. The code is available at https://github.com/ASU-ESIC-FAN-Lab/Dynamic-inference .},
  archive      = {J_TNNLS},
  author       = {Li Yang and Zhezhi He and Yu Cao and Deliang Fan},
  doi          = {10.1109/TNNLS.2022.3199703},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3809-3820},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A progressive subnetwork searching framework for dynamic inference},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Slimming neural networks using adaptive connectivity scores.
<em>TNNLS</em>, <em>35</em>(3), 3794–3808. (<a
href="https://doi.org/10.1109/TNNLS.2022.3198580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, deep neural network (DNN) pruning methods fall into two categories: 1) weight-based deterministic constraints and 2) probabilistic frameworks. While each approach has its merits and limitations, there are a set of common practical issues such as trial-and-error to analyze sensitivity and hyper-parameters to prune DNNs, which plague them both. In this work, we propose a new single-shot, fully automated pruning algorithm called slimming neural networks using adaptive connectivity scores (SNACS). Our proposed approach combines a probabilistic pruning framework with constraints on the underlying weight matrices, via a novel connectivity measure, at multiple levels to capitalize on the strengths of both approaches while solving their deficiencies. In SNACS, we propose a fast hash-based estimator of adaptive conditional mutual information (ACMI), that uses a weight-based scaling criterion, to evaluate the connectivity between filters and prune unimportant ones. To automatically determine the limit up to which a layer can be pruned, we propose a set of operating constraints that jointly define the upper pruning percentage limits across all the layers in a deep network. Finally, we define a novel sensitivity criterion for filters that measures the strength of their contributions to the succeeding layer and highlights critical filters that need to be completely protected from pruning. Through our experimental validation, we show that SNACS is faster by over $17\times $ the nearest comparable method and is the state-of-the-art single-shot pruning method across four standard Dataset-DNN pruning benchmarks: CIFAR10-VGG16, CIFAR10-ResNet56, CIFAR10-MobileNetv2, and ILSVRC2012-ResNet50.},
  archive      = {J_TNNLS},
  author       = {Madan Ravi Ganesh and Dawsin Blanchard and Jason J. Corso and Salimeh Yasaei Sekeh},
  doi          = {10.1109/TNNLS.2022.3198580},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3794-3808},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Slimming neural networks using adaptive connectivity scores},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust self-ensembling network for hyperspectral image
classification. <em>TNNLS</em>, <em>35</em>(3), 3780–3793. (<a
href="https://doi.org/10.1109/TNNLS.2022.3198142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has shown the great potential of deep learning algorithms in the hyperspectral image (HSI) classification task. Nevertheless, training these models usually requires a large amount of labeled data. Since the collection of pixel-level annotations for HSI is laborious and time-consuming, developing algorithms that can yield good performance in the small sample size situation is of great significance. In this study, we propose a robust self-ensembling network (RSEN) to address this problem. The proposed RSEN consists of two subnetworks including a base network and an ensemble network. With the constraint of both the supervised loss from the labeled data and the unsupervised loss from the unlabeled data, the base network and the ensemble network can learn from each other, achieving the self-ensembling mechanism. To the best of our knowledge, the proposed method is the first attempt to introduce the self-ensembling technique into the HSI classification task, which provides a different view on how to utilize the unlabeled data in HSI to assist the network training. We further propose a novel consistency filter to increase the robustness of self-ensembling learning. Extensive experiments on three benchmark HSI datasets demonstrate that the proposed algorithm can yield competitive performance compared with the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yonghao Xu and Bo Du and Liangpei Zhang},
  doi          = {10.1109/TNNLS.2022.3198142},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3780-3793},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust self-ensembling network for hyperspectral image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cascaded attention: Adaptive and gated graph attention
network for multiagent reinforcement learning. <em>TNNLS</em>,
<em>35</em>(3), 3769–3779. (<a
href="https://doi.org/10.1109/TNNLS.2022.3197918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the interactive relationships of agents is critical to improving the collaborative capability of a multiagent system. Some methods model these by predefined rules. However, due to the nonstationary problem, the interactive relationship changes over time and cannot be well captured by rules. Other methods adopt a simple mechanism such as an attention network to select the neighbors the current agent should collaborate with. However, in large-scale multiagent systems, collaborative relationships are too complicated to be described by a simple attention network. We propose an adaptive and gated graph attention network (AGGAT), which models the interactive relationships between agents in a cascaded manner. In the AGGAT, we first propose a graph-based hard attention network that roughly filters irrelevant agents. Then, normal soft attention is adopted to decide the importance of each neighbor. Finally, gated attention further refines the collaborative relationship of agents. By using cascaded attention, the collaborative relationship of agents is precisely learned in a coarse-to-fine style. Extensive experiments are conducted on a variety of cooperative tasks. The results indicate that our proposed method outperforms state-of-the-art baselines.},
  archive      = {J_TNNLS},
  author       = {Shuhan Qi and Xinhao Huang and Peixi Peng and Xuzhong Huang and Jiajia Zhang and Xuan Wang},
  doi          = {10.1109/TNNLS.2022.3197918},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3769-3779},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cascaded attention: Adaptive and gated graph attention network for multiagent reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential-critic GAN: Generating what you want by a cue
of preferences. <em>TNNLS</em>, <em>35</em>(3), 3754–3768. (<a
href="https://doi.org/10.1109/TNNLS.2022.3197313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes differential-critic generative adversarial network (DiCGAN) to learn the distribution of user-desired data when only partial instead of the entire dataset possesses the desired property. DiCGAN generates desired data that meet the user’s expectations and can assist in designing biological products with desired properties. Existing approaches select the desired samples first and train regular GANs on the selected samples to derive the user-desired data distribution. However, the selection of the desired data relies on global knowledge and supervision over the entire dataset. DiCGAN introduces a differential critic that learns from pairwise preferences, which are local knowledge and can be defined on a part of training data. The critic is built by defining an additional ranking loss over the Wasserstein GAN’s critic. It endows the difference of critic values between each pair of samples with the user preference and guides the generation of the desired data instead of the whole data. For a more efficient solution to ensure data quality, we further reformulate DiCGAN as a constrained optimization problem, based on which we theoretically prove the convergence of our DiCGAN. Extensive experiments on a diverse set of datasets with various applications demonstrate that our DiCGAN achieves state-of-the-art performance in learning the user-desired data distributions, especially in the cases of insufficient desired data and limited supervision.},
  archive      = {J_TNNLS},
  author       = {Yinghua Yao and Yuangang Pan and Ivor W. Tsang and Xin Yao},
  doi          = {10.1109/TNNLS.2022.3197313},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3754-3768},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Differential-critic GAN: Generating what you want by a cue of preferences},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A synthetic minority oversampling technique based on
gaussian mixture model filtering for imbalanced data classification.
<em>TNNLS</em>, <em>35</em>(3), 3740–3753. (<a
href="https://doi.org/10.1109/TNNLS.2022.3197156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data imbalance is a common phenomenon in machine learning. In the imbalanced data classification, minority samples are far less than majority samples, which makes it difficult for minority to be effectively learned by classifiers. A synthetic minority oversampling technique (SMOTE) improves the sensitivity of classifiers to minority by synthesizing minority samples without repetition. However, the process of synthesizing new samples in the SMOTE algorithm may lead to problems such as “noisy samples” and “boundary samples.” Based on the above description, we propose a synthetic minority oversampling technique based on Gaussian mixture model filtering (GMF-SMOTE). GMF-SMOTE uses the expected maximum algorithm based on the Gaussian mixture model to group the imbalanced data. Then, the expected maximum filtering algorithm is used to filter out the “noisy samples” and “boundary samples” in the subclasses after grouping. Finally, to synthesize majority and minority samples, we design two dynamic oversampling ratios. Experimental results show that the GMF-SMOTE performs better than the traditional oversampling algorithms on 20 UCI datasets. The population averages of sensitivity and specificity indexes of random forest (RF) on the UCI datasets synthesized by GMF-SMOTE are 97.49% and 97.02%, respectively. In addition, we also record the G-mean and MCC indexes of the RF, which are 97.32% and 94.80%, respectively, significantly better than the traditional oversampling algorithms. More importantly, the two statistical tests show that GMF-SMOTE is significantly better than the traditional oversampling algorithms.},
  archive      = {J_TNNLS},
  author       = {Zhaozhao Xu and Derong Shen and Yue Kou and Tiezheng Nie},
  doi          = {10.1109/TNNLS.2022.3197156},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3740-3753},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A synthetic minority oversampling technique based on gaussian mixture model filtering for imbalanced data classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical context-based emotion recognition with scene
graphs. <em>TNNLS</em>, <em>35</em>(3), 3725–3739. (<a
href="https://doi.org/10.1109/TNNLS.2022.3196831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a better intention inference, we often try to figure out the emotional states of other people in social communications. Many studies on affective computing have been carried out to infer emotions through perceiving human states, i.e., facial expression and body posture. Such methods are skillful in a controlled environment. However, it often leads to misestimation due to the deficiency of effective inputs in unconstrained circumstances, that is, where context-aware emotion recognition appeared. We take inspiration from the advanced reasoning pattern of humans in perceived emotion recognition and propose the hierarchical context-based emotion recognition method with scene graphs. We propose to extract three contexts from the image, i.e., the entity context, the global context, and the scene context. The scene context contains abstract information about entity labels and their relationships. It is similar to the information processing of the human visual sensing mechanism. After that, these contexts are further fused to perform emotion recognition. We carried out a bunch of experiments on the widely used context-aware emotion datasets, i.e., CAER-S, EMOTIC, and BOdy Language Dataset (BoLD). We demonstrate that the hierarchical contexts can benefit emotion recognition by improving the accuracy of the SOTA score from 84.82% to 90.83% on CAER-S. The ablation experiments show that hierarchical contexts provide complementary information. Our method improves the F1 score of the SOTA result from 29.33% to 30.24% (C-F1) on EMOTIC. We also build the image-based emotion recognition task with BoLD-Img from BoLD and obtain a better emotion recognition score (ERS) score of 0.2153.},
  archive      = {J_TNNLS},
  author       = {Shichao Wu and Lei Zhou and Zhengxi Hu and Jingtai Liu},
  doi          = {10.1109/TNNLS.2022.3196831},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3725-3739},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical context-based emotion recognition with scene graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exponentially synchronous results for delayed neural
networks with leakage delay via switched delay idea and AED-ADT method.
<em>TNNLS</em>, <em>35</em>(3), 3713–3724. (<a
href="https://doi.org/10.1109/TNNLS.2022.3196402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time delay has always been one of the main factors affecting the application performance of neural network (NN) systems, and dynamic performance research of NNs with time delays has been the focus of many scholars in recent years. This article enquires into the exponentially synchronous problem of switched delayed NNs with time delay in the leakage term. Adopting an unusual form from a common switched system, the switching modes of the switched delayed NNs system in this article are dependent on time delays. In the first place, the master, slave, and error NNs models are reconstructed into the switched form by introducing the switched delay idea. Then with the help of the admissible edge-dependent average dwell time (AED-ADT) method and delay-dependent switching adjustment indicators, a novel set of generalized delay-mode-dependent multiple Lyapunov–Krasovskii functionals (MLKFs) is built for analyzing the cases where a state-feedback controller exists and does not exist in the model, and where parts of LKFs may increase during the period when the corresponding subsystems are activated. For these cases, several effective exponential synchronization criteria and switching laws are presented accordingly. At last, the verification of the theoretical results is shown through a few examples.},
  archive      = {J_TNNLS},
  author       = {Xiaoyu Zhang and Degang Wang and Bin Yang and Kaoru Ota and Mianxiong Dong and Hongxing Li},
  doi          = {10.1109/TNNLS.2022.3196402},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3713-3724},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponentially synchronous results for delayed neural networks with leakage delay via switched delay idea and AED-ADT method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuroadaptive output formation tracking for heterogeneous
nonlinear multiagent systems with multiple nonidentical leaders.
<em>TNNLS</em>, <em>35</em>(3), 3702–3712. (<a
href="https://doi.org/10.1109/TNNLS.2022.3196118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the practical time-varying output formation tracking (TVOFT) problem for heterogeneous nonlinear multiagent systems (MASs) having multiple leaders, where agents herein could have heterogeneous dynamics and interact with each other under event-triggered communications. It is required that the outputs of followers not only track the predefined convex combination of multiple leaders but also achieve the desired time-varying formation simultaneously. The existing works on formation tracking problems for MASs with multiple leaders depend on the assumption that each follower is a well-informed or uninformed follower, where the well-informed follower is required to have all the leaders as its neighbor. To remove the limitation, a fully distributed observer-based formation tracking control protocol is developed and employed. First, an adaptive state observer with an edge-based event-triggered mechanism for estimating the states of multiple leaders is proposed based on the neighboring interactions, which eliminates the unexpected Zeno behavior. Second, a novel observer is constructed for each follower by exploiting the output information of the follower, in which the adaptive neural network (NN)-based approximation is exploited to compensate for the unknown nonlinearity. A practical TVOFT control protocol is then generated by the proposed observers, where the parameters are determined by an algorithm including five steps. With the help of Lyapunov stability theory and output regulation method, a practical TVOFT criterion for the considered closed-loop system is derived. Finally, the effectiveness of the proposed control scheme is illustrated by a numerical example.},
  archive      = {J_TNNLS},
  author       = {Xiwang Dong and Qing Wang and Jianglong Yu and Jinhu Lü and Zhang Ren},
  doi          = {10.1109/TNNLS.2022.3196118},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3702-3712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuroadaptive output formation tracking for heterogeneous nonlinear multiagent systems with multiple nonidentical leaders},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient spiking neural networks with radix encoding.
<em>TNNLS</em>, <em>35</em>(3), 3689–3701. (<a
href="https://doi.org/10.1109/TNNLS.2022.3195918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have advantages in latency and energy efficiency over traditional artificial neural networks (ANNs) due to their event-driven computation mechanism and the replacement of energy-consuming weight multiplication with addition. However, to achieve high accuracy, it usually requires long spike trains to ensure accuracy, usually more than 1000 time steps. This offsets the computation efficiency brought by SNNs because a longer spike train means a larger number of operations and larger latency. In this article, we propose a radix-encoded SNN, which has ultrashort spike trains. Specifically, it is able to use less than six time steps to achieve even higher accuracy than its traditional counterpart. We also develop a method to fit our radix encoding technique into the ANN-to-SNN conversion approach so that we can train radix-encoded SNNs more efficiently on mature platforms and hardware. Experiments show that our radix encoding can achieve $25\times $ improvement in latency and 1.7% improvement in accuracy compared to the state-of-the-art method using the VGG-16 network on the CIFAR-10 dataset.},
  archive      = {J_TNNLS},
  author       = {Zhehui Wang and Xiaozhe Gu and Rick Siow Mong Goh and Joey Tianyi Zhou and Tao Luo},
  doi          = {10.1109/TNNLS.2022.3195918},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3689-3701},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient spiking neural networks with radix encoding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Theoretical exploration of flexible transmitter model.
<em>TNNLS</em>, <em>35</em>(3), 3674–3688. (<a
href="https://doi.org/10.1109/TNNLS.2022.3195909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network models generally involve two important components, i.e., network architecture and neuron model. Although there are abundant studies about network architectures, only a few neuron models have been developed, such as the MP neuron model developed in 1943 and the spiking neuron model developed in the 1950s. Recently, a new bio-plausible neuron model, flexible transmitter (FT) model (Zhang and Zhou, 2021), has been proposed. It exhibits promising behaviors, particularly on temporal–spatial signals, even when simply embedded into the common feedforward network architecture. This article attempts to understand the properties of the FT network (FTNet) theoretically. Under mild assumptions, we show that: 1) FTNet is a universal approximator; 2) the approximation complexity of FTNet can be exponentially smaller than those of commonly used real-valued neural networks with feedforward/recurrent architectures and is of the same order in the worst case; and 3) any local minimum of FTNet is the global minimum, implying that it is possible to identify global minima by local search algorithms.},
  archive      = {J_TNNLS},
  author       = {Jin-Hui Wu and Shao-Qun Zhang and Yuan Jiang and Zhi-Hua Zhou},
  doi          = {10.1109/TNNLS.2022.3195909},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3674-3688},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Theoretical exploration of flexible transmitter model},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bisection neural network toward reconfigurable hardware
implementation. <em>TNNLS</em>, <em>35</em>(3), 3663–3673. (<a
href="https://doi.org/10.1109/TNNLS.2022.3195821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A hardware-friendly bisection neural network (BNN) topology is proposed in this work for approximately implementing massive pieces of complex functions in arbitrary on-chip configurations. Instead of the conventional reconfigurable fully connected neural network (FC-NN) circuit topology, the proposed hardware-friendly topology performs NN behaviors in a bisection structure, in which each neuron includes two constant synapse connections for both inputs and outputs. Compared with the FC-NN one, the reconfiguration of the BNN circuit topology eliminates the remarkable amount of dummy synapse connections in hardware. As the main target application, this work aims at building a general-purpose BNN circuit topology that offers a great amount of NN regressions. To achieve this target, we prove that the NN behaviors of the FC-NN circuit topologies can be migrated to the BNN circuit topologies equivalently. We introduce two approaches including the refining training algorithm and the inverted-pyramidal strategy to further reduce the number of neurons and synapses. Finally, we conduct the inaccuracy tolerance analysis to suggest the guideline for ultra-efficient hardware implementations. Compared with the state-of-the-art FC-NN circuit topology-based TrueNorth baseline, the proposed design can achieve 17.8– $22.2\times $ hardware reduction and less than 1% inaccuracy.},
  archive      = {J_TNNLS},
  author       = {Yan Chen and Renyuan Zhang and Yirong Kan and Sa Yang and Yasuhiko Nakashima},
  doi          = {10.1109/TNNLS.2022.3195821},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3663-3673},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bisection neural network toward reconfigurable hardware implementation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). EAD-GAN: A generative adversarial network for disentangling
affine transforms in images. <em>TNNLS</em>, <em>35</em>(3), 3652–3662.
(<a href="https://doi.org/10.1109/TNNLS.2022.3195533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a generative adversarial network called explicit affine disentangled generative adversarial network (EAD-GAN), which explicitly disentangles affine transform in a self-supervised manner. We propose an affine transform regularizer to force the InfoGAN to have explicit properties of affine transform. To facilitate training an affine transform encoder, we decompose the affine matrix into two separate matrices and infer the explicit transform parameters by the least-squares method. Unlike the existing approaches, representations learned by the proposed EAD-GAN have clear physical meaning, where transforms, such as rotation, horizontal and vertical zooms, skews, and translations, are explicitly learned from training data. Thus, we set different values of each transform parameter individually to generate specifically affine transformed data by the learned network. We show that the proposed EAD-GAN successfully disentangles these attributes on the MNIST, CelebA, and dSprites datasets. EAD-GAN achieves higher disentanglement scores with a large margin compared to the state-of-the-art methods on the dSprites dataset. For example, on the dSprites dataset, EAD-GAN achieves the MIG and DCI score of 0.59 and 0.96 respectively, compared to 0.37 and 0.71, respectively, for the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Letao Liu and Xudong Jiang and Martin Saerbeck and Justin Dauwels},
  doi          = {10.1109/TNNLS.2022.3195533},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3652-3662},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {EAD-GAN: A generative adversarial network for disentangling affine transforms in images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning disentangled graph convolutional networks locally
and globally. <em>TNNLS</em>, <em>35</em>(3), 3640–3651. (<a
href="https://doi.org/10.1109/TNNLS.2022.3195336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) emerge as the most successful learning models for graph-structured data. Despite their success, existing GCNs usually ignore the entangled latent factors typically arising in real-world graphs, which results in nonexplainable node representations. Even worse, while the emphasis has been placed on local graph information, the global knowledge of the entire graph is lost to a certain extent. In this work, to address these issues, we propose a novel framework for GCNs, termed LGD-GCN, taking advantage of both local and global information for disentangling node representations in the latent space. Specifically, we propose to represent a disentangled latent continuous space with a statistical mixture model, by leveraging neighborhood routing mechanism locally. From the latent space, various new graphs can then be disentangled and learned, to overall reflect the hidden structures with respect to different factors. On the one hand, a novel regularizer is designed to encourage interfactor diversity for model expressivity in the latent space. On the other hand, the factor-specific information is encoded globally via employing a message passing along these new graphs, in order to strengthen intrafactor consistency. Extensive evaluations on both synthetic and five benchmark datasets show that LGD-GCN brings significant performance gains over the recent competitive models in both disentangling and node classification. Particularly, LGD-GCN is able to outperform averagely the disentangled state-of-the-arts by 7.4% on social network datasets.},
  archive      = {J_TNNLS},
  author       = {Jingwei Guo and Kaizhu Huang and Xinping Yi and Rui Zhang},
  doi          = {10.1109/TNNLS.2022.3195336},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3640-3651},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning disentangled graph convolutional networks locally and globally},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Koopman-based MPC with learned dynamics: Hierarchical neural
network approach. <em>TNNLS</em>, <em>35</em>(3), 3630–3639. (<a
href="https://doi.org/10.1109/TNNLS.2022.3194958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a data-driven control strategy for nonlinear dynamical systems, enabling the construction of a Koopman-based linear system associated with nonlinear dynamics. The primary idea is to apply the deep learning technique to the Koopman framework for globally linearizing nonlinear dynamics and impose a Koopman-based model predictive control (MPC) approach to stabilize the nonlinear dynamical systems. In this work, we first generalize the Koopman framework to nonlinear control systems, enabling comprehensive linear analysis and control methods to be effective for nonlinear systems. We next present a hierarchical neural network (HNN) approach to deal with the crucial challenge of the finite-dimensional Koopman representation approximation. In particular, a scale-invariant constrained network in the HNN includes four modules, in which a predictor module and a linear module can accurately approximate the finite Koopman eigenfunctions and Koopman operator, respectively, thus forming the lifted linear system. Then, we design the Koopman-based MPC scheme for controlling nonlinear systems with constraints by adopting the modified MPC with a saturation-like function on the lifted linear system. Importantly, the Koopman-based MPC enjoys higher computational efficiency compared to the classical linear MPC and nonlinear MPC methods. Finally, a physical experiment on an overhead crane system is provided to demonstrate the effectiveness of the proposed data-driven control framework.},
  archive      = {J_TNNLS},
  author       = {Meixi Wang and Xuyang Lou and Wei Wu and Baotong Cui},
  doi          = {10.1109/TNNLS.2022.3194958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3630-3639},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Koopman-based MPC with learned dynamics: Hierarchical neural network approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised multiview feature selection with adaptive
graph learning. <em>TNNLS</em>, <em>35</em>(3), 3615–3629. (<a
href="https://doi.org/10.1109/TNNLS.2022.3194957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data sources become ever more numerous with increased feature dimensionality, feature selection for multiview data has become an important technique in machine learning. Semi-supervised multiview feature selection (SMFS) focuses on the problem of how to obtain a discriminative feature subset from heterogeneous feature spaces in the case of abundant unlabeled data with little labeled data. Most existing methods suffer from unreliable similarity graph structure across different views since they separate the graph construction from feature selection and use the fixed graphs that are susceptible to noisy features. Furthermore, they directly concatenate multiple feature projections for feature selection, neglecting the contribution diversity among projections. To alleviate these problems, we present an SMFS to simultaneously select informative features and learn a unified graph through the data fusion from aspects of feature projection and similarity graph. Specifically, SMFS adaptively weights different feature projections and flexibly fuses them to form a joint weighted projection, preserving the complementarity and consensus of the original views. Moreover, an implicit graph fusion is devised to dynamically learn a compatible graph across views according to the similarity structure in the learned projection subspace, where the undesirable effects of noisy features are largely alleviated. A convergent method is derived to iteratively optimize SMFS. Experiments on various datasets validate the effectiveness and superiority of SMFS over state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Bingbing Jiang and Xingyu Wu and Xiren Zhou and Yi Liu and Anthony G. Cohn and Weiguo Sheng and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2022.3194957},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3615-3629},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semi-supervised multiview feature selection with adaptive graph learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and sparse principal component analysis with adaptive
loss minimization for feature selection. <em>TNNLS</em>, <em>35</em>(3),
3601–3614. (<a
href="https://doi.org/10.1109/TNNLS.2022.3194896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is one of the most successful unsupervised subspace learning methods and has been used in many practical applications. To deal with the outliers in real-world data, robust principal analysis models based on various measure are proposed. However, conventional PCA models can only transform features to unknown subspace for dimensionality reduction and cannot perform features’ selection task. In this article, we propose a novel robust PCA (RPCA) model to mitigate the impact of outliers and conduct feature selection, simultaneously. First, we adopt $\sigma $ -norm as reconstruction error (RE), which plays an important role in robust reconstruction. Second, to conduct feature selection task, we apply $\ell _{2,0}$ -norm constraint to subspace projection. Furthermore, an efficient iterative optimization algorithm is proposed to solve the objective function with nonconvex and nonsmooth constraint. Extensive experiments conducted on several real-world datasets demonstrate the effectiveness and superiority of the proposed feature selection model.},
  archive      = {J_TNNLS},
  author       = {Jintang Bian and Dandan Zhao and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3194896},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3601-3614},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust and sparse principal component analysis with adaptive loss minimization for feature selection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature saliencies in asymmetric hidden markov models.
<em>TNNLS</em>, <em>35</em>(3), 3586–3600. (<a
href="https://doi.org/10.1109/TNNLS.2022.3194597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-life problems are stated as nonlabeled high-dimensional data. Current strategies to select features are mainly focused on labeled data, which reduces the options to select relevant features for unsupervised problems, such as clustering. Recently, feature saliency models have been introduced and developed as clustering models to select and detect relevant variables/features as the model is learned. Usually, these models assume that all variables are independent, which narrows their applicability. This article introduces asymmetric hidden Markov models with feature saliencies, i.e., models capable of simultaneously determining during their learning phase relevant variables/features and probabilistic relationships between variables. The proposed models are compared with other state-of-the-art approaches using synthetic data and real data related to grammatical face videos and wear in ball bearings. We show that the proposed models have better or equal fitness than other state-of-the-art models and provide further data insights.},
  archive      = {J_TNNLS},
  author       = {Carlos Esteban Puerto-Santana and Pedro Larrañaga and Concha Bielza},
  doi          = {10.1109/TNNLS.2022.3194597},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3586-3600},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature saliencies in asymmetric hidden markov models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Small is beautiful: Compressing deep neural networks for
partial domain adaptation. <em>TNNLS</em>, <em>35</em>(3), 3575–3585.
(<a href="https://doi.org/10.1109/TNNLS.2022.3194533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is a promising way to ease the costly data labeling process in the era of deep learning (DL). A practical situation is partial domain adaptation (PDA), where the label space of the target domain is a subset of that in the source domain. Although existing methods yield appealing performance in PDA tasks, it is highly presumable that computation overhead exists in deep PDA models since the target is only a subtask of the original problem. In this work, PDA and model compression are seamlessly integrated into a unified training process. The cross-domain distribution divergence is reduced by minimizing a soft-weighted maximum mean discrepancy (SWMMD), which is differentiable and functions as regularization during network training. We use gradient statistics to compress the overparameterized model to identify and prune redundant channels based on the corresponding scaling factors in batch normalization (BN) layers. The experimental results demonstrate that our method can achieve comparable classification performance to state-of-the-art methods on various PDA tasks, with a significant reduction in model size and computation overhead.},
  archive      = {J_TNNLS},
  author       = {Yuzhe Ma and Xufeng Yao and Ran Chen and Ruiyu Li and Xiaoyong Shen and Bei Yu},
  doi          = {10.1109/TNNLS.2022.3194533},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3575-3585},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Small is beautiful: Compressing deep neural networks for partial domain adaptation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A concurrent multiscale detector for end-to-end image
matching. <em>TNNLS</em>, <em>35</em>(3), 3560–3574. (<a
href="https://doi.org/10.1109/TNNLS.2022.3194079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on end-to-end image matching through joint key-point detection and descriptor extraction. To find repeatable and high discrimination key points, we improve the deep matching network from the perspectives of network structure and network optimization. First, we propose a concurrent multiscale detector (CS-det) network, which consists of several parallel convolutional networks to extract multiscale features and multilevel discriminative information for key-point detection. Moreover, we introduce an attention module to fuse the response maps of various features adaptively. Importantly, we propose two novel rank consistent losses (RC-losses) for network optimization, significantly improving image matching performances. On the one hand, we propose a score rank consistent loss (RC-S-loss) to ensure that the key points have high repeatability. Different from the score difference loss merely focusing on the absolute score of an individual key point, our proposed RC-S-loss pays more attention to the relative score of key points in the image. On the other hand, we propose a score-discrimination RC-loss to ensure that the key point has high discrimination, which can reduce the confusion from other key points in subsequent matching and then further enhance the accuracy of image matching. Extensive experimental results demonstrate that the proposed CS-det improves the mean matching result of deep detector by 1.4%–2.1%, and the proposed RC-losses can boost the matching performances by 2.7%–3.4% than score difference loss. Our source codes are available at https://github.com/iquandou/CS-Net .},
  archive      = {J_TNNLS},
  author       = {Dou Quan and Shuang Wang and Ning Huyan and Yi Li and Ruiqi Lei and Jocelyn Chanussot and Biao Hou and Licheng Jiao},
  doi          = {10.1109/TNNLS.2022.3194079},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3560-3574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A concurrent multiscale detector for end-to-end image matching},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Active crowdsourcing for multilabel annotation.
<em>TNNLS</em>, <em>35</em>(3), 3549–3559. (<a
href="https://doi.org/10.1109/TNNLS.2022.3194022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel annotation is a critical step to generate training sets when learning classification models in various application domains, but asking domain experts to provide labels is usually time-consuming and expensive, which cannot meet the current requirement of the fast evolution of the models in the big data era. Although crowdsourcing provides a fast solution to acquire labels for multilabel learning, it faces the risk of high data acquisition cost and low label quality. This article proposes a novel one-coin label-dependent active crowdsourcing (OCLDAC) method to iteratively query noisy labels from crowd workers and learn multilabel classification models. In each iteration of active learning, integrated labels of instances are first inferred by a novel one-coin label-dependent model, which utilizes a mixture of multiple independent Bernoulli distributions to explore and exploit correlations among the labels to increase the accuracy of truth inference. Then, instances, labels, and workers are selected according to the novel strategies that incorporate the distribution of noisy labels, the prediction probability of learning models, label correlations, and the reliability of crowd workers. Simulations on eight multilabel datasets and evaluation on one real-world crowdsourcing dataset consistently show that the proposed OCLDAC significantly outperforms the state-of-the-art methods and their variants.},
  archive      = {J_TNNLS},
  author       = {Jing Zhang and Ming Wu and Cangqi Zhou and Victor S. Sheng},
  doi          = {10.1109/TNNLS.2022.3194022},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3549-3559},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Active crowdsourcing for multilabel annotation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matrix-weighted consensus of second-order discrete-time
multiagent systems. <em>TNNLS</em>, <em>35</em>(3), 3539–3548. (<a
href="https://doi.org/10.1109/TNNLS.2022.3194010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the matrix-weighted consensus issues for second-order discrete-time multiagent systems on directed network topology. Under the designed matrix-weighted consensus algorithm, based on the eigenvalues of the Laplacian matrix, coupling gains, and discrete interval, we build some consensus conditions for reaching discrete-time consensus and deduce some simplified and straightforward consensus conditions for undirected network topology. Besides, for a given network topology, we theoretically analyze the influence of the coupling gains and discrete intervals on the consensus conditions of the network dynamics. Finally, we offer several simulation examples to validate the obtained results.},
  archive      = {J_TNNLS},
  author       = {Suoxia Miao and Housheng Su and Shiming Chen},
  doi          = {10.1109/TNNLS.2022.3194010},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3539-3548},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Matrix-weighted consensus of second-order discrete-time multiagent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative ETM-based adaptive neural network tracking
control for nonlinear pure-feedback MASs: A special-shaped laplacian
matrix method. <em>TNNLS</em>, <em>35</em>(3), 3528–3538. (<a
href="https://doi.org/10.1109/TNNLS.2022.3194007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article solves the cooperative adaptive tracking control problem for nonlinear pure-feedback multi-agent systems (MASs). Compared with the previous achievements of adaptive control of pure-feedback MASs, the partial derivative of the nonaffine function may not exist by using decoupling technology. In the controller design framework based on the backstepping technique, the additional state variables are processed using the special properties of the radial basis function neural networks (RBF NNs). A special-shaped Laplacian matrix is proposed to unify the leader gain form in the tracking error design process (the coefficient in the second term of tracking error). Furthermore, an event trigger mechanism (ETM) is introduced to save resources. The constructed controller under the ETM can not only stabilize the system states but also make the tracking error reach a small accuracy. Finally, the simulation results demonstrated the feasibility of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Qiangqiang Zhu and Ben Niu and Ding Wang and Shengtao Li and Xiaomei Wang and Jie Kong},
  doi          = {10.1109/TNNLS.2022.3194007},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3528-3538},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cooperative ETM-based adaptive neural network tracking control for nonlinear pure-feedback MASs: A special-shaped laplacian matrix method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous feature selection based on neighborhood
combination entropy. <em>TNNLS</em>, <em>35</em>(3), 3514–3527. (<a
href="https://doi.org/10.1109/TNNLS.2022.3193929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection aims to remove irrelevant or redundant features and thereby remain relevant or informative features so that it is often preferred for alleviating the dimensionality curse, enhancing learning performance, providing better readability and interpretability, and so on. Data that contain numerical and categorical representations are called heterogeneous data, and they exist widely in many real-world applications. Neighborhood rough set (NRS) can effectively deal with heterogeneous data by using neighborhood binary relation, which has been successfully applied to heterogeneous feature selection. In this article, the NRS model as a unified framework is used to design a feature selection method to handle categorical, numerical, and heterogeneous data. First, the concept of neighborhood combination entropy (NCE) is presented. It can reflect the probability of pairs of the neighborhood granules that are probably distinguishable from each other. Then, the conditional neighborhood combination entropy (cNCE) based on NCE is proposed under the condition of considering decision attributes. Moreover, some properties and relationships between cNCE and NCE are derived. Finally, the functions of inner and outer significances are constructed to design a feature selection algorithm based on cNCE (FScNCE). The experimental results show the effectiveness and superiority of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Pengfei Zhang and Tianrui Li and Zhong Yuan and Chuan Luo and Keyu Liu and Xiaoling Yang},
  doi          = {10.1109/TNNLS.2022.3193929},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3514-3527},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heterogeneous feature selection based on neighborhood combination entropy},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPD: Semi-supervised learning and progressive distillation
for 3-d detection. <em>TNNLS</em>, <em>35</em>(3), 3503–3513. (<a
href="https://doi.org/10.1109/TNNLS.2022.3193614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current learning-based 3-D object detection accuracy is heavily impacted by the annotation quality. It is still a challenge to expect an overall high detection accuracy for all classes under different scenarios given the dataset sparsity. To mitigate this challenge, this article proposes a novel method called semi-supervised learning and progressive distillation (SPD), which uses semi-supervised learning (SSL) and knowledge distillation to improve label efficiency. The SPD uses two big backbones to hand the unlabeled/labeled input data augmented by the periodic IO augmentation (PA). Then the backbones are compressed using progressive distillation (PD). Precisely, PA periodically shifts the data augmentation operations between the input and output of the big backbone, aiming to improve the network’s generalization of the unseen and unlabeled data. Using the big backbone can benefit from large-scale augmented data better than the small one. And two backbones are trained by the data scale and ratio-sensitive loss (data-loss). It solves the over-flat caused by the large-scale unlabeled data from PA and helps the big backbone prevent overfitting on the limited-scale labeled data. Hence, using the PA and data loss during SSL training dramatically improves the label efficiency. Next, the trained big backbone set as the teacher CNN is progressively distilled to obtain a small student model, referenced as PD. PD mitigates the problem that student CNN performance degrades when the gap between the student and the teacher is oversized. Extensive experiments are conducted on the indoor datasets SUN RGB-D and ScanNetV2 and outdoor dataset KITTI. Using only 50% labeled data and a 27% smaller model size, SPD performs 0.32 higher than the fully supervised VoteNet [1] which is adopted as our backbone. Besides, using only 2% labeled data, compared to the other fully supervised backbone PV-RCNN [2] , SPD accomplishes a similar accuracy (84.1 and 84.83) and 30% less inference time.},
  archive      = {J_TNNLS},
  author       = {Bangquan Xie and Zongming Yang and Liang Yang and Ruifa Luo and Jun Lu and Ailin Wei and Xiaoxiong Weng and Bing Li},
  doi          = {10.1109/TNNLS.2022.3193614},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3503-3513},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SPD: Semi-supervised learning and progressive distillation for 3-D detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic co-embedding model for temporal attributed networks.
<em>TNNLS</em>, <em>35</em>(3), 3488–3502. (<a
href="https://doi.org/10.1109/TNNLS.2022.3193564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the problem of embedding temporal attributed networks, with the goal of which is to learn dynamic low-dimensional representations over time for temporal attributed networks. Existing temporal network embedding methods only learn the representations for nodes, which are unable to capture the dynamic affinities between nodes and attributes. Moreover, existing co-embedding methods that learn the static embeddings of both nodes and attributes cannot be naturally utilized to obtain their dynamic embeddings for temporal attributed networks. To address these issues, we propose the dynamic co-embedding model for temporal attributed networks (DCTANs) based on the dynamic stochastic state–space framework. Our model captures the dynamics of a temporal attributed network by modeling the abstract belief states representing the condition of the nodes and attributes of current time step, and predicting the transitions between temporal abstract states of two successive time steps. Our model is able to learn embeddings for both nodes and attributes based on their belief states at each time step of the temporal attributed network, while the state transition tendency for predicting the future network can be tracked and the affinities between nodes and attributes can be preserved. Experimental results on real-world networks demonstrate that our model achieves substantial performance gains in several static and dynamic graph mining applications compared with the state-of-the-art static and dynamic models.},
  archive      = {J_TNNLS},
  author       = {Shaowei Tang and Zaiqiao Meng and Shangsong Liang},
  doi          = {10.1109/TNNLS.2022.3193564},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3488-3502},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic co-embedding model for temporal attributed networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and analysis of a novel distributed gradient neural
network for solving consensus problems in a predefined time.
<em>TNNLS</em>, <em>35</em>(3), 3478–3487. (<a
href="https://doi.org/10.1109/TNNLS.2022.3193429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel distributed gradient neural network (DGNN) with predefined-time convergence (PTC) is proposed to solve consensus problems widely existing in multiagent systems (MASs). Compared with previous gradient neural networks (GNNs) for optimization and computation, the proposed DGNN model works in a nonfully connected way, in which each neuron only needs the information of neighbor neurons to converge to the equilibrium point. The convergence and asymptotic stability of the DGNN model are proved according to the Lyapunov theory. In addition, based on a relatively loose condition, three novel nonlinear activation functions are designed to speedup the DGNN model to PTC, which is proved by rigorous theory. Computer numerical results further verify the effectiveness, especially the PTC, of the proposed nonlinearly activated DGNN model to solve various consensus problems of MASs. Finally, a practical case of the directional consensus is presented to show the feasibility of the DGNN model and a corresponding connectivity-testing example is given to verify the influence on the convergence speed.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Lei Jia and Jianhua Dai and Yingkun Cao and Yiwei Li and Quanxin Zhu and Jichun Li and Min Liu},
  doi          = {10.1109/TNNLS.2022.3193429},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3478-3487},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design and analysis of a novel distributed gradient neural network for solving consensus problems in a predefined time},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised multitarget domain adaptation with
dictionary-bridged knowledge exploitation. <em>TNNLS</em>,
<em>35</em>(3), 3464–3477. (<a
href="https://doi.org/10.1109/TNNLS.2022.3193289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) is an emerging learning paradigm that models on unlabeled datasets by leveraging model knowledge built on other labeled datasets, in which the statistical distributions of these datasets are usually not identical. Formally, UDA is to leverage knowledge from a labeled source domain to promote an unlabeled target domain. Although there have been a variety of methods proposed to address the UDA problem, most of them are dedicated to single-source-to-single-target domain, while the works on single-source-to-multitarget domain are relatively rare. Compared to the single-source domain with single-target domain scenario, the UDA from single-source domain to multitarget domain is more challenging since it needs to consider not only the relationships between the source and the target domains but also those among the target domains. To this end, this article proposes a kind of dictionary learning-based unsupervised multitarget domain adaptation method (DL-UMTDA). In DL-UMTDA, a common dictionary is constructed to correlate the single-source and multitarget domains, while individual dictionaries are designed to exploit the private knowledge for the target domains. Through learning the corresponding dictionary representation coefficients in the UDA process, the correlations from the source to the target domains as well as these potential relationships between the target domains can be effectively exploited. In addition, we design an alternating algorithm to solve the DL-UMTDA model with theoretical convergence guarantee. Finally, extensive experiments on benchmark (Office + Caltech) and real datasets (AgeDB, Morph, and CACD) validate the superiority of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Qing Tian and Chuang Ma and Meng Cao and Jun Wan and Zhen Lei and Songcan Chen},
  doi          = {10.1109/TNNLS.2022.3193289},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3464-3477},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised multitarget domain adaptation with dictionary-bridged knowledge exploitation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust stabilizing control of perturbed biological networks
via coordinate transformation and algebraic analysis. <em>TNNLS</em>,
<em>35</em>(3), 3450–3463. (<a
href="https://doi.org/10.1109/TNNLS.2022.3192563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates robust stabilizing control of biological systems modeled by Boolean networks (BNs). A population of BNs is considered where a majority of BNs have the same BN dynamics, but some BNs are inflicted by mutations damaging particular nodes, leading to perturbed dynamics that prohibit global stabilization to the desired attractor. The proposed control strategy consists of two steps. First, the nominal BN is transformed and curtailed into a sub-BN via a simple coordinate transformation and network reduction associated with the desired attractor. The feedback vertex set (FVS) control is then applied to the reduced BN to determine the control inputs for the nominal BN. Next, the control inputs derived in the first step and mutated nodes are applied to the nominal BN so as to identify residual dynamics of perturbed BNs, and additional control inputs are selected according to the canalization effect of each node. The overall control inputs are applied to the BN population, so that the nominal BN converges to the desired attractor and perturbed BNs to their own attractors that are the closest possible to the desired attractor. The performance of the proposed robust control scheme is validated through numerical experiments on random BNs and a complex biological network.},
  archive      = {J_TNNLS},
  author       = {Jung-Min Yang and Chun-Kyung Lee and Kwang-Hyun Cho},
  doi          = {10.1109/TNNLS.2022.3192563},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3450-3463},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust stabilizing control of perturbed biological networks via coordinate transformation and algebraic analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive framework for long-tailed learning via
pretraining and normalization. <em>TNNLS</em>, <em>35</em>(3),
3437–3449. (<a
href="https://doi.org/10.1109/TNNLS.2022.3192475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data in the visual world often present long-tailed distributions. However, learning high-quality representations and classifiers for imbalanced data is still challenging for data-driven deep learning models. In this work, we aim at improving the feature extractor and classifier for long-tailed recognition via contrastive pretraining and feature normalization, respectively. First, we carefully study the influence of contrastive pretraining under different conditions, showing that current self-supervised pretraining for long-tailed learning is still suboptimal in both performance and speed. We thus propose a new balanced contrastive loss and a fast contrastive initialization scheme to improve previous long-tailed pretraining. Second, based on the motivative analysis on the normalization for classifier, we propose a novel generalized normalization classifier that consists of generalized normalization and grouped learnable scaling. It outperforms traditional inner product classifier as well as cosine classifier. Both the two components proposed can improve recognition ability on tail classes without the expense of head classes. We finally build a unified framework that achieves competitive performance compared with state of the arts on several long-tailed recognition benchmarks and maintains high efficiency.},
  archive      = {J_TNNLS},
  author       = {Nan Kang and Hong Chang and Bingpeng Ma and Shiguang Shan},
  doi          = {10.1109/TNNLS.2022.3192475},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3437-3449},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A comprehensive framework for long-tailed learning via pretraining and normalization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Slow down to go better: A survey on slow feature analysis.
<em>TNNLS</em>, <em>35</em>(3), 3416–3436. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal data contain a wealth of valuable information, playing an essential role in various machine-learning tasks. Slow feature analysis (SFA), one of the most classic temporal feature extraction models, has been deeply explored in two decades of development. SFA extracts slowly varying features as high-level representations of temporal data. Its core idea of “slow” has been proven to be consistent with the nature of biological vision and beneficial in capturing significant temporal information for various tasks. So far, SFA has evolved into numerous improved versions and is widely applied in many fields such as computer vision, industrial control, remote sensing, signal processing, and computational biology. However, there currently lacks an insightful review of SFA. In this article, a comprehensive overview of SFA and its extensions is provided for the first time. The formulation and optimization of SFA are introduced. Two mainstream solutions, geometric interpretation, and a gradient-based training method of SFA are presented and discussed. Following that, a taxonomy of the current progress of SFA is proposed. We classify improved versions of SFA into six categories, including dual-input SFA (DISFA), online slow feature analysis (OSFA), probabilistic SFA (PSFA), multimode SFA, nonlinear SFA, and discrete labeled SFA. For each category, we illustrate its main ideas, mathematical principles, and applicable scenarios. In addition, the practical applications of SFA are summarized and presented. Finally, we bring new insights into SFA according to its research status and provide potential research directions, which may serve as a good reference for promoting future work.},
  archive      = {J_TNNLS},
  author       = {Pengyu Song and Chunhui Zhao},
  doi          = {10.1109/TNNLS.2022.3201621},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3416-3436},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Slow down to go better: A survey on slow feature analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recent advances in conventional and deep learning-based
depth completion: A survey. <em>TNNLS</em>, <em>35</em>(3), 3395–3415.
(<a href="https://doi.org/10.1109/TNNLS.2022.3201534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion aims to recover pixelwise depth from incomplete and noisy depth measurements with or without the guidance of a reference RGB image. This task attracted considerable research interest due to its importance in various computer vision-based applications, such as scene understanding, autonomous driving, 3-D reconstruction, object detection, pose estimation, trajectory prediction, and so on. As the system input, an incomplete depth map is usually generated by projecting the 3-D points collected by ranging sensors, such as LiDAR in outdoor environments, or obtained directly from RGB-D cameras in indoor areas. However, even if a high-end LiDAR is employed, the obtained depth maps are still very sparse and noisy, especially in the regions near the object boundaries, which makes the depth completion task a challenging problem. To address this issue, a few years ago, conventional image processing-based techniques were employed to fill the holes and remove the noise from the relatively dense depth maps obtained by RGB-D cameras, while deep learning-based methods have recently become increasingly popular and inspiring results have been achieved, especially for the challenging situation of LiDAR-image-based depth completion. This article systematically reviews and summarizes the works related to the topic of depth completion in terms of input modalities, data fusion strategies, loss functions, and experimental settings, especially for the key techniques proposed in deep learning-based multiple input methods. On this basis, we conclude by presenting the current status of depth completion and discussing several prospects for its future research directions.},
  archive      = {J_TNNLS},
  author       = {Zexiao Xie and Xiaoxuan Yu and Xiang Gao and Kunqian Li and Shuhan Shen},
  doi          = {10.1109/TNNLS.2022.3201534},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3395-3415},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Recent advances in conventional and deep learning-based depth completion: A survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based NOMA system for enhancement of 5G
networks: A review. <em>TNNLS</em>, <em>35</em>(3), 3380–3394. (<a
href="https://doi.org/10.1109/TNNLS.2022.3200825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fresh and rising demands for high-reliability and ultrahigh-capacity wireless communication have led to extensive research into 5G communications. The wide progress in deep learning (DL) and nonorthogonal multiple access (NOMA) technologies provides countless benefits to communication systems. This survey provides the broad scope of DL-based NOMA for the augmentation of 5G networks. It explores various works conducted on NOMA, DL, and their applications in 5G communication. This article further explains the prominence of DL in NOMA and reviews various DL-supported NOMA models exploited for distinct tasks including resource allotment, power allotment, subchannel/channel allotment, signal detection, user detection, and other purposes. It explains the advantages and shortcomings of applying DL to resolve NOMA challenges and also presents the cardinal use cases of NOMA. It then presents a tabulated comparison of diverse DL techniques adopted by available studies for performing heterogeneous operations in NOMA. Finally, this article investigates diverse, significant technical hindrances prevailing in DL-based NOMA systems and in the application of such systems toward 5G enhancement along with future directions for shedding light on paramount developments required in existing systems for invigorating more research and supporting the emerging applications in this field.},
  archive      = {J_TNNLS},
  author       = {Ranjan Kumar Senapati and Paresh J. Tanna},
  doi          = {10.1109/TNNLS.2022.3200825},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3380-3394},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning-based NOMA system for enhancement of 5G networks: A review},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning-based fractional-order adaptive
fault-tolerant formation control of networked fixed-wing UAVs with
prescribed performance. <em>TNNLS</em>, <em>35</em>(3), 3365–3379. (<a
href="https://doi.org/10.1109/TNNLS.2023.3281403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the fault-tolerant formation control (FTFC) problem for networked fixed-wing unmanned aerial vehicles (UAVs) against faults. To constrain the distributed tracking errors of follower UAVs with respect to neighboring UAVs in the presence of faults, finite-time prescribed performance functions (PPFs) are developed to transform the distributed tracking errors into a new set of errors by incorporating user-specified transient and steady-state requirements. Then, the critic neural networks (NNs) are developed to learn the long-term performance indices, which are used to evaluate the distributed tracking performance. Based on the generated critic NNs, actor NNs are designed to learn the unknown nonlinear terms. Moreover, to compensate for the reinforcement learning errors of actor-critic NNs, nonlinear disturbance observers (DOs) with skillfully constructed auxiliary learning errors are developed to facilitate the FTFC design. Furthermore, by using the Lyapunov stability analysis, it is shown that all follower UAVs can track the leader UAV with predesigned offsets, and the distributed tracking errors are finite-time convergent. Finally, comparative simulation results are presented to show the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Ziquan Yu and Jiaxu Li and Yiwei Xu and Youmin Zhang and Bin Jiang and Chun-Yi Su},
  doi          = {10.1109/TNNLS.2023.3281403},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3365-3379},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based fractional-order adaptive fault-tolerant formation control of networked fixed-wing UAVs with prescribed performance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication-efficient and resilient distributed
q-learning. <em>TNNLS</em>, <em>35</em>(3), 3351–3364. (<a
href="https://doi.org/10.1109/TNNLS.2023.3292036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of communication-efficient and resilient multiagent reinforcement learning (MARL). Specifically, we consider a setting where a set of agents are interconnected over a given network, and can only exchange information with their neighbors. Each agent observes a common Markov Decision Process and has a local cost which is a function of the current system state and the applied control action. The goal of MARL is for all agents to learn a policy that optimizes the infinite horizon discounted average of all their costs. Within this general setting, we consider two extensions to existing MARL algorithms. First, we provide an event-triggered learning rule where agents only exchange information with their neighbors if a certain triggering condition is satisfied. We show that this enables learning while reducing the amount of communication. Next, we consider the scenario where some of the agents can be adversarial (as captured by the Byzantine attack model), and arbitrarily deviate from the prescribed learning algorithm. We establish a fundamental trade-off between optimality and resilience when Byzantine agents are present. We then create a resilient algorithm and show almost sure convergence of all reliable agents’ value functions to the neighborhood of the optimal value function of all reliable agents, under certain conditions on the network topology. When the optimal $Q$ -values are sufficiently separated for different actions, we show that all reliable agents can learn the optimal policy under our algorithm.},
  archive      = {J_TNNLS},
  author       = {Yijing Xie and Shaoshuai Mou and Shreyas Sundaram},
  doi          = {10.1109/TNNLS.2023.3292036},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3351-3364},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Communication-efficient and resilient distributed Q-learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multi-agent graph-attention deep reinforcement learning for
post-contingency grid emergency voltage control. <em>TNNLS</em>,
<em>35</em>(3), 3340–3350. (<a
href="https://doi.org/10.1109/TNNLS.2023.3341334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid emergency voltage control (GEVC) is paramount in electric power systems to improve voltage stability and prevent cascading outages and blackouts in case of contingencies. While most deep reinforcement learning (DRL)-based paradigms perform single agents in a static environment, real-world agents for GEVC are expected to cooperate in a dynamically shifting grid. Moreover, due to high uncertainties from combinatory natures of various contingencies and load consumption, along with the complexity of dynamic grid operation, the data efficiency and control performance of the existing DRL-based methods are challenged. To address these limitations, we propose a multi-agent graph-attention (GATT)-based DRL algorithm for GEVC in multi-area power systems. We develop graph convolutional network (GCN)-based agents for feature representation of the graph-structured voltages to improve the decision accuracy in a data-efficient manner. Furthermore, a cutting-edge attention mechanism concentrates on effective information sharing among multiple agents, synergizing different-sized subnetworks in the grid for cooperative learning. We address several key challenges in the existing DRL-based GEVC approaches, including low scalability and poor stability against high uncertainties. Test results in the IEEE benchmark system verify the advantages of the proposed method over several recent multi-agent DRL-based algorithms.},
  archive      = {J_TNNLS},
  author       = {Ying Zhang and Meng Yue and Jianhui Wang and Shinjae Yoo},
  doi          = {10.1109/TNNLS.2023.3341334},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3340-3350},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-agent graph-attention deep reinforcement learning for post-contingency grid emergency voltage control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel dynamic operation optimization method based on
multiobjective deep reinforcement learning for steelmaking process.
<em>TNNLS</em>, <em>35</em>(3), 3325–3339. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a dynamic operation optimization problem for a steelmaking process. The problem is defined to determine optimal operation parameters that bring smelting process indices close to their desired values. The operation optimization technologies have been applied successfully for endpoint steelmaking, but it is still challenging for the dynamic smelting process because of the high temperature and complex physical and chemical reactions. A framework of deep deterministic policy gradient is applied to solve the dynamic operation optimization problem in the steelmaking process. Then, an energy-informed restricted Boltzmann machine method with physical interpretability is developed to construct the actor and critic networks in reinforcement learning (RL) for dynamic decision-making operations. It can provide a posterior probability for each action to guide training in each state. Furthermore, in terms of the design of neural network (NN) architecture, a multiobjective evolutionary algorithm is used to optimize the model hyperparameters, and a knee solution strategy is designed to balance the model accuracy and complexity of neural networks. Experiments are conducted on real data from a steelmaking production process to verify the practicability of the developed model. The experimental results show the advantages and effectiveness of the proposed method compared with other methods. It can meet the requirements of the specified quality of molten steel.},
  archive      = {J_TNNLS},
  author       = {Chang Liu and Lixin Tang and Chenche Zhao},
  doi          = {10.1109/TNNLS.2023.3244945},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3325-3339},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel dynamic operation optimization method based on multiobjective deep reinforcement learning for steelmaking process},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning-based model predictive control for
discrete-time systems. <em>TNNLS</em>, <em>35</em>(3), 3312–3324. (<a
href="https://doi.org/10.1109/TNNLS.2023.3273590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel reinforcement learning-based model predictive control (RLMPC) scheme for discrete-time systems. The scheme integrates model predictive control (MPC) and reinforcement learning (RL) through policy iteration (PI), where MPC is a policy generator and the RL technique is employed to evaluate the policy. Then the obtained value function is taken as the terminal cost of MPC, thus improving the generated policy. The advantage of doing so is that it rules out the need for the offline design paradigm of the terminal cost, the auxiliary controller, and the terminal constraint in traditional MPC. Moreover, RLMPC proposed in this article enables a more flexible choice of prediction horizon due to the elimination of the terminal constraint, which has great potential in reducing the computational burden. We provide a rigorous analysis of the convergence, feasibility, and stability properties of RLMPC. Simulation results show that RLMPC achieves nearly the same performance as traditional MPC in the control of linear systems and exhibits superiority over traditional MPC for nonlinear ones.},
  archive      = {J_TNNLS},
  author       = {Min Lin and Zhongqi Sun and Yuanqing Xia and Jinhui Zhang},
  doi          = {10.1109/TNNLS.2023.3273590},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3312-3324},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based model predictive control for discrete-time systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe reinforcement learning via a model-free safety
certifier. <em>TNNLS</em>, <em>35</em>(3), 3302–3311. (<a
href="https://doi.org/10.1109/TNNLS.2023.3264815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a data-driven safe reinforcement learning (RL) algorithm for discrete-time nonlinear systems. A data-driven safety certifier is designed to intervene with the actions of the RL agent to ensure both safety and stability of its actions. This is in sharp contrast to existing model-based safety certifiers that can result in convergence to an undesired equilibrium point or conservative interventions that jeopardize the performance of the RL agent. To this end, the proposed method directly learns a robust safety certifier while completely bypassing the identification of the system model. The nonlinear system is modeled using linear parameter varying (LPV) systems with polytopic disturbances. To prevent the requirement for learning an explicit model of the LPV system, data-based $\lambda $ -contractivity conditions are first provided for the closed-loop system to enforce robust invariance of a prespecified polyhedral safe set and the system’s asymptotic stability. These conditions are then leveraged to directly learn a robust data-based gain-scheduling controller by solving a convex program. A significant advantage of the proposed direct safe learning over model-based certifiers is that it completely resolves conflicts between safety and stability requirements while assuring convergence to the desired equilibrium point. Data-based safety certification conditions are then provided using Minkowski functions. They are then used to seemingly integrate the learned backup safe gain-scheduling controller with the RL controller. Finally, we provide a simulation example to verify the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Amir Modares and Nasser Sadati and Babak Esmaeili and Farnaz Adib Yaghmaie and Hamidreza Modares},
  doi          = {10.1109/TNNLS.2023.3264815},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3302-3311},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Safe reinforcement learning via a model-free safety certifier},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modified λ-policy iteration based adaptive dynamic
programming for unknown discrete-time linear systems. <em>TNNLS</em>,
<em>35</em>(3), 3291–3301. (<a
href="https://doi.org/10.1109/TNNLS.2023.3244934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the $\lambda $ -policy iteration ( $\lambda $ -PI) method for the optimal control problem of discrete-time linear systems is reconsidered and restated from a novel aspect. First, the traditional $\lambda $ -PI method is recalled, and some new properties of the traditional $\lambda $ -PI are proposed. Based on these new properties, a modified $\lambda $ -PI algorithm is introduced with its convergence proven. Compared with the existing results, the initial condition is further relaxed. The data-driven implementation is then constructed with a new matrix rank condition for verifying the feasibility of the proposed data-driven implementation. A simulation example verifies the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Huaiyuan Jiang and Bin Zhou and Guang-Ren Duan},
  doi          = {10.1109/TNNLS.2023.3244934},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3291-3301},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modified λ-policy iteration based adaptive dynamic programming for unknown discrete-time linear systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hamiltonian-driven adaptive dynamic programming with
efficient experience replay. <em>TNNLS</em>, <em>35</em>(3), 3278–3290.
(<a href="https://doi.org/10.1109/TNNLS.2022.3213566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel efficient experience-replay-based adaptive dynamic programming (ADP) for the optimal control problem of a class of nonlinear dynamical systems within the Hamiltonian-driven framework. The quasi-Hamiltonian is presented for the policy evaluation problem with an admissible policy. With the quasi-Hamiltonian, a novel composite critic learning mechanism is developed to combine the instantaneous data with the historical data. In addition, the pseudo-Hamiltonian is defined to deal with the performance optimization problem. Based on the pseudo-Hamiltonian, the conventional Hamilton–Jacobi–Bellman (HJB) equation can be represented in a filtered form, which can be implemented online. Theoretical analysis is investigated in terms of the convergence of the adaptive critic design and the stability of the closed-loop systems, where parameter convergence can be achieved under a weakened excitation condition. Simulation studies are investigated to verify the efficacy of the presented design scheme.},
  archive      = {J_TNNLS},
  author       = {Yongliang Yang and Yongping Pan and Cheng-Zhong Xu and Donald C. Wunsch},
  doi          = {10.1109/TNNLS.2022.3213566},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3278-3290},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hamiltonian-driven adaptive dynamic programming with efficient experience replay},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed minmax strategy for multiplayer games:
Stability, robustness, and algorithms. <em>TNNLS</em>, <em>35</em>(3),
3265–3277. (<a
href="https://doi.org/10.1109/TNNLS.2022.3215629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a distributed minmax strategy for multiplayer games and develops reinforcement learning (RL) algorithms to solve it. The proposed minmax strategy is distributed, in the sense that it finds each player’s optimal control policy without knowing all the other players’ policies. Each player obtains its distributed control policy by solving a distributed algebraic Riccati equation in a multiplayer noncooperative game. This policy is found against the worst policies of all the other players. We guarantee the existence of distributed minmax solutions and study their $\mathcal {L}_{2}$ and asymptotic stabilities. Under mild conditions, the resulting minmax control policies are shown to improve robust gain and phase margins of multiplayer systems compared to the standard linear–quadratic regulator controller. Distributed minmax solutions are found using both model-based policy iteration and data-driven off-policy RL algorithms. Simulation examples verify the proposed formulation and its computational efficiency over the nondistributed Nash solutions.},
  archive      = {J_TNNLS},
  author       = {Bosen Lian and Vrushabh S. Donge and Wenqian Xue and Frank L. Lewis and Ali Davoudi},
  doi          = {10.1109/TNNLS.2022.3215629},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3265-3277},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed minmax strategy for multiplayer games: Stability, robustness, and algorithms},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constrained-cost adaptive dynamic programming for optimal
control of discrete-time nonlinear systems. <em>TNNLS</em>,
<em>35</em>(3), 3251–3264. (<a
href="https://doi.org/10.1109/TNNLS.2023.3237586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For discrete-time nonlinear systems, this research is concerned with optimal control problems (OCPs) with constrained cost, and a novel value iteration with constrained cost (VICC) method is developed to solve the optimal control law with the constrained cost functions. The VICC method is initialized through a value function constructed by a feasible control law. It is proven that the iterative value function is nonincreasing and converges to the solution of the Bellman equation with constrained cost. The feasibility of the iterative control law is proven. The method to find the initial feasible control law is given. Implementation using neural networks (NNs) is introduced, and the convergence is proven by considering the approximation error. Finally, the property of the present VICC method is shown by two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Qinglai Wei and Tao Li},
  doi          = {10.1109/TNNLS.2023.3237586},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3251-3264},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Constrained-cost adaptive dynamic programming for optimal control of discrete-time nonlinear systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Semiglobal suboptimal output regulation for heterogeneous
multi-agent systems with input saturation via adaptive dynamic
programming. <em>TNNLS</em>, <em>35</em>(3), 3242–3250. (<a
href="https://doi.org/10.1109/TNNLS.2022.3191673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the semiglobal cooperative suboptimal output regulation problem of heterogeneous multi-agent systems with unknown agent dynamics in the presence of input saturation. To solve the problem, we develop distributed suboptimal control strategies from two perspectives, namely, model-based and data-driven. For the model-based case, we design a suboptimal control strategy by using the low-gain technique and output regulation theory. Moreover, when the agents’ dynamics are unknown, we design a data-driven algorithm to solve the problem. We show that proposed control strategies ensure each agent’s output gradually follows the reference signal and achieves interference suppression while guaranteeing closed-loop stability. The theoretical results are illustrated by a numerical simulation example.},
  archive      = {J_TNNLS},
  author       = {Bingjie Wang and Lei Xu and Xinlei Yi and Yao Jia and Tao Yang},
  doi          = {10.1109/TNNLS.2022.3191673},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3242-3250},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semiglobal suboptimal output regulation for heterogeneous multi-agent systems with input saturation via adaptive dynamic programming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). ADP-based event-triggered constrained optimal control on
spatiotemporal process: Application to temperature field in roller kiln.
<em>TNNLS</em>, <em>35</em>(3), 3229–3241. (<a
href="https://doi.org/10.1109/TNNLS.2023.3267516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precise control of the spatiotemporal process in a roller kiln is crucial in the production of Ni–Co-Mn layered cathode material of lithium-ion batteries. Since the product is extremely sensitive to temperature distribution, temperature field control is of great significance. In this article, an event-triggered optimal control (ETOC) method with input constraints for the temperature field is proposed, which takes up an important position in reducing the communication and computation costs. A nonquadratic cost function is adopted to describe the system performance with input constraints. First, we present the problem description of the temperature field event-triggered control, where this field is described by a partial differential equation (PDE). Then, the event-triggered condition is designed according to the information of system states and control inputs. On this basis, a framework of the event-triggered adaptive dynamic programming (ETADP) method that is based on the model reduction technology is proposed for the PDE system. A critic network is used to approach the optimal performance index by a neural network (NN) together with that an actor network is used to optimize the control strategy. Furthermore, an upper bound of the performance index and a lower bound of interexecution times, as well as the stabilities of the impulsive dynamic system and the closed-loop PDE system, are also proved. Simulation verification demonstrates the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Binyan Li and Ning Chen and Biao Luo and Jiayao Chen and Chunhua Yang and Weihua Gui},
  doi          = {10.1109/TNNLS.2023.3267516},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3229-3241},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ADP-based event-triggered constrained optimal control on spatiotemporal process: Application to temperature field in roller kiln},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRL-searcher: A unified approach to multirobot efficient
search for a moving target. <em>TNNLS</em>, <em>35</em>(3), 3215–3228.
(<a href="https://doi.org/10.1109/TNNLS.2023.3274667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the multirobot efficient search (MuRES) for a nonadversarial moving target problem, whose objective is usually defined as either minimizing the target’s expected capture time or maximizing the target’s capture probability within a given time budget. Different from canonical MuRES algorithms, which target only one specific objective, our proposed algorithm, named distributional reinforcement learning-based searcher (DRL-Searcher), serves as a unified solution to both MuRES objectives. DRL-Searcher employs distributional reinforcement learning (DRL) to evaluate the full distribution of a given search policy’s return, that is, the target’s capture time, and thereafter makes improvements with respect to the particularly specified objective. We further adapt DRL-Searcher to the use case without the target’s real-time location information, where only the probabilistic target belief (PTB) information is provided. Lastly, the recency reward is designed for implicit coordination among multiple robots. Comparative simulation results in a range of MuRES test environments show the superior performance of DRL-Searcher to state of the arts. Additionally, we deploy DRL-Searcher to a real multirobot system for moving target search in a self-constructed indoor environment with satisfying results.},
  archive      = {J_TNNLS},
  author       = {Hongliang Guo and Qihang Peng and Zhiguang Cao and Yaochu Jin},
  doi          = {10.1109/TNNLS.2023.3274667},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3215-3228},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DRL-searcher: A unified approach to multirobot efficient search for a moving target},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Supplementary control for quantized discrete-time nonlinear
systems under goal representation heuristic dynamic programming.
<em>TNNLS</em>, <em>35</em>(3), 3202–3214. (<a
href="https://doi.org/10.1109/TNNLS.2022.3201521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with supplementary control of discrete-time nonlinear systems with multiple controllers in the framework of goal representation heuristic dynamic programming (GrHDP), where a logarithmic quantizer is used to govern the network communication. For the addressed problem, a neural network (NN)-based observer is first proposed to estimate the unknown system state in the simultaneous presence of quantized influence. In light of the estimated states and the ideal control inputs via a zero-sum game, a GrHDP algorithm with a reinforced term is developed to implement the supplementary control task, where some novel weight updating rules are constructed by virtue of an additional tunable parameter to improve the system performance. Furthermore, a set of conditions about the stability of estimated error dynamics of both observer states and updated NNs’ weights are derived by resorting to the Lyapunov stability theory. Finally, the effectiveness of the developed method is verified by a power system and a numerical experiment.},
  archive      = {J_TNNLS},
  author       = {Xueli Wang and Derui Ding and Xiaohua Ge and Qing-Long Han},
  doi          = {10.1109/TNNLS.2022.3201521},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3202-3214},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supplementary control for quantized discrete-time nonlinear systems under goal representation heuristic dynamic programming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Model-free q-learning for the tracking problem of linear
discrete-time systems. <em>TNNLS</em>, <em>35</em>(3), 3191–3201. (<a
href="https://doi.org/10.1109/TNNLS.2022.3195357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a model-free Q-learning algorithm is proposed to solve the tracking problem of linear discrete-time systems with completely unknown system dynamics. To eliminate tracking errors, a performance index of the Q-learning approach is formulated, which can transform the tracking problem into a regulation one. Compared with the existing adaptive dynamic programming (ADP) methods and Q-learning approaches, the proposed performance index adds a product term composed of a gain matrix and the reference tracking trajectory to the control input quadratic form. In addition, without requiring any prior knowledge of the dynamics of the original controlled system and command generator, the control policy obtained by the proposed approach can be deduced by an iterative technique relying on the online information of the system state, the control input, and the reference tracking trajectory. In each iteration of the proposed method, the desired control input can be updated by the iterative criteria derived from a precondition of the controlled system and the reference tracking trajectory, which ensures that the obtained control policy can eliminate tracking errors in theory. Moreover, to effectively use less data to obtain the optimal control policy, the off-policy approach is introduced into the proposed algorithm. Finally, the effectiveness of the proposed algorithm is verified by a numerical simulation.},
  archive      = {J_TNNLS},
  author       = {Chun Li and Jinliang Ding and Frank L. Lewis and Tianyou Chai},
  doi          = {10.1109/TNNLS.2022.3195357},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3191-3201},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-free Q-learning for the tracking problem of linear discrete-time systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-efficient off-policy learning for distributed optimal
tracking control of HMAS with unidentified exosystem dynamics.
<em>TNNLS</em>, <em>35</em>(3), 3181–3190. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a data-efficient off-policy reinforcement learning (RL) approach is proposed for distributed output tracking control of heterogeneous multiagent systems (HMASs) using approximate dynamic programming (ADP). Different from existing results that the kinematic model of the exosystem is addressable to partial or all agents, the dynamics of the exosystem are assumed to be completely unknown for all agents in this article. To solve this difficulty, an identifiable algorithm using the experience-replay method is designed for each agent to identify the system matrices of the novel reference model instead of the original exosystem. Then, an output-based distributed adaptive output observer is proposed to provide the estimations of the leader, and the proposed observer not only has a low dimension and less data transmission among agents but also is implemented in a fully distributed way. Besides, a data-efficient RL algorithm is given to design the optimal controller offline along with the system trajectories without solving output regulator equations. An ADP approach is developed to iteratively solve game algebraic Riccati equations (GAREs) using online information of state and input in an online way, which relaxes the requirement of knowing prior knowledge of agents’ system matrices in an offline way. Finally, a numerical example is provided to verify the effectiveness of theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Yong Xu and Zheng-Guang Wu},
  doi          = {10.1109/TNNLS.2022.3172130},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3181-3190},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-efficient off-policy learning for distributed optimal tracking control of HMAS with unidentified exosystem dynamics},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-efficient deep reinforcement learning for attitude
control of fixed-wing UAVs: Field experiments. <em>TNNLS</em>,
<em>35</em>(3), 3168–3180. (<a
href="https://doi.org/10.1109/TNNLS.2023.3263430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attitude control of fixed-wing unmanned aerial vehicles (UAVs) is a difficult control problem in part due to uncertain nonlinear dynamics, actuator constraints, and coupled longitudinal and lateral motions. Current state-of-the-art autopilots are based on linear control and are thus limited in their effectiveness and performance. Gls drl is a machine learning method to automatically discover optimal control laws through interaction with the controlled system that can handle complex nonlinear dynamics. We show in this article that deep reinforcement learning (DRL) can successfully learn to perform attitude control of a fixed-wing UAV operating directly on the original nonlinear dynamics, requiring as little as 3 min of flight data. We initially train our model in a simulation environment and then deploy the learned controller on the UAV in flight tests, demonstrating comparable performance to the state-of-the-art ArduPlane proportional-integral-derivative (PID) attitude controller with no further online learning required. Learning with significant actuation delay and diversified simulated dynamics were found to be crucial for successful transfer to control of the real UAV. In addition to a qualitative comparison with the ArduPlane autopilot, we present a quantitative assessment based on linear analysis to better understand the learning controller’s behavior.},
  archive      = {J_TNNLS},
  author       = {Eivind Bøhn and Erlend M. Coates and Dirk Reinhardt and Tor Arne Johansen},
  doi          = {10.1109/TNNLS.2023.3263430},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3168-3180},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-efficient deep reinforcement learning for attitude control of fixed-wing UAVs: Field experiments},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning control with knowledge shaping.
<em>TNNLS</em>, <em>35</em>(3), 3156–3167. (<a
href="https://doi.org/10.1109/TNNLS.2023.3243631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim at creating a transfer reinforcement learning framework that allows the design of learning controllers to leverage prior knowledge extracted from previously learned tasks and previous data to improve the learning performance of new tasks. Toward this goal, we formalize knowledge transfer by expressing knowledge in the value function in our problem construct, which is referred to as reinforcement learning with knowledge shaping (RL-KS). Unlike most transfer learning studies that are empirical in nature, our results include not only simulation verifications but also an analysis of algorithm convergence and solution optimality. Also different from the well-established potential-based reward shaping methods which are built on proofs of policy invariance, our RL-KS approach allows us to advance toward a new theoretical result on positive knowledge transfer. Furthermore, our contributions include two principled ways that cover a range of realization schemes to represent prior knowledge in RL-KS. We provide extensive and systematic evaluations of the proposed RL-KS method. The evaluation environments not only include classical RL benchmark problems but also include a challenging task of real-time control of a robotic lower limb with a human user in the loop.},
  archive      = {J_TNNLS},
  author       = {Xiang Gao and Jennie Si and He Huang},
  doi          = {10.1109/TNNLS.2023.3243631},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3156-3167},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning control with knowledge shaping},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-varying optimal formation control for second-order
multiagent systems based on neural network observer and reinforcement
learning. <em>TNNLS</em>, <em>35</em>(3), 3144–3155. (<a
href="https://doi.org/10.1109/TNNLS.2022.3158085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses a distributed time-varying optimal formation protocol for a class of second-order uncertain nonlinear dynamic multiagent systems (MASs) based on an adaptive neural network (NN) state observer through the backstepping method and simplified reinforcement learning (RL). Each follower agent is subjected to only local information and measurable partial states due to actual sensor limitations. In view of the distributed optimized formation strategic needs, the uncertain nonlinear dynamics and undetectable states may jointly affect the stability of the time-varying cooperative formation control. Furthermore, focusing on Hamilton–Jacobi–Bellman optimization, it is almost incapable of directly dealing with unknown equations. Above uncertainty and immeasurability processed by adaptive state observer and NN simplified RL are further designed to achieve desired second-order formation configuration at the least cost. The optimization protocol can not only solve the undetectable states and realize the prescribed time-varying formation performance on the premise that all the errors are SGUUB, but also prove the stability and update the critics and actors easily. Through the above-mentioned approaches offer an optimal control scheme to address time-varying formation control. Finally, the validity of the theoretical method is proven by the Lyapunov stability theory and digital simulation.},
  archive      = {J_TNNLS},
  author       = {Jie Lan and Yan-Jun Liu and Dengxiu Yu and Guoxing Wen and Shaocheng Tong and Lei Liu},
  doi          = {10.1109/TNNLS.2022.3158085},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3144-3155},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Time-varying optimal formation control for second-order multiagent systems based on neural network observer and reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety-aware pursuit-evasion games in unknown environments
using gaussian processes and finite-time convergent reinforcement
learning. <em>TNNLS</em>, <em>35</em>(3), 3130–3143. (<a
href="https://doi.org/10.1109/TNNLS.2022.3203977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a safe pursuit-evasion game for enabling finite-time capture, optimal performance as well as adaptation to an unknown cluttered environment. The pursuit-evasion game is formulated as a zero-sum differential game wherein the pursuer seeks to minimize its relative distance to the target while the evader attempts to maximize it. A critic-only reinforcement learning (RL)-based algorithm is then proposed for learning online and in finite time the pursuit-evasion policies and thus enabling finite-time capture of the evader. Safety is ensured by means of barrier functions associated with the obstacles, which are integrated into the running cost. Using Gaussian processes (GPs), a learning-based mechanism is devised for safely learning the unknown environment. Simulation results illustrate the efficacy of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Nikolaos-Marios T. Kokolakis and Kyriakos G. Vamvoudakis},
  doi          = {10.1109/TNNLS.2022.3203977},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3130-3143},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Safety-aware pursuit-evasion games in unknown environments using gaussian processes and finite-time convergent reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved soft actor-critic: Mixing prioritized off-policy
samples with on-policy experiences. <em>TNNLS</em>, <em>35</em>(3),
3121–3129. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft actor-critic (SAC) is an off-policy actor-critic (AC) reinforcement learning (RL) algorithm, essentially based on entropy regularization. SAC trains a policy by maximizing the trade-off between expected return and entropy (randomness in the policy). It has achieved the state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. SAC works in an off-policy fashion where data are sampled uniformly from past experiences (stored in a buffer) using which the parameters of the policy and value function networks are updated. We propose certain crucial modifications for boosting the performance of SAC and making it more sample efficient. In our proposed improved SAC (ISAC), we first introduce a new prioritization scheme for selecting better samples from the experience replay (ER) buffer. Second we use a mixture of the prioritized off-policy data with the latest on-policy data for training the policy and value function networks. We compare our approach with the vanilla SAC and some recent variants of SAC and show that our approach outperforms the said algorithmic benchmarks. It is comparatively more stable and sample efficient when tested on a number of continuous control tasks in MuJoCo environments.},
  archive      = {J_TNNLS},
  author       = {Chayan Banerjee and Zhiyong Chen and Nasimul Noman},
  doi          = {10.1109/TNNLS.2022.3174051},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3121-3129},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improved soft actor-critic: Mixing prioritized off-policy samples with on-policy experiences},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Adaptive optimal control of networked nonlinear systems
with stochastic sensor and actuator dropouts based on reinforcement
learning. <em>TNNLS</em>, <em>35</em>(3), 3107–3120. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive optimal control problem for networked discrete-time nonlinear systems with stochastic packet dropouts in both controller-to-actuator and sensor-to-controller channels. A Bernoulli model-based Hamilton–Jacobi–Bellman (BHJB) equation is first developed to deal with the corresponding nonadaptive optimal control problem with known system dynamics and probability models of packet dropouts. The solvability of the nonadaptive optimal control problem is analyzed, and the stability and optimality of the resulting closed-loop system are proven. Two reinforcement learning (RL)-based policy iteration (PI) and value iteration (VI) algorithms are further developed to obtain the solution to the BHJB equation, and their convergence analysis is also provided. Furthermore, in the absence of a priori knowledge of partial system dynamics and probabilities of packet dropouts, two more online RL-based PI and VI algorithms are developed by using critic–actor approximators and packet dropout probability estimator. It is shown that the concerned adaptive optimal control problem can be solved by the proposed online RL-based PI and VI algorithms. Finally, simulation studies of a single-link manipulator are provided to illustrate the effectiveness of the proposed approaches.},
  archive      = {J_TNNLS},
  author       = {Yi Jiang and Lu Liu and Gang Feng},
  doi          = {10.1109/TNNLS.2022.3183020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3107-3120},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive optimal control of networked nonlinear systems with stochastic sensor and actuator dropouts based on reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial special issue on reinforcement
learning-based control: Data-efficient and resilient methods.
<em>TNNLS</em>, <em>35</em>(3), 3103–3106. (<a
href="https://doi.org/10.1109/TNNLS.2024.3362092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important branch of machine learning, reinforcement learning (RL) has proved its efficiency in many emerging applications in science and engineering. A remarkable advantage of RL is that it enables agents to maximize their cumulative rewards through online exploration and interactions with unknown (or partially unknown) and uncertain environments, which is regarded as a variant of data-driven adaptive optimal control methods. However, the successful implementation of RL-based control systems usually relies on a good quantity of online data due to its data-driven nature. Therefore, it is imperative to develop data-efficient RL methods for control systems to reduce the required number of interactions with the external environment. Moreover, network-aware issues, such as cyberattacks, dropout packet and communication latency, and actuator and sensor faults, are challenging conundrums that threaten the safety, security, stability, and reliability of network control systems. Consequently, it is significant to develop safe and resilient RL mechanisms.},
  archive      = {J_TNNLS},
  author       = {Weinan Gao and Na Li and Kyriakos G. Vamvoudakis and Fei Richard Yu and Zhong-Ping Jiang},
  doi          = {10.1109/TNNLS.2024.3362092},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3103-3106},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial special issue on reinforcement learning-based control: Data-efficient and resilient methods},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible job shop scheduling via dual attention
network-based reinforcement learning. <em>TNNLS</em>, <em>35</em>(3),
3091–3102. (<a
href="https://doi.org/10.1109/TNNLS.2023.3306421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this article presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to construct production-adaptive operation and machine features to support high-quality decision-making. Experimental results using synthetic data as well as public benchmarks corroborate that the proposed approach outperforms both traditional PDRs and the state-of-the-art DRL method. Moreover, it achieves results comparable to exact methods in certain cases and demonstrates favorable generalization ability to large-scale and real-world unseen FJSP tasks.},
  archive      = {J_TNNLS},
  author       = {Runqing Wang and Gang Wang and Jian Sun and Fang Deng and Jie Chen},
  doi          = {10.1109/TNNLS.2023.3306421},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3091-3102},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Flexible job shop scheduling via dual attention network-based reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for blast furnace ironmaking
operation with safety and partial observation considerations.
<em>TNNLS</em>, <em>35</em>(3), 3077–3090. (<a
href="https://doi.org/10.1109/TNNLS.2023.3340741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making proper decision online in complex environment during the blast furnace (BF) operation is a key factor in achieving long-term success and profitability in the steel manufacturing industry. Regulatory lags, ore source uncertainty, and continuous decision requirement make it a challenging task. Recently, reinforcement learning (RL) has demonstrated state-of-the-art performance in various sequential decision-making problems. However, the strict safety requirements make it impossible to explore optimal decisions through online trial and error. Therefore, this article proposes a novel offline RL approach designed to ensure safety, maximize return, and address issues of partially observed states. Specifically, it utilizes an off-policy actor-critic framework to infer the optimal decision from expert operation trajectories. The “actor” in this framework is jointly trained by the supervision and evaluation signals to make decision with low risk and high return. Furthermore, we investigate a recurrent version of the actor and critic networks to better capture the complete observations, which solves the partially observed Markov decision process (POMDP) arising from sensor limitations. Verification within the BF smelting process demonstrates the improvements of the proposed algorithm in performance, i.e., safety and return.},
  archive      = {J_TNNLS},
  author       = {Ke Jiang and Zhaohui Jiang and Xudong Jiang and Yongfang Xie and Weihua Gui},
  doi          = {10.1109/TNNLS.2023.3340741},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3077-3090},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning for blast furnace ironmaking operation with safety and partial observation considerations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatial–temporal variational graph attention autoencoder
using interactive information for fault detection in complex industrial
processes. <em>TNNLS</em>, <em>35</em>(3), 3062–3076. (<a
href="https://doi.org/10.1109/TNNLS.2023.3328399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern industry processes are typically composed of multiple operating units with reaction interaction and energy–mass coupling, which result in a mixed time-varying and spatial–temporal coupling of process variables. It is challenging to develop a comprehensive and precise fault detection model for the multiple interconnected units by simple superposition of the individual unit models. In this study, the fault detection problem is formulated as a spatial–temporal fault detection problem utilizing process data of multiple interconnected unit processes. A spatial–temporal variational graph attention autoencoder (STVGATE) using interactive information is proposed for fault detection, which aims to effectively capture the spatial and temporal features of the interconnected unit processes. First, slow feature analysis (SFA) is implemented to extract temporal information that reveals the dynamic relevance of the process data. Then, an integration method of metric learning and prior knowledge is proposed to construct coupled spatial relationships based on temporal information. In addition, a variational graph attention autoencoder (VGATE) is suggested to extract temporal and spatial information for fault detection, which incorporates the dominances of variational inference and graph attention mechanisms. The proposed method can automatically extract and deeply mine spatial–temporal interactive feature information to boost detection performance. Finally, three industrial process experiments are performed to verify the feasibility and effectiveness of the proposed method. The results demonstrate that the proposed method dramatically increases the fault detection rate (FDR) and reduces the false alarm rate (FAR).},
  archive      = {J_TNNLS},
  author       = {Mingjie Lv and Yonggang Li and Huiping Liang and Bei Sun and Chunhua Yang and Weihua Gui},
  doi          = {10.1109/TNNLS.2023.3328399},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3062-3076},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A Spatial–Temporal variational graph attention autoencoder using interactive information for fault detection in complex industrial processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributed proximal consensus algorithm for energy saving
in ethylene production. <em>TNNLS</em>, <em>35</em>(3), 3052–3061. (<a
href="https://doi.org/10.1109/TNNLS.2023.3320691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a distributed optimization framework in order to solve the plant-wide energy-saving problem of an ethylene plant. First, the ethylene production process is abstracted into a distributed network, and then, a new distributed consensus algorithm is proposed, which is called adaptive step-size-based distributed proximal consensus algorithm (ASS-DPCA). This algorithm can dynamically adjust the step size and automatically abandon the irrational evolutionary route while eliminating the dependence of optimization algorithms on model gradient information. Moreover, the designed algorithm is able to converge to an optimal solution for any convex cost functions and approach to a convex constraint set of agents over an undirected connected graph. Finally, the results of numerical simulation and industrial experiments show that the algorithm can reduce the total energy consumption of an ethylene plant with less computing time and assured consensus.},
  archive      = {J_TNNLS},
  author       = {Rong Nie and Wenli Du and Ting Wang and Zhongmei Li and Shuping He},
  doi          = {10.1109/TNNLS.2023.3320691},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3052-3061},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A distributed proximal consensus algorithm for energy saving in ethylene production},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic submodular-based learning strategy in imbalanced
drifting streams for real-time safety assessment in nonstationary
environments. <em>TNNLS</em>, <em>35</em>(3), 3038–3051. (<a
href="https://doi.org/10.1109/TNNLS.2023.3294788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of real-time safety assessment (RTSA) approaches in nonstationary environments is meaningful to reduce the possibility of significant losses. However, several challenging problems are needed to be well considered. The performance of existing approaches will be negatively affected in the settings of imbalanced drifting streams. In this case, the model design with the incremental update should also be explored. Furthermore, the query strategy should also be well-designed. This article investigates a dynamic submodular-based learning strategy to address such issues. Specifically, an efficient incremental update procedure is designed with the structure of the broad learning system (BLS), which is beneficial to the detection of concept drift. Furthermore, a novel dynamic submodular-based annotation with an activation interval strategy is proposed to select valuable samples in imbalanced drifting streams. The lower bound of annotation value is also proven theoretically with a novel drift adaption mechanism. Numerous experiments are conducted with the realistic data of JiaoLong deep-sea manned submersible. The experimental results show that the proposed approach can achieve better assessment accuracy than typical existing approaches.},
  archive      = {J_TNNLS},
  author       = {Zeyi Liu and Xiao He},
  doi          = {10.1109/TNNLS.2023.3294788},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3038-3051},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic submodular-based learning strategy in imbalanced drifting streams for real-time safety assessment in nonstationary environments},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Delicately reinforced k-nearest neighbor classifier combined
with expert knowledge applied to abnormity forecast in electrolytic
cell. <em>TNNLS</em>, <em>35</em>(3), 3027–3037. (<a
href="https://doi.org/10.1109/TNNLS.2023.3280963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the profit and safety requirements become higher and higher, it is more and more necessary to realize an advanced intelligent analysis for abnormity forecast of the synthetical balance of material and energy (AF-SBME) on aluminum reduction cells (ARCs). Without loss of generality, AF-SBME belongs to classification problems. Its advanced intelligent analysis can be realized by high-performance data-driven classifiers. However, AF-SBME has some difficulties, including a high requirement for interpretability of data-driven classifiers, a small number, and decreasing-over-time correctness of training samples. In this article, based on a preferable data-driven classifier, which is called a reinforced $k$ -nearest neighbor (R-KNN) classifier, a delicately R-KNN combined with expert knowledge (DR-KNN/CE) is proposed. It improves R-KNN in two ways, including using expert knowledge as external assistance and enhancing self-ability to mine and synthesize data knowledge. The related experiments on AF-SBME, where the relevant data are directly sampled from practical production, have demonstrated that the proposed DR-KNN/CE not only makes an effective improvement for R-KNN, but also has a more advanced performance compared with other existing high-performance data-driven classifiers.},
  archive      = {J_TNNLS},
  author       = {Jue Shi and Xiaofang Chen and Yongfang Xie and Hongliang Zhang and Yubo Sun},
  doi          = {10.1109/TNNLS.2023.3280963},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3027-3037},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Delicately reinforced k-nearest neighbor classifier combined with expert knowledge applied to abnormity forecast in electrolytic cell},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified framework for faster clustering via joint schatten
p-norm factorization with optimal mean. <em>TNNLS</em>, <em>35</em>(3),
3012–3026. (<a
href="https://doi.org/10.1109/TNNLS.2023.3327716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the effectiveness and efficiency of subspace clustering in visual tasks, this work introduces a novel approach that automatically eliminates the optimal mean, which is embedded in the subspace clustering framework of low-rank representation (LRR) methods, along with the computationally factored formulation of Schatten $p$ -norm. By addressing the issues related to meaningful computations involved in some LRR methods and overcoming biased estimation of the low-rank solver, we propose faster nonconvex subspace clustering methods through joint Schatten $p$ -norm factorization with optimal mean (JS $p$ NFOM), forming a unified framework for enhancing performance while reducing time consumption. The proposed approach employs tractable and scalable factor techniques, which effectively address the disadvantages of higher computational complexity, particularly when dealing with large-scale coefficient matrices. The resulting nonconvex minimization problems are reformulated and further iteratively optimized by multivariate weighting algorithms, eliminating the need for singular value decomposition (SVD) computations in the developed iteration procedures. Moreover, each subproblem can be guaranteed to obtain the closed-form solver, respectively. The theoretical analyses of convergence properties and computational complexity further support the applicability of the proposed methods in real-world scenarios. Finally, comprehensive experimental results demonstrate the effectiveness and efficiency of the proposed nonconvex clustering approaches compared to existing state-of-the-art methods on several publicly available databases. The demonstrated improvements highlight the practical significance of our work in subspace clustering tasks for visual data analysis. The source code for the proposed algorithms is publicly accessible at https://github.com/ZhangHengMin/TRANSUFFC .},
  archive      = {J_TNNLS},
  author       = {Hengmin Zhang and Jiaoyan Zhao and Bob Zhang and Chen Gong and Jianjun Qian and Jian Yang},
  doi          = {10.1109/TNNLS.2023.3327716},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3012-3026},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unified framework for faster clustering via joint schatten p-norm factorization with optimal mean},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explicit representation and customized fault isolation
framework for learning temporal and spatial dependencies in industrial
processes. <em>TNNLS</em>, <em>35</em>(3), 2997–3011. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typically, industrial processes possess both temporal and spatial dependencies due to intravariable dynamics and intervariable couplings. The two dependencies have different manifestations, indicating diverse process characteristics. However, the existing methods fail to separate temporal and spatial information well, leading to inappropriate representation and inaccurate fault detection and isolation results. This study proposes an explicit representation and customized fault isolation framework to tackle temporal and spatial characteristics, so as to identify and locate anomalies affecting different dependencies. First, we design a double-level separation method for temporal and spatial information. In the first level, we construct two independent auto-encoding modules to extract temporal correlation and spatial graph structure in parallel. In the second level, we propose an information aliasing loss function to guild the two modules to distinguish between temporal and spatial characteristics, further facilitating information separation. By monitoring the explicit temporal and spatial statistics obtained by the two modules, spatiotemporal dependencies of anomalies can be determined for subsequent isolation. Furthermore, we propose a customized isolation strategy for anomalies in temporal and spatial characteristics. By quantifying changes in intravariable temporal dynamics and intervariable spatial graph structure individually, temporal impact and spatial propagation of faults can be finely characterized and isolated. Three examples are adopted to verify the performance of the proposed framework, including a numerical example, a real condensing system of the thermal power plant process, and the Tennessee Eastman benchmark process.},
  archive      = {J_TNNLS},
  author       = {Pengyu Song and Chunhui Zhao and Biao Huang and Jinliang Ding},
  doi          = {10.1109/TNNLS.2023.3262277},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {2997-3011},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Explicit representation and customized fault isolation framework for learning temporal and spatial dependencies in industrial processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integration of encoding and temporal forecasting: Toward
end-to-end NOx prediction for industrial chemical process.
<em>TNNLS</em>, <em>35</em>(3), 2984–2996. (<a
href="https://doi.org/10.1109/TNNLS.2023.3276593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting NOx concentration in fluid catalytic cracking (FCC) regeneration flue gas can guide the real-time adjustment of treatment devices, and then furtherly prevent the excessive emission of pollutants. The process monitoring variables, which are usually high-dimensional time series, can provide valuable information for prediction. Although process features and cross-series correlations can be captured through feature extraction techniques, they are commonly linear transformation, and conducted or trained separately from forecasting model. This process is inefficient and might not be an optimal solution for the following forecasting modeling. Therefore, we propose a time series encoding temporal convolutional network (TSE-TCN). By parameterizing the hidden representation of the encoding–decoding structure with the temporal convolutional network (TCN), and combining the reconstruction error and the prediction error in the objective function, the encoding–decoding procedure and the temporal predicting procedure can be trained by a single optimizer. The effectiveness of the proposed method is verified through an industrial reaction and regeneration process of an FCC unit. Results demonstrate that TSE-TCN outperforms some state-of-art methods with lower root mean square error (RMSE) by 2.74% and higher ${R}^{2}$ score by 3.77%.},
  archive      = {J_TNNLS},
  author       = {Han Jiang and Shucai Zhang and Wenyu Yang and Xin Peng and Weimin Zhong},
  doi          = {10.1109/TNNLS.2023.3276593},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {2984-2996},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Integration of encoding and temporal forecasting: Toward end-to-end NOx prediction for industrial chemical process},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning-motivated intelligent fault diagnosis
designs: A survey, insights, and perspectives. <em>TNNLS</em>,
<em>35</em>(3), 2969–2983. (<a
href="https://doi.org/10.1109/TNNLS.2023.3290974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, transfer learning has attracted a great deal of attention as a new learning paradigm, based on which fault diagnosis (FD) approaches have been intensively developed to improve the safety and reliability of modern automation systems. Because of inevitable factors such as the varying work environment, performance degradation of components, and heterogeneity among similar automation systems, the FD method having long-term applicabilities becomes attractive. Motivated by these facts, transfer learning has been an indispensable tool that endows the FD methods with self-learning and adaptive abilities. On the presentation of basic knowledge in this field, a comprehensive review of transfer learning-motivated FD methods, whose two subclasses are developed based on knowledge calibration and knowledge compromise, is carried out in this survey article. Finally, some open problems, potential research directions, and conclusions are highlighted. Different from the existing reviews of transfer learning, this survey focuses on how to utilize previous knowledge specifically for the FD tasks, based on which three principles and a new classification strategy of transfer learning-motivated FD techniques are also presented. We hope that this work will constitute a timely contribution to transfer learning-motivated techniques regarding the FD topic.},
  archive      = {J_TNNLS},
  author       = {Hongtian Chen and Hao Luo and Biao Huang and Bin Jiang and Okyay Kaynak},
  doi          = {10.1109/TNNLS.2023.3290974},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {2969-2983},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transfer learning-motivated intelligent fault diagnosis designs: A survey, insights, and perspectives},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multigranularity surrogate modeling for evolutionary
multiobjective optimization with expensive constraints. <em>TNNLS</em>,
<em>35</em>(3), 2956–2968. (<a
href="https://doi.org/10.1109/TNNLS.2023.3297624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiobjective optimization problems (MOPs) with expensive constraints pose stiff challenges to existing surrogate-assisted evolutionary algorithms (SAEAs) in a very limited computational cost, due to the fact that the number of expensive constraints for an MOP is often large. For existing SAEAs, they always approximate constraint functions in a single granularity, namely, approximating the constraint violation (CV, coarse-grained) or each constraint (fine-grained). However, the landscape of CV is often too complex to be accurately approximated by a surrogate model. Although the modeling of each constraint function may be simpler than that of CV, approximating all the constraint functions independently may result in tremendous cumulative errors and high computational costs. To address this issue, in this article, we develop a multigranularity surrogate modeling framework for evolutionary algorithms (EAs), where the approximation granularity of constraint surrogates is adaptively determined by the position of the population in the fitness landscape. Moreover, a dedicated model management strategy is also developed to reduce the impact resulting from the errors introduced by constraint surrogates and prevent the population from trapping into local optima. To evaluate the performance of the proposed framework, an implementation called K-MGSAEA is proposed, and the experimental results on a large number of test problems show that the proposed framework is superior to seven state-of-the-art competitors.},
  archive      = {J_TNNLS},
  author       = {Yajie Zhang and Hao Jiang and Ye Tian and Haiping Ma and Xingyi Zhang},
  doi          = {10.1109/TNNLS.2023.3297624},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {2956-2968},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multigranularity surrogate modeling for evolutionary multiobjective optimization with expensive constraints},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Error-triggered adaptive sparse identification for
predictive control and its application to multiple operating conditions
processes. <em>TNNLS</em>, <em>35</em>(3), 2942–2955. (<a
href="https://doi.org/10.1109/TNNLS.2023.3262541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the digital transformation of process manufacturing, identifying the system model from process data and then applying to predictive control has become the most dominant approach in process control. However, the controlled plant often operates under changing operating conditions. What is more, there are often unknown operating conditions such as first appearance operating conditions, which make traditional predictive control methods based on identified model difficult to adapt to changing operating conditions. Moreover, the control accuracy is low during operating condition switching. To solve these problems, this article proposes an error-triggered adaptive sparse identification for predictive control (ETASI4PC) method. Specifically, an initial model is established based on sparse identification. Then, a prediction error-triggered mechanism is proposed to monitor operating condition changes in real time. Next, the previously identified model is updated with the fewest modifications by identifying parameter change, structural change, and combination of changes in the dynamical equations, thus achieving precise control to multiple operating conditions. Considering the problem of low control accuracy during the operating condition switching, a novel elastic feedback correction strategy is proposed to significantly improve the control accuracy in the transition period and ensure accurate control under full operating conditions. To verify the superiority of the proposed method, a numerical simulation case and a continuous stirred tank reactor (CSTR) case are designed. Compared with some state-of-the-art methods, the proposed method can rapidly adapt to frequent changes in operating conditions, and it can achieve real-time control effects even for unknown operating conditions such as first appearance operating conditions.},
  archive      = {J_TNNLS},
  author       = {Keke Huang and Zui Tao and Yishun Liu and Dehao Wu and Chunhua Yang and Weihua Gui},
  doi          = {10.1109/TNNLS.2023.3262541},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {2942-2955},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Error-triggered adaptive sparse identification for predictive control and its application to multiple operating conditions processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning of partially labeled data for quality
prediction based on stacked target-related laplacian autoencoder.
<em>TNNLS</em>, <em>35</em>(3), 2927–2941. (<a
href="https://doi.org/10.1109/TNNLS.2023.3321691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially labeled data, which is common in industrial processes due to the low sampling rate of quality variables, remains an important challenge in soft sensor applications. In order to exploit the information from partially labeled data, a target-related Laplacian autoencoder (TLapAE) is proposed in this work. In TLapAE, a novel target-related Laplacian regularizer is developed, which aims to extract structure-preserving and quality-related features by preserving the feature-target mapping according to the local geometrical structure of the data. In addition, stacked TLapAE (STLapAE) is further constructed to extract deep feature representations of the data by hierarchically stacking TLapAE blocks. For model training, backward propagation equations are derived based on matrix calculus techniques to update the model parameters of the proposed TLapAE. The effectiveness of the proposed STLapAE is evaluated using the butane content prediction case in a debutanizer column, the silicon content prediction case in a blast furnace (BF) ironmaking process, and the ethane concentration prediction case in an ethylene fractionator. The results show that the proposed TLapAE model has significantly improved prediction accuracy compared to soft sensors using only labeled data and other partially labeled data modeling methods.},
  archive      = {J_TNNLS},
  author       = {Bocun He and Xinmin Zhang and Zhihuan Song},
  doi          = {10.1109/TNNLS.2023.3321691},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {2927-2941},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning of partially labeled data for quality prediction based on stacked target-related laplacian autoencoder},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CM-GAN: A cross-modal generative adversarial network for
imputing completely missing data in digital industry. <em>TNNLS</em>,
<em>35</em>(3), 2917–2926. (<a
href="https://doi.org/10.1109/TNNLS.2023.3284666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal data fusion analysis is essential to model the uncertainty of environment awareness in digital industry. However, due to communication failure and cyberattack, the sampled time-series data often have the issue of data missing. In some extreme cases, part of units are unobservable for a long time, which results in complete data missing (CDM). To impute missing data, many models have been proposed. However, they cannot address the CDM issue, because no observation data of the unobservable units can be obtained in this case. Thus, to address the CDM issue, a novel cross-modal generative adversarial network (CM-GAN) is proposed in this article. It combines the cross-modal data fusion technique and the deep adversarial generation technique to construct a cross-modal data generator. This generator can generate long-term time-series data from widely existing spatio-temporal modal data in modern industrial system, and then impute missing value by replacing them with generated data. To test the performance of CM-GAN, extensive experiments are conducted on photovoltaic (PV) power output dataset. Compared with other baseline models, the performance of CM-GAN is generally better and reaches the state-of-the-art level. Moreover, sufficient ablation studies are conducted to present the contribution of the cross-modal data fusion technique and show the reasonability of parameter settings of CM-GAN. Apart from this, some prediction experiments are also conducted. The results show that the PV data recovered by CM-GAN can provide more predictability information for improving the prediction accuracy of deep learning model.},
  archive      = {J_TNNLS},
  author       = {Mingyu Kang and Ran Zhu and Duxin Chen and Xiaolu Liu and Wenwu Yu},
  doi          = {10.1109/TNNLS.2023.3284666},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {2917-2926},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CM-GAN: A cross-modal generative adversarial network for imputing completely missing data in digital industry},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial special issue on learning theories and
methods with application to digitized process manufacturing.
<em>TNNLS</em>, <em>35</em>(3), 2914–2916. (<a
href="https://doi.org/10.1109/TNNLS.2024.3362091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digitization of process manufacturing involves converting information and knowledge into a digital format through technologies, such as artificial intelligence (AI), the Internet of Things (IoT), blockchain, and digital twins. This transformation promotes extension and optimization within the industrial, supply, and value chains, aiming to enhance decision-making efficiency, enable agile operations, and ensure information security and privacy. However, the current learning and operational approaches in the process industry remain rooted in traditional informatization, falling short of the vision for digital transformation. To address this gap, it is crucial to implement fusion analysis, deepen understanding, adopt autonomous learning, and enable intelligent optimization based on life-cycle data. Therefore, it is of fundamental importance to realize the transformation of process manufacturing toward digitalization and intelligentization, i.e., the use of artificial intelligence with decision-making capability, via new learning theories, methods, and algorithms.},
  archive      = {J_TNNLS},
  author       = {Feng Qian and Yaochu Jin and Xinghuo Yu and Yang Tang and Guy B. Marin},
  doi          = {10.1109/TNNLS.2024.3362091},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {2914-2916},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial special issue on learning theories and methods with application to digitized process manufacturing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cardinality constrained portfolio optimization via
alternating direction method of multipliers. <em>TNNLS</em>,
<em>35</em>(2), 2901–2909. (<a
href="https://doi.org/10.1109/TNNLS.2022.3192065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by sparse learning, the Markowitz mean-variance model with a sparse regularization term is popularly used in sparse portfolio optimization. However, in penalty-based portfolio optimization algorithms, the cardinality level of the resultant portfolio relies on the choice of the regularization parameter. This brief formulates the mean-variance model as a cardinality ( $\ell _{0}$ -norm) constrained nonconvex optimization problem, in which we can explicitly specify the number of assets in the portfolio. We then use the alternating direction method of multipliers (ADMMs) concept to develop an algorithm to solve the constrained nonconvex problem. Unlike some existing algorithms, the proposed algorithm can explicitly control the portfolio cardinality. In addition, the dynamic behavior of the proposed algorithm is derived. Numerical results on four real-world datasets demonstrate the superiority of our approach over several state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Zhang-Lei Shi and Xiao Peng Li and Chi-Sing Leung and Hing Cheung So},
  doi          = {10.1109/TNNLS.2022.3192065},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2901-2909},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cardinality constrained portfolio optimization via alternating direction method of multipliers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep unsupervised active learning on learnable graphs.
<em>TNNLS</em>, <em>35</em>(2), 2894–2900. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning has been successfully applied to unsupervised active learning. However, the current method attempts to learn a nonlinear transformation via an auto-encoder while ignoring the sample relation, leaving huge room to design more effective representation learning mechanisms for unsupervised active learning. In this brief, we propose a novel deep unsupervised active learning model via learnable graphs, named ALLGs. ALLG benefits from learning optimal graph structures to acquire better sample representation and select representative samples. To make the learned graph structure more stable and effective, we take into account $k$ -nearest neighbor graph as a priori and learn a relation propagation graph structure. We also incorporate shortcut connections among different layers, which can alleviate the well-known over-smoothing problem to some extent. To the best of our knowledge, this is the first attempt to leverage graph structure learning for unsupervised active learning. Extensive experiments performed on six datasets demonstrate the efficacy of our method.},
  archive      = {J_TNNLS},
  author       = {Handong Ma and Changsheng Li and Xinchu Shi and Ye Yuan and Guoren Wang},
  doi          = {10.1109/TNNLS.2022.3190420},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2894-2900},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep unsupervised active learning on learnable graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Stability analysis of recurrent neural networks with
time-varying delay by flexible terminal interpolation method.
<em>TNNLS</em>, <em>35</em>(2), 2887–2893. (<a
href="https://doi.org/10.1109/TNNLS.2022.3188161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief studies the stability problem of recurrent neural networks with time-varying delay. Based on one tunable parameter $\alpha $ , a flexible terminal interpolation method is proposed to change the interval with fixed terminals as $2^{k+1}-3$ ones with flexible terminals. Associated with the flexible subintervals, a novel Lyapunov–Krasovskii functional with more delay information is constructed. In order to estimate the Lyapunov–Krasovskii functional, a quadratic reciprocally convex inequality is proposed, which covers some existing ones as its special cases. Based on these ingredients, a new stability criterion is derived in the form of linear matrix inequalities. A comprehensive comparison of results is given to illustrate the newly proposed stability criterion.},
  archive      = {J_TNNLS},
  author       = {Zhanshan Wang and Yufeng Tian},
  doi          = {10.1109/TNNLS.2022.3188161},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2887-2893},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability analysis of recurrent neural networks with time-varying delay by flexible terminal interpolation method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network gaussian processes by increasing depth.
<em>TNNLS</em>, <em>35</em>(2), 2881–2886. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed an increasing interest in the correspondence between infinitely wide networks and Gaussian processes. Despite the effectiveness and elegance of the current neural network Gaussian process theory, to the best of our knowledge, all the neural network Gaussian processes (NNGPs) are essentially induced by increasing width. However, in the era of deep learning, what concerns us more regarding a neural network is its depth as well as how depth impacts the behaviors of a network. Inspired by a width-depth symmetry consideration, we use a shortcut network to show that increasing the depth of a neural network can also give rise to a Gaussian process, which is a valuable addition to the existing theory and contributes to revealing the true picture of deep learning. Beyond the proposed Gaussian process by depth, we theoretically characterize its uniform tightness property and the smallest eigenvalue of the Gaussian process kernel. These characterizations can not only enhance our understanding of the proposed depth-induced Gaussian process but also pave the way for future applications. Lastly, we examine the performance of the proposed Gaussian process by regression experiments on two benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Shao-Qun Zhang and Fei Wang and Feng-Lei Fan},
  doi          = {10.1109/TNNLS.2022.3185375},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2881-2886},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network gaussian processes by increasing depth},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Type-dependent average dwell time method and its application
to delayed neural networks with large delays. <em>TNNLS</em>,
<em>35</em>(2), 2875–2880. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the stability of delayed neural networks with large delays. Unlike previous studies, the original large delay is separated into several parts. Then, the delayed neural network is viewed as the switched system with one stable and multiple unstable subsystems. To effectively guarantee the stability of the considered system, the type-dependent average dwell time (ADT) is proposed to handle switches between any two sequences. Besides, multiple Lyapunov functions (MLFs) are employed to establish stability conditions. Adding more delayed state vectors increases the allowable maximum delay bound (AMDB), reducing the conservatism of stability criteria. A general form of the global exponential stability condition is put forward. Finally, a numerical example illustrates the effectiveness, and superiority of our method over the existing one.},
  archive      = {J_TNNLS},
  author       = {Hui-Ting Wang and Yong He and Chuan-Ke Zhang},
  doi          = {10.1109/TNNLS.2022.3184712},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2875-2880},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Type-dependent average dwell time method and its application to delayed neural networks with large delays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PIRNet: Personality-enhanced iterative refinement network
for emotion recognition in conversation. <em>TNNLS</em>, <em>35</em>(2),
2863–2874. (<a
href="https://doi.org/10.1109/TNNLS.2022.3192469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in conversation (ERC) is important for enhancing user experience in human–computer interaction. Unlike vanilla emotion recognition in individual utterances, ERC aims to classify constituent utterances in a dialog into corresponding emotion labels, which makes contextual information crucial. In addition to contextual information, personality traits also affect emotional perception based on psychological findings. Although researchers have proposed several approaches and achieved promising results on ERC, current works in this domain rarely incorporate contextual information and personality influence. To this end, we propose a novel framework to integrate these factors seamlessly, called “Personality-enhanced Iterative Refinement Network (PIRNet).” Specifically, PIRNet is a multistage iterative method. To capture personality influence, PIRNet leverages personality traits to mimic emotional transitions and generates personality-enhanced results. Then we exploit sequence models to capture contextual information in conversations. To verify the effectiveness of our proposed method, we conduct experiments on three benchmark datasets for ERC, that is, IEMOCAP, CMU-MOSI, and CMU-MOSEI. Experimental results demonstrate that our PIRNet succeeds over currently advanced approaches to emotion recognition.},
  archive      = {J_TNNLS},
  author       = {Zheng Lian and Bin Liu and Jianhua Tao},
  doi          = {10.1109/TNNLS.2022.3192469},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2863-2874},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PIRNet: Personality-enhanced iterative refinement network for emotion recognition in conversation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view graph learning by joint modeling of consistency
and inconsistency. <em>TNNLS</em>, <em>35</em>(2), 2848–2862. (<a
href="https://doi.org/10.1109/TNNLS.2022.3192445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph learning has emerged as a promising technique for multi-view clustering due to its ability to learn a unified and robust graph from multiple views. However, existing graph learning methods mostly focus on the multi-view consistency issue, yet often neglect the inconsistency between views, which makes them vulnerable to possibly low-quality or noisy datasets. To overcome this limitation, we propose a new multi-view graph learning framework, which for the first time simultaneously and explicitly models multi-view consistency and inconsistency in a unified objective function, through which the consistent and inconsistent parts of each single-view graph as well as the unified graph that fuses the consistent parts can be iteratively learned. Though optimizing the objective function is NP-hard, we design a highly efficient optimization algorithm that can obtain an approximate solution with linear time complexity in the number of edges in the unified graph. Furthermore, our multi-view graph learning approach can be applied to both similarity graphs and dissimilarity graphs, which lead to two graph fusion-based variants in our framework. Experiments on 12 multi-view datasets have demonstrated the robustness and efficiency of the proposed approach. The code is available at https://github.com/youweiliang/Multi-view_Graph_Learning .},
  archive      = {J_TNNLS},
  author       = {Youwei Liang and Dong Huang and Chang-Dong Wang and Philip S. Yu},
  doi          = {10.1109/TNNLS.2022.3192445},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2848-2862},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-view graph learning by joint modeling of consistency and inconsistency},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Privacy-preserving distributed ADMM with event-triggered
communication. <em>TNNLS</em>, <em>35</em>(2), 2835–2847. (<a
href="https://doi.org/10.1109/TNNLS.2022.3192346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses distributed optimization problems, in which a group of agents cooperatively minimize the sum of their private objective functions via information exchanging. Building on alternating direction method of multipliers (ADMM), we propose a privacy-preserving and communication-efficient decentralized quadratically approximated ADMM algorithm, termed PC-DQM, for solving such type of problems under the scenario of limited communication. In PC-DQM, an event-triggered mechanism is designed to schedule the communication instants for reducing communication cost. Simultaneously, for privacy preservation, a Hessian matrix with perturbed noise is introduced to quadratically approximate the objective function, which results in a closed form of primal vector update and then avoids solving a subproblem at each iteration with possible high computation cost. In addition, the triggered scheme is also utilized to schedule the update of Hessian, which can also reduce computation cost. We theoretically show that PC-DQM can protect privacy but without losing accuracy. In addition, we rigorously prove that PC-DQM converges linearly to the exact optimal solution for strongly convex and smooth objective functions. Finally, numerical simulation is presented to illustrate the effectiveness and efficiency of our algorithm.},
  archive      = {J_TNNLS},
  author       = {Zhen Zhang and Shaofu Yang and Wenying Xu and Kai Di},
  doi          = {10.1109/TNNLS.2022.3192346},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2835-2847},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Privacy-preserving distributed ADMM with event-triggered communication},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subtype-aware dynamic unsupervised domain adaptation.
<em>TNNLS</em>, <em>35</em>(2), 2820–2834. (<a
href="https://doi.org/10.1109/TNNLS.2022.3192315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) has been successfully applied to transfer knowledge from a labeled source domain to target domains without their labels. Recently introduced transferable prototypical networks (TPNs) further address class-wise conditional alignment. In TPN, while the closeness of class centers between source and target domains is explicitly enforced in a latent space, the underlying fine-grained subtype structure and the cross-domain within-class compactness have not been fully investigated. To counter this, we propose a new approach to adaptively perform a fine-grained subtype-aware alignment to improve the performance in the target domain without the subtype label in both domains. The insight of our approach is that the unlabeled subtypes in a class have the local proximity within a subtype while exhibiting disparate characteristics because of different conditional and label shifts. Specifically, we propose to simultaneously enforce subtype-wise compactness and class-wise separation, by utilizing intermediate pseudo-labels. In addition, we systematically investigate various scenarios with and without prior knowledge of subtype numbers and propose to exploit the underlying subtype structure. Furthermore, a dynamic queue framework is developed to evolve the subtype cluster centroids steadily using an alternative processing scheme. Experimental results, carried out with multiview congenital heart disease data and VisDA and DomainNet, show the effectiveness and validity of our subtype-aware UDA, compared with state-of-the-art UDA methods.},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Liu and Fangxu Xing and Jane You and Jun Lu and C.-C. Jay Kuo and Georges El Fakhri and Jonghye Woo},
  doi          = {10.1109/TNNLS.2022.3192315},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2820-2834},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Subtype-aware dynamic unsupervised domain adaptation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReCNAS: Resource-constrained neural architecture search
based on differentiable annealing and dynamic pruning. <em>TNNLS</em>,
<em>35</em>(2), 2805–2819. (<a
href="https://doi.org/10.1109/TNNLS.2022.3192169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The differentiable neural architecture search (NAS) framework has obtained extensive attention and achieved remarkable performance due to its search efficiency. However, most existing differentiable NAS methods still suffer from issues of model collapse, degenerated search-evaluation correlation, and inefficient hardware deployment, which causes the searched architectures to be suboptimal in accuracy and cannot meet different computation resource constraints (e.g., FLOPs and latency). In this article, we propose a novel resource-constrained NAS (ReCNAS) method, which can efficiently search high-performance architectures that satisfy the given constraints, and deal with the issues observed in previous differentiable NAS methods from three aspects: search space, search strategy, and resource adaptability. First, we introduce an elastic densely connected layerwise search space, which decouples the architecture depth representation from the search of candidate operations to alleviate the aggregation of skip connections and architecture redundancies. Second, a scheme of group annealing and progressive pruning is proposed to improve the efficiency and bridge the search-evaluation gap, which steadily forces the architecture parameters close to binary distribution and progressively prunes the inferior operations. Third, we present a novel resource-constrained architecture generation method, which prunes the redundant channel throughout the search based on dynamic programming, making the searched architecture scalable to different devices and requirements. Extensive experimental results demonstrate the efficiency and search stability of our ReCNAS, which is capable of discovering high-performance architectures on different datasets and tasks, surpassing other NAS methods, while tightly meeting the target resource constraints without any tuning required. Besides, the searched architectures show strong generalizability to other complex vision tasks.},
  archive      = {J_TNNLS},
  author       = {Cheng Peng and Yangyang Li and Ronghua Shang and Licheng Jiao},
  doi          = {10.1109/TNNLS.2022.3192169},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2805-2819},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ReCNAS: Resource-constrained neural architecture search based on differentiable annealing and dynamic pruning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nearly optimal control for mixed zero-sum game based on
off-policy integral reinforcement learning. <em>TNNLS</em>,
<em>35</em>(2), 2793–2804. (<a
href="https://doi.org/10.1109/TNNLS.2022.3191847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we solve a class of mixed zero-sum game with unknown dynamic information of nonlinear system. A policy iterative algorithm that adopts integral reinforcement learning (IRL), which does not depend on system information, is proposed to obtain the optimal control of competitor and collaborators. An adaptive update law that combines critic-actor structure with experience replay is proposed. The actor function not only approximates optimal control of every player but also estimates auxiliary control, which does not participate in the actual control process and only exists in theory. The parameters of the actor-critic structure are simultaneously updated. Then, it is proven that the parameter errors of the polynomial approximation are uniformly ultimately bounded. Finally, the effectiveness of the proposed algorithm is verified by two given simulations.},
  archive      = {J_TNNLS},
  author       = {Ruizhuo Song and Gaofu Yang and Frank L. Lewis},
  doi          = {10.1109/TNNLS.2022.3191847},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2793-2804},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nearly optimal control for mixed zero-sum game based on off-policy integral reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial sequence labeling with structured gaussian
processes. <em>TNNLS</em>, <em>35</em>(2), 2783–2792. (<a
href="https://doi.org/10.1109/TNNLS.2022.3191726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing partial sequence labeling models mainly focus on a max-margin framework that fails to provide an uncertainty estimation of the prediction. Furthermore, the unique ground-truth disambiguation strategy employed by these models may include wrong label information for parameter learning. In this article, we propose structured Gaussian processes for partial sequence labeling (SGPPSL), which encodes uncertainty in the prediction and does not need extra effort for model selection and hyperparameter learning. The model employs factor-as-piece approximation that divides the linear-chain graph structure into the set of pieces, which preserves the basic Markov random field structure and effectively avoids handling a large number of candidate output sequences generated by partially annotated data. Then, confidence measure is introduced in the model to address different contributions of candidate labels, which enables the ground-truth label information to be utilized in parameter learning. Based on the derived lower bound of the variational lower bound of the proposed model, variational parameters and confidence measures are estimated in the framework of alternating optimization. Moreover, a weighted Viterbi algorithm is proposed to incorporate confidence measures to sequence prediction, which considers label ambiguity arose from multiple annotations in the training data and thus helps improve the performance. SGPPSL is evaluated on several sequence labeling tasks and the experimental results show the effectiveness of the proposed model.},
  archive      = {J_TNNLS},
  author       = {Xiaolei Lu and Tommy W. S. Chow},
  doi          = {10.1109/TNNLS.2022.3191726},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2783-2792},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Partial sequence labeling with structured gaussian processes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LipSound2: Self-supervised pre-training for lip-to-speech
reconstruction and lip reading. <em>TNNLS</em>, <em>35</em>(2),
2772–2782. (<a
href="https://doi.org/10.1109/TNNLS.2022.3191677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this work is to investigate the impact of crossmodal self-supervised pre-training for speech reconstruction (video-to-audio) by leveraging the natural co-occurrence of audio and visual streams in videos. We propose LipSound2 that consists of an encoder–decoder architecture and location-aware attention mechanism to map face image sequences to mel-scale spectrograms directly without requiring any human annotations. The proposed LipSound2 model is first pre-trained on $\sim 2400$ -h multilingual (e.g., English and German) audio-visual data (VoxCeleb2). To verify the generalizability of the proposed method, we then fine-tune the pre-trained model on domain-specific datasets (GRID and TCD-TIMIT) for English speech reconstruction and achieve a significant improvement on speech quality and intelligibility compared to previous approaches in speaker-dependent and speaker-independent settings. In addition to English, we conduct Chinese speech reconstruction on the Chinese Mandarin Lip Reading (CMLR) dataset to verify the impact on transferability. Finally, we train the cascaded lip reading (video-to-text) system by fine-tuning the generated audios on a pre-trained speech recognition system and achieve the state-of-the-art performance on both English and Chinese benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Leyuan Qu and Cornelius Weber and Stefan Wermter},
  doi          = {10.1109/TNNLS.2022.3191677},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2772-2782},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LipSound2: Self-supervised pre-training for lip-to-speech reconstruction and lip reading},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical similarity learning for aliasing suppression
image super-resolution. <em>TNNLS</em>, <em>35</em>(2), 2759–2771. (<a
href="https://doi.org/10.1109/TNNLS.2022.3191674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a highly ill-posed issue, single-image super-resolution (SISR) has been widely investigated in recent years. The main task of SISR is to recover the information loss caused by the degradation procedure. According to the Nyquist sampling theory, the degradation leads to the aliasing effect and makes it hard to restore the correct textures from low-resolution (LR) images. In practice, there are correlations and self-similarities among the adjacent patches in the natural images. This article considers the self-similarity and proposes a hierarchical image super-resolution network (HSRNet) to suppress the influence of aliasing. We consider the SISR issue in the optimization perspective and propose an iterative solution pattern based on the half-quadratic splitting (HQS) method. To explore the texture with local image prior, we design a hierarchical exploration block (HEB) and progressive increase the receptive field. Furthermore, multilevel spatial attention (MSA) is devised to obtain the relations of adjacent feature and enhance the high-frequency information, which acts as a crucial role for visual experience. The experimental result shows that HSRNet achieves better quantitative and visual performance than other works and remits the aliasing more effectively.},
  archive      = {J_TNNLS},
  author       = {Yuqing Liu and Qi Jia and Jian Zhang and Xin Fan and Shanshe Wang and Siwei Ma and Wen Gao},
  doi          = {10.1109/TNNLS.2022.3191674},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2759-2771},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical similarity learning for aliasing suppression image super-resolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototypical graph contrastive learning. <em>TNNLS</em>,
<em>35</em>(2), 2747–2758. (<a
href="https://doi.org/10.1109/TNNLS.2022.3191086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-level representations are critical in various real-world applications, such as predicting the properties of molecules. However, in practice, precise graph annotations are generally very expensive and time-consuming. To address this issue, graph contrastive learning constructs an instance discrimination task, which pulls together positive pairs (augmentation pairs of the same graph) and pushes away negative pairs (augmentation pairs of different graphs) for unsupervised representation learning. However, since for a query, its negatives are uniformly sampled from all graphs, existing methods suffer from the critical sampling bias issue, i.e., the negatives likely having the same semantic structure with the query, leading to performance degradation. To mitigate this sampling bias issue, in this article, we propose a prototypical graph contrastive learning (PGCL) approach. Specifically, PGCL models the underlying semantic structure of the graph data via clustering semantically similar graphs into the same group and simultaneously encourages the clustering consistency for different augmentations of the same graph. Then, given a query, it performs negative sampling via drawing the graphs from those clusters that differ from the cluster of query, which ensures the semantic difference between query and its negative samples. Moreover, for a query, PGCL further reweights its negative samples based on the distance between their prototypes (cluster centroids) and the query prototype such that those negatives having moderate prototype distance enjoy relatively large weights. This reweighting strategy is proven to be more effective than uniform sampling. Experimental results on various graph benchmarks testify the advantages of our PGCL over state-of-the-art methods. The code is publicly available at https://github.com/ha-lins/PGCL .},
  archive      = {J_TNNLS},
  author       = {Shuai Lin and Chen Liu and Pan Zhou and Zi-Yuan Hu and Shuojia Wang and Ruihui Zhao and Yefeng Zheng and Liang Lin and Eric Xing and Xiaodan Liang},
  doi          = {10.1109/TNNLS.2022.3191086},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2747-2758},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prototypical graph contrastive learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed neural networks training for robotic
manipulation with consensus algorithm. <em>TNNLS</em>, <em>35</em>(2),
2732–2746. (<a
href="https://doi.org/10.1109/TNNLS.2022.3191021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an algorithm that combines actor–critic-based off-policy method with consensus-based distributed training to deal with multiagent deep reinforcement learning problems. Specifically, convergence analysis of a consensus algorithm for a type of nonlinear system with a Lyapunov method is developed, and we use this result to analyze the convergence properties of the actor training parameters and the critic training parameters in our algorithm. Through the convergence analysis, it can be verified that all agents will converge to the same optimal model as the training time goes to infinity. To validate the implementation of our algorithm, a multiagent training framework is proposed to train each Universal Robot 5 (UR5) robot arm to reach the random target position. Finally, experiments are provided to demonstrate the effectiveness and feasibility of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Wenxing Liu and Hanlin Niu and Inmo Jang and Guido Herrmann and Joaquin Carrasco},
  doi          = {10.1109/TNNLS.2022.3191021},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2732-2746},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed neural networks training for robotic manipulation with consensus algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ACERAC: Efficient reinforcement learning in fine time
discretization. <em>TNNLS</em>, <em>35</em>(2), 2719–2731. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main goals of reinforcement learning (RL) is to provide a way for physical machines to learn optimal behavior instead of being programmed. However, effective control of the machines usually requires fine time discretization. The most common RL methods apply independent random elements to each action, which is not suitable in that setting. It is not feasible because it causes the controlled system to jerk and does not ensure sufficient exploration since a single action is not long enough to create a significant experience that could be translated into policy improvement. In our view, these are the main obstacles that prevent the application of RL in contemporary control systems. To address these pitfalls, in this article, we introduce an RL framework and adequate analytical tools for actions that may be stochastically dependent in subsequent time instances. We also introduce an RL algorithm that approximately optimizes a policy that produces such actions. It applies experience replay (ER) to adjust the likelihood of sequences of previous actions to optimize expected $n$ -step returns that the policy yields. The efficiency of this algorithm is verified against four other RL methods [continuous deep advantage updating (CDAU), proximal policy optimization (PPO), soft actor–critic (SAC), and actor–critic with ER (ACER)] in four simulated learning control problems (Ant, HalfCheetah, Hopper, and Walker2D) in diverse time discretization. The algorithm introduced here outperforms the competitors in most cases considered.},
  archive      = {J_TNNLS},
  author       = {Jakub Łyskawa and Paweł Wawrzyński},
  doi          = {10.1109/TNNLS.2022.3190973},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2719-2731},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ACERAC: Efficient reinforcement learning in fine time discretization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding pooling in graph neural networks.
<em>TNNLS</em>, <em>35</em>(2), 2708–2718. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many recent works in the field of graph machine learning have introduced pooling operators to reduce the size of graphs. In this article, we present an operational framework to unify this vast and diverse literature by describing pooling operators as the combination of three functions: selection, reduction, and connection (SRC). We then introduce a taxonomy of pooling operators, based on some of their key characteristics and implementation differences under the SRC framework. Finally, we propose three criteria to evaluate the performance of pooling operators and use them to investigate the behavior of different operators on a variety of tasks.},
  archive      = {J_TNNLS},
  author       = {Daniele Grattarola and Daniele Zambon and Filippo Maria Bianchi and Cesare Alippi},
  doi          = {10.1109/TNNLS.2022.3190922},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2708-2718},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding pooling in graph neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRLIE: Flexible low-light image enhancement via disentangled
representations. <em>TNNLS</em>, <em>35</em>(2), 2694–2707. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement (LIME) aims to convert images with unsatisfied lighting into desired ones. Different from existing methods that manipulate illumination in uncontrollable manners, we propose a flexible framework to take user-specified guide images as references to improve the practicability. To achieve the goal, this article models an image as the combination of two components, that is, content and exposure attribute, from an information decoupling perspective. Specifically, we first adopt a content encoder and an attribute encoder to disentangle the two components. Then, we combine the scene content information of the low-light image with the exposure attribute of the guide image to reconstruct the enhanced image through a generator. Extensive experiments on public datasets demonstrate the superiority of our approach over state-of-the-art alternatives. Particularly, the proposed method allows users to enhance images according to their preferences, by providing specific guide images. Our source code and the pretrained model are available at https://github.com/Linfeng-Tang/DRLIE .},
  archive      = {J_TNNLS},
  author       = {Linfeng Tang and Jiayi Ma and Hao Zhang and Xiaojie Guo},
  doi          = {10.1109/TNNLS.2022.3190880},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2694-2707},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DRLIE: Flexible low-light image enhancement via disentangled representations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Riemannian manifold-based feature space and corresponding
image clustering algorithms. <em>TNNLS</em>, <em>35</em>(2), 2680–2693.
(<a href="https://doi.org/10.1109/TNNLS.2022.3190836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image feature representation is a key factor influencing the accuracy of clustering. Traditional point-based feature spaces represent spectral features of an image independently and introduce spatial relationships of pixels in the image domain to enhance the contextual information expression ability. Mapping-based feature spaces aim to preserve the structure information, but the complex computation and the unexplainability of image features have a great impact on their applications. To this end, we propose an explicit feature space called Riemannian manifold feature space (RMFS) to present the contextual information in a unified way. First, the Gaussian probability distribution function (pdf) is introduced to characterize the features of a pixel in its neighborhood system in the image domain. Then, the feature-related pdfs are mapped to a Riemannian manifold, which constructs the proposed RMFS. In RMFS, a point can express the complex contextual information of corresponding pixel in the image domain, and pixels representing the same object are linearly distributed. This gives us a chance to convert nonlinear image segmentation problems to linear computation. To verify the superiority of the expression ability of the proposed RMFS, a linear clustering algorithm and a fuzzy linear clustering algorithm are proposed. Experimental results show that the proposed RMFS-based algorithms outperform their counterparts in the spectral feature space and the RMFS-based ones without the linear distribution characteristics. This indicates that the RMFS can better express features of an image than spectral feature space, and the expressed features can be easily used to construct linear segmentation models.},
  archive      = {J_TNNLS},
  author       = {Xuemei Zhao and Chen Li and Jun Wu and Xiaoli Li},
  doi          = {10.1109/TNNLS.2022.3190836},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2680-2693},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Riemannian manifold-based feature space and corresponding image clustering algorithms},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic neural–symbolic models with inductive
posterior constraints. <em>TNNLS</em>, <em>35</em>(2), 2667–2679. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural–symbolic models provide a powerful tool to tackle complex visual reasoning tasks by combining symbolic program execution for reasoning and deep representation learning for visual recognition. A probabilistic formulation of such models with stochastic latent variables can obtain an interpretable and legible reasoning system with less supervision. However, it is still nontrivial to generate reasonable symbolic structures without the guidance of domain knowledge, since it generally involves an optimization problem with both continuous and discrete variables. Despite the challenges, the interpretability of such symbolic structures provides an interface to regularize their generation by domain knowledge. In this article, we propose to incorporate the available domain knowledge into the learning process of probabilistic neural–symbolic (PNS) models via posterior constraints that directly regularize the structure posterior. In this way, our model is able to identify a middle point where the structure generation process mainly learns from data but also selectively borrows information from domain knowledge. We further present inductive reasoning where the posterior constraints can be automatically reweighted to handle noisy annotations. The experimental results show that our method achieves state-of-the-art performance on major abstract reasoning datasets and enjoys good generalization capability and data efficiency.},
  archive      = {J_TNNLS},
  author       = {Ke Su and Hang Su and Chongxuan Li and Jun Zhu and Bo Zhang},
  doi          = {10.1109/TNNLS.2022.3190820},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2667-2679},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic Neural–Symbolic models with inductive posterior constraints},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exponential stabilization of semi-markov reaction-diffusion
memristive NNs via event-based spatially pointwise-piecewise switching
control. <em>TNNLS</em>, <em>35</em>(2), 2655–2666. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers both the semi-Markov jumping phenomenon and spatial distribution characteristics when investigating the exponential stabilization of memristive neural networks (MNNs). The introduction of the semi-Markov jumping parameters relaxes the restriction on the sojourn time of Markovian MNNs. To increase the operability while ensuring control effect, a novel event-based spatially pointwise-piecewise switching control scheme is presented under a unified spatial division criterion, in which the pointwise and piecewise control can switch according to the preset event condition for the applicability to different control requirements. Moreover, by constructing a semi-Markov Lyapunov functional and utilizing the properties of the considered cumulative distribution function, the final exponential stabilization criterion and two related corollaries are obtained. Finally, simulation results illustrate the effectiveness and superiority of the proposed control strategy.},
  archive      = {J_TNNLS},
  author       = {Jingtao Man and Zhigang Zeng and Qiang Xiao and Hao Zhang},
  doi          = {10.1109/TNNLS.2022.3190694},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2655-2666},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential stabilization of semi-markov reaction-diffusion memristive NNs via event-based spatially pointwise-piecewise switching control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FAT: Frequency-aware transformation for bridging
full-precision and low-precision deep representations. <em>TNNLS</em>,
<em>35</em>(2), 2640–2654. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning low-bitwidth convolutional neural networks (CNNs) is challenging because performance may drop significantly after quantization. Prior arts often quantize the network weights by carefully tuning hyperparameters such as nonuniform stepsize and layerwise bitwidths, which are complicated since the full- and low-precision representations have large discrepancies. This work presents a novel quantization pipeline, named frequency-aware transformation (FAT), that features important benefits: 1) instead of designing complicated quantizers, FAT learns to transform network weights in the frequency domain to remove redundant information before quantization, making them amenable to training in low bitwidth with simple quantizers; 2) FAT readily embeds CNNs in low bitwidths using standard quantizers without tedious hyperparameter tuning and theoretical analyses show that FAT minimizes the quantization errors in both uniform and nonuniform quantizations; and 3) FAT can be easily plugged into various CNN architectures. Using FAT with a simple uniform/logarithmic quantizer can achieve the state-of-the-art performance in different bitwidths on various model architectures. Consequently, FAT serves to provide a novel frequency-based perspective for model quantization.},
  archive      = {J_TNNLS},
  author       = {Chaofan Tao and Rui Lin and Quan Chen and Zhaoyang Zhang and Ping Luo and Ngai Wong},
  doi          = {10.1109/TNNLS.2022.3190607},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2640-2654},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FAT: Frequency-aware transformation for bridging full-precision and low-precision deep representations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). TEA: A sequential recommendation framework via temporally
evolving aggregations. <em>TNNLS</em>, <em>35</em>(2), 2628–2639. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation aims to choose the most suitable items for a user at a specific timestamp given historical behaviors. Existing methods usually model the user behavior sequence based on transition-based methods such as Markov chain. However, these methods also implicitly assume that the users are independent of each other without considering the influence between users. In fact, this influence plays an important role in sequence recommendation since the behavior of a user is easily affected by others. Therefore, it is desirable to aggregate both user behaviors and the influence between users, which are evolved temporally and involved in the heterogeneous graph of users and items. In this article, we incorporate dynamic user–item heterogeneous graphs to propose a novel sequential recommendation framework. As a result, the historical behaviors as well as the influence between users can be taken into consideration. To achieve this, we first formalize sequential recommendation as a problem to estimate conditional probability given temporal dynamic heterogeneous graphs and user behavior sequences. After that, we exploit the conditional random field to aggregate the heterogeneous graphs and user behaviors for probability estimation and employ the pseudo-likelihood approach to derive a tractable objective function. Finally, we provide scalable and flexible implementations of the proposed framework. Experimental results on three real-world datasets not only demonstrate the effectiveness of our proposed method but also provide some insightful discoveries on the sequential recommendation.},
  archive      = {J_TNNLS},
  author       = {Zijian Li and Ruichu Cai and Fengzhu Wu and Sili Zhang and Hao Gu and Yuexing Hao and Yuguang Yan},
  doi          = {10.1109/TNNLS.2022.3190534},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2628-2639},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TEA: A sequential recommendation framework via temporally evolving aggregations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biased complementary-label learning without true labels.
<em>TNNLS</em>, <em>35</em>(2), 2616–2627. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complementary-label learning (CLL), the complementary transition matrix, denoting the probabilities that true labels flip into complementary labels (CLs) which specify classes observations do not belong to, is crucial to building statistically consistent classifiers. Most existing works implicitly assume that the transition probabilities are identical, which is not true in practice and may lead to undesirable bias in solutions. Few recent works have extended the problem to a biased setting but limit their explorations to modeling the transition matrix by exploiting the complementary class posteriors of anchor points (i.e., instances that almost certainly belong to a specific class). However, due to the severe corruption and unevenness of biased CLs, both anchor points and complementary class posteriors are difficult to predict accurately in the absence of true labels. In this article, rather than directly predicting these two error-prone items, we instead propose a divided-T estimator as an alternative to effectively learn transition matrices from only biased CLs. Specifically, we exploit semantic clustering to mitigate the adverse effects arising from CLs. By introducing the learned semantic clusters as an intermediate class, we factorize the original transition matrix into the product of two easy-to-estimate matrices that are not reliant on the two error-prone items. Both theoretical analyses and empirical results justify the effectiveness of the divided- $T$ estimator for estimating transition matrices under a mild assumption. Experimental results on benchmark datasets further demonstrate that the divided- $T$ estimator outperforms state-of-the-art (SOTA) methods by a substantial margin.},
  archive      = {J_TNNLS},
  author       = {Jianfei Ruan and Qinghua Zheng and Rui Zhao and Bo Dong},
  doi          = {10.1109/TNNLS.2022.3190528},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2616-2627},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Biased complementary-label learning without true labels},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiarmed bandit algorithms on zynq system-on-chip: Go
frequentist or bayesian? <em>TNNLS</em>, <em>35</em>(2), 2602–2615. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiarmed Bandit (MAB) algorithms identify the best arm among multiple arms via exploration–exploitation trade-off without prior knowledge of arm statistics. Their usefulness in wireless radio, Internet of Things (IoT), and robotics demand deployment on edge devices, and hence, a mapping on system-on-chip (SoC) is desired. Theoretically, the Bayesian-approach-based Thompson sampling (TS) algorithm offers better performance than the frequentist-approach-based upper confidence bound (UCB) algorithm. However, TS is not synthesizable due to Beta function. We address this problem by approximating it via a pseudorandom number generator (PRNG)-based architecture and efficiently realize the TS algorithm on Zynq SoC. In practice, the type of arms distribution (e.g., Bernoulli, Gaussian) is unknown, and hence, a single algorithm may not be optimal. We propose a reconfigurable and intelligent MAB (RI-MAB) framework. Here, intelligence enables the identification of appropriate MAB algorithms in an unknown environment, and reconfigurability allows on-the-fly switching between algorithms on the SoC. This eliminates the need for parallel implementation of algorithms resulting in huge savings in resources and power consumption. We analyze the functional correctness, area, power, and execution time of the proposed and existing architectures for various arm distributions, word length, and hardware–software codesign approaches. We demonstrate the superiority of the RI-MAB algorithm and its architecture over the TS and UCB algorithms.},
  archive      = {J_TNNLS},
  author       = {S. V. Sai Santosh and Sumit J. Darak},
  doi          = {10.1109/TNNLS.2022.3190509},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2602-2615},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiarmed bandit algorithms on zynq system-on-chip: Go frequentist or bayesian?},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning better registration to learn better few-shot
medical image segmentation: Authenticity, diversity, and robustness.
<em>TNNLS</em>, <em>35</em>(2), 2588–2601. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address the task of few-shot medical image segmentation (MIS) with a novel proposed framework based on the learning registration to learn segmentation (LRLS) paradigm. To cope with the limitations of lack of authenticity, diversity, and robustness in the existing LRLS frameworks, we propose the better registration better segmentation (BRBS) framework with three main contributions that are experimentally shown to have substantial practical merit. First, we improve the authenticity in the registration-based generation program and propose the knowledge consistency constraint strategy that constrains the registration network to learn according to the domain knowledge. It brings the semantic-aligned and topology-preserved registration, thus allowing the generation program to output new data with great space and style authenticity. Second, we deeply studied the diversity of the generation process and propose the space-style sampling program, which introduces the modeling of the transformation path of style and space change between few atlases and numerous unlabeled images into the generation program. Therefore, the sampling on the transformation paths provides much more diverse space and style features to the generated data effectively improving the diversity. Third, we first highlight the robustness in the learning of segmentation in the LRLS paradigm and propose the mix misalignment regularization, which simulates the misalignment distortion and constrains the network to reduce the fitting degree of misaligned regions. Therefore, it builds regularization for these regions improving the robustness of segmentation learning. Without any bells and whistles, our approach achieves a new state-of-the-art performance in few-shot MIS on two challenging tasks that outperform the existing LRLS-based few-shot methods. We believe that this novel and effective framework will provide a powerful few-shot benchmark for the field of medical image and efficiently reduce the costs of medical image research. All of our code will be made publicly available online.},
  archive      = {J_TNNLS},
  author       = {Yuting He and Rongjun Ge and Xiaoming Qi and Yang Chen and Jiasong Wu and Jean-Louis Coatrieux and Guanyu Yang and Shuo Li},
  doi          = {10.1109/TNNLS.2022.3190452},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2588-2601},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning better registration to learn better few-shot medical image segmentation: Authenticity, diversity, and robustness},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RCT: Resource constrained training for edge AI.
<em>TNNLS</em>, <em>35</em>(2), 2575–2587. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient neural network training is essential for in situ training of edge artificial intelligence (AI) and carbon footprint reduction in general. Train neural network on the edge is challenging because there is a large gap between limited resources on edge and the resource requirement of current training methods. Existing training methods are based on the assumption that the underlying computing infrastructure has sufficient memory and energy supplies. These methods involve two copies of the model parameters, which is usually beyond the capacity of on-chip memory in processors. The data movement between off-chip and on-chip memory incurs large amounts of energy. We propose resource constrained training (RCT) to realize resource-efficient training for edge devices and servers. RCT only keeps a quantized model throughout the training so that the memory requirement for model parameters in training is reduced. It adjusts per-layer bitwidth dynamically to save energy when a model can learn effectively with lower precision. We carry out experiments with representative models and tasks in image classification, natural language processing, and crowd counting applications. Experiments show that on average, 8–15-bit weight update is sufficient for achieving SOTA performance in these applications. RCT saves 63.5%–80% memory for model parameters and saves more energy for communications. Through experiments, we observe that the common practice on the first/last layer in model compression does not apply to efficient training. Also, interestingly, the more challenging a dataset is, the lower bitwidth is required for efficient training.},
  archive      = {J_TNNLS},
  author       = {Tian Huang and Tao Luo and Ming Yan and Joey Tianyi Zhou and Rick Goh},
  doi          = {10.1109/TNNLS.2022.3190451},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2575-2587},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RCT: Resource constrained training for edge AI},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRBM-ClustNet: A deep restricted boltzmann–kohonen
architecture for data clustering. <em>TNNLS</em>, <em>35</em>(2),
2560–2574. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian deep restricted Boltzmann–Kohonen architecture for data clustering termed deep restricted Boltzmann machine (DRBM)-ClustNet is proposed. This core-clustering engine consists of a DRBM for processing unlabeled data by creating new features that are uncorrelated and have large variance with each other. Next, the number of clusters is predicted using the Bayesian information criterion (BIC), followed by a Kohonen network (KN)-based clustering layer. The processing of unlabeled data is done in three stages for efficient clustering of the nonlinearly separable datasets. In the first stage, DRBM performs nonlinear feature extraction by capturing the highly complex data representation by projecting the feature vectors of $d$ dimensions into $n$ dimensions. Most clustering algorithms require the number of clusters to be decided a priori; hence, here, to automate the number of clusters in the second stage, we use BIC. In the third stage, the number of clusters derived from BIC forms the input for the KN, which performs clustering of the feature-extracted data obtained from the DRBM. This method overcomes the general disadvantages of clustering algorithms, such as the prior specification of the number of clusters, convergence to local optima, and poor clustering accuracy on nonlinear datasets. In this research, we use two synthetic datasets, 15 benchmark datasets from the UCI Machine Learning repository, and four image datasets to analyze the DRBM-ClustNet. The proposed framework is evaluated based on clustering accuracy and ranked against other state-of-the-art clustering methods. The obtained results demonstrate that the DRBM-ClustNet outperforms state-of-the-art clustering algorithms.},
  archive      = {J_TNNLS},
  author       = {J. Senthilnath and G. Nagaraj and C. Sumanth Simha and Sushant Kulkarni and Meenakumari Thapa and M. Indiramma and Jón Atli Benediktsson},
  doi          = {10.1109/TNNLS.2022.3190439},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2560-2574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DRBM-ClustNet: A deep restricted Boltzmann–Kohonen architecture for data clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometrical interpretation and design of multilayer
perceptrons. <em>TNNLS</em>, <em>35</em>(2), 2545–2559. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multilayer perceptron (MLP) neural network is interpreted from the geometrical viewpoint in this work, that is, an MLP partition an input feature space into multiple nonoverlapping subspaces using a set of hyperplanes, where the great majority of samples in a subspace belongs to one object class. Based on this high-level idea, we propose a three-layer feedforward MLP (FF-MLP) architecture for its implementation. In the first layer, the input feature space is split into multiple subspaces by a set of partitioning hyperplanes and rectified linear unit (ReLU) activation, which is implemented by the classical two-class linear discriminant analysis (LDA). In the second layer, each neuron activates one of the subspaces formed by the partitioning hyperplanes with specially designed weights. In the third layer, all subspaces of the same class are connected to an output node that represents the object class. The proposed design determines all MLP parameters in a feedforward one-pass fashion analytically without backpropagation. Experiments are conducted to compare the performance of the traditional backpropagation-based MLP (BP-MLP) and the new FF-MLP. It is observed that the FF-MLP outperforms the BP-MLP in terms of design time, training time, and classification performance in several benchmarking datasets. Our source code is available at https://colab.research.google.com/drive/1Gz0L8AnT4ijrUchrhEXXsnaacrFdenn?usp = sharing .},
  archive      = {J_TNNLS},
  author       = {Ruiyuan Lin and Zhiruo Zhou and Suya You and Raghuveer Rao and C.-C. Jay Kuo},
  doi          = {10.1109/TNNLS.2022.3190364},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2545-2559},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Geometrical interpretation and design of multilayer perceptrons},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Personalized federated few-shot learning. <em>TNNLS</em>,
<em>35</em>(2), 2534–2544. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (PFL) learns a personalized model for each client in a decentralized manner, where each client owns private data that are not shared and data among clients are non-independent and identically distributed (i.i.d.) However, existing PFL solutions assume that clients have sufficient training samples to jointly induce personalized models. Thus, existing PFL solutions cannot perform well in a few-shot scenario, where most or all clients only have a handful of samples for training. Furthermore, existing few-shot learning (FSL) approaches typically need centralized training data; as such, these FSL methods are not applicable in decentralized scenarios. How to enable PFL with limited training samples per client is a practical but understudied problem. In this article, we propose a solution called personalized federated few-shot learning (pFedFSL) to tackle this problem. Specifically, pFedFSL learns a personalized and discriminative feature space for each client by identifying which models perform well on which clients, without exposing local data of clients to the server and other clients, and which clients should be selected for collaboration with the target client. In the learned feature spaces, each sample is made closer to samples of the same category and farther away from samples of different categories. Experimental results on four benchmark datasets demonstrate that pFedFSL outperforms competitive baselines across different settings.},
  archive      = {J_TNNLS},
  author       = {Yunfeng Zhao and Guoxian Yu and Jun Wang and Carlotta Domeniconi and Maozu Guo and Xiangliang Zhang and Lizhen Cui},
  doi          = {10.1109/TNNLS.2022.3190359},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2534-2544},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Personalized federated few-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling the uncertainty of information propagation for
rumor detection: A neuro-fuzzy approach. <em>TNNLS</em>, <em>35</em>(2),
2522–2533. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic rumor detection is critical for maintaining a healthy social media environment. The mainstream methods generally learn rich features from information cascades by modeling the cascade as a tree or graph structure where edges are built based on interactions between a tweet and retweets. Some psychology studies have empirically shown that users’ various subjective factors always cause the uncertainty of interactions such as differences among interactive behavior activation thresholds or semantic relevancy. However, previous works model interactions by employing a simple fully connected layer on fixed edge weights in the graph and cannot reasonably describe this inherent uncertainty of complex interactions. In this article, inspired by the fuzzy theory, we propose a novel neuro-fuzzy method, fuzzy graph convolutional networks (FGCNs), to sufficiently understand uncertain interactions in the information cascade in a fuzzy perspective. Specifically, a new strategy of graph construction is first designed to convert each information cascade into a heterogeneous graph structure with the consideration of explicit interactive behaviors between a tweet and its retweet, as well as implicit interactive behaviors among retweets, enriching more structural clues in the graph. Then, we improve graph convolutional networks by incorporating edge fuzzification (EF) modules. The EFs adapt edge weights according to predefined membership to enhance message passing in the graph. The proposed model can provide a stronger relational inductive bias for expressing uncertain interactions and capture more discriminative and robust structural features for rumor detection. Extensive experiments demonstrate the effectiveness and superiority of FGCN on both rumor detection and early rumor detection.},
  archive      = {J_TNNLS},
  author       = {Lingwei Wei and Dou Hu and Wei Zhou and Xin Wang and Songlin Hu},
  doi          = {10.1109/TNNLS.2022.3190348},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2522-2533},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modeling the uncertainty of information propagation for rumor detection: A neuro-fuzzy approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical deep reinforcement learning-based propofol
infusion assistant framework in anesthesia. <em>TNNLS</em>,
<em>35</em>(2), 2510–2521. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to provide a hierarchical reinforcement learning (RL)-based solution to the automated drug infusion field. The learning policy is divided into the tasks of: 1) learning trajectory generative model and 2) planning policy model. The proposed deep infusion assistant policy gradient (DIAPG) model draws inspiration from adversarial autoencoders (AAEs) and learns latent representations of hypnotic depth trajectories. Given the trajectories drawn from the generative model, the planning policy infers a dose of propofol for stable sedation of a patient under total intravenous anesthesia (TIVA) using propofol and remifentanil. Through extensive evaluation, the DIAPG model can effectively stabilize bispectral index (BIS) and effect site concentration given a potentially time-varying target sequence. The proposed DIAPG shows an increased performance of 530% and 15% when a human expert and a standard reinforcement algorithm are used to infuse drugs, respectively.},
  archive      = {J_TNNLS},
  author       = {Won Joon Yun and Myungjae Shin and David Mohaisen and Kangwook Lee and Joongheon Kim},
  doi          = {10.1109/TNNLS.2022.3190379},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2510-2521},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical deep reinforcement learning-based propofol infusion assistant framework in anesthesia},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective video transformer with synchronized
spatiotemporal and spatial self-attention for action recognition.
<em>TNNLS</em>, <em>35</em>(2), 2496–2509. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have come to dominate vision-based deep neural network structures in both image and video models over the past decade. However, convolution-free vision Transformers (ViTs) have recently outperformed CNN-based models in image recognition. Despite this progress, building and designing video Transformers have not yet obtained the same attention in research as image-based Transformers. While there have been attempts to build video Transformers by adapting image-based Transformers for video understanding, these Transformers still lack efficiency due to the large gap between CNN-based models and Transformers regarding the number of parameters and the training settings. In this work, we propose three techniques to improve video understanding with video Transformers. First, to derive better spatiotemporal feature representation, we propose a new spatiotemporal attention scheme, termed synchronized spatiotemporal and spatial attention (SSTSA), which derives the spatiotemporal features with temporal and spatial multiheaded self-attention (MSA) modules. It also preserves the best spatial attention by another spatial self-attention module in parallel, thereby resulting in an effective Transformer encoder. Second, a motion spotlighting module is proposed to embed the short-term motion of the consecutive input frames to the regular RGB input, which is then processed with a single-stream video Transformer. Third, a simple intraclass frame interlacing method of the input clips is proposed that serves as an effective video augmentation method. Finally, our proposed techniques have been evaluated and validated with a set of extensive experiments in this study. Our video Transformer outperforms its previous counterparts on two well-known datasets, Kinetics400 and Something-Something-v2.},
  archive      = {J_TNNLS},
  author       = {Saghir Alfasly and Charles K. Chui and Qingtang Jiang and Jian Lu and Chen Xu},
  doi          = {10.1109/TNNLS.2022.3190367},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2496-2509},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An effective video transformer with synchronized spatiotemporal and spatial self-attention for action recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ADGAN: Attribute-driven generative adversarial network for
synthesis and multiclass classification of pulmonary nodules.
<em>TNNLS</em>, <em>35</em>(2), 2484–2495. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is the leading cause of cancer-related deaths worldwide. According to the American Cancer Society, early diagnosis of pulmonary nodules in computed tomography (CT) scans can improve the five-year survival rate up to 70% with proper treatment planning. In this article, we propose an attribute-driven Generative Adversarial Network (ADGAN) for synthesis and multiclass classification of Pulmonary Nodules. A self-attention U-Net (SaUN) architecture is proposed to improve the generation mechanism of the network. The generator is designed with two modules, namely, self-attention attribute module (SaAM) and a self-attention spatial module (SaSM). SaAM generates a nodule image based on given attributes whereas SaSM specifies the nodule region of the input image to be altered. A reconstruction loss along with an attention localization loss (AL) is used to produce an attention map prioritizing the nodule regions. To avoid resemblance between a generated image and a real image, we further introduce an adversarial loss containing a regularization term based on KL divergence. The discriminator part of the proposed model is designed to achieve the multiclass nodule classification task. Our proposed approach is validated over two challenging publicly available datasets, namely LIDC-IDRI and LUNGX. Exhaustive experimentation on these two datasets clearly indicate that we have achieved promising classification accuracy as compared to other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Rukhmini Roy and Suparna Mazumdar and Ananda S. Chowdhury},
  doi          = {10.1109/TNNLS.2022.3190331},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2484-2495},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ADGAN: Attribute-driven generative adversarial network for synthesis and multiclass classification of pulmonary nodules},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new graph autoencoder-based consensus-guided model for
scRNA-seq cell type detection. <em>TNNLS</em>, <em>35</em>(2),
2473–2483. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-cell RNA sequencing (scRNA-seq) technology is famous for providing a microscopic view to help capture cellular heterogeneity. This characteristic has advanced the field of genomics by enabling the delicate differentiation of cell types. However, the properties of single-cell datasets, such as high dropout events, noise, and high dimensionality, are still a research challenge in the single-cell field. To utilize single-cell data more efficiently and to better explore the heterogeneity among cells, a new graph autoencoder (GAE)-based consensus-guided model (scGAC) is proposed in this article. The data are preprocessed into multiple top-level feature datasets. Then, feature learning is performed by using GAEs to generate new feature matrices, followed by similarity learning based on distance fusion methods. The learned similarity matrices are fed back to the GAEs to guide their feature learning process. Finally, the abovementioned steps are iterated continuously to integrate the final consistent similarity matrix and perform other related downstream analyses. The scGAC model can accurately identify critical features and effectively preserve the internal structure of the data. This can further improve the accuracy of cell type identification.},
  archive      = {J_TNNLS},
  author       = {Dai-Jun Zhang and Ying-Lian Gao and Jing-Xiu Zhao and Chun-Hou Zheng and Jin-Xing Liu},
  doi          = {10.1109/TNNLS.2022.3190289},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2473-2483},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A new graph autoencoder-based consensus-guided model for scRNA-seq cell type detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Composite learning adaptive tracking control for full-state
constrained multiagent systems without using the feasibility condition.
<em>TNNLS</em>, <em>35</em>(2), 2460–2472. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a distributed consensus tracking controller for a class of nonlinear multiagent systems under a directed graph, in which all agents are subject to time-varying asymmetric full-state constraints, internal uncertainties, and external disturbances. The feasibility condition generally required in the existing constrained control is removed by using the proposed nonlinear mapping function (NMF)-based state reconstruction technology, and the Lipschitz condition usually needed in the consensus tracking is also canceled based on the adaptive command-filtered backstepping framework. The composite learning of the neural network-based function approximator (NN-FAP) and the finite-time smooth disturbance observer (DOB) provides a novel scheme for handling internal and external uncertainties simultaneously. One advantage of this scheme is that the use of online historical data of the closed-loop system strengthens the excitation of NN’s learning. Another advantage is that the DOB with NN-FAP embedding realizes that the finite-time observation for external disturbance in the case of the system dynamics is unknown. A complete controller design, sufficient stability analysis, and numerical simulation are provided.},
  archive      = {J_TNNLS},
  author       = {Yunbiao Jiang and Fuyong Wang and Zhongxin Liu and Zengqiang Chen},
  doi          = {10.1109/TNNLS.2022.3190286},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2460-2472},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Composite learning adaptive tracking control for full-state constrained multiagent systems without using the feasibility condition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameterized convex universal approximators for
decision-making problems. <em>TNNLS</em>, <em>35</em>(2), 2448–2459. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameterized max-affine (PMA) and parameterized log-sum-exp (PLSE) networks are proposed for general decision-making problems. The proposed approximators generalize existing convex approximators, namely max-affine (MA) and log-sum-exp (LSE) networks, by considering function arguments of condition and decision variables and replacing the network parameters of MA and LSE networks with continuous functions with respect to the condition variable. The universal approximation theorem (UAT) of PMA and PLSE is proved, which implies that PMA and PLSE are shape-preserving universal approximators for parameterized convex continuous functions. Practical guidelines for incorporating deep neural networks within PMA and PLSE networks are provided. A numerical simulation is performed to demonstrate the performance of the proposed approximators. The simulation results support that PLSE outperforms other existing approximators in terms of a minimizer and optimal value errors with scalable and efficient computation for high-dimensional cases.},
  archive      = {J_TNNLS},
  author       = {Jinrae Kim and Youdan Kim},
  doi          = {10.1109/TNNLS.2022.3190198},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2448-2459},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameterized convex universal approximators for decision-making problems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate density estimation by neural networks.
<em>TNNLS</em>, <em>35</em>(2), 2436–2447. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose nonparametric methods to obtain the Probability Density Function (PDF) to assess the properties of the underlying data generating process (DGP) without imposing any assumptions on the DGP, using neural networks (NNs). The proposed NN has advantages compared to well-known parametric and nonparametric density estimators. Our approach builds on literature on cumulative distribution function (CDF) estimation using NN. We extend this literature by providing analytical derivatives of this obtained CDF. Our approach hence removes the numerical approximation error in differentiating the CDF output, leading to more accurate PDF estimates. The proposed solution applies to any NN model, i.e., for any number of hidden layers or hidden neurons in the multilayer perceptron (MLP) structure. The proposed solution applies the PDF estimation by NN to continuous distributions as well as discrete distributions. We also show that the proposed solution to obtain the PDF leads to good approximations when applied to correlated variables in a multivariate setting. We test the performance of our method in a large Monte Carlo simulation using various complex distributions. Subsequently, we apply our method to estimate the density of the number of vehicle counts per minute measured with road sensors for a time window of 24 h.},
  archive      = {J_TNNLS},
  author       = {Dewi E. W. Peerlings and Jan A. van den Brakel and Nalan Baştürk and Marco J. H. Puts},
  doi          = {10.1109/TNNLS.2022.3190220},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2436-2447},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multivariate density estimation by neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-level knowledge distillation via knowledge alignment
and correlation. <em>TNNLS</em>, <em>35</em>(2), 2425–2435. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) has become a widely used technique for model compression and knowledge transfer. We find that the standard KD method performs the knowledge alignment on an individual sample indirectly via class prototypes and neglects the structural knowledge between different samples, namely, knowledge correlation. Although recent contrastive learning-based distillation methods can be decomposed into knowledge alignment and correlation, their correlation objectives undesirably push apart representations of samples from the same class, leading to inferior distillation results. To improve the distillation performance, in this work, we propose a novel knowledge correlation objective and introduce the dual-level knowledge distillation (DLKD), which explicitly combines knowledge alignment and correlation together instead of using one single contrastive objective. We show that both knowledge alignment and correlation are necessary to improve the distillation performance. In particular, knowledge correlation can serve as an effective regularization to learn generalized representations. The proposed DLKD is task-agnostic and model-agnostic, and enables effective knowledge transfer from supervised or self-supervised pretrained teachers to students. Experiments show that DLKD outperforms other state-of-the-art methods on a large number of experimental settings including: 1) pretraining strategies; 2) network architectures; 3) datasets; and 4) tasks.},
  archive      = {J_TNNLS},
  author       = {Fei Ding and Yin Yang and Hongxin Hu and Venkat Krovi and Feng Luo},
  doi          = {10.1109/TNNLS.2022.3190166},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2425-2435},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual-level knowledge distillation via knowledge alignment and correlation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multitargets joint training lightweight model for object
detection of substation. <em>TNNLS</em>, <em>35</em>(2), 2413–2424. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object detection of the substation is the key to ensuring the safety and reliable operation of the substation. The traditional image detection algorithms use the corresponding texture features of single-class objects and would not handle other different class objects easily. The object detection algorithm based on deep networks has generalization, and its sizeable complex backbone limits the application in the substation monitoring terminals with weak computing power. This article proposes a multitargets joint training lightweight model. The proposed model uses the feature maps of the complex model and the labels of objects in images as training multitargets. The feature maps have deeper feature information, and the feature maps of complex networks have higher information entropy than lightweight networks have. This article proposes the heat pixels method to improve the adequate object information because of the imbalance of the proportion between the foreground and the background. The heat pixels method is designed as a kind of reverse network calculation and reflects the object’s position to the pixels of the feature maps. The temperature of the pixels indicates the probability of the existence of the objects in the locations. Three different lightweight networks use the complex model feature maps and the traditional tags as the training multitargets. The public dataset VOC and the substation equipment dataset are adopted in the experiments. The experimental results demonstrate that the proposed model can effectively improve object detection accuracy and reduce the time-consuming and calculation amount.},
  archive      = {J_TNNLS},
  author       = {Xingyu Yan and Lixin Jia and Hui Cao and Yajie Yu and Tao Wang and Feng Zhang and Qingshu Guan},
  doi          = {10.1109/TNNLS.2022.3190139},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2413-2424},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitargets joint training lightweight model for object detection of substation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global prescribed performance control for strict feedback
systems pursuing uncertain target. <em>TNNLS</em>, <em>35</em>(2),
2403–2412. (<a
href="https://doi.org/10.1109/TNNLS.2022.3189951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, an online solution for reconstructing and predicting the uncertain target trajectory in real-time is proposed based on general regression neural network (GRNN). On this basis, an adaptive tracking control scheme guaranteeing prescribed performance is suggested for a class of strict-feedback systems with unknown control directions. In contrast to existing trajectory reconstruction methods, the one presented in this note does not require prior modeling of the uncertain target or offline training. Contrary to most current state-of-the-art prescribed performance control (PPC) technology, a novel time-varying scaling function and its corresponding translation function are introduced such that no strict constraints on initial conditions are needed, that is, global stability is achieved. The proposed control scheme allows the output of the system to chase the predicted value of the uncertain target, and the tracking error converges to a prescribed small set within a preassigned time, despite unmatched uncertainties and unknown control directions. The benefits of the proposed control scheme are confirmed by numerical simulations.},
  archive      = {J_TNNLS},
  author       = {Zhuwu Shao and Yujuan Wang and Xiang Chen},
  doi          = {10.1109/TNNLS.2022.3189951},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2403-2412},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global prescribed performance control for strict feedback systems pursuing uncertain target},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning flow-based disentanglement. <em>TNNLS</em>,
<em>35</em>(2), 2390–2402. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face reenactment aims to generate the talking face images of a target person given by a face image of source person. It is crucial to learn latent disentanglement to tackle such a challenging task through domain mapping between source and target images. The attributes or talking features due to domains or conditions become adjustable to generate target images from source images. This article presents an information-theoretic attribute factorization (AF) where the mixed features are disentangled for flow-based face reenactment. The latent variables with flow model are factorized into the attribute-relevant and attribute-irrelevant components without the need of the paired face images. In particular, the domain knowledge is learned to provide the condition to identify the talking attributes from real face images. The AF is guided in accordance with multiple losses for source structure, target structure, random-pair reconstruction, and sequential classification. The random-pair reconstruction loss is calculated by means of exchanging the attribute-relevant components within a sequence of face images. In addition, a new mutual information flow is constructed for disentanglement toward domain mapping, condition irrelevance, and condition relevance. The disentangled features are learned and controlled to generate image sequence with meaningful interpretation. Experiments on mouth reenactment illustrate the merit of individual and hybrid models for conditional generation and mapping based on the informative AF.},
  archive      = {J_TNNLS},
  author       = {Jen-Tzung Chien and Sheng-Jhe Huang},
  doi          = {10.1109/TNNLS.2022.3190068},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2390-2402},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning flow-based disentanglement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A pseudoinversion-free method for weight updating in broad
learning system. <em>TNNLS</em>, <em>35</em>(2), 2378–2389. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have evolved into one of the most critical tools in the field of artificial intelligence. As a kind of shallow feedforward neural network, the broad learning system (BLS) uses a training process based on random and pseudoinverse methods, and it does not need to go through a complete training cycle to obtain new parameters when adding nodes. Instead, it performs rapid update iterations on the basis of existing parameters through a series of dynamic update algorithms, which enables BLS to combine high efficiency and accuracy flexibly. The training strategy of BLS is completely different from the existing mainstream neural network training strategy based on the gradient descent algorithm, and the superiority of the former has been proven in many experiments. This article applies an ingenious method of pseudoinversion to the weight updating process in BLS and employs it as an alternative strategy for the dynamic update algorithms in the original BLS. Theoretical analyses and numerical experiments demonstrate the efficiency and effectiveness of BLS aided with this method. The research presented in this article can be regarded as an extended study of the BLS theory, providing an innovative idea and direction for future research on BLS.},
  archive      = {J_TNNLS},
  author       = {Mei Liu and Xiufang Chen and Mingsheng Shang and Hongwei Li},
  doi          = {10.1109/TNNLS.2022.3190043},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2378-2389},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A pseudoinversion-free method for weight updating in broad learning system},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implementing graph-theoretic feature selection by quantum
approximate optimization algorithm. <em>TNNLS</em>, <em>35</em>(2),
2364–2377. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays a significant role in computer science; nevertheless, this task is intractable since its search space scales exponentially with the number of dimensions. Motivated by the potential advantages of near-term quantum computing, three graph-theoretic feature selection (GTFS) methods, including minimum cut (MinCut)-based, densest $k$ -subgraph (DkS)-based, and maximal-independent set/minimal vertex cover (MIS/MVC)-based, are investigated in this article, where the original graph-theoretic problems are naturally formulated as the quadratic problems in binary variables and then solved using the quantum approximate optimization algorithm (QAOA). Specifically, three separate graphs are created from the raw feature set, where the vertex set consists of individual features and pairwise measure describes the edge. The corresponding feature subset is generated by deriving a subgraph from the established graph using QAOA. For the above three GTFS approaches, the solving procedure and quantum circuit for the corresponding graph-theoretic problems are formulated with the framework of QAOA. In addition, those proposals could be employed as a local solver and integrated with the Tabu search algorithm for solving large-scale GTFS problems utilizing limited quantum bit resource. Finally, extensive numerical experiments are conducted with 20 publicly available datasets and the results demonstrate that each model is superior to its classical scheme. In addition, the complexity of each model is only $\mathcal {O}(p n^{2})$ even in the worst cases, where $p$ is the number of layers in QAOA and $n$ is the number of features.},
  archive      = {J_TNNLS},
  author       = {Yaochong Li and Ri-Gui Zhou and Ruiqing Xu and Jia Luo and Wenwen Hu and Ping Fan},
  doi          = {10.1109/TNNLS.2022.3190042},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2364-2377},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Implementing graph-theoretic feature selection by quantum approximate optimization algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Who wants to shop with you: Joint product–participant
recommendation for group-buying service. <em>TNNLS</em>, <em>35</em>(2),
2353–2363. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the great success of group buying (GB) in social e-commerce, opening up a new way of online shopping. In this business model, a user can launch a GB as an initiator to share her interested product with social friends. The GB is clinched once enough friends join in as participants to copurchase the shared product. As such, a successful GB depends on not only whether the initiator can find her interested product but also whether the friends are willing to join in as participants. Most existing recommenders are incompetent in such complex scenario, as they merely seek to help users find their preferred products and cannot help identify potential participants to join in a GB. To this end, we propose a novel joint product–participant recommendation (J2PRec) framework, which recommends both candidate products and participants for maximizing the success rate of a GB. Specifically, J2PRec first designs a relational graph embedding module, which effectively encodes the various relations in GB for learning enhanced user and product embeddings. It then jointly learns the product and participant recommendation tasks under a probabilistic framework to maximize the GB likelihood, i.e., boost the success rate of a GB. Extensive experiments on three real-world datasets demonstrate the superiority of J2PRec for GB recommendation.},
  archive      = {J_TNNLS},
  author       = {Xiao Sha and Zhu Sun and Jie Zhang and Yew-Soon Ong},
  doi          = {10.1109/TNNLS.2022.3190003},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2353-2363},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Who wants to shop with you: Joint Product–Participant recommendation for group-buying service},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RPCA-AENet: Clutter suppression and simultaneous stationary
scene and moving targets imaging in the presence of motion errors.
<em>TNNLS</em>, <em>35</em>(2), 2339–2352. (<a
href="https://doi.org/10.1109/TNNLS.2022.3189997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clutter suppression and ground moving target imaging in synthetic aperture radar (SAR) system have been receiving increasing attention for both civilian and military applications. The problem of clutter suppression and ground moving target imaging in practical applications is much more challenging due to the motion error of the radar platform. In this article, we focus on the problems of clutter suppression and simultaneous stationary and moving target imaging in the presence of motion errors. Specifically, we propose a robust principal component analysis autoencoder network (RPCA-AENet) in a single-channel SAR system. In RPCA-AENet, the encoder transforms the SAR echo into imaging results of stationary scene and ground moving targets, and the decoder regenerates the SAR echo using the obtained imaging results. The encoder is designed by the unfolded robust principal component analysis (RPCA), while the decoder is formulated into two dense layers and one additional layer. Joint reconstruction loss, entropy loss, and measurement distance loss are utilized to guide the training of the RPCA-AENet. Notably, the algorithm operates in a totally self-supervised form and requires no other labeled SAR data. The methodology was tested on numerical SAR data. These tests show that the proposed architecture outperforms other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Wei Pu and Yi Bao},
  doi          = {10.1109/TNNLS.2022.3189997},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2339-2352},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RPCA-AENet: Clutter suppression and simultaneous stationary scene and moving targets imaging in the presence of motion errors},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collective decision of one-vs-rest networks for open-set
recognition. <em>TNNLS</em>, <em>35</em>(2), 2327–2338. (<a
href="https://doi.org/10.1109/TNNLS.2022.3189996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unknown examples that are unseen during training often appear in real-world pattern recognition tasks, and an intelligent self-learning system should be able to distinguish between known examples and unknown examples. Accordingly, open-set recognition (OSR), which addresses the problem of classifying knowns and identifying unknowns, has recently been highlighted. However, conventional deep neural networks (DNNs) using a softmax layer are vulnerable to overgeneralization, producing high confidence scores for unknowns. In this article, we propose a simple OSR method that is based on the intuition that the OSR performance can be maximized by setting strict and sophisticated decision boundaries that reject unknowns while maintaining satisfactory classification performance for knowns. For this purpose, a novel network structure, in which multiple one-vs-rest networks (OVRNs) follow a convolutional neural network (CNN) feature extractor, is proposed. Here, an OVRN is a simple feedforward neural network that is designed to assign confidence scores that are lower than those in the softmax layer to unknown samples so that unknown samples can be more effectively separated from known classes. Furthermore, the collective decision score is modeled by combining the multiple decisions reached by the OVRNs to alleviate overgeneralization. Extensive experiments were conducted on various datasets, and the experimental results show that the proposed method performs significantly better than the state-of-the-art methods by effectively reducing overgeneralization. The code is available at https://github.com/JaeyeonJang/Openset-collective-decision .},
  archive      = {J_TNNLS},
  author       = {Jaeyeon Jang and Chang Ouk Kim},
  doi          = {10.1109/TNNLS.2022.3189996},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2327-2338},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collective decision of one-vs-rest networks for open-set recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Fractional fourier image transformer for multimodal remote
sensing data classification. <em>TNNLS</em>, <em>35</em>(2), 2314–2326.
(<a href="https://doi.org/10.1109/TNNLS.2022.3189994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent development of the joint classification of hyperspectral image (HSI) and light detection and ranging (LiDAR) data, deep learning methods have achieved promising performance owing to their locally sematic feature extracting ability. Nonetheless, the limited receptive field restricted the convolutional neural networks (CNNs) to represent global contextual and sequential attributes, while visual image transformers (VITs) lose local semantic information. Focusing on these issues, we propose a fractional Fourier image transformer (FrIT) as a backbone network to extract both global and local contexts effectively. In the proposed FrIT framework, HSI and LiDAR data are first fused at the pixel level, and both multisource feature and HSI feature extractors are utilized to capture local contexts. Then, a plug-and-play image transformer FrIT is explored for global contextual and sequential feature extraction. Unlike the attention-based representations in classic VIT, FrIT is capable of speeding up the transformer architectures massively and learning valuable contextual information effectively and efficiently. More significantly, to reduce redundancy and loss of information from shallow to deep layers, FrIT is devised to connect contextual features in multiple fractional domains. Five HSI and LiDAR scenes including one newly labeled benchmark are utilized for extensive experiments, showing improvement over both CNNs and VITs.},
  archive      = {J_TNNLS},
  author       = {Xudong Zhao and Mengmeng Zhang and Ran Tao and Wei Li and Wenzhi Liao and Lianfang Tian and Wilfried Philips},
  doi          = {10.1109/TNNLS.2022.3189994},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2314-2326},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fractional fourier image transformer for multimodal remote sensing data classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Refining graph structure for incomplete multi-view
clustering. <em>TNNLS</em>, <em>35</em>(2), 2300–2313. (<a
href="https://doi.org/10.1109/TNNLS.2022.3189763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a challenging problem, incomplete multi-view clustering (MVC) has drawn much attention in recent years. Most of the existing methods contain the feature recovering step inevitably to obtain the clustering result of incomplete multi-view datasets. The extra target of recovering the missing feature in the original data space or common subspace is difficult for unsupervised clustering tasks and could accumulate mistakes during the optimization. Moreover, the biased error is not taken into consideration in the previous graph-based methods. The biased error represents the unexpected change of incomplete graph structure, such as the increase in the intra-class relation density and the missing local graph structure of boundary instances. It would mislead those graph-based methods and degrade their final performance. In order to overcome these drawbacks, we propose a new graph-based method named Graph Structure Refining for Incomplete MVC (GSRIMC). GSRIMC avoids recovering feature steps and just fully explores the existing subgraphs of each view to produce superior clustering results. To handle the biased error, the biased error separation is the core step of GSRIMC. In detail, GSRIMC first extracts basic information from the precomputed subgraph of each view and then separates refined graph structure from biased error with the help of tensor nuclear norm. Besides, cross-view graph learning is proposed to capture the missing local graph structure and complete the refined graph structure based on the complementary principle. Extensive experiments show that our method achieves better performance than other state-of-the-art baselines.},
  archive      = {J_TNNLS},
  author       = {Xiang-Long Li and Man-Sheng Chen and Chang-Dong Wang and Jian-Huang Lai},
  doi          = {10.1109/TNNLS.2022.3189763},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2300-2313},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Refining graph structure for incomplete multi-view clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving knowledge distillation with a customized teacher.
<em>TNNLS</em>, <em>35</em>(2), 2290–2299. (<a
href="https://doi.org/10.1109/TNNLS.2022.3189680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) is a widely used approach to transfer knowledge from a cumbersome network (also known as a teacher) to a lightweight network (also known as a student). However, even though the accuracies of different teachers are similar, the fixed student’s accuracies are significantly different. We find that teachers with more dispersed secondary soft probabilities are more qualified to play their roles. Therefore, an indicator, i.e., the standard deviation $\sigma $ of secondary soft probabilities, is introduced to choose the teacher. Moreover, to make a teacher’s secondary soft probabilities more dispersed, a novel method, dubbed pretraining the teacher under dual supervision (PTDS), is proposed to pretrain a teacher under dual supervision. In addition, we put forward an asymmetrical transformation function (ATF) to further enhance the dispersion degree of the pretrained teachers’ secondary soft probabilities. The combination of PTDS and ATF is termed knowledge distillation with a customized teacher (KDCT). Extensive empirical experiments and analyses are conducted on three computer vision tasks, including image classification, transfer learning, and semantic segmentation, to substantiate the effectiveness of KDCT.},
  archive      = {J_TNNLS},
  author       = {Chao Tan and Jie Liu},
  doi          = {10.1109/TNNLS.2022.3189680},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2290-2299},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving knowledge distillation with a customized teacher},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fixed-time pinning common synchronization and adaptive
synchronization for delayed quaternion-valued neural networks.
<em>TNNLS</em>, <em>35</em>(2), 2276–2289. (<a
href="https://doi.org/10.1109/TNNLS.2022.3189625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the fixed-time pinning common synchronization and adaptive synchronization for quaternion-valued neural networks with time-varying delays. First, to reduce transmission burdens and limit convergence time, a pinning controller which only controls partial nodes directly rather than the entire nodes is proposed based on fixed-time control theory. Then, by Lyapunov function approach and some inequalities techniques, fixed-time common synchronization criterion is established. Second, further to realize the self-regulation function of pinning controller, an adaptive pinning controller which can adjust automatically the control gains is developed, the desired fixed-time adaptive synchronization is achieved for the considered system, and the corresponding criterion is also derived. Finally, the availability of these results is tested by simulation example.},
  archive      = {J_TNNLS},
  author       = {Ziye Zhang and Xiaofeng Wei and Shuzhan Wang and Chong Lin and Jian Chen},
  doi          = {10.1109/TNNLS.2022.3189625},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2276-2289},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fixed-time pinning common synchronization and adaptive synchronization for delayed quaternion-valued neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autoencoder in autoencoder networks. <em>TNNLS</em>,
<em>35</em>(2), 2263–2275. (<a
href="https://doi.org/10.1109/TNNLS.2022.3189239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling complex correlations on multiview data is still challenging, especially for high-dimensional features with possible noise. To address this issue, we propose a novel unsupervised multiview representation learning (UMRL) algorithm, termed autoencoder in autoencoder networks (AE2-Nets). The proposed framework effectively encodes information from high-dimensional heterogeneous data into a compact and informative representation with the proposed bidirectional encoding strategy. Specifically, the proposed AE2-Nets conduct encoding in two directions: the inner-AE-networks extract view-specific intrinsic information (forward encoding), while the outer-AE-networks integrate this view-specific intrinsic information from different views into a latent representation (backward encoding). For the nested architecture, we further provide a probabilistic explanation and extension from hierarchical variational autoencoder. The forward–backward strategy flexibly addresses high-dimensional (noisy) features within each view and encodes complementarity across multiple views in a unified framework. Extensive results on benchmark datasets validate the advantages compared to the state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Changqing Zhang and Yu Geng and Zongbo Han and Yeqing Liu and Huazhu Fu and Qinghua Hu},
  doi          = {10.1109/TNNLS.2022.3189239},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2263-2275},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Autoencoder in autoencoder networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Markov subsampling based on huber criterion. <em>TNNLS</em>,
<em>35</em>(2), 2250–2262. (<a
href="https://doi.org/10.1109/TNNLS.2022.3189069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling is an important technique to tackle the computational challenges brought by big data. Many subsampling procedures fall within the framework of importance sampling, which assigns high sampling probabilities to the samples appearing to have big impacts. When the noise level is high, those sampling procedures tend to pick many outliers and thus often do not perform satisfactorily in practice. To tackle this issue, we design a new Markov subsampling strategy based on Huber criterion (HMS) to construct an informative subset from the noisy full data; the constructed subset then serves as refined working data for efficient processing. HMS is built upon a Metropolis–Hasting procedure, where the inclusion probability of each sampling unit is determined using the Huber criterion to prevent over scoring the outliers. Under mild conditions, we show that the estimator based on the subsamples selected by HMS is statistically consistent with a sub-Gaussian deviation bound. The promising performance of HMS is demonstrated by extensive studies on large-scale simulations and real data examples.},
  archive      = {J_TNNLS},
  author       = {Tieliang Gong and Yuxin Dong and Hong Chen and Bo Dong and Chen Li},
  doi          = {10.1109/TNNLS.2022.3189069},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2250-2262},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Markov subsampling based on huber criterion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatio-spectral fusion method for hyperspectral images
using residual hyper-dense network. <em>TNNLS</em>, <em>35</em>(2),
2235–2249. (<a
href="https://doi.org/10.1109/TNNLS.2022.3189049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-spectral fusion of panchromatic (PAN) and hyperspectral (HS) images is of great importance in improving spatial resolution of images acquired by many commercial HS sensors. DenseNets have recently achieved great success for image super-resolution because they facilitate gradient flow by concatenating all the feature outputs in a feedforward manner. In this article, we propose a residual hyper-dense network (RHDN) that extends the DenseNet to solve the spatio-spectral fusion problem. The overall structure of the proposed RHDN method is a two-branch network, which allows the network to capture the features of HS images within and outside the visible range separately. At each branch of the network, a two-stream strategy of feature extraction is designed to process PAN and HS images individually. A convolutional neural network (CNN) with cascade residual hyper-dense blocks (RHDBs), which allows direct connections between the pairs of layers within the same stream and those across different streams, is proposed to learn more complex combinations between the HS and PAN images. The residual learning is adopted to make the network efficient. Extensive benchmark evaluations well demonstrate that the proposed RHDN fusion method yields significant improvements over many widely accepted state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Jiahui Qu and Zhangchun Xu and Wenqian Dong and Song Xiao and Yunsong Li and Qian Du},
  doi          = {10.1109/TNNLS.2022.3189049},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2235-2249},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A spatio-spectral fusion method for hyperspectral images using residual hyper-dense network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSFG: Stochastically scaling features and gradients for
regularizing graph convolutional networks. <em>TNNLS</em>,
<em>35</em>(2), 2223–2234. (<a
href="https://doi.org/10.1109/TNNLS.2022.3188888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have been successfully applied in various graph-based tasks. In a typical graph convolutional layer, node features are updated by aggregating neighborhood information. Repeatedly applying graph convolutions can cause the oversmoothing issue, i.e., node features at deep layers converge to similar values. Previous studies have suggested that oversmoothing is one of the major issues that restrict the performance of GCNs. In this article, we propose a stochastic regularization method to tackle the oversmoothing problem. In the proposed method, we stochastically scale features and gradients (SSFG) by a factor sampled from a probability distribution in the training procedure. By explicitly applying a scaling factor to break feature convergence, the oversmoothing issue is alleviated. We show that applying stochastic scaling at the gradient level is complementary to that applied at the feature level to improve the overall performance. Our method does not increase the number of trainable parameters. When used together with ReLU, our SSFG can be seen as a stochastic ReLU activation function. We experimentally validate our SSFG regularization method on three commonly used types of graph networks. Extensive experimental results on seven benchmark datasets for four graph-based tasks demonstrate that our SSFG regularization is effective in improving the overall performance of the baseline graph networks. The code is available at https://github.com/vailatuts/SSFG-regularization .},
  archive      = {J_TNNLS},
  author       = {Haimin Zhang and Min Xu and Guoqiang Zhang and Kenta Niwa},
  doi          = {10.1109/TNNLS.2022.3188888},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2223-2234},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SSFG: Stochastically scaling features and gradients for regularizing graph convolutional networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When sparse neural network meets label noise learning: A
multistage learning framework. <em>TNNLS</em>, <em>35</em>(2),
2208–2222. (<a
href="https://doi.org/10.1109/TNNLS.2022.3188799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent methods in network pruning have indicated that a dense neural network involves a sparse subnetwork (called a winning ticket), which can achieve similar test accuracy to its dense counterpart with much fewer network parameters. Generally, these methods search for the winning tickets on well-labeled data. Unfortunately, in many real-world applications, the training data are unavoidably contaminated with noisy labels, thereby leading to performance deterioration of these methods. To address the above-mentioned problem, we propose a novel two-stream sample selection network (TS3-Net), which consists of a sparse subnetwork and a dense subnetwork, to effectively identify the winning ticket with noisy labels. The training of TS3-Net contains an iterative procedure that switches between training both subnetworks and pruning the smallest magnitude weights of the sparse subnetwork. In particular, we develop a multistage learning framework including a warm-up stage, a semisupervised alternate learning stage, and a label refinement stage, to progressively train the two subnetworks. In this way, the classification capability of the sparse subnetwork can be gradually improved at a high sparsity level. Extensive experimental results on both synthetic and real-world noisy datasets (including MNIST, CIFAR-10, CIFAR-100, ANIMAL-10N, Clothing1M, and WebVision) demonstrate that our proposed method achieves state-of-the-art performance with very small memory consumption for label noise learning. Code is available at https://github.com/Runqing-forMost/TS3-Net/tree/master .},
  archive      = {J_TNNLS},
  author       = {Runqing Jiang and Yan Yan and Jing-Hao Xue and Biao Wang and Hanzi Wang},
  doi          = {10.1109/TNNLS.2022.3188799},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2208-2222},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {When sparse neural network meets label noise learning: A multistage learning framework},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relation-aggregated cross-graph correlation learning for
fine-grained image–text retrieval. <em>TNNLS</em>, <em>35</em>(2),
2194–2207. (<a
href="https://doi.org/10.1109/TNNLS.2022.3188569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained image–text retrieval has been a hot research topic to bridge the vision and languages, and its main challenge is how to learn the semantic correspondence across different modalities. The existing methods mainly focus on learning the global semantic correspondence or intramodal relation correspondence in separate data representations, but which rarely consider the intermodal relation that interactively provide complementary hints for fine-grained semantic correlation learning. To address this issue, we propose a relation-aggregated cross-graph (RACG) model to explicitly learn the fine-grained semantic correspondence by aggregating both intramodal and intermodal relations, which can be well utilized to guide the feature correspondence learning process. More specifically, we first build semantic-embedded graph to explore both fine-grained objects and their relations of different media types, which aim not only to characterize the object appearance in each modality, but also to capture the intrinsic relation information to differentiate intramodal discrepancies. Then, a cross-graph relation encoder is newly designed to explore the intermodal relation across different modalities, which can mutually boost the cross-modal correlations to learn more precise intermodal dependencies. Besides, the feature reconstruction module and multihead similarity alignment are efficiently leveraged to optimize the node-level semantic correspondence, whereby the relation-aggregated cross-modal embeddings between image and text are discriminatively obtained to benefit various image–text retrieval tasks with high retrieval performance. Extensive experiments evaluated on benchmark datasets quantitatively and qualitatively verify the advantages of the proposed framework for fine-grained image–text retrieval and show its competitive performance with the state of the arts.},
  archive      = {J_TNNLS},
  author       = {Shu-Juan Peng and Yi He and Xin Liu and Yiu-ming Cheung and Xing Xu and Zhen Cui},
  doi          = {10.1109/TNNLS.2022.3188569},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2194-2207},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Relation-aggregated cross-graph correlation learning for fine-grained Image–Text retrieval},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tchebichef transform domain-based deep learning architecture
for image super-resolution. <em>TNNLS</em>, <em>35</em>(2), 2182–2193.
(<a href="https://doi.org/10.1109/TNNLS.2022.3188452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in the area of artificial intelligence and deep learning have motivated researchers to apply this knowledge to solve multipurpose applications in the area of computer vision and image processing. Super-resolution (SR), in the past few years, has produced remarkable results using deep learning methods. The ability of deep learning methods to learn the nonlinear mapping from low-resolution (LR) images to their corresponding high-resolution (HR) images leads to compelling results for SR in diverse areas of research. In this article, we propose a deep learning-based image SR architecture in the Tchebichef transform domain. This is achieved by integrating a transform layer into the proposed architecture through a customized Tchebichef convolutional layer (TCL). The role of TCL is to convert the LR image from the spatial domain to the orthogonal transform domain using Tchebichef basis functions. The inversion of the transform mentioned earlier is achieved using another layer known as the inverse TCL (ITCL), which converts back the LR images from the transform domain to the spatial domain. It has been observed that using the Tchebichef transform domain for the task of SR takes the advantage of high and low- frequency representation of images that makes the task of SR simplified. Furthermore, a transfer learning-based approach is adopted to enhance the quality of images by considering Covid19 medical images as an additional experiment. It is shown that our architecture enhances the quality of X-ray and CT images of COVID-19, providing a better image quality that may help in clinical diagnosis. Experimental results obtained using the proposed Tchebichef transform domain SR (TTDSR) architecture provides competitive results when compared with most of the deep learning methods employed using a fewer number of trainable parameters.},
  archive      = {J_TNNLS},
  author       = {Ahlad Kumar and Harsh Vardhan Singh and Vijeta Khare},
  doi          = {10.1109/TNNLS.2022.3188452},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2182-2193},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tchebichef transform domain-based deep learning architecture for image super-resolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Snippet policy network v2: Knee-guided neuroevolution for
multi-lead ECG early classification. <em>TNNLS</em>, <em>35</em>(2),
2167–2181. (<a
href="https://doi.org/10.1109/TNNLS.2022.3187741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early time series classification predicts the class label of a given time series before it is completely observed. In time-critical applications, such as arrhythmia monitoring in ICU, early treatment contributes to the patient’s fast recovery, and early warning could even save lives. Hence, in these cases, it is worthy of trading, to some extent, classification accuracy in favor of earlier decisions when the time series data are collected over time. In this article, we propose a novel deep reinforcement learning-based framework, snippet policy network V2 (SPN-V2), for long and varied-length multi-lead electrocardiogram (ECG) early classification. The proposed SNP-V2 contains two main components: snippet representation learning (SRL) and early classification timing learning (ECTL). The SRL is proposed to encode inner-snippet spatial correlations and inter-snippet temporal correlations into the hidden representations of the subsegment (snippet) of the input ECG. ECTL aims to learn a decision agent to classify the time series early and accurately. To optimize the proposed framework, we design a novel knee-guided neuroevolution algorithm (KGNA) to solve cardiovascular diseases’ early classification problem, automatically optimizing the proposed SPN-V2 regarding the tradeoff between accuracy and earliness. In addition, we conduct a series of experiments on two real-world ECG datasets. The experimental results show the superiority of the proposed algorithm over the state-of-the-art competing methods.},
  archive      = {J_TNNLS},
  author       = {Yu Huang and Gary G. Yen and Vincent S. Tseng},
  doi          = {10.1109/TNNLS.2022.3187741},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2167-2181},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Snippet policy network v2: Knee-guided neuroevolution for multi-lead ECG early classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A formal characterization of activation functions in deep
neural networks. <em>TNNLS</em>, <em>35</em>(2), 2153–2166. (<a
href="https://doi.org/10.1109/TNNLS.2022.3187538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a mathematical formulation for describing and designing activation functions in deep neural networks is provided. The methodology is based on a precise characterization of the desired activation functions that satisfy particular criteria, including circumventing vanishing or exploding gradients during training. The problem of finding desired activation functions is formulated as an infinite-dimensional optimization problem, which is later relaxed to solving a partial differential equation. Furthermore, bounds that guarantee the optimality of the designed activation function are provided. Relevant examples with some state-of-the-art activation functions are provided to illustrate the methodology.},
  archive      = {J_TNNLS},
  author       = {Massi Amrouche and Dušan M. Stipanović},
  doi          = {10.1109/TNNLS.2022.3187538},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2153-2166},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A formal characterization of activation functions in deep neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning fair representations via distance correlation
minimization. <em>TNNLS</em>, <em>35</em>(2), 2139–2152. (<a
href="https://doi.org/10.1109/TNNLS.2022.3187165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As machine learning algorithms are increasingly deployed for high-impact automated decision-making, the presence of bias (in datasets or tasks) gradually becomes one of the most critical challenges in machine learning applications. Such challenges range from the bias of race in face recognition to the bias of gender in hiring systems, where race and gender can be denoted as sensitive attributes. In recent years, much progress has been made in ensuring fairness and reducing bias in standard machine learning settings. Among them, learning fair representations with respect to the sensitive attributes has attracted increasing attention due to its flexibility in learning the rich representations based on advances in deep learning. In this article, we propose graph-fair, an algorithmic approach to learning fair representations under the graph Laplacian regularization, which reduces the separation between groups and the clustering within a group by encoding the sensitive attribute information into the graph. We have theoretically proved the underlying connection between graph regularization and distance correlation and show that the latter can be regarded as a standardized version of the former, with an additional advantage of being scale-invariant. Therefore, we naturally adopt the distance correlation as the fairness constraint to decrease the dependence between sensitive attributes and latent representations, called dist-fair. In contrast to existing approaches using measures of dependency and adversarial generators, both graph-fair and dist-fair provide simple fairness constraints, which eliminate the need for parameter tuning (e.g., choosing kernels) and introducing adversarial networks. Experiments conducted on real-world corpora indicate that our proposed fairness constraints applied for representation learning can provide better tradeoffs between fairness and utility results than existing approaches.},
  archive      = {J_TNNLS},
  author       = {Dandan Guo and Chaojie Wang and Baoxiang Wang and Hongyuan Zha},
  doi          = {10.1109/TNNLS.2022.3187165},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2139-2152},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning fair representations via distance correlation minimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compressing features for learning with noisy labels.
<em>TNNLS</em>, <em>35</em>(2), 2124–2138. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised learning can be viewed as distilling relevant information from input data into feature representations. This process becomes difficult when supervision is noisy as the distilled information might not be relevant. In fact, recent research shows that networks can easily overfit all labels including those that are corrupted, and hence can hardly generalize to clean datasets. In this article, we focus on the problem of learning with noisy labels and introduce compression inductive bias to network architectures to alleviate this overfitting problem. More precisely, we revisit one classical regularization named Dropout and its variant Nested Dropout. Dropout can serve as a compression constraint for its feature dropping mechanism, while Nested Dropout further learns ordered feature representations with respect to feature importance. Moreover, the trained models with compression regularization are further combined with co-teaching for performance boost. Theoretically, we conduct bias variance decomposition of the objective function under compression regularization. We analyze it for both single model and co-teaching. This decomposition provides three insights: 1) it shows that overfitting is indeed an issue in learning with noisy labels; 2) through an information bottleneck formulation, it explains why the proposed feature compression helps in combating label noise; and 3) it gives explanations on the performance boost brought by incorporating compression regularization into co-teaching. Experiments show that our simple approach can have comparable or even better performance than the state-of-the-art methods on benchmarks with real-world label noise including Clothing1M and ANIMAL-10N. Our implementation is available at https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/ .},
  archive      = {J_TNNLS},
  author       = {Yingyi Chen and Shell Xu Hu and Xi Shen and Chunrong Ai and Johan A. K. Suykens},
  doi          = {10.1109/TNNLS.2022.3186930},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2124-2138},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Compressing features for learning with noisy labels},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal output discrepancy for loss estimation-based active
learning. <em>TNNLS</em>, <em>35</em>(2), 2109–2123. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep learning succeeds in a wide range of tasks, it highly depends on the massive collection of annotated data which is expensive and time-consuming. To lower the cost of data annotation, active learning has been proposed to interactively query an oracle to annotate a small proportion of informative samples in an unlabeled dataset. Inspired by the fact that the samples with higher loss are usually more informative to the model than the samples with lower loss, in this article we present a novel deep active learning approach that queries the oracle for data annotation when the unlabeled sample is believed to incorporate high loss. The core of our approach is a measurement temporal output discrepancy (TOD) that estimates the sample loss by evaluating the discrepancy of outputs given by models at different optimization steps. Our theoretical investigation shows that TOD lower-bounds the accumulated sample loss thus it can be used to select informative unlabeled samples. On basis of TOD, we further develop an effective unlabeled data sampling strategy as well as an unsupervised learning criterion for active learning. Due to the simplicity of TOD, our methods are efficient, flexible, and task-agnostic. Extensive experimental results demonstrate that our approach achieves superior performances than the state-of-the-art active learning methods on image classification and semantic segmentation tasks. In addition, we show that TOD can be utilized to select the best model of potentially the highest testing accuracy from a pool of candidate models.},
  archive      = {J_TNNLS},
  author       = {Siyu Huang and Tianyang Wang and Haoyi Xiong and Bihan Wen and Jun Huan and Dejing Dou},
  doi          = {10.1109/TNNLS.2022.3186855},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2109-2123},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Temporal output discrepancy for loss estimation-based active learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge distillation using hierarchical self-supervision
augmented distribution. <em>TNNLS</em>, <em>35</em>(2), 2094–2108. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) is an effective framework that aims to transfer meaningful information from a large teacher to a smaller student. Generally, KD often involves how to define and transfer knowledge. Previous KD methods often focus on mining various forms of knowledge, for example, feature maps and refined information. However, the knowledge is derived from the primary supervised task, and thus, is highly task-specific. Motivated by the recent success of self-supervised representation learning, we propose an auxiliary self-supervision augmented task to guide networks to learn more meaningful features. Therefore, we can derive soft self-supervision augmented distributions as richer dark knowledge from this task for KD. Unlike previous knowledge, this distribution encodes joint knowledge from supervised and self-supervised feature learning. Beyond knowledge exploration, we propose to append several auxiliary branches at various hidden layers, to fully take advantage of hierarchical feature maps. Each auxiliary branch is guided to learn self-supervision augmented tasks and distill this distribution from teacher to student. Overall, we call our KD method a hierarchical self-supervision augmented KD (HSSAKD). Experiments on standard image classification show that both offline and online HSSAKD achieves state-of-the-art performance in the field of KD. Further transfer experiments on object detection further verify that HSSAKD can guide the network to learn better features. The code is available at https://github.com/winycg/HSAKD .},
  archive      = {J_TNNLS},
  author       = {Chuanguang Yang and Zhulin An and Linhang Cai and Yongjun Xu},
  doi          = {10.1109/TNNLS.2022.3186807},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2094-2108},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge distillation using hierarchical self-supervision augmented distribution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-knowledge-driven self-organizing fuzzy neural network.
<em>TNNLS</em>, <em>35</em>(2), 2081–2093. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy neural networks (FNNs) hold the advantages of knowledge leveraging and adaptive learning, which have been widely used in nonlinear system modeling. However, it is difficult for FNNs to obtain the appropriate structure in the situation of insufficient data, which limits its generalization performance. To solve this problem, a data-knowledge-driven self-organizing FNN (DK-SOFNN) with a structure compensation strategy and a parameter reinforcement mechanism is proposed in this article. First, a structure compensation strategy is proposed to mine structural information from empirical knowledge to learn the structure of DK-SOFNN. Then, a complete model structure can be acquired by sufficient structural information. Second, a parameter reinforcement mechanism is developed to determine the parameter evolution direction of DK-SOFNN that is most suitable for the current model structure. Then, a robust model can be obtained by the interaction between parameters and dynamic structure. Finally, the proposed DK-SOFNN is theoretically analyzed on the fixed structure case and dynamic structure case. Then, the convergence conditions can be obtained to guide practical applications. The merits of DK-SOFNN are demonstrated by some benchmark problems and industrial applications.},
  archive      = {J_TNNLS},
  author       = {Honggui Han and Hongxu Liu and Junfei Qiao},
  doi          = {10.1109/TNNLS.2022.3186671},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2081-2093},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-knowledge-driven self-organizing fuzzy neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Barrier lyapunov function-based safe reinforcement learning
for autonomous vehicles with optimized backstepping. <em>TNNLS</em>,
<em>35</em>(2), 2066–2080. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time Hamilton–Jacobi–Bellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.},
  archive      = {J_TNNLS},
  author       = {Yuxiang Zhang and Xiaoling Liang and Dongyu Li and Shuzhi Sam Ge and Bingzhao Gao and Hong Chen and Tong Heng Lee},
  doi          = {10.1109/TNNLS.2022.3186528},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2066-2080},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Barrier lyapunov function-based safe reinforcement learning for autonomous vehicles with optimized backstepping},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpolation-based contrastive learning for few-label
semi-supervised learning. <em>TNNLS</em>, <em>35</em>(2), 2054–2065. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) has long been proved to be an effective technique to construct powerful models with limited labels. In the existing literature, consistency regularization-based methods, which force the perturbed samples to have similar predictions with the original ones have attracted much attention for their promising accuracy. However, we observe that the performance of such methods decreases drastically when the labels get extremely limited, e.g., 2 or 3 labels for each category. Our empirical study finds that the main problem lies with the drift of semantic information in the procedure of data augmentation. The problem can be alleviated when enough supervision is provided. However, when little guidance is available, the incorrect regularization would mislead the network and undermine the performance of the algorithm. To tackle the problem, we: 1) propose an interpolation-based method to construct more reliable positive sample pairs and 2) design a novel contrastive loss to guide the embedding of the learned network to change linearly between samples so as to improve the discriminative capability of the network by enlarging the margin decision boundaries. Since no destructive regularization is introduced, the performance of our proposed algorithm is largely improved. Specifically, the proposed algorithm outperforms the second best algorithm (Comatch) with 5.3% by achieving 88.73% classification accuracy when only two labels are available for each class on the CIFAR-10 dataset. Moreover, we further prove the generality of the proposed method by improving the performance of the existing state-of-the-art algorithms considerably with our proposed strategy. The corresponding code is available at https://github.com/xihongyang1999/ICL_SSL .},
  archive      = {J_TNNLS},
  author       = {Xihong Yang and Xiaochang Hu and Sihang Zhou and Xinwang Liu and En Zhu},
  doi          = {10.1109/TNNLS.2022.3186512},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2054-2065},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interpolation-based contrastive learning for few-label semi-supervised learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guidance through surrogate: Toward a generic diagnostic
attack. <em>TNNLS</em>, <em>35</em>(2), 2042–2053. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training (AT) is an effective approach to making deep neural networks robust against adversarial attacks. Recently, different AT defenses are proposed that not only maintain a high clean accuracy but also show significant robustness against popular and well-studied adversarial attacks, such as projected gradient descent (PGD). High adversarial robustness can also arise if an attack fails to find adversarial gradient directions, a phenomenon known as “gradient masking.” In this work, we analyze the effect of label smoothing on AT as one of the potential causes of gradient masking. We then develop a guided mechanism to avoid local minima during attack optimization, leading to a novel attack dubbed guided projected gradient attack (G-PGA). Our attack approach is based on a “match and deceive” loss that finds optimal adversarial directions through guidance from a surrogate model. Our modified attack does not require random restarts a large number of attack iterations or a search for optimal step size. Furthermore, our proposed G-PGA is generic, thus it can be combined with an ensemble attack strategy as we demonstrate in the case of auto-attack, leading to efficiency and convergence speed improvements. More than an effective attack, G-PGA can be used as a diagnostic tool to reveal elusive robustness due to gradient masking in adversarial defenses.},
  archive      = {J_TNNLS},
  author       = {Muzammal Naseer and Salman Khan and Fatih Porikli and Fahad Shahbaz Khan},
  doi          = {10.1109/TNNLS.2022.3186278},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2042-2053},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guidance through surrogate: Toward a generic diagnostic attack},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven inverse reinforcement learning control for
linear multiplayer games. <em>TNNLS</em>, <em>35</em>(2), 2028–2041. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a data-driven inverse reinforcement learning (RL) control algorithm for nonzero-sum multiplayer games in linear continuous-time differential dynamical systems. The inverse RL problem in the games is solved by a learner reconstructing the unknown expert players’ cost functions from demonstrated expert’s optimal state and control input trajectories. The learner, thus, obtains the same control feedback gains and trajectories as the expert, only using data along system trajectories without knowing system dynamics. This article first proposes a model-based inverse RL policy iteration framework that has: 1) policy evaluation step for reconstructing cost matrices using Lyapunov functions; 2) state-reward weight improvement step using inverse optimal control (IOC); and 3) policy improvement step using optimal control. Based on the model-based policy iteration algorithm, this article further develops an online data-driven off-policy inverse RL algorithm without knowing any knowledge of system dynamics or expert control gains. Rigorous convergence and stability analysis of the algorithms are provided. It shows that the off-policy inverse RL algorithm guarantees unbiased solutions while probing noises are added to satisfy the persistence of excitation (PE) condition. Finally, two different simulation examples validate the effectiveness of the proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Bosen Lian and Vrushabh S. Donge and Frank L. Lewis and Tianyou Chai and Ali Davoudi},
  doi          = {10.1109/TNNLS.2022.3186229},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2028-2041},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven inverse reinforcement learning control for linear multiplayer games},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised feature selection with flexible optimal graph.
<em>TNNLS</em>, <em>35</em>(2), 2014–2027. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the unsupervised feature selection method based on spectral analysis, constructing a similarity matrix is a very important part. In existing methods, the linear low-dimensional projection used in the process of constructing the similarity matrix is too hard, it is very challenging to construct a reliable similarity matrix. To this end, we propose a method to construct a flexible optimal graph. Based on this, we propose an unsupervised feature selection method named unsupervised feature selection with flexible optimal graph and $\ell _{2,1}$ -norm regularization (FOG-R). Unlike other methods that use linear projection to approximate the low-dimensional manifold of the original data when constructing a similarity matrix, FOG-R can learn a flexible optimal graph, and by combining flexible optimal graph learning and feature selection into a unified framework to get an adaptive similarity matrix. In addition, an iterative algorithm with a strict convergence proof is proposed to solve FOG-R. $\ell _{2,1}$ -norm regularization will introduce an additional regularization parameter, which will cause parameter-tuning trouble. Therefore, we propose another unsupervised feature selection method, that is, unsupervised feature selection with a flexible optimal graph and $\ell _{2,0}$ -norm constraint (FOG-C), which can avoid tuning additional parameters and obtain a more sparse projection matrix. Most critically, we propose an effective iterative algorithm that can solve FOG-C globally with strict convergence proof. Comparative experiments conducted on 12 public datasets show that FOG-R and FOG-C perform better than the other nine state-of-the-art unsupervised feature selection algorithms.},
  archive      = {J_TNNLS},
  author       = {Hong Chen and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3186171},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2014-2027},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised feature selection with flexible optimal graph},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TraverseNet: Unifying space and time in message passing for
traffic forecasting. <em>TNNLS</em>, <em>35</em>(2), 2003–2013. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to unify spatial dependency and temporal dependency in a non-Euclidean space while capturing the inner spatial–temporal dependencies for traffic data. For spatial–temporal attribute entities with topological structure, the space–time is consecutive and unified while each node’s current status is influenced by its neighbors’ past states over variant periods of each neighbor. Most spatial–temporal neural networks for traffic forecasting study spatial dependency and temporal correlation separately in processing, gravely impaired the spatial–temporal integrity, and ignore the fact that the neighbors’ temporal dependency period for a node can be delayed and dynamic. To model this actual condition, we propose TraverseNet, a novel spatial–temporal graph neural network, viewing space and time as an inseparable whole, to mine spatial–temporal graphs while exploiting the evolving spatial–temporal dependencies for each node via message traverse mechanisms. Experiments with ablation and parameter studies have validated the effectiveness of the proposed TraverseNet, and the detailed implementation can be found from https://github.com/nnzhan/TraverseNet .},
  archive      = {J_TNNLS},
  author       = {Zonghan Wu and Da Zheng and Shirui Pan and Quan Gan and Guodong Long and George Karypis},
  doi          = {10.1109/TNNLS.2022.3186103},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {2003-2013},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TraverseNet: Unifying space and time in message passing for traffic forecasting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bad and good errors: Value-weighted skill scores in deep
ensemble learning. <em>TNNLS</em>, <em>35</em>(2), 1993–2002. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecast verification is a crucial task for assessing the predictive power of prognostic model forecasts and it is usually implemented by checking quality-based skill scores. In this article, we propose a novel approach to realize forecast verification focusing not just on the forecast quality but rather on its value. Specifically, we introduce a strategy for assessing the severity of forecast errors based on the evidence that, on the one hand, a false alarm just anticipating an occurring event is better than one in the middle of consecutive nonoccurring events, and that, on the other hand, a miss of an isolated event has a worse impact than a miss of a single event, which is part of several consecutive occurrences. Relying on this idea, we introduce a notion of value-weighted skill scores giving greater importance to the value of the prediction rather than to its quality. Then, we introduce an ensemble strategy to maximize quality-based and value-weighted skill scores independently of one another. We test it on the predictions provided by deep learning methods for binary classification in the case of four applications concerned with pollution, space weather, stock price, and IoT data stream forecasting. Our experimental studies show that using the ensemble strategy for maximizing the value-weighted skill scores generally improves both the value and quality of the forecast.},
  archive      = {J_TNNLS},
  author       = {Sabrina Guastavino and Michele Piana and Federico Benvenuto},
  doi          = {10.1109/TNNLS.2022.3186068},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1993-2002},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bad and good errors: Value-weighted skill scores in deep ensemble learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quality evaluation of triples in knowledge graph by
incorporating internal with external consistency. <em>TNNLS</em>,
<em>35</em>(2), 1980–1992. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation of knowledge quality (KQ) in multisource knowledge graphs (KGs) is an essential step for many applications, such as fragmented knowledge fusion and knowledge base construction. Many existing quality evaluation methods for multisource knowledge are based on validation from high-quality knowledge bases or statistical analysis of knowledge related to a specific fact from multiple sources, named external consistency (EC)-based methods. However, high-quality KGs are difficult to obtain, and there might exist incorrect knowledge in multisource KGs interfering with KQ evaluation. To address the issue, this article refers to the internal structure of a KG to evaluate the degree to which the contained triples conform to the overall semantic pattern of the KG, such as KG embedding and logic inference-based approaches, defined as internal consistency (IC) evaluation. The IC is integrated with the EC to identify possible incorrect triples and reduce their influences on the KQ evaluation, thus alleviating the interference of incorrect knowledge. The proposed method is verified with multiple datasets, and the results demonstrate that the proposed method could significantly reduce wrong evaluations caused by incorrect knowledge and effectively improve the quality evaluation of triples.},
  archive      = {J_TNNLS},
  author       = {Taiyu Ban and Xiangyu Wang and Lyuzhou Chen and Xingyu Wu and Qiuju Chen and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2022.3186033},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1980-1992},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quality evaluation of triples in knowledge graph by incorporating internal with external consistency},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient neural network compression inspired by compressive
sensing. <em>TNNLS</em>, <em>35</em>(2), 1965–1979. (<a
href="https://doi.org/10.1109/TNNLS.2022.3186008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional neural network compression (NNC) methods decrease the model size and floating-point operations (FLOPs) in the manner of screening out unimportant weight parameters; however, the intrinsic sparsity characteristics have not been fully exploited. In this article, from the perspective of signal processing and analysis for network parameters, we propose to use a compressive sensing (CS)-based method, namely NNCS, for performance improvements. Our proposed NNCS is inspired by the discovery that sparsity levels of weight parameters in the transform domain are greater than those in the original domain. First, to achieve sparse representations for parameters in the transform domain during training, we incorporate a constrained CS model into loss function. Second, the proposed effective training process consists of two steps, where the first step trains raw weight parameters and induces and reconstructs their sparse representations and the second step trains transform coefficients to improve network performances. Finally, we transform the entire neural network into another new domain-based representation, and a sparser parameter distribution can be obtained to facilitate inference acceleration. Experimental results demonstrate that NNCS can significantly outperform the other existing state-of-the-art methods in terms of parameter reductions and FLOPs. With VGGNet on CIFAR-10, we decrease 94.8% parameters and achieve a 76.8% reduction of FLOPs, with 0.13% drop in Top-1 accuracy. With ResNet-50 on ImageNet, we decrease 75.6% parameters and achieve a 78.9% reduction of FLOPs, with 1.24% drop in Top-1 accuracy.},
  archive      = {J_TNNLS},
  author       = {Wei Gao and Yang Guo and Siwei Ma and Ge Li and Sam Kwong},
  doi          = {10.1109/TNNLS.2022.3186008},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1965-1979},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient neural network compression inspired by compressive sensing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Altruistic collaborative learning. <em>TNNLS</em>,
<em>35</em>(2), 1954–1964. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new learning paradigm based on the concept of concordant gradients for ensemble learning strategies. In this paradigm, learners update their weights if and only if the gradients of their cost functions are mutually concordant in a sense given by paper. The objective of the proposed concordant optimization framework is robustness against uncertainties by postponing to a later epoch, the consideration of examples associated with discordant directions during a training phase. Concordance constrained collaboration is shown to be relevant, especially in intricate classification issues where exclusive class labeling involves information bias due to correlated disturbances affecting almost all training examples. The first learning paradigm applies on a gradient descent strategy based on allied agents, subjected to concordance checking before moving forward in training epochs. The second learning paradigm is related to multivariate dense neural matrix fusion, where the fusion operator is itself a learnable neural operator. In addition to these paradigms, this article proposes a new categorical probability transform to enrich the existing collection and propose an alternative scenario for integrating penalized SoftMax information. Finally, this article assesses the relevance of the above contributions with respect to several deep learning frameworks and a collaborative classification involving dependent classes.},
  archive      = {J_TNNLS},
  author       = {Abdourrahmane Mahamane Atto},
  doi          = {10.1109/TNNLS.2022.3185961},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1954-1964},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Altruistic collaborative learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label contrastive learning for abstract visual
reasoning. <em>TNNLS</em>, <em>35</em>(2), 1941–1953. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a long time, the ability to solve abstract reasoning tasks was considered one of the hallmarks of human intelligence. Recent advances in the application of deep learning (DL) methods led to surpassing human abstract reasoning performance, specifically in the most popular type of such problems—Raven’s progressive matrices (RPMs). While the efficacy of DL systems is indeed impressive, the way they approach the RPMs is very different from that of humans. State-of-the-art systems solving RPMs rely on massive pattern-based training and sometimes on exploiting biases in the dataset, whereas humans concentrate on the identification of the rules/concepts underlying the RPM to be solved. Motivated by this cognitive difference, this work aims at combining DL with the human way of solving RPMs. Specifically, we cast the problem of solving RPMs into a multilabel classification framework where each RPM is viewed as a multilabel data point, with labels determined by the set of abstract rules underlying the RPM. For efficient training of the system, we present a generalization of the noise contrastive estimation algorithm to the case of multilabel samples and a new sparse rule encoding scheme for RPMs. The proposed approach is evaluated on the two most popular benchmark datasets [I-RAVEN and procedurally generated matrices (PGM)] and on both of them demonstrate an advantage over the state-of-the-art results.},
  archive      = {J_TNNLS},
  author       = {Mikołaj Małkiński and Jacek Mańdziuk},
  doi          = {10.1109/TNNLS.2022.3185949},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1941-1953},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-label contrastive learning for abstract visual reasoning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From clustering to cluster explanations via neural networks.
<em>TNNLS</em>, <em>35</em>(2), 1926–1940. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent trend in machine learning has been to enrich learned models with the ability to explain their own predictions. The emerging field of explainable AI (XAI) has so far mainly focused on supervised learning, in particular, deep neural network classifiers. In many practical problems, however, the label information is not given and the goal is instead to discover the underlying structure of the data, for example, its clusters. While powerful methods exist for extracting the cluster structure in data, they typically do not answer the question why a certain data point has been assigned to a given cluster. We propose a new framework that can, for the first time, explain cluster assignments in terms of input features in an efficient and reliable manner. It is based on the novel insight that clustering models can be rewritten as neural networks—or “neuralized.” Cluster predictions of the obtained networks can then be quickly and accurately attributed to the input features. Several showcases demonstrate the ability of our method to assess the quality of learned clusters and to extract novel insights from the analyzed data and representations.},
  archive      = {J_TNNLS},
  author       = {Jacob Kauffmann and Malte Esders and Lukas Ruff and Grégoire Montavon and Wojciech Samek and Klaus-Robert Müller},
  doi          = {10.1109/TNNLS.2022.3185901},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1926-1940},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {From clustering to cluster explanations via neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph information aggregation cross-domain few-shot learning
for hyperspectral image classification. <em>TNNLS</em>, <em>35</em>(2),
1912–1925. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most domain adaptation (DA) methods in cross-scene hyperspectral image classification focus on cases where source data (SD) and target data (TD) with the same classes are obtained by the same sensor. However, the classification performance is significantly reduced when there are new classes in TD. In addition, domain alignment, as one of the main approaches in DA, is carried out based on local spatial information, rarely taking into account nonlocal spatial information (nonlocal relationships) with strong correspondence. A graph information aggregation cross-domain few-shot learning (Gia-CFSL) framework is proposed, intending to make up for the above-mentioned shortcomings by combining FSL with domain alignment based on graph information aggregation. SD with all label samples and TD with a few label samples are implemented for FSL episodic training. Meanwhile, intradomain distribution extraction block (IDE-block) and cross-domain similarity aware block (CSA-block) are designed. The IDE-block is used to characterize and aggregate the intradomain nonlocal relationships and the interdomain feature and distribution similarities are captured in the CSA-block. Furthermore, feature-level and distribution-level cross-domain graph alignments are used to mitigate the impact of domain shift on FSL. Experimental results on three public HSI datasets demonstrate the superiority of the proposed method. The codes will be available from the website: https://github.com/YuxiangZhang-BIT/IEEE_TNNLS_Gia-CFSL .},
  archive      = {J_TNNLS},
  author       = {Yuxiang Zhang and Wei Li and Mengmeng Zhang and Shuai Wang and Ran Tao and Qian Du},
  doi          = {10.1109/TNNLS.2022.3185795},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1912-1925},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph information aggregation cross-domain few-shot learning for hyperspectral image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Significance tests of feature relevance for a black-box
learner. <em>TNNLS</em>, <em>35</em>(2), 1898–1911. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An exciting recent development is the uptake of deep neural networks in many scientific fields, where the main objective is outcome prediction with a black-box nature. Significance testing is promising to address the black-box issue and explore novel scientific insights and interpretations of the decision-making process based on a deep learning model. However, testing for a neural network poses a challenge because of its black-box nature and unknown limiting distributions of parameter estimates while existing methods require strong assumptions or excessive computation. In this article, we derive one-split and two-split tests relaxing the assumptions and computational complexity of existing black-box tests and extending to examine the significance of a collection of features of interest in a dataset of possibly a complex type, such as an image. The one-split test estimates and evaluates a black-box model based on estimation and inference subsets through sample splitting and data perturbation. The two-split test further splits the inference subset into two but requires no perturbation. Also, we develop their combined versions by aggregating the $p$ -values based on repeated sample splitting. By deflating the bias-sd-ratio, we establish asymptotic null distributions of the test statistics and the consistency in terms of Type II error. Numerically, we demonstrate the utility of the proposed tests on seven simulated examples and six real datasets. Accompanying this article is our python library dnn-inference ( https://dnn-inference.readthedocs.io/en/latest/ ) that implements the proposed tests.},
  archive      = {J_TNNLS},
  author       = {Ben Dai and Xiaotong Shen and Wei Pan},
  doi          = {10.1109/TNNLS.2022.3185742},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1898-1911},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Significance tests of feature relevance for a black-box learner},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confusion-based metric learning for regularizing zero-shot
image retrieval and clustering. <em>TNNLS</em>, <em>35</em>(2),
1884–1897. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning turns to be attractive in zero-shot image retrieval and clustering (ZSRC) task in which a good embedding/metric is requested such that the unseen classes can be distinguished well. Most existing works deem this “good” embedding just to be the discriminative one and race to devise the powerful metric objectives or the hard-sample mining strategies for learning discriminative deep metrics. However, in this article, we first emphasize that the generalization ability is also a core ingredient of this “good” metric and it largely affects the metric performance in zero-shot settings as a matter of fact. Then, we propose the confusion-based metric learning (CML) framework to explicitly optimize a robust metric. It is mainly achieved by introducing two interesting regularization terms, i.e., the energy confusion (EC) and diversity confusion (DC) terms. These terms daringly break away from the traditional deep metric learning idea of designing discriminative objectives and instead seek to “confuse” the learned model. These two confusion terms focus on local and global feature distribution confusions, respectively. We train these confusion terms together with the conventional deep metric objective in an adversarial manner. Although it seems weird to “confuse” the model learning, we show that our CML indeed serves as an efficient regularization framework for deep metric learning and it is applicable to various conventional metric methods. This article empirically and experimentally demonstrates the importance of learning an embedding/metric with good generalization, achieving the state-of-the-art performances on the popular CUB, CARS, Stanford Online Products, and In-Shop datasets for ZSRC tasks.},
  archive      = {J_TNNLS},
  author       = {Binghui Chen and Weihong Deng and Biao Wang and Lei Zhang},
  doi          = {10.1109/TNNLS.2022.3185668},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1884-1897},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Confusion-based metric learning for regularizing zero-shot image retrieval and clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fixed-time stability of nonlinear impulsive systems and its
application to inertial neural networks. <em>TNNLS</em>, <em>35</em>(2),
1872–1883. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the fixed-time stability (FTS) problem of nonlinear impulsive systems (NISs). By means of the impulsive control mechanism and Lyapunov functions theory, several sufficient conditions are established to ensure the FTS of general NISs. Meanwhile, some novel impulse-dependent settling-time estimation schemes are developed, which fully considers the influence of stabilizing impulses and destabilizing impulses on the convergence rate of the system states. The proposed schemes establish a quantitative relationship between the upper bound of the settling time and impulse effects. It shows that stabilizing impulses can accelerate the convergence rate of the system states and leads to the upper bound of the settling time being smaller. Conversely, destabilizing impulses can reduce it and make the upper bound of the settling time larger. Then, the theoretical results are applied to delayed inertial neural networks (DINNs), where two kinds of controllers are designed to realize fixed-time synchronization of the considered systems in the impulse sense. Finally, some numerical examples are provided to illustrate the validity of the proposed theoretical results.},
  archive      = {J_TNNLS},
  author       = {Lanfeng Hua and Hong Zhu and Shouming Zhong and Yuping Zhang and Kaibo Shi and Oh-Min Kwon},
  doi          = {10.1109/TNNLS.2022.3185664},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1872-1883},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fixed-time stability of nonlinear impulsive systems and its application to inertial neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Self-supervised self-organizing clustering network: A novel
unsupervised representation learning method. <em>TNNLS</em>,
<em>35</em>(2), 1857–1871. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based clustering methods usually regard feature extraction and feature clustering as two independent steps. In this way, the features of all images need to be extracted before feature clustering, which consumes a lot of calculation. Inspired by the self-organizing map network, a self-supervised self-organizing clustering network ( $\text{S}^{3}$ OCNet) is proposed to jointly learn feature extraction and feature clustering, thus realizing a single-stage clustering method. In order to achieve joint learning, we propose a self-organizing clustering header (SOCH), which takes the weight of the self-organizing layer as the cluster centers, and the output of the self-organizing layer as the similarities between the feature and the cluster centers. In order to optimize our network, we first convert the similarities into probabilities which represents a soft cluster assignment, and then we obtain a target for self-supervised learning by transforming the soft cluster assignment into a hard cluster assignment, and finally we jointly optimize backbone and SOCH. By setting different feature dimensions, a Multilayer SOCHs strategy is further proposed by cascading SOCHs. This strategy achieves clustering features in multiple clustering spaces. $\text{S}^{3}$ OCNet is evaluated on widely used image classification benchmarks such as Canadian Institute For Advanced Research (CIFAR)-10, CIFAR-100, Self-Taught Learning (STL)-10, and Tiny ImageNet. Experimental results show that our method significant improvement over other related methods. The visualization of features and images shows that our method can achieve good clustering results.},
  archive      = {J_TNNLS},
  author       = {Shuo Li and Fang Liu and Licheng Jiao and Puhua Chen and Lingling Li},
  doi          = {10.1109/TNNLS.2022.3185638},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1857-1871},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised self-organizing clustering network: A novel unsupervised representation learning method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Mixed-delay-based augmented functional for sampled-data
synchronization of delayed neural networks with communication delay.
<em>TNNLS</em>, <em>35</em>(2), 1847–1856. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synchronization control for delayed neural networks (DNNs) via a sampled-data controller considering communication delay is studied by input delay approach. Although few scholars have put forward the coexistence of transmission delay and communication delay in this problem, no report has clarified the interaction between transmission delay and communication delay. Also, the time-squared terms are underutilized. Thus, a novel augmented Lyapunov functional, which consists of a mixed-delay-based augmented part and a time-squared two-sided looped part, is proposed to fill this gap. In the mixed-delay-based augmented part, not only the information of transmission delay and communication delay themselves, but also the interaction between those two delays is considered. Time-dependent quadratic terms as well as the sampling integral states are introduced in the two-sided looped part, so that more characteristic information of the sampling pattern is encompassed and the relationship of the states at the sampling instant is enhanced. Then, this novel augmented functional is applied to the synchronization control of DNNs. A less conservative synchronization criterion is obtained in the form of linear matrix inequalities. A numerical example illustrates the validity and superiority of the presented synchronization criterion.},
  archive      = {J_TNNLS},
  author       = {Ying Zhang and Yong He and Fei Long and Chuan-Ke Zhang},
  doi          = {10.1109/TNNLS.2022.3185617},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1847-1856},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mixed-delay-based augmented functional for sampled-data synchronization of delayed neural networks with communication delay},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The unified task assignment for underwater data collection
with multi-AUV system: A reinforced self-organizing mapping approach.
<em>TNNLS</em>, <em>35</em>(2), 1833–1846. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the task assignment problem for multiple autonomous underwater vehicles to efficiently collect underwater data from sensors. We formulate a unified framework to consistently address the heterogeneous task assignment problem (nonemergency and emergency cases) without strictly distinguishing the mixed cases. First, a unified problem, which bridges the gap between different constraints and optimization objectives of different cases, is constructed. Then, the proposed reinforced self-organizing mapping algorithm is reinforced in three aspects: the regional learning rate, the self-configuring neuron (SCN) strategy, and the workload balance mechanism. Specifically, the proposed regional learning rate comprehensively considers the individual worth of tasks and the topology to generate the regional learning rate of dynamic task regions, which consists of dynamic remaining tasks and the reconstructed topology. Based on this idea, the constructed unified problem can be solved consistently. Furthermore, the proposed SCN strategy optimizes the neuron population both in quality and quantity, and guides the update of neurons with enriched historical information to improve the mapping ability. This strategy greatly improves learning efficiency and applicability in a wide range of scenarios. Meanwhile, the proposed workload balance mechanism takes into consideration of both the work capability and consumed energy to extend the continuous working capability. The numerical results validate the effectiveness and adaptability of the proposed unified task assignment framework.},
  archive      = {J_TNNLS},
  author       = {Song Han and Tao Zhang and Xinbin Li and Junzhi Yu and Tongwei Zhang and Zhixin Liu},
  doi          = {10.1109/TNNLS.2022.3185611},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1833-1846},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The unified task assignment for underwater data collection with multi-AUV system: A reinforced self-organizing mapping approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matrix measure-based event-triggered impulsive
quasi-synchronization on coupled neural networks. <em>TNNLS</em>,
<em>35</em>(2), 1821–1832. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the quasi-synchronization for a kind of coupled neural networks with time-varying delays is investigated via a novel event-triggered impulsive control approach. In view of the randomly occurring uncertainties (ROUs) in the communication channels, the global quasi-synchronization for the coupled neural networks within a given error bound is considered instead of discussing the complete synchronization. A kind of distributed event-triggered impulsive controllers is presented with considering the Bernoulli stochastic variables based on ROUs, which works at each event-triggered impulsive instant. According to the matrix measure method and the Lyapunov stability theorem, several sufficient conditions for the realization of the quasi-synchronization are successfully derived. Combining with the mathematical methodology with the formula of variation of parameters and the comparison principle for the impulsive systems with time-varying delays, the convergence rate and the synchronization error bound are precisely estimated. Meanwhile, the Zeno behaviors could be eliminated in the coupled neural network with the proposed event-triggered function. Finally, a numerical example is presented to prove the results of theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Chenhui Jiang and Ze Tang and Ju H. Park and Jianwen Feng},
  doi          = {10.1109/TNNLS.2022.3185586},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1821-1832},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Matrix measure-based event-triggered impulsive quasi-synchronization on coupled neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Heat transfer-inspired network for image super-resolution
reconstruction. <em>TNNLS</em>, <em>35</em>(2), 1810–1820. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) is a critical image preprocessing task for many applications. How to recover features as accurately as possible is the focus of SR algorithms. Most existing SR methods tend to guide the image reconstruction process with gradient maps, frequency perception modules, etc. and improve the quality of recovered images from the perspective of enhancing edges, but rarely optimize the neural network structure from the system level. In this article, we conduct an in- depth exploration for the inner nature of the SR network structure. In light of the consistency between thermal particles in the thermal field and pixels in the image domain, we propose a novel heat-transfer-inspired network (HTI-Net) for image SR reconstruction based on the theoretical basis of heat transfer. With the finite difference theory, we use a second-order mixed-difference equation to redesign the residual network (ResNet), which can fully integrate multiple information to achieve better feature reuse. In addition, according to the thermal conduction differential equation (TCDE) in the thermal field, the pixel value flow equation (PVFE) in the image domain is derived to mine deep potential feature information. The experimental results on multiple standard databases demonstrate that the proposed HTI-Net has superior edge detail reconstruction effect and parameter performance compared with the existing SR methods. The experimental results on the microscope chip image (MCI) database consisting of realistic low-resolution (LR) and high-resolution (HR) images show that the proposed HTI-Net for image SR reconstruction can improve the effectiveness of the hardware Trojan detection system.},
  archive      = {J_TNNLS},
  author       = {Mingjin Zhang and Qianqian Wu and Jie Guo and Yunsong Li and Xinbo Gao},
  doi          = {10.1109/TNNLS.2022.3185529},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1810-1820},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heat transfer-inspired network for image super-resolution reconstruction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative regression with adaptive graph diffusion.
<em>TNNLS</em>, <em>35</em>(2), 1797–1809. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new linear regression (LR)-based multiclass classification method, called discriminative regression with adaptive graph diffusion (DRAGD). Different from existing graph embedding-based LR methods, DRAGD introduces a new graph learning and embedding term, which explores the high-order structure information between four tuples, rather than conventional sample pairs to learn an intrinsic graph. Moreover, DRAGD provides a new way to simultaneously capture the local geometric structure and representation structure of data in one term. To enhance the discriminability of the transformation matrix, a retargeted learning approach is introduced. As a result of combining the above-mentioned techniques, DRAGD can flexibly explore more unsupervised information underlying the data and the label information to obtain the most discriminative transformation matrix for multiclass classification tasks. Experimental results on six well-known real-world databases and a synthetic database demonstrate that DRAGD is superior to the state-of-the-art LR methods.},
  archive      = {J_TNNLS},
  author       = {Jie Wen and Shijie Deng and Lunke Fei and Zheng Zhang and Bob Zhang and Zhao Zhang and Yong Xu},
  doi          = {10.1109/TNNLS.2022.3185408},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1797-1809},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative regression with adaptive graph diffusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive semantic-enhanced transformer for image captioning.
<em>TNNLS</em>, <em>35</em>(2), 1785–1796. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the research on image captioning, rich semantic information is very important for generating critical caption words as guiding information. However, semantic information from offline object detectors involves many semantic objects that do not appear in the caption, thereby bringing noise into the decoding process. To produce more accurate semantic guiding information and further optimize the decoding process, we propose an end-to-end adaptive semantic-enhanced transformer (AS-Transformer) model for image captioning. For semantic enhancement information extraction, we propose a constrained weaklysupervised learning (CWSL) module, which reconstructs the semantic object’s probability distribution detected by the multiple instances learning (MIL) through a joint loss function. These strengthened semantic objects from the reconstructed probability distribution can better depict the semantic meaning of images. Also, for semantic enhancement decoding, we propose an adaptive gated mechanism (AGM) module to adjust the attention between visual and semantic information adaptively for the more accurate generation of caption words. Through the joint control of the CWSL module and AGM module, our proposed model constructs a complete adaptive enhancement mechanism from encoding to decoding and obtains visual context that is more suitable for captions. Experiments on the public Microsoft Common Objects in COntext (MSCOCO) and Flickr30K datasets illustrate that our proposed AS-Transformer can adaptively obtain effective semantic information and adjust the attention weights between semantic and visual information automatically, which achieves more accurate captions compared with semantic enhancement methods and outperforms state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Jing Zhang and Zhongjun Fang and Han Sun and Zhe Wang},
  doi          = {10.1109/TNNLS.2022.3185320},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1785-1796},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive semantic-enhanced transformer for image captioning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A three-stage optimal operation strategy of interconnected
microgrids with rule-based deep deterministic policy gradient algorithm.
<em>TNNLS</em>, <em>35</em>(2), 1773–1784. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing requirements of demand response dynamics, competition among different stakeholders, and information privacy protection intensify the challenge of the optimal operation of microgrids. To tackle the above problems, this article proposes a three-stage optimization strategy with a deep reinforcement learning (DRL)-based distributed privacy optimization. In the upper layer of the model, the rule-based deep deterministic policy gradient (DDPG) algorithm is proposed to optimize the load migration problem with demand response, which enhances dynamic characteristics with the interaction between electricity prices and consumer behavior. Due to the competition among different stakeholders and the information privacy requirement in the middle layer of the model, a potential game-based distributed privacy optimization algorithm is improved to seek Nash equilibriums (NEs) with encoded exchange information by a distributed privacy-preserving optimization algorithm, which can ensure the convergence as well as protect privacy information of each stakeholder. In the lower layer of the model of each stakeholder, economic cost and emission rate are both taken as operation objectives, and a gradient descent-based multiobjective optimization method is employed to approach this objective. The simulation results confirm that the proposed three-stage optimization strategy can be a viable and efficient way for the optimal operation of microgrids.},
  archive      = {J_TNNLS},
  author       = {Huifeng Zhang and Dong Yue and Chunxia Dou and Gerhard P. Hancke},
  doi          = {10.1109/TNNLS.2022.3185211},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1773-1784},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A three-stage optimal operation strategy of interconnected microgrids with rule-based deep deterministic policy gradient algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Industrial process monitoring based on dynamic overcomplete
broad learning network. <em>TNNLS</em>, <em>35</em>(2), 1761–1772. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most industrial processes feature high nonlinearity, non-Gaussianity, and time correlation. Models based on overcomplete broad learning system (OBLS) have been successfully applied in the fault monitoring realm, which may relatively deal with the nonlinear and non-Gaussian characteristics. However, these models barely take time correlation into full consideration, hindering the further improvement of the monitoring accuracy of the network. Therefore, an effective dynamic overcomplete broad learning system (DOBLS) based on matrix extension is proposed, which extends the raw data in the batch process with the idea of “time lag” in this article. Subsequently, the OBLS monitoring network is employed to continue the analysis of the extended dynamic input data. Finally, a monitoring model is established to tackle the coexistence of nonlinearity, non-Gaussianity, and time correlation in process data. To illustrate the superiority and feasibility, the proposed model is conducted on the penicillin fermentation simulation platform, the experimental result of which illustrates that the model can extract the feature of process data more comprehensively and be self-updated more efficiently. With shorter training time and higher monitoring accuracy, the proposed model can witness an improvement of average monitoring accuracy by 3.69% and 1.26% in 26 process fault types compared to the state-of-the-art fault monitoring methods BLS and OBLS, respectively.},
  archive      = {J_TNNLS},
  author       = {Chang Peng and Xu Ying and Hu ZhiQi},
  doi          = {10.1109/TNNLS.2022.3185167},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1761-1772},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Industrial process monitoring based on dynamic overcomplete broad learning network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Event-triggered exponential synchronization of the switched
neural networks with frequent asynchronism. <em>TNNLS</em>,
<em>35</em>(2), 1750–1760. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synchronization for a class of switched uncertain neural networks (NNs) with frequent asynchronism based on event-triggered control is researched in this article. Compared with existing works that require one switching during an inter-event interval, frequent switching is allowed in this article. By employing controller-mode-dependent Lyapunov–Krasovskii functionals (LKFs), we devise the control strategy to guarantee that the switched NNs can be synchronized. The proposed LKFs can make full use of system information. Using an improved integral inequality, some sufficient stability conditions formed by linear matrix inequalities (LMIs) are derived for the synchronization of switched uncertain NNs. Average dwell time (ADT) is obtained in the form of inequality that includes the maximum inter-event interval. In addition, the existence of lower bound of inter-event interval is discussed to avoid Zeno behavior. At last, the feasibility of the proposed method is proven by a numerical example.},
  archive      = {J_TNNLS},
  author       = {Chao Ge and Xin Liu and Yajuan Liu and Changchun Hua},
  doi          = {10.1109/TNNLS.2022.3185098},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1750-1760},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered exponential synchronization of the switched neural networks with frequent asynchronism},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive iterative learning fault-tolerant control for state
constrained nonlinear systems with randomly varying iteration lengths.
<em>TNNLS</em>, <em>35</em>(2), 1735–1749. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an adaptive iterative learning fault-tolerant control algorithm for state constrained nonlinear systems with randomly varying iteration lengths subjected to actuator faults. First, the modified parameters updating laws are designed through a new defined tracking error to handle the randomly varying iteration lengths. Second, the radial basis function neural network method is used to deal with the time-iteration-dependent unknown nonlinearity, and a barrier Lyapunov function is given to cope with the state constraint. Finally, a new barrier composite energy function is used to achieve the tracking error convergence of the presented control algorithm along the iteration axis with the state constraint and then followed with the extension to the high-order case. A simulation for a single-link manipulator is given to illustrate the effectiveness of the theoretical studies.},
  archive      = {J_TNNLS},
  author       = {Genfeng Liu and Zhongsheng Hou},
  doi          = {10.1109/TNNLS.2022.3185080},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1735-1749},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive iterative learning fault-tolerant control for state constrained nonlinear systems with randomly varying iteration lengths},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Local sample-weighted multiple kernel clustering with
consensus discriminative graph. <em>TNNLS</em>, <em>35</em>(2),
1721–1734. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel clustering (MKC) is committed to achieving optimal information fusion from a set of base kernels. Constructing precise and local kernel matrices is proven to be of vital significance in applications since the unreliable distant–distance similarity estimation would degrade clustering performance. Although existing localized MKC algorithms exhibit improved performance compared with globally designed competitors, most of them widely adopt the KNN mechanism to localize kernel matrix by accounting for $\tau $ -nearest neighbors. However, such a coarse manner follows an unreasonable strategy that the ranking importance of different neighbors is equal, which is impractical in applications. To alleviate such problems, this article proposes a novel local sample-weighted MKC (LSWMKC) model. We first construct a consensus discriminative affinity graph in kernel space, revealing the latent local structures. Furthermore, an optimal neighborhood kernel for the learned affinity graph is output with naturally sparse property and clear block diagonal structure. Moreover, LSWMKC implicitly optimizes adaptive weights on different neighbors with corresponding samples. Experimental results demonstrate that our LSWMKC possesses better local manifold representation and outperforms existing kernel or graph-based clustering algorithms. The source code of LSWMKC can be publicly accessed from https://github.com/liliangnudt/LSWMKC .},
  archive      = {J_TNNLS},
  author       = {Liang Li and Siwei Wang and Xinwang Liu and En Zhu and Li Shen and Kenli Li and Keqin Li},
  doi          = {10.1109/TNNLS.2022.3184970},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1721-1734},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local sample-weighted multiple kernel clustering with consensus discriminative graph},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed online constrained optimization with feedback
delays. <em>TNNLS</em>, <em>35</em>(2), 1708–1720. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate multiagent distributed online constrained convex optimization problems with feedback delays, where agents make sequential decisions before being aware of the cost and constraint functions. The main purpose of the distributed online constrained convex optimization problem is to cooperatively minimize the sum of time-varying local cost functions subject to time-varying coupled inequality constraints. The feedback information of the distributed online optimization problem is revealed to agents with time delays, which is common in practice. Every node in the system can interact with neighbors through a time-varying sequence of directed communication topologies, which is uniformly strongly connected. The distributed online primal-dual bandit push-sum algorithm that generates primal and dual variables with delayed feedback is used for the presented problem. Expected regret and expected constraint violation are proposed for measuring the performance of the algorithm, and both of them are shown to be sublinear with respect to the total iteration span $T$ in this article. In the end, the optimization problem for the power grid is simulated to justify the proposed theoretical results.},
  archive      = {J_TNNLS},
  author       = {Cong Wang and Shengyuan Xu},
  doi          = {10.1109/TNNLS.2022.3184957},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1708-1720},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed online constrained optimization with feedback delays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding via exploration: Discovery of interpretable
features with deep reinforcement learning. <em>TNNLS</em>,
<em>35</em>(2), 1696–1707. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the environments through interactions has been one of the most important human intellectual activities in mastering unknown systems. Deep reinforcement learning (DRL) has already been known to achieve effective control through human-like exploration and exploitation in many applications. However, the opaque nature of deep neural network (DNN) often hides critical information about feature relevance to control, which is essential for understanding the target systems. In this article, a novel online feature selection framework, namely, the dual-world-based attentive feature selection (D-AFS), is first proposed to identify the contribution of the inputs over the whole control process. Rather than the one world used in most DRL, D-AFS has both the real world and its virtual peer with twisted features. The newly introduced attention-based evaluation (AR) module performs the dynamic mapping from the real world to the virtual world. The existing DRL algorithms, with slight modification, can learn in the dual world. By analyzing the DRL’s response in the two worlds, D-AFS can quantitatively identify respective features’ importance toward control. A set of experiments is performed on four classical control systems in OpenAI Gym. Results show that D-AFS can generate the same or even better feature combinations than the solutions provided by human experts and seven recent feature selection baselines. In all cases, the selected feature representations are closely correlated with the ones used by underlying system dynamic models.},
  archive      = {J_TNNLS},
  author       = {Jiawen Wei and Zhifeng Qiu and Fangyuan Wang and Wenwei Lin and Ning Gui and Weihua Gui},
  doi          = {10.1109/TNNLS.2022.3184956},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1696-1707},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding via exploration: Discovery of interpretable features with deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). BASS: Broad network based on localized stochastic
sensitivity. <em>TNNLS</em>, <em>35</em>(2), 1681–1695. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training of the standard broad learning system (BLS) concerns the optimization of its output weights via the minimization of both training mean square error (MSE) and a penalty term. However, it degrades the generalization capability and robustness of BLS when facing complex and noisy environments, especially when small perturbations or noise appear in input data. Therefore, this work proposes a broad network based on localized stochastic sensitivity (BASS) algorithm to tackle the issue of noise or input perturbations from a local perturbation perspective. The localized stochastic sensitivity (LSS) prompts an increase in the network’s noise robustness by considering unseen samples located within a $Q$ -neighborhood of training samples, which enhances the generalization capability of BASS with respect to noisy and perturbed data. Then, three incremental learning algorithms are derived to update BASS quickly when new samples arrive or the network is deemed to be expanded, without retraining the entire model. Due to the inherent superiorities of the LSS, extensive experimental results on 13 benchmark datasets show that BASS yields better accuracies on various regression and classification problems. For instance, BASS uses fewer parameters (12.6 million) to yield 1% higher Top-1 accuracy in comparison to AlexNet (60 million) on the large-scale ImageNet (ILSVRC2012) dataset.},
  archive      = {J_TNNLS},
  author       = {Ting Wang and Mingyang Zhang and Jianjun Zhang and Wing W. Y. Ng and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3184846},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1681-1695},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BASS: Broad network based on localized stochastic sensitivity},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Stage-wise magnitude-based pruning for recurrent neural
networks. <em>TNNLS</em>, <em>35</em>(2), 1666–1680. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recurrent neural network (RNN) has shown powerful performance in tackling various natural language processing (NLP) tasks, resulting in numerous powerful models containing both RNN neurons and feedforward neurons. On the other hand, the deep structure of RNN has heavily restricted its implementation on mobile devices, where quite a few applications involve NLP tasks. Magnitude-based pruning (MP) is a promising way to address such a challenge. However, the existing MP methods are mostly designed for feedforward neural networks that do not involve a recurrent structure, and, thus, have performed less satisfactorily on pruning models containing RNN layers. In this article, a novel stage-wise MP method is proposed by explicitly taking the featured recurrent structure of RNN into account, which can effectively prune feedforward layers and RNN layers, simultaneously. The connections of neural networks are first grouped into three types according to how they are intersected with recurrent neurons. Then, an optimization-based pruning method is applied to compress each group of connections, respectively. Empirical studies show that the proposed method performs significantly better than the commonly used RNN pruning methods; i.e., up to 96.84% connections are pruned with little or even no degradation of precision indicators on the testing datasets.},
  archive      = {J_TNNLS},
  author       = {Guiying Li and Peng Yang and Chao Qian and Richang Hong and Ke Tang},
  doi          = {10.1109/TNNLS.2022.3184730},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1666-1680},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stage-wise magnitude-based pruning for recurrent neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Multiview deep anomaly detection: A systematic exploration.
<em>TNNLS</em>, <em>35</em>(2), 1651–1665. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection (AD), which models a given normal class and distinguishes it from the rest of abnormal classes, has been a long-standing topic with ubiquitous applications. As modern scenarios often deal with massive high-dimensional complex data spawned by multiple sources, it is natural to consider AD from the perspective of multiview deep learning. However, it has not been formally discussed by the literature and remains underexplored. Motivated by this blank, this article makes fourfold contributions: First, to the best of our knowledge, this is the first work that formally identifies and formulates the multiview deep AD problem. Second, we take recent advances in relevant areas into account and systematically devise various baseline solutions, which lays the foundation for multiview deep AD research. Third, to remedy the problem that limited benchmark datasets are available for multiview deep AD, we extensively collect the existing public data and process them into more than 30 multiview benchmark datasets via multiple means, so as to provide a better evaluation platform for multiview deep AD. Finally, by comprehensively evaluating the devised solutions on different types of multiview deep AD benchmark datasets, we conduct a thorough analysis on the effectiveness of the designed baselines and hopefully provide other researchers with beneficial guidance and insight into the new multiview deep AD topic.},
  archive      = {J_TNNLS},
  author       = {Siqi Wang and Jiyuan Liu and Guang Yu and Xinwang Liu and Sihang Zhou and En Zhu and Yuexiang Yang and Jianping Yin and Wenjing Yang},
  doi          = {10.1109/TNNLS.2022.3184723},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1651-1665},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview deep anomaly detection: A systematic exploration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Research ideas discovery via hierarchical negative
correlation. <em>TNNLS</em>, <em>35</em>(2), 1639–1650. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new research idea may be inspired by the connections of keywords. Link prediction discovers potential nonexisting links in an existing graph and has been applied in many applications. This article explores a method of discovering new research ideas based on link prediction, which predicts the possible connections of different keywords by analyzing the topological structure of the keyword graph. The patterns of links between keywords may be diversified due to different domains and different habits of authors. Therefore, it is often difficult for a single learner to extract diverse patterns of different research domains. To address this issue, groups of learners are organized with negative correlation to encourage the diversity of sublearners. Moreover, a hierarchical negative correlation mechanism is proposed to extract subgraph features in different order subgraphs, which improves the diversity by explicitly supervising the negative correlation on each layer of sublearners. Experiments are conducted to illustrate the effectiveness of the proposed model to discover new research ideas. Under the premise of ensuring the performance of the model, the proposed method consumes less time and computational cost compared with other ensemble methods.},
  archive      = {J_TNNLS},
  author       = {Lyuzhou Chen and Xiangyu Wang and Taiyu Ban and Muhammad Usman and Shikang Liu and Derui Lyu and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2022.3184498},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1639-1650},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Research ideas discovery via hierarchical negative correlation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Finite-time synchronization and h∞ synchronization for
coupled neural networks with multistate or multiderivative couplings.
<em>TNNLS</em>, <em>35</em>(2), 1628–1638. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the finite-time synchronization (FTS) and $H_{\infty }$ synchronization for two types of coupled neural networks (CNNs), that is, the cases with multistate couplings and with multiderivative couplings. By designing appropriate state feedback controllers and parameter adjustment strategies, some FTS and finite-time $H_{\infty }$ synchronization criteria for CNNs with multistate couplings are derived. In addition, we further consider the FTS and finite-time $H_{\infty }$ synchronization problems for CNNs with multiderivative couplings by utilizing state feedback control approach and selecting suitable parameter adjustment schemes. Finally, two simulation examples are given to demonstrate the effectiveness of the proposed criteria.},
  archive      = {J_TNNLS},
  author       = {Jin-Liang Wang and Han-Yu Wu and Tingwen Huang and Shun-Yan Ren},
  doi          = {10.1109/TNNLS.2022.3184487},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1628-1638},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time synchronization and h∞ synchronization for coupled neural networks with multistate or multiderivative couplings},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image regression with structure cycle consistency for
heterogeneous change detection. <em>TNNLS</em>, <em>35</em>(2),
1613–1627. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) between heterogeneous images is an increasingly interesting topic in remote sensing. The different imaging mechanisms lead to the failure of homogeneous CD methods on heterogeneous images. To address this challenge, we propose a structure cycle consistency-based image regression method, which consists of two components: the exploration of structure representation and the structure-based regression. We first construct a similarity relationship-based graph to capture the structure information of image; here, a $k$ -selection strategy and an adaptive-weighted distance metric are employed to connect each node with its truly similar neighbors. Then, we conduct the structure-based regression with this adaptively learned graph. More specifically, we transform one image to the domain of the other image via the structure cycle consistency, which yields three types of constraints: forward transformation term, cycle transformation term, and sparse regularization term. Noteworthy, it is not a traditional pixel value-based image regression, but an image structure regression, i.e., it requires the transformed image to have the same structure as the original image. Finally, change extraction can be achieved accurately by directly comparing the transformed and original images. Experiments conducted on different real datasets show the excellent performance of the proposed method. The source code of the proposed method will be made available at https://github.com/yulisun/AGSCC .},
  archive      = {J_TNNLS},
  author       = {Yuli Sun and Lin Lei and Dongdong Guan and Junzheng Wu and Gangyao Kuang and Li Liu},
  doi          = {10.1109/TNNLS.2022.3184414},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1613-1627},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Image regression with structure cycle consistency for heterogeneous change detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale cross-connected dehazing network with scene depth
fusion. <em>TNNLS</em>, <em>35</em>(2), 1598–1612. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a multiscale cross-connected dehazing network with scene depth fusion. We focus on the correlation between a hazy image and the corresponding depth image. The model encodes and decodes the hazy image and the depth image separately and includes cross connections at the decoding end to directly generate a clean image in an end-to-end manner. Specifically, we first construct an input pyramid to obtain the receptive fields of the depth image and the hazy image at multiple levels. Then, we add the features of the corresponding dimensions in the input pyramid to the encoder. Finally, the two paths of the decoder are cross-connected. In addition, the proposed model uses wavelet pooling and residual channel attention modules (RCAMs) as components. A series of ablation experiments shows that the wavelet pooling and RCAMs effectively improve the performance of the model. We conducted extensive experiments on multiple dehazing datasets, and the results show that the model is superior to other advanced methods in terms of peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and subjective visual effects. The source code and supplementary are available at https://github.com/CCECfgd/MSCDN-master .},
  archive      = {J_TNNLS},
  author       = {Guodong Fan and Min Gan and Bi Fan and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3184164},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1598-1612},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiscale cross-connected dehazing network with scene depth fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Product recognition for unmanned vending machines.
<em>TNNLS</em>, <em>35</em>(2), 1584–1597. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the emerging concept of “unmanned retail” has drawn more and more attention, and the unmanned retail based on the intelligent unmanned vending machines (UVMs) scene has great market demand. However, existing product recognition methods for intelligent UVMs cannot adapt to large-scale categories and have insufficient accuracy. In this article, we propose a method for large-scale categories product recognition based on intelligent UVMs. It can be divided into two parts: 1) first, we explore the similarities and differences between products through manifold learning, and then we build a hierarchical multigranularity label to constrain the learning of representation; and 2) second, we propose a hierarchical label object detection network, which mainly includes coarse-to-fine refine module (C2FRM) and multiple granularity hierarchical loss (MGHL), which are used to assist in capturing multigranularity features. The highlights of our method are mine potential similarity between large-scale category products and optimization through hierarchical multigranularity labels. Besides, we collected a large-scale product recognition dataset GOODS-85 based on the actual UVMs scenario. Experimental results and analysis demonstrate the effectiveness of the proposed product recognition methods.},
  archive      = {J_TNNLS},
  author       = {Chengxu Liu and Zongyang Da and Yuanzhi Liang and Yao Xue and Guoshuai Zhao and Xueming Qian},
  doi          = {10.1109/TNNLS.2022.3184075},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1584-1597},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Product recognition for unmanned vending machines},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolving dual-threshold bienenstock-cooper-munro learning
rules in echo state networks. <em>TNNLS</em>, <em>35</em>(2), 1572–1583.
(<a href="https://doi.org/10.1109/TNNLS.2022.3184004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The strengthening and the weakening of synaptic strength in existing Bienenstock-Cooper-Munro (BCM) learning rule are determined by a long-term potentiation (LTP) sliding modification threshold and the afferent synaptic activities. However, synaptic long-term depression (LTD) even affects low-active synapses during the induction of synaptic plasticity, which may lead to information loss. Biological experiments have found another LTD threshold that can induce either potentiation or depression or no change, even at the activated synapses. In addition, existing BCM learning rules can only select a set of fixed rule parameters, which is biologically implausible and practically inflexible to learn the structural information of input signals. In this article, an evolved dual-threshold BCM learning rule is proposed to regulate the reservoir internal connection weights of the echo-state-network (ESN), which can contribute to alleviating information loss and enhancing learning performance by introducing different optimal LTD thresholds for different postsynaptic neurons. Our experimental results show that the evolved dual-threshold BCM learning rule can result in the synergistic learning of different plasticity rules, effectively improving the learning performance of an ESN in comparison with existing neural plasticity learning rules and some state-of-the-art ESN variants on three widely used benchmark tasks and the prediction of an esterification process.},
  archive      = {J_TNNLS},
  author       = {Xinjie Wang and Yaochu Jin and Wenli Du and Jun Wang},
  doi          = {10.1109/TNNLS.2022.3184004},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1572-1583},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Evolving dual-threshold bienenstock-cooper-munro learning rules in echo state networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical sliding-mode surface-based adaptive
actor–critic optimal control for switched nonlinear systems with unknown
perturbation. <em>TNNLS</em>, <em>35</em>(2), 1559–1571. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the hierarchical sliding-mode surface (HSMS)-based adaptive optimal control problem for a class of switched continuous-time (CT) nonlinear systems with unknown perturbation under an actor–critic (AC) neural networks (NNs) architecture. First, a novel perturbation observer with a nested parameter adaptive law is designed to estimate the unknown perturbation. Then, by constructing an especial cost function related to HSMS, the original control issue is further converted into the problem of finding a series of optimal control policies. The solution to the HJB equation is identified by the HSMS-based AC NNs, where the actor and critic updating laws are developed to implement the reinforcement learning (RL) strategy simultaneously. The critic update law is designed via the gradient descent approach and the principle of standardization, such that the persistence of excitation (PE) condition is no longer needed. Based on the Lyapunov stability theory, all the signals of the closed-loop switched nonlinear systems are strictly proved to be bounded in the sense of uniformly ultimate boundedness (UUB). Finally, the simulation results are presented to verify the validity of the proposed adaptive optimal control scheme.},
  archive      = {J_TNNLS},
  author       = {Haoyan Zhang and Xudong Zhao and Huanqing Wang and Guangdeng Zong and Ning Xu},
  doi          = {10.1109/TNNLS.2022.3183991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1559-1571},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical sliding-mode surface-based adaptive Actor–Critic optimal control for switched nonlinear systems with unknown perturbation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multitrend conditional value at risk for portfolio
optimization. <em>TNNLS</em>, <em>35</em>(2), 1545–1558. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trend representation has been attracting more and more attention recently in portfolio optimization (PO) via machine learning methods. It adopts concepts and phenomena from the field of empirical and behavioral finance when little prior knowledge is obtained or strict statistical assumptions cannot be guaranteed. It is used mostly in estimating the expected asset returns, but hardly in measuring risk. To fill this gap, we propose a novel multitrend conditional value at risk (MT-CVaR), which embeds multiple trends and their influences in CVaR. Besides, we propose a novel PO model with this MT-CVaR as the risk metric and then design a solving algorithm based on the interior point method to compute the portfolio. Extensive experiments on six benchmark datasets from diverse financial markets with different frequencies show that MT-CVaR achieves the state-of-the-art investing performance and risk management.},
  archive      = {J_TNNLS},
  author       = {Zhao-Rong Lai and Cheng Li and Xiaotian Wu and Quanlong Guan and Liangda Fang},
  doi          = {10.1109/TNNLS.2022.3183891},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1545-1558},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitrend conditional value at risk for portfolio optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residual q-networks for value function factorizing in
multiagent reinforcement learning. <em>TNNLS</em>, <em>35</em>(2),
1534–1544. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent reinforcement learning (MARL) is useful in many problems that require the cooperation and coordination of multiple agents. Learning optimal policies using reinforcement learning in a multiagent setting can be very difficult as the number of agents increases. Recent solutions such as value decomposition networks (VDNs), QMIX, QTRAN, and QPLEX adhere to the centralized training and decentralized execution (CTDE) scheme and perform factorization of the joint action-value functions. However, these methods still suffer from increased environmental complexity, and at times fail to converge in a stable manner. We propose a novel concept of residual Q-networks (RQNs) for MARL, which learns to transform the individual $Q$ -value trajectories in a way that preserves the individual-global-max (IGM) criteria, but is more robust in factorizing action-value functions. The RQN acts as an auxiliary network that accelerates convergence and will become obsolete as the agents reach the training objectives. The performance of the proposed method is compared against several state-of-the-art techniques such as QPLEX, QMIX, QTRAN, and VDN, in a range of multiagent cooperative tasks. The results illustrate that the proposed method, in general, converges faster, with increased stability, and shows robust performance in a wider family of environments. The improvements in results are more prominent in environments with severe punishments for noncooperative behaviors and especially in the absence of complete state information during training time.},
  archive      = {J_TNNLS},
  author       = {Rafael Pina and Varuna De Silva and Joosep Hook and Ahmet Kondoz},
  doi          = {10.1109/TNNLS.2022.3183865},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1534-1544},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Residual Q-networks for value function factorizing in multiagent reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Word2Pix: Word to pixel cross-attention transformer in
visual grounding. <em>TNNLS</em>, <em>35</em>(2), 1523–1533. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current one-stage methods for visual grounding encode the language query as one holistic sentence embedding before fusion with visual features for target localization. Such a formulation provides insufficient ability to model query at the word level, and therefore is prone to neglect words that may not be the most important ones for a sentence but are critical for the referred object. In this article, we propose Word2Pix: a one-stage visual grounding network based on the encoder–decoder transformer architecture that enables learning for textual to visual feature correspondence via word to pixel attention. Each word from the query sentence is given an equal opportunity when attending to visual pixels through multiple stacks of transformer decoder layers. In this way, the decoder can learn to model the language query and fuse language with the visual features for target prediction simultaneously. We conduct the experiments on RefCOCO, RefCOCO+, and RefCOCOg datasets, and the proposed Word2Pix outperforms the existing one-stage methods by a notable margin. The results obtained also show that Word2Pix surpasses the two-stage visual grounding models, while at the same time keeping the merits of the one-stage paradigm, namely, end-to-end training and fast inference speed. Code is available at https://github.com/azurerain7/Word2Pix .},
  archive      = {J_TNNLS},
  author       = {Heng Zhao and Joey Tianyi Zhou and Yew-Soon Ong},
  doi          = {10.1109/TNNLS.2022.3183827},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1523-1533},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Word2Pix: Word to pixel cross-attention transformer in visual grounding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel path for semisupervised support vector machine.
<em>TNNLS</em>, <em>35</em>(2), 1512–1522. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised support vector machine (S3VM) is a powerful semisupervised learning model that can use large amounts of unlabeled data to train high-quality classification models. The choice of kernel parameters in the kernel function determines the mapping between the input space and the feature space and is crucial to the performance of the S3VM. Kernel path algorithms have been widely recognized as one of the most efficient tools to trace the solutions with respect to a kernel parameter. However, existing kernel path algorithms are limited to convex problems, while S3VM is nonconvex problem. To address this challenging problem, in this article, we first propose a kernel path algorithm of S3VM (KPS3VM), which can track the solutions of the nonconvex S3VM with respect to a kernel parameter. Specifically, we estimate the position of the breakpoint by monitoring the change of the sample sets. In addition, we also use an incremental and decremental learning algorithm to deal with the Karush–Khun–Tucker violating samples in the process of tracking the solutions. More importantly, we prove the finite convergence of our KPS3VM algorithm. Experimental results on various benchmark datasets not only validate the effectiveness of our KPS3VM algorithm but also show the advantage of choosing the optimal kernel parameters.},
  archive      = {J_TNNLS},
  author       = {Zhou Zhai and Heng Huang and Bin Gu},
  doi          = {10.1109/TNNLS.2022.3183825},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1512-1522},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Kernel path for semisupervised support vector machine},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered-based distributed consensus tracking for
nonlinear multiagent systems with quantization. <em>TNNLS</em>,
<em>35</em>(2), 1501–1511. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an observer-based adaptive neural network (NN) event-triggered distributed consensus tracking problem is investigated for nonlinear multiagent systems with quantization. In the first place, the limited capacity of the communication channel between agents is considered. The event-trigger mechanism and dynamic uniform quantizers are set up to reduce information transmission. The next NN is utilized to handle the unknown nonlinear functions. Finally, in order to estimate the unmeasurable states, an NN-based state observer is designed for each agent by using a dynamic gain function. To settle the difficulty caused by the coupling effects of event-triggered conditions and the scaling function in dynamic uniform quantizers and observers, a distributed control protocol with estimated information of its neighbors is designed, which ensures distributed consensus tracking of the nonlinear multiagent systems without incurring the Zeno behavior. The effectiveness of the control protocol is illustrated by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Jing Zhang and Shuai Liu and Xianfu Zhang and Jianwei Xia},
  doi          = {10.1109/TNNLS.2022.3183639},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1501-1511},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered-based distributed consensus tracking for nonlinear multiagent systems with quantization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid adjusting variables-dependent event-based finite-time
state estimation for two-time-scale markov jump complex networks.
<em>TNNLS</em>, <em>35</em>(2), 1487–1500. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of dynamic event-triggered finite-time $H_{\infty }$ state estimation for a class of discrete-time nonlinear two-time-scale Markov jump complex networks. A hybrid adjusting variables-dependent dynamic event-triggered mechanism (DETM) is proposed to regulate the releases of measurement outputs of a node to a remote state estimator. Such a DETM contains both an additive dynamically adjusting variable (DAV) and a multiplicative adaptively adjusting variable. The aim is to design a DETM-based mode-dependent state estimator, which guarantees that the resultant error dynamics is stochastically finite-time bounded with $H_{\infty }$ performance. By constructing a mode-dependent Lyapunov function with multiple DAVs and a singular perturbation parameter associated with time scales, a matrix-inequalities-based sufficient condition is derived, the feasible solutions of which facilitate the design of the parameters of the state estimator. The validity of the designed state estimator and the superiority of the devised DETM are verified by two examples. It is verified that the devised DETM is capable of saving network resources and simultaneously improving the estimation performance.},
  archive      = {J_TNNLS},
  author       = {Xiongbo Wan and Chao Yang and Chuan-Ke Zhang and Min Wu},
  doi          = {10.1109/TNNLS.2022.3183447},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1487-1500},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hybrid adjusting variables-dependent event-based finite-time state estimation for two-time-scale markov jump complex networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient quantum image classification using single qubit
encoding. <em>TNNLS</em>, <em>35</em>(2), 1472–1486. (<a
href="https://doi.org/10.1109/TNNLS.2022.3179354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The domain of image classification has been seen to be dominated by high-performing deep-learning (DL) architectures. However, the success of this field, as seen over the past decade, has resulted in the complexity of modern methodologies scaling exponentially, commonly requiring millions of parameters. Quantum computing (QC) is an active area of research aimed toward greatly reducing problems of complexity faced in classical computing. With growing interest toward quantum machine learning (QML) for applications of image classification, many proposed algorithms require usage of numerous qubits. In the noisy intermediate-scale quantum (NISQ) era, these circuits may not always be feasible to execute effectively; therefore, we should aim to use each qubit as effectively and efficiently as possible, before adding additional qubits. This article proposes a new single-qubit-based deep quantum neural network for image classification that mimics traditional convolutional neural network (CNN) techniques, resulting in a reduced number of parameters compared with previous works. Our aim is to prove the concept of the initial proposal by demonstrating classification performance of the single-qubit-based architecture, as well as to provide a tested foundation for further development. To demonstrate this, our experiments were conducted using various datasets including MNIST, Fashion-MNIST, and ORL face datasets. To further our proposal in the context of the NISQ era, our experiments were intentionally conducted in noisy simulation environments. Initial test results appear promising, with classification accuracies of 94.6%, 89.5%, and 82.5% achieved on the subsets of MNIST, FMNIST, and ORL face datasets, respectively. In addition, proposals for further investigation and development were considered, where it is hoped that these initial results can be improved.},
  archive      = {J_TNNLS},
  author       = {Philip Easom-McCaldin and Ahmed Bouridane and Ammar Belatreche and Richard Jiang and Somaya Al-Maadeed},
  doi          = {10.1109/TNNLS.2022.3179354},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1472-1486},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient quantum image classification using single qubit encoding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised learning for electroencephalography.
<em>TNNLS</em>, <em>35</em>(2), 1457–1471. (<a
href="https://doi.org/10.1109/TNNLS.2022.3190448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decades of research have shown machine learning superiority in discovering highly nonlinear patterns embedded in electroencephalography (EEG) records compared with conventional statistical techniques. However, even the most advanced machine learning techniques require relatively large, labeled EEG repositories. EEG data collection and labeling are costly. Moreover, combining available datasets to achieve a large data volume is usually infeasible due to inconsistent experimental paradigms across trials. Self-supervised learning (SSL) solves these challenges because it enables learning from EEG records across trials with variable experimental paradigms, even when the trials explore different phenomena. It aggregates multiple EEG repositories to increase accuracy, reduce bias, and mitigate overfitting in machine learning training. In addition, SSL could be employed in situations where there is limited labeled training data, and manual labeling is costly. This article: 1) provides a brief introduction to SSL; 2) describes some SSL techniques employed in recent studies, including EEG; 3) proposes current and potential SSL techniques for future investigations in EEG studies; 4) discusses the cons and pros of different SSL techniques; and 5) proposes holistic implementation tips and potential future directions for EEG SSL practices.},
  archive      = {J_TNNLS},
  author       = {Mohammad H. Rafiei and Lynne V. Gauthier and Hojjat Adeli and Daniel Takabi},
  doi          = {10.1109/TNNLS.2022.3190448},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  number       = {2},
  pages        = {1457-1471},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised learning for electroencephalography},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Reinforcement learning-based tracking control for a three
mecanum wheeled mobile robot. <em>TNNLS</em>, <em>35</em>(1), 1445–1452.
(<a href="https://doi.org/10.1109/TNNLS.2022.3185055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief investigates the robust optimal tracking control for a three Mecanum wheeled mobile robot (MWMR) with the external disturbance by the aid of online actor–critic synchronous learning algorithm. The Euler–Lagrange motion equation of MWMR subject to slipping is established by analyzing the structural characteristics of Mecanum wheels. Concatenating the tracking error with the desired trajectory, the tracking control problem is converted into a time-invariant optimal control problem of an augmented system. Then, an approximate optimal tracking controller is obtained by applying online actor–critic synchronous learning algorithm. With the help of Lyapunov-based analysis, the ultimately bounded tracking can be guaranteed. Finally, simulation results show the effectiveness of synchronous learning algorithm and approximate optimal tracking controller.},
  archive      = {J_TNNLS},
  author       = {Dianfeng Zhang and Guangcang Wang and Zhaojing Wu},
  doi          = {10.1109/TNNLS.2022.3185055},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1445-1452},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based tracking control for a three mecanum wheeled mobile robot},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Power attributed graph embedding and clustering.
<em>TNNLS</em>, <em>35</em>(1), 1439–1444. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning is a central problem of attributed networks (ANs) data analysis in a variety of fields. Given an attributed graph, the objectives are to obtain a representation of nodes and a partition of the set of nodes. Usually, these two objectives are pursued separately via two tasks that are performed sequentially, and any benefit that may be obtained by performing them simultaneously is lost. In this brief, we propose a power-attributed graph embedding and clustering (PAGEC for short) in which the two tasks, embedding and clustering, are considered together. To jointly encode data affinity between node links and attributes, we use a new powered proximity matrix. We formulate a new matrix decomposition model to obtain node representation and node clustering simultaneously. Theoretical analysis shows the close connections between the new proximity matrix and the random walk theory on a graph. Experimental results demonstrate that the PAGEC algorithm performs better, in terms of clustering and embedding, than state-of-the-art algorithms including deep learning methods designed for similar tasks in relation to attributed network datasets with different characteristics.},
  archive      = {J_TNNLS},
  author       = {Lazhar Labiod and Mohamed Nadif},
  doi          = {10.1109/TNNLS.2022.3183273},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1439-1444},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Power attributed graph embedding and clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Centroidal clustering of noisy observations by using th
power distortion measures. <em>TNNLS</em>, <em>35</em>(1), 1430–1438.
(<a href="https://doi.org/10.1109/TNNLS.2022.3183294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of clustering a dataset through multiple noisy observations of its members. The goal is to obtain a clustering that is as faithful to the clustering of the original dataset as possible. We propose a centroidal approach whose distortion measure is the sum of $r$ th powers of the distances between the cluster center and the noisy observations. For $r=2$ , our scheme boils down to the well-known approach of clustering the average of noisy samples. First, we provide a mathematical analysis of our clustering scheme. In particular, we find formulas for the average distortion and the spatial distribution of the cluster centers in the asymptotic regime where the number of centers is large. We then provide an algorithm to numerically optimize the cluster centers in the finite regime. We extend our method to automatically assign weights to noisy observations. Finally, we show that for various practical noise models, with a suitable choice of $r$ , our algorithms can outperform several other existing techniques over various datasets.},
  archive      = {J_TNNLS},
  author       = {Erdem Koyuncu},
  doi          = {10.1109/TNNLS.2022.3183294},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1430-1438},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Centroidal clustering of noisy observations by using th power distortion measures},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronization of tree parity machines using nonbinary
input vectors. <em>TNNLS</em>, <em>35</em>(1), 1423–1429. (<a
href="https://doi.org/10.1109/TNNLS.2022.3180197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural cryptography is the application of artificial neural networks (ANNs) in the subject of cryptography. The functionality of this solution is based on a tree parity machine (TPM). It uses ANNs to perform secure key exchange between network entities. This brief proposes improvements to the synchronization of two TPMs. The improvement is based on learning ANN using input vectors that have a wider range of values than binary ones. As a result, the duration of the synchronization process is reduced. Therefore, TPMs achieve common weights in a shorter time due to the reduction of necessary bit exchanges. This approach improves the security of neural cryptography.},
  archive      = {J_TNNLS},
  author       = {Miłosz Stypiński and Marcin Niemiec},
  doi          = {10.1109/TNNLS.2022.3180197},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1423-1429},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of tree parity machines using nonbinary input vectors},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph few-shot learning via restructuring task graph.
<em>TNNLS</em>, <em>35</em>(1), 1415–1422. (<a
href="https://doi.org/10.1109/TNNLS.2022.3178849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing graph few-shot learning (FSL) methods usually train a model on many task graphs and transfer the learned model to a new task graph. However, the task graphs often contain a great number of isolated nodes, which results in the severe deficiency of learned node embeddings. Furthermore, in the training process, the neglect of task information also constrains the model’s expressive ability. In this brief, we propose a novel metric-based graph few-shot learning approach via restructuring task graph (GFL-RTG). To solve the problems above, we innovatively restructure the task graph by adding class nodes and a task node to the original individual task graph. We first add class nodes and determine the connectivity between class nodes and others via their similarity. Then, we utilize a graph pooling network to learn a task embedding, which is regarded as a task node. Finally, the new task graph is restructured by combining class nodes, task node, and original nodes, which is then used as input to the metric-based graph neural network (GNN) to conduct few-shot learning. Our extensive experiments on three graph-structured datasets demonstrate that our proposed method generally outperforms the state-of-the-art baselines in few-shot learning.},
  archive      = {J_TNNLS},
  author       = {Feng Zhao and Tiancheng Huang and Donglin Wang},
  doi          = {10.1109/TNNLS.2022.3178849},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1415-1422},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph few-shot learning via restructuring task graph},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive neural finite-time control of non-strict feedback
nonlinear systems with non-symmetrical dead-zone. <em>TNNLS</em>,
<em>35</em>(1), 1409–1414. (<a
href="https://doi.org/10.1109/TNNLS.2022.3178366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The control design method for a class of non-strict feedback nonlinear systems is studied in this brief considering uncertain nonlinearities and unknown non-symmetrical input dead-zone. Combining with the finite-time command filtered backstepping (FCFB) technique, a novel finite-time adaptive control approach is proposed in which a neural network-based methodology is adopted to cope with the uncertain nonlinearities in the non-strict feedback form. The input dead-zone model is transformed into a simple linear system with unknown gain and bounded disturbance which is estimated by an adaptive factor. Using the finite-time Lyapunov theory, the system convergence is proved. And the effectiveness of the proposed control scheme is verified through comparative numerical simulations.},
  archive      = {J_TNNLS},
  author       = {Mingjie Cai and Peng Shi and Jinpeng Yu},
  doi          = {10.1109/TNNLS.2022.3178366},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1409-1414},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural finite-time control of non-strict feedback nonlinear systems with non-symmetrical dead-zone},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge-aware network for flow-based video frame interpolation.
<em>TNNLS</em>, <em>35</em>(1), 1401–1408. (<a
href="https://doi.org/10.1109/TNNLS.2022.3178281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame interpolation can up-convert the frame rate and enhance the video quality. In recent years, although interpolation performance has achieved great success, image blur usually occurs at object boundaries owing to the large motion. It has been a long-standing problem and has not been addressed yet. In this brief, we propose to reduce the image blur and get the clear shape of objects by preserving the edges in the interpolated frames. To this end, the proposed edge-aware network (EA-Net) integrates the edge information into the frame interpolation task. It follows an end-to-end architecture and can be separated into two stages, i.e., edge-guided flow estimation and edge-protected frame synthesis. Specifically, in the flow estimation stage, three edge-aware mechanisms are developed to emphasize the frame edges in estimating flow maps, so that the edge maps are taken as auxiliary information to provide more guidance to boost the flow accuracy. In the frame synthesis stage, the flow refinement module is designed to refine the flow map, and the attention module is carried out to adaptively focus on the bidirectional flow maps when synthesizing the intermediate frames. Furthermore, the frame and edge discriminators are adopted to conduct the adversarial training strategy, so as to enhance the reality and clarity of synthesized frames. Experiments on three benchmarks, including Vimeo90k, UCF101 for single-frame interpolation, and Adobe240-fps for multiframe interpolation, have demonstrated the superiority of the proposed EA-Net for the video frame interpolation task.},
  archive      = {J_TNNLS},
  author       = {Bin Zhao and Xuelong Li},
  doi          = {10.1109/TNNLS.2022.3178281},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1401-1408},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Edge-aware network for flow-based video frame interpolation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Circuit implementation and quasi-stabilization of delayed
inertial memristor-based neural networks. <em>TNNLS</em>,
<em>35</em>(1), 1394–1400. (<a
href="https://doi.org/10.1109/TNNLS.2022.3173620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we consider the stability of inertial memristor-based neural networks with time-varying delays. First, delayed inertial memristor-based neural networks are modeled as continuous systems in the flux-current-voltage-time domain via the mathematical model of Hewlett-Packard (HP) memristor. Then, they are reduced to delayed inertial neural networks with interval parameters uncertainties. Quasi-equilibrium points and quasi-stability are proposed. Quasi-stability criteria of delayed inertial memristor-based neural networks are obtained by matrix measure method, the Halanay inequality, and uncertainty technologies. In the end, a numerical example is provided to show the validity of our results.},
  archive      = {J_TNNLS},
  author       = {Youming Xin and Zunshui Cheng and Jinde Cao and Leszek Rutkowski and Yaning Wang},
  doi          = {10.1109/TNNLS.2022.3173620},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1394-1400},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Circuit implementation and quasi-stabilization of delayed inertial memristor-based neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional convolution projecting latent vectors on
condition-specific space. <em>TNNLS</em>, <em>35</em>(1), 1386–1393. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite rapid advancements over the past several years, the conditional generative adversarial networks (cGANs) are still far from being perfect. Although one of the major concerns of the cGANs is how to provide the conditional information to the generator, there are not only no ways considered as the optimal solution but also a lack of related research. This brief presents a novel convolution layer, called the conditional convolution (cConv) layer, which incorporates the conditional information into the generator of the generative adversarial networks (GANs). Unlike the most general framework of the cGANs using the conditional batch normalization (cBN) that transforms the normalized feature maps after convolution, the proposed method directly produces conditional features by adjusting the convolutional kernels depending on the conditions. More specifically, in each cConv layer, the weights are conditioned in a simple but effective way through filter-wise scaling and channel-wise shifting operations. In contrast to the conventional methods, the proposed method with a single generator can effectively handle condition-specific characteristics. The experimental results on CIFAR, LSUN, and ImageNet datasets show that the generator with the proposed cConv layer achieves a higher quality of conditional image generation than that with the standard convolution layer.},
  archive      = {J_TNNLS},
  author       = {Min-Cheol Sagong and Yoon-Jae Yeo and Yong-Goo Shin and Sung-Jea Ko},
  doi          = {10.1109/TNNLS.2022.3172512},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1386-1393},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Conditional convolution projecting latent vectors on condition-specific space},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational bayes ensemble learning neural networks with
compressed feature space. <em>TNNLS</em>, <em>35</em>(1), 1379–1385. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of nonparametric classification from a high-dimensional input vector (small $n$ large $p$ problem). To handle the high-dimensional feature space, we propose a random projection (RP) of the feature space followed by training of a neural network (NN) on the compressed feature space. Unlike regularization techniques (lasso, ridge, etc.), which train on the full data, NNs based on compressed feature space have significantly lower computation complexity and memory storage requirements. Nonetheless, a random compression-based method is often sensitive to the choice of compression. To address this issue, we adopt a Bayesian model averaging (BMA) approach and leverage the posterior model weights to determine: 1) uncertainty under each compression and 2) intrinsic dimensionality of the feature space (the effective dimension of feature space useful for prediction). The final prediction is improved by averaging models with projected dimensions close to the intrinsic dimensionality. Furthermore, we propose a variational approach to the afore-mentioned BMA to allow for simultaneous estimation of both model weights and model-specific parameters. Since the proposed variational solution is parallelizable across compressions, it preserves the computational gain of frequentist ensemble techniques while providing the full uncertainty quantification of a Bayesian approach. We establish the asymptotic consistency of the proposed algorithm under the suitable characterization of the RPs and the prior parameters. Finally, we provide extensive numerical examples for empirical validation of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Zihuan Liu and Shrijita Bhattacharya and Tapabrata Maiti},
  doi          = {10.1109/TNNLS.2022.3172276},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1379-1385},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Variational bayes ensemble learning neural networks with compressed feature space},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized dynamic event-triggered output feedback
adaptive fixed-time funnel control for interconnection nonlinear
systems. <em>TNNLS</em>, <em>35</em>(1), 1364–1378. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A decentralized dynamic event-triggered output feedback adaptive fixed-time (DDETOFAFxT) funnel controller is described for a class of interconnected nonlinear systems (INSs). A novel dynamic event-triggered mechanism is designed, which includes a triggering control input, fixed threshold, decreasing function of tracking error, and a dynamic variable. To obtain the unknown states, a decentralized linear filter is designed. By introducing a prescribed funnel and using an adding a power integrator technique and a neural network method, a DDETOFAFxT funnel controller is designed to obtain better tracking performance and effectively alleviate the computational burden. Furthermore, it is ensured that the tracking error falls into a preset performance funnel. A simulation example is presented to demonstrate the availability of the designed control scheme.},
  archive      = {J_TNNLS},
  author       = {Haibin Sun and Linlin Hou and Yunliang Wei},
  doi          = {10.1109/TNNLS.2022.3183290},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1364-1378},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized dynamic event-triggered output feedback adaptive fixed-time funnel control for interconnection nonlinear systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision–language navigation with beam-constrained global
normalization. <em>TNNLS</em>, <em>35</em>(1), 1352–1363. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision–language navigation (VLN) is a challenging task, which guides an agent to navigate in a realistic environment by natural language instructions. Sequence-to-sequence modeling is one of the most prospective architectures for the task, which achieves the agent navigation goal by a sequence of moving actions. The line of work has led to the state-of-the-art performance. Recently, several studies showed that the beam-search decoding during the inference can result in promising performance, as it ranks multiple candidate trajectories by scoring each trajectory as a whole. However, the trajectory-level score might be seriously biased during ranking. The score is a simple averaging of individual unit scores of the target-sequence actions, and these unit scores could be incomparable among different trajectories since they are calculated by a local discriminant classifier. To address this problem, we propose a global normalization strategy to rescale the scores at the trajectory level. Concretely, we present two global score functions to rerank all candidates in the output beam, resulting in more comparable trajectory scores. In this way, the bias problem can be greatly alleviated. We conduct experiments on the benchmark room-to-room (R2R) dataset of VLN to verify our method, and the results show that the proposed global method is effective, providing significant performance than the corresponding baselines. Our final model can achieve competitive performance on the VLN leaderboard.},
  archive      = {J_TNNLS},
  author       = {Liang Xie and Meishan Zhang and You Li and Wei Qin and Ye Yan and Erwei Yin},
  doi          = {10.1109/TNNLS.2022.3183287},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1352-1363},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Vision–Language navigation with beam-constrained global normalization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised autoregressive domain adaptation for time
series data. <em>TNNLS</em>, <em>35</em>(1), 1341–1351. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) has successfully addressed the domain shift problem for visual applications. Yet, these approaches may have limited performance for time series data due to the following reasons. First, they mainly rely on the large-scale dataset (i.e., ImageNet) for source pretraining, which is not applicable for time series data. Second, they ignore the temporal dimension on the feature space of the source and target domains during the domain alignment step. Finally, most of the prior UDA methods can only align the global features without considering the fine-grained class distribution of the target domain. To address these limitations, we propose a SeLf-supervised AutoRegressive Domain Adaptation (SLARDA) framework. In particular, we first design a self-supervised (SL) learning module that uses forecasting as an auxiliary task to improve the transferability of source features. Second, we propose a novel autoregressive domain adaptation technique that incorporates temporal dependence of both source and target features during domain alignment. Finally, we develop an ensemble teacher model to align class-wise distribution in the target domain via a confident pseudo labeling approach. Extensive experiments have been conducted on three real-world time series applications with 30 cross-domain scenarios. The results demonstrate that our proposed SLARDA method significantly outperforms the state-of-the-art approaches for time series domain adaptation. Our source code is available at: https://github.com/mohamedr002/SLARDA .},
  archive      = {J_TNNLS},
  author       = {Mohamed Ragab and Emadeldeen Eldele and Zhenghua Chen and Min Wu and Chee-Keong Kwoh and Xiaoli Li},
  doi          = {10.1109/TNNLS.2022.3183252},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1341-1351},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised autoregressive domain adaptation for time series data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A timestamp-based inertial best-response dynamics for
distributed nash equilibrium seeking in weakly acyclic games.
<em>TNNLS</em>, <em>35</em>(1), 1330–1340. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the problem of distributed game-theoretic learning in games with finite action sets. A timestamp-based inertial best-response dynamics is proposed for Nash equilibrium seeking by players over a communication network. We prove that if all players adhere to the dynamics, then the states of players will almost surely reach consensus and the joint action profile of players will be absorbed into a Nash equilibrium of the game. This convergence result is proven under the condition of weakly acyclic games and strongly connected networks. Furthermore, to encounter more general circumstances, such as games with graphical action sets, state-based games, and switching communication networks, several variants of the proposed dynamics and its convergent results are also developed. To demonstrate the validity and applicability, we apply the proposed timestamp-based learning dynamics to design distributed algorithms for solving some typical finite games, including the coordination games and congestion games.},
  archive      = {J_TNNLS},
  author       = {Shaolin Tan and Zhihong Fang and Yaonan Wang and Jinhu Lü},
  doi          = {10.1109/TNNLS.2022.3183250},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1330-1340},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A timestamp-based inertial best-response dynamics for distributed nash equilibrium seeking in weakly acyclic games},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved design for hardware implementation of graph-based
large margin classifiers for embedded edge computing. <em>TNNLS</em>,
<em>35</em>(1), 1320–1329. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of connected embedded edge computing Internet of Things (IoT) devices has been increasing over the years, contributing to the significant growth of available data in different scenarios. Thereby, machine learning algorithms arise to enable task automation and process optimization based on those data. However, due to some learning methods’ computational complexity implementing geometric classifiers, it is a challenge to map these on embedded systems or devices with limited resources in size, processing, memory, and power, to accomplish the desired requirements. This hampers the applicability of these methods to complex industrial embedded edge applications. This work evaluates strategies to reduce classifiers’ implementation costs based on the CHIP-clas model, independent of hyperparameter tuning and optimization algorithms. The proposal aims to evaluate the tradeoff between numerical precision and model performance and analyze the hardware implementations of a distance-based classifier. Two 16-b floating-point formats were compared to the 32-b floating-point precision implementation. Also, a new hardware architecture was developed and then compared to the state-of-the-art reference. The results indicate that the model is robust to low precision computation, providing statistically equivalent results compared to the baseline model, also pointing out statistically equivalent performance and a global speed-up factor of approx 4.39 in processing time.},
  archive      = {J_TNNLS},
  author       = {Janier Arias-Garcia and Alan Cândido de Souza and Liliane Gade and Jones Yudi and Frederico Coelho and Cristiano L. Castro and Luiz C. B. Torres and Antonio P. Braga},
  doi          = {10.1109/TNNLS.2022.3183236},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1320-1329},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improved design for hardware implementation of graph-based large margin classifiers for embedded edge computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shilling black-box recommender systems by learning to
generate fake user profiles. <em>TNNLS</em>, <em>35</em>(1), 1305–1319.
(<a href="https://doi.org/10.1109/TNNLS.2022.3183210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the pivotal role of recommender systems (RS) in guiding customers toward the purchase, there is a natural motivation for unscrupulous parties to spoof RS for profits. In this article, we study shilling attacks where an adversarial party injects a number of fake user profiles for improper purposes. Conventional Shilling Attack approaches lack attack transferability (i.e., attacks are not effective on some victim RS models) and/or attack invisibility (i.e., injected profiles can be easily detected). To overcome these issues, we present learning to generate fake user profiles (Leg-UP), a novel attack model based on the generative adversarial network. Leg-UP learns user behavior patterns from real users in the sampled “templates” and constructs fake user profiles. To simulate real users, the generator in Leg-UP directly outputs discrete ratings. To enhance attack transferability, the parameters of the generator are optimized by maximizing the attack performance on a surrogate RS model. To improve attack invisibility, Leg-UP adopts a discriminator to guide the generator to generate undetectable fake user profiles. Experiments on benchmarks have shown that Leg-UP exceeds state-of-the-art shilling attack methods on a wide range of victim RS models. The source code of our work is available at: https://github.com/XMUDM/ShillingAttack .},
  archive      = {J_TNNLS},
  author       = {Chen Lin and Si Chen and Meifang Zeng and Sheng Zhang and Min Gao and Hui Li},
  doi          = {10.1109/TNNLS.2022.3183210},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1305-1319},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Shilling black-box recommender systems by learning to generate fake user profiles},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exponential stability of impulsive timescale-type
nonautonomous neural networks with discrete time-varying and infinite
distributed delays. <em>TNNLS</em>, <em>35</em>(1), 1292–1304. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global exponential stability (GES) for impulsive timescale-type nonautonomous neural networks (ITNNNs) with mixed delays is investigated in this article. Discrete time-varying and infinite distributed delays (DTVIDDs) are taken into consideration. First, an improved timescale-type Halanay inequality is proven by timescale theory. Second, several algebraic inequality criteria are demonstrated by constructing impulse-dependent functions and utilizing timescale analytical techniques. Different from the published works, the theoretical results can be applied to GES for ITNNNs and impulsive stabilization design of timescale-type nonautonomous neural networks (TNNNs) with mixed delays. The improved timescale-type Halanay inequality considers time-varying coefficients and DTVIDDs, which improves and extends some existing ones. GES criteria for ITNNNs cover the stability conditions of discrete-time nonautonomous neural networks (NNs) and continuous-time ones, and these theoretical results hold for NNs with discrete-continuous dynamics. The effectiveness of our new theoretical results is verified by two numerical examples in the end.},
  archive      = {J_TNNLS},
  author       = {Peng Wan and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2022.3183195},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1292-1304},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential stability of impulsive timescale-type nonautonomous neural networks with discrete time-varying and infinite distributed delays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic ensemble selection for imbalanced data streams with
concept drift. <em>TNNLS</em>, <em>35</em>(1), 1278–1291. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning, as a popular method to tackle concept drift in data stream, forms a combination of base classifiers according to their global performances. However, concept drift generally occurs in local data space, causing significantly different performances of a base classifier at different locations. Thus, employing global performance as a criterion to select base classifier is inappropriate. Moreover, data stream is often accompanied by class imbalance problem, which affects the classification accuracy of ensemble learning on minority instances. To drawback these problems, a dynamic ensemble selection for imbalanced data streams with concept drift (DES-ICD) is proposed. For data arrived in chunk-by-chunk, a novel synthetic minority oversampling technique with adaptive nearest neighbors (AnnSMOTE) is developed to generate new minority instances that conform to the new concept. Following that, DES-ICD creates a base classifier on newly arrived data chunk balanced by AnnSMOTE and merges it with historical base classifiers to form a candidate classifier pool. For each query instance, the optimal combination is constructed in terms of the performance of candidate classifiers in its neighborhood. Experimental results for nine synthetic and five real-world datasets show that the proposed method outperforms seven comparative methods on classification accuracy and tracks new concepts in an imbalanced data stream more preciously.},
  archive      = {J_TNNLS},
  author       = {Botao Jiao and Yinan Guo and Dunwei Gong and Qiuju Chen},
  doi          = {10.1109/TNNLS.2022.3183120},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1278-1291},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic ensemble selection for imbalanced data streams with concept drift},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerated learning control for point-to-point tracking
systems. <em>TNNLS</em>, <em>35</em>(1), 1265–1277. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we investigate the accelerated learning control schemes for point-to-point tracking systems (PTSs) with measurement noise. The asymptotic convergence of the generated input sequence has been a long-standing open issue for point-to-point tracking problems because there are infinite possible input candidates that can drive the system dynamics to track the desired reference at specified time instants. An accelerated gradient algorithm and its generalized version with a novel direction regulation matrix are proposed, with the learning gain is adaptively triggered by the practical tracking errors. The learning gain remains constant at the early stage and begins to decrease after a certain number of iterations. The input sequence generated by the proposed scheme converges to a specified limit for any fixed initial input, with the limit being closest to the initial input, in a certain sense. Numerical simulations are provided to verify the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Hao Jiang and Dong Shen and Shunhao Huang and Xinghuo Yu},
  doi          = {10.1109/TNNLS.2022.3183109},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1265-1277},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accelerated learning control for point-to-point tracking systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InfoAT: Improving adversarial training using the information
bottleneck principle. <em>TNNLS</em>, <em>35</em>(1), 1255–1264. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training (AT) has shown excellent high performance in defending against adversarial examples. Recent studies demonstrate that examples are not equally important to the final robustness of models during AT, that is, the so-called hard examples that can be attacked easily exhibit more influence than robust examples on the final robustness. Therefore, guaranteeing the robustness of hard examples is crucial for improving the final robustness of the model. However, defining effective heuristics to search for hard examples is still difficult. In this article, inspired by the information bottleneck (IB) principle, we uncover that an example with high mutual information of the input and its associated latent representation is more likely to be attacked. Based on this observation, we propose a novel and effective adversarial training method (InfoAT). InfoAT is encouraged to find examples with high mutual information and exploit them efficiently to improve the final robustness of models. Experimental results show that InfoAT achieves the best robustness among different datasets and models in comparison with several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Mengting Xu and Tao Zhang and Zhongnian Li and Daoqiang Zhang},
  doi          = {10.1109/TNNLS.2022.3183095},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1255-1264},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {InfoAT: Improving adversarial training using the information bottleneck principle},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stack-propagation framework with slot filling for
multi-domain dialogue state tracking. <em>TNNLS</em>, <em>35</em>(1),
1240–1254. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dialogue state tracking (DST) is a core component of task-oriented dialogue systems. Recent works focus mainly on end-to-end DST models that omit the spoken language understanding (SLU) module to directly obtain the dialogue state based on a user’s dialogue. However, the slot information detected by slot filling in SLU is closely tied to the slot–value pair that needs to be updated in DST. Efficient use of the key slot semantic knowledge obtained by slot filling contributes to improving the performance of DST. Based on this idea, we introduce slot filling as a subtask and build an end-to-end joint model to explicitly integrate the slot information detected by slot filling, which further guides DST. In this article, a novel stack-propagation framework with slot filling for multidomain DST is proposed. The stack-propagation framework is introduced to jointly model slot filling and DST. The framework directly feeds the key slot semantic knowledge detected by slot filling into the DST module. In addition, a slot-masked attention mechanism is designed to enable DST to focus on the key slot information obtained by slot filling. When the slot value is updated, a slot–value softcopy mechanism is designed to enhance the influence of the words marked by key slots. Experiments show that our approach outperforms previous methods and performs outstandingly on two benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Yufan Wang and Tingting He and Jie Mei and Rui Fan and Xinhui Tu},
  doi          = {10.1109/TNNLS.2022.3183081},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1240-1254},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A stack-propagation framework with slot filling for multi-domain dialogue state tracking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A contextual relationship model for deceptive opinion spam
detection. <em>TNNLS</em>, <em>35</em>(1), 1228–1239. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The promotion of e-commerce platforms has changed the lifestyle of several people from traditional marketing to digital marketing where businesses are made online and the concurrence reached high levels. These platforms have helped the ease of purchases while providing more advantages to the customers such as benefiting from a wide range of high-quality products, low prices, buying at any time, and more importantly supplying information and reviews about the products, and so on. Unfortunately, a plethora of companies mislead the customers to buy their products or demote the competitors’ by using deceptive opinion spams which has a negative impact on the decision and the behavior of the purchasers. Deceptive opinion spams are written deliberately to seem legitimate and authentic so that to misguide or delude the customer’s purchases. Consequently, the detection of these opinions is a hard task due to their nature for both humans and machines. Most of the studies are based on traditional machine learning and sparse feature engineering. However, these models do not capture the semantic aspect of reviews. According to many researchers, it is the key to the detection of deceptive opinion spam. Besides, only a few studies consider using contextual information by adopting neural networks in comparison with plenty of traditional machine learning classifiers. These models face numerous shortcomings as long as their representations are obtained while mining each review considering only words, sentences, reviews, or a combination of them, thereby classifying them based on their representations. In fact, deceptive opinions are written by the same deceivers belonging to the same companies with similar aims to promote or demolish a product. In other words, Deceptive opinion spams tend to be semantically coherent with each other. To the best of our knowledge, no model tries to obtain a representation based on the contextual relationships between opinions. This article proposes to use a capsule neural network, bidirectional long short-term memory, attention mechanism, and paragraph vector distributed bag of words to detect deceptive opinion spam. Our model provides a powerful representation of the opinions since it centers on the preservation of their contexts and the relationships between them. The results show that our model significantly outperforms the existing state-of-the-art models.},
  archive      = {J_TNNLS},
  author       = {Anass Fahfouh and Jamal Riffi and Mohamed Adnane Mahraz and Ali Yahyaouy and Hamid Tairi},
  doi          = {10.1109/TNNLS.2022.3183037},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1228-1239},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A contextual relationship model for deceptive opinion spam detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive neural network stochastic-filter-based controller
for attitude tracking with disturbance rejection. <em>TNNLS</em>,
<em>35</em>(1), 1217–1227. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a real-time neural network (NN) stochastic filter-based controller on the Lie group of the special orthogonal group SO(3) as a novel approach to the attitude tracking problem. The introduced solution consists of two parts: a filter and a controller. First, an adaptive NN-based stochastic filter is proposed, which estimates attitude components and dynamics using measurements supplied by onboard sensors directly. The filter design accounts for measurement uncertainties inherent to the attitude dynamics, namely, unknown bias and noise corrupting angular velocity measurements. The closed-loop signals of the proposed NN-based stochastic filter have been shown to be semiglobally uniformly ultimately bounded (SGUUB). Second, a novel control law on SO(3) coupled with the proposed estimator is presented. The control law addresses unknown disturbances. In addition, the closed-loop signals of the proposed filter-based controller have been shown to be SGUUB. The proposed approach offers robust tracking performance by supplying the required control signal given data extracted from low-cost inertial measurement units. While the filter-based controller is presented in continuous form, the discrete implementation is also presented. In addition, the unit-quaternion form of the proposed approach is given. The effectiveness and robustness of the proposed filter-based controller are demonstrated using its discrete form and considering low sampling rate, high initialization error, high level of measurement uncertainties, and unknown disturbances.},
  archive      = {J_TNNLS},
  author       = {Hashim A. Hashim and Kyriakos G. Vamvoudakis},
  doi          = {10.1109/TNNLS.2022.3183026},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1217-1227},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network stochastic-filter-based controller for attitude tracking with disturbance rejection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernelized deep learning for matrix factorization
recommendation system using explicit and implicit information.
<em>TNNLS</em>, <em>35</em>(1), 1205–1216. (<a
href="https://doi.org/10.1109/TNNLS.2022.3182942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current matrix factorization recommendation approaches, the item and the user latent factor vectors are with the same dimension. Thus, the linear dot product is used as the interactive function between the user and the item to predict the ratings. However, the relationship between real users and items is not entirely linear and the existing recommendation model of matrix factorization faces the challenge of data sparsity. To this end, we propose a kernelized deep neural network recommendation model in this article. First, we encode the explicit user—item rating matrix in the form of column vectors and project them to higher dimensions to facilitate the simulation of nonlinear user—item interaction for enhancing the connection between users and items. Second, the algorithm of association rules is used to mine the implicit relation between users and items, rather than simple feature extraction of users or items, for improving the recommendation performance when the datasets are sparse. Third, through the autoencoder and kernelized network processing, the implicit data are connected with the explicit data by the multilayer perceptron network for iterative training instead of doing simple linear weighted summation. Finally, the predicted rating is output through the hidden layer. Extensive experiments were conducted on four public datasets in comparison with several existing well-known methods. The experimental results indicated that our proposed method has obtained improved performance in data sparsity and prediction accuracy.},
  archive      = {J_TNNLS},
  author       = {Xiaoyao Zheng and Zhen Ni and Xiangnan Zhong and Yonglong Luo},
  doi          = {10.1109/TNNLS.2022.3182942},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1205-1216},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Kernelized deep learning for matrix factorization recommendation system using explicit and implicit information},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic motion prediction and skill learning for
human-to-cobot dual-arm handover control. <em>TNNLS</em>,
<em>35</em>(1), 1192–1204. (<a
href="https://doi.org/10.1109/TNNLS.2022.3182973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we focus on human-to-cobot dual-arm handover operations for large box-type objects. The efficiency of handover operations should be ensured and the naturalness as if the handover is going on between two humans. First of all, we study the human–human dual-arm large box-type object natural handover process to guide this research. Then, for efficiency, we combine the probabilistic approach with the online learning algorithm to predict the beginning of the handover task and handover positions. The online updating probabilistic models can deal with not only human givers’ regular motion patterns but also their irregular motion patterns. Then, to guarantee that human givers can perform handover operations naturally, we apply the probabilistic robot skill learning method kernelized movement primitives (KMPs) to adapt the learned receiving skills and fulfill some constraints for safety based on online predicted results. Furthermore, we give special attention to the dual-arm grasp strategy and control design to guarantee a stable grasp. In addition, we equip this handover system on a Baxter cobot and extend its grippers to make it more suitable for dual-arm handover operations. The experimental results show that the proposed handover system can solve human-to-cobot dual-arm handover operations for large box-type objects naturally and efficiently.},
  archive      = {J_TNNLS},
  author       = {Zichen Yan and Wei He and Yuanhang Wang and Liang Sun and Xinbo Yu},
  doi          = {10.1109/TNNLS.2022.3182973},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1192-1204},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic motion prediction and skill learning for human-to-cobot dual-arm handover control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reduced-order neural network synthesis with robustness
guarantees. <em>TNNLS</em>, <em>35</em>(1), 1182–1191. (<a
href="https://doi.org/10.1109/TNNLS.2022.3182893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the wake of the explosive growth in smartphones and cyber-physical systems, there has been an accelerating shift in how data are generated away from centralized data toward on-device-generated data. In response, machine learning algorithms are being adapted to run locally on board, potentially hardware-limited, devices to improve user privacy, reduce latency, and be more energy efficient. However, our understanding of how these device-orientated algorithms behave and should be trained is still fairly limited. To address this issue, a method to automatically synthesize reduced-order neural networks (having fewer neurons) approximating the input–output mapping of a larger one is introduced. The reduced-order neural network’s weights and biases are generated from a convex semidefinite program that minimizes the worst case approximation error with respect to the larger network. Worst case bounds for this approximation error are obtained and the approach can be applied to a wide variety of neural networks architectures. What differentiates the proposed approach to existing methods for generating small neural networks, e.g., pruning, is the inclusion of the worst case approximation error directly within the training cost function, which should add robustness to out-of-sample data points. Numerical examples highlight the potential of the proposed approach. The overriding goal of this article is to generalize recent results in the robustness analysis of neural networks to a robust synthesis problem for their weights and biases.},
  archive      = {J_TNNLS},
  author       = {Ross Drummond and Matthew C. Turner and Stephen R. Duncan},
  doi          = {10.1109/TNNLS.2022.3182893},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1182-1191},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reduced-order neural network synthesis with robustness guarantees},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Outperforming RBM feature-extraction capabilities by
“dreaming” mechanism. <em>TNNLS</em>, <em>35</em>(1), 1172–1181. (<a
href="https://doi.org/10.1109/TNNLS.2022.3182882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by a formal equivalence between the Hopfield model and restricted Boltzmann machines (RBMs), we design a Boltzmann machine, referred to as the dreaming Boltzmann machine (DBM), which achieves better performances than the standard one. The novelty in our model lies in a precise prescription for intralayer connections among hidden neurons whose strengths depend on features correlations. We analyze learning and retrieving capabilities in DBMs, both theoretically and numerically, and compare them to the RBM reference. We find that, in a supervised scenario, the former significantly outperforms the latter. Furthermore, in the unsupervised case, the DBM achieves better performances both in features extraction and representation learning, especially when the network is properly pretrained. Finally, we compare both models in simple classification tasks and find that the DBM again outperforms the RBM reference.},
  archive      = {J_TNNLS},
  author       = {Alberto Fachechi and Adriano Barra and Elena Agliari and Francesco Alemanno},
  doi          = {10.1109/TNNLS.2022.3182882},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1172-1181},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Outperforming RBM feature-extraction capabilities by “Dreaming” mechanism},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-in-graph convolutional network for hyperspectral image
classification. <em>TNNLS</em>, <em>35</em>(1), 1157–1171. (<a
href="https://doi.org/10.1109/TNNLS.2022.3182715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of hyperspectral sensors, accessible hyperspectral images (HSIs) are increasing, and pixel-oriented classification has attracted much attention. Recently, graph convolutional networks (GCNs) have been proposed to process graph-structured data in non-Euclidean domains and have been employed in HSI classification. But most methods based on GCN are hard to sufficiently exploit information of ground objects due to feature aggregation. To solve this issue, in this article, we proposed a graph-in-graph (GiG) model and a related GiG convolutional network (GiGCN) for HSI classification from a superpixel viewpoint. The GiG representation covers information inside and outside superpixels, respectively, corresponding to the local and global characteristics of ground objects. Concretely, after segmenting HSI into disjoint superpixels, each one is converted to an internal graph. Meanwhile, an external graph is constructed according to the spatial adjacent relationships among superpixels. Significantly, each node in the external graph embeds a corresponding internal graph, forming the so-called GiG structure. Then, GiGCN composed of internal and External graph convolution (EGC) is designed to extract hierarchical features and integrate them into multiple scales, improving the discriminability of GiGCN. Ensemble learning is incorporated to further boost the robustness of GiGCN. It is worth noting that we are the first to propose the GiG framework from the superpixel point and the GiGCN scheme for HSI classification. Experiment results on four benchmark datasets demonstrate that our proposed method is effective and feasible for HSI classification with limited labeled samples. For study replication, the code developed for this study is available at https://github.com/ShuGuoJ/GiGCN.git .},
  archive      = {J_TNNLS},
  author       = {Sen Jia and Shuguo Jiang and Shuyu Zhang and Meng Xu and Xiuping Jia},
  doi          = {10.1109/TNNLS.2022.3182715},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1157-1171},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-in-graph convolutional network for hyperspectral image classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Tensor recovery with weighted tensor average rank.
<em>TNNLS</em>, <em>35</em>(1), 1142–1156. (<a
href="https://doi.org/10.1109/TNNLS.2022.3182541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a curious phenomenon in the tensor recovery algorithm is considered: can the same recovered results be obtained when the observation tensors in the algorithm are transposed in different ways? If not, it is reasonable to imagine that some information within the data will be lost for the case of observation tensors under certain transpose operators. To solve this problem, a new tensor rank called weighted tensor average rank (WTAR) is proposed to learn the relationship between different resulting tensors by performing a series of transpose operators on an observation tensor. WTAR is applied to three-order tensor robust principal component analysis (TRPCA) to investigate its effectiveness. Meanwhile, to balance the effectiveness and solvability of the resulting model, a generalized model that involves the convex surrogate and a series of nonconvex surrogates are studied, and the corresponding worst case error bounds of the recovered tensor is given. Besides, a generalized tensor singular value thresholding (GTSVT) method and a generalized optimization algorithm based on GTSVT are proposed to solve the generalized model effectively. The experimental results indicate that the proposed method is effective.},
  archive      = {J_TNNLS},
  author       = {Xiaoqin Zhang and Jingjing Zheng and Li Zhao and Zhengyuan Zhou and Zhouchen Lin},
  doi          = {10.1109/TNNLS.2022.3182541},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1142-1156},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tensor recovery with weighted tensor average rank},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noisy tensor completion via low-rank tensor ring.
<em>TNNLS</em>, <em>35</em>(1), 1127–1141. (<a
href="https://doi.org/10.1109/TNNLS.2022.3181378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor completion is a fundamental tool for incomplete data analysis, where the goal is to predict missing entries from partial observations. However, existing methods often make the explicit or implicit assumption that the observed entries are noise-free to provide a theoretical guarantee of exact recovery of missing entries, which is quite restrictive in practice. To remedy such drawback, this article proposes a novel noisy tensor completion model, which complements the incompetence of existing works in handling the degeneration of high-order and noisy observations. Specifically, the tensor ring nuclear norm (TRNN) and least-squares estimator are adopted to regularize the underlying tensor and the observed entries, respectively. In addition, a nonasymptotic upper bound of estimation error is provided to depict the statistical performance of the proposed estimator. Two efficient algorithms are developed to solve the optimization problem with convergence guarantee, one of which is specially tailored to handle large-scale tensors by replacing the minimization of TRNN of the original tensor equivalently with that of a much smaller one in a heterogeneous tensor decomposition framework. Experimental results on both synthetic and real-world data demonstrate the effectiveness and efficiency of the proposed model in recovering noisy incomplete tensor data compared with state-of-the-art tensor completion models.},
  archive      = {J_TNNLS},
  author       = {Yuning Qiu and Guoxu Zhou and Qibin Zhao and Shengli Xie},
  doi          = {10.1109/TNNLS.2022.3181378},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1127-1141},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Noisy tensor completion via low-rank tensor ring},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective streaming low-tubal-rank tensor approximation via
frequent directions. <em>TNNLS</em>, <em>35</em>(1), 1113–1126. (<a
href="https://doi.org/10.1109/TNNLS.2022.3181097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-tubal-rank tensor approximation has been proposed to analyze large-scale and multidimensional data. However, finding such an accurate approximation is challenging in the streaming setting, due to the limited computational resources. To alleviate this issue, this article extends a popular matrix sketching technique, namely, frequent directions (FDs), for constructing an efficient and accurate low-tubal-rank tensor approximation from streaming data based on the tensor singular value decomposition (t-SVD). Specifically, the new algorithm allows the tensor data to be observed slice by slice but only needs to maintain and incrementally update a much smaller sketch, which could capture the principal information of the original tensor. The rigorous theoretical analysis shows that the approximation error of the new algorithm can be arbitrarily small when the sketch size grows linearly. Extensive experimental results on both synthetic and real multidimensional data further reveal the superiority of the proposed algorithm compared with other sketching algorithms for getting low-tubal-rank approximation, in terms of both efficiency and accuracy.},
  archive      = {J_TNNLS},
  author       = {Qianxin Yi and Chenhao Wang and Kaidong Wang and Yao Wang},
  doi          = {10.1109/TNNLS.2022.3181097},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1113-1126},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effective streaming low-tubal-rank tensor approximation via frequent directions},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spartus: A 9.4 TOp/s FPGA-based LSTM accelerator exploiting
spatio-temporal sparsity. <em>TNNLS</em>, <em>35</em>(1), 1098–1112. (<a
href="https://doi.org/10.1109/TNNLS.2022.3180209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long short-term memory (LSTM) recurrent networks are frequently used for tasks involving time-sequential data, such as speech recognition. Unlike previous LSTM accelerators that either exploit spatial weight sparsity or temporal activation sparsity, this article proposes a new accelerator called “Spartus” that exploits spatio-temporal sparsity to achieve ultralow latency inference. Spatial sparsity is induced using a new column-balanced targeted dropout (CBTD) structured pruning method, producing structured sparse weight matrices for a balanced workload. The pruned networks running on Spartus hardware achieve weight sparsity levels of up to 96% and 94% with negligible accuracy loss on the TIMIT and the Librispeech datasets. To induce temporal sparsity in LSTM, we extend the previous DeltaGRU method to the DeltaLSTM method. Combining spatio-temporal sparsity with CBTD and DeltaLSTM saves on weight memory access and associated arithmetic operations. The Spartus architecture is scalable and supports real-time online speech recognition when implemented on small and large FPGAs. Spartus per-sample latency for a single DeltaLSTM layer of 1024 neurons averages 1 $\mu \text{s}$ . Exploiting spatio-temporal sparsity on our test LSTM network using the TIMIT dataset leads to $46\times $ speedup of Spartus over its theoretical hardware performance to achieve 9.4-TOp/s effective batch-1 throughput and 1.1-TOp/s/W power efficiency.},
  archive      = {J_TNNLS},
  author       = {Chang Gao and Tobi Delbruck and Shih-Chii Liu},
  doi          = {10.1109/TNNLS.2022.3180209},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1098-1112},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spartus: A 9.4 TOp/s FPGA-based LSTM accelerator exploiting spatio-temporal sparsity},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intermittent sampled-data control for local stabilization of
neural networks subject to actuator saturation: A
work-interval-dependent functional approach. <em>TNNLS</em>,
<em>35</em>(1), 1087–1097. (<a
href="https://doi.org/10.1109/TNNLS.2022.3180076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the local stabilization of neural networks (NNs) under intermittent sampled-data control (ISC) subject to actuator saturation. The issue is presented for two reasons: 1) the control input and the network bandwidth are always limited in practical engineering applications and 2) the existing analysis methods cannot handle the effect of the saturation nonlinearity and the ISC simultaneously. To overcome these difficulties, a work-interval-dependent Lyapunov functional is developed for the resulting closed-loop system, which is piecewise-defined, time-dependent, and also continuous. The main advantage of the proposed functional is that the information over the work interval is utilized. Based on the developed Lyapunov functional, the constraints on the basin of attraction (BoA) and the Lyapunov matrices are dropped. Then, using the generalized sector condition and the Lyapunov stability theory, two sufficient criteria for local exponential stability of the closed-loop system are developed. Moreover, two optimization strategies are put forward with the aim of enlarging the BoA and minimizing the actuator cost. Finally, two numerical examples are provided to exemplify the feasibility and reliability of the derived theoretical results.},
  archive      = {J_TNNLS},
  author       = {Yanyan Ni and Zhen Wang and Xia Huang and Qian Ma and Hao Shen},
  doi          = {10.1109/TNNLS.2022.3180076},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1087-1097},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intermittent sampled-data control for local stabilization of neural networks subject to actuator saturation: A work-interval-dependent functional approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-based robust optimal consensus control for nonlinear
multiagent system with local adaptive dynamic programming.
<em>TNNLS</em>, <em>35</em>(1), 1073–1086. (<a
href="https://doi.org/10.1109/TNNLS.2022.3180054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the robust optimal consensus for nonlinear multiagent systems (MASs) through the local adaptive dynamic programming (ADP) approach and the event-triggered control method. Due to the nonlinearities in dynamics, the first part defines a novel measurement error to construct a distributed integral sliding-mode controller, and the consensus errors can approximately converge to the origin in a fixed time. Then, a modified cost function with augmented control is proposed to deal with the unmatched disturbances for the event-based optimal consensus controller. Specifically, a single network local ADP structure with novel concurrent learning is presented to approximate the optimal consensus policies, which guarantees the robustness of the MASs and the uniform ultimate boundedness (UUB) of the neural network (NN) weights’ estimation error and relaxes the requirement of initial admissible control. Finally, an illustrative simulation verifies the effectiveness of the method.},
  archive      = {J_TNNLS},
  author       = {Jie Wang and Zitao Zhang and Bailing Tian and Qun Zong},
  doi          = {10.1109/TNNLS.2022.3180054},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1073-1086},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-based robust optimal consensus control for nonlinear multiagent system with local adaptive dynamic programming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new progressive multisource domain adaptation network with
weighted decision fusion. <em>TNNLS</em>, <em>35</em>(1), 1062–1072. (<a
href="https://doi.org/10.1109/TNNLS.2022.3179805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multisource unsupervised domain adaptation (MUDA) is an important and challenging topic for target classification with the assistance of labeled data in source domains. When we have several labeled source domains, it is difficult to map all source domains and target domain into a common feature space for classifying the targets well. In this article, a new progressive multisource domain adaptation network (PMSDAN) is proposed to further improve the classification performance. PMSDAN mainly consists of two steps for distribution alignment. First, the multiple source domains are integrated as one auxiliary domain to match the distribution with the target domain. By doing this, we can generally reduce the distribution discrepancy between each source and target domains, as well as the discrepancy between different source domains. It can efficiently explore useful knowledge from the integrated source domain. Second, to mine assistance knowledge from each source domain as much as possible, the distribution of the target domain is separately aligned with that of each source domain. A weighted fusion method is employed to combine the multiple classification results for making the final decision. In the optimization of domain adaption, weighted hybrid maximum mean discrepancy (WHMMD) is proposed, and it considers both the interclass and intraclass discrepancies. The effectiveness of the proposed PMSDAN is demonstrated in the experiments comparing with some state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zhun-Ga Liu and Liang-Bo Ning and Zuo-Wei Zhang},
  doi          = {10.1109/TNNLS.2022.3179805},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1062-1072},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A new progressive multisource domain adaptation network with weighted decision fusion},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matching distributions algorithms based on the earth mover’s
distance for ordinal quantification. <em>TNNLS</em>, <em>35</em>(1),
1050–1061. (<a
href="https://doi.org/10.1109/TNNLS.2022.3179355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of quantification learning is to induce models capable of accurately predicting the class distribution for new bags of unseen examples. These models only return the prevalence of each class in the bag because prediction of individual examples is irrelevant in these tasks. A prototypical application of ordinal quantification is to predict the proportion of opinions that fall into each category from one to five stars. Ordinal quantification has hardly been studied in the literature, and in fact, only one approach has been proposed so far. This article presents a comprehensive study of ordinal quantification, analyzing the applicability of the most important algorithms devised for multiclass quantification and proposing three new methods that are based on matching distributions using Earth mover’s distance (EMD). Empirical experiments compare 14 algorithms on synthetic and benchmark data. To statistically analyze the obtained results, we further introduce an EMD-based scoring function. The main conclusion is that methods using a criterion somehow related to EMD, including two of our proposals, obtain significantly better results.},
  archive      = {J_TNNLS},
  author       = {Alberto Castaño and Pablo González and Jaime Alonso González and Juan José del Coz},
  doi          = {10.1109/TNNLS.2022.3179355},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1050-1061},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Matching distributions algorithms based on the earth mover’s distance for ordinal quantification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite-time synchronization of complex dynamical networks
via a novel hybrid controller. <em>TNNLS</em>, <em>35</em>(1),
1040–1049. (<a
href="https://doi.org/10.1109/TNNLS.2022.3185490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of finite-time synchronization (FTS) of complex dynamical networks (CDNs) is investigated in this article. A new control strategy coupling weak finite-time control and finite times of impulsive control is proposed to realize the FTS of CDNs, where the impulses are synchronizing and restricted by maximal impulsive interval (MII), differing from the existing results. In this framework, several global and local FTS criteria are established by using the concept of impulsive degree. The times of impulsive control in the controllers and the settling time, which are all dependent on initial values, are derived optimally. A technical lemma is developed, reflecting the core idea of this article. A simulation example is given to demonstrate the main results finally.},
  archive      = {J_TNNLS},
  author       = {Qiang Xi and Xinzhi Liu and Xiaodi Li},
  doi          = {10.1109/TNNLS.2022.3185490},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1040-1049},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time synchronization of complex dynamical networks via a novel hybrid controller},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NCGNN: Node-level capsule graph neural network for
semisupervised classification. <em>TNNLS</em>, <em>35</em>(1),
1025–1039. (<a
href="https://doi.org/10.1109/TNNLS.2022.3179306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Message passing has evolved as an effective tool for designing graph neural networks (GNNs). However, most existing methods for message passing simply sum or average all the neighboring features to update node representations. They are restricted by two problems: 1) lack of interpretability to identify node features significant to the prediction of GNNs and 2) feature overmixing that leads to the oversmoothing issue in capturing long-range dependencies and inability to handle graphs under heterophily or low homophily. In this article, we propose a node-level capsule graph neural network (NCGNN) to address these problems with an improved message passing scheme. Specifically, NCGNN represents nodes as groups of node-level capsules, in which each capsule extracts distinctive features of its corresponding node. For each node-level capsule, a novel dynamic routing procedure is developed to adaptively select appropriate capsules for aggregation from a subgraph identified by the designed graph filter. NCGNN aggregates only the advantageous capsules and restrains irrelevant messages to avoid overmixing features of interacting nodes. Therefore, it can relieve the oversmoothing issue and learn effective node representations over graphs with homophily or heterophily. Furthermore, our proposed message passing scheme is inherently interpretable and exempt from complex post hoc explanations, as the graph filter and the dynamic routing procedure identify a subset of node features that are most significant to the model prediction from the extracted subgraph. Extensive experiments on synthetic as well as real-world graphs demonstrate that NCGNN can well address the oversmoothing issue and produce better node representations for semisupervised node classification. It outperforms the state of the arts under both homophily and heterophily.},
  archive      = {J_TNNLS},
  author       = {Rui Yang and Wenrui Dai and Chenglin Li and Junni Zou and Hongkai Xiong},
  doi          = {10.1109/TNNLS.2022.3179306},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1025-1039},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {NCGNN: Node-level capsule graph neural network for semisupervised classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anchor association learning for unsupervised video person
re-identification. <em>TNNLS</em>, <em>35</em>(1), 1013–1024. (<a
href="https://doi.org/10.1109/TNNLS.2022.3179133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based person re-identification (re-id) has attracted a significant attention in recent years due to the increasing demand of video surveillance. However, existing methods are usually based on the supervised learning, which requires vast labeled identities across cameras and is not suitable for real scenes. Although some unsupervised approaches have been proposed for video re-id, their performance is far from satisfactory. In this article, we propose an unsupervised anchor association learning (UAAL) framework to address the video-based person re-id task, in which the feature representation of each sampled tracklet is regarded as an anchor. Specifically, we first propose an intracamera anchor association learning (IAAL) term that learns the discriminative anchor by utilizing the affiliation relations between an image and the anchors in each camera. Then, the exponential moving average (EMA) strategy is employed to update the anchor and the updated anchors are stored into an anchor memory module. On top of that, a cross-camera anchor association learning (CAAL) term is introduced to mine potential positive anchor pairs across cameras by presenting a cyclic ranking anchor alignment and threshold filtering method. Extensive experiments conducted on two public datasets show the superiority of the proposed method; for example, our method achieves 73.2% for rank-1 accuracy and 60.1% for mean average precision (mAP) score, respectively, on MARS, similarly 89.7% and 87.0% on DukeMTMC-VideoReID.},
  archive      = {J_TNNLS},
  author       = {Shujun Zeng and Xueping Wang and Min Liu and Qing Liu and Yaonan Wang},
  doi          = {10.1109/TNNLS.2022.3179133},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {1013-1024},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Anchor association learning for unsupervised video person re-identification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-free optimal tracking control of nonlinear
input-affine discrete-time systems via an iterative deterministic
q-learning algorithm. <em>TNNLS</em>, <em>35</em>(1), 999–1012. (<a
href="https://doi.org/10.1109/TNNLS.2022.3178746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel model-free dynamic inversion-based Q-learning (DIQL) algorithm is proposed to solve the optimal tracking control (OTC) problem of unknown nonlinear input-affine discrete-time (DT) systems. Compared with the existing DIQL algorithm and the discount factor-based Q-learning (DFQL) algorithm, the proposed algorithm can eliminate the tracking error while ensuring that it is model-free and off-policy. First, a new deterministic Q-learning iterative scheme is presented, and based on this scheme, a model-based off-policy DIQL algorithm is designed. The advantage of this new scheme is that it can avoid the training of unusual data and improve data utilization, thereby saving computing resources. Simultaneously, the convergence and stability of the designed algorithm are analyzed, and the proof that adding probing noise into the behavior policy does not affect the convergence is presented. Then, by introducing neural networks (NNs), the model-free version of the designed algorithm is further proposed so that the OTC problem can be solved without any knowledge about the system dynamics. Finally, three simulation examples are given to demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Shijie Song and Minglei Zhu and Xiaolin Dai and Dawei Gong},
  doi          = {10.1109/TNNLS.2022.3178746},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {999-1012},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-free optimal tracking control of nonlinear input-affine discrete-time systems via an iterative deterministic Q-learning algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DynG2G: An efficient stochastic graph embedding method for
temporal graphs. <em>TNNLS</em>, <em>35</em>(1), 985–998. (<a
href="https://doi.org/10.1109/TNNLS.2022.3178706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic graph embedding has gained great attention recently due to its capability of learning low-dimensional and meaningful graph representations for complex temporal graphs with high accuracy. However, recent advances mostly focus on learning node embeddings as deterministic “vectors” for static graphs, hence disregarding the key graph temporal dynamics and the evolving uncertainties associated with node embedding in the latent space. In this work, we propose an efficient stochastic dynamic graph embedding method (DynG2G) that applies an inductive feedforward encoder trained with node triplet energy-based ranking loss. Every node per timestamp is encoded as a time-dependent probabilistic multivariate Gaussian distribution in the latent space, and, hence, we are able to quantify the node embedding uncertainty on-the-fly. We have considered eight different benchmarks that represent diversity in size (from 96 nodes to 87626 and from 13398 edges to 4870863) as well as diversity in dynamics, from slowly changing temporal evolution to rapidly varying multirate dynamics. We demonstrate through extensive experiments based on these eight dynamic graph benchmarks that DynG2G achieves new state-of-the-art performance in capturing the underlying temporal node embeddings. We also demonstrate that DynG2G can simultaneously predict the evolving node embedding uncertainty, which plays a crucial role in quantifying the intrinsic dimensionality of the dynamical system over time. In particular, we obtain a “universal” relation of the optimal embedding dimension, $L_{o}$ , versus the effective dimensionality of uncertainty, $D_{u}$ , and infer that $L_{o}=D_{u}$ for all cases. This, in turn, implies that the uncertainty quantification approach we employ in the DynG2G algorithm correctly captures the intrinsic dimensionality of the dynamics of such evolving graphs despite the diverse nature and composition of the graphs at each timestamp. In addition, this $L_{0} - D_{u}$ correlation provides a clear path to selecting adaptively the optimum embedding size at each timestamp by setting $L \ge D_{u}$ .},
  archive      = {J_TNNLS},
  author       = {Mengjia Xu and Apoorva Vikram Singh and George Em Karniadakis},
  doi          = {10.1109/TNNLS.2022.3178706},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {985-998},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DynG2G: An efficient stochastic graph embedding method for temporal graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware attentive multilevel feature fusion for named
entity recognition. <em>TNNLS</em>, <em>35</em>(1), 973–984. (<a
href="https://doi.org/10.1109/TNNLS.2022.3178522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of information explosion, named entity recognition (NER) has attracted widespread attention in the field of natural language processing, as it is fundamental to information extraction. Recently, methods of NER based on representation learning, e.g., character embedding and word embedding, have demonstrated promising recognition results. However, existing models only consider partial features derived from words or characters while failing to integrate semantic and syntactic information, e.g., capitalization, inter-word relations, keywords, and lexical phrases, from multilevel perspectives. Intuitively, multilevel features can be helpful when recognizing named entities from complex sentences. In this study, we propose a novel attentive multilevel feature fusion (AMFF) model for NER, which captures the multilevel features in the current context from various perspectives. It consists of four components to, respectively, capture the local character-level (CL), global character-level (CG), local word-level (WL), and global word-level (WG) features in the current context. In addition, we further define document-level features crafted from other sentences to enhance the representation learning of the current context. To this end, we introduce a novel context-aware attentive multilevel feature fusion (CAMFF) model based on AMFF, to fully leverage document-level features from all the previous inputs. The obtained multilevel features are then fused and fed into a bidirectional long short-term memory (BiLSTM)-conditional random field (CRF) network for the final sequence labeling. Extensive experiments on four benchmark datasets demonstrate that our proposed AMFF and CAMFF models outperform a set of state-of-the-art baseline methods and the features learned from multiple levels are complementary.},
  archive      = {J_TNNLS},
  author       = {Zhiwei Yang and Jing Ma and Hechang Chen and Jiawei Zhang and Yi Chang},
  doi          = {10.1109/TNNLS.2022.3178522},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {973-984},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Context-aware attentive multilevel feature fusion for named entity recognition},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced spatial feature learning for weakly supervised
object detection. <em>TNNLS</em>, <em>35</em>(1), 961–972. (<a
href="https://doi.org/10.1109/TNNLS.2022.3178180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection (WSOD) has become an effective paradigm, which requires only class labels to train object detectors. However, WSOD detectors are prone to learn highly discriminative features corresponding to local objects rather than complete objects, resulting in imprecise object localization. To address the issue, designing backbones specifically for WSOD is a feasible solution. However, the redesigned backbone generally needs to be pretrained on large-scale ImageNet or trained from scratch, both of which require much more time and computational costs than fine-tuning. In this article, we explore to optimize the backbone without losing the availability of the original pretrained model. Since the pooling layer summarizes neighborhood features, it is crucial to spatial feature learning. In addition, it has no learnable parameters, so its modification will not change the pretrained model. Based on the above analysis, we further propose enhanced spatial feature learning (ESFL) for WSOD, which first takes full advantage of multiple kernels in a single pooling layer to handle multiscale objects and then enhances above-average activations within the rectangular neighborhood to alleviate the problem of ignoring unsalient object parts. The experimental results on the PASCAL VOC and the MS COCO benchmarks demonstrate that ESFL can bring significant performance improvement for the WSOD method and achieve state-of-the-art results.},
  archive      = {J_TNNLS},
  author       = {Zhihao Wu and Jie Wen and Yong Xu and Jian Yang and Xuelong Li and David Zhang},
  doi          = {10.1109/TNNLS.2022.3178180},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {961-972},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhanced spatial feature learning for weakly supervised object detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed localization for multi-agent systems with random
noise based on iterative learning. <em>TNNLS</em>, <em>35</em>(1),
952–960. (<a href="https://doi.org/10.1109/TNNLS.2022.3178077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the real-time localization problem for the dynamic multi-agent systems with measurement and communication noises under directed graphs. The barycentric coordinates are introduced to describe the relative position between agents. A novel robust distributed localization estimation algorithm based on iterative learning is proposed. The relative-distance unbiased estimator constructed from the historical iterative information is used to suppress the measurement noise. The designed stochastic approximation method with two iterative-varying gains is used to inhibit the communication noise. Under the zero-mean and independent distributed conditions on the measurement and communication noises, the asymptotic convergence of the proposed methods is derived. The numerical simulation and the QBot-2e robot experiment are conducted to test and verify the effectiveness and the practicability of the proposed methods.},
  archive      = {J_TNNLS},
  author       = {Yunkai Lv and Hao Zhang and Zhuping Wang and Huaicheng Yan},
  doi          = {10.1109/TNNLS.2022.3178077},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {952-960},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed localization for multi-agent systems with random noise based on iterative learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal feature selection with dual correction.
<em>TNNLS</em>, <em>35</em>(1), 938–951. (<a
href="https://doi.org/10.1109/TNNLS.2022.3178075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal feature selection methods aim to identify a Markov boundary (MB) of a class variable, and almost all the existing causal feature selection algorithms use conditional independence (CI) tests to learn the MB. However, in real-world applications, due to data issues (e.g., noisy or small samples), CI tests can be unreliable; thus, causal feature selection algorithms relying on CI tests encounter two types of errors: false positives (i.e., selecting false MB features) and false negatives (i.e., discarding true MB features). Existing algorithms only tackle either false positives or false negatives, and they cannot deal with both types of errors at the same time, leading to unsatisfactory results. To address this issue, we propose a dual-correction-strategy-based MB learning (DCMB) algorithm to correct the two types of errors simultaneously. Specifically, DCMB selectively removes false positives from the MB features currently selected, while selectively retrieving false negatives from the features currently discarded. To automatically determine the optimal number of selected features for the selective removal and retrieval in the dual correction strategy, we design the simulated-annealing-based DCMB (SA-DCMB) algorithm. Using benchmark Bayesian network (BN) datasets, the experimental results demonstrate that DCMB achieves substantial improvements on the MB learning accuracy compared with the existing MB learning methods. Empirical studies in real-world datasets validate the effectiveness of SA-DCMB for classification against state-of-the-art causal and traditional feature selection algorithms.},
  archive      = {J_TNNLS},
  author       = {Xianjie Guo and Kui Yu and Lin Liu and Fuyuan Cao and Jiuyong Li},
  doi          = {10.1109/TNNLS.2022.3178075},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {938-951},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Causal feature selection with dual correction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic event-sampled control of interconnected nonlinear
systems using reinforcement learning. <em>TNNLS</em>, <em>35</em>(1),
923–937. (<a href="https://doi.org/10.1109/TNNLS.2022.3178017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a decentralized dynamic event-based control strategy for nonlinear systems subject to matched interconnections. To begin with, we introduce a dynamic event-based sampling mechanism, which relies on the system’s states and the variables generated by time-based differential equations. Then, we prove that the decentralized event-based controller for the whole system is composed of all the optimal event-based control policies of nominal subsystems. To derive these optimal event-based control policies, we design a critic-only architecture to solve the related event-based Hamilton–Jacobi–Bellman equations in the reinforcement learning framework. The implementation of such an architecture uses only critic neural networks (NNs) with their weight vectors being updated through the gradient descent method together with concurrent learning. After that, we demonstrate that the asymptotic stability of closed-loop nominal subsystems and the uniformly ultimate boundedness stability of critic NNs’ weight estimation errors are guaranteed by using Lyapunov’s approach. Finally, we provide simulations of a matched nonlinear-interconnected plant to validate the present theoretical claims.},
  archive      = {J_TNNLS},
  author       = {Xiong Yang and Mengmeng Xu and Qinglai Wei},
  doi          = {10.1109/TNNLS.2022.3178017},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {923-937},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic event-sampled control of interconnected nonlinear systems using reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive NN controller of nonlinear state-dependent
constrained systems with unknown control direction. <em>TNNLS</em>,
<em>35</em>(1), 913–922. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various constraints commonly exist in most physical systems; however, traditional constraint control methods consider the constraint boundaries only relying on constant or time variable, which greatly restricts applying constraint control to practical systems. To avoid such conservatism, this study develops a new adaptive neural controller for the nonlinear strict-feedback systems subject to state-dependent constraint boundaries. The nonlinear state-dependent mapping is employed in each step of backstepping procedure, and the prescribed transient performance on tracking error and the constraints on system states are ensured without repeatedly verifying the feasibility conditions on virtual controllers. The radial basis function neural network (NN) with less parameters approach is introduced as an identifier to estimate the unknown system dynamics and reduce computation burden. For removing the effect of unknown control direction, the Nussbaum gain technique is integrated into controller design. Based on the Lyapunov analysis, the developed control strategy can ensure that all the closed-loop signals are bounded, and the constraints on full system states and tracking error are achieved. The simulation examples are used to illustrate the effectiveness of the developed control strategy.},
  archive      = {J_TNNLS},
  author       = {Dapeng Li and Hong-Gui Han and Jun-Fei Qiao},
  doi          = {10.1109/TNNLS.2022.3177839},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {913-922},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive NN controller of nonlinear state-dependent constrained systems with unknown control direction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLEAR: Cluster-enhanced contrast for self-supervised graph
representation learning. <em>TNNLS</em>, <em>35</em>(1), 899–912. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies self-supervised graph representation learning, which is critical to various tasks, such as protein property prediction. Existing methods typically aggregate representations of each individual node as graph representations, but fail to comprehensively explore local substructures (i.e., motifs and subgraphs), which also play important roles in many graph mining tasks. In this article, we propose a self-supervised graph representation learning framework named cluster-enhanced Contrast (CLEAR) that models the structural semantics of a graph from graph-level and substructure-level granularities, i.e., global semantics and local semantics, respectively. Specifically, we use graph-level augmentation strategies followed by a graph neural network-based encoder to explore global semantics. As for local semantics, we first use graph clustering techniques to partition each whole graph into several subgraphs while preserving as much semantic information as possible. We further employ a self-attention interaction module to aggregate the semantics of all subgraphs into a local-view graph representation. Moreover, we integrate both global semantics and local semantics into a multiview graph contrastive learning framework, enhancing the semantic-discriminative ability of graph representations. Extensive experiments on various real-world benchmarks demonstrate the efficacy of the proposed CLEAR over current graph self-supervised representation learning approaches on both graph classification and transfer learning tasks.},
  archive      = {J_TNNLS},
  author       = {Xiao Luo and Wei Ju and Meng Qu and Yiyang Gu and Chong Chen and Minghua Deng and Xian-Sheng Hua and Ming Zhang},
  doi          = {10.1109/TNNLS.2022.3177775},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {899-912},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CLEAR: Cluster-enhanced contrast for self-supervised graph representation learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online unsupervised domain adaptation via reducing inter-
and intra-domain discrepancies. <em>TNNLS</em>, <em>35</em>(1), 884–898.
(<a href="https://doi.org/10.1109/TNNLS.2022.3177769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) transfers knowledge from a labeled source domain to an unlabeled target domain on cross-domain object recognition by reducing a distribution discrepancy between the source and target domains (interdomain discrepancy). Prevailing methods on UDA were presented based on the premise that target data are collected in advance. However, in online scenarios, the target data often arrive in a streamed manner, such as visual image recognition in daily monitoring, which means that there is a distribution discrepancy between incoming target data and collected target data (intradomain discrepancy). Consequently, most existing methods need to re-adapt the incoming data and retrain a new model on online data. This paradigm is difficult to meet the real-time requirements of online tasks. In this study, we propose an online UDA framework via jointly reducing interdomain and intradomain discrepancies on cross-domain object recognition where target data arrive in a streamed manner. Specifically, the proposed framework comprises two phases: classifier training and online recognition phases. In the former, we propose training a classifier on a shared subspace where there is a lower interdomain discrepancy between the two domains. In the latter, a low-rank subspace alignment method is introduced to adapt incoming data to the shared subspace by reducing the intradomain discrepancy. Finally, online recognition results can be obtained by the trained classifier. Extensive experiments on DA benchmarks and real-world datasets are employed to evaluate the performance of the proposed framework in online scenarios. The experimental results show the superiority of the proposed framework in online recognition tasks.},
  archive      = {J_TNNLS},
  author       = {Yalan Ye and Tongjie Pan and Qianhe Meng and Jingjing Li and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2022.3177769},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {884-898},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online unsupervised domain adaptation via reducing inter- and intra-domain discrepancies},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classifier ensemble based on multiview optimization for
high-dimensional imbalanced data classification. <em>TNNLS</em>,
<em>35</em>(1), 870–883. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional class imbalanced data have plagued the performance of classification algorithms seriously. Because of a large number of redundant/invalid features and the class imbalanced issue, it is difficult to construct an optimal classifier for high-dimensional imbalanced data. Classifier ensemble has attracted intensive attention since it can achieve better performance than an individual classifier. In this work, we propose a multiview optimization (MVO) to learn more effective and robust features from high-dimensional imbalanced data, based on which an accurate and robust ensemble system is designed. Specifically, an optimized subview generation (OSG) in MVO is first proposed to generate multiple optimized subviews from different scenarios, which can strengthen the classification ability of features and increase the diversity of ensemble members simultaneously. Second, a new evaluation criterion that considers the distribution of data in each optimized subview is developed based on which a selective ensemble of optimized subviews (SEOS) is designed to perform the subview selective ensemble. Finally, an oversampling approach is executed on the optimized view to obtain a new class rebalanced subset for the classifier. Experimental results on 25 high-dimensional class imbalanced datasets indicate that the proposed method outperforms other mainstream classifier ensemble methods.},
  archive      = {J_TNNLS},
  author       = {Yuhong Xu and Zhiwen Yu and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3177695},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {870-883},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Classifier ensemble based on multiview optimization for high-dimensional imbalanced data classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prioritized experience-based reinforcement learning with
human guidance for autonomous driving. <em>TNNLS</em>, <em>35</em>(1),
855–869. (<a href="https://doi.org/10.1109/TNNLS.2022.3177685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) requires skillful definition and remarkable computational efforts to solve optimization and control problems, which could impair its prospect. Introducing human guidance into RL is a promising way to improve learning performance. In this article, a comprehensive human guidance-based RL framework is established. A novel prioritized experience replay mechanism that adapts to human guidance in the RL process is proposed to boost the efficiency and performance of the RL algorithm. To relieve the heavy workload on human participants, a behavior model is established based on an incremental online learning method to mimic human actions. We design two challenging autonomous driving tasks for evaluating the proposed algorithm. Experiments are conducted to access the training and testing performance and learning mechanism of the proposed algorithm. Comparative results against the state-of-the-art methods suggest the advantages of our algorithm in terms of learning efficiency, performance, and robustness.},
  archive      = {J_TNNLS},
  author       = {Jingda Wu and Zhiyu Huang and Wenhui Huang and Chen Lv},
  doi          = {10.1109/TNNLS.2022.3177685},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {855-869},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prioritized experience-based reinforcement learning with human guidance for autonomous driving},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multistructure contrastive learning for pretraining event
representation. <em>TNNLS</em>, <em>35</em>(1), 842–854. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event representation aims to transform individual events from a narrative event chain into a set of low-dimensional vectors to help support a series of downstream applications, e.g., similarity differentiation and missing event prediction. Traditional event representation models tend to focus on single modeling perspectives and thus are incapable of capturing physically disconnected yet semantically connected event segments. We, therefore, propose a heterogeneous event graph model (HeterEvent) to explicitly represent such event segments. Furthermore, another challenge in traditional event representation models is inherited from the datasets themselves. Data sparsity and insufficient labeled data are commonly encountered in event chains, easily leading to overfitting and undertraining. Therefore, we extend HeterEvent with a multistructure contrastive learning framework (MulCL) to alleviate the training risks from two structural perspectives. From the sequential perspective, a sequential-view contrastive learning component (SeqCL) is designed to facilitate the acquisition of sequential characteristics. From the graph perspective, a graph-view contrastive learning component (GraCL) is proposed to enhance the robustness of graph training by comparing different corrupted graphs. Experimental results confirm that our proposed MulCL $_{[W+E]}$ model outperforms state-of-the-art baselines. Specifically, compared with the previously proposed supervised model HeterEvent $_{[W+E]}$ [Zheng et al. (2020)], MulCL $_{[W+E]}$ shows an average improvement of 5.3% in terms of accuracy for the inference-ability-based tasks. For the representation-ability-based tasks, MulCL $_{[W+E]}$ achieves an average improvement of 2.7% in terms of accuracy for the hard similarity tasks and an improvement of 4.1% in terms of the Spearman’s correlation for the transitive sentence similarity task, respectively.},
  archive      = {J_TNNLS},
  author       = {Jianming Zheng and Fei Cai and Jun Liu and Yanxiang Ling and Honghui Chen},
  doi          = {10.1109/TNNLS.2022.3177641},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {842-854},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multistructure contrastive learning for pretraining event representation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic and static representation learning network for
recommendation. <em>TNNLS</em>, <em>35</em>(1), 831–841. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing review-based recommendation methods learn a latent representation of user and item from user-generated reviews by a static strategy, which are unable to capture the dynamic evolution of users’ interests and the dynamic attraction of items. Here, we propose a dynamic and static representation learning network (DSRLN) to improve the rating prediction accuracy by exploring fine-grained representations of users and items. Specifically, we built DSRLN with a dynamic representation extractor to model the dynamic evolution of users’ interests by exploring the inner relations of an interaction sequence, and with a static representation extractor to model the users’ intrinsic preferences by learning the semantic coherence and feature strength information from reviews. To identify the different influences of dynamic and static features for different users, a personalized adaptive fusion module was designed using a weighted attention mechanism. Extensive experiments on five real-world datasets from Amazon demonstrated the superiority of the proposed model, and the additional ablation studies verified the effectiveness of the components designed in the DSRLN model.},
  archive      = {J_TNNLS},
  author       = {Tongcun Liu and Siyuan Lou and Jianxin Liao and Hailin Feng},
  doi          = {10.1109/TNNLS.2022.3177611},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {831-841},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic and static representation learning network for recommendation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Game-based backstepping design for strict-feedback nonlinear
multi-agent systems based on reinforcement learning. <em>TNNLS</em>,
<em>35</em>(1), 817–830. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the game-based backstepping control method is proposed for the high-order nonlinear multi-agent system with unknown dynamic and input saturation. Reinforcement learning (RL) is employed to get the saddle point solution of the tracking game between each agent and the reference signal for achieving robust control. Specifically, the approximate optimal solution of the established Hamilton–Jacobi–Isaacs (HJI) equation is obtained by policy iteration for each subsystem, and the single network adaptive critic (SNAC) architecture is used to reduce the computational burden. In addition, based on the separation operation of the error term from the derivative of the value function, we achieve the different proportions of the two agents in the game to realize the regulation of the final equilibrium point. Different from the general use of the neural network for system identification, the unknown nonlinear dynamic term is approximated based on the state difference obtained by the command filter. Furthermore, a sufficient condition is established to guarantee that the whole system and each subsystem included are uniformly ultimately bounded. Finally, simulation results are given to show the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jia Long and Dengxiu Yu and Guoxing Wen and Li Li and Zhen Wang and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3177461},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {817-830},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Game-based backstepping design for strict-feedback nonlinear multi-agent systems based on reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-graph global and local concept factorization for data
clustering. <em>TNNLS</em>, <em>35</em>(1), 803–816. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering a wide range of applications of nonnegative matrix factorization (NMF), many NMF and their variants have been developed. Since previous NMF methods cannot fully describe complex inner global and local manifold structures of the data space and extract complex structural information, we propose a novel NMF method called dual-graph global and local concept factorization (DGLCF). To properly describe the inner manifold structure, DGLCF introduces the global and local structures of the data manifold and the geometric structure of the feature manifold into CF. The global manifold structure makes the model more discriminative, while the two local regularization terms simultaneously preserve the inherent geometry of data and features. Finally, we analyze convergence and the iterative update rules of DGLCF. We illustrate clustering performance by comparing it with latest algorithms on four real-world datasets.},
  archive      = {J_TNNLS},
  author       = {Ning Li and Chengcai Leng and Irene Cheng and Anup Basu and Licheng Jiao},
  doi          = {10.1109/TNNLS.2022.3177433},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {803-816},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual-graph global and local concept factorization for data clustering},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CARRNN: A continuous autoregressive recurrent neural network
for deep representation learning from sporadic temporal data.
<em>TNNLS</em>, <em>35</em>(1), 792–802. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning temporal patterns from multivariate longitudinal data is challenging especially in cases when data is sporadic, as often seen in, e.g., healthcare applications where the data can suffer from irregularity and asynchronicity as the time between consecutive data points can vary across features and samples, hindering the application of existing deep learning models that are constructed for complete, evenly spaced data with fixed sequence lengths. In this article, a novel deep learning-based model is developed for modeling multiple temporal features in sporadic data using an integrated deep learning architecture based on a recurrent neural network (RNN) unit and a continuous-time autoregressive (CAR) model. The proposed model, called CARRNN, uses a generalized discrete-time autoregressive (AR) model that is trainable end-to-end using neural networks modulated by time lags to describe the changes caused by the irregularity and asynchronicity. It is applied to time-series regression and classification tasks for Alzheimer’s disease progression modeling, intensive care unit (ICU) mortality rate prediction, human activity recognition, and event-based digit recognition, where the proposed model based on a gated recurrent unit (GRU) in all cases achieves significantly better predictive performance than the state-of-the-art methods using RNNs, GRUs, and long short-term memory (LSTM) networks.},
  archive      = {J_TNNLS},
  author       = {Mostafa Mehdipour Ghazi and Lauge Sørensen and Sébastien Ourselin and Mads Nielsen},
  doi          = {10.1109/TNNLS.2022.3177366},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {792-802},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CARRNN: A continuous autoregressive recurrent neural network for deep representation learning from sporadic temporal data},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Delayed impulsive control for lag synchronization of
delayed neural networks involving partial unmeasurable states.
<em>TNNLS</em>, <em>35</em>(1), 783–791. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the framework of impulsive control, this article deals with the lag synchronization problem of neural networks involving partially unmeasurable states, where the time delay in impulses is fully addressed. Since the complexity of external environment and uncertainty of networks, which may lead to a result that the information of partial states is unmeasurable, the key problem for lag synchronization control is how to utilize the information of measurable states to design suitable impulsive control. By using linear matrix inequality (LMI) and transition matrix method coupled with dimension expansion technique, some sufficient conditions are derived to guarantee lag synchronization, where the requirement for information of all states is needless. Moreover, our proposed conditions not only allow the existence of unmeasurable states but also reduce the restrictions on the number of measurable states, which shows the generality of our results and wide-application in practice. Finally, two illustrative examples and their numerical simulations are presented to demonstrate the effectiveness of main results.},
  archive      = {J_TNNLS},
  author       = {Mingyue Li and Xueyan Yang and Xiaodi Li},
  doi          = {10.1109/TNNLS.2022.3177234},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {783-791},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Delayed impulsive control for lag synchronization of delayed neural networks involving partial unmeasurable states},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeRP: Implicit neural representation learning with prior
embedding for sparsely sampled image reconstruction. <em>TNNLS</em>,
<em>35</em>(1), 770–782. (<a
href="https://doi.org/10.1109/TNNLS.2022.3177134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image reconstruction is an inverse problem that solves for a computational image based on sampled sensor measurement. Sparsely sampled image reconstruction poses additional challenges due to limited measurements. In this work, we propose a methodology of implicit Neural Representation learning with Prior embedding (NeRP) to reconstruct a computational image from sparsely sampled measurements. The method differs fundamentally from previous deep learning-based image reconstruction approaches in that NeRP exploits the internal information in an image prior and the physics of the sparsely sampled measurements to produce a representation of the unknown subject. No large-scale data is required to train the NeRP except for a prior image and sparsely sampled measurements. In addition, we demonstrate that NeRP is a general methodology that generalizes to different imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI). We also show that NeRP can robustly capture the subtle yet significant image changes required for assessing tumor progression.},
  archive      = {J_TNNLS},
  author       = {Liyue Shen and John Pauly and Lei Xing},
  doi          = {10.1109/TNNLS.2022.3177134},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {770-782},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {NeRP: Implicit neural representation learning with prior embedding for sparsely sampled image reconstruction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A transfer learning-based method for personalized state of
health estimation of lithium-ion batteries. <em>TNNLS</em>,
<em>35</em>(1), 759–769. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State of health (SOH) estimation of lithium-ion batteries (LIBs) is of critical importance for battery management systems (BMSs) of electronic devices. An accurate SOH estimation is still a challenging problem limited by diverse usage conditions between training and testing LIBs. To tackle this problem, this article proposes a transfer learning-based method for personalized SOH estimation of a new battery. More specifically, a convolutional neural network (CNN) combined with an improved domain adaptation method is used to construct an SOH estimation model, where the CNN is used to automatically extract features from raw charging voltage trajectories, while the domain adaptation method named maximum mean discrepancy (MMD) is adopted to reduce the distribution difference between training and testing battery data. This article extends MMD from classification tasks to regression tasks, which can therefore be used for SOH estimation. Three different datasets with different charging policies, discharging policies, and ambient temperatures are used to validate the effectiveness and generalizability of the proposed method. The superiority of the proposed SOH estimation method is demonstrated through the comparison with direct model training using state-of-the-art machine learning methods and several other domain adaptation approaches. The results show that the proposed transfer learning-based method has wide generalizability as well as a positive precision improvement.},
  archive      = {J_TNNLS},
  author       = {Guijun Ma and Songpei Xu and Tao Yang and Zhenbang Du and Limin Zhu and Han Ding and Ye Yuan},
  doi          = {10.1109/TNNLS.2022.3176925},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {759-769},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A transfer learning-based method for personalized state of health estimation of lithium-ion batteries},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Exponential synchronization of memristor-based competitive
neural networks with reaction- diffusions and infinite distributed
delays. <em>TNNLS</em>, <em>35</em>(1), 745–758. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking into account the infinite distributed delays and reaction-diffusions, this article investigates the global exponential synchronization problem of a class of memristor-based competitive neural networks (MCNNs) with different time scales. Based on the Lyapunov–Krasovskii functional and inequality approach, an adaptive control approach is proposed to ensure the exponential synchronization of the addressed drive-response networks. The closed-loop system is a discontinuous and delayed partial differential system in a cascade form, involving the spatial diffusion, the infinite distributed delays, the parametric adaptive law, the state-dependent switching parameters, and the variable structure controllers. By combining the theories of nonsmooth analysis, partial differential equation (PDE) and adaptive control, we present a new analytical method for rigorously deriving the synchronization of the states of the complex system. The derived $m$ -norm ( $m \geq 2$ )-based synchronization criteria are easily verified and the theoretical results are easily extended to memristor-based neural networks (NNs) without different time scales and reaction-diffusions. Finally, numerical simulations are presented to verify the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Leimin Wang and Chuan-Ke Zhang},
  doi          = {10.1109/TNNLS.2022.3176887},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {745-758},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential synchronization of memristor-based competitive neural networks with reaction- diffusions and infinite distributed delays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic probabilistic pruning: A general framework for
hardware-constrained pruning at different granularities. <em>TNNLS</em>,
<em>35</em>(1), 733–744. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unstructured neural network pruning algorithms have achieved impressive compression ratios. However, the resulting—typically irregular—sparse matrices hamper efficient hardware implementations, leading to additional memory usage and complex control logic that diminishes the benefits of unstructured pruning. This has spurred structured coarse-grained pruning solutions that prune entire feature maps or even layers, enabling efficient implementation at the expense of reduced flexibility. Here, we propose a flexible new pruning mechanism that facilitates pruning at different granularities (weights, kernels, and feature maps) while retaining efficient memory organization (e.g., pruning exactly $k$ -out-of- $n$ weights for every output neuron or pruning exactly $k$ -out-of- $n$ kernels for every feature map). We refer to this algorithm as dynamic probabilistic pruning (DPP). DPP leverages the Gumbel-softmax relaxation for differentiable $k$ -out-of- $n$ sampling, facilitating end-to-end optimization. We show that DPP achieves competitive compression ratios and classification accuracy when pruning common deep learning models trained on different benchmark datasets for image classification. Relevantly, the dynamic masking of DPP facilitates for joint optimization of pruning and weight quantization in order to even further compress the network, which we show as well. Finally, we propose novel information-theoretic metrics that show the confidence and pruning diversity of pruning masks within a layer.},
  archive      = {J_TNNLS},
  author       = {Lizeth Gonzalez-Carabarin and Iris A. M. Huijben and Bastian Veeling and Alexandre Schmid and Ruud J. G. van Sloun},
  doi          = {10.1109/TNNLS.2022.3176809},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {733-744},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic probabilistic pruning: A general framework for hardware-constrained pruning at different granularities},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prefixed-time local intermittent sampling synchronization of
stochastic multicoupling delay reaction–diffusion dynamic networks.
<em>TNNLS</em>, <em>35</em>(1), 718–732. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the problem of prefixed-time synchronization for stochastic multicoupled delay dynamic networks with reaction–diffusion terms and discontinuous activation by means of local intermittent sampling control. Notably, unlike the existing common fixed-time synchronization, this article puts forward a new synchronization concept, prefixed-time synchronization, based on the fact that stochastic noise and discontinuous activation can be seen everywhere in practical engineering, which can effectively perfect and improve the existing works. Specifically, a local intermittent in the time domain and point sampling control strategy in the spatial domain is proposed instead of a simple single intermittent control approach, which greatly reduces the control cost. In addition, by some effective means, including the famous Young’s inequality, Jensen’s inequality, and Hölder’s inequality, we obtain two different synchronization criteria of the networks without delay and with multicoupling delays and deeply reveal the quantitative relationship among control period, point sampling length, and network scale. Finally, a numerical example is given to verify the effectiveness of the developed method and the practicability by Chua’s circuit model.},
  archive      = {J_TNNLS},
  author       = {Kui Ding and Quanxin Zhu and Tingwen Huang},
  doi          = {10.1109/TNNLS.2022.3176648},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {718-732},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prefixed-time local intermittent sampling synchronization of stochastic multicoupling delay Reaction–Diffusion dynamic networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive neural fixed-time tracking control for high-order
nonlinear systems. <em>TNNLS</em>, <em>35</em>(1), 708–717. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of adaptive neural fixed-time tracking control for high-order systems is addressed in this article. In order to handle the difficulties from the uncertain nonlinearities within the original systems, the radial basis function neural networks (RBF NNs) are introduced to approximate the unknown nonlinear functions, and the adding a power integrator is applied to overcome the obstacle from high-order terms. It is proven that all signals in the closed-loop system are bounded and the output signal can eventually converge to a small neighborhood of the reference signal. Simulation results further verify the approaches developed.},
  archive      = {J_TNNLS},
  author       = {Jiawei Ma and Huanqing Wang and Junfei Qiao},
  doi          = {10.1109/TNNLS.2022.3176625},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {708-717},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural fixed-time tracking control for high-order nonlinear systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient harmonic neural networks with compound discrete
cosine transform filters and shared reconstruction filters.
<em>TNNLS</em>, <em>35</em>(1), 693–707. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The harmonic neural network (HNN) learns a combination of discrete cosine transform (DCT) filters to obtain an integrated feature from all spectra in the frequency domain. HNN, however, faces two challenges in learning and inference processes. First, the spectrum feature learned by HNN is insufficient and limited because the number of DCT filters is much smaller than that of feature maps. In addition, the number of parameters and the computation costs of HNN are significantly high because the intermediate spectrum layers are expanded multiple times. These two challenges will severely harm the performance and efficiency of HNN. To solve these problems, we first propose the compound DCT (C-DCT) filters integrating the nearest DCT filters to retrieve rich spectrum features to improve the performance. To significantly reduce the model size and computation complexity for improving the efficiency, the shared reconstruction filter is then proposed to share and dynamically drop the meta-filters in every frequency branch. Integrating the C-DCT filters with the shared reconstruction filters, the efficient harmonic network (EH-Net) is introduced. Extensive experiments on different datasets demonstrate that the proposed EH-Nets can effectively reduce the model size and computation complexity while maintaining the model performance. The code has been released at https://github.com/zhangle408/EH-Nets .},
  archive      = {J_TNNLS},
  author       = {Yao Lu and Le Zhang and Xiaofei Yang and Yicong Zhou},
  doi          = {10.1109/TNNLS.2022.3176611},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {693-707},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient harmonic neural networks with compound discrete cosine transform filters and shared reconstruction filters},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep structured state learning for next-period
recommendation. <em>TNNLS</em>, <em>35</em>(1), 680–692. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User activities in real systems are usually time-sensitive. But, most of the existing sequential models in recommender systems neglect the time-related signals. In this article, we find that users’ temporal behaviors tend to be driven by their regularly changing states, which provides a new perspective on learning users’ dynamic preference. However, since the individual state is usually latent, the event space is high dimensional, and meanwhile, temporal dependency of states is personalized and complex; it is challenging to represent, model, and learn the time-evolving patterns of user’s state. Focusing on these challenges, we propose a deep structured state learning (DSSL) framework, which is able to learn the representation of temporal states and the complex state dependency for time-sensitive recommendation. Extensive experiments demonstrate that the DSSL achieves competitive results on four real-world recommendation datasets. Furthermore, experiments also show some interesting rules for designing the state dependency network.},
  archive      = {J_TNNLS},
  author       = {Wen Wen and Fangyu Liang},
  doi          = {10.1109/TNNLS.2022.3176409},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {680-692},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep structured state learning for next-period recommendation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative fault-tolerant control for a class of nonlinear
MASs by resilient learning approach. <em>TNNLS</em>, <em>35</em>(1),
670–679. (<a href="https://doi.org/10.1109/TNNLS.2022.3176392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a learning-based resilient fault-tolerant control method is proposed for a class of uncertain nonlinear multiagent systems (MASs) to enhance the security and reliability against denial-of-service (DoS) attacks and actuator faults. With the framework of cooperative output regulation, the developed algorithm consists of designing a distributed resilient observer and a decentralized fault-tolerant controller. Specifically, by using the data-driven method, an online resilient learning algorithm is first presented to learn the unknown exosystem matrix in the presence of DoS attacks. Then, a distributed resilient observer is proposed working against DoS attacks. In addition, based on the developed observer, a decentralized adaptive fault-tolerant controller is designed to compensate for actuator faults. Moreover, the convergence of error systems is shown by using the Lyapunov stability theory. The effectiveness of our result is examined by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Chao Deng and Dong Yue and Wei-Wei Che and Xiangpeng Xie},
  doi          = {10.1109/TNNLS.2022.3176392},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {670-679},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cooperative fault-tolerant control for a class of nonlinear MASs by resilient learning approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethink, revisit, revise: A spiral reinforced self-revised
network for zero-shot learning. <em>TNNLS</em>, <em>35</em>(1), 657–669.
(<a href="https://doi.org/10.1109/TNNLS.2022.3176282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current approaches to zero-shot learning (ZSL) struggle to learn generalizable semantic knowledge capable of capturing complex correlations. Inspired by Spiral Curriculum, which enhances learning processes by revisiting knowledge, we propose a form of spiral learning that revisits visual representations based on a sequence of attribute groups (e.g., a combined group of color and shape). Spiral learning aims to learn generalized local correlations, enabling models to gradually enhance global learning and, thus, understand complex correlations. Our implementation is based on a two-stage reinforced self-revised (RSR) framework: preview and review. RSR first previews visual information to construct diverse attribute groups in a weakly supervised manner. Then, it spirally learns refined localities based on attribute groups and uses localities to revise global semantic correlations. Our framework outperforms state-of-the-art algorithms on four benchmark datasets in both zero-shot and generalized zero-shot settings, which demonstrates the effectiveness of spiral learning in learning generalizable and complex correlations. We also conduct extensive analysis to show that attribute groups and reinforced decision processes can capture complementary semantic information to improve predictions and aid explainability.},
  archive      = {J_TNNLS},
  author       = {Zhe Liu and Yun Li and Lina Yao and Julian McAuley and Sam Dixon},
  doi          = {10.1109/TNNLS.2022.3176282},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {657-669},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rethink, revisit, revise: A spiral reinforced self-revised network for zero-shot learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An XGBoost-based fitted q iteration for finding the optimal
STI strategies for HIV patients. <em>TNNLS</em>, <em>35</em>(1),
648–656. (<a href="https://doi.org/10.1109/TNNLS.2022.3176204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational algorithm proposed in this article is an important step toward the development of computational tools that could help guide clinicians to personalize the management of human immunodeficiency virus (HIV) infection. In this article, an XGBoost-based fitted Q iteration algorithm is proposed for finding the optimal structured treatment interruption (STI) strategies for HIV patients. Using the XGBoost-based fitted Q iteration algorithm, we can obtain acceptable and optimal STI strategies with fewer training data, when compared with the extra-tree-based fitted Q iteration algorithm, deep Q-networks (DQNs), and proximal policy optimization (PPO) algorithm. In addition, the XGBoost-based fitted Q iteration algorithm is computationally more efficient than the extra-tree-based fitted Q iteration algorithm.},
  archive      = {J_TNNLS},
  author       = {Yahe Yu and Hien Tran},
  doi          = {10.1109/TNNLS.2022.3176204},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {648-656},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An XGBoost-based fitted q iteration for finding the optimal STI strategies for HIV patients},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully connected networks on a diet with the mediterranean
matrix multiplication. <em>TNNLS</em>, <em>35</em>(1), 634–647. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes the Mediterranean matrix multiplication, a new, simple and practical randomized algorithm that samples angles between the rows and columns of two matrices with sizes $m, n, $ and $p$ to approximate matrix multiplication in $O(k(mn+np+mp))$ steps, where $k$ is a constant only related to the precision desired. The number of instructions carried out is mainly bounded by bitwise operators, amenable to a simplified processing architecture and compressed matrix weights. Results show that the method is superior in size and number of operations to the standard approximation with signed matrices. Equally important, this article demonstrates a first application to machine learning inference by showing that weights of fully connected layers can be compressed between $30\times $ and $100\times $ with little to no loss in inference accuracy. The requirements for pure floating-point operations are also down as our algorithm relies mainly on simpler bitwise operators.},
  archive      = {J_TNNLS},
  author       = {Hassan Eshkiki and Benjamin Mora and Xianghua Xie},
  doi          = {10.1109/TNNLS.2022.3176197},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {634-647},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully connected networks on a diet with the mediterranean matrix multiplication},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-growing binary activation network: A novel deep
learning model with dynamic architecture. <em>TNNLS</em>,
<em>35</em>(1), 624–633. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a deep learning model, the network architecture is crucial as a model with inappropriate architecture often suffers from performance degradation or parameter redundancy. However, it is experiential and difficult to find the appropriate architecture for a certain application. To tackle this problem, we propose a novel deep learning model with dynamic architecture, named self-growing binary activation network (SGBAN), which can extend the design of a fully connected network (FCN) progressively, resulting in a more compact architecture with higher performance on a certain task. This constructing process is more efficient than neural architecture search methods that train mass of networks to search for the optimal one. Concretely, the training technique of SGBAN is based on the function-preserving transformations that can expand the architecture and combine the information in the new data without neglecting the knowledge learned in the previous steps. The experimental results on four different classification tasks, i.e., Iris, MNIST, CIFAR-10, and CIFAR-100, demonstrate the effectiveness of SGBAN. On the one hand, SGBAN achieves competitive accuracy when compared with the FCN composed of the same architecture, which indicates that the new training technique has the equivalent optimization ability as the traditional optimization methods. On the other hand, the architecture generated by SGBAN achieves 0.59% improvements of accuracy, with only 33.44% parameters when compared with the FCNs composed of manual design architectures, i.e., 500+150 hidden units, on MNIST. Furthermore, we demonstrate that replacing the fully connected layers of the well-trained VGG-19 with SGBAN can gain a slightly improved performance with less than 1% parameters on all these tasks. Finally, we show that the proposed method can conduct the incremental learning tasks and outperform the three outstanding incremental learning methods, i.e., learning without forgetting, elastic weight consolidation, and gradient episodic memory, on both the incremental learning tasks on Disjoint MNIST and Disjoint CIFAR-10.},
  archive      = {J_TNNLS},
  author       = {Zeyang Zhang and Yidong Chen and Changle Zhou},
  doi          = {10.1109/TNNLS.2022.3176027},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {624-633},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-growing binary activation network: A novel deep learning model with dynamic architecture},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noise-tolerant discrimination indexes for fuzzy γ covering
and feature subset selection. <em>TNNLS</em>, <em>35</em>(1), 609–623.
(<a href="https://doi.org/10.1109/TNNLS.2022.3175922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy $\beta $ covering (FBC) has attracted considerable attention in recent years. Nevertheless, as the basic information granularity of FBC, fuzzy $\beta $ neighborhood does not satisfy reflexivity, which may lead to instability in classification learning and decision-making. Although a few studies have involved reflexive fuzzy $\beta $ neighborhoods, they only focus on a single fuzzy covering and cannot effectively deal with the information representation and information fusion of multiple fuzzy coverings. Moreover, there is a lack of investigation on noise-tolerant uncertainty measures for FBC, as well as their application in feature selection. Motivated by these issues, we investigate a noise-tolerant variable precision discrimination index (VPDI) by means of a new reflexive fuzzy covering neighborhood. To this end, fuzzy $\gamma $ neighborhood with reflexivity is introduced to characterize the information fusion of a fuzzy covering family. An uncertainty measure called fuzzy $\gamma $ neighborhood discrimination index is then presented to reflect the discriminatory power of fuzzy covering families. Some variants of the uncertainty measure, such as variable precision joint discrimination index, variable precision conditional discrimination index, and variable precision mutual discrimination index, are then put forth by means of fuzzy decision. These VPDIs can be used as an evaluation metric for a family of fuzzy coverings. Finally, the knowledge reduction of fuzzy covering decision systems is addressed from the point of keeping the discriminatory power, and a heuristic feature selection algorithm is designed by means of the variable precision conditional discrimination index. The experiments on 16 public datasets exhibit that the proposed algorithm can effectively reduce redundant features and achieve competitive results compared with six state-of-the-art feature selection algorithms. Moreover, it demonstrates strong robustness to the interference of random noise.},
  archive      = {J_TNNLS},
  author       = {Zhehuang Huang and Jinjin Li},
  doi          = {10.1109/TNNLS.2022.3175922},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {609-623},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Noise-tolerant discrimination indexes for fuzzy γ covering and feature subset selection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Resilient optimal defensive strategy of micro-grids system
via distributed deep reinforcement learning approach against FDI attack.
<em>TNNLS</em>, <em>35</em>(1), 598–608. (<a
href="https://doi.org/10.1109/TNNLS.2022.3175917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing false data injection (FDI) attack on the demand side brings great challenges to the energy management of interconnected microgrids. To address those aspects, this article proposes a resilient optimal defensive strategy with the distributed deep reinforcement learning (DRL) approach. To evaluate the FDI attack on demand response (DR), an online evaluation approach with the recursive least-square (RLS) method is proposed to evaluate the extent of supply security or voltage stability of the microgrids system is affected by the FDI attack. On the basis of evaluated security confidence, a distributed actor network learning approach is proposed to deduce optimal network weight, which can generate an optimal defensive scheme to ensure the economic and security issue of the microgrids system. From the methodology’s view, it can also enhance the autonomy of each microgrid as well as accelerate DRL efficiency. According to those simulation results, it can reveal that the proposed method can evaluate FDI attack impact well and an improved distributed DRL approach can be a viable and promising way for the optimal defense of microgrids against the FDI attack on the demand side.},
  archive      = {J_TNNLS},
  author       = {Huifeng Zhang and Dong Yue and Chunxia Dou and Gerhard P. Hancke},
  doi          = {10.1109/TNNLS.2022.3175917},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {598-608},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Resilient optimal defensive strategy of micro-grids system via distributed deep reinforcement learning approach against FDI attack},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural-network-based adaptive event-triggered asymptotically
consensus tracking control for nonlinear nonstrict-feedback MASs: An
improved dynamic surface approach. <em>TNNLS</em>, <em>35</em>(1),
584–597. (<a href="https://doi.org/10.1109/TNNLS.2022.3175956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the asymptotic tracking control problem for a class of nonlinear multi-agent systems (MASs) is researched by the combination of radial basis function neural networks (RBF NNs) and an improved dynamic surface control (DSC) technology. It’s important to emphasize that the MASs studied in this article are nonlinear and nonstrict-feedback systems, where the nonlinear functions are unknown. In order to satisfy the requirement that all items in the controller must be available, the unknown nonlinearities in the system are flexibly approximated by utilizing RBF NNs technique. Moreover, the issue of “complexity explosion” in the backstepping procedure is handled by improving the traditional DSC technology, and meanwhile, the influences of the boundary layers caused by the filters in the DSC procedure are eliminated skillfully through the compensation terms. In addition, the relative threshold event-triggered strategy is developed for the designed controllers to reduce the waste of communication resources, where Zeno phenomenon is successfully avoided. It is observed that the new presented control strategy ensures that all the closed-loop systems variables are uniformly ultimately bounded (UUB), and furthermore all the outputs of followers are able to track the output of the leader with zero tracking errors. Finally, the simulation results are presented to show the effectiveness of the obtained design scheme.},
  archive      = {J_TNNLS},
  author       = {Bocheng Yan and Ben Niu and Xudong Zhao and Huanqing Wang and Wendi Chen and Xiaomei Liu},
  doi          = {10.1109/TNNLS.2022.3175956},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {584-597},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive event-triggered asymptotically consensus tracking control for nonlinear nonstrict-feedback MASs: An improved dynamic surface approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural-network-based adaptive fixed-time control for
nonlinear multiagent non-affine systems. <em>TNNLS</em>, <em>35</em>(1),
570–583. (<a href="https://doi.org/10.1109/TNNLS.2022.3175929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, the adaptive neural network consensus control problem is addressed for a class of non-affine multiagent systems (MASs) with actuator faults and stochastic disturbances. To overcome difficulties associated with actuator faults and uncertain functions of the designed MAS, a neural network fault-tolerant control scheme is developed. Moreover, an adaptive backstepping controller is developed to solve the non-affine appearance in multiagent stochastic non-affine systems using the mean value theorem. Being different from the existing control methods, the developed adaptive fixed-time control approach can ensure that the outputs of all followers track the reference signal synchronously in the fixed time, and all signals of the controlled system are semi-globally uniformly fixed-time stable. The simulation results confirm that the presented control strategy is effective in achieving control goals.},
  archive      = {J_TNNLS},
  author       = {Wen Bai and Peter Xiaoping Liu and Huanqing Wang},
  doi          = {10.1109/TNNLS.2022.3175929},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {570-583},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive fixed-time control for nonlinear multiagent non-affine systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GNN model for time-varying matrix inversion with robust
finite-time convergence. <em>TNNLS</em>, <em>35</em>(1), 559–569. (<a
href="https://doi.org/10.1109/TNNLS.2022.3175899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a type of recurrent neural networks (RNNs) modeled as dynamic systems, the gradient neural network (GNN) is recognized as an effective method for static matrix inversion with exponential convergence. However, when it comes to time-varying matrix inversion, most of the traditional GNNs can only track the corresponding time-varying solution with a residual error, and the performance becomes worse when there are noises. Currently, zeroing neural networks (ZNNs) take a dominant role in time-varying matrix inversion, but ZNN models are more complex than GNN models, require knowing the explicit formula of the time-derivative of the matrix, and intrinsically cannot avoid the inversion operation in its realization in digital computers. In this article, we propose a unified GNN model for handling both static matrix inversion and time-varying matrix inversion with finite-time convergence and a simpler structure. Our theoretical analysis shows that, under mild conditions, the proposed model bears finite-time convergence for time-varying matrix inversion, regardless of the existence of bounded noises. Simulation comparisons with existing GNN models and ZNN models dedicated to time-varying matrix inversion demonstrate the advantages of the proposed GNN model in terms of convergence speed and robustness to noises.},
  archive      = {J_TNNLS},
  author       = {Yinyan Zhang and Shuai Li and Jian Weng and Bolin Liao},
  doi          = {10.1109/TNNLS.2022.3175899},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {559-569},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GNN model for time-varying matrix inversion with robust finite-time convergence},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metric learning-based fault diagnosis and anomaly detection
for industrial data with intraclass variance. <em>TNNLS</em>,
<em>35</em>(1), 547–558. (<a
href="https://doi.org/10.1109/TNNLS.2022.3175888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial system monitoring includes fault diagnosis and anomaly detection, which have received extensive attention, since they can recognize the fault types and detect unknown anomalies. However, a separate fault diagnosis method or anomaly detection method cannot identify unknown faults and distinguish between different fault types simultaneously; thus, it is difficult to meet the increasing demand for safety and reliability of industrial systems. Besides, the actual system often operates in varying working conditions and is disturbed by the noise, which results in the intraclass variance of the raw data and degrades the performance of industrial system monitoring. To solve these problems, a metric learning-based fault diagnosis and anomaly detection method is proposed. Fault diagnosis and anomaly detection are adaptively fused in the proposed end-to-end model, where anomaly detection can prevent the model from misjudging the unknown anomaly as the known type, while fault diagnosis can identify the specific type of system fault. In addition, a novel multicenter loss is introduced to restrain the intraclass variance. Compared with manual feature extraction that can only extract suboptimal features, it can learn discriminant features automatically for both fault diagnosis and anomaly detection tasks. Experiments on three-phase flow (TPF) facility and Case Western Reserve University (CWRU) bearing have demonstrated that the proposed method can avoid the interference of intraclass variances and learn features that are effective for identifying tasks. Moreover, it achieves the best performance in both fault diagnosis and anomaly detection.},
  archive      = {J_TNNLS},
  author       = {Keke Huang and Shujie Wu and Bei Sun and Chunhua Yang and Weihua Gui},
  doi          = {10.1109/TNNLS.2022.3175888},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {547-558},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Metric learning-based fault diagnosis and anomaly detection for industrial data with intraclass variance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SparseVoxNet: 3-d object recognition with sparsely
aggregation of 3-d dense blocks. <em>TNNLS</em>, <em>35</em>(1),
532–546. (<a href="https://doi.org/10.1109/TNNLS.2022.3175775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic recognition of 3-D objects in a 3-D model by convolutional neural network (CNN) methods has been successfully applied to various tasks, e.g., robotics and augmented reality. Three-dimensional object recognition is mainly performed by analyzing the object using multi-view images, depth images, graphs, or volumetric data. In some cases, using volumetric data provides the most promising results. However, existing recognition techniques on volumetric data have many drawbacks, such as losing object details on converting points to voxels and the large size of the input volume data that leads to substantial 3-D CNNs. Using point clouds could also provide very promising results; however, point-cloud-based methods typically need sparse data entry and time-consuming training stages. Thus, using volumetric could be a more efficient and flexible recognizer for our special case in the School of Medicine, Shanghai Jiao Tong University. In this article, we propose a novel solution to 3-D object recognition from volumetric data using a combination of three compact CNN models, low-cost SparseNet, and feature representation technique. We achieve an optimized network by estimating extra geometrical information comprising the surface normal and curvature into two separated neural networks. These two models provide supplementary information to each voxel data that consequently improve the results. The primary network model takes advantage of all the predicted features and uses these features in Random Forest (RF) for recognition purposes. Our method outperforms other methods in training speed in our experiments and provides an accurate result as good as the state-of-the-art.},
  archive      = {J_TNNLS},
  author       = {Ahmad Karambakhsh and Bin Sheng and Ping Li and Huating Li and Jinman Kim and Younhyun Jung and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3175775},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {532-546},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SparseVoxNet: 3-D object recognition with sparsely aggregation of 3-D dense blocks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating neural ODEs using model order reduction.
<em>TNNLS</em>, <em>35</em>(1), 519–531. (<a
href="https://doi.org/10.1109/TNNLS.2022.3175757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding nonlinear dynamical systems into artificial neural networks is a powerful new formalism for machine learning. By parameterizing ordinary differential equations (ODEs) as neural network layers, these Neural ODEs are memory-efficient to train, process time series naturally, and incorporate knowledge of physical systems into deep learning (DL) models. However, the practical applications of Neural ODEs are limited due to long inference times because the outputs of the embedded ODE layers are computed numerically with differential equation solvers that can be computationally demanding. Here, we show that mathematical model order reduction (MOR) methods can be used for compressing and accelerating Neural ODEs by accurately simulating the continuous nonlinear dynamics in low-dimensional subspaces. We implement our novel compression method by developing Neural ODEs that integrate the necessary subspace-projection and interpolation operations as layers of the neural network. We validate our approach by comparing it to neuron pruning and singular value decomposition (SVD)-based weight truncation methods from the literature in image and time-series classification tasks. The methods are evaluated by acceleration versus accuracy when adjusting the level of compression. On this spectrum, we achieve a favorable balance over existing methods by using MOR when compressing a convolutional Neural ODE. In compressing a recurrent Neural ODE, SVD-based weight truncation yields good performance. Based on our results, our integration of MOR with Neural ODEs can facilitate efficient, dynamical system-driven DL in resource-constrained applications.},
  archive      = {J_TNNLS},
  author       = {Mikko Lehtimäki and Lassi Paunonen and Marja-Leena Linne},
  doi          = {10.1109/TNNLS.2022.3175757},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {519-531},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accelerating neural ODEs using model order reduction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Finite-time passivity and synchronization of multi-weighted
complex dynamical networks under PD control. <em>TNNLS</em>,
<em>35</em>(1), 507–518. (<a
href="https://doi.org/10.1109/TNNLS.2022.3175747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on finite-time passivity (FTP) and finite-time synchronization (FTS) for complex dynamical networks with multiple state/derivative couplings based on the proportional–derivative (PD) control method. Several criteria of FTP for complex dynamical networks with multiple state couplings (CDNMSCs) are formulated by utilizing the PD controller and constructing an appropriate Lyapunov function. Furthermore, FTP is further used to investigate the FTS in CDNMSCs under the PD controller. In addition, the FTP and FTS for complex dynamical networks with multiple derivative couplings (CDNMDCs) are also studied by exploiting the PD control method and some inequality techniques. Finally, two numerical examples are worked out to demonstrate the validity of the presented PD controllers.},
  archive      = {J_TNNLS},
  author       = {Jin-Liang Wang and Lin-Hao Zhao and Huai-Ning Wu and Tingwen Huang},
  doi          = {10.1109/TNNLS.2022.3175747},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {507-518},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time passivity and synchronization of multi-weighted complex dynamical networks under PD control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simulation-aided handover prediction from video using
recurrent image-to-motion networks. <em>TNNLS</em>, <em>35</em>(1),
494–506. (<a href="https://doi.org/10.1109/TNNLS.2022.3175720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep neural networks have opened up new possibilities for visuomotor robot learning. In the context of human–robot or robot–robot collaboration, such networks can be trained to predict future poses and this information can be used to improve the dynamics of cooperative tasks. This is important, both in terms of realizing various cooperative behaviors, and for ensuring safety. In this article, we propose a recurrent neural architecture, capable of transforming variable-length input motion videos into a set of parameters describing a robot trajectory, where predictions can be made after receiving only a few frames. A simulation environment is utilized to expand the training database and to improve generalization capability of the network. The resulting architecture demonstrates good accuracy when predicting handover trajectories, with models trained on synthetic and real data showing better performance than when trained on real or simulated data only. The computed trajectories enable the execution of handover tasks with uncalibrated robots, which was verified in an experiment with two real robots.},
  archive      = {J_TNNLS},
  author       = {Matija Mavsar and Barry Ridge and Rok Pahič and Jun Morimoto and Aleš Ude},
  doi          = {10.1109/TNNLS.2022.3175720},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {494-506},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Simulation-aided handover prediction from video using recurrent image-to-motion networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). THPs: Topological hawkes processes for learning causal
structure on event sequences. <em>TNNLS</em>, <em>35</em>(1), 479–493.
(<a href="https://doi.org/10.1109/TNNLS.2022.3175622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning causal structure among event types on multitype event sequences is an important but challenging task. Existing methods, such as the Multivariate Hawkes processes, mostly assumed that each sequence is independent and identically distributed. However, in many real-world applications, it is commonplace to encounter a topological network behind the event sequences such that an event is excited or inhibited not only by its history but also by its topological neighbors. Consequently, the failure in describing the topological dependency among the event sequences leads to the error detection of the causal structure. By considering the Hawkes processes from the view of temporal convolution, we propose a topological Hawkes process (THP) to draw a connection between the graph convolution in the topology domain and the temporal convolution in time domains. We further propose a causal structure learning method on THP in a likelihood framework. The proposed method is featured with the graph convolution-based likelihood function of THP and a sparse optimization scheme with an Expectation-Maximization of the likelihood function. Theoretical analysis and experiments on both synthetic and real-world data demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Ruichu Cai and Siyu Wu and Jie Qiao and Zhifeng Hao and Keli Zhang and Xi Zhang},
  doi          = {10.1109/TNNLS.2022.3175622},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {479-493},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {THPs: Topological hawkes processes for learning causal structure on event sequences},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based chance-constrained reinforcement learning via
separated proportional-integral lagrangian. <em>TNNLS</em>,
<em>35</em>(1), 466–478. (<a
href="https://doi.org/10.1109/TNNLS.2022.3175595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safety is essential for reinforcement learning (RL) applied in the real world. Adding chance constraints (or probabilistic constraints) is a suitable way to enhance RL safety under uncertainty. Existing chance-constrained RL methods, such as the penalty methods and the Lagrangian methods, either exhibit periodic oscillations or learn an overconservative or unsafe policy. In this article, we address these shortcomings by proposing a separated proportional-integral Lagrangian (SPIL) algorithm. We first review the constrained policy optimization process from a feedback control perspective, which regards the penalty weight as the control input and the safe probability as the control output. Based on this, the penalty method is formulated as a proportional controller, and the Lagrangian method is formulated as an integral controller. We then unify them and present a proportional-integral Lagrangian method to get both their merits with an integral separation technique to limit the integral value to a reasonable range. To accelerate training, the gradient of safe probability is computed in a model-based manner. The convergence of the overall algorithm is analyzed. We demonstrate that our method can reduce the oscillations and conservatism of RL policy in a car-following simulation. To prove its practicality, we also apply our method to a real-world mobile robot navigation task, where our robot successfully avoids a moving obstacle with highly uncertain or even aggressive behaviors.},
  archive      = {J_TNNLS},
  author       = {Baiyu Peng and Jingliang Duan and Jianyu Chen and Shengbo Eben Li and Genjin Xie and Congsheng Zhang and Yang Guan and Yao Mu and Enxin Sun},
  doi          = {10.1109/TNNLS.2022.3175595},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {466-478},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-based chance-constrained reinforcement learning via separated proportional-integral lagrangian},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale human activity recognition and anticipation
network. <em>TNNLS</em>, <em>35</em>(1), 451–465. (<a
href="https://doi.org/10.1109/TNNLS.2022.3175480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks have been leveraged to achieve huge improvements in video understanding and human activity recognition performance in the past decade. However, most existing methods focus on activities that have similar time scales, leaving the task of action recognition on multiscale human behaviors less explored. In this study, a two-stream multiscale human activity recognition and anticipation (MS-HARA) network is proposed, which is jointly optimized using a multitask learning method. The MS-HARA network fuses the two streams of the network using an efficient temporal–channel attention (TCA)-based fusion approach to improve the model’s representational ability for both temporal and spatial features. We investigate the multiscale human activities from two basic categories, namely, midterm activities and long-term activities. The network is designed to function as part of a real-time processing framework to support interaction and mutual understanding between humans and intelligent machines. It achieves state-of-the-art results on several datasets for different tasks and different application domains. The midterm and long-term action recognition and anticipation performance, as well as the network fusion, are extensively tested to show the efficiency of the proposed network. The results show that the MS-HARA network can easily be extended to different application domains.},
  archive      = {J_TNNLS},
  author       = {Yang Xing and Stuart Golodetz and Aluna Everitt and Andrew Markham and Niki Trigoni},
  doi          = {10.1109/TNNLS.2022.3175480},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {451-465},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiscale human activity recognition and anticipation network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tri-branch convolutional neural networks for top-k focused
academic performance prediction. <em>TNNLS</em>, <em>35</em>(1),
439–450. (<a href="https://doi.org/10.1109/TNNLS.2022.3175068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Academic performance prediction aims to leverage student-related information to predict their future academic outcomes, which is beneficial to numerous educational applications, such as personalized teaching and academic early warning. In this article, we reveal the students’ behavior trajectories by mining campus smartcard records, and capture the characteristics inherent in trajectories for academic performance prediction. Particularly, we carefully design a tri-branch convolutional neural network (CNN) architecture, which is equipped with rowwise, columnwise, and depthwise convolutions and attention operations, to effectively capture the persistence, regularity, and temporal distribution of student behavior in an end-to-end manner, respectively. However, different from existing works mainly targeting at improving the prediction performance for the whole students, we propose to cast academic performance prediction as a top- $k$ ranking problem, and introduce a top- $k$ focused loss to ensure the accuracy of identifying academically at-risk students. Extensive experiments were carried out on a large-scale real-world dataset, and we show that our approach substantially outperforms recently proposed methods for academic performance prediction. For the sake of reproducibility, our codes have been released at https://github.com/ZongJ1111/Academic-Performance-Prediction .},
  archive      = {J_TNNLS},
  author       = {Chaoran Cui and Jian Zong and Yuling Ma and Xinhua Wang and Lei Guo and Meng Chen and Yilong Yin},
  doi          = {10.1109/TNNLS.2022.3175068},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {439-450},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tri-branch convolutional neural networks for top-k focused academic performance prediction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-based output quantized synchronization control for
multiple delayed neural networks. <em>TNNLS</em>, <em>35</em>(1),
428–438. (<a href="https://doi.org/10.1109/TNNLS.2022.3175027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concentrates on the global exponential synchronization problem of multiple neural networks with time delay by the event-based output quantized coupling control method. In order to reduce the signal transmission cost and avoid the difficulty of obtaining the systems’ full states, this article adopts the event-triggered control and output quantized control. A new dynamic event-triggered mechanism is designed, in which the control parameters are time-varying functions. Under weakened coupling matrix conditions, by using a Halanay-type inequality, some simple and easily verified sufficient conditions to ensure the exponential synchronization of multiple neural networks are presented. Moreover, the Zeno behaviors of the system are excluded. Some numerical examples are given to verify the effectiveness of the theoretical analysis in this article.},
  archive      = {J_TNNLS},
  author       = {Yue Chen and Song Zhu and Mouquan Shen and Xiaoyang Liu and Shiping Wen},
  doi          = {10.1109/TNNLS.2022.3175027},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {428-438},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-based output quantized synchronization control for multiple delayed neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Event-triggered distributed data-driven iterative learning
bipartite formation control for unknown nonlinear multiagent systems.
<em>TNNLS</em>, <em>35</em>(1), 417–427. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we investigate the event-triggering time-varying trajectory bipartite formation tracking problem for a class of unknown nonaffine nonlinear discrete-time multiagent systems (MASs). We first obtain an equivalent linear data model with a dynamic parameter of each agent by employing the pseudo-partial-derivative technique. Then, we propose an event-triggered distributed model-free adaptive iterative learning bipartite formation control scheme by using the input/output data of MASs without employing either the plant structure or any knowledge of the dynamics. To improve the flexibility and network communication resource utilization, we construct an observer-based event-triggering mechanism with a dead-zone operator. Furthermore, we rigorously prove the convergence of the proposed algorithm, where each agent’s time-varying trajectory bipartite formation tracking error is reduced to a small range around zero. Finally, four simulation studies further validate the designed control approach’s effectiveness, demonstrating that the proposed scheme is also suitable for the homogeneous MASs to achieve time-varying trajectory bipartite formation tracking.},
  archive      = {J_TNNLS},
  author       = {Huarong Zhao and Hongnian Yu and Li Peng},
  doi          = {10.1109/TNNLS.2022.3174885},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {417-427},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered distributed data-driven iterative learning bipartite formation control for unknown nonlinear multiagent systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proportional-integral observer-based state estimation for
markov memristive neural networks with sensor saturations.
<em>TNNLS</em>, <em>35</em>(1), 405–416. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the resilient proportional-integral observer (PIO) problem for Markov switching memristive neural networks (MSMNNs) with randomly occurring sensor saturation within a finite-time interval. The Markov switching of memristive neural networks is regulated by a higher level deterministic switching signal, whose transition probabilities are piecewise time-varying and can be depicted by the average dwell-time strategy. Meanwhile, a Bernoulli stochastic process associated with an uncertain packet arriving rate is adopted to describe the randomly occurring sensor saturation. The aim is to design a resilient PIO such that the augmented dynamic has the property of stochastic finite-time boundedness while meeting the desired $\mathcal {H}_{\infty }$ performance index. By applying the Lyapunov method and the average dwell-time scheme, sufficient criteria are established for MSMNNs, and a unified design method is presented for the existence of the PIO. Lastly, the attained theoretical results are validated via a numerical simulation.},
  archive      = {J_TNNLS},
  author       = {Jun Cheng and Lidan Liang and Huaicheng Yan and Jinde Cao and Shengda Tang and Kaibo Shi},
  doi          = {10.1109/TNNLS.2022.3174880},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {405-416},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Proportional-integral observer-based state estimation for markov memristive neural networks with sensor saturations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive learning-based dual dynamic GCN for SAR image
scene classification. <em>TNNLS</em>, <em>35</em>(1), 390–404. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a typical label-limited task, it is significant and valuable to explore networks that enable to utilize labeled and unlabeled samples simultaneously for synthetic aperture radar (SAR) image scene classification. Graph convolutional network (GCN) is a powerful semisupervised learning paradigm that helps to capture the topological relationships of scenes in SAR images. While the performance is not satisfactory when existing GCNs are directly used for SAR image scene classification with limited labels, because few methods to characterize the nodes and edges for SAR images. To tackle these issues, we propose a contrastive learning-based dual dynamic GCN (DDGCN) for SAR image scene classification. Specifically, we design a novel contrastive loss to capture the structures of views and scenes, and develop a clustering-based contrastive self-supervised learning model for mapping SAR images from pixel space to high-level embedding space, which facilitates the subsequent node representation and message passing in GCNs. Afterward, we propose a multiple features and parameter sharing dual network framework called DDGCN. One network is a dynamic GCN to keep the local consistency and nonlocal dependency of the same scene with the help of a node attention module and a dynamic correlation matrix learning algorithm. The other is a multiscale and multidirectional fully connected network (FCN) to enlarge the discrepancies between different scenes. Finally, the features obtained by the two branches are fused for classification. A series of experiments on synthetic and real SAR images demonstrate that the proposed method achieves consistently better classification performance than the existing methods.},
  archive      = {J_TNNLS},
  author       = {Fang Liu and Xiaoxue Qian and Licheng Jiao and Xiangrong Zhang and Lingling Li and Yuanhao Cui},
  doi          = {10.1109/TNNLS.2022.3174873},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {390-404},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Contrastive learning-based dual dynamic GCN for SAR image scene classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wavelet probabilistic neural networks. <em>TNNLS</em>,
<em>35</em>(1), 376–389. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel wavelet probabilistic neural network (WPNN), which is a generative-learning wavelet neural network that relies on the wavelet-based estimation of class probability densities, is proposed. In this new neural network approach, the number of basis functions employed is independent of the number of data inputs, and in that sense, it overcomes the well-known drawback of traditional probabilistic neural networks (PNNs). Since the parameters of the proposed network are updated at a low and constant computational cost, it is particularly aimed at data stream classification and anomaly detection in off-line settings and online environments where the length of data is assumed to be unconstrained. Both synthetic and real-world datasets are used to assess the proposed WPNN. Significant performance enhancements are attained compared to state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Edgar S. García-Treviño and Pu Yang and Javier A. Barria},
  doi          = {10.1109/TNNLS.2022.3174705},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {376-389},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Wavelet probabilistic neural networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware portfolio management with risk-sensitive
multiagent network. <em>TNNLS</em>, <em>35</em>(1), 362–375. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep neural networks (DNNs) have gained considerable attention in recent years, there have been several cases applying DNNs to portfolio management (PM). Although some researchers have experimentally demonstrated its ability to make a profit, it is still insufficient to use in real situations because existing studies have failed to answer how risky investment decisions are. Furthermore, even though the objective of PM is to maximize returns within a risk tolerance, they overlook the predictive uncertainty of DNNs in the process of risk management. To overcome these limitations, we propose a novel framework called risk-sensitive multiagent network (RSMAN), which includes risk-sensitive agents (RSAs) and a risk adaptive portfolio generator (RAPG). Standard DNNs do not understand the risks of their decision, whereas RSA can take risk-sensitive decisions by estimating market uncertainty and parameter uncertainty. Acting as a trader, this agent is trained via reinforcement learning from dynamic trading simulations to estimate the distribution of reward and via unsupervised learning to assess parameter uncertainty without labeled data. We also present an RAPG that can generate a portfolio fitting the user’s risk appetite without retraining by exploiting the estimated information from the RSAs. We tested our framework on the U.S. and Korean real financial markets to demonstrate the practicality of the RSMAN.},
  archive      = {J_TNNLS},
  author       = {Kidon Park and Hong-Gyu Jung and Tae-San Eom and Seong-Whan Lee},
  doi          = {10.1109/TNNLS.2022.3174642},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {362-375},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Uncertainty-aware portfolio management with risk-sensitive multiagent network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of an adaptive artifact subspace reconstruction
based on hebbian/anti-hebbian learning networks for enhancing BCI
performance. <em>TNNLS</em>, <em>35</em>(1), 348–361. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–computer interface (BCI) actively translates the brain signals into executable actions by establishing direct communication between the human brain and external devices. Recording brain activity through electroencephalography (EEG) is generally contaminated with both physiological and nonphysiological artifacts, which significantly hinders the BCI performance. Artifact subspace reconstruction (ASR) is a well-known statistical technique that automatically removes artifact components by determining the rejection threshold based on the initial reference EEG segment in multichannel EEG recordings. In real-world applications, the fixed threshold may limit the efficacy of the artifact correction, especially when the quality of the reference data is poor. This study proposes an adaptive online ASR technique by integrating the Hebbian/anti-Hebbian neural networks into the ASR algorithm, namely, principle subspace projection ASR (PSP-ASR) and principal subspace whitening ASR (PSW-ASR) that segmentwise self-organize the artifact subspace by updating the synaptic weights according to the Hebbian and anti-Hebbian learning rules. The effectiveness of the proposed algorithm is compared to the conventional ASR approaches on benchmark EEG dataset and three BCI frameworks, including steady-state visual evoked potential (SSVEP), rapid serial visual presentation (RSVP), and motor imagery (MI) by evaluating the root-mean-square error (RMSE), the signal-to-noise ratio (SNR), the Pearson correlation, and classification accuracy. The results demonstrated that the PSW-ASR algorithm effectively removed the EEG artifacts and retained the activity-specific brain signals compared to the PSP-ASR, standard ASR (Init-ASR), and moving-window ASR (MW-ASR) methods, thereby enhancing the SSVEP, RSVP, and MI BCI performances. Finally, our empirical results from the PSW-ASR algorithm suggested the choice of an aggressive cutoff range of ${c =}\,\,1$ –10 for activity-specific BCI applications and a moderate range of ${c &amp;gt; }10$ for the benchmark dataset and general BCI applications.},
  archive      = {J_TNNLS},
  author       = {Bo-Yu Tsai and Sandeep Vara Sankar Diddi and Li-Wei Ko and Shuu-Jiun Wang and Chi-Yuan Chang and Tzyy-Ping Jung},
  doi          = {10.1109/TNNLS.2022.3174528},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {348-361},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Development of an adaptive artifact subspace reconstruction based on Hebbian/Anti-hebbian learning networks for enhancing BCI performance},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative output regulation for linear multiagent systems
via distributed fixed-time event-triggered control. <em>TNNLS</em>,
<em>35</em>(1), 338–347. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the cooperative output regulation for linear multiagent systems (MASs) via the distributed event-triggered strategy in fixed time. A novel fixed-time event-triggered control protocol is proposed using a dynamic compensator method. It is shown that based on the designed control scheme, the cooperative output regulation problem is addressed in fixed time and the agents in the communication network are subject to intermittent communication with their neighbors. Simultaneously, with the proposed event-triggering mechanism, Zeno behavior can be ruled out by choosing the appropriate parameters. Different from the existing strategies, both the compensator and control law are designed with intermittent communication in fixed time, where the convergence time is independent of any initial conditions. Moreover, for the case that the states are not available, the output regulation problem can further be addressed by the distributed observer-based output feedback controller with the fixed-time event-triggered compensator and event-triggered mechanism. Finally, a simulation example is provided to illustrate the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Zheng Zhang and Shiming Chen and Yuanshi Zheng},
  doi          = {10.1109/TNNLS.2022.3174416},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {338-347},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cooperative output regulation for linear multiagent systems via distributed fixed-time event-triggered control},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Epoch-evolving gaussian process guided learning for
classification. <em>TNNLS</em>, <em>35</em>(1), 326–337. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional mini-batch gradient descent algorithms are usually trapped in the local batch-level distribution information, resulting in the “zig-zag” effect in the learning process. To characterize the correlation information between the batch-level distribution and the global data distribution, we propose a novel learning scheme called epoch-evolving Gaussian process guided learning (GPGL) to encode the global data distribution information in a non-parametric way. Upon a set of class-aware anchor samples, our GP model is built to estimate the class distribution for each sample in mini-batch through label propagation from the anchor samples to the batch samples. The class distribution, also named the context label, is provided as a complement for the ground-truth one-hot label. Such a class distribution structure has a smooth property and usually carries a rich body of contextual information that is capable of speeding up the convergence process. With the guidance of the context label and ground-truth label, the GPGL scheme provides a more efficient optimization through updating the model parameters with a triangle consistency loss. Furthermore, our GPGL scheme can be generalized and naturally applied to the current deep models, outperforming the state-of-the-art optimization methods on six benchmark datasets.},
  archive      = {J_TNNLS},
  author       = {Jiabao Cui and Xuewei Li and Hanbin Zhao and Hui Wang and Bin Li and Xi Li},
  doi          = {10.1109/TNNLS.2022.3174207},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {326-337},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Epoch-evolving gaussian process guided learning for classification},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MEN: Mutual enhancement networks for sign language
recognition and education. <em>TNNLS</em>, <em>35</em>(1), 311–325. (<a
href="https://doi.org/10.1109/TNNLS.2022.3174031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of existing sign language recognition approaches is typically limited by the scale of training data. To address this issue, we propose a mutual enhancement network (MEN) for joint sign language recognition and education. First, a sign language recognition system built upon a spatial–temporal network is proposed to recognize the semantic category of a given sign language video. Besides, a sign language education system is developed to detect the failure modes of learners and further guide them to sign correctly. Our theoretical contribution lies in formulating the above two systems as an estimation–maximization (EM) framework, which can progressively boost each other. The recognition system could become more robust and accurate with more training data collected by the education system, while the education system could guide the learners to sign more precisely, benefiting from the hand shape analysis module of the recognition system. Experimental results on three large-scale sign language recognition datasets validate the superiority of the proposed framework.},
  archive      = {J_TNNLS},
  author       = {Zhengzhe Liu and Lei Pang and Xiaojuan Qi},
  doi          = {10.1109/TNNLS.2022.3174031},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {311-325},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MEN: Mutual enhancement networks for sign language recognition and education},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Parameter-free and scalable incomplete multiview clustering
with prototype graph. <em>TNNLS</em>, <em>35</em>(1), 300–310. (<a
href="https://doi.org/10.1109/TNNLS.2022.3173742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering (MVC) seamlessly combines homogeneous information and allocates data samples into different communities, which has shown significant effectiveness for unsupervised tasks in recent years. However, some views of samples may be incomplete due to unfinished data collection or storage failure in reality, which refers to the so-called incomplete multiview clustering (IMVC). Despite many IMVC pioneer frameworks have been introduced, the majority of their approaches are limited by the cubic time complexity and quadratic space complexity which heavily prevent them from being employed in large-scale IMVC tasks. Moreover, the massively introduced hyper-parameters in existing methods are not practical in real applications. Inspired by recent unsupervised multiview prototype progress, we propose a novel parameter-free and scalable incomplete multiview clustering framework with the prototype graph termed PSIMVC-PG to solve the aforementioned issues. Different from existing full pair-wise graph studying, we construct an incomplete prototype graph to flexibly capture the relations between existing instances and discriminate prototypes. Moreover, PSIMVC-PG can directly obtain the prototype graph without pre-process of searching hyper-parameters. We conduct massive experiments on various incomplete multiview tasks, and the performances show clear advantages over existing methods. The code of PSIMVC-PG can be publicly downloaded at https://github.com/wangsiwei2010/PSIMVC-PG .},
  archive      = {J_TNNLS},
  author       = {Miaomiao Li and Siwei Wang and Xinwang Liu and Suyuan Liu},
  doi          = {10.1109/TNNLS.2022.3173742},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {300-310},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter-free and scalable incomplete multiview clustering with prototype graph},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Secure communication via chaotic synchronization based on
reservoir computing. <em>TNNLS</em>, <em>35</em>(1), 285–299. (<a
href="https://doi.org/10.1109/TNNLS.2022.3173516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information security occupies a very important part of national security. Chaos communication can provide high-level physical layer security, but its harsh claims on the chaotic system parameters of the transmitter and the receiver resulting in reduced synchronization coefficient and more difficult consistent synchronization of point to multipoint networking. In this article, a chaotic synchronization and communication system based on reservoir computing (RC) has been proposed. In this scheme, the trained RC highly synchronized with the emitter acts as the receiver with simplified structure under the premise of ensuring safety. Simultaneously, the cross-prediction algorithm has been proposed to weaken the accumulation effect of prediction synchronization error of RC and facilitate the realization of long-term communication. Furthermore, the tolerance of the system performance to the signal-to-noise ratio with the variations of the mask coefficients has been investigated, and the optimal operation point under the condition of the adjustable number of nodes and leakage rate of RC has been numerically analyzed. The simulation results show that the normalized mean-square error of synchronization of 10−6 magnitude and the bit error rate of decryption at 10−8 level can be obtained. Finally, from the operational perspective, a 100-m short-distance experiment confirms that its communication performance is consistent with the simulation results. We strongly believe that the proposed system offers the opportunity of a new research direction in chaotic secure communications.},
  archive      = {J_TNNLS},
  author       = {Jiayue Liu and Jianguo Zhang and Yuncai Wang},
  doi          = {10.1109/TNNLS.2022.3173516},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {285-299},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Secure communication via chaotic synchronization based on reservoir computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward accurate binarized neural networks with sparsity for
mobile application. <em>TNNLS</em>, <em>35</em>(1), 272–284. (<a
href="https://doi.org/10.1109/TNNLS.2022.3173498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While binarized neural networks (BNNs) have attracted great interest, popular approaches proposed so far mainly exploit the symmetric $sign$ function for feature binarization, i.e., to binarize activations into −1 and +1 with a fixed threshold of 0. However, whether this option is optimal has been largely overlooked. In this work, we propose the Sparsity-inducing BNN (Si-BNN) to quantize the activations to be either 0 or +1, which better approximates ReLU using 1-bit. We further introduce trainable thresholds into the backward function of binarization to guide the gradient propagation. Our method dramatically outperforms the current state-of-the-art, lowering the performance gap between full-precision networks and BNNs on mainstream architectures, achieving the new state-of-the-art on binarized AlexNet (Top-1 50.5%), ResNet-18 (Top-1 62.2%), and ResNet-50 (Top-1 68.3%). At inference time, Si-BNN still enjoys the high efficiency of bit-wise operations. In our implementation, the running time of binary AlexNet on the CPU can be competitive with the popular GPU-based deep learning framework.},
  archive      = {J_TNNLS},
  author       = {Peisong Wang and Xiangyu He and Jian Cheng},
  doi          = {10.1109/TNNLS.2022.3173498},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {272-284},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward accurate binarized neural networks with sparsity for mobile application},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structured domain adaptation with online relation
regularization for unsupervised person re-ID. <em>TNNLS</em>,
<em>35</em>(1), 258–271. (<a
href="https://doi.org/10.1109/TNNLS.2022.3173489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims at adapting the model trained on a labeled source-domain dataset to an unlabeled target-domain dataset. The task of UDA on open-set person reidentification (re-ID) is even more challenging as the identities (classes) do not have overlap between the two domains. One major research direction was based on domain translation, which, however, has fallen out of favor in recent years due to inferior performance compared with pseudo-label-based methods. We argue that domain translation has great potential on exploiting valuable source-domain data but the existing methods did not provide proper regularization on the translation process. Specifically, previous methods only focus on maintaining the identities of the translated images while ignoring the intersample relations during translation. To tackle the challenges, we propose an end-to-end structured domain adaptation framework with an online relation-consistency regularization term. During training, the person feature encoder is optimized to model intersample relations on-the-fly for supervising relation-consistency domain translation, which in turn improves the encoder with informative translated images. The encoder can be further improved with pseudo labels, where the source-to-target translated images with ground-truth identities and target-domain images with pseudo identities are jointly used for training. In the experiments, our proposed framework is shown to achieve state-of-the-art performance on multiple UDA tasks of person re-ID. With the synthetic $\to $ real translated images from our structured domain-translation network, we achieved second place in the Visual Domain Adaptation Challenge (VisDA) in 2020.},
  archive      = {J_TNNLS},
  author       = {Yixiao Ge and Feng Zhu and Dapeng Chen and Rui Zhao and Xiaogang Wang and Hongsheng Li},
  doi          = {10.1109/TNNLS.2022.3173489},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {258-271},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Structured domain adaptation with online relation regularization for unsupervised person re-ID},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TryonCM2: Try-on-enhanced fashion compatibility modeling
framework. <em>TNNLS</em>, <em>35</em>(1), 246–257. (<a
href="https://doi.org/10.1109/TNNLS.2022.3173295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, fashion compatibility modeling, which can score the matching degree of several complementary fashion items, has gained increasing research attention. Previous studies have primarily learned the features of fashion items and utilize their interaction as the fashion compatibility. However, the try-on looking of an outfit help us to learn the fashion compatibility in a combined manner, where items are spatially distributed and partially covered by other items. Inspired by this, we design a try-on-enhanced fashion compatibility modeling framework, named TryonCM2, which incorporates the try-on appearance with the item interaction to enhance the fashion compatibility modeling. Specifically, we treat each outfit as a sequence of items and adopt the bidirectional long short-term memory (LSTM) network to capture the latent interaction of fashion items. Meanwhile, we synthesize a try-on template image to depict the try-on appearance of an outfit. And then, we regard the outfit as a sequence of multiple image stripes, i.e., local content, of the try-on template, and adopt the bidirectional LSTM network to capture the contextual structure in the try-on appearance. Ultimately, we combine the fashion compatibility lying in the item interaction and try-on appearance as the final compatibility of the outfit. Both the objective and subjective experiments on the existing FOTOS dataset demonstrate the superiority of our framework over the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Xue Dong and Xuemeng Song and Na Zheng and Jianlong Wu and Hongjun Dai and Liqiang Nie},
  doi          = {10.1109/TNNLS.2022.3173295},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {246-257},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TryonCM2: Try-on-enhanced fashion compatibility modeling framework},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised enhancement for named entity disambiguation
via multimodal graph convolution. <em>TNNLS</em>, <em>35</em>(1),
231–245. (<a href="https://doi.org/10.1109/TNNLS.2022.3173179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity disambiguation (NED) finds the specific meaning of an entity mention in a particular context and links it to a target entity. With the emergence of multimedia, the modalities of content on the Internet have become more diverse, which poses difficulties for traditional NED, and the vast amounts of information make it impossible to manually label every kind of ambiguous data to train a practical NED model. In response to this situation, we present MMGraph, which uses multimodal graph convolution to aggregate visual and contextual language information for accurate entity disambiguation for short texts, and a self-supervised simple triplet network (SimTri) that can learn useful representations in multimodal unlabeled data to enhance the effectiveness of NED models. We evaluated these approaches on a new dataset, MMFi, which contains multimodal supervised data and large amounts of unlabeled data. Our experiments confirm the state-of-the-art performance of MMGraph on two widely used benchmarks and MMFi. SimTri further improves the performance of NED methods. The dataset and code are available at https://github.com/LanceZPF/NNED_MMGraph .},
  archive      = {J_TNNLS},
  author       = {Pengfei Zhou and Kaining Ying and Zhenhua Wang and Dongyan Guo and Cong Bai},
  doi          = {10.1109/TNNLS.2022.3173179},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {231-245},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised enhancement for named entity disambiguation via multimodal graph convolution},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial information bottleneck. <em>TNNLS</em>,
<em>35</em>(1), 221–230. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information bottleneck (IB) principle has been adopted to explain deep learning in terms of information compression and prediction, which are balanced by a tradeoff hyperparameter. How to optimize the IB principle for better robustness and figure out the effects of compression through the tradeoff hyperparameter are two challenging problems. Previous methods attempted to optimize the IB principle by introducing random noise into learning the representation and achieved the state-of-the-art performance in the nuisance information compression and semantic information extraction. However, their performance on resisting adversarial perturbations is far less impressive. To this end, we propose an adversarial IB (AIB) method without any explicit assumptions about the underlying distribution of the representations, which can be optimized effectively by solving a min–max optimization problem. Numerical experiments on synthetic and real-world datasets demonstrate its effectiveness on learning more invariant representations and mitigating adversarial perturbations compared to several competing IB methods. In addition, we analyze the adversarial robustness of diverse IB methods contrasting with their IB curves and reveal that IB models with the hyperparameter $\beta $ corresponding to the knee point in the IB curve achieve the best tradeoff between compression and prediction and has the best robustness against various attacks.},
  archive      = {J_TNNLS},
  author       = {Penglong Zhai and Shihua Zhang},
  doi          = {10.1109/TNNLS.2022.3172986},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {221-230},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial information bottleneck},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EDCompress: Energy-aware model compression for dataflows.
<em>TNNLS</em>, <em>35</em>(1), 208–220. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge devices demand low energy consumption, cost, and small form factor. To efficiently deploy convolutional neural network (CNN) models on the edge device, energy-aware model compression becomes extremely important. However, existing work did not study this problem well because of the lack of considering the diversity of dataflow types in hardware architectures. In this article, we propose EDCompress (EDC), an energy-aware model compression method for various dataflows. It can effectively reduce the energy consumption of various edge devices, with different dataflow types. Considering the very nature of model compression procedures, we recast the optimization process to a multistep problem and solve it by reinforcement learning algorithms. We also propose a multidimensional multistep (MDMS) optimization method, which shows higher compressing capability than the traditional multistep method. Experiments show that EDC could improve $20\times $ , $17\times $ , and $26\times $ energy efficiency in VGG-16, MobileNet, and LeNet-5 networks, respectively, with negligible loss of accuracy. EDC could also indicate the optimal dataflow type for specific neural networks in terms of energy consumption, which can guide the deployment of CNN on hardware.},
  archive      = {J_TNNLS},
  author       = {Zhehui Wang and Tao Luo and Rick Siow Mong Goh and Joey Tianyi Zhou},
  doi          = {10.1109/TNNLS.2022.3172941},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {208-220},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {EDCompress: Energy-aware model compression for dataflows},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multigraph fusion for dynamic graph convolutional network.
<em>TNNLS</em>, <em>35</em>(1), 196–207. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional network (GCN) outputs powerful representation by considering the structure information of the data to conduct representation learning, but its robustness is sensitive to the quality of both the feature matrix and the initial graph. In this article, we propose a novel multigraph fusion method to produce a high-quality graph and a low-dimensional space of original high-dimensional data for the GCN model. Specifically, the proposed method first extracts the common information and the complementary information among multiple local graphs to obtain a unified local graph, which is then fused with the global graph of the data to obtain the initial graph for the GCN model. As a result, the proposed method conducts the graph fusion process twice to simultaneously learn the low-dimensional space and the intrinsic graph structure of the data in a unified framework. Experimental results on real datasets demonstrated that our method outperformed the comparison methods in terms of classification tasks.},
  archive      = {J_TNNLS},
  author       = {Jiangzhang Gan and Rongyao Hu and Yujie Mo and Zhao Kang and Liang Peng and Yonghua Zhu and Xiaofeng Zhu},
  doi          = {10.1109/TNNLS.2022.3172588},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {196-207},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multigraph fusion for dynamic graph convolutional network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VGN: Value decomposition with graph attention networks for
multiagent reinforcement learning. <em>TNNLS</em>, <em>35</em>(1),
182–195. (<a href="https://doi.org/10.1109/TNNLS.2022.3172572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although value decomposition networks and the follow on value-based studies factorizes the joint reward function to individual reward functions for a kind of cooperative multiagent reinforcement problem, in which each agent has its local observation and shares a joint reward signal, most of the previous efforts, however, ignored the graphical information between agents. In this article, a new value decomposition with graph attention network (VGN) method is developed to solve the value functions by introducing the dynamical relationships between agents. It is pointed out that the decomposition factor of an agent in our approach can be influenced by the reward signals of all the related agents and two graphical neural network-based algorithms (VGN-Linear and VGN-Nonlinear) are designed to solve the value functions of each agent. It can be proved theoretically that the present methods satisfy the factorizable condition in the centralized training process. The performance of the present methods is evaluated on the StarCraft Multiagent Challenge (SMAC) benchmark. Experiment results show that our method outperforms the state-of-the-art value-based multiagent reinforcement algorithms, especially when the tasks are with very hard level and challenging for existing methods.},
  archive      = {J_TNNLS},
  author       = {Qinglai Wei and Yugu Li and Jie Zhang and Fei-Yue Wang},
  doi          = {10.1109/TNNLS.2022.3172572},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {182-195},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {VGN: Value decomposition with graph attention networks for multiagent reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite-time output synchronization of multiple weighted
reaction–diffusion neural networks with adaptive output couplings.
<em>TNNLS</em>, <em>35</em>(1), 169–181. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article mainly considers the output synchronization (OS) problem of multiple weighted and adaptive output coupled reaction–diffusion neural networks (RDNNs) without and with coupling delays in finite time. Without coupling delays, an adaptive control law and an output feedback controller are, respectively, proposed to ensure that the multiple weighted and output coupled RDNNs are output synchronized and $H_{\infty }$ output synchronized in finite time. With coupling delays, an adaptive coupling weights control scheme and a novel feedback controller are put forward to make the multiple weighted RDNNs with output couplings achieve OS in finite time. Moreover, the finite-time $H_{\infty }$ OS is considered in the presence of external disturbances. By the Lyapunov approach, several finite-time OS and $H_{\infty }$ OS criteria are given. Finally, two simulation examples are presented to justify the effectiveness of the proposed adaptive control laws and controllers.},
  archive      = {J_TNNLS},
  author       = {Qian Qiu and Housheng Su},
  doi          = {10.1109/TNNLS.2022.3172490},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {169-181},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time output synchronization of multiple weighted Reaction–Diffusion neural networks with adaptive output couplings},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed observer-based leader–follower consensus of
multiple euler–lagrange systems. <em>TNNLS</em>, <em>35</em>(1),
157–168. (<a href="https://doi.org/10.1109/TNNLS.2022.3172484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the leader–follower consensus problem of multiple Euler–Lagrange (EL) systems, where each agent suffers uncertain external disturbances, and the communication links among agents experience faults. Besides, we consider a more general case that only a portion of followers can measure partial components of leader’s output and access the dynamic information of leader. The main idea of solving the consensus problem in this article is proceeded in two steps. First, we design an adaptive distributed observer to estimate the full state information of leader in real time with resilience to communication link faults. Second, based on the proposed distributed observer, we propose a proportional–integral (PI) control protocol for each agent to track the trajectory of leader, which is model-independent and robust to uncertain external disturbances. Distinct from the existing leader–follower consensus protocols of multiple EL systems, the proposed distributed observer-based PI consensus protocol in this article is model-independent, which is irrelevant to the structures or features of EL system model. Finally, we present a simulation example to show the resilience of the above adaptive distributed observer and the robustness of the distributed observer-based consensus protocol.},
  archive      = {J_TNNLS},
  author       = {Mingkang Long and Housheng Su and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2022.3172484},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {157-168},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed observer-based Leader–Follower consensus of multiple Euler–Lagrange systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Distributed constrained optimization with delayed
subgradient information over time-varying network under adaptive
quantization. <em>TNNLS</em>, <em>35</em>(1), 143–156. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider a distributed constrained optimization problem with delayed subgradient information over the time-varying communication network, where each agent can only communicate with its neighbors and the communication channel has a limited data rate. We propose an adaptive quantization method to address this problem. A mirror descent algorithm with delayed subgradient information is established based on the theory of Bregman divergence. With a non-Euclidean Bregman projection-based scheme, the proposed method essentially generalizes many previous classical Euclidean projection-based distributed algorithms. Through the proposed adaptive quantization method, the optimal value without any quantization error can be obtained. Furthermore, comprehensive analysis on the convergence of the algorithm is carried out and our results show that the optimal convergence rate $O(1/ (T)^{1/2})$ can be obtained under appropriate conditions. Finally, numerical examples are presented to demonstrate the effectiveness of our results.},
  archive      = {J_TNNLS},
  author       = {Jie Liu and Zhan Yu and Daniel W. C. Ho},
  doi          = {10.1109/TNNLS.2022.3172450},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {143-156},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed constrained optimization with delayed subgradient information over time-varying network under adaptive quantization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully convolutional network-based self-supervised learning
for semantic segmentation. <em>TNNLS</em>, <em>35</em>(1), 132–142. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning has achieved great success in many computer vision tasks, its performance relies on the availability of large datasets with densely annotated samples. Such datasets are difficult and expensive to obtain. In this article, we focus on the problem of learning representation from unlabeled data for semantic segmentation. Inspired by two patch-based methods, we develop a novel self-supervised learning framework by formulating the jigsaw puzzle problem as a patch-wise classification problem and solving it with a fully convolutional network. By learning to solve a jigsaw puzzle comprising 25 patches and transferring the learned features to semantic segmentation task, we achieve a 5.8% point improvement on the Cityscapes dataset over the baseline model initialized from random values. It is noted that we use only about 1/6 training images of Cityscapes in our experiment, which is designed to imitate the real cases where fully annotated images are usually limited to a small number. We also show that our self-supervised learning method can be applied to different datasets and models. In particular, we achieved competitive performance with the state-of-the-art methods on the PASCAL VOC2012 dataset using significantly fewer time costs on pretraining.},
  archive      = {J_TNNLS},
  author       = {Zhengeng Yang and Hongshan Yu and Yong He and Wei Sun and Zhi-Hong Mao and Ajmal Mian},
  doi          = {10.1109/TNNLS.2022.3172423},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {132-142},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully convolutional network-based self-supervised learning for semantic segmentation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BENN: Bias estimation using a deep neural network.
<em>TNNLS</em>, <em>35</em>(1), 117–131. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing existing methods for bias detection in machine learning (ML) models is challenging since each method: 1) explores a different ethical aspect of bias, which may result in contradictory output among the different methods; 2) provides output in a different range/scale and therefore cannot be compared with other methods; and 3) requires different input, thereby requiring a human expert’s involvement to adjust each method according to the model examined. In this article, we present BENN, a novel bias estimation method that uses a pretrained unsupervised deep neural network. Given an ML model and data samples, BENN provides a bias estimation for every feature based on the examined model’s predictions. We evaluated BENN using three benchmark datasets, one proprietary churn prediction model used by a European telecommunications company, and a synthetic dataset that includes both a biased feature and a fair one. BENN’s results were compared with an ensemble of 21 existing bias estimation methods. The evaluation results show that BENN provides bias estimations that are aligned with those of the ensemble while offering significant advantages, including the fact that it is a generic approach (i.e., can be applied to any ML model) and does not require a domain expert.},
  archive      = {J_TNNLS},
  author       = {Amit Giloni and Edita Grolman and Tanja Hagemann and Ronald Fromm and Sebastian Fischer and Yuval Elovici and Asaf Shabtai},
  doi          = {10.1109/TNNLS.2022.3172365},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {117-131},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BENN: Bias estimation using a deep neural network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated generalized face presentation attack detection.
<em>TNNLS</em>, <em>35</em>(1), 103–116. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face presentation attack detection (fPAD) plays a critical role in the modern face recognition pipeline. An fPAD model with good generalization can be obtained when it is trained with face images from different input distributions and different types of spoof attacks. In reality, training data (both real face images and spoof images) are not directly shared between data owners due to legal and privacy issues. In this article, with the motivation of circumventing this challenge, we propose a federated face presentation attack detection (FedPAD) framework that simultaneously takes advantage of rich fPAD information available at different data owners while preserving data privacy. In the proposed framework, each data owner (referred to as data centers) locally trains its own fPAD model. A server learns a global fPAD model by iteratively aggregating model updates from all data centers without accessing private data in each of them. Once the learned global model converges, it is used for fPAD inference. To equip the aggregated fPAD model in the server with better generalization ability to unseen attacks from users, following the basic idea of FedPAD, we further propose a federated generalized face presentation attack detection (FedGPAD) framework. A federated domain disentanglement strategy is introduced in FedGPAD, which treats each data center as one domain and decomposes the fPAD model into domain-invariant and domain-specific parts in each data center. Two parts disentangle the domain-invariant and domain-specific features from images in each local data center. A server learns a global fPAD model by only aggregating domain-invariant parts of the fPAD models from data centers, and thus, a more generalized fPAD model can be aggregated in server. We introduce the experimental setting to evaluate the proposed FedPAD and FedGPAD frameworks and carry out extensive experiments to provide various insights about federated learning for fPAD.},
  archive      = {J_TNNLS},
  author       = {Rui Shao and Pramuditha Perera and Pong C. Yuen and Vishal M. Patel},
  doi          = {10.1109/TNNLS.2022.3172316},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {103-116},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Federated generalized face presentation attack detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AN-GCN: An anonymous graph convolutional network against
edge-perturbing attacks. <em>TNNLS</em>, <em>35</em>(1), 88–102. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have revealed the vulnerability of graph convolutional networks (GCNs) to edge-perturbing attacks, such as maliciously inserting or deleting graph edges. However, theoretical proof of such vulnerability remains a big challenge, and effective defense schemes are still open issues. In this article, we first generalize the formulation of edge-perturbing attacks and strictly prove the vulnerability of GCNs to such attacks in node classification tasks. Following this, an anonymous GCN, named AN-GCN, is proposed to defend against edge-perturbing attacks. In particular, we present a node localization theorem to demonstrate how GCNs locate nodes during their training phase. In addition, we design a staggered Gaussian noise-based node position generator and a spectral graph convolution-based discriminator (in detecting the generated node positions). Furthermore, we provide an optimization method for the designed generator and discriminator. It is demonstrated that the AN-GCN is secure against edge-perturbing attacks in node classification tasks, as AN-GCN is developed to classify nodes without the edge information (making it impossible for attackers to perturb edges anymore). Extensive evaluations verify the effectiveness of the general edge-perturbing attack (G-EPA) model in manipulating the classification results of the target nodes. More importantly, the proposed AN-GCN can achieve 82.7% in node classification accuracy without the edge-reading permission, which outperforms the state-of-the-art GCN.},
  archive      = {J_TNNLS},
  author       = {Ao Liu and Beibei Li and Tao Li and Pan Zhou and Rui Wang},
  doi          = {10.1109/TNNLS.2022.3172296},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {88-102},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AN-GCN: An anonymous graph convolutional network against edge-perturbing attacks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonlocal self-similarity-based weighted tensor low-rank
decomposition for multichannel image completion with mixture noise.
<em>TNNLS</em>, <em>35</em>(1), 73–87. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multichannel image completion with mixture noise is a challenging problem in the fields of machine learning, computer vision, image processing, and data mining. Traditional image completion models are not appropriate to deal with this problem directly since their reconstruction priors may mismatch corruption priors. To address this issue, we propose a novel nonlocal self-similarity-based weighted tensor low-rank decomposition (NSWTLD) model that can achieve global optimization and local enhancement. In the proposed model, based on the corruption priors and the reconstruction priors, a pixel weighting strategy is given to characterize the joint effects of missing data, the Gaussian noise, and the impulse noise. To discover and utilize the accurate nonlocal self-similarity information to enhance the restoration quality of the details, the traditional nonlocal learning framework is optimized by employing improved index determination of patch group and handling strip noise caused by patch overlapping. In addition, an efficient and convergent algorithm is presented to solve the NSWTLD model. Comprehensive experiments are conducted on four types of multichannel images under various corruption scenarios. The results demonstrate the efficiency and effectiveness of the proposed model.},
  archive      = {J_TNNLS},
  author       = {Mengying Xie and Xiaolan Liu and Xiaowei Yang},
  doi          = {10.1109/TNNLS.2022.3172184},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {73-87},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A nonlocal self-similarity-based weighted tensor low-rank decomposition for multichannel image completion with mixture noise},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Code-aligned autoencoders for unsupervised change detection
in multimodal remote sensing images. <em>TNNLS</em>, <em>35</em>(1),
60–72. (<a href="https://doi.org/10.1109/TNNLS.2022.3172183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image translation with convolutional autoencoders has recently been used as an approach to multimodal change detection (CD) in bitemporal satellite images. A main challenge is the alignment of the code spaces by reducing the contribution of change pixels to the learning of the translation function. Many existing approaches train the networks by exploiting supervised information of the change areas, which, however, is not always available. We propose to extract relational pixel information captured by domain-specific affinity matrices at the input and use this to enforce alignment of the code spaces and reduce the impact of change pixels on the learning objective. A change prior is derived in an unsupervised fashion from pixel pair affinities that are comparable across domains. To achieve code space alignment, we enforce pixels with similar affinity relations in the input domains to be correlated also in code space. We demonstrate the utility of this procedure in combination with cycle consistency. The proposed approach is compared with the state-of-the-art machine learning and deep learning algorithms. Experiments conducted on four real and representative datasets show the effectiveness of our methodology.},
  archive      = {J_TNNLS},
  author       = {Luigi Tommaso Luppino and Mads Adrian Hansen and Michael Kampffmeyer and Filippo Maria Bianchi and Gabriele Moser and Robert Jenssen and Stian Normann Anfinsen},
  doi          = {10.1109/TNNLS.2022.3172183},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {60-72},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Code-aligned autoencoders for unsupervised change detection in multimodal remote sensing images},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Costate-supplement ADP for model-free optimal control of
discrete-time nonlinear systems. <em>TNNLS</em>, <em>35</em>(1), 45–59.
(<a href="https://doi.org/10.1109/TNNLS.2022.3172126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive dynamic programming (ADP) scheme utilizing a costate function is proposed for optimal control of unknown discrete-time nonlinear systems. The state–action data are obtained by interacting with the environment under the iterative scheme without any model information. In contrast with the traditional ADP scheme, the collected data in the proposed algorithm are generated with different policies, which improves data utilization in the learning process. In order to approximate the cost function more accurately and to achieve a better policy improvement direction in the case of insufficient data, a separate costate network is introduced to approximate the costate function under the actor–critic framework, and the costate is utilized as supplement information to estimate the cost function more precisely. Furthermore, convergence properties of the proposed algorithm are analyzed to demonstrate that the costate function plays a positive role in the convergence process of the cost function based on the alternate iteration mode of the costate function and cost function under a mild assumption. The uniformly ultimately bounded (UUB) property of all the variables is proven by using the Lyapunov approach. Finally, two numerical examples are presented to demonstrate the effectiveness and computation efficiency of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jun Ye and Yougang Bian and Biao Luo and Manjiang Hu and Biao Xu and Rongjun Ding},
  doi          = {10.1109/TNNLS.2022.3172126},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {45-59},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Costate-supplement ADP for model-free optimal control of discrete-time nonlinear systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer adaptation learning: A decade survey.
<em>TNNLS</em>, <em>35</em>(1), 23–44. (<a
href="https://doi.org/10.1109/TNNLS.2022.3183326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world we see is ever-changing and it always changes with people, things, and the environment. Domain is referred to as the state of the world at a certain moment. A research problem is characterized as transfer adaptation learning (TAL) when it needs knowledge correspondence between different moments/domains. TAL aims to build models that can perform tasks of target domain by learning knowledge from a semantic-related but distribution different source domain. It is an energetic research field of increasing influence and importance, which is presenting a blowout publication trend. This article surveys the advances of TAL methodologies in the past decade, and the technical challenges and essential problems of TAL have been observed and discussed with deep insights and new perspectives. Broader solutions of TAL being created by researchers are identified, i.e., instance reweighting adaptation, feature adaptation, classifier adaptation, deep network adaptation, and adversarial adaptation, which are beyond the early semisupervised and unsupervised split. The survey helps researchers rapidly but comprehensively understand and identify the research foundation, research status, theoretical limitations, future challenges, and understudied issues (universality, interpretability, and credibility) to be broken in the field toward generalizable representation in open-world scenarios.},
  archive      = {J_TNNLS},
  author       = {Lei Zhang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2022.3183326},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {23-44},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transfer adaptation learning: A decade survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Backdoor learning: A survey. <em>TNNLS</em>, <em>35</em>(1),
5–22. (<a href="https://doi.org/10.1109/TNNLS.2022.3182979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backdoor attack intends to embed hidden backdoors into deep neural networks (DNNs), so that the attacked models perform well on benign samples, whereas their predictions will be maliciously changed if the hidden backdoor is activated by attacker-specified triggers. This threat could happen when the training process is not fully controlled, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, there is still no comprehensive and timely review of it. In this article, we present the first comprehensive survey of this realm. We summarize and categorize existing backdoor attacks and defenses based on their characteristics, and provide a unified framework for analyzing poisoning-based backdoor attacks. Besides, we also analyze the relation between backdoor attacks and relevant fields (i.e., adversarial attacks and data poisoning), and summarize widely adopted benchmark datasets. Finally, we briefly outline certain future research directions relying upon reviewed works. A curated list of backdoor-related resources is also available at https://github.com/THUYimingLi/backdoor-learning-resources .},
  archive      = {J_TNNLS},
  author       = {Yiming Li and Yong Jiang and Zhifeng Li and Shu-Tao Xia},
  doi          = {10.1109/TNNLS.2022.3182979},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {1},
  number       = {1},
  pages        = {5-22},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Backdoor learning: A survey},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
