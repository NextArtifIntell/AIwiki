<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc---212">TC - 212</h2>
<ul>
<li><details>
<summary>
(2024). Automatic generation and optimization framework of NoC-based
neural network accelerator through reinforcement learning. <em>TC</em>,
<em>73</em>(12), 2882–2896. (<a
href="https://doi.org/10.1109/TC.2024.3441822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choices of dataflows, which are known as intra-core neural network (NN) computation loop nest scheduling and inter-core hardware mapping strategies, play a critical role in the performance and energy efficiency of NoC-based neural network accelerators. Confronted with an enormous dataflow exploration space, this paper proposes an automatic framework for generating and optimizing the full-layer-mappings based on two reinforcement learning algorithms including A2C and PPO. Combining soft and hard constraints, this work transforms the mapping configuration into a sequential decision problem and aims to explore the performance and energy efficient hardware mapping for NoC systems. We evaluate the performance of the proposed framework on 10 experimental neural networks. The results show that compared with the direct-X mapping, the direct-Y mapping, GA-base mapping, and NN-aware mapping, our optimization framework reduces the average execution time of 10 experimental NNs by 9.09 $\%$ , improves the throughput by 11.27 $\%$ , reduces the energy by 12.62 $\%$ , and reduces the time-energy-product (TEP) by 14.49 $\%$ . The results also show that the performance enhancement is related to the coefficient of variation of the neural network to be computed.},
  archive      = {J_TC},
  author       = {Yongqi Xue and Jinlun Ji and Xinming Yu and Shize Zhou and Siyue Li and Xinyi Li and Tong Cheng and Shiping Li and Kai Chen and Zhonghai Lu and Li Li and Yuxiang Fu},
  doi          = {10.1109/TC.2024.3441822},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2882-2896},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Automatic generation and optimization framework of NoC-based neural network accelerator through reinforcement learning},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel lagrange multipliers-driven adaptive offloading for
vehicular edge computing. <em>TC</em>, <em>73</em>(12), 2868–2881. (<a
href="https://doi.org/10.1109/TC.2024.3457729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular Edge Computing (VEC) is a transportation-specific version of Mobile Edge Computing (MEC) designed for vehicular scenarios. Task offloading allows vehicles to send computational tasks to nearby Roadside Units (RSUs) in order to reduce the computation cost for the overall system. However, the state-of-the-art solutions have not fully addressed the challenge of large-scale task result feedback with low delay, due to the extremely flexible network structure and complex traffic data. In this paper, we explore the joint task offloading and resource allocation problem with result feedback cost in the VEC. In particular, this study develops a VEC computing offloading scheme, namely, a Lagrange multipliers-based adaptive computing offloading with prediction model, considering multiple RSUs and vehicles within their coverage areas. First, the VEC network architecture employs GAN to establish a prediction model, utilizing the powerful predictive capabilities of GAN to forecast the maximum distance of future trajectories, thereby reducing the decision space for task offloading. Subsequently, we propose a real-time adaptive model and adjust the parameters in different scenarios to accommodate the dynamic characteristic of the VEC network. Finally, we apply Lagrange Multiplier-based Non-Uniform Genetic Algorithm (LM-NUGA) to make task offloading decision. Effectively, this algorithm provides reliable and efficient computing services. The results from simulation indicate that our proposed scheme efficiently reduces the computation cost for the whole VEC system. This paves the way for a new generation of disruptive and reliable offloading schemes.},
  archive      = {J_TC},
  author       = {Liang Zhao and Tianyu Li and Guiying Meng and Ammar Hawbani and Geyong Min and Ahmed Y. Al-Dubai and Albert Y. Zomaya},
  doi          = {10.1109/TC.2024.3457729},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2868-2881},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Novel lagrange multipliers-driven adaptive offloading for vehicular edge computing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mutual-influence-aware heuristic method for quantum
circuit mapping. <em>TC</em>, <em>73</em>(12), 2855–2867. (<a
href="https://doi.org/10.1109/TC.2024.3441825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum circuit mapping (QCM) is a crucial preprocessing step for executing a logical circuit (LC) on noisy intermediate-scale quantum (NISQ) devices. Balancing the introduction of extra gates and the efficiency of preprocessing poses a significant challenge for the mapping process. To address this challenge, we propose the mutual-influence-aware (MIA) heuristic method by integrating an initial mapping search framework, an initial mapping generator, and a heuristic circuit mapper. Initially, the framework utilizes the generator to obtain a favorable starting point for the initial mapping search. With this starting point, the search process can efficiently discover a promising initial mapping within a few bidirectional iterations. The circuit mapper considers mutual influences of SWAP gates and is invoked once per iteration. Ultimately, the best result from all iterations is considered the QCM outcome. The experimental results on extensive benchmark circuits demonstrate that, compared to the iterated local search (ILS) method, which represents the current state-of-the-art, our MIA method introduces a similar number of extra gates while achieving nearly 95 times faster execution.},
  archive      = {J_TC},
  author       = {Kui Ye and Shengxin Dai and Bing Guo and Yan Shen and Chuanjie Liu and Kejun Bi and Fei Chen and Yuchuan Hu and Mingjie Zhao},
  doi          = {10.1109/TC.2024.3441825},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2855-2867},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A mutual-influence-aware heuristic method for quantum circuit mapping},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning acceleration optimization of stress boundary
value problem solvers. <em>TC</em>, <em>73</em>(12), 2844–2854. (<a
href="https://doi.org/10.1109/TC.2024.3441828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The solution to boundary value problems is of great significance in industrial software applications. In this paper, we propose a novel deep learning method for simulating stress field distributions in simply supported beams, aiming to serve as a solver for stress boundary value problems. Our regression network, Stress-EA, utilizes the convolution encoder module and additive attention to accurately estimate the stress in the beam. By comparing the Stress-EA prediction results with the stress values calculated using ABAQUS, we achieve a mean absolute error (MAE) of less than 0.06. This indicates a high level of consistency between the stress values obtained from the two approaches. Moreover, the prediction time of Stress-EA is significantly shorter, taking only 0.0011s, compared to the calculation time of ABAQUS, which is 16.91s. This demonstrates the high accuracy and low computational latency of our model. Furthermore, our model exhibits smaller model parameters, requires less computation, and has a shorter prediction time compared to training results obtained using classic and advanced networks. To accelerate training, we utilize data parallel methods, achieving up to 1.89 speedup on a dual-GPU platform without compromising accuracy. This advancement enhances the computing efficiency for large-scale industrial software applications.},
  archive      = {J_TC},
  author       = {Yongsheng Chen and Zhuowei Wang and Xiaoyu Song and Zhe Yan and Lianglun Cheng},
  doi          = {10.1109/TC.2024.3441828},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2844-2854},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Deep learning acceleration optimization of stress boundary value problem solvers},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAPE: Criticality-aware performance and energy optimization
policy for NCFET-based caches. <em>TC</em>, <em>73</em>(12), 2830–2843.
(<a href="https://doi.org/10.1109/TC.2024.3457734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caches are crucial yet power-hungry components in present-day computing systems. With the Negative Capacitance Fin Field-Effect Transistor (NCFET) gaining significant attention due to its internal voltage amplification, allowing for better operation at lower voltages (stronger ON-current and reduced leakage current), the introduction of NCFET technology in caches can reduce power consumption without loss in performance. Apart from the benefits offered by the technology, we leverage the unique characteristics offered by NCFETs and propose a dynamic voltage scaling based criticality-aware performance and energy optimization policy (CAPE) for on-chip caches. We present the first work towards optimizing energy in NCFET-based caches with minimal impact on performance. Compared to operating at a nominal voltage of 0.7 V, CAPE shows improvement in Last-Level Cache (LLC) energy savings by up to 19.2%, while the baseline policies devised for traditional CMOS- (/FinFET-) based caches are ineffective in improving NCFET-based LLC energy savings. Compared to the considered baseline policies, our CAPE policy also demonstrates better LLC energy-delay product (EDP) and throughput savings.},
  archive      = {J_TC},
  author       = {Divya Praneetha Ravipati and Ramanuj Goel and Victor M. van Santen and Hussam Amrouch and Preeti Ranjan Panda},
  doi          = {10.1109/TC.2024.3457734},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2830-2843},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CAPE: Criticality-aware performance and energy optimization policy for NCFET-based caches},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging GPU in homomorphic encryption: Framework design
and analysis of BFV variants. <em>TC</em>, <em>73</em>(12), 2817–2829.
(<a href="https://doi.org/10.1109/TC.2024.3457733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic Encryption (HE) enhances data security by enabling computations on encrypted data, advancing privacy-focused computations. The BFV scheme, a promising HE scheme, raises considerable performance challenges. Graphics Processing Units (GPUs), with considerable parallel processing abilities, offer an effective solution. In this work, we present an in-depth study on accelerating and comparing BFV variants on GPUs, including Bajard-Eynard-Hasan-Zucca (BEHZ), Halevi-Polyakov-Shoup (HPS), and recent variants. We introduce a universal framework for all variants, propose optimized BEHZ implementation, and first support HPS variants with large parameter sets on GPUs. We also optimize low-level arithmetic and high-level operations, minimizing instructions for modular operations, enhancing hardware utilization for base conversion, and implementing efficient reuse strategies and fusion methods to reduce computational and memory consumption. Leveraging our framework, we offer comprehensive comparative analyses. Performance evaluation shows a 31.9 $\times$ speedup over OpenFHE running on a multi-threaded CPU and 39.7% and 29.9% improvement for tensoring and relinearization over the state-of-the-art GPU BEHZ implementation. The leveled HPS variant records up to 4 $\times$ speedup over other variants, positioning it as a highly promising alternative for specific applications.},
  archive      = {J_TC},
  author       = {Shiyu Shen and Hao Yang and Wangchen Dai and Lu Zhou and Zhe Liu and Yunlei Zhao},
  doi          = {10.1109/TC.2024.3457733},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2817-2829},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Leveraging GPU in homomorphic encryption: Framework design and analysis of BFV variants},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Component dependencies based network-on-chip test.
<em>TC</em>, <em>73</em>(12), 2805–2816. (<a
href="https://doi.org/10.1109/TC.2024.3457732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On-line test of NoC is essential for its reliability. This paper proposed an integral test solution for on-line test of NoC to reduce the test cost and improve the reliability of NOC. The test solution includes a new partitioning method, as well as a test method and a test schedule which are based on the proposed partitioning method. The new partitioning method partitions the NoC into a new type of basis unit under test (UUT) named as interdependent components based unit under test (iDC-UUT), which applies component test methods. The iDC-UUT have very low level of functional interdependency and simple physical connection, which results in small test overhead and high test coverage. The proposed test method consists of DFT architecture, test wrapper and test vectors, which can speed-up the test procedure and further improve the test coverage. The proposed test schedule reduces the blockage probability of data packets during testing by increasing the degree of test disorder, so as to further reduce the test cost. Experimental results show that the proposed test solution reduces power and area by 12.7% and 22.7% over an existing test solution. The average latency is reduced by 22.6% to 38.4% over the existing test solution.},
  archive      = {J_TC},
  author       = {Letian Huang and Tianjin Zhao and Ziren Wang and Junkai Zhan and Junshi Wang and Xiaohang Wang},
  doi          = {10.1109/TC.2024.3457732},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2805-2816},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Component dependencies based network-on-chip test},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Falic: An FPGA-based multi-scalar multiplication accelerator
for zero-knowledge proof. <em>TC</em>, <em>73</em>(12), 2791–2804. (<a
href="https://doi.org/10.1109/TC.2024.3449121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Falic, a novel FPGA-based accelerator to accelerate multi-scalar multiplication (MSM), the most time-consuming phase of zk-SNARK proof generation. Falic innovates three techniques. First, it leverages globally asynchronous locally synchronous (GALS) strategy to build multiple small and lightweight MSM cores to parallelize the independent inner product computation on different portions of the scalar vector and point vector. Second, each MSM core contains just one large-integer modular multiplier (LIMM) that is multiplexed to perform the point additions (PADDs) generated during MSM. We strike a balance between the throughput and hardware cost by batching the appropriate number of PADDs and selecting the computation graph of PADD with proper parallelism degree. Finally, the performance is further improved by a simple cache structure that enables the computation reuse. We implement Falic on two different FPGAs with different hardware resources, i.e., the Xilinx U200 and Xilinx U250. Compared to the prior FPGA-based accelerator, Falic improves the MSM throughput by $3.9\boldsymbol{\times}$ . Experimental results also show that Falic achieves a throughput speedup of up to $1.62\boldsymbol{\times}$ and saves as much as $8.5\boldsymbol{\times}$ energy compared to an RTX 2080Ti GPU.},
  archive      = {J_TC},
  author       = {Yongkui Yang and Zhenyan Lu and Jingwei Zeng and Xingguo Liu and Xuehai Qian and Zhibin Yu},
  doi          = {10.1109/TC.2024.3449121},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2791-2804},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Falic: An FPGA-based multi-scalar multiplication accelerator for zero-knowledge proof},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-key attack on full-round shadow designed for IoT
nodes. <em>TC</em>, <em>73</em>(12), 2776–2790. (<a
href="https://doi.org/10.1109/TC.2024.3449040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of the Internet of Things (IoT), many innovative lightweight block ciphers have been introduced to meet the stringent security demands of IoT devices. Among these, the Shadow cipher stands out for its compactness, making it particularly well-suited for deployment in resource-constrained IoT nodes (IEEE Internet of Things Journal, 2021). This paper demonstrates two real-time attacks on Shadow for the first time: real-time plaintext recovery and key recovery. Firstly, numerous properties of Shadow are discussed, illustrating an equivalent representation of the two-round Shadow and the relationship between the round keys. Secondly, we introduce multiple two-round iterative linear approximations. Employing these approximations enables the derivation of full-round linear distinguishers. Moreover, we have uncovered numerous linear relationships between plaintext and ciphertext. Real-time plaintext recovery is achievable based on these established relationships. On average, it takes 5 seconds to recover the plaintext for a fixed ciphertext of Shadow-32. Thirdly, many properties of the propagation of difference through SIMON-like function are illustrated. According to these properties, various differential distinguishers up to full rounds are presented, allowing real-time key recovery. Specifically, the 64-bit master key of Shadow-32 can be retrieved in around two days on average. Experiments verify all our results.},
  archive      = {J_TC},
  author       = {Yuhan Zhang and Wenling Wu and Lei Zhang and Yafei Zheng},
  doi          = {10.1109/TC.2024.3449040},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2776-2790},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Single-key attack on full-round shadow designed for IoT nodes},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCARF: Securing chips with a robust framework against
fabrication-time hardware trojans. <em>TC</em>, <em>73</em>(12),
2761–2775. (<a href="https://doi.org/10.1109/TC.2024.3449082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The globalization of the semiconductor industry has introduced security challenges to Integrated Circuits (ICs), particularly those related to the threat of Hardware Trojans (HTs) – malicious logic that can be introduced during IC fabrication. While significant efforts are directed towards verifying the correctness and reliability of ICs, their security is often overlooked. In this paper, we propose a comprehensive framework that integrates a suite of methodologies for both front-end and back-end stages of design, aimed at enhancing the security of ICs. Initially, we outline a systematic methodology to transform existing verification assets into potent security checkers by repurposing verification assertions. To further improve security, we introduce an innovative methodology for integrating online monitors during physical synthesis – a back-end insertion providing an additional layer of defense. Experimental results demonstrate a significant increase in security, measured by our introduced metric, Security Coverage (SC), with a marginal rise in area and power consumption, typically under 20%. The insertion of online monitors during physical synthesis enhances security metrics by up to 33.5%. This holistic framework offers a comprehensive defense mechanism across the entire spectrum of IC design.},
  archive      = {J_TC},
  author       = {Mohammad Eslami and Tara Ghasempouri and Samuel Pagliarini},
  doi          = {10.1109/TC.2024.3449082},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2761-2775},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SCARF: Securing chips with a robust framework against fabrication-time hardware trojans},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online container scheduling with fast function startup and
low memory cost in edge computing. <em>TC</em>, <em>73</em>(12),
2747–2760. (<a href="https://doi.org/10.1109/TC.2024.3441836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extending serverless computing to the edge has emerged as a promising approach to support service, but startup containerized serverless functions lead to the cold-start delay. Recent research has introduced container caching methods to alleviate the cold-start delay, including cache as the entire container or the Zygote container. However, container caching incurs memory costs. The system must ensure fast function startup and low memory cost of edge servers, which has been overlooked in the literature. This paper aims to jointly optimize startup delay and memory cost. We formulate an online joint optimization problem that encompasses container scheduling decisions, including invocation distribution, container startup, and container caching. To solve the problem, we propose an online algorithm with a competitive ratio and low computational complexity. The proposed algorithm decomposes the problem into two subproblems and solves them sequentially. Each container is assigned a randomized strategy, and these container-level decisions are merged to constitute overall container caching decisions. Furthermore, a greedy-based subroutine is designed to solve the subproblem associated with invocation distribution and container startup decisions. Experiments on the real-world dataset indicate that the algorithm can reduce average startup delay by up to 23% and lower memory costs by up to 15%.},
  archive      = {J_TC},
  author       = {Zhenzheng Li and Jiong Lou and Jianfei Wu and Jianxiong Guo and Zhiqing Tang and Ping Shen and Weijia Jia and Wei Zhao},
  doi          = {10.1109/TC.2024.3441836},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2747-2760},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Online container scheduling with fast function startup and low memory cost in edge computing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware implementation of unsigned approximate hybrid
square rooters for error-resilient applications. <em>TC</em>,
<em>73</em>(12), 2734–2746. (<a
href="https://doi.org/10.1109/TC.2024.3457731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the authors proposed an approximate hybrid square rooter (AHSQR). It is the combination of array and logarithmic-based square rooter (SQR) to create a balance between accuracy and hardware performance. An array-based SQR is utilized as an exact SQR (ESQR) to obtain the MSBs of output for high precision, while a logarithmic SQR is used to estimate the remaining output digits to enhance design metrics. A modified AHSQR (MAHSQR) is also proposed to retain accuracy at increasing degrees of approximation by computing the square root of LSBs using the ESQR unit. This reduces the mean relative error distance by up to 31% and the normalized mean error distance by up to 26%. Various accuracy metrics and hardware characteristics are evaluated and analyzed for 16-bit unsigned exact, state-of-the-art, and proposed SQRs. The proposed SQRs are designed using Verilog and implemented using Artix7 FPGA. The results show that the proposed SQRs performances are improved compared to the state-of-the-art methods by being approximately 70% smaller, 2.5 times faster, and consuming only 25% of the power of the ESQR. Applications of the proposed SQRs as a Sobel edge detector, and K-means clustering for image processing, and an envelope detector for communication systems are also included.},
  archive      = {J_TC},
  author       = {Lalit Bandil and Bal Chand Nagar},
  doi          = {10.1109/TC.2024.3457731},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2734-2746},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hardware implementation of unsigned approximate hybrid square rooters for error-resilient applications},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ROLoad-PMP: Securing sensitive operations for kernels and
bare-metal firmware. <em>TC</em>, <em>73</em>(12), 2722–2733. (<a
href="https://doi.org/10.1109/TC.2024.3449105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common way for attackers to compromise victim systems is hijacking sensitive operations (e.g., control-flow transfers) with attacker-controlled inputs. Existing solutions in general only protect parts of these targets and have high performance overheads, which are impractical and hard to deploy on systems with limited resources (e.g., IoT devices) or for low-level software like kernels and bare-metal firmware. In this paper, we present a lightweight hardware-software co-design solution ROLoad-PMP to protect sensitive operations from being hijacked for low-level software. First, we propose new instructions, which only load data from read-only memory regions with specific keys, to guarantee the integrity of pointees pointed by (potentially corrupted) data pointers. Then, we provide a program hardening mechanism to protect sensitive operations, by classifying and placing their operands into read-only memory with different keys at compile-time and loading them with ROLoad-PMP-family instructions at runtime. We have implemented an FPGA-based prototype of ROLoad-PMP based on RISC-V, and demonstrated an important defense application, i.e., forward-edge control-flow integrity. Results showed that ROLoad-PMP only costs few extra hardware resources ( $\lt 1.40\%$ ). Moreover, it enables many lightweight (e.g., with negligible overheads $\lt 0.853\%$ ) defenses, and provides broader and stronger security guarantees than existing hardware solutions, e.g., ARM BTI and Intel CET.},
  archive      = {J_TC},
  author       = {Wende Tan and Chenyang Li and Yangyu Chen and Yuan Li and Chao Zhang and Jianping Wu},
  doi          = {10.1109/TC.2024.3449105},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2722-2733},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ROLoad-PMP: Securing sensitive operations for kernels and bare-metal firmware},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BiRD: Bi-directional input reuse dataflow for enhancing
depthwise convolution performance on systolic arrays. <em>TC</em>,
<em>73</em>(12), 2708–2721. (<a
href="https://doi.org/10.1109/TC.2024.3449103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depthwise convolution (DWConv) is an effective technique for reducing the size and computational requirements of convolutional neural networks. However, DWConv&#39;s input reuse pattern is not easily transformed into dense matrix multiplications, leading to low utilization of processing elements (PEs) on existing systolic arrays. In this paper, we introduce a novel systolic array dataflow mechanism called BiRD , designed to maximize input reuse and boost DWConv performance. BiRD utilizes two directions of input reuse and necessitates only minor modifications to a typical weight-stationary type systolic array. We evaluate BiRD on the Gemmini platform, comparing it with existing dataflow types. The results demonstrate that BiRD achieves significant performance improvements in computation time reduction, while incurring minimal area overhead and improved energy consumption compared to other dataflow types. For example, on a 32 $\times{}$ 32 systolic array, it results in a 9.8% area overhead, significantly smaller than other dataflow types for DWConv. Compared to matrix multiplication-based DWConv, BiRD achieves a 4.7 $\times{}$ performance improvement for DWConv layers of MobileNet-V2, resulting in a 55.8% reduction in total inference computation time and a 44.9% reduction in energy consumption. Our results highlight the effectiveness of BiRD in enhancing the performance of DWConv on systolic arrays.},
  archive      = {J_TC},
  author       = {Mingeon Park and Seokjin Hwang and Hyungmin Cho},
  doi          = {10.1109/TC.2024.3449103},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2708-2721},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BiRD: Bi-directional input reuse dataflow for enhancing depthwise convolution performance on systolic arrays},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HGNAS: Hardware-aware graph neural architecture search for
edge devices. <em>TC</em>, <em>73</em>(12), 2693–2707. (<a
href="https://doi.org/10.1109/TC.2024.3449108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are becoming increasingly popular for graph-based learning tasks such as point cloud processing due to their state-of-the-art (SOTA) performance. Nevertheless, the research community has primarily focused on improving model expressiveness, lacking consideration of how to design efficient GNN models for edge scenarios with real-time requirements and limited resources. Examining existing GNN models reveals varied execution across platforms and frequent Out-Of-Memory (OOM) problems, highlighting the need for hardware-aware GNN design. To address this challenge, this work proposes a novel hardware-aware graph neural architecture search framework tailored for resource constraint edge devices, namely HGNAS. To achieve hardware awareness, HGNAS integrates an efficient GNN hardware performance predictor that evaluates the latency and peak memory usage of GNNs in milliseconds. Meanwhile, we study GNN memory usage during inference and offer a peak memory estimation method, enhancing the robustness of architecture evaluations when combined with predictor outcomes. Furthermore, HGNAS constructs a fine-grained design space to enable the exploration of extreme performance architectures by decoupling the GNN paradigm. In addition, the multi-stage hierarchical search strategy is leveraged to facilitate the navigation of huge candidates, which can reduce the single search time to a few GPU hours. To the best of our knowledge, HGNAS is the first automated GNN design framework for edge devices, and also the first work to achieve hardware awareness of GNNs across different platforms. Extensive experiments across various applications and edge devices have proven the superiority of HGNAS. It can achieve up to a $10.6\boldsymbol{\times}$ speedup and an $82.5\%$ peak memory reduction with negligible accuracy loss compared to DGCNN on ModelNet40.},
  archive      = {J_TC},
  author       = {Ao Zhou and Jianlei Yang and Yingjie Qi and Tong Qiao and Yumeng Shi and Cenlin Duan and Weisheng Zhao and Chunming Hu},
  doi          = {10.1109/TC.2024.3449108},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2693-2707},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HGNAS: Hardware-aware graph neural architecture search for edge devices},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TOP: Towards open &amp; predictable heterogeneous SoCs.
<em>TC</em>, <em>73</em>(12), 2678–2692. (<a
href="https://doi.org/10.1109/TC.2024.3441849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring predictability in modern real-time Systems-on-Chip (SoCs) is an increasingly critical concern for many application domains such as automotive, robotics, and industrial automation. An effective approach involves the modeling and development of hardware components, such as interconnects and shared memory resources, to evaluate or enforce their deterministic behavior. Unfortunately, these IPs are often closed-source, and these studies are limited to the single modules that must later be integrated with third-party IPs in more complex SoCs, hindering the precision and scope of modeling and compromising the overall predictability. With the coming-of-age of open-source instruction set architectures (RISC-V) and hardware, major opportunities for changing this status quo are emerging. This study introduces an innovative methodology for modeling and analyzing State-of-the-Art (SoA) open-source SoCs for low-power cyber-physical systems. Our approach models and analyzes the entire set of open-source IPs within these SoCs and then provides a comprehensive analysis of the entire architecture. We validate this methodology on a sample heterogenous low-power RISC-V architecture through RTL simulation and FPGA implementation, minimizing pessimism in bounding the service time of transactions crossing the architecture between 28% and 1%, which is considerably lower when compared to similar SoA works.},
  archive      = {J_TC},
  author       = {Luca Valente and Francesco Restuccia and Davide Rossi and Ryan Kastner and Luca Benini},
  doi          = {10.1109/TC.2024.3441849},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2678-2692},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TOP: Towards open &amp; predictable heterogeneous SoCs},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMBEA: Aggressive maximal biclique enumeration in large
bipartite graph computing. <em>TC</em>, <em>73</em>(12), 2664–2677. (<a
href="https://doi.org/10.1109/TC.2024.3441864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximal biclique enumeration (MBE) in bipartite graphs is a fundamental problem in data mining with widespread applications. Many recent works solve this problem based on the set-enumeration (SE) tree, which sequentially traverses vertices to generate the enumeration tree nodes representing distinct bicliques, then checks whether these bicliques are maximal or not. However, existing MBE algorithms only expand bicliques with untraversed vertices to ensure distinction, which often necessitate extensive node checks to eliminate non-maximal bicliques, resulting in significant computational overhead during the enumeration process. To address this issue, we propose an aggressive set-enumeration (ASE) tree that aggressively expands all bicliques to their maximal form, thus avoiding costly node checks on non-maximal bicliques. This aggressive enumeration may produce multiple duplicate maximal bicliques, but we efficiently eliminate these duplicates by leveraging the connection between parent and child nodes and conducting low-cost node checking. Additionally, we introduce an aggressive merge-based pruning (AMP) approach that aggressively merges vertices sharing the same local neighbors. This helps prune numerous duplicate node generations caused by subsets of merged vertices. We integrate the AMP approach into the ASE tree, and present the Aggressive Maximal Biclique Enumeration Algorithm (AMBEA). Experimental results show that AMBEA is 1.15 $\times$ to 5.32 $\times$ faster than its closest competitor and exhibits better scalability and parallelization capabilities on larger bipartite graphs.},
  archive      = {J_TC},
  author       = {Zhe Pan and Xu Li and Shuibing He and Xuechen Zhang and Rui Wang and Yunjun Gao and Gang Chen and Xian-He Sun},
  doi          = {10.1109/TC.2024.3441864},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2664-2677},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AMBEA: Aggressive maximal biclique enumeration in large bipartite graph computing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling efficient deep learning on MCU with transient
redundancy elimination. <em>TC</em>, <em>73</em>(12), 2649–2663. (<a
href="https://doi.org/10.1109/TC.2024.3449102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying deep neural networks (DNNs) with satisfactory performance in resource-constrained environments is challenging. This is especially true of microcontrollers due to their tight space and computational capabilities. However, there is a growing demand for DNNs on microcontrollers, as executing large DNNs on microcontrollers is critical to reducing energy consumption, increasing performance efficiency, and eliminating privacy concerns. This paper presents a novel and systematic data redundancy elimination method to implement efficient DNNs on microcontrollers through innovations in computation and space optimization. By making the optimization itself a trainable component in the target neural networks, this method maximizes performance benefits while keeping the DNN accuracy stable. Experiments are performed on two microcontroller boards with three popular DNNs, namely CifarNet, ZfNet and SqueezeNet. Experiments show that this solution eliminates more than 96% of computations in DNNs and makes them fit well on microcontrollers, yielding 3.4-5 $\times$ speedup with little loss of accuracy.},
  archive      = {J_TC},
  author       = {Jiesong Liu and Feng Zhang and Jiawei Guan and Hsin-Hsuan Sung and Xiaoguang Guo and Saiqin Long and Xiaoyong Du and Xipeng Shen},
  doi          = {10.1109/TC.2024.3449102},
  journal      = {IEEE Transactions on Computers},
  month        = {12},
  number       = {12},
  pages        = {2649-2663},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling efficient deep learning on MCU with transient redundancy elimination},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-performance tensor-train primitives using GPU tensor
cores. <em>TC</em>, <em>73</em>(11), 2634–2648. (<a
href="https://doi.org/10.1109/TC.2024.3441831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning tensor-train (TT) structure (a.k.a matrix product state (MPS) representation) from large-scale high-dimensional data has been a common task in big data analysis, deep learning, and quantum machine learning. However, tensor-train algorithms are compute-intensive, which hinders their real-world applications. In this paper, we present high-performance tensor-train primitives using GPU tensor cores and demonstrate three applications. First, we use GPU tensor cores to optimize tensor-train primitives, including tensor contraction, singular value decomposition, and data transfer and computing. Second, we utilize the optimized primitives to accelerate tensor-train decomposition algorithms for big data analysis. Further, we propose a shard mode for high-order tensor computations on multiple GPUs. Third, we apply the optimized primitives to accelerate the tensor-train layer for compressing deep neural networks. Last, we utilize the optimized primitives to accelerate a quantum machine learning algorithm called Density Matrix Renormalization Group (DMRG) . In performance evaluations, our third-order TT tensor decomposition achieves up to $3.34\times$ and $6.91\times$ speedups over two popular libraries (namely T3F and tntorch) on an A100 GPU, respectively. The proposed sixth-order tensor-train decomposition achieves up to a speedup of $5.01\times$ over T3F on multiple A100 GPUs. Our tensor-train layer for a fully connected neural network achieves a compression ratio of $65.3\times$ at the cost of $0.3\%$ drop in accuracy and a speedup of $1.53\times$ over a PyTorch implementation on CUDA cores. The optimized DMRG algorithm achieves up to a speedup of $14.0\times$ over TensorNetwork, indicating the potential of the optimized tensor primitives for the classical simulation of quantum machine learning algorithms.},
  archive      = {J_TC},
  author       = {Xiao-Yang Liu and Hao Hong and Zeliang Zhang and Weiqin Tong and Jean Kossaifi and Xiaodong Wang and Anwar Walid},
  doi          = {10.1109/TC.2024.3441831},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2634-2648},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-performance tensor-train primitives using GPU tensor cores},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint pruning and channel-wise mixed-precision quantization
for efficient deep neural networks. <em>TC</em>, <em>73</em>(11),
2619–2633. (<a href="https://doi.org/10.1109/TC.2024.3449084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The resource requirements of deep neural networks (DNNs) pose significant challenges to their deployment on edge devices. Common approaches to address this issue are pruning and mixed-precision quantization, which lead to latency and memory occupation improvements. These optimization techniques are usually applied independently. We propose a novel methodology to apply them jointly via a lightweight gradient-based search, and in a hardware-aware manner, greatly reducing the time required to generate Pareto-optimal DNNs in terms of accuracy versus cost (i.e., latency or memory). We test our approach on three edge-relevant benchmarks, namely CIFAR-10, Google Speech Commands, and Tiny ImageNet. When targeting the optimization of the memory footprint, we are able to achieve a size reduction of 47.50% and 69.54% at iso-accuracy with the baseline networks with all weights quantized at 8 and 2-bit, respectively. Our method surpasses a previous state-of-the-art approach with up to 56.17% size reduction at iso-accuracy. With respect to the sequential application of state-of-the-art pruning and mixed-precision optimizations, we obtain comparable or superior results, but with a significantly lowered training time. In addition, we show how well-tailored cost models can improve the cost versus accuracy trade-offs when targeting specific hardware for deployment.},
  archive      = {J_TC},
  author       = {Beatrice Alessandra Motetti and Matteo Risso and Alessio Burrello and Enrico Macii and Massimo Poncino and Daniele Jahier Pagliari},
  doi          = {10.1109/TC.2024.3449084},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2619-2633},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Joint pruning and channel-wise mixed-precision quantization for efficient deep neural networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Memristor-based approximate query architecture for
in-memory hyperdimensional computing. <em>TC</em>, <em>73</em>(11),
2605–2618. (<a href="https://doi.org/10.1109/TC.2024.3441861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new computing paradigm, hyperdimensional computing (HDC) has gradually manifested its advantages in edge-side intelligent applications by virtue of its interpretability, hardware-friendliness and robustness. The core of HDC is to encode input samples into a hypervector, and then use it to query the class hypervector space. Compared with the conventional architecture that uses CMOS-based circuits to complete the computation in the query operation, the hyperdimensional associative memory (HAM) enables the query operation to be completed in memory, which significantly reduces the query delay and energy consumption. However, the existing HDC algorithms require the HAM to achieve high precision query in inference, which leads to the complex structure of the HAM, and thus makes the area and energy consumption of the HAM unable to be further reduced. In this paper, a novel efficient HAM architecture based on approximate query method is proposed, to simplify the existing architecture. Meanwhile, a training method of HDC which matches the proposed HAM architecture is proposed to compensate for the decrease in accuracy caused by approximate query. Experimental results show that the proposed HAM framework can save more than 60% of area and energy consumption, and achieve accuracy comparable to existing state-of-the-art methods by using the proposed training method.},
  archive      = {J_TC},
  author       = {Tianyang Yu and Bi Wu and Ke Chen and Gong Zhang and Weiqiang Liu},
  doi          = {10.1109/TC.2024.3441861},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2605-2618},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Memristor-based approximate query architecture for in-memory hyperdimensional computing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards cost-effective and robust packaging in multi-leader
BFT blockchain systems. <em>TC</em>, <em>73</em>(11), 2590–2604. (<a
href="https://doi.org/10.1109/TC.2024.3398510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Byzantine fault-tolerant (BFT) systems, maintaining consistency amidst malicious replicas is crucial, especially for blockchain systems. Recent innovations in this field have integrated multiple leaders into the BFT consensus mechanism to boost scalability and efficiency. However, the existing approaches often lead to excessive consumption of storage, bandwidth, and CPU resources due to redundant transactions. And the attempting to mitigate resource wastage inadvertently reduces resilience against Byzantine failures. To this end, we propose PeterHofe, an innovative ring-based approach for collaborative transaction processing. PeterHofe focuses on balancing resource utilization and minimizing the influence of Byzantine leaders, thereby enhancing transaction processing speed and overall system reliability. PeterHofe innovates by partitioning the transaction hash space into various buckets and creating a complex mappings between these buckets and the replicas, effectively reducing the control of Byzantine replicas. In developing PeterHofe, we concentrate on three primary objectives: 1) the creation of a permutation-based ring structure that enhances resistance to Byzantine censorship, backed by thorough mathematical proofs and analyses; 2) the development of a Prophecy-Implementation mechanism aimed at minimizing transaction replication while scrutinizing potential malicious activities; 3) to ensure the applicability of our proposed method across various types of multi-leader BFT consensus protocols, we have developed an additional asynchronous protocol to ensure consistent application of the packaging strategy. We have implemented PeterHofe using the latest significant frameworks, Narwhal and Tusk, and our empirical results affirm its capability to simultaneously minimize resource waste and bolster system robustness. Specifically, PeterHofe demonstrates efficiency in resource utilization, achieving a 20-fold reduction of resource waste when compared to the Random-based Strategy. When against the advanced Hash-based Partitioning Strategy, it reduces malicious transaction control by at least 66 $\%$ , leading to up to 75 $\%$ lower latency. In scenarios of high traffic, our approach significantly outperforms existing strategies in throughput. Against the Random-based Strategy, it achieves a 6.11 $\%$ increase, and when compared to the Hash-based Partitioning Strategy, the improvement is 20 $\%$ .},
  archive      = {J_TC},
  author       = {Xiulong Liu and Zhiyuan Zheng and Wenbin Wang and Hao Xu and Fengjun Xiao and Keqiu Li},
  doi          = {10.1109/TC.2024.3398510},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2590-2604},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards cost-effective and robust packaging in multi-leader BFT blockchain systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PruneAug: Bridging DNN pruning and inference latency on
diverse sparse platforms using automatic layerwise block pruning.
<em>TC</em>, <em>73</em>(11), 2576–2589. (<a
href="https://doi.org/10.1109/TC.2024.3441855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although pruning is an effective technique to reduce the number of weights in deep neural networks (DNNs), it remains challenging for the resulting sparse networks to perform low-latency inference on everyday hardware. This problem is mainly caused by the incompatibility between the unstructured sparsity adopted for accuracy preservation and the sparse platform&#39;s (the combination of sparse kernel library and the underlying hardware) expectation of regular sparse patterns. In order to resolve this conflict, we propose PruneAug, an augmentation over existing unstructured pruning methods that finds block-sparse networks with much lower latency but preserves the accuracy. The fundamental idea of PruneAug is to prune the network with a layerwise block dimension assignment in a platform-aware fashion. Subject to an accuracy-loss constraint, PruneAug minimizes the latency of the block sparse network by jointly optimizing this layerwise block dimension assignment and the network&#39;s sparsity level. Admittedly, this approach expands the solution space. To curb our search cost, we include multiple optimizations while designing PruneAug&#39;s search space and strategy. Our evaluation over diverse pruning methods, DNNs, datasets, and sparse platforms shows that PruneAug enables different pruning methods to achieve speedup (as much as $\boldsymbol{\sim}13\boldsymbol{\times}$ depending on the platform) while maintaining competitive accuracy relative to unstructured sparsity, extracting the full potential of sparse platforms.},
  archive      = {J_TC},
  author       = {Hanfei Geng and Yifei Liu and Yujie Zheng and Li Lyna Zhang and Jingwei Sun and Yujing Wang and Yang Wang and Guangzhong Sun and Mao Yang and Ting Cao and Yunxin Liu},
  doi          = {10.1109/TC.2024.3441855},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2576-2589},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PruneAug: Bridging DNN pruning and inference latency on diverse sparse platforms using automatic layerwise block pruning},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized quantum circuit of AES with interlacing-uncompute
structure. <em>TC</em>, <em>73</em>(11), 2563–2575. (<a
href="https://doi.org/10.1109/TC.2024.3449094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the post-quantum era, the security level of encryption algorithms is often evaluated based on the quantum resources required to attack AES. In this work, we make thoroughly estimations on various performance metrics of the quantum circuit of AES-128/192/256. Firstly, we introduce a generic round structure for in-place implementation of the AES algorithm, maximizing the parallelism between nonlinear components. Specifically, when employed as an encryption oracle, our structure reduces the $T$ -depth from $2rd$ to $(r+1)d$ . Furthermore, by leveraging the properties of block-cyclic matrices, we present an in-place implementation circuit for MixColumn with depth 10, utilizing 105 CNOT gates. In relation to the S-box, we have assessed its minimum circuit width at different $T$ -depths and provide multiple versions of circuit implementations for a depth-width trade-off. Finally, based on our optimized S-box circuit, we conduct a comprehensive analysis of the implementation complexity of different round structures, where our structure exhibits significant advantages in terms of low $T$ -depth.},
  archive      = {J_TC},
  author       = {Mengyuan Zhang and Tairong Shi and Wenling Wu and Han Sui},
  doi          = {10.1109/TC.2024.3449094},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2563-2575},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimized quantum circuit of AES with interlacing-uncompute structure},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-augmented scheduling. <em>TC</em>, <em>73</em>(11),
2548–2562. (<a href="https://doi.org/10.1109/TC.2024.3441856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent revival in learning theory has provided us with improved capabilities for accurate predictions. This work contributes to an emerging research agenda of online scheduling with predictions by studying makespan minimization in uniformly related machine non-clairvoyant scheduling with job size predictions. Our task is to design online algorithms that use predictions and have performance guarantees tied to prediction quality. We first propose a simple algorithm-independent prediction error metric to quantify prediction quality. Then we design an offline improved 2-relaxed decision procedure approximating the optimal schedule to effectively use the predictions. With the decision procedure, we propose an online $O(\min\{\log\eta,\log m\})$ -competitive static scheduling algorithm assuming a known prediction error. We use this algorithm to construct a robust $O(\min\{\log\eta,\log m\})$ -competitive static scheduling algorithm that does not assume a known error. Finally, we extend these static scheduling algorithms to address dynamic scheduling where jobs arrive over time. The dynamic scheduling algorithms attain the same competitive ratios as the static ones. The presented algorithms require just moderate predictions to break the $\Omega(\log m)$ competitive ratio lower bound, showing the potential of predictions in managing uncertainty.},
  archive      = {J_TC},
  author       = {Tianming Zhao and Wei Li and Albert Y. Zomaya},
  doi          = {10.1109/TC.2024.3441856},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2548-2562},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Learning-augmented scheduling},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Response-time analysis of bundled gang tasks under
partitioned FP scheduling. <em>TC</em>, <em>73</em>(11), 2534–2547. (<a
href="https://doi.org/10.1109/TC.2024.3441823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of parallel task models for real-time systems has become fundamental due to the increasing computational demand of modern applications. Recently, gang scheduling has gained attention for improving performance in tightly synchronized parallel applications. Nevertheless, existing studies often overestimate computational demand by assuming a constant number of cores for each task. In contrast, the bundled model accurately represents internal parallelism by means of a string of segments demanding for a variable number of cores. This model is particularly relevant to modern real-time systems, as it allows transforming general parallel tasks into bundled tasks while preserving accurate parallelism. However, it has only been analyzed for global scheduling, which carries analytical pessimism and considerable run-time overheads. This paper introduces two response-time analysis techniques for parallel real-time tasks under partitioned, fixed-priority gang scheduling under the bundled model, together with a set of specialized allocation heuristics. Experimental results compare the proposed methods against state-of-the-art approaches.},
  archive      = {J_TC},
  author       = {Veronica Rispo and Federico Aromolo and Daniel Casini and Alessandro Biondi},
  doi          = {10.1109/TC.2024.3441823},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2534-2547},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Response-time analysis of bundled gang tasks under partitioned FP scheduling},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Land of oz: Resolving orderless writes in zoned namespace
SSDs. <em>TC</em>, <em>73</em>(11), 2520–2533. (<a
href="https://doi.org/10.1109/TC.2024.3441866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zoned Namespace (ZNS) SSDs present a new class of storage devices that promise low cost, stable performance, and software-defined capability. ZNS abstracts the SSD into an array of zones that can only be written sequentially. Although ZNS-compatible filesystems (e.g., F2FS) launch sequential writes, the zone write constraint may be violated due to orderless Linux I/O stack. Hence, the write queue depth of each zone must be limited to one, which severely degrades the performance of small writes. In this paper, we propose oZNS SSD (o: orderless), which allows multiple writes to be submitted to a zone and processed out-of-order in the SSD. To bridge the gap between ZNS sequential write contract and orderless writes on flash, a lightweight indirection layer is introduced and carefully designed by exploiting the characteristics of out-of-order writes. Specifically, memory-efficient metadata structures are devised to record the write deviations of data pages, and a two-tier buffering mechanism is employed to reduce metadata and accelerate metadata access. Moreover, a read-try policy removes metadata access from read critical path and thus improves the data read efficiency. Experimental results show that oZNS can achieve up to 8.6$\times$&lt;mml:math display=&quot;inline&quot;&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;/mml:math&gt; higher write performance than traditional ZNS while providing comparable read performance.},
  archive      = {J_TC},
  author       = {Yingjia Wang and You Zhou and Fei Wu and Jie Zhang and Ming-Chang Yang},
  doi          = {10.1109/TC.2024.3441866},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2520-2533},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Land of oz: Resolving orderless writes in zoned namespace SSDs},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge-MPQ: Layer-wise mixed-precision quantization with
tightly integrated versatile inference units for edge computing.
<em>TC</em>, <em>73</em>(11), 2504–2519. (<a
href="https://doi.org/10.1109/TC.2024.3441860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the prevailing deep neural networks compression techniques, layer-wise mixed-precision quantization (MPQ) strikes a better balance between accuracy and efficiency than uniform quantization schemes. However, existing MPQ strategies either lack hardware awareness or incur huge computation costs, limiting their deployment at the edge. Additionally, researchers usually make a one-time decision between post-training quantization (PTQ) and quantization-aware training (QAT) based on the quantized bit-width or hardware requirements. In this paper, we propose the tight integration of versatile MPQ inference units supporting INT2-INT8 and INT16 precisions, which feature a hierarchical multiplier architecture, into a RISC-V processor pipeline through micro-architecture and Instruction Set Architecture (ISA) co-design. Synthesized with a 14nm technology, the design delivers a speedup of $15.50\times$ to $47.67\times$ over the baseline RV64IMA core when running a single convolution layer kernel and achieves up to 2.86 GOPS performance. This work also achieves an energy efficiency at 20.51 TOPS/W, which not only exceeds contemporary state-of-the-art MPQ hardware solutions at the edge, but also marks a significant advancement in the field. We also propose a novel MPQ search algorithm that incorporates both hardware awareness and training necessity. The algorithm samples layer-wise sensitivities using a set of newly proposed metrics and runs a heuristics search. Evaluation results show that this search algorithm achieves $2.2\%\sim 6.7\%$ higher inference accuracy under similar hardware constraints compared to state-of-the-art MPQ strategies. Furthermore we expand the search space using a dynamic programming (DP) strategy to perform search with more fine-grained accuracy intervals and support multi-dimensional search. This further improves the inference accuracy by over $1.3\%$ compared to a greedy-based search.},
  archive      = {J_TC},
  author       = {Xiaotian Zhao and Ruge Xu and Yimin Gao and Vaibhav Verma and Mircea R. Stan and Xinfei Guo},
  doi          = {10.1109/TC.2024.3441860},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2504-2519},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Edge-MPQ: Layer-wise mixed-precision quantization with tightly integrated versatile inference units for edge computing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A parallel tag cache for hardware managed tagged memory in
multicore processors. <em>TC</em>, <em>73</em>(11), 2488–2503. (<a
href="https://doi.org/10.1109/TC.2024.3441835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware-managed tagged memory is the dominant way of supporting tags in current processor designs. Most of these processors reserve a hidden tag partition in the memory dedicated for tags and use a small tag cache (TC) to reduce the extra memory accesses introduced by the tag partition. Recent research shows that storing tags in a hierarchical tag table (HTT) inside the tag partition allows efficient compression in a TC, but the use of the HTT causes special data inconsistency issues when multiple related tag accesses are served simultaneously. How to design a parallel TC for multicore processors remains an open problem. We proposed the first TC capable of serving multiple tag accesses in parallel. It adopts a two-phase locking procedure to maintain data consistency and integrates seven techniques, where three are firstly proposed, and two are theoretical concepts materialized into usable solutions for the first time. Single-core and multicore performance results show that the proposed TC is effective in reducing both the extra amount of memory accesses to the tag partition and the overhead in execution time. It is important to provide enough number of trackers in multicore processors while providing extra trackers is beneficial for running HTT/TC ineffective applications.},
  archive      = {J_TC},
  author       = {Wei Song and Da Xie and Zihan Xue and Peng Liu},
  doi          = {10.1109/TC.2024.3441835},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2488-2503},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A parallel tag cache for hardware managed tagged memory in multicore processors},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting structured feature and runtime isolation for
high-performant recommendation serving. <em>TC</em>, <em>73</em>(11),
2474–2487. (<a href="https://doi.org/10.1109/TC.2024.3449749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation serving with deep learning models is one of the most valuable services of modern E-commerce companies. In production, to accommodate billions of recommendation queries with stringent service level agreements, high-performant recommendation serving systems play an essential role in meeting such daunting demand. Unfortunately, existing model serving frameworks fail to achieve efficient serving due to unique challenges such as 1) the input format mismatch between service needs and the model&#39;s ability and 2) heavy software contentions to concurrently execute the constrained operations. To address the above challenges, we propose RecServe , a high-performant serving system for recommendation with the optimized design of structured features and SessionGroups for recommendation serving. With structured features , RecServe packs single-user-multiple-candidates inputs by semi-automatically transforming computation graphs with annotated input tensors, which can significantly reduce redundant network transmission, data movements, and useless computations. With session group , RecServe further adopts resource isolations for multiple compute streams and cost-aware operator scheduler with critical-path-based schedule policy to enable concurrent kernel execution, further improving serving throughput. The experiment results demonstrate that RecServe can achieve maximum performance speedups of 12.3 $\boldsymbol{\times}$ and $22.0\boldsymbol{\times}$ compared to the state-of-the-art serving system on CPU and GPU platforms, respectively.},
  archive      = {J_TC},
  author       = {Xin You and Hailong Yang and Siqi Wang and Tao Peng and Chen Ding and Xinyuan Li and Bangduo Chen and Zhongzhi Luan and Tongxuan Liu and Yong Li and Depei Qian},
  doi          = {10.1109/TC.2024.3449749},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {2474-2487},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploiting structured feature and runtime isolation for high-performant recommendation serving},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DIPER: Detection and identification of pathogens using edit
distance-tolerant resistive CAM. <em>TC</em>, <em>73</em>(10),
2463–2473. (<a href="https://doi.org/10.1109/TC.2023.3315829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel resistive edit distance-tolerant content addressable memory for computational genomics applications, particularly for detection and identification of pathogens of pandemic importance. Unlike state-of-the-art approximate search solutions that tolerate small number of replacements between the query pattern and the stored data, DIPER tolerates insertions and deletions, ubiquitous in genomics. DIPER achieves up to 1.7 $\boldsymbol{\times}$ higher $\bf{\textit{F}_{1}}$ score for high-quality DNA reads and up to 6.2 $\boldsymbol{\times}$ higher $\bf{\textit{F}_{1}}$ score for DNA reads with 15% error rate, compared to state-of-the-art DNA classification tool Kraken2. Simulated at 500 MHz, DIPER provides 910 $\boldsymbol{\times}$ average speedup over Kraken2.},
  archive      = {J_TC},
  author       = {Itay Merlin and Esteban Garzón and Alexander Fish and Leonid Yavits},
  doi          = {10.1109/TC.2023.3315829},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {2463-2473},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DIPER: Detection and identification of pathogens using edit distance-tolerant resistive CAM},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating finite-field and torus fully homomorphic
encryption via compute-enabled (s)RAM. <em>TC</em>, <em>73</em>(10),
2449–2462. (<a href="https://doi.org/10.1109/TC.2023.3301116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully Homomorphic Encryption (FHE) allows outsourced computation on clients’ encrypted data while preserving data privacy. FHE&#39;s high computational intensity incurs high overhead from data transfer with hardware such as CPU, GPU, and FPGA, due to the inherent separation between computing and data. To overcome this limitation, Compute-Enabled RAM (CE-RAM) has been explored; however, prior work using CE-RAM to accelerate FHE only explores a simple implementation of a finite-field FHE scheme and did not explore algorithmic optimizations. In this paper, we investigate CE-RAM acceleration FHE more deeply, implementing both the finite-field B/FV and torus-based TFHE cryptosystems in CE-RAM with common FHE optimizations. This is the first work to explore using CE-RAM to accelerate TFHE. For B/FV, we explore parameter-specific algorithmic optimizations specifically designed for CE-RAM friendliness. We evaluate our implementation as compared to prior work in CE-RAM FHE acceleration and other hardware acceleration strategies. We demonstrate speedups of up to 784x for B/FV homomorphic multiplication and 38x for TFHE bootstrapping as compared to CPU implementations. We also discuss the overhead of CE-RAM for FHE on energy and area consumption, showing comparable or improved performance as compared to other work or hypothetical near-memory accelerators.},
  archive      = {J_TC},
  author       = {Jonathan Takeshita and Dayane Reis and Ting Gong and Michael T. Niemier and Xiaobo Sharon Hu and Taeho Jung},
  doi          = {10.1109/TC.2023.3301116},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {2449-2462},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating finite-field and torus fully homomorphic encryption via compute-enabled (S)RAM},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LPAH: Illustrating efficient live patching with alignment
holes in kernel data. <em>TC</em>, <em>73</em>(10), 2434–2448. (<a
href="https://doi.org/10.1109/TC.2024.3424263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Linux kernel is regularly updated to enhance security, improve performance, and introduce new functionalities. Traditional updating methods typically require rebooting, leading to service disruptions and potential data loss. Live-patching technology dynamically updates the kernel modules without rebooting, ensuring continuous service availability. However, this technique has its drawbacks. Since live-patching alters the original structure of data types, it can no longer utilize base offsets to access the members, imposing considerable overheads. This paper proposes LPAH (Live Patching with Alignment Holes), a live patching system that leverages the fragmented space generated by compile-time alignment for data types, to enable effective live patching updates for security vulnerability fixes, feature enhancements, and user-defined patching tasks. LPAH capitalizes on the relationship between these alignment holes and data objects. This approach ensures efficient access to extended data members while preserving the original data&#39;s integrity. This approach allows other functions to remain unaffected by updates and replacements through explicit type casts. Extensive experimental results show that LPAH offers valid and robust live patching for multiple real vulnerabilities in the Linux kernel, without degrading performance. Our method provides an efficient way to install security patches in the Linux kernel, and thus reenforces kernel security.},
  archive      = {J_TC},
  author       = {Chao Su and Xiaoshuang Xing and Xiaolu Cheng and Rui Guo and Chuanwen Luo},
  doi          = {10.1109/TC.2024.3424263},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {2434-2448},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LPAH: Illustrating efficient live patching with alignment holes in kernel data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SongC: A compiler for hybrid near-memory and in-memory
many-core architecture. <em>TC</em>, <em>73</em>(10), 2420–2433. (<a
href="https://doi.org/10.1109/TC.2023.3311948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building hybrid systems that incorporate various processing-in-memory (PIM) devices and processing-near-memory (PNM) technologies can offer complementary advantages in both efficiency and flexibility, while many-core architectures show great potential in deploying data-centric parallel applications with high performance. Compilers for the hybrid PN/IM architecture are critical for enabling such computing systems to be put into practical use. However, most of the existing neural network compilers for PIM or PNM are optimized from the perspective of an operator, and cannot effectively take advantage of a decentralized core-level dataflow with large on-chip memory access bandwidth. Here, we propose a full-stack System-on-graph Compiler (SongC) framework for many-core architecture, which optimizes the efficiency of the PIM devices and leverages the flexibility of the PNM architectures. SongC establishes multi-level graph abstractions to clarify the critical deployment challenges at different levels and generalizes the standard optimizations, decoupling versatile algorithms and diverse types of hardware. To handle the complexity of many-core resource utilization, we also establish a simulation-compilation interaction flow, including a just-in-time evaluator to boost the scheduling search and an extended Roofline model, referred to as the Palace model, to guide the search. Experiments demonstrate the various optimizations and overall performance of SongC and reveal the capability of strategy exploration.},
  archive      = {J_TC},
  author       = {Junfeng Lin and Huanyu Qu and Songchen Ma and Xinglong Ji and Hongyi Li and Xiaochuan Li and Chenhang Song and Weihao Zhang},
  doi          = {10.1109/TC.2023.3311948},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {2420-2433},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SongC: A compiler for hybrid near-memory and in-memory many-core architecture},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hiding in plain sight: Adversarial attack via style transfer
on image borders. <em>TC</em>, <em>73</em>(10), 2405–2419. (<a
href="https://doi.org/10.1109/TC.2024.3416761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Convolution Neural Networks (CNNs) have become the cornerstone of image classification, but the emergence of adversarial image attacks brings serious security risks to CNN-based applications. As a local perturbation attack, the border attack can achieve high success rates by only modifying the pixels around the border of an image, which is a novel attack perspective. However, existing border attacks have shortcomings in stealthiness and are easily detected. In this article, we propose a novel stealthy border attack method based on deep feature alignment. Specifically, we propose a deep feature alignment algorithm based on style transfer to guarantee the stealthiness of adversarial borders. The algorithm takes the deep feature difference between the adversarial and the original borders as the stealthiness loss and thus ensures good stealthiness of the generated adversarial images. To ensure high attack success rates simultaneously, we apply cross entropy to design the targeted attack loss and use margin loss as well as Leaky ReLU to design the untargeted attack loss. Experiments show that the structural similarity between the generated adversarial images and the original images is 8.8% higher than the state-of-art border attack method, indicating that our proposed adversarial images have better stealthiness. At the same time, the success rate of our attack in the face of defense methods is much higher, which is about four times that of the state-of-art border attack under the adversarial training defense.},
  archive      = {J_TC},
  author       = {Haiyan Zhang and Xinghua Li and Jiawei Tang and Chunlei Peng and Yunwei Wang and Ning Zhang and Yingbin Miao and Ximeng Liu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TC.2024.3416761},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {2405-2419},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hiding in plain sight: Adversarial attack via style transfer on image borders},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GraNDe: Efficient near-data processing architecture for
graph neural networks. <em>TC</em>, <em>73</em>(10), 2391–2404. (<a
href="https://doi.org/10.1109/TC.2023.3283677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Network (GNN) models have attracted attention, given their high accuracy in interpreting graph data. One of the primary building blocks of a GNN model is aggregation, which gathers and averages the feature vectors corresponding to the nodes adjacent to each node. Aggregation works by multiplying the adjacency and feature matrices. The size of both matrices exceeds the on-chip cache capacity for many realistic datasets, and the adjacency matrix is highly sparse. These characteristics lead to little data reuse, causing intensive main-memory accesses during the aggregation process. Thus, aggregation exhibits memory-intensive characteristics and dominates most of the total execution time. In this paper, we propose GraNDe, an NDP architecture that accelerates memory-intensive aggregation operations by locating NDP modules near DRAM datapath to exploit rank-level parallelism. GraNDe maximizes bandwidth utilization by separating the memory channel path with the buffer chip in between so that pre-/post-processing in the host processor and reduction in NDP modules operate simultaneously. By exploring the preferred data mappings of the operand matrices to DRAM ranks, we architect GraNDe to support adaptive matrix mapping that applies the optimal mapping for each layer depending on the dimension of the layer and the configuration of a memory system. We also propose adj-bundle broadcasting and re-tiling optimizations to reduce the transfer time for adjacency matrix data and to improve feature vector data reusability by exploiting tiling with consideration of adjacency between nodes. GraNDe achieves 3.01× and 1.69× on average, and up to 4.00× and 1.98× speedups of GCN aggregation over the baseline system and the state-of-the-art NDP architecture for GCN, respectively.},
  archive      = {J_TC},
  author       = {Sungmin Yun and Hwayong Nam and Jaehyun Park and Byeongho Kim and Jung Ho Ahn and Eojin Lee},
  doi          = {10.1109/TC.2023.3283677},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {2391-2404},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GraNDe: Efficient near-data processing architecture for graph neural networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VSPIM: SRAM processing-in-memory DNN acceleration via
vector-scalar operations. <em>TC</em>, <em>73</em>(10), 2378–2390. (<a
href="https://doi.org/10.1109/TC.2023.3285095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing-in-Memory (PIM) has been widely explored for accelerating data-intensive machine learning computation that mainly consists of general-matrix-multiplication (GEMM), by mitigating the burden of data movements and exploiting the ultra-high memory parallelism. The two mainstreams of PIM, the analog- and digital-type, have both been exploited in accelerating machine learning workloads by numerous outstanding prior works. Currently, the digital-PIM is increasingly favored due to the broader computing support and the avoidance of errors caused by intrinsic non-idealities, e.g., process variation. Nevertheless, it still lacks further optimization considering the characteristics of the GEMM computation, including better efficient data layout and scheduling, and the ability to handle the sparsity of activations at the bit-level. To boost the performance and efficiency of digital SRAM PIM, we propose the architecture called VSPIM that performs the computation in a bit-serial fashion, with unique support of vector-scalar computing pattern. The novelties of the VSPIM can be concluded as follows: 1) support bit-serial based scalar-vector computing via ingenious parallel bit-broadcasting; 2) refine the GEMM mapping strategy and computing pattern to enhance performance and efficiency; 3) powered by the introduced scalar-vector operation, the bit-sparsity of activation is leveraged to halt unnecessary computation to maximize efficiency and throughput. Our comprehensive evaluation shows that, compared to the state-of-the-art SRAM-based digital-PIM design (Neural Cache), VSPIM can significantly boost the performance and energy efficiency by up to $8.87\times$ and $4.81\times$ respectively, with negligible area overhead, upon multiple representative neural networks.},
  archive      = {J_TC},
  author       = {Chen Nie and Chenyu Tang and Jie Lin and Huan Hu and Chenyang Lv and Ting Cao and Weifeng Zhang and Li Jiang and Xiaoyao Liang and Weikang Qian and Yanan Sun and Zhezhi He},
  doi          = {10.1109/TC.2023.3285095},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {2378-2390},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VSPIM: SRAM processing-in-memory DNN acceleration via vector-scalar operations},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical higher-order correlation attacks against
code-based masking. <em>TC</em>, <em>73</em>(10), 2364–2377. (<a
href="https://doi.org/10.1109/TC.2024.3424208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Masking is one of the most well-established methods to thwart side-channel attacks. Many masking schemes have been proposed in the literature, and code-based masking emerges and unifies several masking schemes in a coding-theoretic framework. In this work, we investigate the side-channel resistance of code-based masking from a non-profiling perspective by utilizing correlation-based side-channel attacks. We present a systematic evaluation of correlation attacks with various higher-order (centered) moments and then present the form of optimal correlation attacks. Interestingly, the Pearson correlation coefficient between the hypothetical leakage and the measured traces is connected to the signal-to-noise ratio in higher-order moments, and it turns out to be easy to evaluate rather than launch repeated attacks. We also identify some ineffective higher-order correlation attacks at certain orders when the device leaks under the Hamming weight leakage model. Our theoretical findings are verified through both simulated and real-world measurements.},
  archive      = {J_TC},
  author       = {Wei Cheng and Jingdian Ming and Sylvain Guilley and Jean-Luc Danger},
  doi          = {10.1109/TC.2024.3424208},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {2364-2377},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Statistical higher-order correlation attacks against code-based masking},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Highly evasive targeted bit-trojan on deep neural networks.
<em>TC</em>, <em>73</em>(9), 2350–2363. (<a
href="https://doi.org/10.1109/TC.2024.3416705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bit-Trojan attacks based on Bit-Flip Attacks (BFAs) have emerged as severe threats to Deep Neural Networks (DNNs) deployed in safety-critical systems since they can inject Trojans during the model deployment stage without accessing training supply chains. Existing works are mainly devoted to improving the executability of Bit-Trojan attacks, while seriously ignoring the concerns on evasiveness. In this paper, we propose a highly Evasive Targeted Bit-Trojan (ETBT) with evasiveness improvements from three aspects, i.e., reducing the number of bit-flips (improving executability), smoothing activation distribution, and reducing accuracy fluctuation. Specifically, key neuron extraction is utilized to identify essential neurons from DNNs precisely and decouple the key neurons between different classes, thus improving the evasiveness regarding accuracy fluctuation and executability. Additionally, activation-constrained trigger generation is devised to eliminate the differences between activation distributions of Trojaned and clean models, which enhances evasiveness from the perspective of activation distribution. Ultimately, the strategy of constrained target bits search is designed to reduce bit-flip numbers, directly benefits the evasiveness of ETBT. Benchmark-based experiments are conducted to evaluate the superiority of ETBT. Compared with existing works, ETBT can significantly improve evasiveness-relevant performances with much lower computation overheads, better robustness, and generalizability. Our code is released at https://github.com/bluefier/ETBT .},
  archive      = {J_TC},
  author       = {Lingxin Jin and Wei Jiang and Jinyu Zhan and Xiangyu Wen},
  doi          = {10.1109/TC.2024.3416705},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2350-2363},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Highly evasive targeted bit-trojan on deep neural networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Juliet: A configurable processor for computing on encrypted
data. <em>TC</em>, <em>73</em>(9), 2335–2349. (<a
href="https://doi.org/10.1109/TC.2024.3416752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully homomorphic encryption (FHE) has become progressively more viable in the years since its original inception in 2009. At the same time, leveraging state-of-the-art schemes in an efficient way for general computation remains prohibitively difficult for the average programmer. In this work, we introduce a new design for a fully homomorphic processor, dubbed Juliet, to enable faster operations on encrypted data using the state-of-the-art TFHE and cuFHE libraries for both CPU and GPU evaluation. To improve usability, we define an expressive assembly language and instruction set architecture (ISA) judiciously designed for end-to-end encrypted computation. We demonstrate Juliet&#39;s capabilities with a broad range of realistic benchmarks including cryptographic algorithms, such as the lightweight ciphers Simon and Speck , as well as logistic regression (LR) inference and matrix multiplication.},
  archive      = {J_TC},
  author       = {Charles Gouert and Dimitris Mouris and Nektarios Georgios Tsoutsos},
  doi          = {10.1109/TC.2024.3416752},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2335-2349},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Juliet: A configurable processor for computing on encrypted data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ERA-BS: Boosting the efficiency of ReRAM-based PIM
accelerator with fine-grained bit-level sparsity. <em>TC</em>,
<em>73</em>(9), 2320–2334. (<a
href="https://doi.org/10.1109/TC.2023.3290869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resistive Random-Access-Memory (ReRAM) crossbar is one of the most promising neural network accelerators, thanks to its in-memory and in-situ analog computing abilities for Matrix Multiplication-and-Accumulations (MACs). The key limitations are: 1) the number of rows and columns of ReRAM cells for concurrent execution of MACs is constrained, resulting in limited in-memory computing throughput; 2) the cost of high-precision analog-to-digital (A/D) conversions that can offset the efficiency and performance benefits of ReRAM-based Process-In-Memory (PIM). Meanwhile, it is challenging to deploy Deep Neural Network (DNN) models with a large model size in the crossbar since the sparsity of DNNs cannot be effectively exploited in the crossbar structure, especially the sparsity in the activation. As a countermeasure, we develop a novel ReRAM-based PIM accelerator, namely ERA-BS, which pays attention to the correlation between the bit-level sparsity (in both weights and activations) and the performance of the ReRAM-based crossbar. We propose a superior bit-flip scheme combined with the exponent-based quantization, which can adaptively flip the bits of the mapped DNNs to release redundant space without sacrificing the accuracy much or incurring much hardware overhead. Meanwhile, we design an architecture that can integrate the techniques to shrink the crossbar footprint to be used massively. We further propose a dynamic activation sparsity exploitation scheme in conjunction with the tightly coupled structure nature of the crossbar, including crossbar-aware activation pruning and ancillary run-time hardware support. In such a way, we exploit fine-grained sparsity weights (static) and activations (dynamic), respectively, to improve performance while reducing the energy consumption of computation with negligible overheads. Our experiments on a wide variety of networks show that compared to the well-known ReRAM-based PIM accelerator like “ISAAC”, ERA-BS can achieve up to 43 $\times$ , 78 $\times$ , and 73 $\times$ in terms of energy efficiency, area-efficiency, and throughput, respectively. Compared to the state-of-the-art ReRAM-based design “PIM-Prune”, ERA-BS can also achieve 5.3 $\times$ energy efficiency, 7.2 $\times$ area efficiency, and 32 $\times$ performance gain with a similar or even higher accuracy.},
  archive      = {J_TC},
  author       = {Fangxin Liu and Wenbo Zhao and Zongwu Wang and Yongbiao Chen and Xiaoyao Liang and Li Jiang},
  doi          = {10.1109/TC.2023.3290869},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2320-2334},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ERA-BS: Boosting the efficiency of ReRAM-based PIM accelerator with fine-grained bit-level sparsity},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient fault-tolerant path embedding for 3D torus network
using locally faulty blocks. <em>TC</em>, <em>73</em>(9), 2305–2319. (<a
href="https://doi.org/10.1109/TC.2024.3416695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D tori are significant interconnection architectures in building supercomputers and parallel computing systems. Due to the rapid growth of edge faults and the crucial role of path structures in large-scale distributed systems, fault-tolerant path embedding and correlated issues have drawn widespread researches. However, existing path embedding methods are based on traditional fault models, allowing all faults to be near the same node, so they usually only focus on theoretical proof and generate linear fault-tolerance related to dimension $n$ . In order to improve the fault-tolerance of 3D torus, we first propose a novel conditional fault model called the Locally Faulty Block model (LFB model). On the basis of this model, the Hamiltonian paths with large-scale edge defects in torus are investigated. After that, we construct an Hamiltonian path embedding algorithm HP-LFB into torus with $O(N)$ under the LFB model, where $N$ is the number of nodes in torus. Furthermore, we present an adaptive routing algorithm HoeFA, which is based on the method of distance vector to limit the use of virtual channels (VCs). We also make a comparison with state-of-the-art schemes, indicating that our scheme enhance other comprehensive results. The experiment indicated that HP-LFB can sustain the dynamic degradation of the batting average of establishing Hamiltonian paths, with the added faulty edges exceeding fault-tolerance.},
  archive      = {J_TC},
  author       = {Weibei Fan and Fu Xiao and Mengjie Lv and Lei Han and Shui Yu},
  doi          = {10.1109/TC.2024.3416695},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2305-2319},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient fault-tolerant path embedding for 3D torus network using locally faulty blocks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling reliable memory-mapped i/o with auto-snapshot for
persistent memory systems. <em>TC</em>, <em>73</em>(9), 2290–2304. (<a
href="https://doi.org/10.1109/TC.2024.3416683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent memory (PM) is promising to be the next-generation storage device with better I/O performance. Since the traditional I/O path is too lengthy to drive PM featuring low latency and high bandwidth, prior works proposed memory-mapped I/O (MMIO) to shorten the I/O path to PM. However, native MMIO directly maps files into the user address space, which puts files at risk of being corrupted by scribbles and non-atomic I/O interfaces, causing serious reliability issues. To address these issues, we propose RMMIO, an efficient user-space library that provides reliable MMIO for PM systems. RMMIO provides atomic I/O interfaces and lightweight snapshots to ensure the reliability of MMIO. Compared with existing schemes, RMMIO mitigates additional writes and extra software overheads caused by reliability guarantees, thus achieving MMIO-like performance. In addition, we also propose an automatic snapshot with efficient memory management for RMMIO to minimize data loss incurred by reliability issues. The experimental results of microbenchmarks show that RMMIO achieves 8.49x and 2.31x higher throughput than ext4-DAX and the state-of-the-art MMIO-based scheme, respectively, while ensuring data reliability. The real-world application accelerated by RMMIO achieves at most 7.06x higher throughput than that of ext4-DAX.},
  archive      = {J_TC},
  author       = {Bo Ding and Wei Tong and Yu Hua and Zhangyu Chen and Xueliang Wei and Dan Feng},
  doi          = {10.1109/TC.2024.3416683},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2290-2304},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling reliable memory-mapped I/O with auto-snapshot for persistent memory systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EcoFlow: Efficient convolutional dataflows on low-power
neural network accelerators. <em>TC</em>, <em>73</em>(9), 2275–2289. (<a
href="https://doi.org/10.1109/TC.2023.3272282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dilated and transposed convolutions are widely used in modern convolutional neural networks (CNNs). These kernels are used extensively during CNN training and inference of applications such as image segmentation and high-resolution image generation. We find that commonly-used low-power CNN inference accelerators are not optimized for both these convolutional kernels. Dilated and transposed convolutions introduce significant zero padding when mapped to the underlying spatial architecture, significantly degrading performance and energy efficiency. Existing approaches that address this issue require significant design changes to the otherwise simple, efficient, and well-adopted architectures used to compute direct convolutions. To address this challenge, we propose EcoFlow, a new set of dataflows and mapping algorithms for dilated and transposed convolutions. These algorithms are tailored to execute efficiently on existing low-cost, small-scale spatial architectures and requires minimal changes to existing accelerators. At its core, EcoFlow eliminates zero padding through careful dataflow orchestration and data mapping tailored to the spatial architecture. We evaluate EcoFlow on CNN training workloads and Generative Adversarial Network (GAN) workloads. Experiments in SASiML, our new cycle-accurate simulator, show that, using a common CNN inference accelerator, EcoFlow 1) reduces end-to-end CNN training time between 7-85%, and 2) improves end-to-end GAN training performance between 29-42%, compared to state-of-the-art CNN dataflows. SASiML is publicly and freely available at https://github.com/CMU-SAFARI/sasiml .},
  archive      = {J_TC},
  author       = {Lois Orosa and Skanda Koppula and Yaman Umuroglu and Konstantinos Kanellopoulos and Juan Gómez-Luna and Michaela Blott and Kees Vissers and Onur Mutlu},
  doi          = {10.1109/TC.2023.3272282},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2275-2289},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EcoFlow: Efficient convolutional dataflows on low-power neural network accelerators},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CBANA: A lightweight, efficient, and flexible cache behavior
analysis framework. <em>TC</em>, <em>73</em>(9), 2262–2274. (<a
href="https://doi.org/10.1109/TC.2024.3416747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cache miss analysis has become one of the most important things to improve the execution performance of a program. Generally, the approaches for analyzing cache misses can be categorized into dynamic analysis and static analysis. The former collects sampling statistics during program execution but is limited to specialized hardware support and incurs expensive execution overhead. The latter avoids the limitations but faces two challenges: inaccurate execution path prediction and inefficient analysis resulted by the explosion of the program state graph. To overcome these challenges, we propose CBANA, an LLVM- and process address space-based lightweight, efficient, and flexible cache behavior analysis framework. CBANA significantly improves the prediction accuracy of the execution path with awareness of inputs. To improve analysis efficiency and utilize the program preprocessing, CBANA refactors loop structures to reduce search space and dynamically splices intermediate results to reduce unnecessary or redundant computations. CBANA also supports configurable hardware parameter settings, and decouples the module of cache replacement policy from other modules. Thus, its flexibility is established. We evaluate CBANA by using the popular open benchmark PolyBench, graph workloads, and our synthetic workloads with good and poor data locality. Compared with the popular dynamic cache analysis tools Perf and Valgrind, the cache miss gap is less than 3.79% and 2.74% respectively with over ten thousand data accesses for the synthetic workloads, and the time reduction is up to 92.38% and 97.51% for the multiple-path workloads. Compared with the popular static cache analysis tool Heptane, CBANA achieves a time reduction of 97.71% while ensuring accuracy at the same time.},
  archive      = {J_TC},
  author       = {Qilin Hu and Yan Ding and Chubo Liu and Keqin Li and Kenli Li and Albert Y. Zomaya},
  doi          = {10.1109/TC.2024.3416747},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2262-2274},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CBANA: A lightweight, efficient, and flexible cache behavior analysis framework},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ToEx: Accelerating generation stage of transformer-based
language models via token-adaptive early exit. <em>TC</em>,
<em>73</em>(9), 2248–2261. (<a
href="https://doi.org/10.1109/TC.2024.3404051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based language models have recently gained popularity in numerous natural language processing (NLP) applications due to their superior performance compared to traditional algorithms. These models involve two execution stages: summarization and generation. The generation stage accounts for a significant portion of the total execution time due to its auto-regressive property, which necessitates considerable and repetitive off-chip accesses. Consequently, our objective is to minimize off-chip accesses during the generation stage to expedite transformer execution. To achieve the goal, we propose a token-adaptive early exit (ToEx) that generates output tokens using fewer decoders, thereby reducing off-chip accesses for loading weight parameters. Although our approach has the potential to minimize data communication, it brings two challenges: 1) inaccurate self-attention computation, and 2) significant overhead for exit decision. To overcome these challenges, we introduce a methodology that facilitates accurate self-attention by lazily performing computations for previously exited tokens. Moreover, we mitigate the overhead of exit decision by incorporating a lightweight output embedding layer. We also present a hardware design to efficiently support the proposed work. Evaluation results demonstrate that our work can reduce the number of decoders by 2.6 $\times$ on average. Accordingly, it achieves 3.2 $\times$ speedup on average compared to transformer execution without our work.},
  archive      = {J_TC},
  author       = {Myeonggu Kang and Junyoung Park and Hyein Shin and Jaekang Shin and Lee-Sup Kim},
  doi          = {10.1109/TC.2024.3404051},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2248-2261},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ToEx: Accelerating generation stage of transformer-based language models via token-adaptive early exit},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum support vector machine for classifying noisy data.
<em>TC</em>, <em>73</em>(9), 2233–2247. (<a
href="https://doi.org/10.1109/TC.2024.3416619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy data is ubiquitous in quantum computer, greatly affecting the performance of various algorithms. However, existing quantum support vector machine models are not equipped with anti-noise ability, and often deliver low performance when learning accurate hyperplane normal vectors from noisy data. To attack this issue, an anti-noise quantum support vector machine algorithm is developed in this paper. Specifically, a weight factor is first embedded into the hinge loss, so as to construct the objective function of anti-noise support vector machine. And then, an alternative iterative optimization strategy and a quantum circuit are designed for solving the objective function, aiming to obtain the normal vector and intercept of the hyperplane that finally divides the data. Finally, the classification and anti-noise effect of the algorithm are verified on artificial dataset and public dataset. Experimental results show that the proposed algorithm is efficient, yet maintains stable accuracy in noisy data.},
  archive      = {J_TC},
  author       = {Jiaye Li and Yangding Li and Jiagang Song and Jian Zhang and Shichao Zhang},
  doi          = {10.1109/TC.2024.3416619},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2233-2247},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Quantum support vector machine for classifying noisy data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novas: Tackling online dynamic video analytics with service
adaptation at mobile edge servers. <em>TC</em>, <em>73</em>(9),
2220–2232. (<a href="https://doi.org/10.1109/TC.2024.3416675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video analytics at mobile edge servers offers significant benefits like reduced response time and enhanced privacy. However, guaranteeing various quality-of-service (QoS) requirements of dynamic video analysis requests on heterogeneous edge devices remains challenging. In this paper, we propose a scalable online video analytics scheme, called Novas, which automatically makes precise service configuration adjustments upon constant video content changes. Specifically, Novas leverages the filtered confidence sum and a two-window t-test to online detect accuracy fluctuations without ground truth information. In such cases, Novas efficiently estimates the performance of all potential service configurations through a singular value decomposition (SVD)-based collaborative filtering method. Finally, given the NP-hardness of the optimal scheduling problem, a heuristic scheduling strategy that maximizes the minimum remaining resources is devised to schedule the most suitable configurations to servers for execution. We evaluate the effectiveness of Novas through extensive hybrid experiments conducted on a dedicated testbed. Results show that Novas can achieve a substantial over 27 $\times$ improvement in satisfying the accuracy requirements compared with existing methods adopting fixed configurations, while ensuring latency requirements. Moreover, Novas improves the goodput of the system by an average of 37.86% compared to existing state-of-the-art scheduling solutions.},
  archive      = {J_TC},
  author       = {Liang Zhang and Hongzi Zhu and Wen Fei and Yunzhe Li and Mingjin Zhang and Jiannong Cao and Minyi Guo},
  doi          = {10.1109/TC.2024.3416675},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2220-2232},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Novas: Tackling online dynamic video analytics with service adaptation at mobile edge servers},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified and fully automated framework for wavelet-based
attacks on random delay. <em>TC</em>, <em>73</em>(9), 2206–2219. (<a
href="https://doi.org/10.1109/TC.2024.3416682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a common defense against side-channel attacks, random delay insertion introduces noise into the executive flow of encryption, which increases attack complexity. Accordingly, various techniques are exploited to mitigate the defense effect of such insertions. As an advanced mathematical technique, wavelet analysis is considered to be a more effective technology according to its detailed and comprehensive interpretation of signals. In this paper, we propose a unified and fully automated wavelet-based attack framework (denoted as UWAF ), whose data processing is kept within one unified wavelet domain, with three enhanced components: denoising, alignment and key extraction. We put forward a new idea of combining machine learning with wavelet analysis to realize the full automation of the program for attack framework, rendering it possible to search exhaustively for the optimal combination of parameter settings in wavelet transform. Our proposal finds a new setting of wavelet parameters that have not been exploited ever before and achieves the performance enhancement for about 20 times fewer traces required for successful key recovery. UWAF is compared with several mainstream attack frameworks. Experimental results show that it outperforms those counterparts, and can be considered as an effective framework-level solution to defeat the countermeasure of random delay insertion.},
  archive      = {J_TC},
  author       = {Qianmei Wu and Fan Zhang and Shize Guo and Kun Yang and Haoting Shen},
  doi          = {10.1109/TC.2024.3416682},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2206-2219},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A unified and fully automated framework for wavelet-based attacks on random delay},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SimBU: Self-similarity-based hybrid binary-unary computing
for nonlinear functions. <em>TC</em>, <em>73</em>(9), 2192–2205. (<a
href="https://doi.org/10.1109/TC.2024.3398512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unary computing is a relatively new method for implementing arbitrary nonlinear functions that uses unpacked thermometer number encoding, enabling much lower hardware costs. In its original form, unary computing provides no trade-off between accuracy and hardware cost. In this work, we propose a novel self-similarity-based method to optimize the previous hybrid binary-unary work and provide it with the trade-off between accuracy and hardware cost by introducing controlled levels of approximation. Looking for self-similarity between different parts of a function allows us to implement a very small subset of core unique subfunctions and derive the rest of the subfunctions from this core using simple linear transformations. We compare our method to previous works such as FloPoCo-LUT (lookup table), HBU (hybrid binary-unary) and FloPoCo-PPA (piecewise polynomial approximation) on several 8–12-bit nonlinear functions including Log, Exp, Sigmoid, GELU, Sin, and Sqr, which are frequently used in neural networks and image processing applications. The area $\times$ delay hardware cost of our method is on average 32%–60% better than previous methods in both exact and approximate implementations. We also extend our method to multivariate nonlinear functions and show on average 78%–92% improvement over previous work.},
  archive      = {J_TC},
  author       = {Alireza Khataei and Gaurav Singh and Kia Bazargan},
  doi          = {10.1109/TC.2024.3398512},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2192-2205},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SimBU: Self-similarity-based hybrid binary-unary computing for nonlinear functions},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LMChain: An efficient load-migratable beacon-based sharding
blockchain system. <em>TC</em>, <em>73</em>(9), 2178–2191. (<a
href="https://doi.org/10.1109/TC.2024.3404057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharding is an important technology that utilizes group parallelism to enhance the scalability and performance of blockchain. However, the existing solutions use a historical transaction-based approach to reallocate shards, which cannot handle temporary overload and incurs additional overhead during the reallocation process. To this end, this paper proposes LMChain, an efficient load-migratable beacon-based sharding blockchain system. The primary goal of LMChain is to eliminate reliance on historical transactions and achieve the high performance. Specifically, we redesign the state maintenance data structure in Beacon Shard to effectively manage all account states at the shard level. Then, we innovatively propose a load-migratable transaction processing protocol built upon the new data structure. To mitigate read-write conflicts during the selection of migration transactions, we adopt a novel graph partitioning scheme. We also adopt a relay-based method to handle cross-shard transactions and resolve inter-shard state read-write conflicts. We implement the LMChain prototype and conduct experiments in a real network environment comprising 17 cloud servers. Experimental results show that, compared with state-of-the-art solutions, LMChain effectively reduces the average transaction waiting latency of overloaded transactions by 30% to 48% in different cases within 16 transaction shards, while improving throughput by 3% to 10%.},
  archive      = {J_TC},
  author       = {Dengcheng Hu and Jianrong Wang and Xiulong Liu and Qi Li and Keqiu Li},
  doi          = {10.1109/TC.2024.3404057},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2178-2191},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LMChain: An efficient load-migratable beacon-based sharding blockchain system},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the security of quotient filters: Attacks and potential
countermeasures. <em>TC</em>, <em>73</em>(9), 2165–2177. (<a
href="https://doi.org/10.1109/TC.2024.3371793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security of probabilistic data structures is increasingly important due to their wide adoption in many computing systems and applications. In particular, the security of approximate membership check filters such as Bloom or cuckoo filters has been recently studied showing how an attacker can degrade the filter performance in some settings. In this paper, we consider for the first time the security of another popular approximate membership check filter, the Quotient Filter (QF). Our analysis and simulations show that quotient filters are vulnerable to both white and black box attackers that can cause insertion failures and degrade the filter performance very significantly. An interesting finding is that quotient filters are vulnerable to a new type of attack, not applicable to Bloom or cuckoo filters, that can degrade the speed of queries dramatically. The paper also briefly discusses and evaluates potential countermeasures to detect and protect against those attacks.},
  archive      = {J_TC},
  author       = {Pedro Reviriego and Miguel González and Niv Dayan and Gabriel Huecas and Shanshan Liu and Fabrizio Lombardi},
  doi          = {10.1109/TC.2024.3371793},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2165-2177},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On the security of quotient filters: Attacks and potential countermeasures},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A combined trend virtual machine consolidation strategy for
cloud data centers. <em>TC</em>, <em>73</em>(9), 2150–2164. (<a
href="https://doi.org/10.1109/TC.2024.3416734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual machine (VM) consolidation strategies are widely used in cloud data centers (CDC) to optimize resource utilization and reduce total energy consumption. Although existing strategies consider current and future resource utilization, the impact of sudden bursts in historical resource utilization on the hosts has been underestimated in uncertain future periods. Insufficient analysis of historical resource utilization may increase the risk of host overloading and Service Level Agreement Violation (SLAV). By defining historical and future trends based on resource utilization, we propose a novel combined trend VM consolidation (CTVMC) strategy which can effectively reduce energy consumption and SLAV. The VMs with the largest combined trend are selected for migration to prevent host overloading. Based on the temporal locality and prediction technique, CTVMC then employs the past, present, and future resource utilization to filter candidate hosts, and identifies the most complementary host to place VM using combined trends. We conduct extensive simulation experiments with PlanetLab Trace and Google Cluster Trace in the CloudSim simulator. Compared with the well-known strategies, CTVMC strategy using the PlanetLab Trace can reduce the number of migrations by over 72.39%, SLAV by over 75.85%, and ESV (a combined metric that judges the trade-off between energy consumption and SLAV) by over 81.54%. According to the Google Cluster Trace, our strategy can reduce the number of migrations by over 61.51%, SLAV by over 37.37%, and ESV by over 35.30%.},
  archive      = {J_TC},
  author       = {Yuxuan Chen and Zhen Zhang and Yuhui Deng and Geyong Min and Lin Cui},
  doi          = {10.1109/TC.2024.3416734},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2150-2164},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A combined trend virtual machine consolidation strategy for cloud data centers},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ISSA: Architecting CNN accelerators using input-skippable,
set-associative computing-in-memory. <em>TC</em>, <em>73</em>(9),
2136–2149. (<a href="https://doi.org/10.1109/TC.2024.3404060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among several emerging architectures, computing in memory (CIM), which features in-situ analog computation, is a potential solution to the data movement bottleneck of the Von Neumann architecture for artificial intelligence (AI). Interestingly, more strengths of CIM significantly different from in-situ analog computation are not widely known yet. In this work, we point out that mutually stationary vectors (MSVs), which can be maximized by introducing associativity to CIM, are another inherent power unique to CIM. By MSVs, CIM exhibits significant freedom to dynamically vectorize the stored data (e.g., weights) to perform agile computation using the dynamically formed vectors. We have designed and realized an SA-CIM silicon prototype and corresponding architecture and acceleration schemes in the TSMC 28 nm process. More specifically, the contributions of this paper are fivefold: 1) We identify MSVs as new features that can be exploited to improve the current performance and energy challenges of the CIM-based hardware. 2) We propose SA-CIM to enhance MSVs (input-reordering flexibility) for skipping the zeros, small values, and sparse vectors. 3) We propose channel swapping to enhance the zero-skipping technique. 4) We propose a transposed systolic dataflow to efficiently conduct conv3 $\times$ 3 while being capable of exploiting input-skipping schemes. 5) We propose a design flow to search for optimal aggressive skipping scheme setups while satisfying the accuracy loss constraint. The proposed ISSA architecture improves the throughput by $1.91\times$ to $2.97\times$ speedup and the energy efficiency by $2.5\times$ to $4.2\times$ .},
  archive      = {J_TC},
  author       = {Yun-Chen Lo and Jun-Shen Wu and Chia-Chun Wang and Yu-Chih Tsai and Chih-Chen Yeh and Wen-Chien Ting and Ren-Shuo Liu},
  doi          = {10.1109/TC.2024.3404060},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2136-2149},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ISSA: Architecting CNN accelerators using input-skippable, set-associative computing-in-memory},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HYDRA: A hybrid resistance drift resilient architecture for
phase change memory-based neural network accelerators. <em>TC</em>,
<em>73</em>(9), 2123–2135. (<a
href="https://doi.org/10.1109/TC.2024.3404096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory Computing (IMC) using Phase Change Memory (PCM) has proven to be effective for efficient processing of Deep Neural Networks (DNNs). However, with the use of multi-level cell PCM (MLC-PCM) in NVMs-based accelerators, errors due to resistance drift in MLC-PCM can severely degrade the DNNs accuracy. In this paper, an analysis of the impact of resistance drift errors on accuracy of MLC-PCM based DNN accelerator shows that the drift errors alone can significantly impact the accuracy. This paper proposes Hydra, which is a hybrid resistance drift resilient architecture for MLC-PCM based DNN accelerators which use IMC for efficient computations. Hydra utilizes Tri-level cell PCM, which has a negligible resistance drift error rate, to store the critical bits of DNNs parameters and MLC-PCM (4-level cell), which has a higher error rate (but offers more storage density), for the non-critical bits. Experimental results on various DNN architectures, configurations and datasets show that, with the presence of resistance drift errors in PCM, Hydra can maintain the baseline accuracy of DNNs for up to 1 year (resistance drift is time-dependent), whereas conventional drift tolerance techniques lead to a significant accuracy drop in just a few seconds.},
  archive      = {J_TC},
  author       = {Thai-Hoang Nguyen and Muhammad Imran and Jaehyuk Choi and Joon-Sung Yang},
  doi          = {10.1109/TC.2024.3404096},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2123-2135},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HYDRA: A hybrid resistance drift resilient architecture for phase change memory-based neural network accelerators},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BlockCompass: A benchmarking platform for blockchain
performance. <em>TC</em>, <em>73</em>(8), 2111–2122. (<a
href="https://doi.org/10.1109/TC.2024.3404103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology has gained momentum due to its immutability and transparency. Several blockchain platforms, each with different consensus protocols, have been proposed. However, choosing and configuring such a platform is a non-trivial task. Numerous benchmarking tools have been introduced to test the performance of blockchain solutions. Yet, these tools are often limited to specific blockchain platforms or require complex configurations. Moreover, they tend to focus on one-off batch evaluation models, which may not be ideal for longer-running instances under continuous workloads. In this work, we present BlockCompass , an all-inclusive blockchain benchmarking tool that can be easily configured and extended. We demonstrate how BlockCompass can evaluate the performance of various blockchain platforms and configurations, including Ethereum Proof-of-Authority, Ethereum Proof-of-Work, Hyperledger Fabric Raft, Hyperledger Sawtooth with Proof-of-Elapsed-Time, Practical Byzantine Fault Tolerance, and Raft consensus algorithms, against workloads that continuously fluctuate over time. We show how continuous transactional workloads may be more appropriate than batch workloads in capturing certain stressful events for the system. Finally, we present the results of a usability study about the convenience and effectiveness offered by BlockCompass in blockchain benchmarking.},
  archive      = {J_TC},
  author       = {Mohammadreza Rasolroveicy and Wejdene Haouari and Marios Fokaefs},
  doi          = {10.1109/TC.2024.3404103},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2111-2122},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BlockCompass: A benchmarking platform for blockchain performance},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BSR-FL: An efficient byzantine-robust privacy-preserving
federated learning framework. <em>TC</em>, <em>73</em>(8), 2096–2110.
(<a href="https://doi.org/10.1109/TC.2024.3404102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a technique that enables clients to collaboratively train a model by sharing local models instead of raw private data. However, existing reconstruction attacks can recover the sensitive training samples from the shared models. Additionally, the emerging poisoning attacks also pose severe threats to the security of FL. However, most existing Byzantine-robust privacy-preserving federated learning solutions either reduce the accuracy of aggregated models or introduce significant computation and communication overheads. In this paper, we propose a novel B lockchain-based S ecure and R obust F ederated L earning (BSR-FL) framework to mitigate reconstruction attacks and poisoning attacks. BSR-FL avoids accuracy loss while ensuring efficient privacy protection and Byzantine robustness. Specifically, we first construct a lightweight non-interactive functional encryption (NIFE) scheme to protect the privacy of local models while maintaining high communication performance. Then, we propose a privacy-preserving defensive aggregation strategy based on NIFE, which can resist encrypted poisoning attacks without compromising model privacy through secure cosine similarity and incentive-based Byzantine-tolerance aggregation. Finally, we utilize the blockchain system to assist in facilitating the processes of federated learning and the implementation of protocols. Extensive theoretical analysis and experiments demonstrate that our new BSR-FL has enhanced privacy security, robustness, and high efficiency.},
  archive      = {J_TC},
  author       = {Honghong Zeng and Jie Li and Jiong Lou and Shijing Yuan and Chentao Wu and Wei Zhao and Sijin Wu and Zhiwen Wang},
  doi          = {10.1109/TC.2024.3404102},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2096-2110},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BSR-FL: An efficient byzantine-robust privacy-preserving federated learning framework},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DPU-direct: Unleashing remote accelerators via enhanced RDMA
for disaggregated datacenters. <em>TC</em>, <em>73</em>(8), 2081–2095.
(<a href="https://doi.org/10.1109/TC.2024.3404089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents DPU-Direct, an accelerator disaggregation system that connects accelerator nodes (ANs) and CPU nodes (CNs) over a standard Remote Direct Memory Access (RDMA) network. DPU-Direct eliminates the latency introduced by the CPU-based network stack, and PCIe interconnects between network I/O and the accelerator. The DPU-Direct system architecture includes a DPU Wrapper hardware architecture, an RDMA-based Accelerator Access Pattern (RAAP), and a CN-side programming model. The DPU Wrapper connects accelerators directly with the RDMA engine, turning ANs into disaggregation-native devices. The RAAP provides the CN with low-latency and high throughput accelerator semantics based on standard RDMA operations. Our FPGA prototype demonstrates DPU-Direct&#39;s efficacy with two proof-of-concept applications: AES encryption and key-value cache, which are computationally intensive and latency-sensitive. DPU-Direct yields a 400x speedup in AES encryption over the CPU baseline and matches the performance of the locally integrated AES accelerator. For key-value cache, DPU-Direct reduces the average end-to-end latency by 1.66x for GETs and 1.30x for SETs over the CPU-RDMA-Polling baseline, reducing latency jitter by over 10x for both operations.},
  archive      = {J_TC},
  author       = {Yunkun Liao and Jingya Wu and Wenyan Lu and Xiaowei Li and Guihai Yan},
  doi          = {10.1109/TC.2024.3404089},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2081-2095},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DPU-direct: Unleashing remote accelerators via enhanced RDMA for disaggregated datacenters},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A machine learning-empowered cache management scheme for
high-performance SSDs. <em>TC</em>, <em>73</em>(8), 2066–2080. (<a
href="https://doi.org/10.1109/TC.2024.3404064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NAND Flash-based solid-state drives (SSDs) have gained widespread usage in data storage thanks to their exceptional performance and low power consumption. The computational capability of SSDs has been elevated to tackle complex algorithms. Inside an SSD, a DRAM cache for frequently accessed requests reduces response time and write amplification (WA), thereby improving SSD performance and lifetime. Existing caching schemes, based on temporal locality, overlook its variations, which potentially reduces cache hit rates. Some caching schemes bolster performance via flash-aware techniques but at the expense of the cache hit rate. To address these issues, we propose a random forest machine learning C lassifier-empowered C ache scheme named CCache, where I/O requests are classified into critical, intermediate, and non-critical ones according to their access status. After designing a machine learning model to predict these three types of requests, we implement a trie-level linked list to manage the cache placement and replacement. CCache safeguards critical requests for cache service to the greatest extent, while granting the highest priority to evicting request accessed by non-critical requests. CCache – considering chip state when processing non-critical requests – is implemented in an SSD simulator (SSDSim). CCache outperforms the alternative caching schemes, including LRU, CFLRU, LCR, NCache, ML_WP, and CCache_ANN, in terms of response time, WA, erase count, and hit ratio. The performance discrepancy between CCache and the OPT scheme is marginal. For example, CCache reduces the response time of the competitors by up to 41.9% with an average of 16.1%. CCache slashes erase counts by a maximum of 67.4%, with an average of 21.3%. The performance gap between CCache and and OPT is merely 2.0%-3.0%.},
  archive      = {J_TC},
  author       = {Hui Sun and Chen Sun and Haoqiang Tong and Yinliang Yue and Xiao Qin},
  doi          = {10.1109/TC.2024.3404064},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2066-2080},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A machine learning-empowered cache management scheme for high-performance SSDs},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FutureDID: A fully decentralized identity system with
multi-party verification. <em>TC</em>, <em>73</em>(8), 2051–2065. (<a
href="https://doi.org/10.1109/TC.2024.3398509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized identity (DID) systems conforming to the World Wide Web Consortium (W3C) Decentralized Identifiers (DIDs) and Verifiable Credentials Data Model recommendations have recently attracted attention due to their better autonomy, interoperability, and openness design. However, those W3C recommendations lack a design for addressing the single point of failure (SPOF) and identity revocation, which could seriously compromise the robustness and practicality of DID systems. To remedy these limitations, we propose FutureDID, a DID system that enables multiple parties to jointly issue credentials and efficiently revoke DID identities, providing a robust and practical DID system. FutureDID is designed with a multi-party credential issuing mechanism based on distributed key generation technology, which transforms trust from a single entity to distributed committees and facilitates authentication between issuers, making it more resistant to SPOF. Moreover, the underlying blockchain system is built on a chameleon hash function to ensure tamper-proof and enable efficient identity revocation. We have implemented a prototype system using FISCO BCOS and conducted extensive evaluations to demonstrate the effectiveness and practicality of our system. Our evaluations have shown that FutureDID provides a significant improvement in efficiency, achieving at least a 60 $\times$ efficiency improvement in identity revocation compared to state-of-the-art systems.},
  archive      = {J_TC},
  author       = {Haotian Deng and Jinwen Liang and Chuan Zhang and Ximeng Liu and Liehuang Zhu and Song Guo},
  doi          = {10.1109/TC.2024.3398509},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2051-2065},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FutureDID: A fully decentralized identity system with multi-party verification},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward finding s-box circuits with optimal multiplicative
complexity. <em>TC</em>, <em>73</em>(8), 2036–2050. (<a
href="https://doi.org/10.1109/TC.2024.3398507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new method to find S-box circuits with optimal multiplicative complexity (MC), i.e., MC-optimal S-box circuits. We provide new observations for efficiently constructing circuits and computing MC, combined with a popular pathfinding algorithm named A*. In our search, the A* algorithm outputs a path of length MC, corresponding to an MC-optimal circuit. Based on an in-depth analysis of the process of computing MC, we enable the A* algorithm to function within our graph to investigate a wider range of S-boxes than existing methods such as the SAT-solver-based tool [1] and LIGHTER [2] . We provide implementable MC-optimal circuits for all the quadratic 5-bit bijective S-boxes and existing 5-bit almost-perfect nonlinear (APN) S-boxes. Furthermore, we present MC-optimal circuits for 6-bit S-boxes such as Sarkar Gold, Sarkar Quadratic, and some quadratic permutations. Finally, we theoretically demonstrate new lower bounds for the MCs of S-boxes, providing tighter bounds for the MCs of AES and MISTY S-boxes than previously known. This study complements previous results on MC-optimal S-box circuits and is intended to provide further insight into this field.},
  archive      = {J_TC},
  author       = {Yongjin Jeon and Seungjun Baek and Jongsung Kim},
  doi          = {10.1109/TC.2024.3398507},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2036-2050},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Toward finding S-box circuits with optimal multiplicative complexity},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AQA: An adaptive post-training quantization method for
activations of CNNs. <em>TC</em>, <em>73</em>(8), 2025–2035. (<a
href="https://doi.org/10.1109/TC.2024.3398503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The post-training quantization (PTQ) is a common technology to improve the efficiency of embedded neural network accelerators. Existing PTQ schemes for CNN activations usually rely on calibration dataset with good data representation to reduce quantization overflow in inference, which is not always effective due to large variation and uncertainty of the inference input data in practice. This paper proposes an adaptive PTQ method for activations (AQA), which monitors the quantization overflow of activations, adaptively updates the quantization parameters, and re-quantizes the activations on-the-fly when the overflow degree is over a threshold. The key challenges in implementing the AQA method are to reduce the associated side-effects in increasing computational complexity, processing time and hardware resource usage. We propose a series of design optimizations for quantization overflow monitor, quantization parameters update and re-quantization to successfully address the challenges. The proposed AQA method is implemented in a CNN accelerator and evaluated on VGG16, ResNet18 and MobileNetV2 on several datasets. Experiment results show that the adaptation method makes the models’ inference accuracy stable over various quantization overflow degrees, while the static quantization method suffers from significant accuracy degradation. The costs introduced by the adaptation method include 5% power consumption increase and 4% throughput degradation.},
  archive      = {J_TC},
  author       = {Yun Wang and Qiang Liu},
  doi          = {10.1109/TC.2024.3398503},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2025-2035},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AQA: An adaptive post-training quantization method for activations of CNNs},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revocable and efficient blockchain-based fine-grained access
control against EDoS attacks in cloud storage. <em>TC</em>,
<em>73</em>(8), 2012–2024. (<a
href="https://doi.org/10.1109/TC.2024.3398502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users have become accustomed to storing data on the cloud using ciphertext policy attribute-based encryption (CP-ABE) for fine-grained access control. However, this encryption method does not consider the ability of malicious users to launch thousands of file download requests when launching an economic denial of sustainability attack (EDoS), which may be more expensive for data owners. Existing solutions typically use a cloud server to verify the download permissions of the data users. However, cloud servers are not completely trusted and cloud server providers and colluding data users can still launch an EDoS attack. With our scheme, using CP-ABE, a blockchain is introduced for verifying the download permission of data users. In addition, we propose a new mechanism to solve the problem of malicious user revocations under EDoS attacks by updating the ciphertext and symmetric encryption technology. A formal security proof has demonstrated that the proposed scheme is suitable for plaintext attack security. Theoretical and experimental analyses show that our scheme performs more efficiently than previous methods.},
  archive      = {J_TC},
  author       = {Qingyang Zhang and Chang Xu and Hong Zhong and Chengjie Gu and Jie Cui},
  doi          = {10.1109/TC.2024.3398502},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2012-2024},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Revocable and efficient blockchain-based fine-grained access control against EDoS attacks in cloud storage},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReDas: A lightweight architecture for supporting
fine-grained reshaping and multiple dataflows on systolic array.
<em>TC</em>, <em>73</em>(8), 1997–2011. (<a
href="https://doi.org/10.1109/TC.2024.3398500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The systolic accelerator is one of the premier architectural choices for DNN acceleration. However, the conventional systolic architecture suffers from low PE utilization due to the mismatch between the fixed array and diverse DNN workloads. Recent studies have proposed flexible systolic array architectures to adapt to DNN models. However, these designs support only coarse-grained reshaping or significantly increase hardware overhead. In this study, we propose ReDas, a flexible and lightweight systolic array that supports dynamic fine-grained reshaping and multiple dataflows. First, ReDas integrates lightweight and reconfigurable roundabout data paths, which achieve fine-grained reshaping using only short connections between adjacent PEs. Second, we redesign the PE microarchitecture and integrate a set of multi-mode data buffers around the array. The PE structure enables additional data bypassing and flexible data switching. Simultaneously, the multi-mode buffers facilitate fine-grained reallocation of on-chip memory resources, adapting to various dataflow requirements. ReDas can dynamically reconfigure to up to 129 different logical shapes and 3 dataflows for a $128\times 128$ array. Finally, we propose an efficient mapper to generate appropriate configurations for each layer of DNN workloads. Compared to the conventional systolic array, ReDas can achieve about 4.6 $\times$ speedup and 8.3 $\times$ energy-delay product (EDP) reduction.},
  archive      = {J_TC},
  author       = {Meng Han and Liang Wang and Limin Xiao and Tianhao Cai and Zeyu Wang and Xiangrong Xu and Chenhao Zhang},
  doi          = {10.1109/TC.2024.3398500},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {1997-2011},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ReDas: A lightweight architecture for supporting fine-grained reshaping and multiple dataflows on systolic array},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective huge page strategies for TLB miss reduction in
nested virtualization. <em>TC</em>, <em>73</em>(8), 1983–1996. (<a
href="https://doi.org/10.1109/TC.2024.3398498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Huge page strategies, such as Linux Transparent Huge Page (THP), have become a prevalent solution to mitigate the performance bottleneck caused by increasingly high memory address translation overhead. However, in cloud environments, virtualization presents a two-fold challenge, exacerbating address translation overhead and undermining the effectiveness of huge page strategies. To effectively reduce address translation overhead, huge page strategies in the host and guest virtual machines (VMs) must work in concert for “proper huge page alignment”, i.e., huge pages in guest VMs being backed by host huge pages. This requires a cross-layer coordinating mechanism, which has been designed targeting non-nested virtualization settings. The paper introduces xGemini as an efficient solution targeting nested virtualization settings, where addressing these issues is particularly challenging, given the additional obstacles in creating synergy between host and guest VMs, due to an extra layer of page mappings by guest hypervisors. xGemini addresses these challenges by improving the shadow paging mechanism. Evaluation based on the KVM/Linux prototype implementation and diverse real-world applications shows xGemini greatly reduces TLB misses and enhances application performance in nested virtualization.},
  archive      = {J_TC},
  author       = {Weiwei Jia and Jiyuan Zhang and Jianchen Shan and Xiaoning Ding},
  doi          = {10.1109/TC.2024.3398498},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {1983-1996},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Effective huge page strategies for TLB miss reduction in nested virtualization},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ada-WL: An adaptive wear-leveling aware data migration
approach for flexible SSD array scaling in clusters. <em>TC</em>,
<em>73</em>(8), 1967–1982. (<a
href="https://doi.org/10.1109/TC.2024.3398493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the flash-based Solid State Drive (SSD) array has been widely implemented in real-world large-scale clusters. With the increasing number of users in upper-tier applications and the burst of Input/Output requests in this data explosive era, data centers need to continuously scale up to meet real-time data storage needs. However, the classical disk array scaling methods are designed based on HDDs, ignoring the wear leveling and garbage collection characteristics of SSD. This leads to penalties due to the vast lifetime gap between extended SSDs and the original in-use SSDs while scaling the SSD array, including extra triggered wear leveling I/O, latency in average response time, etc. To address these problems, we propose an Adaptive Wear-Leveling aware data migration approach for flexible SSD array scaling in clusters. It manages the interdisk wear leveling based on Model Reference Adaptive Control, which includes an SSD behavior emulator, Kalman filter estimator, and adaptive law. To demonstrate the effectiveness of this approach, we conducted several simulations and implementations on actual hardware. The evaluation results show that Ada-WL has the self-adaptability to optimize the wear leveling management parameters for various states of SSD arrays, diverse workloads, and scaling performed multiple times, significantly improving performance for SSD array scaling.},
  archive      = {J_TC},
  author       = {Yunfei Gu and Linhui Liu and Chentao Wu and Jie Li and Minyi Guo},
  doi          = {10.1109/TC.2024.3398493},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {1967-1982},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Ada-WL: An adaptive wear-leveling aware data migration approach for flexible SSD array scaling in clusters},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing neural network reliability: Insights from
hardware/software collaboration with neuron vulnerability quantization.
<em>TC</em>, <em>73</em>(8), 1953–1966. (<a
href="https://doi.org/10.1109/TC.2024.3398492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the reliability of deep neural networks (DNNs) is paramount in safety-critical applications. Although introducing supplementary fault-tolerant mechanisms can augment the reliability of DNNs, an efficiency tradeoff may be introduced. This study reveals the inherent fault tolerance of neural networks, where individual neurons exhibit varying degrees of fault tolerance, by thoroughly exploring the structural attributes of DNNs. We thereby develop a hardware/software collaborative method that guarantees the reliability of DNNs while minimizing performance degradation. We introduce the neuron vulnerability factor (NVF) to quantify the susceptibility to soft errors. We propose two efficient methods that leverage the NVF to minimize the negative effects of soft errors on neurons. First, we present a novel computational scheduling scheme. By prioritizing error-prone neurons, the expedited completion of their computations is facilitated to mitigate the risk of neural computing errors that arise from soft errors without sacrificing efficiency. Second, we propose the NVF-guided heterogeneous memory system. We employ variable-strength error-correcting codes and tailor their error-correction mechanisms to the vulnerability profile of specific neurons to ensure a highly targeted approach for error mitigation. Our experimental results demonstrate that the proposed scheme enhances the neural network accuracy by 18% on average, while significantly reducing the fault-tolerance overhead.},
  archive      = {J_TC},
  author       = {Jing Wang and Jinbin Zhu and Xin Fu and Di Zang and Keyao Li and Weigong Zhang},
  doi          = {10.1109/TC.2024.3398492},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {1953-1966},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enhancing neural network reliability: Insights from Hardware/Software collaboration with neuron vulnerability quantization},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relieving write disturbance for phase change memory with
RESET-aware data encoding. <em>TC</em>, <em>73</em>(8), 1939–1952. (<a
href="https://doi.org/10.1109/TC.2024.3398490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The write disturbance (WD) problem is becoming increasingly severe in PCM due to the continuous scaling down of memory technology. Previous studies have attempted to transform WD-vulnerable data patterns of the new data to alleviate the WD problem. However, through a wide spectrum of real-world benchmarks, we have discovered that simply transforming WD-vulnerable data patterns does not proportionally reduce (or may even increase) WD errors. To address this issue, we present ResEnc , a RESET-aware data encoding scheme that reduces RESET operations to mitigate the WD problem in both wordlines and bitlines for PCM. It dynamically establishes a mask word for each block for data encoding and adaptively selects an appropriate encoding granularity based on the diverse write patterns. ResEnc finally reassigns the mask words of unchanged blocks to changed blocks for exploring a further reduction of WD errors. Extensive experiments show that ResEnc can reduce 16.8-87.0% of WD errors, shorten 5.6-39.6% of write latency, and save 7.0-43.1% of write energy for PCM.},
  archive      = {J_TC},
  author       = {Ronglong Wu and Zhirong Shen and Jianqiang Chen and Chengshuo Zheng and Zhiwei Yang and Jiwu Shu},
  doi          = {10.1109/TC.2024.3398490},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {1939-1952},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Relieving write disturbance for phase change memory with RESET-aware data encoding},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoDA: A co-design framework for versatile and efficient
attention accelerators. <em>TC</em>, <em>73</em>(8), 1924–1938. (<a
href="https://doi.org/10.1109/TC.2024.3398488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a primary component of Transformers, attention mechanism suffers from quadratic computational complexity. To achieve efficient implementations, its hardware accelerator designs have aroused great research interest. However, most existing accelerators only support a single type of application and a single type of attention, making it difficult to meet the demands of diverse application scenarios. Additionally, they mainly focus on the dynamic pruning of attention matrices, which requires the deployment of pre-processing units, thereby reducing overall hardware efficiency. This paper presents CoDA which is an algorithm, dataflow and architecture co-design framework for versatile and efficient attention accelerators. The designed accelerator supports both NLP and CV applications, and can be configured into the mode supporting low-rank attention or low-rank plus sparse attention. We apply algorithmic transformations to low-rank attention to significantly reduce computational complexity. To prevent an increase in storage overhead resulting from the proposed algorithmic transformations, we carefully design the dataflows and adopt a block-wise fashion. Down-scaling softmax is further supported by architecture and dataflow co-design. Moreover, we propose a softmax sharing strategy to reduce the area cost. Our experiment results demonstrate that the proposed accelerator outperforms the state-of-the-art designs in terms of throughput, area efficiency and energy efficiency.},
  archive      = {J_TC},
  author       = {Wenjie Li and Aokun Hu and Ningyi Xu and Guanghui He},
  doi          = {10.1109/TC.2024.3398488},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {1924-1938},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CoDA: A co-design framework for versatile and efficient attention accelerators},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uniformity and independence of h3 hash functions for bloom
filters. <em>TC</em>, <em>73</em>(8), 1913–1923. (<a
href="https://doi.org/10.1109/TC.2024.3398426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the effects of violating the conditions of hash function uniformity and/or independence on the false positive probability of Bloom Filters (BF). To this end, we focus on hash functions of the H3 family with a partitioned memory organization for fast hardware implementations of BFs. We first introduce a dependence metric that quantifies hash function uniformity and independence. We then state and prove the necessary and sufficient conditions on the BF parameters for constructing uniform and independent hash functions. Finally, we derive an analytical expression for the exact false positive probability of a BF with hash functions that are not necessarily uniform or independent. We verify our expression with a hardware test bench and explore the effects of losing uniformity and independence through an experimental study that systematically sweeps different dependence metric values and numbers of hash functions. We demonstrate the effects of violating hash function uniformity and independence on the stated target false positive probability for selected previous works in the literature. As an important finding, we show that uniformity of individual hash functions is essential, whereas limited dependencies between hash functions can be tolerated without a negative effect on the false positive probability.},
  archive      = {J_TC},
  author       = {Furkan Koltuk and Ece Güran Schmidt},
  doi          = {10.1109/TC.2024.3398426},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {1913-1923},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Uniformity and independence of h3 hash functions for bloom filters},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TensorMap: A deep RL-based tensor mapping framework for
spatial accelerators. <em>TC</em>, <em>73</em>(8), 1899–1912. (<a
href="https://doi.org/10.1109/TC.2024.3398424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mapping of tensor computation is a complex and important process for spatial accelerators. Today&#39;s mapping works depend on hand-tuned kernel libraries or search-based heuristics from human experts. The former is time-intensive while the latter easily leads to sub-optimal performance. In this paper, we propose TensorMap, a deep reinforcement learning (RL)-based mapping framework for tensor computations on spatial accelerators. We propose a sequential generation mode for mapping optimization and construct a coarse-grained action space to reduce the complexity of the mapping search space. An efficient policy network is devised to optimize mapping primitives in the RL-based search. We then propose a stop signal that is sampled from Bernoulli distribution to facilitate multi-level loop unrolling for spatial accelerators. Finally, a genetic algorithm is employed to further refine the optimized mappings. In the experiments, we demonstrate TensorMap&#39;s ability for different spatial accelerators with various tensor computations. On TPU, TensorMap provides 2.6 $\times$ , 2.7 $\times$ , and 2.4 $\times$ better energy-delay product (EDP) on average compared with FlexTensor, Ansor, and AMOS respectively. On Eyeriss, TensorMap provides 2.1 $\times$ , 1.8 $\times$ , and 1.7 $\times$ better EDP on average compared with FlexTensor, Ansor, and AMOS respectively.},
  archive      = {J_TC},
  author       = {Fuyu Wang and Minghua Shen and Yutong Lu and Nong Xiao},
  doi          = {10.1109/TC.2024.3398424},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {1899-1912},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TensorMap: A deep RL-based tensor mapping framework for spatial accelerators},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective hardware-mapping co-optimisation for
multi-DNN workloads on chiplet-based accelerators. <em>TC</em>,
<em>73</em>(8), 1883–1898. (<a
href="https://doi.org/10.1109/TC.2024.3386067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need to efficiently execute different Deep Neural Networks (DNNs) on the same computing platform, coupled with the requirement for easy scalability, makes Multi-Chip Module (MCM)-based accelerators a preferred design choice. Such an accelerator brings together heterogeneous sub-accelerators in the form of chiplets, interconnected by a Network-on-Package (NoP). This paper addresses the challenge of selecting the most suitable sub-accelerators, configuring them, determining their optimal placement in the NoP, and mapping the layers of a predetermined set of DNNs spatially and temporally. The objective is to minimise execution time and energy consumption during parallel execution while also minimising the overall cost, specifically the silicon area, of the accelerator. This paper presents MOHaM, a framework for multi-objective hardware-mapping co-optimisation for multi-DNN workloads on chiplet-based accelerators. MOHaM exploits a multi-objective evolutionary algorithm that has been specialised for the given problem by incorporating several customised genetic operators. MOHaM is evaluated against state-of-the-art Design Space Exploration (DSE) frameworks on different multi-DNN workload scenarios. The solutions discovered by MOHaM are Pareto optimal compared to those by the state-of-the-art. Specifically, MOHaM-generated accelerator designs can reduce latency by up to $96\%$ and energy by up to $96.12\%$ .},
  archive      = {J_TC},
  author       = {Abhijit Das and Enrico Russo and Maurizio Palesi},
  doi          = {10.1109/TC.2024.3386067},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {1883-1898},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-objective hardware-mapping co-optimisation for multi-DNN workloads on chiplet-based accelerators},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Construction of reed-solomon erasure codes with four
parities based on systematic vandermonde matrices. <em>TC</em>,
<em>73</em>(7), 1875–1882. (<a
href="https://doi.org/10.1109/TC.2024.3387069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2021, Tang et al. proposed an improved construction of Reed-Solomon (RS) erasure codes with four parity symbols to accelerate the computation of Reed-Muller (RM) transform-based RS algorithm. The idea is to change the original Vandermonde parity-check matrix into a systematic Vandermonde parity-check matrix. However, the construction relies on a computer search and requires that the size of the information vector of RS codes does not exceed $52$ . This paper improves its idea and proposes a purely algebraic construction. The proposed method has a more explicit construction, a wider range of codeword lengths, and competitive encoding/erasure decoding computational complexity.},
  archive      = {J_TC},
  author       = {Leilei Yu and Yunghsiang S. Han},
  doi          = {10.1109/TC.2024.3387069},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1875-1882},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Construction of reed-solomon erasure codes with four parities based on systematic vandermonde matrices},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid-memcached: A novel approach for memcached persistence
optimization with hybrid memory. <em>TC</em>, <em>73</em>(7), 1866–1874.
(<a href="https://doi.org/10.1109/TC.2024.3385279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memcached is a widely adopted, high-performance, in-memory key-value object caching system utilized in data centers. Nonetheless, its data is stored in volatile DRAM, making the cached data susceptible to loss during system shutdowns. Consequently, cold restarts experience significant delays. Persistent memory is a byte-addressable, large-capacity, and non-volatility storage media, which can be employed to avoid the cold restart problem. However, deploying Memcached on persistent memory requires consideration of issues such as write endurance, asymmetric read/write latency and bandwidth, and write granularity of persistent memory. In this paper, we propose Hybrid-Memcached, an optimized Memcached framework based on a hybrid combination of DRAM and persistent memory. Hybrid-Memcached includes three key components: (1) a DRAM-based data aggregation buffer to avoid multiple fine-grained writes, which extends the write endurance of persistent memory, (2) a data-object alignment mechanism to avoid write amplification, and (3) a non-temporal store instruction-based writing strategy to improve the bandwidth utilization. We have implemented Hybrid-Memcached on the Intel Optane persistent memory. Several micros-benchmarks are designed to evaluate Hybrid-Memcached by varying read/write ratios, access distributions, and key-value item sizes. Additionally, we evaluated it with the YCSB benchmark, showing a 21.2% performance improvement for fully write-intensive workloads and 11.8% for read-write balanced workloads.},
  archive      = {J_TC},
  author       = {Zhang Jiang and Xianduo Li and Tianxiang Peng and Haoran Li and Jingxuan Hong and Jin Zhang and Xiaoli Gong},
  doi          = {10.1109/TC.2024.3385279},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1866-1874},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hybrid-memcached: A novel approach for memcached persistence optimization with hybrid memory},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Raptor-t: A fused and memory-efficient sparse transformer
for long and variable-length sequences. <em>TC</em>, <em>73</em>(7),
1852–1865. (<a href="https://doi.org/10.1109/TC.2024.3389507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models have made significant advancements across various domains, largely due to the self-attention mechanism&#39;s ability to capture contextual relationships in input sequences. However, processing long sequences remains computationally expensive for Transformer models, primarily due to the $O(n^{2})$ complexity associated with self-attention. To address this, sparse attention has been proposed to reduce the quadratic dependency to linear. Nevertheless, deploying the sparse transformer efficiently encounters two major obstacles: 1) Existing system optimizations are less effective for the sparse transformer due to the algorithm&#39;s approximation properties leading to fragmented attention, and 2) the variability of input sequences results in computation and memory access inefficiencies. We present Raptor-T, a cutting-edge transformer framework designed for handling long and variable-length sequences. Raptor-T harnesses the power of the sparse transformer to reduce resource requirements for processing long sequences while also implementing system-level optimizations to accelerate inference performance. To address the fragmented attention issue, Raptor-T employs fused and memory-efficient Multi-Head Attention. Additionally, we introduce an asynchronous data processing method to mitigate GPU-blocking operations caused by sparse attention. Furthermore, Raptor-T minimizes padding for variable-length inputs, effectively reducing the overhead associated with padding and achieving balanced computation on GPUs. In evaluation, we compare Raptor-T&#39;s performance against state-of-the-art frameworks on an NVIDIA A100 GPU. The experimental results demonstrate that Raptor-T outperforms FlashAttention-2 and FasterTransformer, achieving an impressive average end-to-end performance improvement of 3.41X and 3.71X, respectively.},
  archive      = {J_TC},
  author       = {Hulin Wang and Donglin Yang and Yaqi Xia and Zheng Zhang and Qigang Wang and Jianping Fan and Xiaobo Zhou and Dazhao Cheng},
  doi          = {10.1109/TC.2024.3389507},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1852-1865},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Raptor-T: A fused and memory-efficient sparse transformer for long and variable-length sequences},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobility-aware utility maximization in digital twin-enabled
serverless edge computing. <em>TC</em>, <em>73</em>(7), 1837–1851. (<a
href="https://doi.org/10.1109/TC.2024.3388897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by data and models, the digital twin technique presents a new concept of optimizing system design, process monitoring, decision-making and more, through performing comprehensive virtual-reality interaction and continuous mapping. By introducing serverless computing to Mobile Edge Computing (MEC) environments, the emerging serverless edge computing paradigm facilitates the communication-efficient digital twin services, and promises agile, fine-grained and cost-efficient provisioning of limited edge resources, where serverless functions are implemented by containers in cloudlets (edge servers). However, the nonnegligible cold start delay of containers deteriorates the responsiveness of digital twin services dramatically and the perceived user service experience. In this paper, we investigate delay-sensitive query service provisioning in digital twin-empowered serverless edge computing by considering user mobility. With digital twins of users deployed in the remote cloud, referred to as primary digital twins, we deploy their digital twin replicas based on serverless functions in cloudlets to mitigate the query service delay while enhancing user service satisfaction that is expressed as a utility function. We study two optimization problems with the aim of maximizing the accumulative utility gain: the digital twin replica placement problem per time slot, and the dynamic digital twin replica placement problem over a finite time horizon. We first formulate an Integer Linear Program (ILP) solution for the digital twin replica placement problem when the problem size is small; otherwise, we propose an approximation algorithm for the problem with a provable approximation ratio. We then design an online algorithm for the dynamic digital twin replica placement problem, and a performance-guaranteed online algorithm for a special case of the problem by assuming each user issues a query at each time slot. Finally, we evaluate the performance of the proposed algorithms for placing digital twin replicas in MEC networks through simulations. The results demonstrate the proposed algorithms are promising, outperforming their counterparts.},
  archive      = {J_TC},
  author       = {Jing Li and Song Guo and Weifa Liang and Jianping Wang and Quan Chen and Wenchao Xu and Kang Wei and Xiaohua Jia},
  doi          = {10.1109/TC.2024.3388897},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1837-1851},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Mobility-aware utility maximization in digital twin-enabled serverless edge computing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ara2: Exploring single- and multi-core vector processing
with an efficient RVV 1.0 compliant open-source processor. <em>TC</em>,
<em>73</em>(7), 1822–1836. (<a
href="https://doi.org/10.1109/TC.2024.3388896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector processing is highly effective in boosting processor performance and efficiency for data-parallel workloads. In this paper, we present Ara2, the first fully open-source vector processor to support the RISC-V V 1.0 frozen ISA. We evaluate Ara2&#39;s performance on a diverse set of data-parallel kernels for various problem sizes and vector-unit configurations, achieving an average functional-unit utilization of 95% on the most computationally intensive kernels. We pinpoint performance boosters and bottlenecks, including the scalar core, memories, and vector architecture, providing insights into the main vector architecture&#39;s performance drivers. Leveraging the openness of the design, we implement Ara2 in a 22nm technology, characterize its PPA metrics on various configurations (2-16 lanes), and analyze its microarchitecture and implementation bottlenecks. Ara2 achieves a state-of-the-art energy efficiency of 37.8 DP-GFLOPS/W (0.8V) and 1.35GHz of clock frequency (critical path: $\sim$ 40 FO4 gates). Finally, we explore the performance and energy-efficiency trade-offs of multi-core vector processors: we find that multiple vector cores help overcome the scalar core issue-rate bound that limits short-vector performance. For example, a cluster of eight 2-lane Ara2 (16 FPUs) achieves more than 3x better performance than a 16-lane single-core Ara2 (16 FPUs) when executing a 32x32x32 matrix multiplication, with 1.5x improved energy efficiency.},
  archive      = {J_TC},
  author       = {Matteo Perotti and Matheus Cavalcante and Renzo Andri and Lukas Cavigelli and Luca Benini},
  doi          = {10.1109/TC.2024.3388896},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1822-1836},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Ara2: Exploring single- and multi-core vector processing with an efficient RVV 1.0 compliant open-source processor},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LogSay: An efficient comprehension system for log numerical
reasoning. <em>TC</em>, <em>73</em>(7), 1809–1821. (<a
href="https://doi.org/10.1109/TC.2024.3386068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of smart systems and applications, high volume logs are generated that record important data for system maintenance. System developers are usually required to analyze logs to track the status of the system or applications. Therefore, it is essential to find the answers in large-scale logs when they have some questions. In this work, we design a multi-step “Retriever-Reader” question-answering system, namely LogSay, which aims at predicting answers accurately and efficiently. Our system can not only answers simple questions, such as a segment log or span, but also can answer complex logical questions through numerical reasoning. LogSay has two key components: Log Retriever and Log Reasoner , and we designed five operators to implement them. Log Retriever aims at retrieving some relevant logs based on a question. Then, Log Reasoner performs numerical reasoning to infer the final answer. In addition, due to the lack of available question-answering datasets for system logs, we constructed question-answering datasets based on three public log datasets and will make them publicly available. Our evaluation results show that LogSay outperforms the state-of-the-art works in terms of accuracy and efficiency.},
  archive      = {J_TC},
  author       = {Jiaxing Qi and Zhongzhi Luan and Shaohan Huang and Carol Fung and Hailong Yang},
  doi          = {10.1109/TC.2024.3386068},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1809-1821},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LogSay: An efficient comprehension system for log numerical reasoning},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Error-detection schemes for analog content-addressable
memories. <em>TC</em>, <em>73</em>(7), 1795–1808. (<a
href="https://doi.org/10.1109/TC.2024.3386065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analog content-addressable memories (in short, a-CAMs) have been recently introduced as accelerators for machine-learning tasks, such as tree-based inference or implementation of nonlinear activation functions. The cells in these memories contain nanoscale memristive devices, which may be susceptible to various types of errors, such as manufacturing defects, inaccurate programming of the cells, or drifts in their contents over time. The objective of this work is to develop techniques for overcoming the reliability issues that are caused by such error events. To this end, several coding schemes are presented for the detection of errors in a-CAMs. These schemes consist of an encoding stage, a detection cycle (which is performed periodically), and some minor additions to the hardware. During encoding, redundancy symbols are programmed into a portion of the a-CAM (or, alternatively, are written into an external memory). During each detection cycle, a certain set of input vectors is applied to the a-CAM. The schemes differ in several ways, e.g., in the range of alphabet sizes that they are most suitable for, in the tradeoff that each provides between redundancy and hardware additions, or in the type of errors that they handle (Hamming metric versus $L_{1}$ metric).},
  archive      = {J_TC},
  author       = {Ron M. Roth},
  doi          = {10.1109/TC.2024.3386065},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1795-1808},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Error-detection schemes for analog content-addressable memories},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incendio: Priority-based scheduling for alleviating cold
start in serverless computing. <em>TC</em>, <em>73</em>(7), 1780–1794.
(<a href="https://doi.org/10.1109/TC.2024.3386063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In serverless computing, cold start results in long response latency. Existing approaches strive to alleviate the issue by reducing the number of cold starts. However, our measurement based on real-world production traces shows that the minimum number of cold starts does not equate to the minimum response latency, and solely focusing on optimizing the number of cold starts will lead to sub-optimal performance. The root cause is that functions have different priorities in terms of latency benefits by transferring a cold start to a warm start. In this paper, we propose Incendio , a serverless computing framework exploiting priority-based scheduling to minimize the overall response latency from the perspective of cloud providers. We reveal the priority of a function is correlated to multiple factors and design a priority model based on Spearman&#39;s rank correlation coefficient. We integrate a hybrid Prophet-LightGBM prediction model to dynamically manage runtime pools, which enables the system to prewarm containers in advance and terminate containers at the appropriate time. Furthermore, to satisfy the low-cost and high-accuracy requirements in serverless computing, we propose a Clustered Reinforcement Learning-based function scheduling strategy. The evaluations show that Incendio speeds up the native system by 1.4 $\times$ , and achieves 23% and 14.8% latency reductions compared to two state-of-the-art approaches.},
  archive      = {J_TC},
  author       = {Xinquan Cai and Qianlong Sang and Chuang Hu and Yili Gong and Kun Suo and Xiaobo Zhou and Dazhao Cheng},
  doi          = {10.1109/TC.2024.3386063},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1780-1794},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Incendio: Priority-based scheduling for alleviating cold start in serverless computing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design, implementation and evaluation of a new variable
latency integer division scheme. <em>TC</em>, <em>73</em>(7), 1767–1779.
(<a href="https://doi.org/10.1109/TC.2024.3386060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integer division is key for various applications and often represents the performance bottleneck due to its inherent mathematical properties that limit its parallelization. This paper presents a new data-dependent variable latency division algorithm derived from the classic non-performing restoring method. The proposed technique exploits the relationship between the number of leading zeros in the divisor and in the partial remainder to dynamically detect and skip those iterations that result in a simple left shift. While a similar principle has been exploited in previous works, the proposed approach outperforms existing variable latency divider schemes in average latency and power consumption. We detail the algorithm and its implementation in four variants, offering versatility for the specific application requirements. For each variant, we report the average latency evaluated with different benchmarks, and we analyze the synthesis results for both FPGA and ASIC deployment, reporting clock speed, average execution time, hardware resources, and energy consumption, compared with existing fixed and variable latency dividers.},
  archive      = {J_TC},
  author       = {Marco Angioli and Marcello Barbirotta and Abdallah Cheikh and Antonio Mastrandrea and Francesco Menichelli and Saeid Jamili and Mauro Olivieri},
  doi          = {10.1109/TC.2024.3386060},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1767-1779},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Design, implementation and evaluation of a new variable latency integer division scheme},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis and mitigation of shared resource contention on
heterogeneous multicore: An industrial case study. <em>TC</em>,
<em>73</em>(7), 1753–1766. (<a
href="https://doi.org/10.1109/TC.2024.3386059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a solution to the industrial challenge put forth by ARM in 2022. We systematically analyze the effect of shared resource contention to an augmented reality head-up display (AR-HUD) case-study application of the industrial challenge on a heterogeneous multicore platform, NVIDIA Jetson Nano. We configure the AR-HUD application such that it can process incoming image frames in real-time at 20Hz on the platform. We use Microarchitectural Denial-of-Service (DoS) attacks as aggressor workloads of the challenge and show that they can dramatically impact the latency and accuracy of the AR-HUD application. This results in significant deviations of the estimated trajectories from known ground truths, despite our best effort to mitigate their influence by using cache partitioning and real-time scheduling of the AR-HUD application. To address the challenge, we propose RT-Gang++, a partitioned real-time gang scheduling framework with last-level cache (LLC) and integrated GPU bandwidth throttling capabilities. By applying RT-Gang++, we are able to achieve desired level of performance of the AR-HUD application even in the presence of fully loaded aggressor tasks.},
  archive      = {J_TC},
  author       = {Michael Bechtel and Heechul Yun},
  doi          = {10.1109/TC.2024.3386059},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1753-1766},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Analysis and mitigation of shared resource contention on heterogeneous multicore: An industrial case study},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LAC: A workload intensity-aware caching scheme for
high-performance SSDs. <em>TC</em>, <em>73</em>(7), 1738–1752. (<a
href="https://doi.org/10.1109/TC.2024.3385290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inside an NAND Flash-based solid-state disk (SSD), utilizing DRAM-based write-back caching is a practical approach to bolstering the SSD performance. Existing caching schemes overlook the problem of high user I/Os intensity due to the dramatic increment of I/Os accesses. The hefty I/O intensity causes access conflict of I/O requests inside an SSD: a large number of requests are blocked to impair response time. Conventional passive update caching schemes merely replace pages upon access misses in event of full cache. Tail latency occurs facing a colossal I/O intensity. Active write-back caching schemes utilize idle time among requests coupled with free internal bandwidth to flush dirty data into flash memory in advance, lowering response time. Frequent active write-back operations, however, cause access conflict of requests – a culprit that expands write amplification (WA) and degrades SSD lifetime. We address the above issues by proposing a workL oad intensity-aware and A ctive parallel Caching scheme - LAC - that is powered by collaborative-load awareness. LAC fends off user I/Os’ access conflict under high-I/O-intensity workloads. If the I/O intensity is low – intervals between consecutive I/O requests are large – and the target die is free, LAC actively and concurrently writes dirty data of adjacent addresses back to the die, cultivating clean data generated by the active write-back. Replacing clean data in priority can reduce response time and prevent flash transactions from being blocked. We devise a data protection method to write back cold data based on various criteria in the cache replacement and active write-backs. Thus, LAC reduces WA incurred by actively writing back hot data and extends SSD lifetime. We compare LAC against the six caching schemes (LRU, CFLRU, GCaR-LRU, MQSim, VS-Batch, and Co-Active) in the modern MQSim simulator. The results unveil that LAC trims response time and erase count by up to 78.5% and 47.8%, with an average of 64.4% and 16.6%, respectively.},
  archive      = {J_TC},
  author       = {Hui Sun and Haoqiang Tong and Yinliang Yue and Xiao Qin},
  doi          = {10.1109/TC.2024.3385290},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1738-1752},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LAC: A workload intensity-aware caching scheme for high-performance SSDs},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COALA: A compiler-assisted adaptive library routines
allocation framework for heterogeneous systems. <em>TC</em>,
<em>73</em>(7), 1724–1737. (<a
href="https://doi.org/10.1109/TC.2024.3385269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experienced developers often leverage well-tuned libraries and allocate their routines for computing tasks to enhance performance when building modern scientific and engineering applications. However, such well-tuned libraries are meticulously customized for specific target architectures or environments. Additionally, the performance of their routines is significantly impacted by the actual input data of computing tasks, which often remains uncertain until runtime. Accordingly, statically allocating these library routines may hinder the adaptability of applications and compromise performance, particularly in the context of heterogeneous systems. To address this issue, we propose the Compiler-Assisted Adaptive Library Routines Allocation (COALA) framework for heterogeneous systems. COALA is a fully automated mechanism that employs compiler assistance for dynamic allocation of the most suitable routine to each computing task on heterogeneous systems. It allows the deployment of varying allocation policies tailored to specific optimization targets. During the application compilation process, COALA reconstructs computing tasks and inserts a probe for each of these tasks. Probes serve the purpose of conveying vital information about the requirements of each task, including its computing objective, data size, and computing flops, to a user-level allocation component at runtime. Subsequently, the allocation component utilizes the probe information along with the allocation policy to assign the most optimal library routine for executing the computing tasks. In our prototype, we further introduce and deploy a performance-oriented allocation policy founded on a machine learning-based performance evaluation method for library routines. Experimental verification and evaluation on two heterogeneous systems reveal that COALA can significantly improve application performance, with gains of up to 4.3x for numerical simulation software and 4.2x for machine learning applications, and enhance system utilization by up to 27.8%.},
  archive      = {J_TC},
  author       = {Qinyun Cai and Guanghua Tan and Wangdong Yang and Xianhao He and Yuwei Yan and Keqin Li and Kenli Li},
  doi          = {10.1109/TC.2024.3385269},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1724-1737},
  shortjournal = {IEEE Trans. Comput.},
  title        = {COALA: A compiler-assisted adaptive library routines allocation framework for heterogeneous systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VKernel: Enhancing container isolation via private code and
data. <em>TC</em>, <em>73</em>(7), 1711–1723. (<a
href="https://doi.org/10.1109/TC.2024.3383988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container technology is increasingly adopted in cloud environments. However, the lack of isolation in the shared kernel becomes a significant barrier to the wide adoption of containers. The challenges lie in how to simultaneously attain high performance and isolation. On the one hand, kernel-level isolation mechanisms, such as seccomp , capabilities , and apparmor , achieve good performance without much overhead, but lack the support for per-container customization. On the other hand, user-level and VM-based isolation offer superior security guarantees and allow for customization, since a container is assigned a dedicated kernel, but at the cost of high overhead. We present vKernel, a kernel isolation framework. It maintains a minimal set of code and data that are either sensitive or prone to interference in a vKernel Instance (vKI). vKernel relies on inline hooks to intercept and redirect requests sent to the host kernel to a vKI, where container-specific security rules, functions, and data are implemented. Through case studies, we demonstrate that under vKernel user-defined data isolation and kernel customization can be supported with a reasonable engineering effort. An evaluation of vKernel with micro-benchmarks, cloud services, real-world applications show that vKernel achieves good security guarantees, but with much less overhead.},
  archive      = {J_TC},
  author       = {Hang Huang and Honglei Wang and Jia Rao and Song Wu and Hao Fan and Chen Yu and Hai Jin and Kun Suo and Lisong Pan},
  doi          = {10.1109/TC.2024.3383988},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1711-1723},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VKernel: Enhancing container isolation via private code and data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-grained trace collection, analysis, and management of
diverse container images. <em>TC</em>, <em>73</em>(7), 1698–1710. (<a
href="https://doi.org/10.1109/TC.2024.3383966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container technology is getting popular in cloud environments due to its lightweight feature and convenient deployment. The container registry plays a critical role in container-based clouds, as many container startups involve downloading layer-structured container images from a container registry. However, the container registry is struggling to efficiently manage images (i.e., transfer and store) with the emergence of diverse services and new image formats. The reason is that the container registry manages images uniformly at layer granularity. On the one hand, such uniform layer-level management probably cannot fit the various requirements of different kinds of containerized services well. On the other hand, new image formats organizing data in blocks or files cannot benefit from such uniform layer-level image management. In this paper, we perform the first analysis of image traces at multiple granularities (i.e., image-, layer-, and file-level) for various services and provide an in-depth comparison of different image formats. The traces are collected from a production-level container registry, amounting to 24 million requests and involving more than 184 TB of transferred data. We provide a number of valuable insights, including request patterns of services, file-level access patterns, and bottlenecks associated with different image formats. Based on these insights, we also propose two optimizations to improve image transfer and application deployment.},
  archive      = {J_TC},
  author       = {Zhuo Huang and Qi Zhang and Hao Fan and Song Wu and Chen Yu and Hai Jin and Jun Deng and Jing Gu and Zhimin Tang},
  doi          = {10.1109/TC.2024.3383966},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1698-1710},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-grained trace collection, analysis, and management of diverse container images},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Xvpfloat: RISC-v ISA extension for variable extended
precision floating point computation. <em>TC</em>, <em>73</em>(7),
1683–1697. (<a href="https://doi.org/10.1109/TC.2024.3383964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key concern in the field of scientific computation is the convergence of numerical solvers when applied to large problems. The numerical workarounds used to improve convergence are often problem specific, time consuming and require skilled numerical analysts. An alternative is to simply increase the working precision of the computation, but this is difficult due to the lack of efficient hardware support for extended precision. We propose Xvpfloat , a RISC-V ISA extension for dynamically variable and extended precision computation, a hardware implementation and a full software stack. Our architecture provides a comprehensive implementation of this ISA, with up to 512 bits of significand, including full support for common rounding modes and heterogeneous precision arithmetic operations. The memory subsystem handles IEEE 754 extendable formats, and features specialized indexed loads and stores with hardware-assisted prefetching. This processor can either operate standalone or as an accelerator for a general purpose host. We demonstrate that the number of solver iterations can be reduced up to $5\boldsymbol{\times}$ and, for certain, difficult problems, convergence is only possible with very high precision ( $\boldsymbol{\geq}$ 384 bits). This accelerator provides a new approach to accelerate large scale scientific computing.},
  archive      = {J_TC},
  author       = {Eric Guthmuller and César Fuguet and Andrea Bocco and Jérôme Fereyre and Riccardo Alidori and Ihsane Tahir and Yves Durand},
  doi          = {10.1109/TC.2024.3383964},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1683-1697},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Xvpfloat: RISC-V ISA extension for variable extended precision floating point computation},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic adaptive framework for practical byzantine fault
tolerance consensus protocol in the internet of things. <em>TC</em>,
<em>73</em>(7), 1669–1682. (<a
href="https://doi.org/10.1109/TC.2024.3377921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Practical Byzantine Fault Tolerance (PBFT) protocol-supported blockchain can provide decentralized security and trust mechanisms for the Internet of Things (IoT). However, the PBFT protocol is not specifically designed for IoT applications. Consequently, adapting PBFT to the dynamic changes of an IoT environment with incomplete information represents a challenge that urgently needs to be addressed. To this end, we introduce DA-PBFT, a PBFT dynamic adaptive framework based on a multi-agent architecture. DA-PBFT divides the dynamic adaptive process into two sub-processes: optimality-seeking and optimization decision-making. During the optimality-seeking process, a PBFT optimization model is constructed based on deep reinforcement learning. This model is designed to generate PBFT optimization strategies for consensus nodes. In the optimization decision-making process, a PBFT optimization decision consensus mechanism is constructed based on the Borda count method. This mechanism ensures consistency in PBFT optimization decisions within an environment characterized by incomplete information. Furthermore, we designed a dynamic adaptive incentive mechanism to explore the Nash equilibrium conditions and security aspects of DA-PBFT. The experimental results demonstrate that DA-PBFT is capable of achieving consistency in PBFT optimization decisions within an environment of incomplete information, thereby offering robust and efficient transaction throughput for IoT applications.},
  archive      = {J_TC},
  author       = {Chunpei Li and Wangjie Qiu and Xianxian Li and Chen Liu and Zhiming Zheng},
  doi          = {10.1109/TC.2024.3377921},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1669-1682},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A dynamic adaptive framework for practical byzantine fault tolerance consensus protocol in the internet of things},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Un-IOV: Achieving bare-metal level i/o virtualization
performance for cloud usage with migratability, scalability and
transparency. <em>TC</em>, <em>73</em>(7), 1655–1668. (<a
href="https://doi.org/10.1109/TC.2024.3375589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I/O virtualization is utilized by cloud platforms to provide tenants with efficient, scalable, and manageable network and storage services. The de-facto industrial standard, paravirtualization, offers rich cloud functionality by introducing split front-end and back-end drivers in the guest and host operating systems, respectively. Given this fact, paravirtualization incurs host inefficiency and performance overhead. Thus, emerging hardware virtio accelerators (i.e., SRIOV-capable devices that conform to virtio specification) with device passthrough technologies mitigate the performance issue. However, adopting these devices presents the challenge of insufficient support for live migration. This paper proposes Un-IOV, a novel I/O virtualization system that simultaneously achieves bare-metal level I/O performance and migratability. The key idea is to develop a new hybrid virtualization stack with: (1) a host-bypassed direct data path for virtio accelerators, and (2) a relayed control path guaranteeing seamless live migration support. Un-IOV achieves high scalability by consuming minimum host resources. Extensive experiment results demonstrate that Un-IOV achieves superior network and storage virtualization performance than software implementations with comparable performance of direct passthrough I/O virtualization, while imposing zero guest modification (i.e., guest transparency).},
  archive      = {J_TC},
  author       = {Zongpu Zhang and Chenbo Xia and Cunming Liang and Jian Li and Chen Yu and Tiwei Bie and Roberts Martin and Daly Dan and Xiao Wang and Yong Liu and Haibing Guan},
  doi          = {10.1109/TC.2024.3375589},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1655-1668},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Un-IOV: Achieving bare-metal level I/O virtualization performance for cloud usage with migratability, scalability and transparency},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based generation of hardware/software architectures
with hybrid schedulers for robotics systems. <em>TC</em>,
<em>73</em>(7), 1640–1654. (<a
href="https://doi.org/10.1109/TC.2023.3323804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic systems compute data from multiple sensors to perform several actions (e.g., path planning, object detection). FPGA-based architectures for such systems may consist of several accelerators to process compute-intensive algorithms. Designing and implementing such complex systems tends to be an arduous task. This work proposes a modeling approach to generate architectures for such applications, compliant with existing robotics middlewares (e.g., ROS, ROS2). The challenge is to have a compact, yet expressive description of the system with just enough information to generate all required components and to integrate existing algorithms. The system model must be application-independent and leverage FPGA advantages, such as concurrency, energy efficiency, and acceleration due to custom designs, surpassing software-based solutions. Previous work mainly focused on individual accelerators rather than all components involved in a system and their interactions. The proposed approach exploits the advantages of model-driven engineering and model-based code generation to produce all components, i.e., message converters as middleware interfaces and wrappers to integrate algorithms. Data type and data flow analysis are performed to derive the necessary information to generate the components and their connections. Six different schedulers are proposed to cover multiple scenarios. Solutions to several identified challenges for generating entire systems from such models are evaluated using four different use cases.},
  archive      = {J_TC},
  author       = {Ariel Podlubne and Johannes Mey and Andreas Andreou and Sergio Pertuz and Uwe Aßmann and Diana Göhringer},
  doi          = {10.1109/TC.2023.3323804},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {1640-1654},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Model-based generation of Hardware/Software architectures with hybrid schedulers for robotics systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved fault analysis on subterranean 2.0. <em>TC</em>,
<em>73</em>(6), 1631–1639. (<a
href="https://doi.org/10.1109/TC.2024.3371784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subterranean 2.0, a NIST second round lightweight cryptographic primitive, was introduced by Daemen et al. in 2020. It has three modes of operation: Subterranean-SAE, Subterranean- deck , and Subterranean-XOF. So far, most of the existing practical-time implementable attacks on Subterranean-SAE fall under the nonce misuse setting scenario. In this paper, we present significantly improved Differential Fault Analysis on Subterranean-SAE and Subterranean- deck . We consider a more challenging framework of unknown fault injection round, and achieve improved execution time as well as data complexity over the best known fault attack available in the literature. We utilize deep neural networks and also correlation coefficient for generation of signatures and matching them. Two general frameworks are proposed for fault location identification assuming that fault injection round is unknown. Finally, we use a $SAT$ solver to efficiently recover the embedded encryption key with no more than $\mathbf{5}$ distinct faults. Experimental results reveal that the total time (online phase) required to mount the attack on Subterranean-SAE (Subterranean- deck ) is 1234.6 (1334.6) seconds.},
  archive      = {J_TC},
  author       = {Sandip Kumar Mondal and Prakash Dey and Himadry Sekhar Roy and Avishek Adhikari and Subhamoy Maitra},
  doi          = {10.1109/TC.2024.3371784},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1631-1639},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improved fault analysis on subterranean 2.0},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ElasticDNN: On-device neural network remodeling for adapting
evolving vision domains at edge. <em>TC</em>, <em>73</em>(6), 1616–1630.
(<a href="https://doi.org/10.1109/TC.2024.3375608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Executing deep neural networks (DNN) based vision tasks on edge devices encounters challenging scenarios of significant and continually evolving data domains (e.g. background or subpopulation shift). With limited resources, the state-of-the-art domain adaptation (DA) methods either cause high training overheads on large DNN models, or incur significant accuracy losses when adapting small/compressed models in an online fashion. The inefficient resource scheduling among multiple applications further degrades their overall model accuracy. In this paper, we present ElasticDNN, a framework that enables online DNN remodeling for applications encountering evolving domain drifts at edge. Its first key component is the master-surrogate DNN models, which can dynamically generate a small surrogate DNN by retaining and training the large master DNN&#39;s most relevant regions pertinent to the new domain. The second novelty of ElasticDNN is the filter-grained resource scheduling, which allocates GPU resources based on online accuracy estimation and DNN remodeling of co-running applications. We fully implement ElasticDNN and demonstrate its effectiveness through extensive experiments. The results show that, compared to existing online DA methods using the same model sizes, ElasticDNN improves accuracy by 23.31% and reduces adaption time by 35.67x. In the more challenging multi-application scenario, ElasticDNN improves accuracy by an average of 25.91%.},
  archive      = {J_TC},
  author       = {Qinglong Zhang and Rui Han and Chi Harold Liu and Guoren Wang and Lydia Y. Chen},
  doi          = {10.1109/TC.2024.3375608},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1616-1630},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ElasticDNN: On-device neural network remodeling for adapting evolving vision domains at edge},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized task offloading in edge computing: An
offline-to-online reinforcement learning approach. <em>TC</em>,
<em>73</em>(6), 1603–1615. (<a
href="https://doi.org/10.1109/TC.2024.3377912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized task offloading among cooperative edge nodes has been a promising solution to enhance resource utilization and improve users’ Quality of Experience (QoE) in edge computing. However, current decentralized methods, such as heuristics and game theory-based methods, either optimize greedily or depend on rigid assumptions, failing to adapt to the dynamic edge environment. Existing DRL-based approaches train the model in a simulation and then apply it in practical systems. These methods may perform poorly because of the divergence between the practical system and the simulated environment. Other methods that train and deploy the model directly in real-world systems face a cold-start problem, which will reduce the users’ QoE before the model converges. This paper proposes a novel offline-to-online DRL called (O2O-DRL). It uses the heuristic task logs to warm-start the DRL model offline. However, offline and online data have different distributions, so using offline methods for online fine-tuning will ruin the policy learned offline. To avoid this problem, we use on-policy DRL to fine-tune the model and prevent value overestimation. We evaluate O2O-DRL with other approaches in a simulation and a Kubernetes-based testbed. The performance results show that O2O-DRL outperforms other methods and solves the cold-start problem.},
  archive      = {J_TC},
  author       = {Hongcai Lin and Lei Yang and Hao Guo and Jiannong Cao},
  doi          = {10.1109/TC.2024.3377912},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1603-1615},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Decentralized task offloading in edge computing: An offline-to-online reinforcement learning approach},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaptMD: Balancing space and performance in NUMA
architectures with adaptive memory deduplication. <em>TC</em>,
<em>73</em>(6), 1588–1602. (<a
href="https://doi.org/10.1109/TC.2024.3375592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory deduplication effectively relieves the memory space bottleneck by removing duplicate pages, especially in virtualized systems in which virtual machines run the same OS and similar applications. However, due to the non-uniform access latencies in NUMA architectures, memory deduplication poses a trade-off between memory savings and access performance: global deduplication across NUMA nodes realizes high memory savings, but leads to frequent cross-node remote access after deduplication and results in performance degradations. In contrast, local deduplication avoids remote access, but limits deduplication effectiveness. We design AdaptMD, an adaptive memory deduplication system that addresses the space-performance trade-off in NUMA architectures. AdaptMD leverages hotness awareness to globally deduplicate only cold pages to reduce remote access. It also migrates similar applications to the same NUMA node to allow local deduplication without remote access. We further make AdaptMD readily configurable to address various deployment scenarios. Experiments show that AdaptMD achieves high memory savings as in global deduplication, while achieving similar access performance as in local deduplication.},
  archive      = {J_TC},
  author       = {Lulu Yao and Yongkun Li and Patrick P. C. Lee and Xiaoyang Wang and Yinlong Xu},
  doi          = {10.1109/TC.2024.3375592},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1588-1602},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AdaptMD: Balancing space and performance in NUMA architectures with adaptive memory deduplication},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HPDK: A hybrid PM-DRAM key-value store for high i/o
throughput. <em>TC</em>, <em>73</em>(6), 1575–1587. (<a
href="https://doi.org/10.1109/TC.2024.3377914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the design of an architecture that replaces Disk with Persistent Memory (PM) to achieve the highest I/O throughput in Log-Structured Merge Tree (LSM-Tree) based key-value stores (KVS). Most existing LSM-Tree based KVSs use PM as an intermediate or smoothing layer, which fails to fully exploit PM&#39;s unique advantages to maximize I/O throughput. However, due to PM&#39;s distinct characteristics, such as byte addressability and short erasure time, simply replacing existing storage with PM does not yield optimal I/O performance. Furthermore, LSM-Tree based KVSs often face slow read performance. To tackle these challenges, this paper presents HPDK, a hybrid PM-DRAM KVS that combines level compression for LSM-Trees in PM with a B ${}^{+}$ -tree based in-memory search index in DRAM, resulting in high write and read throughput. HPDK also employs a key-value separation design and a live-item rate-based dynamic merge method to reduce the volume of PM writes. We implement and evaluate HPDK using a real PM drive, and our extensive experiments show that HPDK provides 1.25-11.8 and 1.47-36.4 times higher read and write throughput, respectively, compared to other state-of-the-art LSM-Tree based approaches.},
  archive      = {J_TC},
  author       = {Bihui Liu and Zhenyu Ye and Qiao Hu and Yupeng Hu and Yuchong Hu and Yang Xu and Keqin Li},
  doi          = {10.1109/TC.2024.3377914},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1575-1587},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HPDK: A hybrid PM-DRAM key-value store for high I/O throughput},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A computing-in-memory-based one-class hyperdimensional
computing model for outlier detection. <em>TC</em>, <em>73</em>(6),
1559–1574. (<a href="https://doi.org/10.1109/TC.2024.3371782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present ODHD , an algorithm for outlier detection based on hyperdimensional computing (HDC), a non-classical learning paradigm. Along with the HDC-based algorithm, we propose IM-ODHD , a computing-in-memory (CiM) implementation based on hardware/software (HW/SW) codesign for improved latency and energy efficiency. The training and testing phases of ODHD may be performed with conventional CPU/GPU hardware or our IM-ODHD , SRAM-based CiM architecture using the proposed HW/SW codesign techniques. We evaluate the performance of ODHD on six datasets from different application domains using three metrics, namely accuracy, F1 score, and ROC-AUC, and compare it with multiple baseline methods such as OCSVM, isolation forest, and autoencoder. The experimental results indicate that ODHD outperforms all the baseline methods in terms of these three metrics on every dataset for both CPU/GPU and CiM implementations. Furthermore, we perform an extensive design space exploration to demonstrate the tradeoff between delay, energy efficiency, and performance of ODHD . We demonstrate that the HW/SW codesign implementation of the outlier detection on IM-ODHD is able to outperform the GPU-based implementation of ODHD by at least 331.5 $\times$ /889 $\times$ in terms of training/testing latency (and on average 14.0 $\times$ /36.9 $\times$ in terms of training/testing energy consumption).},
  archive      = {J_TC},
  author       = {Ruixuan Wang and Sabrina Hassan Moon and Xiaobo Sharon Hu and Xun Jiao and Dayane Reis},
  doi          = {10.1109/TC.2024.3371782},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1559-1574},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A computing-in-memory-based one-class hyperdimensional computing model for outlier detection},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FPGA-accelerated range-limited molecular dynamics.
<em>TC</em>, <em>73</em>(6), 1544–1558. (<a
href="https://doi.org/10.1109/TC.2024.3375613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long timescale Molecular Dynamics (MD) simulation of small molecules is crucial in drug design and basic science. To accelerate a small data set that is executed for a large number of iterations, high-efficiency is required. Recent work in this domain has demonstrated that among COTS devices only FPGA-centric clusters can scale beyond a few processors. The problem addressed here is that, as the number of on-chip processors has increased from fewer than 10 into the hundreds, previous intra-chip routing solutions are no longer viable. We find, however, that through various design innovations, high efficiency can be maintained. These include replacing the previous broadcast networks with ring-routing and then augmenting the rings with out-of-order and caching mechanisms. Others are adding a level of hierarchical filtering and memory recycling. Two novel optimized architectures emerge, together with a number of variations. These are validated, analyzed, and evaluated. We find that in the domain of interest speed-ups over GPUs are achieved. The potential impact is that this system promises to be the basis for scalable long timescale MD with commodity clusters.},
  archive      = {J_TC},
  author       = {Chunshu Wu and Chen Yang and Sahan Bandara and Tong Geng and Anqi Guo and Pouya Haghi and Ang Li and Martin Herbordt},
  doi          = {10.1109/TC.2024.3375613},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1544-1558},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FPGA-accelerated range-limited molecular dynamics},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monotonicity of multi-term floating-point adders.
<em>TC</em>, <em>73</em>(6), 1531–1543. (<a
href="https://doi.org/10.1109/TC.2024.3371783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the literature on algorithms for computing multi-term addition $s_{n}=\sum_{i=1}^{n}x_{i}$ in floating-point arithmetic it is often shown that a hardware unit that has single normalization and rounding improves precision, area, latency, and power consumption, compared with the use of standard add or fused multiply–add units. However, non-monotonicity can appear when computing sums with a subclass of multi-term addition units, which is currently not explored in the literature. We prove that computing multi-term floating-point addition with $n\geq 4$ , without normalization of intermediate quantities, can result in non-monotonicity—increasing one of the addends $x_{i}$ decreases the sum $s_{n}$ . Summation is required in dot product and matrix multiplication operations, operations that are increasingly appearing in the hardware of high-performance computers, and knowing where monotonicity is preserved can be of interest to the developers and users. Non-monotonicity of summation in existent hardware devices that implement a specific class of multi-term adders may have appeared unintentionally as a consequence of design choices that reduce circuit area and other metrics. To demonstrate our findings we simulate non-monotonic multi-term adders in MATLAB using the CPFloat custom-precision floating-point simulator.},
  archive      = {J_TC},
  author       = {Mantas Mikaitis},
  doi          = {10.1109/TC.2024.3371783},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1531-1543},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Monotonicity of multi-term floating-point adders},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomize the running function when it is disclosed.
<em>TC</em>, <em>73</em>(6), 1516–1530. (<a
href="https://doi.org/10.1109/TC.2024.3371776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Address space layout randomization (ASLR) can hide code addresses, which has been widely adopted by security solutions. However, code probes can bypass it. In real attack scenarios, a single code probe can only obtain very limited code information instead of the information of the entire code segment. So, randomizing the entire code segment is unnecessary. How to minimize the size of the randomized object is a key to reducing the complexity and overhead for ASLR methods. Moreover, ASLR needs to be completed between the time after code probe occurs and before the probed code is used by attackers, otherwise it is meaningless. How to select an appropriate randomization time point is a basic condition for achieving effective address hiding. In this paper, we propose a runtime partial randomization method RandFun. It only randomizes the probed function with parallel threads. And the randomization is performed when and only when potential code probes are detected. In addition, RandFun can protect the probed code from being used as gadgets, whether during or after randomization. Experiments and analysis show RandFun has a good defense effect on code probes and only introduces 1.6% overhead to CPU.},
  archive      = {J_TC},
  author       = {YongGang Li and Yu Bao and Yeh-Ching Chung},
  doi          = {10.1109/TC.2024.3371776},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1516-1530},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Randomize the running function when it is disclosed},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UniSched: A unified scheduler for deep learning training
jobs with different user demands. <em>TC</em>, <em>73</em>(6),
1500–1515. (<a href="https://doi.org/10.1109/TC.2024.3371794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of deep learning training (DLT) jobs in modern GPU clusters calls for efficient deep learning (DL) scheduler designs. Due to the extensive applications of DL technology, developers may have different demands for their DLT jobs. It is important for a GPU cluster to support all these demands and efficiently execute those DLT jobs. Unfortunately, existing DL schedulers mainly focus on part of those demands, and cannot provide comprehensive scheduling services. In this work, we present UniSched , a unified scheduler to optimize different types of scheduling objectives (e.g., guaranteeing the deadlines of SLO jobs, minimizing the latency of best-effort jobs). Meanwhile, UniSched supports different job stopping criteria (e.g., iteration-based, performance-based). UniSched includes two key components: Estimator for estimating the job duration, and Selector for selecting jobs and allocating resources. We perform large-scale simulations over the job traces from the production clusters. Compared to state-of-the-art schedulers, UniSched can significantly decrease the deadline miss rate of SLO jobs by up to 6.84 $\times$ , and the latency of best-effort jobs by up to 4.02 $\times$ , To demonstrate the practicality of UniSched , we implement and deploy a prototype on Kubernetes in a physical cluster consisting of 64 GPUs.},
  archive      = {J_TC},
  author       = {Wei Gao and Zhisheng Ye and Peng Sun and Tianwei Zhang and Yonggang Wen},
  doi          = {10.1109/TC.2024.3371794},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1500-1515},
  shortjournal = {IEEE Trans. Comput.},
  title        = {UniSched: A unified scheduler for deep learning training jobs with different user demands},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reordering and compression for hypergraph processing.
<em>TC</em>, <em>73</em>(6), 1486–1499. (<a
href="https://doi.org/10.1109/TC.2024.3377915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraphs are applicable to various domains such as social contagion, online groups, and protein structures due to their effective modeling of multivariate relationships. However, the increasing size of hypergraphs has led to high computation costs, necessitating efficient acceleration strategies. Existing approaches often require consideration of algorithm-specific issues, making them difficult to directly apply to arbitrary hypergraph processing tasks. In this paper, we propose a compression-array acceleration strategy involving hypergraph reordering to improve memory access efficiency, which can be applied to various hypergraph processing tasks without considering the algorithm itself. We introduce a new metric called closeness to optimize the ordering of vertices and hyperedges in the one-dimensional array representation. Moreover, we present an $\frac{1}{2w}$ -approximation algorithm to obtain the optimal ordering of vertices and hyperedges. We also develop an efficient update mechanism for dynamic hypergraphs. Our extensive experiments demonstrate significant improvements in hypergraph processing performance, reduced cache misses, and reduced memory footprint. Furthermore, our method can be integrated into existing hypergraph processing frameworks, such as Hygra, to enhance their performance.},
  archive      = {J_TC},
  author       = {Yu Liu and Qi Luo and Mengbai Xiao and Dongxiao Yu and Huashan Chen and Xiuzhen Cheng},
  doi          = {10.1109/TC.2024.3377915},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1486-1499},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reordering and compression for hypergraph processing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Big-PERCIVAL: Exploring the native use of 64-bit posit
arithmetic in scientific computing. <em>TC</em>, <em>73</em>(6),
1472–1485. (<a href="https://doi.org/10.1109/TC.2024.3377890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy requirements in many scientific computing workloads result in the use of double-precision floating-point arithmetic in the execution kernels. Nevertheless, emerging real-number representations, such as posit arithmetic, show promise in delivering even higher accuracy in such computations. In this work, we explore the native use of 64-bit posits in a series of numerical benchmarks and compare their timing performance, accuracy and hardware cost to IEEE 754 doubles. In addition, we also study the conjugate gradient method for numerically solving systems of linear equations in real-world applications. For this, we extend the PERCIVAL RISC-V core and the Xposit custom RISC-V extension with posit64 and quire operations. Results show that posit64 can obtain up to 4 orders of magnitude lower mean square error than doubles. This leads to a reduction in the number of iterations required for convergence in some iterative solvers. However, leveraging the quire accumulator register can limit the order of some operations such as matrix multiplications. Furthermore, detailed FPGA and ASIC synthesis results highlight the significant hardware cost of 64-bit posit arithmetic and quire. Despite this, the large accuracy improvements achieved with the same memory bandwidth suggest that posit arithmetic may provide a potential alternative representation for scientific computing.},
  archive      = {J_TC},
  author       = {David Mallasén and Alberto A. Del Barrio and Manuel Prieto-Matias},
  doi          = {10.1109/TC.2024.3377890},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1472-1485},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Big-PERCIVAL: Exploring the native use of 64-bit posit arithmetic in scientific computing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prefender: A prefetching defender against cache side channel
attacks as a pretender. <em>TC</em>, <em>73</em>(6), 1457–1471. (<a
href="https://doi.org/10.1109/TC.2024.3377891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cache side channel attacks are increasingly alarming in modern processors due to the recent emergence of Spectre and Meltdown attacks. A typical attack performs intentional cache access and manipulates cache states to leak secrets by observing the victim&#39;s cache access patterns. Different countermeasures have been proposed to defend against both general and transient execution based attacks. Despite their effectiveness, they mostly trade some level of performance for security, or have restricted security scope. In this paper, we seek an approach to enforcing security while maintaining performance. We leverage the insight that attackers need to access cache in order to manipulate and observe cache state changes for information leakage. Specifically, we propose Prefender , a secure prefetcher that learns and predicts attack-related accesses for prefetching the cachelines to simultaneously help security and performance. Our results show that Prefender is effective against several cache side channel attacks while maintaining or even improving performance for SPEC CPU 2006 and 2017 benchmarks.},
  archive      = {J_TC},
  author       = {Luyi Li and Jiayi Huang and Lang Feng and Zhongfeng Wang},
  doi          = {10.1109/TC.2024.3377891},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1457-1471},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Prefender: A prefetching defender against cache side channel attacks as a pretender},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated FPGA accelerator for deep learning-based 2D/3D
path planning. <em>TC</em>, <em>73</em>(6), 1442–1456. (<a
href="https://doi.org/10.1109/TC.2024.3377895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning is a crucial component for realizing the autonomy of mobile robots. However, due to limited computational resources on mobile robots, it remains challenging to deploy state-of-the-art methods and achieve real-time performance. To address this, we propose P3Net (PointNet-based Path Planning Networks), a lightweight deep-learning-based method for 2D/3D path planning, and design an IP core (P3NetCore) targeting FPGA SoCs (Xilinx ZCU104). P3Net improves the algorithm and model architecture of the recently-proposed MPNet. P3Net employs an encoder with a PointNet backbone and a lightweight planning network in order to extract robust point cloud features and sample path points from a promising region. P3NetCore is comprised of the fully-pipelined point cloud encoder, batched bidirectional path planner, and parallel collision checker, to cover most part of the algorithm. On the 2D (3D) datasets, P3Net with the IP core runs 30.52–186.36x and 7.68–143.62x (15.69–93.26x and 5.30–45.27x) faster than ARM Cortex CPU and Nvidia Jetson while only consuming 0.255W (0.809W), and is up to 1278.14x (455.34x) power-efficient than the workstation. P3Net improves the success rate by up to 28.2% and plans a near-optimal path, leading to a significantly better tradeoff between computation and solution quality than MPNet and the state-of-the-art sampling-based methods.},
  archive      = {J_TC},
  author       = {Keisuke Sugiura and Hiroki Matsutani},
  doi          = {10.1109/TC.2024.3377895},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1442-1456},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An integrated FPGA accelerator for deep learning-based 2D/3D path planning},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAS: General-purpose in-memory-computing accelerator for
sparse matrix multiplication. <em>TC</em>, <em>73</em>(6), 1427–1441.
(<a href="https://doi.org/10.1109/TC.2024.3371790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix multiplication is widely used in various practical applications. Different accelerators have been proposed to speed up sparse matrix-dense vector multiplication (SpMV), sparse matrix-sparse vector multiplication (SpMSpV), sparse matrix-dense matrix multiplication (SpMM), and sparse matrix-sparse matrix multiplication (SpMSpM). The performance of traditional sparse matrix multiplication accelerators is typically bounded by memory access due to the poor data locality and irregular memory access. In-memory computing (IMC) is a promising technique to alleviate the memory bottleneck. Previous IMC studies are mostly focused on accelerating a single sparse matrix multiplication function. In this paper, we propose GAS, a general-purpose IMC accelerator for sparse matrix multiplication. GAS integrates non-volatile memory based content-addressable memory (CAM) arrays and multiply-add computation (MAC) arrays to support sparse matrices represented in the double-precision floating-point format. Using a unified outer product based multiplication methodology, GAS supports the acceleration of SpMV, SpMSpv, SpMM, and SpMSpM. We further propose four optimization techniques to speed up the computation of GAS. GAS achieves significant speedups and energy savings over central processing unit (CPU) and graphics processing unit (GPU) implementations. Compared with state-of-the-art traditional and IMC-based accelerators, GAS not only supports more functions, but also achieves higher performance and energy efficiency.},
  archive      = {J_TC},
  author       = {Xiaoyu Zhang and Zerun Li and Rui Liu and Xiaoming Chen and Yinhe Han},
  doi          = {10.1109/TC.2024.3371790},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1427-1441},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GAS: General-purpose in-memory-computing accelerator for sparse matrix multiplication},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient deadlock avoidance for 2-d mesh NoCs that use OQ
or VOQ routers. <em>TC</em>, <em>73</em>(5), 1414–1426. (<a
href="https://doi.org/10.1109/TC.2024.3365954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network-on-chips (NoCs) are currently a widely used approach for achieving scalability of multi-cores to many-cores, as well as for interconnecting other vital system-on-chip (SoC) components. Each entity in 2D mesh-based NoCs has a router responsible for forwarding packets between the dimensions as well as the entity itself, and it is essentially a 5-port switch. With respect to the routing algorithm, there are important trade-offs between routing performance and the efficiency of overcoming potential deadlocks. Common deadlock avoidance techniques including the turn model usually involve restrictions of certain paths a packet can take at the cost of a higher probability for network congestion. In contrast, deadlock resolution techniques, as well as some avoidance schemes, provide more path flexibility at the expense of hardware complexity, such as by incorporating (or assuming) dedicated buffers. This paper provides a deadlock avoidance algorithm for NoC routers based on output-queues (OQs) or virtual-output queues (VOQs), with a focus on their use on field-programmable gate-arrays (FPGAs). The proposed approach features fewer path restrictions than common techniques, and can be based on existing routing algorithms as a baseline, deadlock-free or not. This requires no modification to the queueing topology, and the required logic is minimal. Our algorithm approaches the performance of fully-adaptive algorithms, while maintaining deadlock freedom.},
  archive      = {J_TC},
  author       = {Philippos Papaphilippou and Thiem Van Chu},
  doi          = {10.1109/TC.2024.3365954},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1414-1426},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient deadlock avoidance for 2-D mesh NoCs that use OQ or VOQ routers},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-domain generalized predictor for neural architecture
search system. <em>TC</em>, <em>73</em>(5), 1400–1413. (<a
href="https://doi.org/10.1109/TC.2024.3365949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance predictors are used to reduce architecture evaluation costs in neural architecture search, which however suffers from a large amount of budget consumption in annotating substantial architectures trained from scratch. Hence, how to leverage existing annotated architectures to train a generalized predictor to find the optimal architecture on unseen target search spaces becomes a new research topic. To solve this issue, we propose a Single-Domain Generalized Predictor (SDGP), which aims to make the predictor only trained on a single source search space but perform well on target search spaces. In meta-learning, we firstly adopt feature extractor in learning the domain-invariant features of the architectures. Then, a neural predictor is trained to map the architectures to the accuracy of the candidate architectures over the target domain simulated on the source search space. Moreover, a novel multi-head attention driven regularizer is designed to regulate the predictor to further improve the generalization ability of the predictor for the feature extractor. A series of experimental results have shown that the proposed predictor outperforms the state-of-the-art predictors in generalization and achieves significant performance gains in finding the optimal architectures with test error 2.40% on CIFAR-10 and 23.20% on ImageNet1k within 0.01 GPU days.},
  archive      = {J_TC},
  author       = {Lianbo Ma and Haidong Kang and Guo Yu and Qing Li and Qiang He},
  doi          = {10.1109/TC.2024.3365949},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1400-1413},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Single-domain generalized predictor for neural architecture search system},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The design of a lossless deduplication scheme to eliminate
fine-grained redundancy for JPEG image storage systems. <em>TC</em>,
<em>73</em>(5), 1385–1399. (<a
href="https://doi.org/10.1109/TC.2024.3363456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image data storage has grown explosively, so image deduplication is used to save storage by eliminating redundancy between different images. However, traditional image deduplication cannot eliminate fine-grained redundancy nor guarantee lossless results. In this work, we propose imDedup, a lossless and fine-grained deduplication scheme for JPEG image storage systems. Specifically, imDedup uses a novel sampling hash method, Feature Bitmap, to detect similar images in a fast way by utilizing the information distribution of JPEG data. Meanwhile, it uses Idelta, a novel delta encoder that incorporates image compression into deduplication, to guarantee the non-redundant data can be re-compressed via image encoding and thus improves the compression ratio. Besides, we propose the DCHash and Fixed-Point Matching (FPM) techniques to further speed up Idelta. We also propose imDedup-plus, which dynamically chooses the DCHash-based or FPM-based compressor to achieve higher throughputs without sacrificing the compression ratio. Experimental results demonstrate the superiority of the imDedup-based methods on five datasets. Compared with the state-of-the-art similarity detector and delta encoder, imDedup achieves 1.8–4.4 $\boldsymbol{\times}$ higher throughputs and 1.3–1.7 $\boldsymbol{\times}$ higher compression ratios, respectively. Besides, imDedup-plus can further achieve 1.3–2.9 $\boldsymbol{\times}$ higher throughputs than imDedup without sacrificing the compression ratio.},
  archive      = {J_TC},
  author       = {Cai Deng and Xiangyu Zou and Qi Chen and Bo Tang and Wen Xia},
  doi          = {10.1109/TC.2024.3363456},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1385-1399},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The design of a lossless deduplication scheme to eliminate fine-grained redundancy for JPEG image storage systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing CNN computation using RISC-v custom instruction
sets for edge platforms. <em>TC</em>, <em>73</em>(5), 1371–1384. (<a
href="https://doi.org/10.1109/TC.2024.3362060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefit from the custom instruction extension capabilities, RISC-V architecture can be optimized for many domain-specific applications. In this paper, we propose seven RISC-V SIMD (single instruction multiple data) custom instructions that can significantly optimize the convolution, activation and pool operations in CNN inference computation. More specifically, instruction CONV23 can greatly speed up the operation of $F(2\times 2,3\times 3)$ . With the adoption of Winograd algorithm, the number of multiplications can be reduced from 36 to 16, and the execution time is also reduced from 140 to 21 clock cycles. These custom instructions can be executed in batch mode within the acceleration module where the immediate data can be reused, so the latency and energy overhead associated with excess memory accesses can be eliminated. Using inline assembler in C language, the custom instructions can be called and compiled together with C source code. A revised RISC-V processor, RI5CY-Accel is constructed on FPGA to accommodate these custom instructions. Revised LeNet-5, VGG16 and ResNet18 model; called LeNet-Accel, VGG16-Accel and ResNet18-Accel are also optimized based on RI5CY-Accel architecture. Benchmark experiments demonstrated that the inference of LeNet-Accel, VGG16-Accel and ResNet18-Accel based on RI5CY-Accel can greatly reduce the execution latency by over 76.6%, 88.8% and 87.1%, with the total energy consumption saving of 74.8%, 87.8% and 85.1% respectively.},
  archive      = {J_TC},
  author       = {Shihang Wang and Xingbo Wang and Zhiyuan Xu and Bingzhen Chen and Chenxi Feng and Qi Wang and Terry Tao Ye},
  doi          = {10.1109/TC.2024.3362060},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1371-1384},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing CNN computation using RISC-V custom instruction sets for edge platforms},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed program deployment for resource-aware
programmable switches. <em>TC</em>, <em>73</em>(5), 1357–1370. (<a
href="https://doi.org/10.1109/TC.2024.3355786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programmable switches allow data plane to program how packets are processed, which enables flexibility for network management tasks, e.g., packet scheduling and flow measurement. Existing studies focus on program deployment at a single switch, while deployment across the whole data plane is still a challenging issue, especially manifested in the difficulty in joint correct implementation of P4 programs, resource load balancing of network devices, and optimization of network performance. In this paper, we present RED, a Resource-Efficient and Distributed program deployment solution for programmable switches. First of all, we analyze data plane programs to estimate the resource utilization and divide them into two categories for further processing. Then, the proposed merging and splitting algorithms are selectively applied to merge or split the pending programs. Finally, we consolidate the scarce resources of the whole data plane for distributed program deployment. Extensive experiments with both testbed and large-scale simulations are conducted and comparison results show that 1) RED achieves network-wide resource balancing in a distributed way and the latency of processing packets within the switch was reduced by 16.7%. 2) RED improves the speedup by two orders of magnitude compared to P4Visor in merging program and merges more 18% tables than SPEED; 3) RED allows overwhelmed P4 programs to be deployed on multiple switches normally when their required resources exceed the limit of a single switch. RED makes the overwhelmed programs to be deployed on switches and switch throughput increased by 10.7%.},
  archive      = {J_TC},
  author       = {Fuliang Li and Songlin Chen and Xingxin Jia and Chengxi Gao and Pengfei Wang and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2024.3355786},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1357-1370},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Distributed program deployment for resource-aware programmable switches},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). <span class="math inline">𝒪(<em>n</em>)</span>O(n) key–value
sort with active compute memory. <em>TC</em>, <em>73</em>(5), 1341–1356.
(<a href="https://doi.org/10.1109/TC.2024.3371773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the Active Compute Memory (ACM), a near-memory-processing architecture capable of performing key–value sort directly in the DRAM. In the ACM architecture, sort is merely the writing of data into memory with one addressing protocol (perspective) and reading it back with different perspective. The first perspective is conventional, based on the data address; the second perspective is the sorted order. The ACM requires additional tables to store the meta-data and moderate control logic enhancements that can be implemented directly in the DRAM silicon. By these modest enhancements to DRAM, ACM exploits the parallelism inherently available in the row buffer to enable sort with $O(n)$ complexity. This leads to an order of magnitude improvement in ACM performance and energy compared to conventional $O(n\log{}n)$ CPU-centric sort algorithms. The ACM also shows superior performance compared to other near-memory sort accelerators. This is because the ACM processing is done near the row buffer and it exploits much lower memory access latency, higher bandwidth and wider parallel processing. The sort operation covered in this paper is just an example of an address management operation that can be efficiently implemented directly in the DRAM silicon. We release as an open source the simulation infrastructure for the ACM performance and energy modeling. We would encourage the community to use it, adapt it to other PIM proposals, and share their own evaluations.},
  archive      = {J_TC},
  author       = {Pouya Esmaili-Dokht and Miquel Guiot and Petar Radojković and Xavier Martorell and Eduard Ayguadé and Jesus Labarta and Jason Adlard and Paolo Amato and Marco Sforzin},
  doi          = {10.1109/TC.2024.3371773},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1341-1356},
  shortjournal = {IEEE Trans. Comput.},
  title        = {$\mathcal{O}(n)$O(n) Key–Value sort with active compute memory},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I/o causality based in-line data deduplication for
non-volatile memory enabled storage systems. <em>TC</em>,
<em>73</em>(5), 1327–1340. (<a
href="https://doi.org/10.1109/TC.2024.3365961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication technologies are widely exploited to reduce capacity demands for storage. Previous chunk-based offline deduplication technologies often cause serious performance overhead due to data chunking and indexing. Particularly, they are not efficient for non-volatile memory (NVM) based storage systems because they cannot fully exploit the byte-addressability feature of NVMs for fine-grained deduplication. In this paper, we propose I/O Causality based In-line Deduplication (ICID) to maximize the deduplication ratio for NVM-based storage systems. Unlike previous inline deduplication schemes that use hash indexes to identify duplicate data slices, ICID records memory-copy operations in a B-tree structure to achieve causality-based inline deduplication. We propose two novel techniques to manage memory-copy records in the B-tree efficiently. First, to speed up the B-tree lookup, we group memory-copy records targeted to the same page in a B-tree node to improve data locality. Second, we exploit the spatial locality of memory accesses to identify outdated memory-copy records, and delete them in time to reduce memory consumption of the B-tree. We evaluate ICID in a system equipped with Intel Optane DC Persistent Memory Modules. For a typical KV store–LevelDB, our experimental results show that ICID achieves up to 16 $\boldsymbol{\times}$ higher deduplication ratio and reduces the time cost of data deduplication by 47% on average compared with state-of-the-art deduplication schemes.},
  archive      = {J_TC},
  author       = {Haikun Liu and Xiaozhong Jin and Chencheng Ye and Xiaofei Liao and Hai Jin and Yu Zhang},
  doi          = {10.1109/TC.2024.3365961},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1327-1340},
  shortjournal = {IEEE Trans. Comput.},
  title        = {I/O causality based in-line data deduplication for non-volatile memory enabled storage systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NDSTRNG: Non-deterministic sampling-based true random number
generator on SoC FPGA systems. <em>TC</em>, <em>73</em>(5), 1313–1326.
(<a href="https://doi.org/10.1109/TC.2024.3365955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random number generation is essential for applications in simulation, numerical analysis, and data encryption. The ubiquitous presence of system-on-chip (SoC) field-programmable gate array (FPGA) embedded devices in critical sectors necessitates robust random number generators (RNGs) that operate within these specialized environments. Traditional RNGs in GNU/Linux systems derive entropy from peripheral hardware events, which are scarce in SoC FPGA platforms lacking standard PC peripherals. Addressing this challenge, this paper proposes a novel random number generator named NDSTRNG that leverages the unique hardware structure of the SoC FPGA and the inherent randomness of GNU/Linux. The proposed generator employs a non-deterministic sampling model to circumvent reliance on various peripherals while ensuring unbiased output via a linear feedback shift register (LFSR)-based post-processing method. We implement this random number generator in SoC FPGA GNU/Linux using minimal FPGA resources and only one Linux task for sampling. NDSTRNG achieved a throughput exceeding 700 Kbps. Moreover, the entropy source of the generator is evaluated using NIST SP 800-90B, while the quality of the generated random numbers is assessed through ENT, NIST SP 800-22, and DIEHARDER. The results confirm that NDSTRNG meets the stringent criteria for both high-quality and high-speed random number generation, making it suitable for deployment in communication, defense, and medical domains where reliable RNGs are indispensable.},
  archive      = {J_TC},
  author       = {Yucong Chen and Yanshan Tian and Rui Zhou and Diego Martínez Castro and Deke Guo and Qingguo Zhou},
  doi          = {10.1109/TC.2024.3365955},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1313-1326},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NDSTRNG: Non-deterministic sampling-based true random number generator on SoC FPGA systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BFT-DSN: A byzantine fault-tolerant decentralized storage
network. <em>TC</em>, <em>73</em>(5), 1300–1312. (<a
href="https://doi.org/10.1109/TC.2024.3365953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of blockchain and its applications, the amount of data stored on decentralized storage networks (DSNs) has grown exponentially. DSNs bring together affordable storage resources from around the world to provide robust, decentralized storage services for tens of thousands of decentralized applications (dApps). However, existing DSNs do not offer verifiability when implementing erasure coding for redundant storage, making them vulnerable to Byzantine encoders. Additionally, there is a lack of Byzantine fault-tolerant consensus for optimal resilience in DSNs. This paper introduces BFT-DSN, a Byzantine fault-tolerant decentralized storage network designed to address these challenges. BFT-DSN combines storage-weighted BFT consensus with erasure coding and incorporates homomorphic fingerprints and weighted threshold signatures for decentralized verification. The implementation of BFT-DSN demonstrates its comparable performance in terms of storage cost and latency as well as superior performance in Byzantine resilience when compared to existing industrial decentralized storage networks.},
  archive      = {J_TC},
  author       = {Hechuan Guo and Minghui Xu and Jiahao Zhang and Chunchi Liu and Rajiv Ranjan and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TC.2024.3365953},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1300-1312},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BFT-DSN: A byzantine fault-tolerant decentralized storage network},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Breaking the DECT standard cipher with lower time cost.
<em>TC</em>, <em>73</em>(5), 1290–1299. (<a
href="https://doi.org/10.1109/TC.2024.3365943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The DECT Standard Cipher (DSC) is a proprietary stream cipher used for encryption in the Digital Enhanced Cordless Telecommunications (DECT), which is a standard for short range cordless communication and widely deployed worldwide both in residential and enterprise environments. New weaknesses of the DSC stream cipher which are not discovered in previous works are explored and analyzed in this paper. Based on these weaknesses, new practical key recovery attacks and distinguishing attack on DSC with lower time cost are proposed. The first cryptanalytic result show that DSC can be broken in about 13.12 seconds in the known IV setting, when an offline phase that takes about 58.33 minutes is completed. After then, a distinguishing attack on DSC in the related key chosen IV setting is given, which has a time complexity of only 2 encryptions and a success probability of almost 1. Finally, based on the slide property, a key recovery attack on DSC with practical complexities is proposed. The experimental result shows that DSC can be broken on a common PC within about 44.97 seconds in the multiple related key setting. The attacks on DSC proposed in this paper clearly show that a well-designed initialization is absolutely necessary to design a secure stream cipher.},
  archive      = {J_TC},
  author       = {Lin Ding and Zhengting Li and Ziyu Guan and Xinhai Wang and Zheng Wu},
  doi          = {10.1109/TC.2024.3365943},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1290-1299},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Breaking the DECT standard cipher with lower time cost},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating sparse DNNs based on tiled GEMM. <em>TC</em>,
<em>73</em>(5), 1275–1289. (<a
href="https://doi.org/10.1109/TC.2024.3365942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pruning can reduce the computation cost of deep neural network (DNN) models. However, sparse models often produce randomly-distributed weights to maintain accuracy, leading to irregular computations. Consequently, unstructured sparse models cannot achieve meaningful speedup on commodity hardware built for dense matrix computations. Accelerators are usually modified or designed with structured sparsity-optimized architectures for exploiting sparsity. For example, the Ampere architecture introduces a sparse tensor core, which adopts the 2:4 sparsity pattern. We propose a pruning method that builds upon the insight that matrix multiplication generally breaks the large matrix into multiple smaller tiles for parallel execution. We present the “tile-wise” sparsity pattern, which maintains a structured sparsity pattern at the tile level for efficient execution but allows for irregular pruning at the global scale to maintain high accuracy. In addition, the tile-wise sparsity is implemented at the global memory level, and the 2:4 sparsity executes at the register level inside the sparse tensor core. We can combine these two patterns into a “tile-vector-wise” ( TVW ) sparsity pattern to explore more fine-grained sparsity and further accelerate the sparse DNN models. We evaluate the TVW on the GPU, achieving averages of 1.85 $\boldsymbol{\times}$ , 2.75 $\boldsymbol{\times}$ , and 22.18 $\boldsymbol{\times}$ speedups over the dense model, block sparsity, and unstructured sparsity.},
  archive      = {J_TC},
  author       = {Cong Guo and Fengchen Xue and Jingwen Leng and Yuxian Qiu and Yue Guan and Weihao Cui and Quan Chen and Minyi Guo},
  doi          = {10.1109/TC.2024.3365942},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1275-1289},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating sparse DNNs based on tiled GEMM},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A detailed historical and statistical analysis of the
influence of hardware artifacts on SPEC integer benchmark performance.
<em>TC</em>, <em>73</em>(5), 1262–1274. (<a
href="https://doi.org/10.1109/TC.2024.3365941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Standard Performance Evaluation Corporation (SPEC) CPU benchmark has been widely used as a measure of computing performance for decades. The SPEC is an industry-standardized, CPU-intensive benchmark suite and the collective data provide a proxy for the history of worldwide CPU and system performance. Past efforts have not provided or enabled answers to questions such as, how has the SPEC benchmark suite evolved empirically over time and what micro-architecture artifacts have had the most influence on performance?—have any micro-benchmarks within the suite had undue influence on the results and comparisons among the codes?—can the answers to these questions provide insights to the future of computer system performance? To answer these questions, we detail our historical and statistical analysis of specific hardware artifacts (clock frequencies, core counts, etc.) on the performance of the SPEC benchmarks since 1995. We discuss in detail several methods to normalize across benchmark evolutions. We perform both isolated and collective sensitivity analyses for various hardware artifacts and we identify one benchmark (libquantum) that had somewhat undue influence on performance outcomes. We also present the use of SPEC data to predict future performance.},
  archive      = {J_TC},
  author       = {Yueyao Wang and Samuel Furman and Nicolas Hardy and Margaret Ellis and Godmar Back and Yili Hong and Kirk Cameron},
  doi          = {10.1109/TC.2024.3365941},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1262-1274},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A detailed historical and statistical analysis of the influence of hardware artifacts on SPEC integer benchmark performance},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NDRec: A near-data processing system for training
large-scale recommendation models. <em>TC</em>, <em>73</em>(5),
1248–1261. (<a href="https://doi.org/10.1109/TC.2024.3365939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep neural networks (DNNs) have enabled highly effective recommendation models for diverse web services. In such DNN-based recommendation models, the embedding layer comprises the majority of model parameters. As these models scale rapidly, the embedding layer&#39;s memory capacity and bandwidth requirements threaten to exceed the limits of current computing architectures. We observe the embedding layer&#39;s computational demands increase much more slowly than its storage needs, suggesting an opportunity to offload embeddings to storage hardware. In this work, we present NDRec , a near-data processing system to train large-scale recommendation models. NDRec offloads both the parameters and the computation of the embedding layer to computational storage devices (CSDs), using coherence interconnects (CXLs) for communication between GPUs and CSDs. By leveraging the statistical properties of embedding access patterns, we develop an optimized CSD memory hierarchy and caching strategy. A lookahead embedding scheme enables concurrent execution of embeddings and other operations, hiding latency and reducing memory bandwidth requirements. We evaluate NDRec using real-world and synthetic benchmarks. Results demonstrate NDRec achieves up to 4.33 $\boldsymbol{\times}$ and 3.97 $\boldsymbol{\times}$ speedups over heterogeneous CPU-GPU platforms and GPU caching, respectively. NDRec also reduces per-iteration energy consumption by up to 54.9%.},
  archive      = {J_TC},
  author       = {Shiyu Li and Yitu Wang and Edward Hanson and Andrew Chang and Yang Seok Ki and Hai Li and Yiran Chen},
  doi          = {10.1109/TC.2024.3365939},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1248-1261},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NDRec: A near-data processing system for training large-scale recommendation models},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TIE: Fast experiment-driven ML-based configuration tuning
for in-memory data analytics. <em>TC</em>, <em>73</em>(5), 1233–1247.
(<a href="https://doi.org/10.1109/TC.2024.3365937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, experiment-driven machine-learning (ML) based configuration tuning for in-memory data analytics such as Apache Spark become popular because they can achieve high speedups. However, experiment-driven ML-based approaches naturally need a large number of iterations and each iteration generates a configuration with a probabilistic strategy and executes the program on a real cluster with the configuration. It therefore takes a long time to optimize the performance of an in-memory data analytics program, and thereby hinders these approaches from being widely used in practice. To address this issue, we propose a novel as well as simple approach dubbed Terminating-It-Early (TIE) to reduce the time needed to perform the experiment executions but to achieve speedups similar to those obtained by experiment-driven ML-based approaches. The key idea is that, during the process of searching for the optimal configuration which produces the shortest execution time for a program, we terminate an experiment program execution with a trial configuration as soon as possible when we find its execution time is longer than a predefined threshold (e.g., the shortest execution time thus far). In contrast, traditional experiment-driven ML-based approaches always run all experiment executions completely. We employ 19 Apache Spark programs running on a physical cluster as well as a virtual cluster to evaluate TIE. We compare the tuning time used to find the optimal configuration of a program and the optimized execution time of a program obtained by TIE against those obtained by CherryPick and a reinforcement learning (RL) based approach. The experimental results show that on physical machines, TIE reduces the tuning time used by CherryPick and the RL-based approach by factors of $2.39\times$ and $1.68\times$ on average, respectively. On virtual machines, the corresponding factors are $2.79\times$ and $1.71\times$ . Moreover, the average optimized execution time of the 19 programs tuned by TIE is slightly shorter than those tuned by CherryPick and the RL-based approach.},
  archive      = {J_TC},
  author       = {Chao Chen and Jinhan Xin and Zhibin Yu},
  doi          = {10.1109/TC.2024.3365937},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1233-1247},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TIE: Fast experiment-driven ML-based configuration tuning for in-memory data analytics},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TetriX: Flexible architecture and optimal mapping for
tensorized neural network processing. <em>TC</em>, <em>73</em>(5),
1219–1232. (<a href="https://doi.org/10.1109/TC.2024.3365936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous growth of deep neural network model size and complexity hinders the adoption of large models in resource-constrained platforms. Tensor decomposition has been shown effective in reducing the model size by large compression ratios, but the resulting tensorized neural networks (TNNs) require complex and versatile tensor shaping for tensor contraction, causing a low processing efficiency for existing hardware architectures. This work presents TetriX, a co-design of flexible architecture and optimal workload mapping for efficient and flexible TNN processing. TetriX adopts a unified processing architecture to support both inner and outer product. A hybrid mapping scheme is proposed to eliminate complex tensor shaping by alternating between inner and outer product in a sequence of tensor contractions. Finally, a mapping-aware contraction sequence search (MCSS) is proposed to identify the contraction sequence and workload mapping for achieving the optimal latency on TetriX. Remarkably, combining TetriX with MCSS outperforms the single-mode inner-product and outer-product baselines by up to 46.8 $\boldsymbol{\times}$ in performance across the collected TNN workloads. TetriX is the first work to support all existing tensor decomposition methods. Compared to a TNN accelerator designed for the hierarchical Tucker method, TetriX achieves improvements of 6.5 $\boldsymbol{\times}$ and 1.1 $\boldsymbol{\times}$ in inference throughput and efficiency, respectively.},
  archive      = {J_TC},
  author       = {Jie-Fang Zhang and Cheng-Hsun Lu and Zhengya Zhang},
  doi          = {10.1109/TC.2024.3365936},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1219-1232},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TetriX: Flexible architecture and optimal mapping for tensorized neural network processing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GateKeeper-GPU: Fast and accurate pre-alignment filtering in
short read mapping. <em>TC</em>, <em>73</em>(5), 1206–1218. (<a
href="https://doi.org/10.1109/TC.2024.3365931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the last step of short read mapping, the candidate locations of the reads on the reference genome are verified to compute their differences from the corresponding reference segments using sequence alignment algorithms. Calculating the similarities and differences between two sequences is still computationally expensive since approximate string matching techniques traditionally inherit dynamic programming algorithms with quadratic time and space complexity. We introduce GateKeeper-GPU, a fast and accurate pre-alignment filter that efficiently reduces the need for expensive sequence alignment. GateKeeper-GPU provides two main contributions: first, improving the filtering accuracy of GateKeeper (a lightweight pre-alignment filter), and second, exploiting the massive parallelism provided by the large number of GPU threads of modern GPUs to examine numerous sequence pairs rapidly and concurrently. By reducing the work, GateKeeper-GPU provides an acceleration of 2.9 $\boldsymbol{\times}$ to sequence alignment and up to $1.4\boldsymbol{\times}$ speedup to the end-to-end execution time of a comprehensive read mapper (mrFAST). GateKeeper-GPU is available at https://github.com/BilkentCompGen/GateKeeper-GPU},
  archive      = {J_TC},
  author       = {Zülal Bingöl and Mohammed Alser and Onur Mutlu and Ozcan Ozturk and Can Alkan},
  doi          = {10.1109/TC.2024.3365931},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1206-1218},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GateKeeper-GPU: Fast and accurate pre-alignment filtering in short read mapping},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient execution of arbitrarily complex cross-shard
contracts for blockchain sharding. <em>TC</em>, <em>73</em>(5),
1190–1205. (<a href="https://doi.org/10.1109/TC.2024.3365929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharding is a promising solution to enhance the scalability of blockchain. However, previous sharding systems adopt the lock-based cross-shard protocol to exclusively handle one-shot cross-shard transactions, leading to low-efficiency executions and unavailable calls when handling complex cross-shard contracts that introduce multi-shot cross-shard transactions to invoke multiple contracts managed by different shards. In this paper, we aim to enable efficient execution of arbitrarily complex cross-shard contracts in blockchain sharding systems. First, we perform a calling-flow analysis on Ethereum contracts with more than 180 million real-world transactions and find that about $30\%$ transactions invoke complex contracts. Then, motivated by the properties of these complex contracts, we propose an off-chain execution model, called ShardCon, to achieve efficient executions for complex cross-shard contracts by decoupling the contract execution from the cross-shard consensus. Next, we introduce a cross-shard contract execution engine and a contract-driven deployment rule to the overheads introduced by off-chain executions. Moreover, to adapt to the multi-chain property of a sharding system, we introduce an off-chain state atomic commit protocol. Finally, we implement a prototype and evaluate it with concrete cross-shard contracts, showing that ShardCon can achieve more than 10x increase in throughput and 2x decrease in confirmation latency than the state-of-the-art sharding systems.},
  archive      = {J_TC},
  author       = {Jianting Zhang and Wuhui Chen and Zicong Hong and Gang Xiao and Linlin Du and Zibin Zheng},
  doi          = {10.1109/TC.2024.3365929},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1190-1205},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient execution of arbitrarily complex cross-shard contracts for blockchain sharding},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Achieving DRAM-like PCM by trading off capacity for latency.
<em>TC</em>, <em>73</em>(4), 1180–1189. (<a
href="https://doi.org/10.1109/TC.2024.3355779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase Change Memory (PCM) is considered one of the most promising scalable non-volatile main memory alternatives to DRAM. It provides $\sim$ 4x-5x cost per bit advantage over DRAM, thus enabling cost-effective dense main memory solution. However, PCM accesses are slower than DRAM, which leads to significantly poorer overall system performance (upto 80% higher execution time for memory intensive applications based on our analysis). To use PCM as a viable DRAM replacement, the performance gap between the two memory technologies has to be bridged, primarily by improving PCM read latency. In this work we propose an optimized PCM architecture, PCM-Duplicate, that trades off capacity to improve PCM read latency. In PCM-Duplicate, every row in the PCM subarray has a duplicate row. During memory read, both the rows are activated simultaneously. As a result, the bitline discharges through two PCM cells. This reduces the discharge time significantly, bringing down the overall sensing latency by $ \gt $ 3x compared to baseline PCM. While the overall PCM density benefit over DRAM halves, it still provides 2x more capacity than DRAM while having almost comparable read latency. PCM-Duplicate can either be used as low-cost DRAM main memory alternative or it can be used to replace the DRAM-based last level cache used in today&#39;s hybrid main memory systems for the slower PCM memories. Both these system options not only improve main memory capacity but also allow main memory based persistence by replacing DRAM and making the entire main memory non-volatile.},
  archive      = {J_TC},
  author       = {Irina Alam and Puneet Gupta},
  doi          = {10.1109/TC.2024.3355779},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1180-1189},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Achieving DRAM-like PCM by trading off capacity for latency},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constructing connected-dominating-set with maximum lifetime
in cognitive radio networks. <em>TC</em>, <em>73</em>(4), 1165–1179. (<a
href="https://doi.org/10.1109/TC.2013.77">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connected-dominating-set (CDS) is a representative technique for constructing virtual backbones of wireless networks and thus facilitates implementation of many tasks including broadcasting, routing, etc. Most of existing works on CDS aim at constructing the minimum CDS (MCDS), so as to reduce the communication overhead over the CDS. However, MCDS may not work well in cognitive radio networks (CRNs) where communication links are prone to failure due to stochastic activities of primary users (PUs). A MCDS without consideration of the stochastic activities of PUs easily becomes invalid when the PUs become active. This study addresses a new CDS construction problem by considering the PUs’ activities. Our problem is to maximize the lifetime of the CDS while minimizing the size of the CDS, where the lifetime of a CDS is defined as the expected duration that the CDS is maintained valid. We show that the problem is NP-hard and propose a three-phase centralized algorithm. Given a CRN, the centralized algorithm can compute a CDS such that the lifetime of the CDS is maximized (optimal), and the size of the CDS is upper-bounded. We further present a two-phase localized algorithm which requires 2-hop information. Extensive simulations are conducted to evaluate the proposed algorithms.},
  archive      = {J_TC},
  author       = {Zhiyong Lin and Hai Liu and Xiaowen Chu and Yiu-Wing Leung and Ivan Stojmenovic},
  doi          = {10.1109/TC.2013.77},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1165-1179},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Constructing connected-dominating-set with maximum lifetime in cognitive radio networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Value of information: A comprehensive metric for client
selection in federated edge learning. <em>TC</em>, <em>73</em>(4),
1152–1164. (<a href="https://doi.org/10.1109/TC.2024.3355777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated edge learning (FEEL) is a novel paradigm that enables privacy-preserving and distributed machine learning on end devices. However, FEEL faces challenges from data/system heterogeneity among the participating clients and resource constraints of edge networks, which affect the efficiency and accuracy of the learning process. In this paper, we propose a comprehensive framework for client selection in FEEL based on the concept of Value-of-Information (VoI), which measures how valuable a client is for the global model aggregation. Our framework consists of two independent components: a VoI estimator that uses reinforcement learning to learn the relationship between VoI and various heterogeneous factors of clients; and a greedy client selector that chooses the most valuable clients under network resource constraints. Compared with most of the previous works that use concrete criteria to evaluate and select heterogeneous clients, our VoI-based approach is more comprehensive. Extensive experiments on different datasets and learning tasks are conducted, which show that our framework outperforms several state-of-the-art methods in terms of accuracy.},
  archive      = {J_TC},
  author       = {Yifei Zou and Shikun Shen and Mengbai Xiao and Peng Li and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TC.2024.3355777},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1152-1164},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Value of information: A comprehensive metric for client selection in federated edge learning},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards secure runtime customizable trusted execution
environment on FPGA-SoC. <em>TC</em>, <em>73</em>(4), 1138–1151. (<a
href="https://doi.org/10.1109/TC.2024.3355772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing sensitive data and deploying well-designed Intellectual Property (IP) cores on remote Field Programmable Gate Array (FPGA) are prone to private data leakage and IP theft. One effective solution is constructing Trusted Execution Environment (TEE) and its secure boot process on FPGA-SoC (FPGA System on Chip). This paper aims to establish Secure Runtime Customizable TEE (SrcTEE) on FPGA-SoC through the design of a novel secure boot scheme and the design of the following three components: 1) CrloadIP, which enforces access control on TEE applications deploying IP at runtime such that SrcTEE can alleviate threats from unauthorized TEE applications and then SrcTEE can be adjusted dynamically and securely; 2) CexecIP, which not only enables the execution of newly-installed IP cores without modifying the operating system of FPGA-SoC TEE, but also prevents insider attacks from executing IPs in SrcTEE; 3) CremoAT, which can provide the newly-measured SrcTEE state and establish a secure communication path between remote verifiers and SrcTEE. Our secure boot scheme supports refreshable root trust key, and assures the authenticity and integrity of boot codes during the SrcTEE booting process. We conduct a security analysis of SrcTEE and its performance evaluation on Xilinx Zynq UltraScale+ XCZU15EG 2FFVB1156 MPSoC.},
  archive      = {J_TC},
  author       = {Yanling Wang and Xiaolin Chang and Haoran Zhu and Jianhua Wang and Yanwei Gong and Lin Li},
  doi          = {10.1109/TC.2024.3355772},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1138-1151},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards secure runtime customizable trusted execution environment on FPGA-SoC},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed multihop task offloading in massive
heterogeneous IoT systems. <em>TC</em>, <em>73</em>(4), 1126–1137. (<a
href="https://doi.org/10.1109/TC.2024.3355767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is an emerging technology to satisfy time-varying demands of computation-intensive applications of Internet of Things (IoT) devices. Multi-hop task offloading is one of the key techniques to provide edge services to areas with poor server coverage via multi-hop task forwarding. However, the existing multi-hop offloading approaches have primarily assumed that complete information can be obtained, which does not always hold in heterogeneous IoT systems. To overcome this limitation, we propose a novel two-stage method with incomplete information (TMII) to minimize overall task execution cost with practical IoT systems. Specifically, a hierarchical minority game (HMG) is proposed to estimate the offloading costs by the hierarchical estimation model and the historical data in Stage I. By comparing the estimated offloading cost with the local cost, each IoT device individually decides where to execute the tasks. In Stage II, a tree-based routing mechanism schedules the transmission efficient paths for the offloading nodes by building distributed tree structures. The augmented paths balance the transmission loads to further reduce the offloading delay. Furthermore, the extensive simulation experiments demonstrate TMII outperforms the state-of-the-art approaches in terms of overall cost reduction with significantly reduced communication overhead.},
  archive      = {J_TC},
  author       = {Wenjie Huang and Zhiwei Zhao and Geyong Min and Jiajun Chen},
  doi          = {10.1109/TC.2024.3355767},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1126-1137},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Distributed multihop task offloading in massive heterogeneous IoT systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain-based portable authenticated data transmission
for mobile edge computing: A universally composable secure solution.
<em>TC</em>, <em>73</em>(4), 1114–1125. (<a
href="https://doi.org/10.1109/TC.2024.3355759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing (MEC) systems, data is frequently transmitted between MEC servers and users holding mobile devices for supporting related services. However, critical threats towards data confidentiality and authenticity are raised: adversaries always attempt to extract data content from the transmission and impersonate others to spread malicious data for profits. Furthermore, users have to store the (secret and public) keys used for data transmission locally. Consequently, only devices maintaining the keys can be utilized to access the services provided by MEC servers, and “portability” cannot be achieved. In this paper, we propose a portable authenticated data transmission scheme (dubbed Biplane) via blockchain for MEC systems. Biplane is based on two techniques. One is a blockchain-based authenticated hybrid encryption mechanism, which guarantees data authenticity and confidentiality without requiring a third party (e.g., a Certificate Authority) to assist the MEC servers in certifying users’ public keys. The other one is a blockchain-based portable key management mechanism, which enables the user to transmit data without maintaining any parameters in her/his local devices. We formally prove that Biplane achieves confidential and authenticated data transmission in the universally composable (UC) framework. We also conduct a comprehensive evaluation to demonstrate that Biplane is efficient.},
  archive      = {J_TC},
  author       = {Shiyu Li and Yuan Zhang and Yaqing Song and Nan Cheng and Kan Yang and Hongwei Li},
  doi          = {10.1109/TC.2024.3355759},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1114-1125},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Blockchain-based portable authenticated data transmission for mobile edge computing: A universally composable secure solution},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Game-based adaptive FLOPs and partition point decision
mechanism with latency and energy-efficient tradeoff for edge
intelligence. <em>TC</em>, <em>73</em>(4), 1099–1113. (<a
href="https://doi.org/10.1109/TC.2024.3354033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the product of the combination of edge computing and artificial intelligence, edge intelligence (EI) not only solves the problem of insufficient computing capacity of the end device, but also can provide users with various types of intelligent services. However, offline and online model partitioning methods respectively have problems of poor adaptability to the real computing environment and delayed feedback. In addition, previous work on optimizing energy consumption through model partitioning often ignores the latency of intelligent services. Similarly, the energy consumption of end devices and edge servers is usually not considered when optimizing latency. Therefore, we propose game-based adaptive floating-point operations and partition point decision mechanism (GAFPD) to efficiently find the optimal partition point that reduces latency and improves energy efficiency simultaneously in a dynamically changing computing environment. Numerous simulation experiments and robot-based EI system experiments show that GAFPD can simultaneously reduce the latency of intelligent services and improve the energy efficiency of edge devices, while exhibiting strong adaptability to bandwidth changes.},
  archive      = {J_TC},
  author       = {Xin Niu and Yajing Huang and Zhiwei Wang and Chen Yu and Hai Jin},
  doi          = {10.1109/TC.2024.3354033},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1099-1113},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Game-based adaptive FLOPs and partition point decision mechanism with latency and energy-efficient tradeoff for edge intelligence},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedRFQ: Prototype-based federated learning with reduced
redundancy, minimal failure, and enhanced quality. <em>TC</em>,
<em>73</em>(4), 1086–1098. (<a
href="https://doi.org/10.1109/TC.2024.3353455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a powerful technique that enables collaborative learning among different clients. Prototype-based federated learning is a specific approach that improves the performance of local models by integrating class prototypes. However, prototype-based federated learning faces several challenges, such as prototype redundancy and prototype failure, which can limit its accuracy. In addition, it is also susceptible to poisoning attacks and server malfunction, which can degrade the quality of prototypes. To address these issues, we propose FedRFQ, a prototype-based federated learning approach that aims to reduce redundancy, minimize failure, and improve quality. FedRFQ leverages the SoftPool mechanism with prototype-based federated learning, which effectively mitigates prototype redundancy and prototype failure on Non-IID data. Moreover, we introduce the BFT-detect algorithm, a BFT detectable aggregation algorithm, to ensure the security of FedRFQ against poisoning attacks and server malfunction. Finally, we conducted experiments on three different datasets, namely MNIST, FEMNIST, and CIFAR-10. The results demonstrate that FedRFQ outperforms existing baselines in terms of accuracy when handling Non-IID data.},
  archive      = {J_TC},
  author       = {Biwei Yan and Hongliang Zhang and Minghui Xu and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TC.2024.3353455},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1086-1098},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FedRFQ: Prototype-based federated learning with reduced redundancy, minimal failure, and enhanced quality},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FaaSBatch: Boosting serverless efficiency with in-container
parallelism and resource multiplexing. <em>TC</em>, <em>73</em>(4),
1071–1085. (<a href="https://doi.org/10.1109/TC.2024.3352834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With high scalability and flexibility, serverless computing is becoming the most promising computing model. Existing serverless computing platforms initiate a container for each function invocation, which leads to a huge waste of computing resources. Our examinations reveal that (i) executing invocations concurrently within a single container can provide comparable performance to that provided by multiple containers (i.e., traditional approaches); (ii) redundant resources generated within a container result in memory resource waste, which prolongs the execution time of function invocations. Motivated by these insightful observations, we propose FaaSBatch - a serverless framework that reduces invocation latency and saves scarce computing resources. In particular, FaaSBatch first classifies concurrent function requests into different function groups according to the invocation information. Next, FaaSBatch batches the invocations of each group, aiming to minimize resource utilization. Then, FaaSBatch utilizes an inline parallel policy to map each group of batched invocations into a single container. Finally, FaaSBatch expands and executes invocations of containers in parallel. To further reduce invocation latency and resource utilization, within each container, FaaSBatch reuses redundant resources created during function execution. We conduct extensive experiments based on Azure traces to evaluate the effectiveness and performance of FaaSBatch. We compare FaaSBatch with three state-of-the-art schedulers Vanilla, SFS, and Kraken. Our experimental results show that FaaSBatch effectively and remarkably slashes invocation latency and resource overhead. For instance, when executing I/O functions, FaaSBatch cuts back the invocation latency of Vanilla, SFS, and Kraken by up to 72.58%, 74.10%, and 72.62%, respectively; FaaSBatch also slashes the resource overhead of Vanilla, SFS, and Kraken by 70.2% to 98.40%, 67.74% to 98.12%, and 43.01% to 78.90%, respectively.},
  archive      = {J_TC},
  author       = {Zhaorui Wu and Yuhui Deng and Yi Zhou and Jie Li and Shujie Pang and Xiao Qin},
  doi          = {10.1109/TC.2024.3352834},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1071-1085},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FaaSBatch: Boosting serverless efficiency with in-container parallelism and resource multiplexing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed learning for large-scale models at edge with
privacy protection. <em>TC</em>, <em>73</em>(4), 1060–1070. (<a
href="https://doi.org/10.1109/TC.2024.3352814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data and strong computing power have promoted artificial intelligence to the era of big models. In particular, ChatGPT&#39;s debut heralded the vigorous development of large models. It is an urgent problem to train large models with trillion-level parameters efficiently. Traditional single-machine training stores all data and model parameters in memory. However, due to the limitation of memory and communication resources, when the amount of data or model parameters increases, the problem of memory shortage and communication blocking often occurs. Therefore, distributed training is the most effective ways to solve the above problems and improve training efficiency. In this paper, we propose the algorithm DL-DP , which can achieve an asymptotically optimal convergence rate $O(1/{\sqrt{TK\Gamma^{*}}})$ while satisfying $\varepsilon$ -differential privacy, where $T$ is the local epoch number, $K$ is the global maximum iteration number and $\Gamma^{*}$ is the minimum covering index. In particular, when $\Gamma^{*}=N$ , DL-DP achieves a convergence rate of $O(1/\sqrt{TKN})$ , which is equivalent to the best-known FedAvg approach implemented by training the full model at each client. When $\Gamma^{*}=1$ , DL-DP achieves a convergence rate of $O(1/\sqrt{TK})$ , which is comparable to OAP that assumes all parameters need to be trained at least once in each iteration. Finally, our algorithm has been demonstrated to converge through extensive experiments.},
  archive      = {J_TC},
  author       = {Yuan Yuan and Shuzhen Chen and Dongxiao Yu and Zengrui Zhao and Yifei Zou and Lizhen Cui and Xiuzhen Cheng},
  doi          = {10.1109/TC.2024.3352814},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1060-1070},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Distributed learning for large-scale models at edge with privacy protection},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated test cases generator for IEC 61131-3 structured
text based dynamic symbolic execution. <em>TC</em>, <em>73</em>(4),
1048–1059. (<a href="https://doi.org/10.1109/TC.2024.3351285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programmable Logic Controllers (PLCs) are specialized computers extensively utilized in industrial control fields. Since they control industrial equipment, software faults in PLCs can result in significant losses. However, current testing for PLC programs is mainly manual, and there are very few automatic testing tools. Structured Text (ST) is one of the five PLC programming languages stipulated by the IEC 61131-3 standard, suitable for writing complex control logic. This paper proposes an automatic unit test case generation framework for ST programs based on Dynamic Symbolic Execution and PLC states, as well as a supporting algorithm, and implements the PLCAutoTester tool. PLCAutoTester supports the automatic generation of ST program unit test cases that comply with statement coverage, branch coverage, and MC/DC coverage criterion. We evaluated the PLCAutoTester using 20 PLC programs and compared it with S ${}_{YM}$ PLC. The experimental results show that PLCAutoTester can generate unit test cases with high coverage in a short time. And in 11 common programs, PLCAutoTester is able to generate test cases with almost the same statement coverage as S ${}_{YM}$ PLC while reducing the number of test cases by 95%.},
  archive      = {J_TC},
  author       = {Jianqi Shi and Yinghao Chen and Qin Li and Yanhong Huang and Yang Yang and Mengyan Zhao},
  doi          = {10.1109/TC.2024.3351285},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1048-1059},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Automated test cases generator for IEC 61131-3 structured text based dynamic symbolic execution},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge generation scheduling for DAG tasks using deep
reinforcement learning. <em>TC</em>, <em>73</em>(4), 1034–1047. (<a
href="https://doi.org/10.1109/TC.2024.3350243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed acyclic graph (DAG) tasks are currently adopted in the real-time domain to model complex applications from the automotive, avionics, and industrial domains that implement their functionalities through chains of intercommunicating tasks. This paper studies the problem of scheduling real-time DAG tasks by presenting a novel schedulability test based on the concept of trivial schedulability . Using this schedulability test, we propose a new DAG scheduling framework ( edge generation scheduling—EGS ) that attempts to minimize the DAG width by iteratively generating edges while guaranteeing the deadline constraint. We study how to efficiently solve the problem of generating edges by developing a deep reinforcement learning algorithm combined with a graph representation neural network to learn an efficient edge generation policy for EGS. We evaluate the effectiveness of the proposed algorithm by comparing it with state-of-the-art DAG scheduling heuristics and an optimal mixed-integer linear programming baseline. Experimental results show that the proposed algorithm outperforms the state-of-the-art by requiring fewer processors to schedule the same DAG tasks. https://github.com/binqi-sun/egs},
  archive      = {J_TC},
  author       = {Binqi Sun and Mirco Theile and Ziyuan Qin and Daniele Bernardini and Debayan Roy and Andrea Bastoni and Marco Caccamo},
  doi          = {10.1109/TC.2024.3350243},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1034-1047},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Edge generation scheduling for DAG tasks using deep reinforcement learning},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomizing set-associative caches against conflict-based
cache side-channel attacks. <em>TC</em>, <em>73</em>(4), 1019–1033. (<a
href="https://doi.org/10.1109/TC.2024.3349659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conflict-based cache side-channel attacks against the last-level cache (LLC) is a widely exploited method for information leaking. Cache randomization has recently been accepted as a promising defense. Most of recent designs randomize skewed caches rather than classic set-associative caches; however, skewed caches incur substantial performance overhead both in area and runtime. We cautiously argue that randomized set-associative caches can be sufficiently strengthened and possess a better chance to be adopted in the near future. For the first time, a dynamically randomized set-associative cache has been implemented in the LLC of a Linux capable multicore processor. A single-cycle hash logic is designed for randomizing the cache set indices. A multi-step relocation scheme is used to reduce the cost in remapping the cache layout. The randomized cache layout is remapped periodically for limiting the time window available to attackers. An attack detector is implemented to catch attacks in action and consequently trigger extra remaps. The evaluation results show that the randomized LLC has been sufficiently strengthened to thwart all existing fast algorithms for searching eviction sets with only marginal runtime overhead, and small area and power overhead.},
  archive      = {J_TC},
  author       = {Wei Song and Zihan Xue and Jinchi Han and Zhenzhen Li and Peng Liu},
  doi          = {10.1109/TC.2024.3349659},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1019-1033},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Randomizing set-associative caches against conflict-based cache side-channel attacks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). User-distribution-aware federated learning for efficient
communication and fast inference. <em>TC</em>, <em>73</em>(4),
1004–1018. (<a href="https://doi.org/10.1109/TC.2023.3327513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning as a service (DLaaS) that promotes deep learning-based applications by selling computing services from IT companies to end-users has introduced potential privacy leaks from users and cloud servers. Federated learning (FL) provides an emerging distributed paradigm that enables numerous users to collaboratively train deep-learning models while protecting user privacy and data security. However, many FL-related existing works only focus on improving communication bottlenecks due to frequent model parameter transmission, but ignore the performance degradation incurred by imbalanced user distribution and high inference latency due to the high complexity of deep-learning models in the emerging IoT-edge-cloud FL. In this paper, we propose an efficient user-distribution-aware hierarchical FL for communication-efficient training and fast inference in the IoT-edge-cloud DLaaS architecture. Specifically, we propose a user-distribution-aware hierarchical FL architecture to cope with the performance degradation owing to the imbalanced user distribution. The proposed architecture also features a lightweight deep neural network that adopts the designed lightweight fire modules as components and has a side branch for communication-efficient training and fast inference. Extensive experiments demonstrate that the proposed schemes significantly boost the accuracy by up to 67.12%, save 47.98% communication costs, and accelerate inference by up to 87.24 $\boldsymbol{\times}$ compared to benchmarking methods.},
  archive      = {J_TC},
  author       = {Yangguang Cui and Zhixing Zhang and Nuo Wang and Liying Li and Chunwei Chang and Tongquan Wei},
  doi          = {10.1109/TC.2023.3327513},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1004-1018},
  shortjournal = {IEEE Trans. Comput.},
  title        = {User-distribution-aware federated learning for efficient communication and fast inference},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-source information fusion based DLaaS for traffic flow
prediction. <em>TC</em>, <em>73</em>(4), 994–1003. (<a
href="https://doi.org/10.1109/TC.2023.3236902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is the key to transportation safety and efficiency. The advance in machine learning and deep learning has promoted the development of intelligent transportation systems. For example, the emergence of Deep Learning as a Service (DLaaS) has benefitted researchers a lot in dealing with large scale dataset and complex deep learning algorithms. In traffic forecasting, despite the success of deep learning-based models, there are still shortcomings, such as inadequate use of temporal and spatial traffic information, and indirect modeling of dependencies in traffic data. To address these challenges, we learn the transportation network in the form of a graph, and use graph wavelet as a key component to extract well-positioned features from the graph based on the transportation network. Compared with graph convolution, graph wavelets are very flexible and do not need to specify adjacent regions in the topological graph structure for feature extraction. At the same time, we propose to combine the multi-information fusion traffic control and guidance collaborative neural network and the results obtained are better than the benchmark algorithms. The results by comparison with several baseline methods show that our proposed method can outperform all the baseline methods.},
  archive      = {J_TC},
  author       = {He-xuan Hu and Zhen-zhou Lin and Qiang Hu and Ye Zhang and Wei Wei and Wei Wang},
  doi          = {10.1109/TC.2023.3236902},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {994-1003},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-source information fusion based DLaaS for traffic flow prediction},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MalFox: Camouflaged adversarial malware example generation
based on conv-GANs against black-box detectors. <em>TC</em>,
<em>73</em>(4), 980–993. (<a
href="https://doi.org/10.1109/TC.2023.3236901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is a thriving field currently stuffed with many practical applications and active research topics. It allows computers to learn from experience and to understand the world in terms of a hierarchy of concepts, with each being defined through its relations to simpler concepts. Relying on the strong capabilities of deep learning, we propose a convolutional generative adversarial network-based (Conv-GAN) framework titled MalFox, targeting adversarial malware example generation against third-party black-box malware detectors. Motivated by the rival game between malware authors and malware detectors, MalFox adopts a confrontational approach to produce perturbation paths, with each formed by up to three methods (namely Obfusmal, Stealmal, and Hollowmal) to generate adversarial malware examples. To demonstrate the effectiveness of MalFox, we collect a large dataset consisting of both malware and benignware programs, and investigate the performance of MalFox in terms of accuracy, detection rate, and evasive rate of the generated adversarial malware examples. Our evaluation indicates that the accuracy can be as high as 99.0% which significantly outperforms the other 12 well-known learning models. Furthermore, the detection rate is dramatically decreased by 56.8% on average, and the average evasive rate is noticeably improved by up to 56.2%.},
  archive      = {J_TC},
  author       = {Fangtian Zhong and Xiuzhen Cheng and Dongxiao Yu and Bei Gong and Shuaiwen Song and Jiguo Yu},
  doi          = {10.1109/TC.2023.3236901},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {980-993},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MalFox: Camouflaged adversarial malware example generation based on conv-GANs against black-box detectors},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vector-indistinguishability: Location dependency based
privacy protection for successive location data. <em>TC</em>,
<em>73</em>(4), 970–979. (<a
href="https://doi.org/10.1109/TC.2023.3236900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide use of GPS enabled devices and Location-Based Services, location privacy has become an increasingly worrying challenge to our community. Existing approaches provide solid bases for addressing this challenge. However, they mainly focus on independent location perturbation or whole trajectory perturbation with little attention to location dependency based perturbation between two successive locations. This may result in that perturbed successive locations lose mutual distance and direction dependency, which in turn can lead to significant data utility loss. To address this problem, we propose a new location dependency based privacy notion named as vector-indistinguishability (vector-ind). Vector-ind defines a vector to represent the dependency relationship between two successive locations. Correspondingly, it consists of distance-indistinguishability for distance dependency and direction-indistinguishability for direction dependency. Noise generation for applying Differential Privacy is based on the distance and direction, hence can reflect the location dependency to ensure data utility after perturbation. We also present four mechanisms to achieve vector-ind notion. Finally, we evaluate the empirical privacy and utility of vector-ind in real-world GPS data to demonstrate how our proposed mechanisms for vector-ind can protect location privacy with high data utility in successive location data.},
  archive      = {J_TC},
  author       = {Ying Zhao and Jinjun Chen},
  doi          = {10.1109/TC.2023.3236900},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {970-979},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Vector-indistinguishability: Location dependency based privacy protection for successive location data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OFEI: A semi-black-box android adversarial sample attack
framework against DLaaS. <em>TC</em>, <em>73</em>(4), 956–969. (<a
href="https://doi.org/10.1109/TC.2023.3236872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing popularity of Android devices, Android malware is seriously threatening the safety of users. Although such threats can be detected by deep learning as a service (DLaaS), deep neural networks as the weakest part of DLaaS are often deceived by the adversarial samples elaborated by attackers. In this paper, we propose a new semi-black-box attack framework called one-feature-each-iteration (OFEI) to craft Android adversarial samples. This framework modifies as few features as possible and requires less classifier information to fool the classifier. We conduct a controlled experiment to evaluate our OFEI framework by comparing it with the benchmark methods JSMF, GenAttack and pointwise attack. The experimental results show that our OFEI has a higher misclassification rate of 98.25%. Furthermore, OFEI can extend the traditional white-box attack methods in the image field, such as fast gradient sign method (FGSM) and DeepFool, to craft adversarial samples for Android. Finally, to enhance the security of DLaaS, we use two uncertainties of the Bayesian neural network to construct the combined uncertainty, which is used to detect adversarial samples and achieves a high detection rate of 99.28%.},
  archive      = {J_TC},
  author       = {Guangquan Xu and Guohua Xin and Litao Jiao and Jian Liu and Shaoying Liu and Meiqi Feng and Xi Zheng},
  doi          = {10.1109/TC.2023.3236872},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {956-969},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OFEI: A semi-black-box android adversarial sample attack framework against DLaaS},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GFBE: A generalized and fine-grained blockchain evaluation
framework. <em>TC</em>, <em>73</em>(3), 942–955. (<a
href="https://doi.org/10.1109/TC.2024.3349654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional performance evaluation is crucial for blockchain systems as it enables appropriate blockchain choosing for a given scenario and helps to pinpoint the bottleneck module of a blockchain system to optimize its performance. However, the existing evaluation frameworks for blockchain suffer from low system generality, inefficient workload execution, and incomprehensible evaluation metrics. In order to overcome their limitations, we design and implement the Generalized and Fine-grained Blockchain Evaluation (GFBE) framework. Specifically, we abstract 3 types of Universal Evaluation Interface (UEI) via the dynamic proxying approach to enable generalized evaluation of heterogeneous blockchain systems. Through the design of Lua-based workloads plugin with high flexibility and reusability, GFBE improves the efficiency of workload execution. To achieve comprehensive measurement, we define 15 key performance metrics across hierarchical layers of blockchain architecture. We also implement and deploy GFBE on 16 machines each with 8 CPUs and 16GB RAM, and evaluate three open-source blockchain systems namely Ethereum, ChainMaker, and Haihe smart chain. The experimental results demonstrate that GFBE efficiently and accurately measure 15 key performance metrics such as Contract Execution Efficiency at the contract layer, Consensus Agreement Time Ratio at the consensus layer, and State Query Time at the data layer. Compared with state-of-the-art frameworks such as BLOCKBENCH, Log-based, and Caliper, GFBE distinguishes itself as the only framework that encompasses the appealing features of universal interface, reusable workload, and all-layer metrics.},
  archive      = {J_TC},
  author       = {Liyuan Ma and Xiulong Liu and Yuhan Li and Chenyu Zhang and Gaowei Shi and Keqiu Li},
  doi          = {10.1109/TC.2024.3349654},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {942-955},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GFBE: A generalized and fine-grained blockchain evaluation framework},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal compression for encrypted key-value store in cloud
systems. <em>TC</em>, <em>73</em>(3), 928–941. (<a
href="https://doi.org/10.1109/TC.2024.3349653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Key-value store is adopted by many applications due to its high performance in processing big data workloads. With the increasing concern for privacy, some privacy-preserving key-value storage systems have been proposed. A remarkable solution is to group key-value pairs into packs and then compress and encrypt each pack separately. The selection of pack size is important for key-value storage systems because it affects both the storage cost in the cloud and the bandwidth cost for data retrieval. However, existing data packing strategies do not consider the trade-off between them. In this paper, we study the optimal compression problem for encrypted key-value stores, aiming to minimize the overall cost of data outsourcing. To solve this problem, we devise an optimal pack size computation scheme, which considers both storage and bandwidth costs. Then, we propose a privacy-preserving key-value storage system. It balances the impact caused by encryption and compression without compromising system performance. Meanwhile, it supports dynamic updates and rich types of queries. Finally, we formally analyze the security of our design. Performance evaluations demonstrate that our proposed pack size computation scheme can minimize the overall cost of data outsourcing, and the designed key-value storage system is feasible in practice.},
  archive      = {J_TC},
  author       = {Chen Zhang and Qingyuan Xie and Mingyue Wang and Yu Guo and Xiaohua Jia},
  doi          = {10.1109/TC.2024.3349653},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {928-941},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimal compression for encrypted key-value store in cloud systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research and application of general information measures
based on a unified model. <em>TC</em>, <em>73</em>(3), 915–927. (<a
href="https://doi.org/10.1109/TC.2024.3349650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enable comparisons among various information systems, it is necessary to establish general measures based on a unified model. This paper sets out four concise postulates about information, from which a unified information model is derived using axiomatic methods, which provide a theoretical foundation for developing a series of general information measures. By using volume , the most commonly used information measure, a formula is derived for the relationship between information and mass, energy, and time in closed systems. This formula can be used to analyze and optimize the power consumption requirements of information systems. Through eleven corollaries, the author also demonstrates that the proposed theory can express classical or commonly used principles in information science and technology for each type of measure. This finding suggests that a unified theoretical system that integrates many classical information principles can be constructed. Finally, the output information from some information systems is tested and evaluated, and the results show that the proposed general information measures can be used to support the design and evaluation of various information systems.},
  archive      = {J_TC},
  author       = {Jianfeng Xu},
  doi          = {10.1109/TC.2024.3349650},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {915-927},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Research and application of general information measures based on a unified model},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gem5Tune: A parameter auto-tuning framework for gem5
simulator to reduce errors. <em>TC</em>, <em>73</em>(3), 902–914. (<a
href="https://doi.org/10.1109/TC.2023.3347675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer architecture simulators are widely used to explore new architectures, e.g., the gem5 simulator. However, gem5 has significant performance errors that may lead to misleading research results. Researchers typically reduce errors with the target machine by manual calibration methods, which are time-consuming and require significant expertise. This paper presents gem5Tune, a parameter auto-tuning framework for the gem5 simulator to reduce errors. Applying black-box optimization (BBO) methods, recommended for TPE-based Bayesian optimization, gem5Tune minimizes the error between gem5 and the target machine within a limited number of iterations. Three optimization methods, instruction calibration, sensitivity analysis, and dynamic pruning, are proposed to accelerate the error convergence. Experimental results show that compared to the manual calibration method, gem5Tune significantly reduces performance errors between gem5 and three modern ARM servers by more than 10% (13.83%, 10.86%, and 25.22%, respectively) for SPEC CPU benchmarks. It also scales effectively to PARSEC and SPLASH-2x benchmarks and reduces the errors of architectural events.},
  archive      = {J_TC},
  author       = {Yudi Qiu and Tao Huang and Yuxin Tang and Yanwei Liu and Yang Kong and Xulin Yu and Xiaoyang Zeng and Yibo Fan},
  doi          = {10.1109/TC.2023.3347675},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {902-914},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Gem5Tune: A parameter auto-tuning framework for gem5 simulator to reduce errors},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing graph random walk acceleration via efficient
dataflow and hybrid memory architecture. <em>TC</em>, <em>73</em>(3),
887–901. (<a href="https://doi.org/10.1109/TC.2023.3347674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph random walk sampling is becoming increasingly important with the widespread popularity of graph applications. It aims to capture the desirable graph properties by launching multiple walkers to collect feature paths. However, previous research suffers long sampling latency and severe memory access bottlenecks due to intrinsic data dependency and skewed vertex distribution. Thus, in this paper, we propose FastRW, a dedicated accelerator to boost graph random walk operation on FPGAs. Specifically, FastRW first integrates multiple parallel processing engines to achieve data-level parallelism, where each processing engine also leverages dataflow scheduling to resolve data dependency and hide long sampling latency. Secondly, FastRW leverages a combination of multiple storage resources to implement a hybrid memory architecture adapted to skewed vertex distribution. By integrating the above optimizations, FastRW develops a performance model to take advantage of the balance between computation parallelism and bandwidth demand. We evaluate FastRW with two classic sampling algorithms on a wide range of real-world graph datasets. The experimental results show that FastRW achieves a speedup of 37.52 $\boldsymbol{\times}$ on average over the system running on two 8-core Intel CPUs. FastRW also achieves an average of 28.04 $\boldsymbol{\times}$ speedup over the architecture implemented on V100 GPU.},
  archive      = {J_TC},
  author       = {Yingxue Gao and Teng Wang and Lei Gong and Chao Wang and Yiqing Hu and Yi Yang and Zhongming Liu and Xi Li and Xuehai Zhou},
  doi          = {10.1109/TC.2023.3347674},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {887-901},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enhancing graph random walk acceleration via efficient dataflow and hybrid memory architecture},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint virtual network function placement and flow routing in
edge-cloud continuum. <em>TC</em>, <em>73</em>(3), 872–886. (<a
href="https://doi.org/10.1109/TC.2023.3347671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Function Virtualization (NFV) is becoming one of the most popular paradigms for providing cost-efficient, flexible, and easily-managed network services by migrating network functions from dedicated hardware to commercial general-purpose servers. Despite the benefits of NFV, it remains a challenge to deploy Service Function Chains (SFCs), placing virtual network functions (VNFs) and routing the corresponding flow between VNFs, in the edge-cloud continuum with the objective of jointly optimizing resource and latency. In this paper, we formulate the SFC Deployment Problem (SFCD). To address this NP-hard problem, we first introduce a constant approximation algorithm for a simplified SFCD limited at the edge, followed by a promotional algorithm for SFCD in the edge-cloud continuum, which also maintains a provable constant approximation ratio. Furthermore, we provide an online algorithm for deploying sequentially-arriving SFCs in the edge-cloud continuum and prove the online algorithm achieves a constant competitive ratio. Extensive simulations demonstrate that on average, the total costs of our offline and online algorithms are around 1.79 and 1.80 times the optimal results, respectively, and significantly smaller than the theoretical bounds. In addition, our proposed algorithms consistently outperform the popular benchmarks, showing the superiority of our algorithms.},
  archive      = {J_TC},
  author       = {Yingling Mao and Xiaojun Shang and Yu Liu and Yuanyuan Yang},
  doi          = {10.1109/TC.2023.3347671},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {872-886},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Joint virtual network function placement and flow routing in edge-cloud continuum},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Honeycomb: Ordered key-value store acceleration on an
FPGA-based SmartNIC. <em>TC</em>, <em>73</em>(3), 857–871. (<a
href="https://doi.org/10.1109/TC.2023.3345173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory ordered key-value stores are an important building block in modern distributed applications. We present Honeycomb, a hybrid software-hardware system for accelerating read-dominated workloads on ordered key-value stores that provides linearizability for all operations including scans. Honeycomb stores a B-Tree in host memory. It executes put , update and delete on a CPU. At the same time, it offloads scan and get onto an FPGA-based SmartNIC. This approach enables large stores and simplifies the FPGA implementation but raises the challenge of data access and synchronization across the slow PCIe bus. We describe how Honeycomb overcomes this challenge with careful data structure design, caching, request parallelism with out-of-order execution, wait-free read operations, and fast synchronization between the CPU and the FPGA. For read-heavy YCSB workloads, Honeycomb increases the throughput of a state-of-the-art ordered key-value store by at least $1.8\times$ . For scan-heavy workloads inspired by cloud storage, Honeycomb increases the throughput by more than $2\times$ . The cost-performance, which is more important for large-scale deployments, is improved by at least $1.5\times$ on these workloads.},
  archive      = {J_TC},
  author       = {Junyi Liu and Aleksandar Dragojević and Shane Fleming and Antonios Katsarakis and Dario Korolija and Igor Zablotchi and Ho-Cheung Ng and Anuj Kalia and Miguel Castro},
  doi          = {10.1109/TC.2023.3345173},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {857-871},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Honeycomb: Ordered key-value store acceleration on an FPGA-based SmartNIC},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning the error features of approximate multipliers for
neural network applications. <em>TC</em>, <em>73</em>(3), 842–856. (<a
href="https://doi.org/10.1109/TC.2023.3345163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate multipliers (AMs) have widely been investigated to pursue high-performance and energy-efficient hardware designs for error-tolerant applications, such as neural networks (NNs). The computing accuracy of an AM has been evaluated by using statistical error features; however, it is difficult to estimate the quality of a specific application using AMs. Thus, it is a great challenge to select or design appropriate AMs for an accuracy-constrained application. This paper proposes an application-oriented error evaluation framework for AMs with the aim of exploring the correlation between statistical error features of AMs and the accuracy degradation in AM-based NN applications. Specifically, based on the Dropout Feature Ranking technique, statistical error features of AMs are extensively studied and ranked by their importance to the accuracy of AM-based NN applications. The three most informative features are obtained to construct error models to predict the accuracy loss of AM-based NN applications. The constructed classification models show a probability higher than 96% for correctly classifying the AMs into three categories in accordance with the induced accuracy loss in AM-based NN applications. Furthermore, regression models can predict the accuracy of NN applications using an AM with a deviation as low as 6%. These results show that the proposed error evaluation framework can guide an efficient selection of AMs for NN applications by using just several AM error features, instead of running time-consuming and complicated hardware simulation. The obtained statistical error features can also provide a guidance for the design or generation of application-oriented AMs. Moreover, the proposed framework is applicable for quickly analyzing and selecting other approximate circuits for error-tolerant applications.},
  archive      = {J_TC},
  author       = {Hai Mo and Yong Wu and Honglan Jiang and Zining Ma and Fabrizio Lombardi and Jie Han and Leibo Liu},
  doi          = {10.1109/TC.2023.3345163},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {842-856},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Learning the error features of approximate multipliers for neural network applications},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multidomain fault models covering the analog side of a smart
or cyber–physical system. <em>TC</em>, <em>73</em>(3), 829–841. (<a
href="https://doi.org/10.1109/TC.2023.3345135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, the industrial world has been involved in a massive revolution guided by the adoption of digital technologies. In this context, complex systems like cyber-physical systems play a fundamental role since they were designed and realized by composing heterogeneous components. The combined simulation of the behavioral models of these components allows to reproduce the nominal behavior of the real system. Similarly, a smart system is a device that integrates heterogeneous components but in a miniaturized form factor. The development of smart or cyber-physical systems, in combination with faulty behaviors modeled for the different physical domains composing the system, enables to support advanced functional safety assessment at the system level. A methodology to create and inject multi-domain fault models in the analog side of these systems has been proposed by exploiting the physical analogy between the electrical and mechanical domains to infer a new mechanical fault taxonomy. Thus, standard electrical fault models are injected into the electrical part, while the derived mechanical fault models are injected directly into the mechanical part. The entire flow has been applied to two case studies: a direct current motor connected with a gear train, and a three-axis accelerometer.},
  archive      = {J_TC},
  author       = {Francesco Tosoni and Nicola Dall’Ora and Enrico Fraccaroli and Sara Vinco and Franco Fummi},
  doi          = {10.1109/TC.2023.3345135},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {829-841},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multidomain fault models covering the analog side of a smart or Cyber–Physical system},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HiBid: A cross-channel constrained bidding system with
budget allocation by hierarchical offline deep reinforcement learning.
<em>TC</em>, <em>73</em>(3), 815–828. (<a
href="https://doi.org/10.1109/TC.2023.3343111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online display advertising platforms service numerous advertisers by providing real-time bidding (RTB) for the scale of billions of ad requests every day. The bidding strategy handles ad requests cross multiple channels to maximize the number of clicks under the set financial constraints, i.e., total budget and cost-per-click (CPC), etc. Different from existing works mainly focusing on single channel bidding, we explicitly consider cross-channel constrained bidding with budget allocation. Specifically, we propose a hierarchical offline deep reinforcement learning (DRL) framework called “HiBid”, consisted of a high-level planner equipped with auxiliary loss for non-competitive budget allocation, and a data augmentation enhanced low-level executor for adaptive bidding strategy in response to allocated budgets. Additionally, a CPC-guided action selection mechanism is introduced to satisfy the cross-channel CPC constraint. Through extensive experiments on both the large-scale log data and online A/B testing, we confirm that HiBid outperforms six baselines in terms of the number of clicks, CPC satisfactory ratio, and return-on-investment (ROI). We also deploy HiBid on Meituan advertising platform to already service tens of thousands of advertisers every day.},
  archive      = {J_TC},
  author       = {Hao Wang and Bo Tang and Chi Harold Liu and Shangqin Mao and Jiahong Zhou and Zipeng Dai and Yaqi Sun and Qianlong Xie and Xingxing Wang and Dong Wang},
  doi          = {10.1109/TC.2023.3343111},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {815-828},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HiBid: A cross-channel constrained bidding system with budget allocation by hierarchical offline deep reinforcement learning},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GreedW: A flexible and efficient decentralized framework for
distributed machine learning. <em>TC</em>, <em>73</em>(3), 801–814. (<a
href="https://doi.org/10.1109/TC.2023.3343110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever-increasing demand for computing power in deep learning, distributed training techniques have proven to be effective in meeting these demands. However, current existing state-of-the-art distributed training frameworks, such as Parameter Server (PS), Ring-All-Reduce, and their varieties, still face significant challenges. In particular, the existence of communication bottlenecks can severely limit the efficiency and scalability of distributed training frameworks, making it difficult to fully and effectively exert the computing power of large-scale clusters, especially in the presence of dynamic and ever-changing network environments. To address these issues and further maximize the utilization of the computing power of clusters, in this paper we propose an efficient and dynamic distributed training framework, named GreedW. GreedW can greatly improve the training efficiency of workers by dynamically constructing an adaptive customized communication network and adaptively scheduling the workload. Specifically, GreedW employs a greedy strategy to dynamically construct the communication network tree in each iteration for gradient transmission with minimum communication cost and applies a heterogeneity-aware workload allocation scheme to adaptively balance the heavy traffic across heterogeneous workers in the cluster taking into account the available computing capabilities of each node, which effectively alleviates the network bottleneck. It is worth noting that GreedW is enabled to dynamically adjust the assigned job on each worker node based on their completion time during each round of model aggregation to ensure that each worker node completes its assignments around the same time, thus mitigating the intractable straggler issue and minimizing their idle waiting time. Comprehensive experimental evaluations on three different-scaled training models (i.e., Mnist-2NN, Mnist-CNN, and TextCNN) for image recognition and natural language processing tasks demonstrate that GreedW outperforms the existing state-of-the-art frameworks in terms of training efficiency, system flexibility, and robustness.},
  archive      = {J_TC},
  author       = {Ting Wang and Xin Jiang and Qin Li and Haibin Cai},
  doi          = {10.1109/TC.2023.3343110},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {801-814},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GreedW: A flexible and efficient decentralized framework for distributed machine learning},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A GPU-enabled real-time framework for compressing and
rendering volumetric videos. <em>TC</em>, <em>73</em>(3), 789–800. (<a
href="https://doi.org/10.1109/TC.2023.3343104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, volumetric videos have emerged as an attractive multimedia application providing highly immersive watching experiences since viewers could adjust their viewports at 6 degrees-of-freedom. However, the point cloud frames composing the video are prohibitively large, and effective compression techniques should be developed. There are two classes of compression methods. One suggests exploiting the conventional video codecs (2D-based methods) and the other proposes to compress the points in 3D space directly (3D-based methods). Though the 3D-based methods feature fast coding speeds, their compression ratios are low since the failure of leveraging inter-frame redundancy. To resolve this problem, we design a patch-wise compression framework working in the 3D space. Specifically, we search rigid moves of patches via the iterative closest point algorithm and construct a common geometric structure, which is followed by color compensation. We implement our decoder on a GPU platform so that real-time decoding and rendering are realized. We compare our method with GROOT, the state-of-the-art 3D-based compression method, and it reduces the bitrate by up to 5.98 $\times$ . Moreover, by trimming invisible content, our scheme achieves comparable bandwidth demand of V-PCC, the representative 2D-based method, in FoV-adaptive streaming.},
  archive      = {J_TC},
  author       = {Dongxiao Yu and Ruopeng Chen and Xin Li and Mengbai Xiao and Guanghui Zhang and Yao Liu},
  doi          = {10.1109/TC.2023.3343104},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {789-800},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A GPU-enabled real-time framework for compressing and rendering volumetric videos},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain-based distributed multiagent reinforcement
learning for collaborative multiobject tracking framework. <em>TC</em>,
<em>73</em>(3), 778–788. (<a
href="https://doi.org/10.1109/TC.2023.3343102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of smart cities, video surveillance has become more prevalent in urban areas. The rapid growth of data brings challenges to video processing and analysis. Multi-object tracking (MOT), one of the most fundamental tasks in computer vision, has a wide range of applications and development prospects. MOT aims to locate multiple objects and maintain their unique identities by analyzing the video frame by frame. Most existing MOT frameworks are deployed in centralized systems, which are convenient for management but have problems such as weak algorithm adaptability, limited system scalability, and poor data security. In this paper, we propose a distributed MOT algorithm based on multi-agent reinforcement learning (DMARL-Tracker), which formulates MOT as a Markov decision process (MDP). Each object adjusts its tracking strategy during interactions with the environment. The benchmark results on MOT17 and MOT20 prove that our proposed algorithm achieves state-of-the-art (SOTA) performance. Based on this, we further integrate DMARL-Tracker into the blockchain and propose a blockchain-based collaborative MOT framework. All nodes collaborate and share information through the blockchain, achieving adaptation in different complex scenarios while ensuring data security. The simulation results show that our framework achieves good performance in terms of tracking and resource consumption.},
  archive      = {J_TC},
  author       = {Jiahao Shen and Hao Sheng and Shuai Wang and Ruixuan Cong and Da Yang and Yang Zhang},
  doi          = {10.1109/TC.2023.3343102},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {778-788},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Blockchain-based distributed multiagent reinforcement learning for collaborative multiobject tracking framework},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An occlusion and noise-aware stereo framework based on light
field imaging for robust disparity estimation. <em>TC</em>,
<em>73</em>(3), 764–777. (<a
href="https://doi.org/10.1109/TC.2023.3343098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo vision is widely studied for depth information extraction. However, occlusion and noise pose significant challenges to traditional methods due to failure in photo consistency. In this paper, an occlusion and noise-aware stereo framework named ONAF is proposed to get a robust depth estimation by integrating the advantages of correspondence cues and refocusing cues from light field (LF). ONAF consists of two special depth cue extractors: correspondence depth cue extractor (CCE) and refocusing depth cue extractor (RCE). CCE extracts accurate correspondence depth cues in occlusion areas based on multi-direction Ray-Epipolar Plane Images (Ray-EPIs) from LF, which are more robust than traditional multi-direction EPIs. RCE generates accurate refocusing depth cues in noise areas, benefitting from the many-to-one integration strategy and the directional perception of texture and occlusion based on multi-direction focal stacks from LF. Attention mechanism is introduced to complementarily fuse CCE and RCE to generate optimum depth maps. The experimental results prove the effectiveness of ONAF, which outperforms state-of-the-art disparity estimation methods, especially in occlusion and noise areas.},
  archive      = {J_TC},
  author       = {Da Yang and Zhenglong Cui and Hao Sheng and Rongshan Chen and Ruixuan Cong and Shuai Wang and Zhang Xiong},
  doi          = {10.1109/TC.2023.3343098},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {764-777},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An occlusion and noise-aware stereo framework based on light field imaging for robust disparity estimation},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight and chip-level reconfigurable architecture for
next-generation IoT end devices. <em>TC</em>, <em>73</em>(3), 747–763.
(<a href="https://doi.org/10.1109/TC.2023.3343094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of IoT applications calls for re-configurable IoT devices that can easily extend new functionality on demand. However, in the current architecture, updating chip functions on the end device is highly coupled with the local microprocessor in both hardware and software aspects, leading to inadequate flexibility. In this paper, we propose LEGO, a lightweight architecture with chip-level plug-and-play capabilities for IoT end devices. To achieve this, we first decoupling the control over heterogeneous chips from end devices to the gateway, and design a novel Unified Chip Description Language (UCDL) to access various types of functional chips uniformly. To supporting chips plug-and-play, we design a novel signal converting circuit on end devices to generate all required underlying signals for chip control. We also design a layered instruction orchestrator and hierarchical scheduler to minimize transmission overhead. The results show that our LEGO system can respond to chips plug-and-play within 0.13 seconds, and the lightweight architecture could reduce 49% $\sim$ 61% of power consumption in practical scenarios compared with traditional IoT end devices that are controlled by a microprocessor. The lightweight and easy-to-deploy features of LEGO makes it helpful to reduce deployment cost, thus conducive to accelerating large-scale applications.},
  archive      = {J_TC},
  author       = {Chong Zhang and Songfan Li and Yihang Song and Qianhe Meng and Li Lu and Hongzi Zhu and Xin Wang},
  doi          = {10.1109/TC.2023.3343094},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {747-763},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A lightweight and chip-level reconfigurable architecture for next-generation IoT end devices},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoder reduction approximation scheme for booth
multipliers. <em>TC</em>, <em>73</em>(3), 735–746. (<a
href="https://doi.org/10.1109/TC.2023.3343093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing approximate Booth multipliers fail to keep up with modern approximate multipliers such as truncation-based approximate logarithmic multipliers. This paper introduces a new approximation scheme for Booth multipliers that can operate with negligible error rates using only $N/4$ Booth decoders, instead of the traditional $N/2$ Booth decoders. The proposed 16-bit BD16.4 approximate Booth multiplier reduces the Normalized Mean Error Deviation (NMED) by 96.5% and the Power-Area-Product (PAP) by 69.6%, when compared to a state-of-the-art approximate logarithmic multiplier. Additionally, the proposed BD16.4 approximate multiplier reduces the NMED by 94.4% and PAP by 74.8%, when compared to a state-of-the-art higher-radix approximate Booth multiplier. The proposed 8-bit approximate Booth multipliers reduce the NMED by up to 74% and PAP by up to 5% when compared to the existing state-of-the-art approximate logarithmic multipliers. We validated the results derived in this paper through a neural network inference experiment, where the proposed approximate multipliers showed a negligible drop in inference accuracy compared to the exact Booth multipliers and the state-of-the-art approximate logarithmic multipliers (ALM). The proposed approximate multipliers achieved a Power-Delay-Product reduction of 63% (vs. exact) and 21.22% (vs. ALM) in 16-bit experiments and a reduction of 67% (vs. exact) and 8.75% (vs. ALM) in 8-bit experiments.},
  archive      = {J_TC},
  author       = {Muhammad Hamis Haider and Hao Zhang and Seok-Bum Ko},
  doi          = {10.1109/TC.2023.3343093},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {735-746},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Decoder reduction approximation scheme for booth multipliers},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Branch predictor design for energy harvesting powered
nonvolatile processors. <em>TC</em>, <em>73</em>(3), 722–734. (<a
href="https://doi.org/10.1109/TC.2023.3339977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-volatile processors are proposed for ambient energy harvesting systems to enable accumulative computing across power failures. They employ nonvolatile memory for processor status backup before power outage and resume the system after power recovers. A straightforward backup policy is to back up all volatile data in processors, but it induces high backup cost. In this paper, we focus on branch predictor, an important component in processor, and propose efficient backup schemes to reduce backup cost while maintaining its prediction ability. We first analyze the modules in both traditional and artificial intelligence (AI) assisted designs of branch predictor, and accordingly propose three backup mechanisms pertaining to saturation-driven, locality-driven and maturity-driven backup. On the basis of these mechanisms, adaptive backup branch predictors are designed. Evaluation shows that, with traditional Tournament architecture, the proposed design achieves 15.9% and 54.1% energy reduction when compared with no-backup and all-backup strategy. For AI assisted branch predictor, the proposed design achieves 27.5% and 82.2% energy saving.},
  archive      = {J_TC},
  author       = {Mengying Zhao and Shuo Xu and Lihao Dong and Chun Jason Xue and Dongxiao Yu and Xiaojun Cai and Zhiping Jia},
  doi          = {10.1109/TC.2023.3339977},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {722-734},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Branch predictor design for energy harvesting powered nonvolatile processors},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A practical adversarial attack against sequence-based deep
learning malware classifiers. <em>TC</em>, <em>73</em>(3), 708–721. (<a
href="https://doi.org/10.1109/TC.2023.3339955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence-based deep learning models (e.g., RNNs), can detect malware by analyzing its behavioral sequences. Meanwhile, these models are susceptible to adversarial attacks. Attackers can create adversarial samples that alter the sequence characteristics of behavior sequences to deceive malware classifiers. The existing methods for generating adversarial samples typically involve deleting or replacing crucial behaviors in the original data sequences, or inserting benign behaviors that may violate the behavior constraints. However, these methods that directly manipulate sequences make adversarial samples difficult to implement or apply in practice. In this paper, we propose an adversarial attack approach based on Deep Q-Network and a heuristic backtracking search strategy, which can generate perturbation sequences that satisfy practical conditions for successful attacks. Subsequently, we utilize a novel transformation approach that maps modifications back to the source code, thereby avoiding the need to directly modify the behavior log sequences. We conduct an evaluation of our approach, and the results confirm its effectiveness in generating adversarial samples from real-world malware behavior sequences, which have a high success rate in evading anomaly detection models. Furthermore, our approach is practical and can generate adversarial samples while maintaining the functionality of the modified software.},
  archive      = {J_TC},
  author       = {Kai Tan and Dongyang Zhan and Lin Ye and Hongli Zhang and Binxing Fang},
  doi          = {10.1109/TC.2023.3339955},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {708-721},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A practical adversarial attack against sequence-based deep learning malware classifiers},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CDS: Coupled data storage to enhance read performance of 3D
TLC NAND flash memory. <em>TC</em>, <em>73</em>(3), 694–707. (<a
href="https://doi.org/10.1109/TC.2023.3338474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the strong demand of massive storage capacity, the density of flash memory has been improved in terms of technology node scaling, multi-bit per cell technique, and 3D stacking. However, these techniques also degrade read performance and reliability. The long read latency comes from increased data sensing time and time-consuming ECC decoding time. Storing multiple bits per cell results in more read reference voltages and increased latency of identifying appropriate threshold voltages. To deal with error correction, LDPC is widely used in flash memory to provide stronger ECC capability. However, LDPC incurs a long decoding latency when bit errors are numerous. In this work, we propose coupled data storage (CDS) to improve the read performance of 3D NAND flash-memory storage devices. CDS supports two modes to improve read latency: The high read-speed mode is designed to improve data sensing time with reduced voltage states, while the data correction mode is designed to mitigate bit errors and LDPC overhead. Experiment results showed that CDS could reduce 50 $\sim$ 66.6% read latency and 25.7 $\sim$ 27.5% write latency under the high read-speed mode. For the data correction mode, RBER could be decreased by 37 $\sim$ 52% and the lifetime could be prolonged to 1.6 to 3 times.},
  archive      = {J_TC},
  author       = {Wan-Ling Wu and Jen-Wei Hsieh and Hao-Yu Ku},
  doi          = {10.1109/TC.2023.3338474},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {694-707},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CDS: Coupled data storage to enhance read performance of 3D TLC NAND flash memory},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor recurrent neural network with differential privacy.
<em>TC</em>, <em>73</em>(3), 683–693. (<a
href="https://doi.org/10.1109/TC.2023.3236868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural network (RNN), a branch of deep learning, is a powerful model for sequential data that has outstanding performance on a wide range of important Internet of Things (IoT) tasks. This unprecedented growth of RNN model has however encountered both heterogeneous IoT data and privacy issues. Existing RNN model can not deal with heterogeneous sequential data; often the larger datasets used in training of RNN model contain sensitive information. To tackle these challenges and for the first time, this research proposes a novel differentially private tensor-based RNN (DPTRNN) that can be applied in many challenging deep learning sequence tasks for IoT systems. Specifically, to process heterogeneous sequential data, we propose a tensor-based RNN model. To guarantee privacy, we develop a tensor-based back-propagation through time algorithm with perturbation to avoid exposing the sensitive information for training the tensor-based RNN model within the framework of differential privacy. Thorough security analysis shows that the differential private tensor-based RNN efficiently protects the confidentiality of sensitive user information for IoT. Our results from extensive experiments on two challenging large video datasets suggest that our proposed scheme is practical with guarantee of data privacy preservation and acceptable accuracy loss.},
  archive      = {J_TC},
  author       = {Jun Feng and Laurence T. Yang and Bocheng Ren and Deqing Zou and Mianxiong Dong and Shunli Zhang},
  doi          = {10.1109/TC.2023.3236868},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {683-693},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tensor recurrent neural network with differential privacy},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QoS prediction and adversarial attack protection for
distributed services under DLaaS. <em>TC</em>, <em>73</em>(3), 669–682.
(<a href="https://doi.org/10.1109/TC.2021.3077738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-Learning-as-a-service (DLaaS) has received increasing attention due to its novelty as a diagram for deploying deep learning techniques. However, DLaaS faces performance and security issues that urgently need to be addressed. Given the limited computation resources and concern of benefits, Quality-of-Service (QoS) metrics should be revised to optimize the performance and reliability of distributed DLaaS systems. New users and services dynamically and continuously join and leave such a system, resulting in cold start issues, and additionally, the increasing demand for robust network connections requires the model to evaluate the uncertainty. To address such performance problems, we propose in this article a deep learning-based model called embedding enhanced probability neural network, in which information is extracted from inside the graph structure and then estimated the mean and variance values for the prediction distribution. The adversarial attack is a severe threat to model security under DLaaS. Due to such, the service recommender system&#39;s vulnerability is tackled, and adversarial training with uncertainty-aware loss to protect the model in noisy and adversarial environments is investigated and proposed. Extensive experiments on a large-scale real-world QoS dataset are conducted, and comprehensive analysis verifies the robustness and effectiveness of the proposed model.},
  archive      = {J_TC},
  author       = {Wei Liang and Yuhui Li and Jianlong Xu and Zheng Qin and Dafang Zhang and Kuan-Ching Li},
  doi          = {10.1109/TC.2021.3077738},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {669-682},
  shortjournal = {IEEE Trans. Comput.},
  title        = {QoS prediction and adversarial attack protection for distributed services under DLaaS},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure deep learning in defense in
deep-learning-as-a-service computing systems in digital twins.
<em>TC</em>, <em>73</em>(3), 656–668. (<a
href="https://doi.org/10.1109/TC.2021.3077687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Digital Twins (DTs) bring convenience to city managers, they also generate new challenges to city network security. Currently, cyberspace security becomes increasingly complicated. Intrusion detection and Deep Learning (DL) are combined with shunning security threats in service computing systems and improving network defense capabilities. DTs can be applied to network security. People&#39;s understanding of cyberspace security can be improved using DTs to digitally define, model, and display the network environment and security status. The intrusion detection data are optimized based on DL technology, and a network intrusion detection algorithm integrated with Deep Neural Network (DNN) model is proposed. In the cloud service system, a trust model based on Keyed-Hashing-based Self-Synchronization (KHSS) is introduced. This model predicts the security state and detects attacks according to existing malicious attacks, ensuring the network security defense system&#39;s regular operation. Finally, simulation experiments verify the Deep Belief Networks (DBN) model&#39;s feasibility and the cloud trust model. The DBN algorithm proposed improves the correct detection rate of unknown samples by 4.05% compared with the Support Vector Machine (SVM) algorithm. From the 20,100 pieces of data in the test dataset, the number of correct attacks detected by the DBN algorithm exceeds those by the SVM algorithm by 818. DBN algorithm requires a short detection time while ensuring optimal detection accuracy. The KHSS+DBN model predicts cloud security states, and the results are the same as the actual states, with an error of only 1%∼2%.},
  archive      = {J_TC},
  author       = {Zhihan Lv and Dongliang Chen and Bin Cao and Houbing Song and Haibin Lv},
  doi          = {10.1109/TC.2021.3077687},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {656-668},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Secure deep learning in defense in deep-learning-as-a-service computing systems in digital twins},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient preprocessing-based approach to mitigate
advanced adversarial attacks. <em>TC</em>, <em>73</em>(3), 645–655. (<a
href="https://doi.org/10.1109/TC.2021.3076826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks are well-known to be vulnerable to Adversarial Examples. Recently, advanced gradient-based attacks were proposed (e.g., BPDA and EOT), which can significantly increase the difficulty and complexity of designing effective defenses. In this paper, we present a study towards the opportunity of mitigating those powerful attacks with only pre-processing operations. We make the following two contributions. First, we perform an in-depth analysis of those attacks and summarize three fundamental properties that a good defense solution should have. Second, we design a lightweight preprocessing function with these properties and the capability of preserving the model’s usability and robustness against these threats. Extensive evaluations indicate that our solutions can effectively mitigate all existing standard and advanced attack techniques, and beat 11 state-of-the-art defense solutions published in top-tier conferences over the past 2 years.},
  archive      = {J_TC},
  author       = {Han Qiu and Yi Zeng and Qinkai Zheng and Shangwei Guo and Tianwei Zhang and Hewu Li},
  doi          = {10.1109/TC.2021.3076826},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {645-655},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An efficient preprocessing-based approach to mitigate advanced adversarial attacks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A secure and decentralized DLaaS platform for edge resource
scheduling against adversarial attacks. <em>TC</em>, <em>73</em>(3),
631–644. (<a href="https://doi.org/10.1109/TC.2021.3074806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Computing is promising for latency-sensitive applications. However, current edge resource scheduling is inefficient. Deep Learning as a Service (DLaaS) provides deep learning methods to optimize the resource scheduling problem, but faces great challenges of security and reliability. On one hand, DLaaS training agents and raw data are exposed to various adversarial attacks. On the other hand, dishonest DLaaS trainers can generate poisoned models to attack the DLaaS system. In this article, we propose SAPE , a S ecure and decentralized DL A aS P latform in E dge computing. SAPE allows users to submit their tasks, which will be scheduled to the appropriate edge clusters to minimize the task execution time. We formulate the resource scheduling problem and develop the federated deep reinforcement learning (DRL) method to optimize the problem and resist the adversarial attacks of DLaaS. We utilize blockchain and propose a consortium-based verification scheme to improve the reliability of the federated training process, protecting the DLaaS models from being poisoned and compromised. We conduct experiments to evaluate the latency and security performance of SAPE and the federated DRL scheduling policy. The results show that SAPE outperforms the traditional schemes when defending against adversarial attacks towards the DLaaS platform in edge computing.},
  archive      = {J_TC},
  author       = {Laizhong Cui and Ziteng Chen and Shu Yang and Ruiyu Chen and Zhong Ming},
  doi          = {10.1109/TC.2021.3074806},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {631-644},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A secure and decentralized DLaaS platform for edge resource scheduling against adversarial attacks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic circuits for computing weighted ratio with
applications to multiclass bayesian inference machine. <em>TC</em>,
<em>73</em>(2), 621–630. (<a
href="https://doi.org/10.1109/TC.2023.3329998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference is one method of statistical inference in machine learning. It predicts the probability that a given test belongs to a certain class and is widely used in various applications such as medical diagnosis, spam classification and fraud detection. The conventional binary architecture of computing the posterior probability is inefficient in practical implementation, which is involved in multiplication, addition and division operations. Recently, it has been shown that simple Muller C-elements, the asynchronous logic units, can perform stochastic Bayesian inference motivated by its truth table when the data is encoded as the bit-stream. The Bayesian inference machine is therefore implemented with low hardware cost. However, such an architecture is employed to compute the posterior probability of two classes only. This brief presents two stochastic circuit designs for computing the weighted ratio with multiple weights for generalized multi-class Bayesian machines. The first design is mainly based on the JK flip flop and multiplexers. The second approach is to construct the finite state machine (FSM) by manipulating the correlation between the input bit-streams. The FSM-based design requires fewer random number sources (RNSs) as compared to the JK flip flop-based implementation. These facts lead to a reduction of hardware area and energy. Simulation results show that the accuracy of the proposed JK flip flop-based and FSM-based designs is almost the same in the tested data sets. As compared to the traditional binary design, the circuit area of the proposed stochastic design is improved by $96\%$ at least in the cases of three and four classes. The consumed energy per operation is reduced by $58.1\%$ at least in the cases of three and four classes.},
  archive      = {J_TC},
  author       = {Shao-I Chu and Chi-Long Wu and Tzu-Heng Chien and Bing-Hong Liu and Tu N. Nguyen},
  doi          = {10.1109/TC.2023.3329998},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {621-630},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stochastic circuits for computing weighted ratio with applications to multiclass bayesian inference machine},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time related-key attack on full-round shadow designed
for IoT nodes. <em>TC</em>, <em>73</em>(2), 613–620. (<a
href="https://doi.org/10.1109/TC.2023.3315057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Things (IoT), many new lightweight block ciphers are designed in recent years to meet the security demand in IoT devices. Shadow is a lightweight block cipher designed for IoT Nodes (IEEE Internet of Things Journal, 2021). In this article, an efficient attack on full-round Shadow is proposed based on the idea of a related-key differential attack. First, a differential transfer property for AND operation is illustrated. This property demonstrates a link between the difference and the input value. If the difference of the input is not zero, to lead to a zero difference, there are some constraints on the input value. Furthermore, two properties for Shadow family ciphers are identified. According to these properties, some related keys on Shadow will lead to an internal collision for the subkey generator, which will eventually lead to a full-round distinguisher. Finally, with the idea of related-key differential attack, an efficient attack is applied to Shadow. For Shadow-32, with 4 related keys, 8 master key bits can be derived in about 0.044 seconds on average. For Shadow-64, with 4 related keys, 24 master key bits can be derived in about 3.9 hours on average. All our theoretical results are verified by experiments.},
  archive      = {J_TC},
  author       = {Kai Zhang and Xuejia Lai and Lei Wang and Jie Guan and Bin Hu and Senpeng Wang and Tairong Shi},
  doi          = {10.1109/TC.2023.3315057},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {613-620},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Real-time related-key attack on full-round shadow designed for IoT nodes},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximation- and quantization-aware training for graph
neural networks. <em>TC</em>, <em>73</em>(2), 599–612. (<a
href="https://doi.org/10.1109/TC.2023.3337319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are one of the best-performing models for processing graph data. They are known to have considerable computational complexity, despite the smaller number of parameters compared to traditional Deep Neural Networks (DNNs). Operations-to-parameters ratio for GNNs can be tens and hundreds of times higher than for DNNs, depending on the input graph size. This complexity indicates the importance of arithmetic operation optimization within GNNs through model quantization and approximation. In this work, for the first time, we combine both approaches and implement quantization- and approximation-aware training for GNNs to sustain their accuracy under the errors induced by inexact multiplications. We employ matrix multiplication CUDA kernel to speed up the simulation of approximate multiplication within GNNs. Further, we demonstrate the execution speed, accuracy, and energy efficiency of GNNs with approximate multipliers in comparison with quantized low-bit GNNs. We evaluate the performance of state-of-the-art GNN architectures (i.e., GIN, SAGE, GCN, and GAT) on various datasets and tasks (i.e., Reddit-Binary, Collab for graph classification, Cora and PubMed for node classification) with a wide range of approximate multipliers. Our framework is available online: https://github.com/TUM-AIPro/AxC-GNN .},
  archive      = {J_TC},
  author       = {Rodion Novkin and Florian Klemme and Hussam Amrouch},
  doi          = {10.1109/TC.2023.3337319},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {599-612},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Approximation- and quantization-aware training for graph neural networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital twin-assisted federated learning service
provisioning over mobile edge networks. <em>TC</em>, <em>73</em>(2),
586–598. (<a href="https://doi.org/10.1109/TC.2023.3337317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) offers collaborative machine learning without data exposure, but challenges arise in the mobile edge network (MEC) environment due to limited resources and dynamic conditions. This paper presents a Digital Twin (DT)-assisted FL platform for MEC networks and introduces a novel multi-FL service framework to address resource dynamics and mobile users. We leverage DT models to optimize device scheduling and MEC resource allocation, aiming to maximize utility across FL services. Our work includes heuristic and constant approximation algorithms for offline multi-FL service scenarios and we also investigate an online setting of our solution with dynamic bandwidth and moving client conditions. To adapt to changing network conditions, we utilize historical bandwidth data in DTs and implement a deep reinforcement learning algorithm, Ra_DDPG, for automatic bandwidth allocation. Evaluation results demonstrate a significant 49.8% increase in system utility compared to a benchmark algorithm, showcasing the effectiveness of our approach.},
  archive      = {J_TC},
  author       = {Ruirui Zhang and Zhenzhen Xie and Dongxiao Yu and Weifa Liang and Xiuzhen Cheng},
  doi          = {10.1109/TC.2023.3337317},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {586-598},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Digital twin-assisted federated learning service provisioning over mobile edge networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Fully learnable hyperdimensional computing framework with
ultratiny accelerator for edge-side applications. <em>TC</em>,
<em>73</em>(2), 574–585. (<a
href="https://doi.org/10.1109/TC.2023.3337316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired hyperdimensional computing (HDC) is a new computational paradigm that encodes input sample into a hypervector (generally with dimensions of $2K-10K$ ), and performs simple arithmetic and logic operations in the hyperdimensional space to complete perceptual tasks like human brain. Due to its simplicity, interpretability, and robustness, HDC has gradually become a competitor and substitute for deep neural network (DNN) in many tasks. However, there exists an accuracy gap between existing heuristic HDC algorithms and DNN in computer vision tasks, as existing encoding methods have difficulty in filtering out large amount of background and noise in the images, and effectively extracting the spatial structure features of images. In addition, the existing hardware for HDC deployment mainly focuses on in-memory computing (IMC), application specific integrated circuit (ASIC), or high-capacity field programmable gate array (high-capacity FPGA), which cannot meet the flexibility, small area, and low power requirements of edge-side applications. In this paper, a fully learnable HDC framework with learnable preprocessing, encoding and querying, is proposed to boost the accuracy in computer vision tasks, as well as an ultra-tiny accelerator based on edge-side FPGA which matches the proposed framework. Experiments show that on multiple commonly-used image datasets, the proposed HDC framework has an average computation reduction of 80% compared to other most advanced strategies, while achieves a 1.2% accuracy increase. Evaluation on edge-side FPGA shows that compared to other FPGA based state-of-the-art designs, the proposed accelerator saves more than $10\boldsymbol{\times}$ hardware resource and power consumption.},
  archive      = {J_TC},
  author       = {Tianyang Yu and Bi Wu and Ke Chen and Gong Zhang and Weiqiang Liu},
  doi          = {10.1109/TC.2023.3337316},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {574-585},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fully learnable hyperdimensional computing framework with ultratiny accelerator for edge-side applications},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-efficient exposed datapath architecture with a RISC-v
instruction set mode. <em>TC</em>, <em>73</em>(2), 560–573. (<a
href="https://doi.org/10.1109/TC.2023.3337313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transport triggered architectures (TTAs) follow the static programming model of very long instruction word (VLIW) processors but expose additional information of the processor datapath in the programming interface, which enables low-level code optimizations but results in lower code density. Multi-instruction-set architectures add flexiblity via their ability to switch instruction sets during execution. The added flexibility is interesting for VLIW-style processors because it enables reducing the large instruction stream energy footprint by using an instruction set with enhanced code density in regions with limited opportunities for exploitation of instruction level parallelism. In this article, we introduce a dual instruction-set architecture, “Dual-IS”, that implements both RISC-V and TTA instruction sets with shared datapath resources by means of a lightweight microcode unit. In order to utilize the flexible architecture automatically, we introduce a compilation method that is able to independently target code for both instruction sets based on static code analysis and a microarchitectural model of the processor. Compared to a single-ISA TTA processor, we were able to lower the instruction stream energy consumption 45% on average in the best design point, which resulted in a total energy consumption reduction of 26% and a 0.4% lower run time.},
  archive      = {J_TC},
  author       = {Kari Hepola and Joonas Multanen and Pekka Jääskeläinen},
  doi          = {10.1109/TC.2023.3337313},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {560-573},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-efficient exposed datapath architecture with a RISC-V instruction set mode},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wrong-path-aware entangling instruction prefetcher.
<em>TC</em>, <em>73</em>(2), 548–559. (<a
href="https://doi.org/10.1109/TC.2023.3337308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instruction prefetching is instrumental for guaranteeing a high flow of instructions through the processor front end for applications whose working set does not fit in the lower-level caches. Examples of such applications are server workloads, whose instruction footprints are constantly growing. There are two main techniques to mitigate this problem: fetch directed prefetching (or decoupled front end) and instruction cache (L1I) prefetching. This work extends the state-of-the-art Entangling prefetcher to avoid training during wrong-path execution. Our Entangling wrong-path-aware prefetcher is equipped with microarchitectural techniques that eliminate more than 99% of wrong-path pollution, thus reaching 98.9% of the performance of an ideal wrong-path-aware solution. Next, we propose two microarchitectural optimizations able to further increase performance benefits by 1.8%, on average. All this is achieved with just 304 bytes. Finally, we study the interplay between the L1I prefetcher and a decoupled front end. Our analysis shows that due to pollution caused by wrong-path instructions, the degree of decoupling cannot be increased unlimitedly without negative effects on the energy-delay product (EDP). Furthermore, the closer to ideal is the L1I prefetcher, the less decoupling is required. For example, our Entangling prefetcher reaches an optimal EDP with a decoupling degree of 64 instructions.},
  archive      = {J_TC},
  author       = {Alberto Ros and Alexandra Jimborean},
  doi          = {10.1109/TC.2023.3337308},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {548-559},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Wrong-path-aware entangling instruction prefetcher},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive SAT modeling for optimal pattern retargeting in
IEEE 1687 networks. <em>TC</em>, <em>73</em>(2), 536–547. (<a
href="https://doi.org/10.1109/TC.2023.3336198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wide variety of embedded instruments are increasingly integrated within modern System-on-Chips (SoCs) for the purpose of monitoring, debugging, and testing. Integrating this heterogeneous set of embedded instruments within the same chip necessitates having an efficient access network. Such a network should ensure minimal access time to the embedded instruments. The IJTAG standard was introduced as an efficient access methodology for instruments embedded in chips. The required configurations to access the desired instruments are generated in a process called pattern retargeting. For optimal retargeting, it is important to minimize the time taken to find the right configuration vectors as well as the time to access the desired instruments. In this work, we express the two execution times using the term Dynamic Access Time (DAT). This work proposes an adaptive retargeting model based on the Boolean Satisfiability Problem (SAT) that properly fits any arbitrary IJTAG network. The proposed model should provide substantial improvement, especially for runtime applications requiring dynamic retargeting, such as debugging operations. To assess the effectiveness of our proposed model, a comparison between state-of-the-art retargeting techniques and our work is performed. The results show an improvement in retargeting time of 60%, on average, compared to previous SAT-based retargeting approaches.},
  archive      = {J_TC},
  author       = {Abrar A. Ibrahim and Ahmed M. Y. Ibrahim and M. Watheq El-Kharashi and Mona Safar},
  doi          = {10.1109/TC.2023.3336198},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {536-547},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive SAT modeling for optimal pattern retargeting in IEEE 1687 networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RISC-v custom instructions of elementary functions for IoT
endpoint devices. <em>TC</em>, <em>73</em>(2), 523–535. (<a
href="https://doi.org/10.1109/TC.2023.3336174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computation of elementary functions is required in many tasks of Internet of Things (IoT) endpoint devices, for example, communications, image processing, and biomedical signal processing. IoT endpoint devices generally adopt software approaches to compute elementary functions, which take many cycles. To improve efficiency, this work proposes custom instructions for elementary functions to the open-source RISC-V instruction set architecture (ISA). In particular, several variants of the custom instructions (fast, intermediate, and tiny variants) are developed to satisfy the needs of various types of IoT devices. Microarchitecture design and VLSI circuit design are then proposed to efficiently support the extended ISA. Both software emulation and on-board evaluation of the new architecture are carried out with testbenches covering typical communication and computation tasks for IoT devices. The custom instructions gain speedups ranging from 3.3 to 18.0 compared to a baseline RV32IM design. ASIC synthesis results under TSMC 28nm technology demonstrate that the power overhead is $ \lt $ 5% with the tiny variant, $ \lt $ 17% with the intermediate variant, and $ \lt $ 26% with the fast variant, which is not significant considering the achieved speedup. The experimental results further confirm that the proposed custom instructions are computation-efficient and versatile to adapt to different IoT devices for various applications.},
  archive      = {J_TC},
  author       = {Yuxing Chen and Xinrui Wang and Suwen Song and Lang Feng and Zhongfeng Wang},
  doi          = {10.1109/TC.2023.3336174},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {523-535},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RISC-V custom instructions of elementary functions for IoT endpoint devices},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MMDataLoader: Reusing preprocessed data among concurrent
model training tasks. <em>TC</em>, <em>73</em>(2), 510–522. (<a
href="https://doi.org/10.1109/TC.2023.3336161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data preprocessing plays an important role in deep learning, which directly affects the training efficiency. Data preprocessing is performed on the CPU. The preprocessed data are then fed to the models that are trained on the GPU. We observe that data preprocessing on the CPU can potentially create a bottleneck in the entire process of a model training task. In order to tackle this issue, we have developed MMDataLoader, which enables reusing preprocessed data among multiple model training tasks. MMDataLoader automatically constructs a data preprocessing pipeline based on each task&#39;s specific preprocessing workflow, allowing for maximum data reuse and reduced computing workload on the CPU. Unlike conventional data loaders that operate at the task level and provide data provision services to specific training tasks, MMDataLoader operates at the server level and provides data for all concurrently running tasks. We have conducted extensive experiments. The results show that MMDataLoader can significantly increase preprocessing throughput without affecting model convergence when compared to conventional methods where model training tasks are executed concurrently. For instance, with three tasks running, the preprocessing throughput can increase by 1.6x to 3.15x, depending on the tasks being executed and the proportion of preprocessing operations that are shared among them.},
  archive      = {J_TC},
  author       = {Hai Jin and Zhanyang Zhu and Ligang He and Yuhao Li and Yusheng Hua and Xuanhua Shi},
  doi          = {10.1109/TC.2023.3336161},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {510-522},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MMDataLoader: Reusing preprocessed data among concurrent model training tasks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast inner-product algorithms and architectures for deep
neural network accelerators. <em>TC</em>, <em>73</em>(2), 495–509. (<a
href="https://doi.org/10.1109/TC.2023.3334140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP&#39;s clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform.},
  archive      = {J_TC},
  author       = {Trevor E. Pogue and Nicola Nicolici},
  doi          = {10.1109/TC.2023.3334140},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {495-509},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast inner-product algorithms and architectures for deep neural network accelerators},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating neural networks for diverse networking
classification tasks via hardware-aware neural architecture search.
<em>TC</em>, <em>73</em>(2), 481–494. (<a
href="https://doi.org/10.1109/TC.2023.3333253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) are widely used in classification-based networking analysis to help traffic transmission and system security. However, there are heterogeneous network devices (e.g., switches and routers) in a network. Manually customizing NNs with specific device requirements (e.g., max allowed running latency) can be time-consuming and labor-intensive. Furthermore, the diverse data characteristics of different networking classification tasks add to the burden of NN customization. This paper introduces Loong, a neural architecture search (NAS) based system that automatically generates NNs for various networking tasks and devices. Loong includes a neural operation embedding module, which embeds candidate neural operations into the layer to be designed. Then, the layer-wise training is used to generate a task-specific NN layer by layer. This layer-wise scheme simultaneously trains and selects candidate neural operations using gradient feedback. Finally, only the important operations are selected to form the layer, maximizing accuracy. By incorporating multiple objectives, including deployment memory and running latency of devices, into the training and selection of NNs, Loong is able to customize NNs for heterogeneous network devices. Experiments show that Loong&#39;s NNs outperform 13 manual-designed and NAS-based NNs, with a 4.11% improvement in F1-score. Additionally, Loong&#39;s NNs achieve faster (7.92X) speeds on commodity devices.},
  archive      = {J_TC},
  author       = {Guorui Xie and Qing Li and Zhenning Shi and Hanbin Fang and Shengpeng Ji and Yong Jiang and Zhenhui Yuan and Lianbo Ma and Mingwei Xu},
  doi          = {10.1109/TC.2023.3333253},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {481-494},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Generating neural networks for diverse networking classification tasks via hardware-aware neural architecture search},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Age-aware data selection and aggregator placement for timely
federated continual learning in mobile edge computing. <em>TC</em>,
<em>73</em>(2), 466–480. (<a
href="https://doi.org/10.1109/TC.2023.3333213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated continual learning (FCL) is emerging as a key technology for time-sensitive applications in highly adaptive environments including autonomous driving and industrial digital twin. Each FCL trains machine learning models using newly-generated datasets as soon as possible, to obtain a highly accurate machine learning model for new event predictions. The age of data , defined as the time difference between the generation time of a dataset and the current time, is widely adopted as a key criterion to evaluate both timeline and quality of training. In this paper, we study the problem of age-aware FCL in a mobile edge computing (MEC) network. We not only investigate optimization techniques that optimize the data selection and aggregator placement for FCL but also implement a real system as a prototype for age-aware FCL. Specifically, we first propose an approximation algorithm with a provable approximation ratio for the age-aware data selection and aggregator placement problem for FCL with a single request. In real application scenarios, there are usually multiple FCL requests that require to train models, and delays in the MEC network are usually uncertain. We then study the problem of age-aware data selection and aggregator placement problem for FCL with uncertain delays and multiple requests, by devising an online learning algorithm with a bounded regret based on contextual bandits. We finally implement a prototype for FCL in an MEC network, with various heterogeneous user equipments (UEs) and cloudlets with different computing capabilities in the network. Experiment results show that the performance of the proposed algorithms outperform existing studies, by achieving 47% lower age of data and 12% higher model accuracy.},
  archive      = {J_TC},
  author       = {Zichuan Xu and Lin Wang and Weifa Liang and Qiufen Xia and Wenzheng Xu and Pan Zhou and Omer F. Rana},
  doi          = {10.1109/TC.2023.3333213},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {466-480},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Age-aware data selection and aggregator placement for timely federated continual learning in mobile edge computing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scheduling inputs in early exit neural networks.
<em>TC</em>, <em>73</em>(2), 451–465. (<a
href="https://doi.org/10.1109/TC.2023.3333189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early exit neural networks (EENs) reduce the processing times of deep convolutional neural networks by means of internal classifiers (ICs) that allow jobs, being the input of the EEN, to exit early from the processing pipeline. However, the current designs used in pervasive systems ignore variability in data arrival rates, exposing EEN-based services to potential loss of the incoming jobs, due to finite input buffer capacity. Motivated by this issue, we introduce and study the early exit scheduling problem, which aims at dynamically configuring IC thresholds at runtime to achieve effective trade-offs between job classification accuracy, processing time, and job loss ratio. We argue that deciding the EEN exit layer for a job at the start of its processing makes the problem mathematically tractable, allowing us to develop policies to control buffer backlog, classification accuracy, and processing time across the EEN layers. The main contribution of the paper is the introduction of single-exit IC threshold configurations as a mechanism to allow the scheduling policy to reliably predict the best EEN exit layer of each input job. Three scheduling policies that leverage this idea are proposed to dynamically schedule job arrivals to an EEN-based service. The proposed solution, here tailored to EENs based on convolutional neural networks (CNNs), is fairly general and can be applied to different use cases. The two application scenarios considered in this paper focus on image classification and intrusion detection. Experiments on some popular CNNs for the two aforementioned application scenarios indicate that the proposed policies can achieve significant savings in processing times and improve job loss ratio compared to both ordinary EENs and CNNs while still providing high mean classification accuracy.},
  archive      = {J_TC},
  author       = {Giuliano Casale and Manuel Roveri},
  doi          = {10.1109/TC.2023.3333189},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {451-465},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scheduling inputs in early exit neural networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VALIANT: An EDA flow for side-channel leakage evaluation and
tailored protection. <em>TC</em>, <em>73</em>(2), 436–450. (<a
href="https://doi.org/10.1109/TC.2023.3333164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power side-channels give rise to several potent attack vectors for leaking information in digital circuits. While a plethora of (mathematically robust) solutions exist to tackle such side-channels, their deployment through existing VLSI design-flows remains an important engineering issue. Besides, most existing solutions result in significant hardware overhead hindering their practical usage for resource-constrained settings, such as Internet-of-Things (IoT) or embedded devices. In this paper, we address both of these issues through an integrated electronic design automation (EDA) tool-flow operating on gate-level designs. Based on an interesting observation that not every net in a design is equally susceptible to side-channel leakage, we devise a generic testing mechanism and lightweight albeit customizable protection strategy for a given trace count. We first analytically establish the observation based on certain physical properties of VLSI circuits and also validate it on ISCAS benchmark circuits. Next, we present a tool called VALIANT , which can identify the leaking nets for a given number of traces from the gate-level netlist of a cipher. VALIANT works alongside state-of-the-art design automation tools and, therefore, can be directly incorporated in existing design flows. After identifying the leaky subset of nets in a design, we propose a lightweight variant of an existing masking scheme to eliminate the leakage concerning a given trace count. The main feature of our protection scheme is that it takes into account subset of nets are not “leaky” and optimizes the usage of randomness and extra gates according to this information to minimize the overhead. Experimental evaluation over state-of-the-art lightweight S-Boxes and the GIFT block cipher establishes the efficacy of the proposed idea for generating lightweight protected solutions in an automated manner.},
  archive      = {J_TC},
  author       = {Rajat Sadhukhan and Sayandeep Saha and Sudipta Paria and Swarup Bhunia and Debdeep Mukhopadhyay},
  doi          = {10.1109/TC.2023.3333164},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {436-450},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VALIANT: An EDA flow for side-channel leakage evaluation and tailored protection},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSDedup: Layered secure deduplication for cloud storage.
<em>TC</em>, <em>73</em>(2), 422–435. (<a
href="https://doi.org/10.1109/TC.2023.3331953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To implement encrypted data deduplication in a cloud storage system, users must encrypt files using special encryption algorithms (e.g., convergent encryption (CE)), which cannot provide strong protection. The confidential level of an outsourced file is determined by the user himself/herself subjectively or by the owner number of the file objectively. These files owned by a few users are considered strictly confidential and require strong protection. In this paper, we design, analyze and implement LSDedup, which attains a high storage efficiency while providing strictly confidential files (SCFiles) with strong protection. LSDedup allows cloud users to securely interact with cloud servers to check the confidential level of an outsourced file. Users encrypt the SCFiles using standard symmetric encryption algorithms to achieve a high security level, whereas encrypting the less confidential files (LSFiles) using CE such that cloud servers can perform deduplication. LSDedup is designed to prevent cloud servers reporting fake confidential level and a fake file user claiming the ownership of the file. Formal analysis is provided to justify its security. Besides, we implement an LSDedup prototype using Alibaba Cloud as backend storage. Our evaluations demonstrate that LSDedup can work with existing cloud service providers’ APIs and achieves modest performance overhead.},
  archive      = {J_TC},
  author       = {Mingyang Song and Zhongyun Hua and Yifeng Zheng and Hejiao Huang and Xiaohua Jia},
  doi          = {10.1109/TC.2023.3331953},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {422-435},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LSDedup: Layered secure deduplication for cloud storage},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A secure and flexible blockchain-based offline payment
protocol. <em>TC</em>, <em>73</em>(2), 408–421. (<a
href="https://doi.org/10.1109/TC.2023.3331823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-chain transactions seek to address the low on-chain scalability and enable blockchain-based payments over unreliable on-chain networks. The key problem with existing works is that they fail to balance security and flexibility in their designs. These studies would have been more useful if they could provide a sense of security without compromising their flexibility. We hypothesize that two offline parties having loosely synchronized clocks and channels with known bounded latency can conduct off-chain transactions while maintaining a high level of security and flexibility: we introduce a novel blockchain-based offline payment protocol that supports our hypothesis. Our work leverages on-chain smart contracts and offline wallet interactions to build resilience against intermittent on-chain connectivity. Our protocol achieves flexible and trusted computations with the use of platform-agnostic Trusted Execution Environments (TEEs) and open transactions. We empirically evaluate our design over the mainstream Intel Software Guard Extensions (SGX) and compare our protocol with state-of-the-art solutions. We found that our protocol attains high efficiency and exhibits an advanced level of security and flexibility in functionality. We evaluate our construction against several real-world attacks. We prove the security and robustness of our scheme based on a practical universally composable framework with synchronous settings. This work contributes to the existing knowledge of safe and user-friendly offline payment solutions for the blockchain technology.},
  archive      = {J_TC},
  author       = {Wanqing Jie and Wangjie Qiu and Arthur Sandor Voundi Koe and Jianhong Li and Yin Wang and Yaqi Wu and Jin Li and Zhiming Zheng},
  doi          = {10.1109/TC.2023.3331823},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {408-421},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A secure and flexible blockchain-based offline payment protocol},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trading aggregate statistics over private internet of things
data. <em>TC</em>, <em>73</em>(2), 394–407. (<a
href="https://doi.org/10.1109/TC.2023.3331816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of Internet of Things (IoT) data has pushed the boundaries of data analysis, but it has also raised concerns about the increasing commoditization of personal privacy. The intriguing problem of trading private IoT data is the focal point of this paper. We delve into three fundamental questions: Firstly, we determine the minimum privacy that a broker must purchase from data owners to achieve aggregate statistics with a fixed accuracy goal. Secondly, we propose an innovative arbitrage-free pricing framework that empowers brokers to set prices for aggregate statistics, maximizing utility while ensuring fairness. Lastly, we address the challenge of fairly compensating data owners for their privacy losses while providing economic guarantees. To achieve these objectives, we propose a data trading framework in this paper. Our framework considers ubiquitous data correlations and the potential involvement of untrusted brokers, ensuring that the total compensations remain within the brokers’ budget; each owner is guaranteed to receive non-negative revenues, and cunning data owners cannot exploit misreported privacy demands to gain extra compensations. Experiments on the MovieLens dataset demonstrated the robustness of our framework.},
  archive      = {J_TC},
  author       = {Zaobo He and Zhipeng Cai},
  doi          = {10.1109/TC.2023.3331816},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {394-407},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Trading aggregate statistics over private internet of things data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DBMS-assisted live migration of virtual machines.
<em>TC</em>, <em>73</em>(2), 380–393. (<a
href="https://doi.org/10.1109/TC.2023.3329943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live migration of virtual machines (VMs) is a technique that moves active VMs between different physical hosts without losing any running states. Although it is desirable for administrators that the live migration is completed as quickly as possible, the pre-copy-based live migration, widely used in modern hypervisors, does not satisfy this demand on the current trend that VMs on which running applications are performance-critical such as database management systems (DBMSes) have quite large memory. DMigrate , presented in this paper, shortens the time for live-migrating VMs with even large memory DBMSes. To quickly produce the running state of the migrating VMs on the destination, DMigrate performs regular memory transfers while simultaneously constructing the DBMS&#39;s buffer-pool by fetching the data items from the shared storage. We prototyped DMigrate on MySQL 5.7.30, QEMU 5.1.0, and Linux 4.18.20. The experimental results show that the migration time of the prototype is up to 1.71 $\boldsymbol{\times}$ and 1.71 $\boldsymbol{\times}$ shorter under workloads, including sysbench and TPC-C, than the default pre-copy and post-copy schemes, respectively.},
  archive      = {J_TC},
  author       = {Kota Asanuma and Hiroshi Yamada},
  doi          = {10.1109/TC.2023.3329943},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {380-393},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DBMS-assisted live migration of virtual machines},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HASP: Hierarchical asynchronous parallelism for multi-NN
tasks. <em>TC</em>, <em>73</em>(2), 366–379. (<a
href="https://doi.org/10.1109/TC.2023.3329937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of deep learning has propelled many real-world artificial intelligence applications. Many of these applications integrate multiple neural networks (multi-NN) to cater to various functionalities. There are two challenges of multi-NN acceleration: (1) competition for shared resources becomes a bottleneck, and (2) heterogeneous workloads exhibit remarkably different computing-memory characteristics and various synchronization requirements. Therefore, resource isolation and fine-grained resource allocation for each task are two fundamental requirements for multi-NN computing systems. Although a number of multi-NN acceleration technologies have been explored, few can completely fulfill both of these requirements, especially for mobile scenarios. This paper reports a Hierarchical Asynchronous Parallel Model (HASP) to enhance multi-NN performance to meet both requirements. HASP can be implemented on a multicore processor that adopts Multiple Instruction Multiple Data (MIMD) or Single Instruction Multiple Thread (SIMT) architectures, with minor adaptive modification needed. Further, a prototype chip is developed to validate the hardware effectiveness of this design. A corresponding mapping strategy is also developed, allowing the proposed architecture to simultaneously promote resource utilization and throughput. With the same workload, the prototype chip demonstrates 3.62 $\boldsymbol{\times}$ , and 3.51 $\boldsymbol{\times}$ higher throughput over Planaria and 8.68 $\boldsymbol{\times}$ , 2.61 $\boldsymbol{\times}$ over Jetson AGX Orin for MobileNet-V1 and ResNet50, respectively.},
  archive      = {J_TC},
  author       = {Hongyi Li and Songchen Ma and Taoyi Wang and Weihao Zhang and Guanrui Wang and Chenhang Song and Huanyu Qu and Junfeng Lin and Cheng Ma and Jing Pei and Rong Zhao},
  doi          = {10.1109/TC.2023.3329937},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {366-379},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HASP: Hierarchical asynchronous parallelism for multi-NN tasks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ADC-free ReRAM-based in-situ accelerator for
energy-efficient binary neural networks. <em>TC</em>, <em>73</em>(2),
353–365. (<a href="https://doi.org/10.1109/TC.2022.3224800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever-increasing parameter size of deep learning models, conventional ASIC-based accelerators in mobile environments suffer from low energy budget due to limited memory capacity and frequent data movements. Binary neural networks (BNNs) deployed in ReRAM-based in-situ accelerators provide a promising solution, and various related architectures have been proposed recently. However, their performances are largely compromised by the tremendous cost of domain conversion via analog-to-digital converters (ADCs), essential for mixed-signal processing in ReRAM. This article identifies two root causes of the need for such ADCs and proposes effective solutions to address them. First, we minimize redundant operations in BNNs and reduce the number of ReRAM arrays with ADCs approximately by half. We also propose a partial-sum range adjustment technique based on a layer remapping to deal with the remaining ADCs. Proper handling of the partial-sum distribution allows ReRAM-based in-situ processing without domain conversion, completely bypassing the need for ADCs. Experimental results show that the proposed architecture achieves a 3.44x speedup and 91.5% energy savings, making it an attractive solution for on-device AI at the edge.},
  archive      = {J_TC},
  author       = {Hyeonuk Kim and Youngbeom Jung and Lee-Sup Kim},
  doi          = {10.1109/TC.2022.3224800},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {353-365},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ADC-free ReRAM-based in-situ accelerator for energy-efficient binary neural networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STR: Secure computation on additive shares using the
share-transform-reveal strategy. <em>TC</em>, <em>73</em>(2), 340–352.
(<a href="https://doi.org/10.1109/TC.2021.3073171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of cloud computing probably benefits many of us while the privacy risks brought by semi-honest cloud servers have aroused the attention of more and more people and legislatures. In the last two decades, plenty of works seek to outsource various specific tasks to servers while ensuring the security of private data. The tasks to be outsourced are countless; however, the computations involved are similar. In this article, we construct a series of novel protocols that support the secure computation of various functions on numbers (e.g., the basic elementary functions) and matrices (e.g., the calculation of eigenvectors and eigenvalues) on arbitrary $n\geq 2$ servers. All protocols only require constant rounds of interactions and achieve low computation complexity. Moreover, the proposed $n$ -party protocols ensure the security of private data even though $n-1$ servers collude. The convolutional neural network models are utilized as the case studies to verify the protocols. The theoretical analysis and experimental results demonstrate the correctness, efficiency, and security of the proposed protocols.},
  archive      = {J_TC},
  author       = {Zhihua Xia and Qi Gu and Wenhao Zhou and Lizhi Xiong and Jian Weng and Naixue Xiong},
  doi          = {10.1109/TC.2021.3073171},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {340-352},
  shortjournal = {IEEE Trans. Comput.},
  title        = {STR: Secure computation on additive shares using the share-transform-reveal strategy},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AUV surfacing control with adversarial attack against DLaaS
framework. <em>TC</em>, <em>73</em>(2), 327–339. (<a
href="https://doi.org/10.1109/TC.2021.3072072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we consider surfacing control problems for the autonomous underwater vehicle in three-dimensional space under emergencies. Most model-based controllers cannot effectively solve these problems due to the unknown task environment model. Moreover, existing deep reinforcement learning (DRL) methods have some limitations, such as slow convergence or overestimation bias. For these purposes, we propose a model-free DRL algorithm based on the Deep Deterministic Policy Gradient within the paradigm of deep learning as a service (DLaaS). The algorithm combines existing expert episodes as demonstration and transitions from the interaction between the agent and task environment, which potentially reduce risk from adversarial attack. We propose a variable delay learning mechanism to improve the performance of the proposed algorithm. The simulation results show that our method can converge faster and has a more robust performance with adversarial attack than the existing DRL method under the premise of solving the AUV surfacing control tasks for safety.},
  archive      = {J_TC},
  author       = {Tianze Zhang and Xuhong Miao and Yibin Li and Lei Jia and Yinghao Zhuang},
  doi          = {10.1109/TC.2021.3072072},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {327-339},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AUV surfacing control with adversarial attack against DLaaS framework},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating adversarial attacks based on denoising &amp;
reconstruction with finance authentication system case study.
<em>TC</em>, <em>73</em>(2), 314–326. (<a
href="https://doi.org/10.1109/TC.2021.3066614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques were widely adopted in various scenarios as a service. However, they are found naturally exposed to adversarial attacks. Such imperceptible-perturbation-based attacks can cause severe damage in nowaday authentication systems that adopt DNNs as the core, such as fingerprint liveness detection systems, face recognition systems, etc. This article avoids improving the model’s robustness and realizes the defense against adversarial attacks based on denoising and reconstruction. Our proposed method can be viewed as a two-step defense framework. The first step denoises the input adversarial example, then reconstructing the sample to close to the original clean image and help the target model output the original label. The proposed method is evaluated using six kinds of state-of-art adversarial attacks, including the adaptive attacks, which are known as the strongest attacks. We also specifically focus on demonstrating the effectiveness of our proposed work in Finance Authentication systems as a real-life case study. Experimental results reveal that our method is more robust than the previous super-resolution-only defense in respect of attaining a higher averaging accuracy over clean and distorted samples. To the best of our knowledge, it’s the first work that reveals a comprehensive defense framework against adversarial attacks over Finance Authentication systems.},
  archive      = {J_TC},
  author       = {Juzhen Wang and Yiqi Hu and Yiren Qi and Ziwen Peng and Changjia Zhou},
  doi          = {10.1109/TC.2021.3066614},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {314-326},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Mitigating adversarial attacks based on denoising &amp; reconstruction with finance authentication system case study},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FAWA: Fast adversarial watermark attack. <em>TC</em>,
<em>73</em>(2), 301–313. (<a
href="https://doi.org/10.1109/TC.2021.3065172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, adversarial attacks have shown to lead the state-of-the-art deep neural networks (DNNs) to misclassification. However, most adversarial attacks are generated according to whether they are perceptual to human visual system, measured by geometric metrics such as the $\ell _2$ -norm, which ignores the common watermarks in cyber-physical systems. In this article, we propose a fast adversarial watermark attack (FAWA) method based on fast differential evolution technique, which optimally superimposes a watermark on an image to fool DNNs. We also attempt to explain the reason why the attack is successful and propose two hypotheses on the vulnerability of DNN classifiers and the influence of the watermark attack on higher-layer features extraction respectively. In addition, we propose two countermeasure methods against FAWA based on random rotation and median filtering respectively. Experimental results show that our method achieves 41.3 percent success rate in fooling VGG-16 and have good transferability. Our approach is also shown to be effective in deceiving deep learning as a service (DLaaS) systems as well as the physical world. The proposed FAWA, hypotheses, and the countermeasure methods, provide a timely help for DNN designers to gain some knowledge of model vulnerability while designing DNN classifiers and related DLaaS applications.},
  archive      = {J_TC},
  author       = {Hao Jiang and Jintao Yang and Guang Hua and Lixia Li and Ying Wang and Shenghui Tu and Song Xia},
  doi          = {10.1109/TC.2021.3065172},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {301-313},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FAWA: Fast adversarial watermark attack},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified digit selection for radix-4 recurrence division and
square root. <em>TC</em>, <em>73</em>(1), 292–300. (<a
href="https://doi.org/10.1109/TC.2023.3305760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Division and square root are fundamental operations required by most computer systems. They are commonly implemented in hardware using radix-4 recurrence, which produces a 2-bit result digit on each step. Unified digit selection logic chooses the next quotient or square root digit based on a residual and divisor or square root approximation. This paper presents the first derivation of digit selection constants for unified radix-4 recurrence division and square root.},
  archive      = {J_TC},
  author       = {David Harris and James Stine and Miloš Ercegovac and Alberto Nannarelli and Katherine Parry and Cedar Turek},
  doi          = {10.1109/TC.2023.3305760},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {292-300},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Unified digit selection for radix-4 recurrence division and square root},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correct-by-construction design of custom accelerator
microarchitectures. <em>TC</em>, <em>73</em>(1), 278–291. (<a
href="https://doi.org/10.1109/TC.2023.3329243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern application-specific System-on-Chip designs include a variety of accelerator blocks that customize microcontrollers with domain-specific instruction sets and optimized microarchitectures. Unfortunately, accelerator implementations can be highly error-prone, undermining the reliability and security of the entire system. In spite of recent successes in formal methods, full verification of a complex accelerator microarchitecture is still beyond the scope of state-of-the-art formal technologies. In this paper, we address this problem through a novel methodology for incremental verification that can be tightly integrated with the design process. Our approach depends on a new foundation for microarchitecture correctness that enables viewing microarchitecture features as program transformations in a compiler design. The foundations enable designing microarchitecture features as incremental, semantics-preserving optimizations. We show how to use the foundations to develop correct-by-construction implementations of various advanced features of modern microprocessors. We demonstrate the viability of the foundations in designing correct-by-construction methodology for a superscalar microarchitectural implementation of the Versatile Tensor Accelerator.},
  archive      = {J_TC},
  author       = {Jin Yang and Zhenkun Yang and Jeremy Casas and Sandip Ray},
  doi          = {10.1109/TC.2023.3329243},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {278-291},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Correct-by-construction design of custom accelerator microarchitectures},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A high-performance, energy-efficient modular DMA engine
architecture. <em>TC</em>, <em>73</em>(1), 263–277. (<a
href="https://doi.org/10.1109/TC.2023.3329930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data transfers are essential in today&#39;s computing systems as latency and complex memory access patterns are increasingly challenging to manage. Direct memory access engines (DMAES) are critically needed to transfer data independently of the processing elements, hiding latency and achieving high throughput even for complex access patterns to high-latency memory. With the prevalence of heterogeneous systems, DMAEs must operate efficiently in increasingly diverse environments. This work proposes a modular and highly configurable open-source DMAE architecture called intelligent DMA (iDMA), split into three parts that can be composed and customized independently. The front-end implements the control plane binding to the surrounding system. The mid-end accelerates complex data transfer patterns such as multi-dimensional transfers, scattering, or gathering. The back-end interfaces with the on-chip communication fabric (data plane). We assess the efficiency of iDMA in various instantiations: In high-performance systems, we achieve speedups of up to 15.8 $\boldsymbol{\times}$ with only 1% additional area compared to a base system without a DMAE. We achieve an area reduction of 10% while improving ML inference performance by 23% in ultra-low-energy edge AI systems over an existing DMAE solution. We provide area, timing, latency, and performance characterization to guide its instantiation in various systems.},
  archive      = {J_TC},
  author       = {Thomas Benz and Michael Rogenmoser and Paul Scheffler and Samuel Riedel and Alessandro Ottaviano and Andreas Kurth and Torsten Hoefler and Luca Benini},
  doi          = {10.1109/TC.2023.3329930},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {263-277},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A high-performance, energy-efficient modular DMA engine architecture},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero and narrow-width value-aware compression for quantized
convolutional neural networks. <em>TC</em>, <em>73</em>(1), 249–262. (<a
href="https://doi.org/10.1109/TC.2023.3315051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks are normally used in systems with dedicated neural processing units for CNN-related computations. For high performance and low hardware overheads, CNN datatype quantization is applied. As an additional optimization, to further reduce DRAM accesses, compression algorithms have been used for CNN data. However, conventional zero value-aware compression algorithms suffer from a reduction in compression ratio with the latest quantized CNNs, owing to the small number of zero values. Moreover, the appropriate zero run-length code width can be changed dynamically based on the CNNs, layers, and quantization datatypes. As another compressible data value for increasing the compression ratio, the latest quantized CNNs have many narrow-width values. Because low-precision quantization reduces the data bit width, CNN data are gathered into a few discrete values and incur a biased data distribution. These discrete values become narrow-width values, and constitute a large proportion of the biased distribution. In this article, we propose an efficient compression algorithm for quantized CNNs, ENCORE, which utilizes variable zero run-length encoding and compresses narrow-width values. With the latest quantized CNNs, ENCORE shows higher compression ratios, 93.55% and 50.85% in Mobilenet v1 and Tiny YOLO v3, respectively, than conventional zero value-aware CNN data compression algorithms.},
  archive      = {J_TC},
  author       = {Myeongjae Jang and Jinkwon Kim and Haejin Nam and Soontae Kim},
  doi          = {10.1109/TC.2023.3315051},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {249-262},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Zero and narrow-width value-aware compression for quantized convolutional neural networks},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SafeDRL: Dynamic microservice provisioning with reliability
and latency guarantees in edge environments. <em>TC</em>,
<em>73</em>(1), 235–248. (<a
href="https://doi.org/10.1109/TC.2023.3329194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a key technology of 5G, network function virtualization enables each monolithic service to be divided into microservices, facilitating their deployment and management in edge environments. One of the most critical issues in 5G is how to support dynamically arriving mission-critical services with low-latency and high-reliability requirements in distributed edge environments. However, most existing works focus on how to provide reliable services without considering latency, and their heuristics struggle to cope with high-dimensional constraints and complex environments with heterogeneous infrastructure and services. In this paper, we propose a SafeDRL algorithm to resource-efficiently support these dynamically arriving services while meeting their reliability and latency requirements. Specifically, we first formulate the problem as an integer nonlinear programming and prove its NP-hardness. To tackle this problem, our SafeDRL algorithm captures delayed rewards in dynamic environments by reinforcement learning, and corrects constraint violations with high-quality feasible solutions based on expert intervention, and prunes unnecessary backup instances for optimality. The algorithm is proved to have a bounded approximation ratio in general cases. Extensive trace-driven simulations show that, compared with the state-of-the-art solution, SafeDRL can save resource costs by up to 49.32% and improve the service acceptance ratio by up to 55% with acceptable execution time.},
  archive      = {J_TC},
  author       = {Yue Zeng and Zhihao Qu and Song Guo and Baoliu Ye and Jie Zhang and Jing Li and Bin Tang},
  doi          = {10.1109/TC.2023.3329194},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {235-248},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SafeDRL: Dynamic microservice provisioning with reliability and latency guarantees in edge environments},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyclebite: Extracting task graphs from unstructured
compute-programs. <em>TC</em>, <em>73</em>(1), 221–234. (<a
href="https://doi.org/10.1109/TC.2023.3327504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting portable performance in an application requires structuring that program into a data-flow graph of coarse-grained tasks (CGTs). Structuring applications that interconnect multiple external libraries and custom code (i.e., “Code From The Wild” (CFTW)) is challenging. When experts manually restructure a program, they trivialize the extraction of structure; however, this expertise is not broadly available. Automatic structuring approaches focus on the intersection of hot code and static loops, ignoring the data dependencies between tasks and significantly reducing the scope of analyzeable programs. This work addresses the problem of extracting the data-flow graph of CGTs from CFTW. To that end, we present Cyclebite. Our approach extracts CGTs from unstructured compute-programs by detecting CGT candidates in the simplified Markov Control Graph (MCG), and localizing CGTs in an epoch profile. Additionally, the epoch profile extracts the data dependence between CGTs required to build the data-flow graph of CGTs. Cyclebite demonstrates a robust selectivity for critical CGTs relative to the state-of-the-art (SoA), leading to a potential speedup of 12x on average and thread-scaling of 24x on average compared to modern compiler optimizers. We validate the results of Cyclebite and compare them to two SoA techniques using an input corpus of 25 open-source C/C++ libraries with 2,019 unique execution profiles.},
  archive      = {J_TC},
  author       = {Benjamin R. Willis and Aviral Shrivastava and Joshua Mack and Shail Dave and Chaitali Chakrabarti and John Brunhaver},
  doi          = {10.1109/TC.2023.3327504},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {221-234},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Cyclebite: Extracting task graphs from unstructured compute-programs},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SplitDB: Closing the performance gap for LSM-tree-based
key-value stores. <em>TC</em>, <em>73</em>(1), 206–220. (<a
href="https://doi.org/10.1109/TC.2023.3326982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log Structured Merge Tree (LSM tree) serves as the core data storage engine in modern key-value stores. Its adoption is rapidly accelerated with cloud computing and data center development. Acknowledging its widespread use, the LSM tree still faces severe performance issues such as write stall, write amplification, and read inefficiency. This article presents research on improving LSM-tree-based key-value store performance using emerging Non-Volatile Memory (NVM) technology. Our performance diagnosis reveals that the above-mentioned issues result primarily from intensive hot key-value data processing, which is compounded by slow storage devices. To address hotspot bottlenecks, we propose a split log-structured merge tree over hybrid storage by leveraging the intrinsic hot and cold data separation property of the LSM tree. Our approach promotes frequently accessed, small-sized high levels onto fast NVM and offloads the remaining cold, large-sized low levels into slow devices, effectively closing the performance gap for DRAM-disk-based LSM trees. Additionally, we optimize the split LSM tree read and write performance by proposing a variety of novel techniques. We build a hotspot-aware key-value database named SplitDB and perform extensive experiments. Experimental results demonstrate that SplitDB effectively prevents write stalls, achieves a 6-fold write reduction, and improves read throughputs by 3.5 times compared to state-of-the-art key-value databases.},
  archive      = {J_TC},
  author       = {Miao Cai and Xuzhen Jiang and Junru Shen and Baoliu Ye},
  doi          = {10.1109/TC.2023.3326982},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {206-220},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SplitDB: Closing the performance gap for LSM-tree-based key-value stores},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid edge-cloud collaborator resource scheduling approach
based on deep reinforcement learning and multiobjective optimization.
<em>TC</em>, <em>73</em>(1), 192–205. (<a
href="https://doi.org/10.1109/TC.2023.3326977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative resource scheduling between edge terminals and cloud centers is regarded as a promising means of effectively completing computing tasks and enhancing quality of service. In this paper, to further improve the achievable performance, the edge cloud resource scheduling (ECRS) problem is transformed into a multi-objective Markov decision process based on task dependency and features extraction. A multi-objective ECRS model is proposed by considering the task completion time, cost, energy consumption and system reliability as the four objectives. Furthermore, a hybrid approach based on deep reinforcement learning (DRL) and multi-objective optimization are employed in our work. Specifically, DRL preprocesses the workflow, and a multi-objective optimization method strives to find the Pareto-optimal workflow scheduling decision. Various experiments are performed on three real data sets with different numbers of tasks. The results obtained demonstrate that the proposed hybrid DRL and multi-objective optimization design outperforms existing design approaches.},
  archive      = {J_TC},
  author       = {Jiangjiang Zhang and Zhenhu Ning and Muhammad Waqas and Hisham Alasmary and Shanshan Tu and Sheng Chen},
  doi          = {10.1109/TC.2023.3326977},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {192-205},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hybrid edge-cloud collaborator resource scheduling approach based on deep reinforcement learning and multiobjective optimization},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient and secure data sharing scheme for edge-enabled
IoT. <em>TC</em>, <em>73</em>(1), 178–191. (<a
href="https://doi.org/10.1109/TC.2023.3325668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharing the big data generated by IoT via cloud is slow and expensive. Besides, transmitting and sharing data among IoT devices via cloud may be insecure. To address these issues, a novel efficient and secure data sharing scheme termed EB-SDSS (Edge Blockchain Secure Data Sharing Scheme) is proposed in this paper for edge-enabled IoT applications. EB-SDSS constructs a blockchain on edge servers. It guarantees the confidentiality and unforgeability of data by combining the symmetric encryption scheme with an edge blockchain. To ensure the device authenticity and the reliability of shared data, EB-SDSS adopts a certificateless signature scheme. It also provides efficient large-scale data searches for IoT devices through a locality-sensitive hashing algorithm. EB-SDSS has been proven to be secure against the adaptive chosen message attacks under the random oracle model. The experimental results indicate that EB-SDSS is feasible for IoT inter-device data sharing.},
  archive      = {J_TC},
  author       = {Jiguo Yu and Biwei Yan and Huayi Qi and Shengling Wang and Wei Cheng},
  doi          = {10.1109/TC.2023.3325668},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {178-191},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An efficient and secure data sharing scheme for edge-enabled IoT},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient deep reinforcement learning-based automatic
cache replacement policy in cloud block storage systems. <em>TC</em>,
<em>73</em>(1), 164–177. (<a
href="https://doi.org/10.1109/TC.2023.3325625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of cloud services, cloud block storage (CBS) systems have been widely deployed by cloud providers. Cloud cache plays a vital role in maintaining high and stable performance in cloud block storage systems. In the past few decades, much research has been conducted on the design of cache replacement policies. Prior work frequently relies on manually-engineered heuristics to capture the most common cache access patterns, or predict the reuse distance and try to identify the blocks that are either cache-friendly or cache-averse. Researchers are now applying recent advances in machine learning to guide cache replacement policy, augmenting or replacing traditional heuristics and data structures. However, most existing approaches depend on a certain environment which restricted their application, e.g., some methods only consider the on-chip cache consisting of program counters (PCs). Moreover, those approaches with attractive hit rates are usually unable to deal with modern irregular workloads, due to the limited feature used. In contrast, we propose a cloud cache replacement framework to automatically learn the relationship between the probability distribution of different replacement policies and workload distribution by using deep reinforcement learning. We train an end-to-end cache replacement policy based on the requested address with two efficient and stable cache replacement policies. Furthermore, by using prioritized experience replay and setting parameter constraints, our framework can accelerate the offline training process without affecting the cloud application. We have evaluated our proposed framework by using block-based I/O traces collected from Alibaba Cloud and Tencent Cloud, two of the largest cloud providers in the world, and several open-source traces. Experimental results show that our method not only outperforms several state-of-the-art cache methods in hit rate, but also reduces request latency and data traffic to the backend storage.},
  archive      = {J_TC},
  author       = {Yang Zhou and Fang Wang and Zhan Shi and Dan Feng},
  doi          = {10.1109/TC.2023.3325625},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {164-177},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An efficient deep reinforcement learning-based automatic cache replacement policy in cloud block storage systems},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bit-balance: Model-hardware codesign for accelerating NNs by
exploiting bit-level sparsity. <em>TC</em>, <em>73</em>(1), 152–163. (<a
href="https://doi.org/10.1109/TC.2023.3324477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bit-serial architectures can handle Neural Networks (NNs) with different weight precision, achieving higher resource efficiency compared with bit-parallel architectures. Besides, the weights contain abundant zero bits owing to the fault tolerance of NNs, indicating that bit sparsity of NNs can be further exploited for performance improvement. However, the irregular proportion of zero bits in each weight causes imbalanced workloads in the Processing Element (PE) array, which degrades performance or induces overhead for sparse processing. Thus, this article proposed a channel-wise bit-sparsity quantization method that keeps the non-zero bit number of each weight in each channel from exceeding a certain threshold and clusters the channels with the same threshold to balance the workloads in PE array with little accuracy loss. Then, we co-designed a sparse bit-serial architecture, called Bit-balance, to improve overall performance, supporting weight-bit sparsity and adaptive bitwidth computation. The whole design was implemented with 65 nm technology at 1 GHz and performs at 447-, 37-, 59-, 240-, and 19-frame/s for AlexNet, VGG-16, ResNet-50, GoogleNet, and Yolo-v3 respectively. Compared with sparse bit-serial accelerator, Bitlet, Bit-balance achieves 1.6 $\boldsymbol{\times}$ 2.1 $\boldsymbol{\times}$ energy efficiency (frame/J) and 2.3 $\boldsymbol{\times}$ 3.6 $\boldsymbol{\times}$ resource efficiency (frame/mm ${}^{\mathbf{2}}$ ).},
  archive      = {J_TC},
  author       = {Wenhao Sun and Zhiwei Zou and Deng Liu and Wendi Sun and Song Chen and Yi Kang},
  doi          = {10.1109/TC.2023.3324477},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {152-163},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Bit-balance: Model-hardware codesign for accelerating NNs by exploiting bit-level sparsity},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling HW-based task scheduling in large multicore
architectures. <em>TC</em>, <em>73</em>(1), 138–151. (<a
href="https://doi.org/10.1109/TC.2023.3323781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Task Scheduling is an enticing programming model aiming to ease the development of parallel programs with intrinsically irregular or data-dependent parallelism. The performance of such solutions relies on the ability of the Task Scheduling HW/SW stack to efficiently evaluate dependencies at runtime and schedule work to available cores. Traditional SW-only systems implicate scheduling overheads of around 30K processor cycles per task, which severely limit the ( core count , task granularity ) combinations that they might adequately handle. Previous work on HW-accelerated Task Scheduling has shown that such systems might support high performance scheduling on processors with up to eight cores, but questions remained regarding the viability of such solutions to support the greater number of cores now frequently found in high-end SMP systems. The present work presents an FPGA-proven, tightly-integrated, Linux-capable, 30-core RISC-V system with hardware accelerated Task Scheduling. We use this implementation to show that HW Task Scheduling can still offer competitive performance at such high core count, and describe how this organization includes hardware and software optimizations that make it even more scalable than previous solutions. Finally, we outline ways in which this architecture could be augmented to overcome inter-core communication bottlenecks, mitigating the cache-degradation effects usually involved in the parallelization of highly optimized serial code.},
  archive      = {J_TC},
  author       = {Lucas Morais and Carlos Álvarez and Daniel Jiménez-González and Juan Miguel de Haro and Guido Araujo and Michael Frank and Alfredo Goldman and Xavier Martorell},
  doi          = {10.1109/TC.2023.3323781},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {138-151},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling HW-based task scheduling in large multicore architectures},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reliability-critical path identifying method with local
and global adjacency probability matrix in combinational circuits.
<em>TC</em>, <em>73</em>(1), 123–137. (<a
href="https://doi.org/10.1109/TC.2023.3323772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient identification of reliability-critical paths (RCPs) not only facilitates fault localization and troubleshooting but also allows circuit designers to improve circuit reliability at a low cost. This article proposes a local and global adjacency probability matrix-based approach (LGAPM) to quickly and efficiently identify RCPs of combinational logic circuits. The approach reflects the criticality of the overall reliability of the circuit as well as the local criticality of gates in the path. In addition, we design a pruning-based method to accelerate RCP identification in large-scale circuits. The experimental results of the LGAPM on all 74 series circuits, ISCAS-85, and partial EPFL benchmark circuits show that the 74181 circuit with a minimum of 17 paths and the EPFL-remainder10 circuit with a maximum of 8.081 × 10 8 paths take times of about 0.18s and 33931.04s, respectively. The average accuracy on small and medium-scale circuits is 94.24%, and the average stability on all-size circuits is 86.19%. Compared to the SAT-based method, hill-climbing algorithm, and random method, LGAPM’s metrics are superior and more appropriate for large-scale circuits. The overall circuit reliability can be improved from 0.7726 to 0.9238 on average by hardening a tiny number of gates in the identified the most RCPs and the average cost savings is 4.08 times over random hardening methods.},
  archive      = {J_TC},
  author       = {Zhanhui Shi and Jie Xiao and Weidong Zhu and Jianhui Jiang},
  doi          = {10.1109/TC.2023.3323772},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {123-137},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A reliability-critical path identifying method with local and global adjacency probability matrix in combinational circuits},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computation off-loading in resource-constrained edge
computing systems based on deep reinforcement learning. <em>TC</em>,
<em>73</em>(1), 109–122. (<a
href="https://doi.org/10.1109/TC.2023.3321938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a computational paradigm that brings resources closer to the network edge, such as base stations or gateways, in order to provide quick and efficient computing services for mobile devices while relieving pressure on the core network. However, the current computing power of edge servers are insufficient to handle the high number of tasks generated by access devices. Additionally, some mobile devices may not fully utilize their computing resources. To maximize the use of resources, we propose a novel edge computing system architecture consisting of a resource-constrained edge server and three computing groups. Tasks from each group can be offloaded to either the edge server or the corresponding computing group for execution. We focus on optimizing the computation offloading of devices to minimize the maximum overall task processing latency in the system. This problem is proved to be NP-hard. To solve it, we propose a DQN-based resource utilization task scheduling (DQNRTS) algorithm that has two desirable characteristics: 1) it effectively utilizes the computing resources in the system and 2) it uses deep reinforcement learning to make intelligent scheduling decisions based on system state information. Experimental results demonstrate that the DQNRTS algorithm is capable of reducing the processing latency of the system by converging to optimal solutions.},
  archive      = {J_TC},
  author       = {Chuanwen Luo and Jian Zhang and XiaoLu Cheng and Yi Hong and Zhibo Chen and Xiaoshuang Xing},
  doi          = {10.1109/TC.2023.3321938},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {109-122},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Computation off-loading in resource-constrained edge computing systems based on deep reinforcement learning},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Split-radix based compact hardware architecture for
CRYSTALS-kyber. <em>TC</em>, <em>73</em>(1), 97–108. (<a
href="https://doi.org/10.1109/TC.2023.3320040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facing the threat of large-scale quantum computers to traditional public-key cryptography, the National Institute of Standards and Technology has conducted Post-Quantum Cryptography algorithms evaluation for a long time, and CRYSTALS-Kyber has been selected to enter the standardization process. In the previous literature, hardware designs can significantly improve the performance of CRYSTALS-Kyber, and the most time-consuming operations are Number Theoretic Transform (NTT) and point-wise multiplication (PWM). However, the split-radix algorithm, which has a lower theoretical complexity in the FFT, has rarely been studied in the NTT. In this paper, we studied whether there are advantages of introducing split-radix algorithms into the NTT defined by CRYSTALS-Kyber and detailed derived the split-radix algorithms for the forward and inverse NTT without pre- or post-processing. By further optimizing the split-radix algorithm for the forward NTT, one of the three modular multipliers in the $\boldsymbol{L}$ -shaped butterfly unit is replaced by shifting-and-addition, which will reduce the hardware resource consumption. Besides, we proposed a recombined formula for PWM, which reduces the capacity of the intermediate data RAM for PWM by 25%. Together with the proposed hardware scheduling method, the above algorithms can improve performance and save hardware resources.},
  archive      = {J_TC},
  author       = {Wenbo Guo and Shuguo Li},
  doi          = {10.1109/TC.2023.3320040},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {97-108},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Split-radix based compact hardware architecture for CRYSTALS-kyber},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). General bootstrapping approach for RLWE-based homomorphic
encryption. <em>TC</em>, <em>73</em>(1), 86–96. (<a
href="https://doi.org/10.1109/TC.2023.3318405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic Encryption (HE) makes it possible to compute on encrypted data without decryption. In lattice-based HE, a ciphertext contains noise, which accumulates along with homomorphic computations. Bootstrapping refreshes the noise and it is possible to perform arbitrary-depth computations on HE with bootstrapping, which we call Fully Homomorphic Encryption (FHE). In this article, we propose a new general bootstrapping technique for RLWE-based schemes and its practical instantiation for FHE. It can be applied to all three RLWE-based leveled FHE schemes: Brakerski-Gentry-Vaikuntanathan (BGV), Brakerski/Fan-Vercauteren (BFV), and Cheon-Kim-Kim-Song (CKKS) with minor deviations in the algorithms. Our new construction of bootstrapping extracts a noiseless ciphertext for a part of the input, scales it, and finally removes it. In contrast with previous bootstrapping algorithms, the proposed method consumes only 1–2 levels and uses smaller parameters. For BGV and BFV, our new bootstrapping does not have any restrictions on a plaintext modulus unlike typical cases of the previous methods. The error introduced by our approach for CKKS is comparable to a rescaling error, allowing us to preserve a large amount of precision after bootstrapping.},
  archive      = {J_TC},
  author       = {Andrey Kim and Maxim Deryabin and Jieun Eom and Rakyong Choi and Yongwoo Lee and Whan Ghang and Donghoon Yoo},
  doi          = {10.1109/TC.2023.3318405},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {86-96},
  shortjournal = {IEEE Trans. Comput.},
  title        = {General bootstrapping approach for RLWE-based homomorphic encryption},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applying delta compression to packed datasets for efficient
data reduction. <em>TC</em>, <em>73</em>(1), 73–85. (<a
href="https://doi.org/10.1109/TC.2023.3318404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backup systems often adopt deduplication techniques for data reduction. Real-world backup products often group files into larger units (called packed files) before deduplicating them. The grouping entails inserting metadata immediately before the contents of each file in the packed file. Some metadata change with every backup, producing substantial similar (non-duplicate) chunks. Delta compression can remove redundancy among those similar chunks but cannot be applied to HDD-based backup storage because I/Os required for fetching base chunks result in severe throughput loss. For packed datasets, some duplicate chunks, called persistent fragmented chunks (PFCs), are rewritten every backup. We observe that corresponding chunk pairs surrounding identical PFCs are non-identical due to different metadata but similar to each other. In this article, we propose PFC-delta to perform high-performance delta compression for the aforementioned similar chunks on top of deduplication. PFC-delta identifies and prefetches potential base chunks stored along with PFCs by piggybacking on the routine I/Os during deduplication, thus avoiding extra I/Os. We also propose a hash-less delta encoding approach to reduce extra computational overheads. Evaluation results with four real-world datasets show that PFC-delta improves both compression ratio and restore performance, while increasing the backup throughput on all but one datasets.},
  archive      = {J_TC},
  author       = {Yucheng Zhang and Hong Jiang and Chunzhi Wang and Wei Huang and Meng Chen and Yongxuan Zhang and Le Zhang},
  doi          = {10.1109/TC.2023.3318404},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {73-85},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Applying delta compression to packed datasets for efficient data reduction},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A secure and robust knowledge transfer framework via
stratified-causality distribution adjustment in intelligent
collaborative services. <em>TC</em>, <em>73</em>(1), 58–72. (<a
href="https://doi.org/10.1109/TC.2023.3318403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of device-edge-cloud collaborative computing techniques has actively contributed to the popularization and application of intelligent service models. The intensity of knowledge transfer plays a vital role in enhancing the performance of intelligent services. However, the existing knowledge transfer methods are mainly implemented through data fine-tuning and model distillation, which may cause the leakage of data privacy or model copyright in intelligent collaborative systems. To address this issue, we propose a secure and robust knowledge transfer framework through stratified-causality distribution adjustment (SCDA) for device-edge-cloud collaborative services. Specifically, a simple yet effective density-based estimation is first employed to obtain uncertainty scores that guide the space stratification, which is conducive to reconstructing low-density distribution regions from high-density distribution regions more adaptively and accurately. Subsequently, we devise a novel causality-aware generative model to generate synthetic features for the out-of-distribution domain by exploring the relationship between factors and variables. Ultimately, we introduce a cycle-consistent minimax optimization mechanism to ensure the effectiveness and dependability of knowledge transfer through the influence minimization and the diversity maximization. Furthermore, extensive experiments demonstrate that our scheme can protect the security of data privacy and model copyright in intelligent collaborative services through adaptive distribution adjustment.},
  archive      = {J_TC},
  author       = {Ju Jia and Siqi Ma and Lina Wang and Yang Liu and Robert H. Deng},
  doi          = {10.1109/TC.2023.3318403},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {58-72},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A secure and robust knowledge transfer framework via stratified-causality distribution adjustment in intelligent collaborative services},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward an SGX-friendly java runtime. <em>TC</em>,
<em>73</em>(1), 44–57. (<a
href="https://doi.org/10.1109/TC.2023.3318400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware enclaves assist in constructing a trusted execution environment (TEE) to store private code and data and thus become an appealing solution to enhance applications’ security. Nevertheless, state-of-the-art enclave implementations like Intel Software Guard Extensions (SGX) have severe performance issues and hinder the deployment of more complicated applications, especially those written in high-level languages like Java. To reduce the performance overhead, prior work has partitioned applications or rebuilt lightweight language runtimes, but they either require manual labor from developers or fail to provide full-fledged support for existing applications. This work instead provides SAJ , a runtime built upon a full-fledged Java virtual machine (JVM) and thus requires no modifications to applications. SAJ first analyzes the performance of vanilla JVMs running in enclaves and finds that the memory management overhead and boot phase are culprits for performance slowdown. For memory management, SAJ introduces SGX-aware heap layout and garbage collector, which reduces both GC and application execution time. As for the boot phase, SAJ introduces an address-conscious launching mechanism to improve the boot performance. The evaluation under representative Java applications shows that SAJ can reduce the overall GC pause time, application time, and boot time by 2.93 $\boldsymbol{\times}$ , 2.58 $\boldsymbol{\times}$ , and 2.73 $\boldsymbol{\times}$ on average, respectively.},
  archive      = {J_TC},
  author       = {Mingyu Wu and Zhe Li and Haibo Chen and Binyu Zang and Shaojun Wang and Lei Yu and Sanhong Li and Haitao Song},
  doi          = {10.1109/TC.2023.3318400},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {44-57},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Toward an SGX-friendly java runtime},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASHL: An adaptive multi-stage distributed deep learning
training scheme for heterogeneous environments. <em>TC</em>,
<em>73</em>(1), 30–43. (<a
href="https://doi.org/10.1109/TC.2023.3315847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increment of data sets and models sizes, distributed deep learning has been proposed to accelerate training and improve the accuracy of DNN models. The parameter server framework is a popular collaborative architecture for data-parallel training, which works well for homogeneous environments by properly aggregating the computation/communication capabilities of different workers. However, in heterogeneous environments, the resources of different workers vary a lot. Some stragglers may seriously limit the whole speed, which impacts the overall training process. In this paper, we propose an adaptive multi-stage distributed deep learning training framework, named ASHL, for heterogeneous environments. First, a profiling scheme is proposed to capture the capabilities of each worker to reasonably plan the training and communication tasks on each worker, and lay the foundation for the formal training. Second, a hybrid-mode training scheme (i.e., coarse-grained and fined-grained training) is proposed to balance the model accuracy and training speed. The coarse-grained training scheme (named AHL) adopts an asynchronous communication strategy, which involves less frequent communications. Its main goal is to make the model quickly converge to a certain level. The fine-grained training stage (named SHL) uses a semi-asynchronous communication strategy and adopts a high communication frequency. Its main goal is to improve the model convergence effect. Finally, a compression-based communication scheme is proposed to further increase the communication efficiency of the training process. Our experimental results show that ASHL reduces the overall training time by more than 35% to converge to the same degree and has better generalization ability compared with state-of-the-art schemes like ADSP.},
  archive      = {J_TC},
  author       = {Zhaoyan Shen and Qingxiang Tang and Tianren Zhou and Yuhao Zhang and Zhiping Jia and Dongxiao Yu and Zhiyong Zhang and Bingzhe Li},
  doi          = {10.1109/TC.2023.3315847},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {30-43},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ASHL: An adaptive multi-stage distributed deep learning training scheme for heterogeneous environments},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisit and benchmarking of automated quantization toward
fair comparison. <em>TC</em>, <em>73</em>(1), 18–29. (<a
href="https://doi.org/10.1109/TC.2023.3315836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated quantization has emerged as an entirely new design paradigm to automate the optimal configuration of bitwidth for deep neural networks (DNNs), making the DNN more memory-efficient and faster to execute on hardware with limited resources. Reinforcement learning (RL) and differentiable neural architecture search (DNAS) are two main solution paths that have shown their superiority. Yet, there are countless methods with various implementations within each path. It has been hard to comprehend their differences and make a relatively fair comparison due to the lack of a benchmark framework and a clear analysis of which aspects are common, respectively distinct, between different implementations. To this end, we introduce BenQ to pave the way towards fair comparisons in two separate race tracks, i.e., intra-comparison of the RL-based and the DNAS-based methods, respectively. We provide a systematic approach, which helps to reveal relatively vital aspects of different implementations. Finally, we conduct comprehensive experi-ments on VGG, AlexNet, ResNet, GoogleNet, MobileNet-V2, and Vision Transformer (ViT), and the new observations shed light on potential future directions for automated quantization to move forward.},
  archive      = {J_TC},
  author       = {Zheng Wei and Xingjun Zhang and Zeyu Ji and Jingbo Li and Jia Wei},
  doi          = {10.1109/TC.2023.3315836},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {18-29},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Revisit and benchmarking of automated quantization toward fair comparison},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedGKD: Toward heterogeneous federated learning via global
knowledge distillation. <em>TC</em>, <em>73</em>(1), 3–17. (<a
href="https://doi.org/10.1109/TC.2023.3315066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning, as one enabling technology of edge intelligence, has gained substantial attention due to its efficacy in training deep learning models without data privacy and network bandwidth concerns. However, due to the heterogeneity of the edge computing system and data, many methods suffer from the “client-drift” issue that could considerably impede the convergence of global model training: local models on clients can drift apart, and the aggregated model can be different from the global optimum. To tackle this issue, one intuitive idea is to guide the local model training by global teachers, i.e., past global models, where each client learns the global knowledge from past global models via adaptive knowledge distillation techniques. Inspired by these insights, we propose a novel approach for heterogeneous federated learning, FedGKD , which fuses the knowledge from historical global models and guides local training to alleviate the “client-drift” issue. In this paper, we evaluate FedGKD through extensive experiments across various CV and NLP datasets ( i.e., CIFAR-10/100, Tiny-ImageNet, AG News, SST5) under different heterogeneous settings. The proposed method is guaranteed to converge under common assumptions and outperforms the state-of-the-art baselines in the non-IID federated setting.},
  archive      = {J_TC},
  author       = {Dezhong Yao and Wanning Pan and Yutong Dai and Yao Wan and Xiaofeng Ding and Chen Yu and Hai Jin and Zheng Xu and Lichao Sun},
  doi          = {10.1109/TC.2023.3315066},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {3-17},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FedGKD: Toward heterogeneous federated learning via global knowledge distillation},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial: EiC farewell and introduction of new EiC.
<em>TC</em>, <em>73</em>(1), 1–2. (<a
href="https://doi.org/10.1109/TC.2023.3341528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TC},
  author       = {Avinash Karanth},
  doi          = {10.1109/TC.2023.3341528},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {1-2},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Editorial: EiC farewell and introduction of new EiC},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
