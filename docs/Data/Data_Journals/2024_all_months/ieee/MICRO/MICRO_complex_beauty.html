<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro---72">MICRO - 72</h2>
<ul>
<li><details>
<summary>
(2024e). Unpriced and crucial. <em>MICRO</em>, <em>44</em>(6),
100–102. (<a href="https://doi.org/10.1109/MM.2024.3478428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many crucial inputs for digital businesses have a hidden value because they cost nothing to use. Examples include user-contributed content and editorial services to Wikipedia, open-source software, and freemium services for business, such as Hugging Face’s platform, Google’s TensorFlow framework, and Meta’s PyTorch. Crucial and unpriced services have become so pervasive that knowing how to use unpriced services has become necessary in IT entrepreneurship, altering the core bottleneck for starting frontier activities.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2024.3478428},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {100-102},
  shortjournal = {IEEE Micro},
  title        = {Unpriced and crucial},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A review of wisconsin alumni research foundation v.
Apple—part i. <em>MICRO</em>, <em>44</em>(6), 92–96. (<a
href="https://doi.org/10.1109/MM.2024.3504881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is the first part in a series that reviews the decisions that the district judge and appellate panel made in Wisconsin Alumni Research Foundation v. Apple.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2024.3504881},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {92-96},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part i},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Puss in boots: Formalizing arm’s virtual memory system
architecture. <em>MICRO</em>, <em>44</em>(6), 83–91. (<a
href="https://doi.org/10.1109/MM.2024.3422668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present our formalization of Arm’s Virtual Memory System Architecture (VMSA). This work has been developed with, and ratified by, Arm and its partners, and is now a part of the Arm Architecture Reference Manual. Additionally, we present our experimental validation methodology, which required extending kernel virtual machine (KVM) unit tests, a test harness for the KVM. We used this infrastructure to run approximately 1300 VMSA litmus tests on a variety of Arm machines, thereby validating our model with respect to existing hardware. Our testing uncovered infidelities in the definition of a feature called translation table hardware management, which led Arm to relax its architecture to accommodate those cases. Finally, as part of this work, we uncovered subtleties in the definition of a feature of the VMSA called enhanced translation synchronization (ETS), which led Arm to deprecate ETS and replace it with a stronger feature called ETS2.},
  archive      = {J_MICRO},
  author       = {Jade Alglave and Richard Grisenthwaite and Artem Khyzha and Luc Maranget and Nikos Nikoleris},
  doi          = {10.1109/MM.2024.3422668},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {83-91},
  shortjournal = {IEEE Micro},
  title        = {Puss in boots: Formalizing arm’s virtual memory system architecture},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMD XDNA NPU in ryzen AI processors. <em>MICRO</em>,
<em>44</em>(6), 73–82. (<a
href="https://doi.org/10.1109/MM.2024.3423692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The AMD Ryzen 7040 series is the first x86 processor with an integrated neural processing unit (NPU). The AMD XDNA technology in the NPU of Ryzen artificial intelligence (AI) processors provides optimized compute and memory resources for the needs for AI workloads and a specialized data movement architecture that allows to minimize bandwidth requirements leading to higher performance and efficiency. Across a selection of neural network benchmarks, XDNA provides between 4.3× and 33× better performance per watt, leading to extended battery life. The AI-optimized capabilities of the Ryzen 7040 NPU enable new AI experiences that are not possible without XDNA, making it a fundamental component in today’s Ryzen-AI-powered devices and setting the foundation for an exciting roadmap toward future AI capabilities in mobile PCs.},
  archive      = {J_MICRO},
  author       = {Alejandro Rico and Satyaprakash Pareek and Javier Cabezas and David Clarke and Baris Ozgul and Francisco Barat and Yao Fu and Stephan Münz and Dylan Stuart and Patrick Schlangen and Pedro Duarte and Sneha Date and Indrani Paul and Jian Weng and Sonal Santan and Vinod Kathail and Ashish Sirasao and Juanjo Noguera},
  doi          = {10.1109/MM.2024.3423692},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {73-82},
  shortjournal = {IEEE Micro},
  title        = {AMD XDNA NPU in ryzen AI processors},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DNNDaSher: A compiler framework for dataflow compatible
end-to-end acceleration on IBM AIU. <em>MICRO</em>, <em>44</em>(6),
63–72. (<a href="https://doi.org/10.1109/MM.2024.3423750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence unit (AIU) is a specialized accelerator card from IBM offering state-of-the-art compute capabilities (hundreds of tera-operations) through dataflow-driven compute arrays attached to a multilevel hierarchy of distributed memory elements. In mapping entire AI models, functional correctness hinges on maintaining dataflow compatibility between producer–consumer operations, i.e., the element organization with which a tensor is produced in memory must match the organization expected by the consumer(s). This paper presents a key component in AIU’s compiler stack, DNN Data-Shuffler (DnnDaSher), a systematic framework to analyze such dataflow incompatibilities and invoke an intermediate operation to shuffle tensor elements within and/or across memory elements to resolve the discrepancy. It targets opportunities to eliminate shuffles and increase granularity of memory accesses. Compared to well-optimized baseline implementations of four Convolutional Neural Networks and Transformer benchmarks, DNNDaSher achieves 1.27×–4.12× (average 2.3×) end-to-end latency improvement based on measured execution cycles on the AIU.},
  archive      = {J_MICRO},
  author       = {Sanchari Sen and Shubham Jain and Sarada Krithivasan and Swagath Venkataramani and Vijayalakshmi Srinivasan},
  doi          = {10.1109/MM.2024.3423750},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {63-72},
  shortjournal = {IEEE Micro},
  title        = {DNNDaSher: A compiler framework for dataflow compatible end-to-end acceleration on IBM AIU},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monza: An energy-minimal, general-purpose dataflow
system-on-chip for the internet of things. <em>MICRO</em>,
<em>44</em>(6), 52–62. (<a
href="https://doi.org/10.1109/MM.2024.3426611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes the design and implementation of Monza, a testchip system-on-chip featuring Efficient Computer Company’s Fabric processor. The Fabric is a general-purpose, spatial dataflow architecture and compiler designed to minimize energy. The Fabric offers a unique combination of familiar software DevOps and extreme energy efficiency. Monza includes a small Fabric processor, RISC-V core, 1-MB static random-access memory, digital I/O, and integrated clock generation and power conversion. Monza is implemented in 22-nm FD-SOI and was taped out in quarter 4 of 2023. Signoff timing and power simulations estimate Fabric efficiency of up to 1250 GOPS/W on dense convolution written in C.},
  archive      = {J_MICRO},
  author       = {Nathan Beckmann and Brandon Lucia and Graham Gobieski and Tony Nowatzki and Thomas Jackson and Guénolé Lallement and Keyi Zhang and Amolak Nagi and Atharv Sathe and Harsh Desai},
  doi          = {10.1109/MM.2024.3426611},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {52-62},
  shortjournal = {IEEE Micro},
  title        = {Monza: An energy-minimal, general-purpose dataflow system-on-chip for the internet of things},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallelization strategies for DLRM embedding bag operator
on AMD CPUs. <em>MICRO</em>, <em>44</em>(6), 44–51. (<a
href="https://doi.org/10.1109/MM.2024.3423785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning recommendation models (DLRMs) are deployed extensively to support personalized recommendations and consume a large fraction of artificial intelligence (AI) cycles in modern datacenters with embedding stage being a critical component. Modern CPUs execute a lot of DLRM cycles because they are cost effective compared to GPUs and other accelerators. Our paper addresses key bottlenecks in accelerating the embedding stage on CPUs. Specifically, this work 1) explores novel threading schemes that parallelize embedding bag, 2) pushes the envelope on realized bandwidth by improving data reuse in caches, and 3) studies the impact of parallelization on load imbalance. The new embedding bag kernels have been prototyped in the ZenDNN software stack. When put together, our work on fourth generation EPYC processors achieve up to 9.9x improvement in embedding bag performance over state-of-the-art implementations, and improve realized bandwidth of up to 5.7x over DDR bandwidth.},
  archive      = {J_MICRO},
  author       = {Krishnakumar Nair and Avinash-Chandra Pandey and Siddappa Karabannavar and Meena Arunachalam and John Kalamatianos and Varun Agrawal and Saurabh Gupta and Ashish Sirasao and Elliott Delaye and Steve Reinhardt and Rajesh Vivekanandham and Ralph Wittig and Vinod Kathail and Padmini Gopalakrishnan and Satyaprakash Pareek and Rishabh Jain and Mahmut Taylan Kandemir and Jun-Liang Lin and Gulsum Gudukbay Akbulut and Chita R. Das},
  doi          = {10.1109/MM.2024.3423785},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {44-51},
  shortjournal = {IEEE Micro},
  title        = {Parallelization strategies for DLRM embedding bag operator on AMD CPUs},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composition of experts on the SN40L reconfigurable dataflow
unit. <em>MICRO</em>, <em>44</em>(6), 34–43. (<a
href="https://doi.org/10.1109/MM.2024.3428548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monolithic large language models (LLMs) pose significant challenges in training and serving during an active deployment. By contrast, composition of experts (CoE) is a modular approach that lowers the cost and complexity of training and serving. In this article, we explore unique hardware challenges for CoE models, such as lower operational intensity and the cost of switching between models. We describe the SambaNova SN40L Reconfigurable Dataflow Unit (RDU), which combines streaming dataflow and a new three-tier memory system with static random-access memory (RAM), high-bandwidth memory, and double data rate dynamic RAM. A single eight-socket SN40L node achieves speedups between 2 and 13× due to aggressive operator fusion over an optimized baseline. The SN40L node deploys Samba-CoE, a 1 trillion-parameter CoE with a 19× smaller machine footprint, speeds up model switching time by 15–31× and achieves an overall speedup of 3.7× over a DGX H100 and 6.6× over a DGX A100.},
  archive      = {J_MICRO},
  author       = {Raghu Prabhakar and Ram Sivaramakrishnan and Darshan Gandhi and Yun Du and Mingran Wang and Xiangyu Song and Kejie Zhang and Tianren Gao and Angela Wang and Xiaoyan Li and Joshua Brot and Calvin Leung and Tuowen Zhao and Mark Gottscho and Zhengyu Chen and Kaizhao Liang and Swayambhoo Jain and Urmish Thakker and Kevin J. Brown and Kunle Olukotun},
  doi          = {10.1109/MM.2024.3428548},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {34-43},
  shortjournal = {IEEE Micro},
  title        = {Composition of experts on the SN40L reconfigurable dataflow unit},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A latency processing unit: A latency-optimized and highly
scalable processor for large language model inference. <em>MICRO</em>,
<em>44</em>(6), 17–33. (<a
href="https://doi.org/10.1109/MM.2024.3420728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive arrival of OpenAI’s ChatGPT has fueled the globalization of large language models (LLMs), which consist of billions of pretrained parameters that embody the aspects of syntax and semantics. HyperAccel introduces a latency processing unit (LPU), a latency-optimized and highly scalable processor architecture for the acceleration of LLM inference. The LPU perfectly balances memory bandwidth and compute logic with streamlined dataflow to maximize performance and efficiency. The LPU is equipped with an expandable synchronization link that hides data synchronization latency among multiple LPUs. HyperDex complements the LPU as an intuitive software framework to run LLM applications. The LPU achieves 1.25 ms/token and 20.9 ms/token for the 1.3B and 66B models, respectively, which is 2.09× and 1.37× faster, respectively, than a GPU. The LPU, synthesized using Samsung’s 4-nm process, has a total area of 0.824 mm2 and power consumption of 284.31 mW. LPU-based servers achieve 1.33× and 1.32× energy efficiency over Nvidia’s H100 and L4 servers, respectively.},
  archive      = {J_MICRO},
  author       = {Seungjae Moon and Jung-Hoon Kim and Junsoo Kim and Seongmin Hong and Junseo Cha and Minsu Kim and Sukbin Lim and Gyubin Choi and Dongjin Seo and Jongho Kim and Hunjong Lee and Hyunjun Park and Ryeowook Ko and Soongyu Choi and Jongse Park and Jinwon Lee and Joo-Young Kim},
  doi          = {10.1109/MM.2024.3420728},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {17-33},
  shortjournal = {IEEE Micro},
  title        = {A latency processing unit: A latency-optimized and highly scalable processor for large language model inference},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronous, low-latency, off-module interface for the IBM
z16 telum processor. <em>MICRO</em>, <em>44</em>(6), 8–16. (<a
href="https://doi.org/10.1109/MM.2024.3424506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes the IBM z16 off-module high-speed serializer/deserializer interface that connects Telum Processor chips. The crosspoint interface bus (X-bus) is designed to provide low-latency data transfer via a synchronous design for scalable system performance while supporting a dynamic, asynchronous mode to handle clocking failures and lane degrades. High availability is achieved through data packet protection, command voting, cancel, data replay recovery, interface retraining, and lane degrade. The z16 X-bus design is described in the context of two alternatives: a wide, parallel, synchronous, low-latency interface with error correction and a narrow, serial, asynchronous, high-latency interface with error detection and replay, both of which fall short of the high performance and low pin count required for the reliable, tightly coupled, coherent, symmetrical multiprocessor system.},
  archive      = {J_MICRO},
  author       = {Patrick Meaney and Ashutosh Mishra and Rajat Rao},
  doi          = {10.1109/MM.2024.3424506},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {8-16},
  shortjournal = {IEEE Micro},
  title        = {Synchronous, low-latency, off-module interface for the IBM z16 telum processor},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on contemporary industry products 2024.
<em>MICRO</em>, <em>44</em>(6), 6–7. (<a
href="https://doi.org/10.1109/MM.2024.3503212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It gives me much pleasure to introduce this Special Issue on Contemporary Industry Products 2024. This issue is intended to serve as a venue for highlighting some of the latest industry innovations in microprocessor, accelerator, system architecture, and large-scale system optimization technologies. Aside from our external call-for-papers, we recruited papers including ones reviewed and recommended by the 51st ACM/IEEE International Symposium on Computer Architecture (ISCA-51) Industry Track that could not be included in the final ISCA program due to space limitations. We accepted eight excellent papers to appear in this Special Issue, representing a roughly equal mix of papers from industry stalwarts and newer players.},
  archive      = {J_MICRO},
  author       = {John B. Carter},
  doi          = {10.1109/MM.2024.3503212},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on contemporary industry products 2024},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Strategic pivot of long-standing x86 rivals.
<em>MICRO</em>, <em>44</em>(6), 4–5. (<a
href="https://doi.org/10.1109/MM.2024.3504192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2024.3504192},
  journal      = {IEEE Micro},
  month        = {11-12},
  number       = {6},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Strategic pivot of long-standing x86 rivals},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Commercial and scientific prototypes. <em>MICRO</em>,
<em>44</em>(5), 90–92. (<a
href="https://doi.org/10.1109/MM.2024.3441788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2024.3441788},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {90-92},
  shortjournal = {IEEE Micro},
  title        = {Commercial and scientific prototypes},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024f). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part XII: Patent
families. <em>MICRO</em>, <em>44</em>(5), 83–88. (<a
href="https://doi.org/10.1109/MM.2024.3467488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2024.3467488},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {83-88},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part XII: Patent families},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying CO2 emission reduction through spatial
partitioning in deep learning recommendation system workloads.
<em>MICRO</em>, <em>44</em>(5), 75–82. (<a
href="https://doi.org/10.1109/MM.2024.3373443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The resource demand of modern applications has been increasing at a dizzying pace. This rising prominence is accompanied by a rising concern for the greenhouse gas emissions and carbon footprints of these applications. Deep learning recommendation models (DLRMs) are one example of the important models that are rising in importance and ubiquity in data centers. In this work, we analyze the impact of spatial partitioning of workloads that can decouple the geographical constraint and thus achieve reduced CO2 emissions by using DLRM online training as the workload under study.},
  archive      = {J_MICRO},
  author       = {Andrei Bersatti and Euna Kim and Hyesoon Kim},
  doi          = {10.1109/MM.2024.3373443},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {75-82},
  shortjournal = {IEEE Micro},
  title        = {Quantifying CO2 emission reduction through spatial partitioning in deep learning recommendation system workloads},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Server architecture from enterprise to post-moore.
<em>MICRO</em>, <em>44</em>(5), 65–73. (<a
href="https://doi.org/10.1109/MM.2024.3418975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Luiz Barroso started his career at Digital Equipment Corporation, investigating workload-optimized multiprocessor server architectures marketed to enterprises in the 1990s. These high-margin, low-volume products lost their market to more cost-effective enterprise servers built from high-volume desktop CPUs riding Moore’s law. The enterprise market has slowly transitioned to the cloud, where desktop PCs have formed the backbone of computing in data centers since the early 2000s to minimize cost and maximize the return on investment. Moving forward, with the absence of Moore’s law, future servers require a clean-slate, cross-stack design to scale in compute, communication, and storage capacity while reducing operational, capital, and environmental costs.},
  archive      = {J_MICRO},
  author       = {Babak Falsafi and Michael Ferdman and Boris Grot},
  doi          = {10.1109/MM.2024.3418975},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {65-73},
  shortjournal = {IEEE Micro},
  title        = {Server architecture from enterprise to post-moore},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tales of the tail: Past and future. <em>MICRO</em>,
<em>44</em>(5), 57–64. (<a
href="https://doi.org/10.1109/MM.2024.3413649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tail latency has been the defining performance metric for interactive services since the inception of cloud computing. Although various hardware and software techniques have been employed to improve tail latency for these applications, recent trends across the cloud system stack require revisiting them. Over the past few years, cloud hardware has become increasingly heterogeneous, and cloud software has been dominated by event-driven modular programming frameworks, as well as the proliferation of artificial intelligence. To guarantee tail latency in this new landscape, several system advances are required. In this paper, we first review what tail latency means for cloud services, the key innovations that improved it in the past, the trends that require revisiting them, as well as the innovations that will be required for tail latency constraints to be met in the next generation of warehouse-scale computers.},
  archive      = {J_MICRO},
  author       = {Christina Delimitrou and Michael Marty},
  doi          = {10.1109/MM.2024.3413649},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {57-64},
  shortjournal = {IEEE Micro},
  title        = {Tales of the tail: Past and future},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing NVMe storage for large-scale deployment: Key
technologies and strategies in alibaba cloud. <em>MICRO</em>,
<em>44</em>(5), 47–56. (<a
href="https://doi.org/10.1109/MM.2024.3426514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-volatile Memory Express (NVMe) storage has the advantage of ultraperformance. It has gained widespread adoption within cloud data centers, and its significance is only growing. Despite its widespread use, it still faces numerous challenges, such as optimizing performance, managing cost-effectiveness, and ensuring reliability for large-scale deployments. At Alibaba Cloud, we have designed numerous software–hardware co-design techniques tailored for NVMe storage deployment. These include hardware-assisted NVMe virtualization techniques developed explicitly for bare metal and virtual machines (VMs). BM-Store is a novel hardware-assisted virtualization architecture for bare-metal instances. Furthermore, the Cloud Infrastructure Processing Unit incorporates an embedded virtualization acceleration unit for a VM. LightPool, an NVMe over Fabrics-based high-performance storage pool architecture, enhances resource utilization for cloud-native distributed databases. Additionally, we delve into the technical challenges and opportunities presented by NVMe storage in the realm of serverless computing and artificial intelligence.},
  archive      = {J_MICRO},
  author       = {Yiquan Chen and Yuan Xie and Yijing Wang and Jiexiong Xu and Zhen Jin and Anyu Li and Xiaoyan Fu and Qiang Liu and Wenzhi Chen},
  doi          = {10.1109/MM.2024.3426514},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {47-56},
  shortjournal = {IEEE Micro},
  title        = {Optimizing NVMe storage for large-scale deployment: Key technologies and strategies in alibaba cloud},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond efficiency: Scaling AI sustainably. <em>MICRO</em>,
<em>44</em>(5), 37–46. (<a
href="https://doi.org/10.1109/MM.2024.3409275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Barroso’s seminal contributions in energy-proportional warehouse-scale computing launched an era where modern data centers have become more energy efficient and cost-effective than ever before. Simultaneously, modern AI applications have driven ever-increasing demands in computing, highlighting the importance of optimizing efficiency across the entire deep learning model development cycle. This article characterizes the carbon impact of AI, including both operational carbon emissions from training and inference and embodied carbon emissions from data center construction and hardware manufacturing. We highlight key efficiency optimization opportunities for cutting-edge AI technologies, from deep learning recommendation models to multimodal generative AI tasks. To scale AI sustainably, we must also go beyond efficiency and optimize across the lifecycle of computing infrastructures, from hardware manufacturing to data center operations and end-of-life processing for the hardware.},
  archive      = {J_MICRO},
  author       = {Carole-Jean Wu and Bilge Acun and Ramya Raghavendra and Kim Hazelwood},
  doi          = {10.1109/MM.2024.3409275},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {37-46},
  shortjournal = {IEEE Micro},
  title        = {Beyond efficiency: Scaling AI sustainably},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data center power and energy management: Past, present, and
future. <em>MICRO</em>, <em>44</em>(5), 30–36. (<a
href="https://doi.org/10.1109/MM.2024.3426478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article overviews some of the key past developments in cloud data center power and energy management, where we are today, and what the future could be. This topic is gaining enormous renewed interest in the context of the conflicting needs of the AI revolution and the climate crisis.},
  archive      = {J_MICRO},
  author       = {Ricardo Bianchini and Christian Belady and Anand Sivasubramaniam},
  doi          = {10.1109/MM.2024.3426478},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {30-36},
  shortjournal = {IEEE Micro},
  title        = {Data center power and energy management: Past, present, and future},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The tail at amazon web services scale. <em>MICRO</em>,
<em>44</em>(5), 23–29. (<a
href="https://doi.org/10.1109/MM.2024.3420070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While tail latency events are individually rare, they are compounded in large-scale distributed systems, leading to poor end-to-end experiences. Networking is one of the key contributors to tail latency, due to network congestion, hardware failures, or transient updates causing high queuing delays and packet drops with millisecond-to-second recovery times. At Amazon Web Services (AWS), we originally designed the scalable reliable datagram (SRD) protocol to decrease these latencies in high-performance computing workloads and distributed machine learning training. SRD utilizes multiple network paths and, combined with microsecond-scale retransmission and path failover, allows the use of the lowest latency paths at the cost of some redundant retransmissions. Additionally, AWS designed its own Flash-based storage media to reduce its tail latency, making network latency the main contributor to tail latency of networked storage applications. SRD was then used as the transport protocol for Elastic Block Storage (the AWS-native storage area network), where high-percentile latency is reduced by as much as 90%.},
  archive      = {J_MICRO},
  author       = {Leah Shalev and Hani Ayoub and Nafea Bshara and Yuval Fatael and Ori Golan and Omer Ilany and Anna Levin and Zorik Machulsky and Kevin Milczewski and Marc Olson and Valentin Priescu and Shyam Rajagopal and Ali Saidi},
  doi          = {10.1109/MM.2024.3420070},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {23-29},
  shortjournal = {IEEE Micro},
  title        = {The tail at amazon web services scale},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Twenty five years of warehouse-scale computing.
<em>MICRO</em>, <em>44</em>(5), 11–22. (<a
href="https://doi.org/10.1109/MM.2024.3409469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When Google was founded in 1998, it was already clear that successful web search would require enormous amounts of computing power and storage, and that no single computer would be able to handle this task. Consequently, its infrastructure design marked a fundamental shift toward an approach now widely embraced as warehouse-scale computing (WSC). Starting as a niche approach optimized for web search, over the past two and a half decades, WSC has evolved dramatically. Today, it’s the mainstream approach underpinning all hyperscale companies and cloud platforms and is poised to be the foundation for the next wave of artificial intelligence/machine learning computing in the cloud. In this article, we chronicle the evolution of WSC, highlighting pivotal milestones, lessons learned, and the vast opportunities that lie ahead.},
  archive      = {J_MICRO},
  author       = {Parthasarathy Ranganathan and Urs Hölzle},
  doi          = {10.1109/MM.2024.3409469},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {11-22},
  shortjournal = {IEEE Micro},
  title        = {Twenty five years of warehouse-scale computing},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on the past, present, and future of
warehouse-scale computing. <em>MICRO</em>, <em>44</em>(5), 6–7. (<a
href="https://doi.org/10.1109/MM.2024.3467468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {John L. Hennessy and Christos Kozyrakis and Gabriel Falcão},
  doi          = {10.1109/MM.2024.3467468},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on the past, present, and future of warehouse-scale computing},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). The path to powering intelligence. <em>MICRO</em>,
<em>44</em>(5), 4–5. (<a
href="https://doi.org/10.1109/MM.2024.3467528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2024.3467528},
  journal      = {IEEE Micro},
  month        = {9-10},
  number       = {5},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {The path to powering intelligence},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Navigating applications development in generative AI.
<em>MICRO</em>, <em>44</em>(4), 122–124. (<a
href="https://doi.org/10.1109/MM.2024.3425209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In its earliest prototypes, GitHub CoPilot stood apart from other tools. It demonstrated a remarkable ability to accelerate the work of intermediate coders by 20% to 40%, mainly with standardized languages like Python. This is a significant productivity gain in a labor-intensive activity, where such breakthroughs are rare.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2024.3425209},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {122-124},
  shortjournal = {IEEE Micro},
  title        = {Navigating applications development in generative AI},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part XI: Patent
families. <em>MICRO</em>, <em>44</em>(4), 116–120. (<a
href="https://doi.org/10.1109/MM.2024.3433209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In previous parts of this series, I analyzed.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2024.3433209},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {116-120},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part XI: Patent families},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed brain–computer interfacing with a networked
multiaccelerator architecture. <em>MICRO</em>, <em>44</em>(4), 106–115.
(<a href="https://doi.org/10.1109/MM.2024.3411881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SCALO is the first distributed brain–computer interface (BCI) consisting of multiple wireless-networked implants placed on different brain regions. SCALO unlocks new treatment options for debilitating neurological disorders and new research into brainwide network behavior. Achieving the fast and low-power communication necessary for real-time processing has historically restricted BCIs to single brain sites. SCALO also adheres to tight power constraints but enables fast distributed processing. Central to SCALO’s efficiency is its realization as a full stack distributed system of brain implants with accelerator-rich compute. SCALO balances modular system layering with aggressive cross-layer hardware–software co-design to integrate compute, networking, and storage. The result is a lesson in designing energy-efficient networked distributed systems with hardware accelerators from the ground up.},
  archive      = {J_MICRO},
  author       = {Raghavendra Pradyumna Pothukuchi and Karthik Sriram and Michał Gerasimiuk and Muhammed Ugur and Rajit Manohar and Anurag Khandelwal and Abhishek Bhattacharjee},
  doi          = {10.1109/MM.2024.3411881},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {106-115},
  shortjournal = {IEEE Micro},
  title        = {Distributed Brain–Computer interfacing with a networked multiaccelerator architecture},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AuRORA: A full-stack solution for scalable and virtualized
accelerator integration. <em>MICRO</em>, <em>44</em>(4), 97–105. (<a
href="https://doi.org/10.1109/MM.2024.3409546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the increasingly demanding compute requirements of modern workloads, systems on chip (SoCs) must provide an accelerator-rich hardware architecture and software programming interface. However, scalability remains a first-order concern, as introducing additional unmanageable complexity to either physical design or software integration may prohibit the deployment of new accelerators. To address these challenges, this work presents AuRORA, an accelerator integration methodology that provides a scalable physical accelerator interface while preserving software semantics with minimal overhead for accelerator access. AuRORA provides a new accelerator integration methodology that preserves the software and hardware interface of a tightly CPU-coupled accelerator while physically disaggregating the accelerators away from a host CPU. To address software scalability, AuRORA also includes a lightweight software runtime for an SoC with heterogeneous accelerators, providing low-overhead access to these accelerators for multitenant applications.},
  archive      = {J_MICRO},
  author       = {Seah Kim and Jerry Zhao and Krste Asanović and Borivoje Nikolić and Yakun Sophia Shao},
  doi          = {10.1109/MM.2024.3409546},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {97-105},
  shortjournal = {IEEE Micro},
  title        = {AuRORA: A full-stack solution for scalable and virtualized accelerator integration},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Programmable olfactory computing. <em>MICRO</em>,
<em>44</em>(4), 88–96. (<a
href="https://doi.org/10.1109/MM.2024.3409619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although smell is arguably the most visceral of senses, olfactory computing has been barely explored in the mainstream. We argue that this is a good time to explore olfactory computing as driver applications are emerging, sensors are dramatically better, and nontraditional form factors that would be required to support olfactory computing have widespread acceptance. Through a comprehensive review of literature, we identify the key algorithms needed to support a wide variety of olfactory computing tasks. We profiled these algorithms on existing hardware and identified several characteristics, including the preponderance of fixed-point computation, linear operations, and real arithmetic; a variety of data-memory requirements; and opportunities for data-level parallelism. We propose Ahromaa, a heterogeneous architecture for olfactory computing that targets power- and energy-constrained olfactory computing workloads, and evaluate it against the baseline architectures of a microcontroller unit (MCU), coarse-grained reconfigurable array, and an MCU with packed single instruction, multiple data.},
  archive      = {J_MICRO},
  author       = {Nathaniel Bleier and Abigail Wezelis and Lav Varshney and Rakesh Kumar},
  doi          = {10.1109/MM.2024.3409619},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {88-96},
  shortjournal = {IEEE Micro},
  title        = {Programmable olfactory computing},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical online reinforcement learning for microprocessors
with micro-armed bandit. <em>MICRO</em>, <em>44</em>(4), 80–87. (<a
href="https://doi.org/10.1109/MM.2024.3408719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although online reinforcement learning (RL) has shown promise for microarchitecture decision making, processor vendors are still reluctant to adopt it. There are two main reasons that make RL-based solutions unattractive. First, they have high complexity and storage overhead. Second, many RL agents are engineered for a specific problem and are not reusable. In this work, we propose a way to tackle these shortcomings. We find that, in diverse microarchitecture problems, only a few actions are useful in a given time window. Motivated by this property, we design Micro-Armed Bandit (or Bandit for short), an RL agent that is based on the low-complexity Multi-Armed Bandit algorithms. We show that Bandit can match or exceed the performance of more complex RL and non-RL alternatives in two different problems: data prefetching and instruction fetch thread selection in simultaneous multithreaded processors. We believe that Bandit’s simplicity, reusability, and small storage overhead make online RL more practical for microarchitecture.},
  archive      = {J_MICRO},
  author       = {Gerasimos Gerogiannis and Josep Torrellas},
  doi          = {10.1109/MM.2024.3408719},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {80-87},
  shortjournal = {IEEE Micro},
  title        = {Practical online reinforcement learning for microprocessors with micro-armed bandit},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware-assisted fault isolation: Going beyond the limits
of software-based sandboxing. <em>MICRO</em>, <em>44</em>(4), 70–79. (<a
href="https://doi.org/10.1109/MM.2024.3422977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware-assisted fault isolation (HFI) is a minimal extension to current processors that supports secure, flexible, and efficient in-process isolation. HFI addresses the limitations of existing software-based fault isolation (SFI) systems, including runtime overheads, limited scalability, vulnerability to Spectre attacks, and limited compatibility with existing code and binaries. HFI can be seamlessly integrated into existing SFI systems (e.g., WebAssembly) or directly sandbox unmodified native binaries. To ease adoption, HFI relies only on incremental changes to existing high-performance processors.},
  archive      = {J_MICRO},
  author       = {Shravan Narayan and Tal Garfinkel and Mohammadkazem Taram and Joey Rudek and Daniel Moghimi and Evan Johnson and Chris Fallin and Anjo Vahldiek-Oberwagner and Michael LeMay and Ravi Sahita and Dean Tullsen and Deian Stefan},
  doi          = {10.1109/MM.2024.3422977},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {70-79},
  shortjournal = {IEEE Micro},
  title        = {Hardware-assisted fault isolation: Going beyond the limits of software-based sandboxing},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RowPress vulnerability in modern DRAM chips. <em>MICRO</em>,
<em>44</em>(4), 60–69. (<a
href="https://doi.org/10.1109/MM.2024.3409521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory isolation is a critical property for system reliability, security, and safety. We demonstrate RowPress, a dynamic random-access memory (DRAM) read disturbance phenomenon different from the well-known RowHammer. RowPress induces bitflips by keeping a DRAM row open for a long period of time instead of repeatedly opening and closing the row. We experimentally characterize RowPress bitflips, showing their widespread existence in commodity off-the-shelf DDR4 DRAM chips. We demonstrate RowPress bitflips in a real system that already has RowHammer protection, and propose effective mitigation techniques that protect DRAM against both RowHammer and RowPress.},
  archive      = {J_MICRO},
  author       = {Haocong Luo and Ataberk Olgun and Abdullah Giray Yağlikçi and Yahya Can Tuğrul and Steve Rhyner and Meryem Banu Cavlak and Joël Lindegger and Mohammad Sadrosadati and Onur Mutlu},
  doi          = {10.1109/MM.2024.3409521},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {60-69},
  shortjournal = {IEEE Micro},
  title        = {RowPress vulnerability in modern DRAM chips},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mosaic pages: Big TLB reach with small pages.
<em>MICRO</em>, <em>44</em>(4), 52–59. (<a
href="https://doi.org/10.1109/MM.2024.3409181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces mosaic pages, which increase translation lookaside buffer (TLB) reach by compressing multiple, discrete translations into one TLB entry. Mosaic leverages virtual contiguity for locality, but does not use physical contiguity. Mosaic relies on recent advances in hashing theory to constrain memory mappings, in order to realize this physical address compression without reducing memory utilization or increasing swapping. Mosaic reduces TLB misses in several workloads by 6%–81%. Our results show that Mosaic ’s constraints on memory mappings do not harm performance, we never see conflicts before memory is 98% full in our experiments—at which point a traditional design would also likely swap. Timing and area analyses on a commercial 28-nm CMOS process indicate that the hashing required on the critical path can run at a maximum frequency of 4 GHz, indicating that a Mosaic TLB is unlikely to affect clock frequency.},
  archive      = {J_MICRO},
  author       = {Jaehyun Han and Krishnan Gosakan and William Kuszmaul and Ibrahim N. Mubarek and Nirjhar Mukherjee and Karthik Sriram and Guido Tagliavini and Evan West and Michael A. Bender and Abhishek Bhattacharjee and Alex Conway and Martín Farach-Colton and Jayneel Gandhi and Rob Johnson and Sudarsun Kannan and Donald E. Porter},
  doi          = {10.1109/MM.2024.3409181},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {52-59},
  shortjournal = {IEEE Micro},
  title        = {Mosaic pages: Big TLB reach with small pages},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contiguitas: The pursuit of physical memory contiguity in
data centers. <em>MICRO</em>, <em>44</em>(4), 44–51. (<a
href="https://doi.org/10.1109/MM.2024.3406933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unabating growth of the memory needs of emerging data center applications has exacerbated the scalability bottleneck of virtual memory. However, reducing the overhead of address translation will remain onerous until the physical memory contiguity predicament gets resolved. To address this problem, Contiguitas provides ample physical memory contiguity by design. We identify that the primary cause of memory fragmentation in Meta’s data centers is unmovable allocations scattered across the address space that impedes contiguity. To this end, Contiguitas in the operating system separates movable allocations from unmovable ones by placing them into two different dynamically adjustable regions in physical memory. Furthermore, Contiguitas drastically reduces unmovable allocations through hardware extensions that transparently migrate unmovable pages while they remain in use. Our experiments in production at Meta’s data centers show that Contiguitas achieves end-to-end performance improvements of 2%–18%. Full-system simulations of the Contiguitas hardware show that it can efficiently migrate unmovable allocations without affecting applications.},
  archive      = {J_MICRO},
  author       = {Kaiyang Zhao and Kaiwen Xue and Ziqi Wang and Dan Schatzberg and Leon Yang and Antonis Manousis and Johannes Weiner and Rik Van Riel and Bikash Sharma and Chunqiang Tang and Dimitrios Skarlatos},
  doi          = {10.1109/MM.2024.3406933},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {44-51},
  shortjournal = {IEEE Micro},
  title        = {Contiguitas: The pursuit of physical memory contiguity in data centers},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end cloud application cloning with ditto.
<em>MICRO</em>, <em>44</em>(4), 34–43. (<a
href="https://doi.org/10.1109/MM.2024.3419067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of publicly available cloud services has been a recurring problem in architecture and systems. Although open source benchmarks exist, they do not capture the complexity of cloud services. Application cloning is a promising approach, however, prior work is limited to CPU-/cache-centric, single-node services. We present Ditto, a framework for cloning end-to-end cloud applications and monolithic and microservices that captures input–output and network activity as well as kernel operations, in addition to application logic. Ditto takes a hierarchical approach to application cloning, capturing the dependency graph across services, recreating each tier’s control/dataflow, and generating system calls and assembly that mimics individual applications. Ditto does not reveal the logic of the original application, facilitating publicly sharing clones of production services. We show that across a diverse set of applications, Ditto accurately captures their resource characteristics as well as their performance metrics, is portable across platforms, and facilitates a wide range of studies.},
  archive      = {J_MICRO},
  author       = {Mingyu Liang and Yu Gan and Yueying Li and Carlos Torres and Abhishek Dhanotia and Mahesh Ketkar and Christina Delimitrou},
  doi          = {10.1109/MM.2024.3419067},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {34-43},
  shortjournal = {IEEE Micro},
  title        = {End-to-end cloud application cloning with ditto},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Per-instruction cycle stacks through time-proportional event
analysis. <em>MICRO</em>, <em>44</em>(4), 27–33. (<a
href="https://doi.org/10.1109/MM.2024.3407377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding what applications spend time on and why is critical for effective performance optimization. Unfortunately, current state-of-the-art performance analysis tools are generally unable to provide this information. The fundamental reason is that they lack time proportionality; i.e., in many cases, they do not attribute execution time to the instructions and performance events that the architecture is exposing the latency of. Time-proportional event analysis (TEA) creates per-instruction cycle stacks, which clearly and accurately explain what the application spends time on and why at the level of individual static instructions. TEA requires executing the application only once; it is accurate (with an average error of 2.1%); and its hardware implementation incurs negligible runtime, power, and area overheads of 1.1%, 0.1%, and 249 bits per core, respectively.},
  archive      = {J_MICRO},
  author       = {Björn Gottschall and Lieven Eeckhout and Magnus Jahre},
  doi          = {10.1109/MM.2024.3407377},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {27-33},
  shortjournal = {IEEE Micro},
  title        = {Per-instruction cycle stacks through time-proportional event analysis},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoupled vector runahead for prefetching nested
memory-access chains. <em>MICRO</em>, <em>44</em>(4), 20–26. (<a
href="https://doi.org/10.1109/MM.2024.3406891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoupled vector runahead (DVR) exploits massive amounts of memory-level parallelism to improve the performance of applications that feature indirect memory accesses by dynamically inferring loop bounds at runtime, recognizing striding loads, and speculatively vectorizing the subsequent instructions that are part of an indirect chain. DVR runs as an on-demand, speculative, in-order, lightweight hardware subthread alongside the main thread within the core. DVR incurs minimal hardware overhead while delivering a substantial performance boost.},
  archive      = {J_MICRO},
  author       = {Ajeya Naithani and Jaime Roelandts and Sam Ainsworth and Timothy M. Jones and Lieven Eeckhout},
  doi          = {10.1109/MM.2024.3406891},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {20-26},
  shortjournal = {IEEE Micro},
  title        = {Decoupled vector runahead for prefetching nested memory-access chains},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous and heterogenous multithreading: Exploiting
simultaneous and heterogeneous parallelism in accelerator-rich
architectures. <em>MICRO</em>, <em>44</em>(4), 11–19. (<a
href="https://doi.org/10.1109/MM.2024.3414941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The addition of domain-specific hardware accelerators and general-purpose processors that support vector and scalar models makes modern computers undoubtedly heterogeneous. However, existing programming models and runtime systems target using the most efficient category of processing units to delegate computation from each code region, undermining the potential parallelism that heterogeneous processing units can provide. Simultaneous and heterogenous multithreading (SHMT) is a programming and execution model that activates all possible heterogeneous processing units for computation from a code region to enable “real” heterogeneous parallelism. SHMT presents an abstraction and a runtime system to facilitate parallel execution. Despite the new type of parallelism, SHMT also needs to additionally address the heterogeneity in data precision that various processing units support to ensure the quality of the result. This article implements and evaluates SHMT on an embedded system platform with a GPU and an edge tensor processing unit.},
  archive      = {J_MICRO},
  author       = {Kuan-Chieh Hsu and Hung-Wei Tseng},
  doi          = {10.1109/MM.2024.3414941},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {11-19},
  shortjournal = {IEEE Micro},
  title        = {Simultaneous and heterogenous multithreading: Exploiting simultaneous and heterogeneous parallelism in accelerator-rich architectures},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on top picks from the 2023 computer
architecture conferences. <em>MICRO</em>, <em>44</em>(4), 6–10. (<a
href="https://doi.org/10.1109/MM.2024.3431308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is our pleasure to introduce the IEEE Micro Special Issue on Top Picks from the 2023 Computer Architecture Conferences. This special issue includes 12 articles chosen by a selection committee (SC) as being the most significant research articles in computer architecture in 2023 in terms of their potential for long-term impact. The committee also selected an additional 12 articles to be recognized with an honorable mention (see “Honorable Mentions”).},
  archive      = {J_MICRO},
  author       = {Yan Solihin},
  doi          = {10.1109/MM.2024.3431308},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {6-10},
  shortjournal = {IEEE Micro},
  title        = {Special issue on top picks from the 2023 computer architecture conferences},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024f). Top picks ignite innovation. <em>MICRO</em>,
<em>44</em>(4), 4–5. (<a
href="https://doi.org/10.1109/MM.2024.3433189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This IEEE Micro Special Issue on Top Picks includes 12 outstanding papers selected from those published in 2023 computer architecture conferences. When we were prepping and putting together all the articles in this special issue, another significant event in the computer industry was unfolding in Taipei, Taiwan, in June: the 2024 COMPUTEX Taipei. The event hosted more than 1500 exhibitors from 136 countries and regions, showcasing the latest innovations and sleek products in information and computing technologies. Unparalleled to previous equivalent events, the 2024 COMPUTEX organizers invited an elite group of influential leaders from today’s technology juggernauts to deliver keynotes. The program features AMD CEO Lisa Su, Delta Electronics General Director Tzi-cker Chiueh, Intel CEO Pat Gelsinger, MediaTek CEO Rick Tsai, NXP CTO Lars Reger, Qualcomm CEO Cristiano Amon, and Supermicro CEO Charles Liang. They were joined by a couple of additional speakers, ARM CEO Rene Haas and Nvidia CEO Jensen Huang, who delivered their keynotes outside the COMPUTEX exhibition hall. These Top Picks speakers represent the ecosystem companies of the computing infrastructure supply chain that help shape, bolster, and accelerate the Fourth Industrial Revolution, driven by the prevailing and unstoppable artificial intelligence (AI) tsunami. The unveiled systems push the frontiers of technology, reaching the pinnacle of computer architecture design, system integration, advanced packaging techniques, and leading-edge process technologies.},
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2024.3433189},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Top picks ignite innovation},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The worlds i see: Curiosity, exploration, and the discovery
at the dawn of AI—fei-fei li (new york, NY, USA: Flatiron books, 2023,
336 pp.). <em>MICRO</em>, <em>44</em>(3), 82–84. (<a
href="https://doi.org/10.1109/MM.2024.3390268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fei-Fei Li is known for leading the development of ImageNet, which helped catalyze machine learning approaches to vision recognition, and for being an essential voice shaping the science behind artificial intelligence (AI) today. She offers an extended, thoughtful, and heartfelt memoir in this book.1 Beautifully written and grounded in many rich, thought-provoking observations, the book describes her journey from being a child immigrant from China to her present position as a Stanford professor.},
  archive      = {J_MICRO},
  author       = {Reviewed by Shane Greenstein},
  doi          = {10.1109/MM.2024.3390268},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {82-84},
  shortjournal = {IEEE Micro},
  title        = {The worlds i see: curiosity, exploration, and the discovery at the dawn of AI—Fei-fei li (New york, NY, USA: flatiron books, 2023, 336 pp.)},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part x: Patent
families. <em>MICRO</em>, <em>44</em>(3), 76–80. (<a
href="https://doi.org/10.1109/MM.2024.3394670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is the next article in the series on the patenting behavior and characteristics of computer architecture companies. This article continues to analyze the characteristics for patent families.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2024.3394670},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {76-80},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part x: Patent families},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining multiple tiny machine learning models for
multimodal context-aware stress recognition on constrained
microcontrollers. <em>MICRO</em>, <em>44</em>(3), 67–75. (<a
href="https://doi.org/10.1109/MM.2023.3329218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As stress continues to be a major health concern, there is growing interest in developing effective stress management systems that can detect and mitigate stress. Deep neural networks (DNNs) have shown their effectiveness in accurately classifying stress, but most of the existing solutions rely on the cloud or large, obtrusive devices for inference. The emergence of tiny machine learning provides an opportunity to bridge this gap and enable ubiquitous intelligent systems. In this article, we propose a context-aware stress detection approach that uses a microcontroller to continuously infer physical activity to mitigate motion artifacts when inferring stress from heart rate and electrodermal activity. We deploy two DNNs onto a single resource-constrained microcontroller for real-world stress recognition, with the resultant stress and activity recognition models achieving 88% and 98% accuracy, respectively. Our proposed context-aware approach improves the accuracy and privacy of stress detection systems while eliminating the need to store or transmit sensitive health data.},
  archive      = {J_MICRO},
  author       = {Michael Gibbs and Kieran Woodward and Eiman Kanjo},
  doi          = {10.1109/MM.2023.3329218},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {67-75},
  shortjournal = {IEEE Micro},
  title        = {Combining multiple tiny machine learning models for multimodal context-aware stress recognition on constrained microcontrollers},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-performance cooling for power electronics via
electrochemical additive manufacturing. <em>MICRO</em>, <em>44</em>(3),
58–66. (<a href="https://doi.org/10.1109/MM.2024.3360255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise in adoption of electric vehicles has driven rapid development of traction inverter components. The advanced SiC and GaN devices used in these inverters have high power densities, creating a thermal management challenge and, therefore, issues with device performance and reliability. This paper introduces an advanced liquid-cooled thermal management solution for power electronics. Utilizing a novel 3-D metal printing technology called electrochemical additive manufacturing (ECAM), copper cooling structures are printed directly onto the ceramic substrate of the component, thereby eliminating thermal interface materials and significantly reducing the thermal resistance of the system-level stack. Additionally, improving fin efficiency and heat transfer through high surface area, triply periodic minimal surface cooling structures is demonstrated. The use of ECAM-printed cooling structures in traction inverter applications is shown to have great potential for realizing significant gains in performance, via thermal resistance improvements in the range of 60%–120%.},
  archive      = {J_MICRO},
  author       = {Ian Winfield and Tim Ouradnik and Joseph Madril and Michael Matthews and Guillermo Romero},
  doi          = {10.1109/MM.2024.3360255},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {58-66},
  shortjournal = {IEEE Micro},
  title        = {High-performance cooling for power electronics via electrochemical additive manufacturing},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inside the cerebras wafer-scale cluster. <em>MICRO</em>,
<em>44</em>(3), 49–57. (<a
href="https://doi.org/10.1109/MM.2024.3386628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The compute and memory demands of machine learning have driven the industry to use clusters of thousands of GPUs to train state-of-the-art models. However, scaling performance on GPU clusters is inherently complex, requires many forms of parallelism, and often has poor scaling performance. The Cerebras Wafer-Scale Cluster provides exafloating-point-operations-per-second-level compute while avoiding these limitations by using only data parallelism. The cluster is built around the CS-2 system with the Wafer-Scale Engine 2 processor that enables even the largest models to run on a single chip without partitioning. This unique property allows the cluster architecture to decouple memory and compute. Model weights are stored in an external memory device, MemoryX, and compute scaling is enabled using a special broadcast–reduce fabric, SwarmX. This article describes the challenges of GPU scaling and details the internal design of the Cerebras Wafer-Scale Cluster architecture and its unique ability to overcome these challenges.},
  archive      = {J_MICRO},
  author       = {Sean Lie},
  doi          = {10.1109/MM.2024.3386628},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {49-57},
  shortjournal = {IEEE Micro},
  title        = {Inside the cerebras wafer-scale cluster},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The breakthrough memory solutions for improved performance
on LLM inference. <em>MICRO</em>, <em>44</em>(3), 40–48. (<a
href="https://doi.org/10.1109/MM.2024.3375352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have changed our lives, but they require unprecedented computing resources—especially large memory capacity and high bandwidth to process weights. However, while the logic process was developing, the speed of development of the memory process could not keep up, causing problems that resulted in the performance of LLMs being hindered by memory. Samsung has introduced breakthrough processing-in-memory/processing-near-memory (PIM/PNM) solutions that enhance the main memory bandwidth. With the high bandwidth memory PIM-based GPU-cluster system and LPDDR5-PIM-based system, the performance of transformer-based LLMs improved by up to 1.9$ \times $× and 2.7$ \times $×, respectively. The Compute eXpress Link (CXL)-based PNM solution serves memory-centric computing systems by implementing logic inside the CXL memory controller. This results in a performance gain of more than 4.4$ \times $× with an energy reduction of about 53% with PNM. Furthermore, we provide PIM/PNM software stacks, including an AI compiler targeting the acceleration of AI models.},
  archive      = {J_MICRO},
  author       = {Byeongho Kim and Sanghoon Cha and Sangsoo Park and Jieun Lee and Sukhan Lee and Shin-haeng Kang and Jinin So and Kyungsoo Kim and Jin Jung and Jong-Geon Lee and Sunjung Lee and Yoonah Paik and Hyeonsu Kim and Jin-Seong Kim and Won-Jo Lee and Yuhwan Ro and YeonGon Cho and Jin Hyun Kim and JoonHo Song and Jaehoon Yu and Seungwon Lee and Jeonghyeon Cho and Kyomin Sohn},
  doi          = {10.1109/MM.2024.3375352},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {40-48},
  shortjournal = {IEEE Micro},
  title        = {The breakthrough memory solutions for improved performance on LLM inference},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI and memory wall. <em>MICRO</em>, <em>44</em>(3), 33–39.
(<a href="https://doi.org/10.1109/MM.2024.3373763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of unprecedented unsupervised training data, along with neural scaling laws, has resulted in an unprecedented surge in model size and compute requirements for serving/training large language models. However, the main performance bottleneck is increasingly shifting to memory bandwidth. Over the past 20 years, peak server hardware floating-point operations per second have been scaling at 3.0${\times}$× per two years, outpacing the growth of dynamic random-access memory and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every two years, respectively. This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. Here, we analyze encoder and decoder transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models. We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation.},
  archive      = {J_MICRO},
  author       = {Amir Gholami and Zhewei Yao and Sehoon Kim and Coleman Hooper and Michael W. Mahoney and Kurt Keutzer},
  doi          = {10.1109/MM.2024.3373763},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {33-39},
  shortjournal = {IEEE Micro},
  title        = {AI and memory wall},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The first direct mesh-to-mesh photonic fabric.
<em>MICRO</em>, <em>44</em>(3), 25–32. (<a
href="https://doi.org/10.1109/MM.2024.3387828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intel developed the Programmable Integrated Unified Memory Architecture (PIUMA) to address the inefficiencies seen in conventional processor architectures for at-scale sparse graph analytics. PIUMA adopted copackaged optical (CPO) modules to provide 1 TB/s/direction of network bandwidth. PIUMA-fused traditional network switch logic into the compute logic to make a disaggregated switch fabric in a HyperX topology. Utilizing the latest CPO technology, PIUMA converts the on-die mesh protocol directly to the optical fabric and back to seamlessly glue together all PIUMA chips in a system into a large virtual die. Measured silicon demonstrates a best-case latency of &lt;46 ns per connection mesh stop to mesh stop through the optical fabric in the first prototype, which is not fully optimized. This was all combined into a customized application-specified integrated circuit of 316 mm2 in 7-nm manufacturing and advanced packaging technology to enable the direct CPO fabric requirements.},
  archive      = {J_MICRO},
  author       = {Jason Howard and Joshua B. Fryman and Shamsul Abedin},
  doi          = {10.1109/MM.2024.3387828},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {25-32},
  shortjournal = {IEEE Micro},
  title        = {The first direct mesh-to-mesh photonic fabric},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMD ryzen 7040 series. <em>MICRO</em>, <em>44</em>(3),
18–24. (<a href="https://doi.org/10.1109/MM.2024.3394479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The AMD Ryzen 7040 Series processors are designed to strike the right balance between domain-specific accelerators and general-purpose compute. The system on chip features “Zen 4,” RDNA 3, AMD XDNA architecture, and additional accelerators with a focus on delivering power-efficient performance.},
  archive      = {J_MICRO},
  author       = {Mahesh Subramony and David Kramer and Indrani Paul},
  doi          = {10.1109/MM.2024.3394479},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {18-24},
  shortjournal = {IEEE Micro},
  title        = {AMD ryzen 7040 series},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMD next-generation “zen 4” core and 4th gen AMD EPYC server
CPUs. <em>MICRO</em>, <em>44</em>(3), 8–17. (<a
href="https://doi.org/10.1109/MM.2024.3375070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 4th gen AMD EPYC server processor family brings the “Zen 4” core to the data center and cloud market and introduces a family of unique processors that leverage advanced chiplet architecture to efficiently optimize each processor to specific market needs. The “Zen 4” core is the latest generation of AMD’s high-performance and power-efficient x86 cores. Based on the “Zen 3” microarchitecture, it delivers a major step in power efficiency and performance with the inclusion of power-efficient AVX-512 support. The “Zen 4c” core further improves power efficiency by targeting a lower boost frequency. The 4th gen AMD EPYC processor family expands to include “Bergamo,” a high core-count, power-efficient, cloud-focused processor, and “Siena,” a high-capability processor with a streamlined, power-efficient form factor. These complement the record-breaking performance of “Genoa” and massive performance per core of “Genoa-X.” AMD’s customer-focused approach and chiplet design enable timely delivery of these industry-leading processors.},
  archive      = {J_MICRO},
  author       = {Ravi Bhargava and Kai Troester},
  doi          = {10.1109/MM.2024.3375070},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {8-17},
  shortjournal = {IEEE Micro},
  title        = {AMD next-generation “Zen 4” core and 4th gen AMD EPYC server CPUs},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on hot chips 2023. <em>MICRO</em>,
<em>44</em>(3), 6–7. (<a
href="https://doi.org/10.1109/MM.2024.3396008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue of IEEE Micro is devoted to selected top-pick articles presented at Hot Chips 2023. The Hot Chips Conference serves as a leading venue for presenting the technical details of innovative microchips on a wide range of topics, including computing, memory, interconnection, and cooling technologies. This particular issue features articles from AMD, Intel, UC Berkeley, Samsung, Cerebras, and Fabric8Labs on different topics including server CPUs, photonic interconnects, processing-in-memory, and artificial intelligence accelerators, as well as new cooling technologies.},
  archive      = {J_MICRO},
  author       = {Heiner Litz and Natalia Vassilieva},
  doi          = {10.1109/MM.2024.3396008},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot chips 2023},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). An incoming world of decoupling siliconomy. <em>MICRO</em>,
<em>44</em>(3), 4–5. (<a
href="https://doi.org/10.1109/MM.2024.3394671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue presents top works selected from 2023 Hot Chips symposium (HotChips 2023.) Beyond their influence to our daily lives, semiconductor chips have also emerged as a critical asset in nation-to-nation geopolitics.},
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2024.3394671},
  journal      = {IEEE Micro},
  month        = {5},
  number       = {3},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {An incoming world of decoupling siliconomy},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Party like it’s 1999? <em>MICRO</em>, <em>44</em>(2),
78–80. (<a href="https://doi.org/10.1109/MM.2024.3372349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative AI has created a gold rush today, but that rush has not yet grown into either a productivity boom or a financial bubble. There are good reasons to think this rush could become either one. Some productivity gains seems likely, but the emergence of a financial bubble is more difficult to predict. Do today&#39;s conditions resemble those that created a bubble in the late 1990s? We consider a few crucial similarities and differences between the dot-com boom and telecom bubble of the late 1990s and the recent experience with generative AI},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2024.3372349},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {78-80},
  shortjournal = {IEEE Micro},
  title        = {Party like it’s 1999?},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part IX: Patent
families. <em>MICRO</em>, <em>44</em>(2), 72–77. (<a
href="https://doi.org/10.1109/MM.2024.3373342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is the next article in the series on the patenting behavior and characteristics of computer architecture companies. This article continues to analyze the characteristics for patent families.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2024.3373342},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {72-77},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part IX: Patent families},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fully digital and row-pipelined compute-in-memory neural
network accelerator with system-on-chip-level benchmarking for
augmented/virtual reality applications. <em>MICRO</em>, <em>44</em>(2),
61–70. (<a href="https://doi.org/10.1109/MM.2023.3338059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute-in-memory (CIM) has emerged as an effective technique to address memory access bottlenecks for deep neural networks (DNNs). Augmented/virtual reality (AR/VR) devices require running high-performance DNN inference at tight power budgets, making CIMs ideal candidates for low-power on-device acceleration. While high energy efficiencies have been reported at the CIM macro levels, the energy efficiencies of CIM-based accelerators at the system-on-chip (SoC) level have been underexplored for realistic system integration considerations. In this work, we present a CIM accelerator architecture comprising 16 row-pipelined, fully digital CIM macros and provide a comprehensive analysis of CIM energy-efficiency benefits at the SoC level targeting representative AR/VR workloads. Two key results are as follows. 1) Realistic SoC-level CIM accelerator energy efficiency may be ∼50% lower than the CIM macro-level peak energy efficiency when additional logic, memory hierarchies, and NN-dependent suboptimal compute utilization are considered. 2) The CIM accelerator still demonstrates up to ∼2.1× energy savings at the SoC level compared to a systolic-array-based DNN accelerator at iso-peak throughput.},
  archive      = {J_MICRO},
  author       = {H. Ekin Sumbul and Jae-sun Seo and Daniel H. Morris and Edith Beigne},
  doi          = {10.1109/MM.2023.3338059},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {61-70},
  shortjournal = {IEEE Micro},
  title        = {A fully digital and row-pipelined compute-in-memory neural network accelerator with system-on-chip-level benchmarking for Augmented/Virtual reality applications},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pipelined and partitionable forward error correction and
cyclic redundancy check circuitry implementation for PCI express 6.0 and
compute express link 3.0. <em>MICRO</em>, <em>44</em>(2), 50–59. (<a
href="https://doi.org/10.1109/MM.2023.3328832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sixth generation of PCIe (PCIe 6.0) specification adopted four-level pulse-amplitude modulation signaling at 64 GT/s for maintaining the same channel reach, cost, and power profile as previous generations. Lightweight forward error correction (FEC), a strong cyclic redundancy check (CRC), and link-level replay mechanisms deliver low latency, high bandwidth efficiency, and high reliability. Compute Express Link (CXL) uses PCIe 6.0 physical layer with latency-optimization mechanisms. Our nonpipelined implementation of FEC and the CRC is incorporated into PCIe 6.0 and CXL 3.0 specifications. We also propose a partitionable and pipelined implementation for FEC and the CRC for lowering gate count and latency. Synthesis results from the Synopsys Design Compiler demonstrates that for a 16-lane (x16) PCIe/CXL link partitionable to up to x4s, with four independent controllers using independently partitionable logic, we achieve a gate count of approximately 100,000 for the transmit and receive side with a FEC + CRC delay of less than 1 ns in each direction.},
  archive      = {J_MICRO},
  author       = {Debendra Das Sharma and Swadesh Choudhary},
  doi          = {10.1109/MM.2023.3328832},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {50-59},
  shortjournal = {IEEE Micro},
  title        = {Pipelined and partitionable forward error correction and cyclic redundancy check circuitry implementation for PCI express 6.0 and compute express link 3.0},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling artificial intelligence supercomputers with
domain-specific networks. <em>MICRO</em>, <em>44</em>(2), 41–49. (<a
href="https://doi.org/10.1109/MM.2023.3330079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systems designed for artificial intelligence (AI) training and inference exhibit characteristics of both capacity and capability systems that require both tight coupling and strong scaling for model parallelism as well as weak scaling for data parallelism in distributed systems. In addition, managing enormous, 100 billion-parameter language models and trillion-token datasets introduces formidable computational challenges for today’s supercomputing infrastructure. Communication and computation are two intertwined aspects of parallel computing, including AI domain-specific supercomputers, and this article explores the vital role of interconnection networks in large-scale systems. This work argues how domain-specific networks are a critical enabling technology necessary for AI supercomputers. In particular, we advocate for flexible, low-latency interconnects capable of delivering high throughput across massive scales with tens of thousands of endpoints. Additionally, we stress the importance of reliability and resilience in handling long-duration training workloads and the demanding inference needs of domain-specific workloads.},
  archive      = {J_MICRO},
  author       = {Dennis Abts and John Kim},
  doi          = {10.1109/MM.2023.3330079},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {41-49},
  shortjournal = {IEEE Micro},
  title        = {Enabling artificial intelligence supercomputers with domain-specific networks},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-speed data communication with advanced networks in
large language model training. <em>MICRO</em>, <em>44</em>(2), 31–40.
(<a href="https://doi.org/10.1109/MM.2024.3360081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) like Generative Pre-trained Transformer, Bidirectional Encoder Representations from Transformers, and T5 are pivotal in natural language processing. Their distributed training is influenced by high-speed interconnects. This article characterizes their training performance across various interconnects and communication protocols: TCP/IP, Internet Protocol over InfiniBand, (IPoIB), and Remote Direct Memory Access (RDMA), using data and model parallelism. RDMA-100 Gbps outperforms IPoIB-100 Gbps and TCP/IP-10 Gbps, with average gains of 2.5x and 4.8x in data parallelism, while in model parallelism, the gains were 1.1x and 1.2x. RDMA achieves the highest interconnect utilization (up to 60 Gbps), compared to IPoIB with up to 20 Gbps and TCP/IP with up to 9 Gbps. Larger models demand increased communication bandwidth, with AllReduce in data parallelism consuming up to 91% of training time, and forward receive and back-embedding AllReduce in model parallelism taking up to 90%. The larger-scale experiment confirms that communication predominates iterations. Our findings underscore the significance of communication in distributed LLM training and present opportunities for optimization.},
  archive      = {J_MICRO},
  author       = {Liuyao Dai and Hao Qi and Weicong Chen and Xiaoyi Lu},
  doi          = {10.1109/MM.2024.3360081},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {31-40},
  shortjournal = {IEEE Micro},
  title        = {High-speed data communication with advanced networks in large language model training},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprex: In-network compression for accelerating IoT
analytics at scale. <em>MICRO</em>, <em>44</em>(2), 20–30. (<a
href="https://doi.org/10.1109/MM.2023.3343498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enable the Internet of Things (IoT) to scale at the level of next-generation smart cities and grids, there is a need for a cost-effective infrastructure for hosting IoT analytics applications. Offload and acceleration via smartNICs have been shown to provide benefits to these workloads. However, even with offload, long-term analysis on IoT data still needs to operate on a massive number of device updates, often in the form of small messages. Despite offloading, the ingestion of these updates continues to present server bottlenecks. In this article, we present domain-specific compression and batching engines that leverage the unique properties of IoT messages to reduce the load on analytics servers and improve their scalability. Using a prototype system based on InnovaFlex programmable smartNICs and several representative IoT benchmarks, we demonstrate that these techniques achieve up to 7$\times$× improvement over existing offload approaches.},
  archive      = {J_MICRO},
  author       = {Rafael Oliveira and Ada Gavrilovska},
  doi          = {10.1109/MM.2023.3343498},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {20-30},
  shortjournal = {IEEE Micro},
  title        = {Comprex: In-network compression for accelerating IoT analytics at scale},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compression analysis for BlueField-2/-3 data processing
units: Lossy and lossless perspectives. <em>MICRO</em>, <em>44</em>(2),
8–19. (<a href="https://doi.org/10.1109/MM.2023.3343636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data processing unit (DPU) with programmable smart network interface card containing system-on-chip (SoC) cores is now a valuable addition to the host CPU, finding use in high-performance computing (HPC) and data center clusters for its advanced features, notably, a hardware-based data compression engine (C-engine). With the convergence of big data, HPC, and machine learning, data volumes burden communication and storage, making efficient compression vital. This positions DPUs as tools to accelerate compression workloads and enhance data-intensive applications. This article characterizes lossy (e.g., SZ3) and lossless (e.g., DEFLATE, lz4, and zlib) compression algorithms using seven real-world datasets on Nvidia BlueField-2/-3 DPUs. We explore the potential opportunities for offloading these compression workloads from the host. Our findings demonstrate that the C-engine within the DPU can achieve up to 26.8x speedup compared to its SoC core. We also provide insights on harnessing BlueField for compression, presenting seven crucial takeaways to steer future compression research with DPUs.},
  archive      = {J_MICRO},
  author       = {Yuke Li and Arjun Kashyap and Yanfei Guo and Xiaoyi Lu},
  doi          = {10.1109/MM.2023.3343636},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {8-19},
  shortjournal = {IEEE Micro},
  title        = {Compression analysis for BlueField-2/-3 data processing units: Lossy and lossless perspectives},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on hot interconnects 30. <em>MICRO</em>,
<em>44</em>(2), 6–7. (<a
href="https://doi.org/10.1109/MM.2024.3373338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IEEE Hot Interconnects Symposium celebrated its 30th year in 2023 with an exceptional series of presentations from industry and academia on the design, implementation, and effective use of high-performance interconnects. A core role of the Symposium is to promote the dissemination of cutting-edge research in the field. To this end, the 2023 Symposium included eight peer-reviewed presentations. This special issue of IEEE Micro presents revised and expanded versions of five of the best of these contributions.},
  archive      = {J_MICRO},
  author       = {Scott Levy and Whit Schonbein},
  doi          = {10.1109/MM.2024.3373338},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot interconnects 30},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Beyond wires: The future of interconnects. <em>MICRO</em>,
<em>44</em>(2), 4–5. (<a
href="https://doi.org/10.1109/MM.2024.3373336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue introduces a selection of outstanding papers originally presented at Hot Interconnects (HotI-30) in 2023 and features an article from Meta Reality Labs exploring the use of compute-in-memory for AR/VR applications.},
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2024.3373336},
  journal      = {IEEE Micro},
  month        = {3},
  number       = {2},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Beyond wires: The future of interconnects},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). After the gold rush. <em>MICRO</em>, <em>44</em>(1), 76–78.
(<a href="https://doi.org/10.1109/MM.2023.3339186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What determines market prospects during and after a commercial gold rush, such as the boom presently taking place in commercial generative AI? Many firms face similar technical challenges and commercial risks, and the resolution of one firm’s challenge correlates with that of another. That provides a way of cataloging risks, and the general prospects of some categories of firms, even though it does not lead to insight related to the prospects of specific firms.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2023.3339186},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {76-78},
  shortjournal = {IEEE Micro},
  title        = {After the gold rush},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Analysis of historical patenting behavior and patent
characteristics of computer architecture companies—part VIII: Patent
families. <em>MICRO</em>, <em>44</em>(1), 70–74. (<a
href="https://doi.org/10.1109/MM.2024.3353948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is the next article in the series on the patenting behavior and characteristics of computer architecture companies. This article analyzes the characteristics for patent families.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2024.3353948},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {70-74},
  shortjournal = {IEEE Micro},
  title        = {Analysis of historical patenting behavior and patent characteristics of computer architecture Companies—Part VIII: Patent families},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acceleration of a classic McEliece postquantum cryptosystem
with cache processing. <em>MICRO</em>, <em>44</em>(1), 59–68. (<a
href="https://doi.org/10.1109/MM.2023.3304425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The National Institute of Standards and Technology’s postquantum cryptography standardization process is in its fourth round, with a first key encapsulation mechanism standard based on learning with errors and three candidates based on error-correcting codes. These primitives’ implementation are designed to be optimal on classical hardware architecture targets. However, emerging architectures with processing in memory (PIM), made to be multipurpose, contrary to cryptographic coprocessors, have proven their efficiency in multiple use cases and show better overall computational speed. In this article, we show that classic McEliece performance can be improved on PIM architectures. Notably, the public-key-generation benefits of a 12.6× speedup on architectures with bit-line operations. We also describe an open source RISC-V simulator specifically developed for our experiments, including both in-cache and vectored operations. We discuss how these architecture changes may open the possibility of redesigning primitives or parameter sets for better efficiency.},
  archive      = {J_MICRO},
  author       = {Cyrius Nugier and Vincent Migliore},
  doi          = {10.1109/MM.2023.3304425},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {59-68},
  shortjournal = {IEEE Micro},
  title        = {Acceleration of a classic McEliece postquantum cryptosystem with cache processing},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy by memory design: Visions and open problems.
<em>MICRO</em>, <em>44</em>(1), 49–58. (<a
href="https://doi.org/10.1109/MM.2023.3337094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The threat to data privacy has never been more alarming than it is today. Among existing privacy-enhancing technologies, differential privacy (DP) is widely accepted as the de facto standard for privacy preservation. Yet, the software-based implementation of DP mechanisms is neither friendly for lightweight devices nor secure against side-channel attacks. In this article, we propose a first-of-its-kind design regime that realizes DP in hardware memories. The salient feature of this novel design lies in its transformation of the notorious memory noises at subnominal voltages into the desired DP noises, thereby achieving power savings and privacy preservation simultaneously: a “win-win” outcome. We demonstrate the feasibility of this design regime using a 1-Kb memory prototype based on 45-nm technology. For future prospects, a research road map that contains open research problems is delineated for the broad research community.},
  archive      = {J_MICRO},
  author       = {Jianqing Liu and Na Gong},
  doi          = {10.1109/MM.2023.3337094},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {49-58},
  shortjournal = {IEEE Micro},
  title        = {Privacy by memory design: Visions and open problems},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cachet: Low-overhead integrity verification on metadata
cache in secure nonvolatile memory systems. <em>MICRO</em>,
<em>44</em>(1), 38–48. (<a
href="https://doi.org/10.1109/MM.2023.3335354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data confidentiality, integrity, and persistence are essential in secure nonvolatile memory (NVM) systems. However, coupling authenticated memory encryption with security metadata persistence incurs nonnegligible performance overheads. Particularly, the integrity update process for the metadata cache bottlenecks execution performance. In this article, we propose Cachet, a novel integrity verification scheme. Instead of integrity trees, which require multiple hash calculations to update their integrity, Cachet employs set hash functions to authenticate the metadata cache. The observation that underlies Cachet is that the integrity of the metadata cache is never verified at runtime, and the recovery process necessitates the restoration of all data within the metadata cache. Cachet allows the metadata integrity update with two parallel hash calculations, without imposing additional overheads during system recovery. Our evaluation results show that Cachet reduces execution time by 21%, NVM writes by 30%, and power consumption overheads by 22% compared to state-of-the-art solutions.},
  archive      = {J_MICRO},
  author       = {Tatsuya Kubo and Shinya Takamaeda-Yamazaki},
  doi          = {10.1109/MM.2023.3335354},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {38-48},
  shortjournal = {IEEE Micro},
  title        = {Cachet: Low-overhead integrity verification on metadata cache in secure nonvolatile memory systems},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COOL-NPU: Complementary online learning neural processing
unit. <em>MICRO</em>, <em>44</em>(1), 28–37. (<a
href="https://doi.org/10.1109/MM.2023.3330169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors propose a complementary online learning neural processing unit (COOL-NPU) to implement a highly accurate and high-energy-efficient online learning system. It reduces the energy consumption by combining the training methods of convolutional neural network (CNN) and spiking neural network (SNN) and eliminates the power overhead due to the redundant weight update by training trigger with SNN gradient. The proposed SNN core reduces the energy consumption of SNN-gradient generation by two-step encoding and reduces inference power by hierarchical cache with lookup table -mode. In addition, it supports neuron-level event-driven backward operation to maximize the effect of the training trigger. Fabricated with Samsung 28-nm CMOS technology, the COOL-NPU achieves 6.94 mJ/frame and 0.73 mAP for object detection, resulting in 47.7% energy reduction with a slight accuracy loss compared to previous state of the art.},
  archive      = {J_MICRO},
  author       = {Sangyeob Kim and Soyeon Kim and Seongyon Hong and Sangjin Kim and Jiwon Choi and Donghyeon Han and Hoi-Jun Yoo},
  doi          = {10.1109/MM.2023.3330169},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {28-37},
  shortjournal = {IEEE Micro},
  title        = {COOL-NPU: Complementary online learning neural processing unit},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A low-power artificial-intelligence-based 3-d rendering
processor with hybrid deep neural network computing. <em>MICRO</em>,
<em>44</em>(1), 17–27. (<a
href="https://doi.org/10.1109/MM.2023.3328965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A low-power artificial intelligence (AI)-based 3-D rendering processor is proposed for metaverse solutions in mobile platforms. It suggests a brain-inspired rendering acceleration architecture designed with a visual perception core. It removes useless computations by realizing 1) spatial attention, 2) temporal familiarity, and 3) top-down attention. The remaining deep neural network (DNN) inference tasks are accelerated by a hybrid neural engine that utilizes both coarse-grained and fine-grained sparsity exploitation simultaneously. It divides the DNN tasks into sparse and dense data and allocates them to the two different neural engines, which focus on zero skipping and data reusability, respectively. Thanks to the centrifugal sampling-based workload prediction, it can dynamically divide DNN computations while minimizing peak signal-to-noise ratio loss caused by the prediction. Fabricated with 28-nm CMOS technology, the processor successfully demonstrates a maximum 118 frames-per-second rendering while consuming 99.95% lower power compared with modern GPUs.},
  archive      = {J_MICRO},
  author       = {Donghyeon Han and Junha Ryu and Sangyeob Kim and Sangjin Kim and Jongjun Park and Hoi-Jun Yoo},
  doi          = {10.1109/MM.2023.3328965},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {17-27},
  shortjournal = {IEEE Micro},
  title        = {A low-power artificial-intelligence-based 3-D rendering processor with hybrid deep neural network computing},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A compressed spiking neural network onto a memcapacitive
in-memory computing array. <em>MICRO</em>, <em>44</em>(1), 8–16. (<a
href="https://doi.org/10.1109/MM.2023.3285529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) enable the execution of deep learning-compatible tasks and approximation algorithms with low latency and low power consumption by operating on a neuromorphic system. Adopting analog in-memory computing (AiMC) in a neuromorphic system can build a system that has an advantage in memory density over a pure digital implementation. However, sensing the AiMC output with simple circuitry inevitably leads to unintended nonlinearities. In this study, we design a neuromorphic circuit using memcapacitive AiMC synapses with ultra-low power. We combine circuit nonlinearity-aware training (CNAT) with network compression techniques to prevent the SNN from losing accuracy caused by the neuron circuit’s nonlinearity and the synapse’s low resolution. The training runs on a machine learning framework and does not need to incorporate computationally intensive SPICE simulations. As simulated, our circuit performs MNIST classifications with almost no loss from ideal accuracy (97.64%) and consumes 15.7 nJ per inference.},
  archive      = {J_MICRO},
  author       = {Reon Oshio and Takuya Sugahara and Atsushi Sawada and Mutsumi Kimura and Renyuan Zhang and Yasuhiko Nakashima},
  doi          = {10.1109/MM.2023.3285529},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {8-16},
  shortjournal = {IEEE Micro},
  title        = {A compressed spiking neural network onto a memcapacitive in-memory computing array},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on COOL chips. <em>MICRO</em>, <em>44</em>(1),
6–7. (<a href="https://doi.org/10.1109/MM.2024.3353949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This introduction to the special issue on low-power, high speed chips (COOL chips) discusses state-of-the-art COOL chips and the challenges facing researchers. It introduces four articles exploring different solutions for reducing power consumption and enhancing chip performance.},
  archive      = {J_MICRO},
  author       = {Ryusuke Egawa and Yasutaka Wada},
  doi          = {10.1109/MM.2024.3353949},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on COOL chips},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Computing with COOL chips. <em>MICRO</em>, <em>44</em>(1),
4–5. (<a href="https://doi.org/10.1109/MM.2024.3354368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue, IEEE MICRO welcomes the newly inaugurated Editor-in-Chief Dr. Hsien-Hsin Sean Lee and introduces the Special Issue on COOL chips for the state-of-the-art in low-power design for computing.},
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2024.3354368},
  journal      = {IEEE Micro},
  month        = {1},
  number       = {1},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Computing with COOL chips},
  volume       = {44},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
