<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIMJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="bimj---124">BIMJ - 124</h2>
<ul>
<li><details>
<summary>
(2024). Landmarking for left-truncated competing risk data.
<em>BIMJ</em>, <em>66</em>(8), e202400083. (<a
href="https://doi.org/10.1002/bimj.202400083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landmarking is an alternative to complex multistate models when the aim is to calculate dynamic predictions. We develop the concept of landmarking for the case of left truncation and competing risks from the application background of drug safety assessment in pregnancy. The method is illustrated with a cohort study of the German Embryotox Pharmacovigilance Institute in Berlin to assess if the risk or the cumulative incidence of adverse pregnancy outcomes, like spontaneous abortions (SABs), is increased in fluoroquinolone-exposed women. Furthermore, we conduct an extensive simulation study to compare the dynamic predictions and coefficient estimates obtained by landmarking to those from nonparametric multistate models and classical time-dependent covariate Cox regression. The results from the simulation study indicate that attenuation of the effects is present in the landmark estimates, also in the complex setting considered here, but the estimates are still close to those from the multistate models. Regarding the Berlin fluoroquinolone data, the fluoroquinolone exposure of a pregnant woman in the first trimester seems to increase her cumulative incidence of elective termination of pregnancy over women never exposed before, but there is no evidence of a significantly increased risk or cumulative incidence in exposed women for SABs. This supports previous results on the same data, which were driven from an analysis without landmarking methods.},
  archive      = {J_BIMJ},
  author       = {Theresa Unseld and Tobias Bluhmki and Jan Beyersmann and Evelin Beck and Stephanie Padberg and Regina Stegherr},
  doi          = {10.1002/bimj.202400083},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e202400083},
  shortjournal = {Bio. J.},
  title        = {Landmarking for left-truncated competing risk data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group integrative dynamic factor models with application to
multiple subject brain connectivity. <em>BIMJ</em>, <em>66</em>(8),
e202300370. (<a href="https://doi.org/10.1002/bimj.202300370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel framework for dynamic factor model-based group-level analysis of multiple subjects time-series data, called GRoup Integrative DYnamic factor (GRIDY) models. The framework identifies and characterizes intersubject similarities and differences between two predetermined groups by considering a combination of group spatial information and individual temporal dynamics. Furthermore, it enables the identification of intrasubject similarities and differences over time by employing different model configurations for each subject. Methodologically, the framework combines a novel principal angle-based rank selection algorithm and a noniterative integrative analysis framework. Inspired by simultaneous component analysis, this approach also reconstructs identifiable latent factor series with flexible covariance structures. The performance of the GRIDY models is evaluated through simulations conducted under various scenarios. An application is also presented to compare resting-state functional MRI data collected from multiple subjects in autism spectrum disorder and control groups.},
  archive      = {J_BIMJ},
  author       = {Younghoon Kim and Zachary F. Fisher and Vladas Pipiras},
  doi          = {10.1002/bimj.202300370},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e202300370},
  shortjournal = {Bio. J.},
  title        = {Group integrative dynamic factor models with application to multiple subject brain connectivity},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-cohort mixture analysis: A data integration approach
with applications on gestational age and DNA-methylation-derived
gestational age acceleration metrics. <em>BIMJ</em>, <em>66</em>(8),
e202300270. (<a href="https://doi.org/10.1002/bimj.202300270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data integration of multiple studies can provide enhanced exposure contrast and statistical power to examine associations between environmental exposure mixtures and health outcomes. Extant research has combined populations and identified an overall mixture–outcome association, without accounting for differences across studies. We extended the Bayesian Weighted Quantile Sum (BWQS) regression to a hierarchical framework to analyze mixtures across cohorts. The hierarchical BWQS (HBWQS) approach aggregates sample size of multiple cohorts to calculate an overall mixture index, thereby identifying the most harmful exposure(s) across cohorts; and provides cohort-specific associations between the overall mixture index and the outcome. We showed results from 10 simulated scenarios including four mixture components in three, eight, and ten populations, and two real-case examples on the association between prenatal metal mixture exposure—comprising arsenic, cadmium, and lead—and both gestational age and epigenetic-derived gestational age acceleration metrics. Simulated scenarios showed good empirical coverage and little bias for all HBWQS-estimated parameters. The Watanabe–Akaike information criterion showed a better average performance for the HBWQS regression than the BWQS across scenarios. HBWQS results incorporating cohorts within the national Environmental influences on Child Health Outcomes (ECHO) program from three different sites showed that the environmental mixture was negatively associated with gestational age in a single site. The HBWQS approach facilitates the combination of multiple cohorts and accounts for individual cohort differences in mixture analyses. HBWQS findings can be used to develop regulations, policies, and interventions regarding multiple co-occurring environmental exposures and it will maximize the use of extant publicly available data.},
  archive      = {J_BIMJ},
  author       = {Elena Colicino and Roberto Ascari and Hachem Saddiki and Francheska Merced-Nieves and Nicolò Foppa Pedretti and Kathi Huddleston and Robert O Wright and Rosalind J Wright},
  doi          = {10.1002/bimj.202300270},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e202300270},
  shortjournal = {Bio. J.},
  title        = {Cross-cohort mixture analysis: A data integration approach with applications on gestational age and DNA-methylation-derived gestational age acceleration metrics},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The replication of equivalence studies. <em>BIMJ</em>,
<em>66</em>(8), e202300232. (<a
href="https://doi.org/10.1002/bimj.202300232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Replication studies are increasingly conducted to assess the credibility of scientific findings. Most of these replication attempts target studies with a superiority design, but there is a lack of methodology regarding the analysis of replication studies with alternative types of designs, such as equivalence. In order to fill this gap, we propose two approaches, the two-trials rule and the sceptical two one-sided tests (TOST) procedure, adapted from methods used in superiority settings. Both methods have the same overall Type-I error rate, but the sceptical TOST procedure allows replication success even for nonsignificant original or replication studies. This leads to a larger project power and other differences in relevant operating characteristics. Both methods can be used for sample size calculation of the replication study, based on the results from the original one. The two methods are applied to data from the Reproducibility Project: Cancer Biology.},
  archive      = {J_BIMJ},
  author       = {Charlotte Micheloud and Leonhard Held},
  doi          = {10.1002/bimj.202300232},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e202300232},
  shortjournal = {Bio. J.},
  title        = {The replication of equivalence studies},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semiparametric two-sample density ratio model with a
change point. <em>BIMJ</em>, <em>66</em>(8), e202300214. (<a
href="https://doi.org/10.1002/bimj.202300214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The logistic regression model for a binary outcome with a continuous covariate can be expressed equivalently as a two-sample density ratio model for the covariate. Utilizing this equivalence, we study a change-point logistic regression model within the corresponding density ratio modeling framework. We investigate estimation and inference methods for the density ratio model and develop maximal score-type tests to detect the presence of a change point. In contrast to existing work, the density ratio modeling framework facilitates the development of a natural Kolmogorov–Smirnov type test to assess the validity of the logistic model assumptions. A simulation study is conducted to evaluate the finite-sample performance of the proposed tests and estimation methods. We illustrate the proposed approach using a mother-to-child HIV-1 transmission data set and an oral cancer data set.},
  archive      = {J_BIMJ},
  author       = {Jiahui Feng and Kin Yau Wong and Chun Yin Lee},
  doi          = {10.1002/bimj.202300214},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e202300214},
  shortjournal = {Bio. J.},
  title        = {A semiparametric two-sample density ratio model with a change point},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixture cure semiparametric accelerated failure time models
with partly interval-censored data. <em>BIMJ</em>, <em>66</em>(8),
e202300203. (<a href="https://doi.org/10.1002/bimj.202300203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical survival analysis, the situation of no event for a patient can arise even after a long period of waiting time, which means a portion of the population may never experience the event of interest. Under this circumstance, one remedy is to adopt a mixture cure Cox model to analyze the survival data. However, if there clearly exhibits an acceleration (or deceleration) factor among their survival times, then an accelerated failure time (AFT) model will be preferred, leading to a mixture cure AFT model. In this paper, we consider a penalized likelihood method to estimate the mixture cure semiparametric AFT models, where the unknown baseline hazard is approximated using Gaussian basis functions. We allow partly interval-censored survival data which can include event times and left-, right-, and interval-censoring times. The penalty function helps to achieve a smooth estimate of the baseline hazard function. We will also provide asymptotic properties to the estimates so that inferences can be made on regression parameters and hazard-related quantities. Simulation studies are conducted to evaluate the model performance, which includes a comparative study with an existing method from the smcure R package. The results show that our proposed penalized likelihood method has acceptable performance in general and produces less bias when faced with the identifiability issue compared to smcure . To illustrate the application of our method, a real case study involving melanoma recurrence is conducted and reported. Our model is implemented in our R package aftQnp which is available from https://github.com/Isabellee4555/aftQnP .},
  archive      = {J_BIMJ},
  author       = {Isabel Li and Jun Ma and Benoit Liquet},
  doi          = {10.1002/bimj.202300203},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e202300203},
  shortjournal = {Bio. J.},
  title        = {Mixture cure semiparametric accelerated failure time models with partly interval-censored data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain selection for gaussian process data: An application
to electrocardiogram signals. <em>BIMJ</em>, <em>66</em>(8), e70018. (<a
href="https://doi.org/10.1002/bimj.70018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes and the Kullback–Leibler divergence have been deeply studied in statistics and machine learning. This paper marries these two concepts and introduce the local Kullback–Leibler divergence to learn about intervals where two Gaussian processes differ the most. We address subtleties entailed in the estimation of local divergences and the corresponding interval of local maximum divergence as well. The estimation performance and the numerical efficiency of the proposed method are showcased via a Monte Carlo simulation study. In a medical research context, we assess the potential of the devised tools in the analysis of electrocardiogram signals.},
  archive      = {J_BIMJ},
  author       = {Nicolás Hernández and Gabriel Martos},
  doi          = {10.1002/bimj.70018},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70018},
  shortjournal = {Bio. J.},
  title        = {Domain selection for gaussian process data: An application to electrocardiogram signals},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting interactions in high-dimensional data using cross
leverage scores. <em>BIMJ</em>, <em>66</em>(8), e70014. (<a
href="https://doi.org/10.1002/bimj.70014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a variable selection method for interactions in regression models on large data in the context of genetics. The method is intended for investigating the influence of single-nucleotide polymorphisms (SNPs) and their interactions on health outcomes, which is a p ≫ n $p\gg n$ problem. We introduce cross leverage scores (CLSs) to detect interactions of variables while maintaining interpretability. Using this method, it is not necessary to consider every possible interaction between variables individually, which would be very time-consuming even for moderate amounts of variables. Instead, we calculate the CLS for each variable and obtain a measure of importance for this variable. Calculating the scores remains time-consuming for large data sets. The key idea for scaling to large data is to divide the data into smaller random batches or consecutive windows of variables. This avoids complex and time-consuming computations on high-dimensional matrices by performing the computations only for small subsets of the data, which is less costly. We compare these methods to provable approximations of CLS based on sketching, which aims at summarizing data succinctly. In a simulation study, we show that the CLSs are directly linked to the importance of a variable in the sense of an interaction effect. We further show that the approximation approaches are appropriate for performing the calculations efficiently on arbitrarily large data while preserving the interaction detection effect of the CLS. This underlines their scalability to genome wide data. In addition, we evaluate the methods on real data from the HapMap project.},
  archive      = {J_BIMJ},
  author       = {Sven Teschke and Katja Ickstadt and Alexander Munteanu},
  doi          = {10.1002/bimj.70014},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70014},
  shortjournal = {Bio. J.},
  title        = {Detecting interactions in high-dimensional data using cross leverage scores},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model selection for ordinary differential equations: A
statistical testing approach. <em>BIMJ</em>, <em>66</em>(8), e70013. (<a
href="https://doi.org/10.1002/bimj.70013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinary differential equations (ODEs) are foundational tools in modeling intricate dynamics across a gamut of scientific disciplines. Yet, a possibility to represent a single phenomenon through multiple ODE models, driven by different understandings of nuances in internal mechanisms or abstraction levels, presents a model selection challenge. This study introduces a testing-based approach for ODE model selection amidst statistical noise. Rooted in the model misspecification framework, we adapt classical statistical paradigms (Vuong and Hotelling) to the ODE context, allowing for the comparison and ranking of diverse causal explanations without the constraints of nested models. Our simulation studies numerically investigate the statistical properties of the test, demonstrating its attainment of the nominal size and power across various settings. Real-world data examples further underscore the algorithm&#39;s applicability in practice. To foster accessibility and encourage real-world applications, we provide a user-friendly Python implementation of our model selection algorithm, bridging theoretical advancements with hands-on tools for the scientific community.},
  archive      = {J_BIMJ},
  author       = {Itai Dattner and Shota Gugushvili and Oleksandr Laskorunskyi},
  doi          = {10.1002/bimj.70013},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70013},
  shortjournal = {Bio. J.},
  title        = {Model selection for ordinary differential equations: A statistical testing approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A matched design for causal inference with survey data:
Evaluation of medical marijuana legalization in kentucky and tennessee.
<em>BIMJ</em>, <em>66</em>(8), e70012. (<a
href="https://doi.org/10.1002/bimj.70012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A concern surrounding marijuana legalization is that driving after marijuana use may become more prevalent. Survey data are valuable for estimating policy effects, however their observational nature and unequal sampling probabilities create challenges for causal inference. To estimate population-level effects using survey data, we propose a matched design and implement sensitivity analyses to quantify how robust conclusions are to unmeasured confounding. Both theoretical justification and simulation studies are presented. We found no support that marijuana legalization increased tolerant behaviors and attitudes toward driving after marijuana use, and these conclusions seem moderately robust to unmeasured confounding.},
  archive      = {J_BIMJ},
  author       = {Marco H. Benedetti and Bo Lu and Motao Zhu},
  doi          = {10.1002/bimj.70012},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70012},
  shortjournal = {Bio. J.},
  title        = {A matched design for causal inference with survey data: Evaluation of medical marijuana legalization in kentucky and tennessee},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-based decision making: Estimands for sequential
prediction under interventions. <em>BIMJ</em>, <em>66</em>(8), e70011.
(<a href="https://doi.org/10.1002/bimj.70011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction models are used among others to inform medical decisions on interventions. Typically, individuals with high risks of adverse outcomes are advised to undergo an intervention while those at low risk are advised to refrain from it. Standard prediction models do not always provide risks that are relevant to inform such decisions: for example, an individual may be estimated to be at low risk because similar individuals in the past received an intervention which lowered their risk. Therefore, prediction models supporting decisions should target risks belonging to defined intervention strategies. Previous works on prediction under interventions assumed that the prediction model was used only at one time point to make an intervention decision. In clinical practice, intervention decisions are rarely made only once: they might be repeated, deferred, and reevaluated. This requires estimated risks under interventions that can be reconsidered at several potential decision moments. In the current work, we highlight key considerations for formulating estimands in sequential prediction under interventions that can inform such intervention decisions. We illustrate these considerations by giving examples of estimands for a case study about choosing between vaginal delivery and cesarean section for women giving birth. Our formalization of prediction tasks in a sequential, causal, and estimand context provides guidance for future studies to ensure that the right question is answered and appropriate causal estimation approaches are chosen to develop sequential prediction models that can inform intervention decisions.},
  archive      = {J_BIMJ},
  author       = {Kim Luijken and Paweł Morzywołek and Wouter van Amsterdam and Giovanni Cinà and Jeroen Hoogland and Ruth Keogh and Jesse H. Krijthe and Sara Magliacane and Thijs van Ommen and Niels Peek and Hein Putter and Maarten van Smeden and Matthew Sperrin and Junfeng Wang and Daniala L. Weir and Vanessa Didelez and Nan van Geloven},
  doi          = {10.1002/bimj.70011},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70011},
  shortjournal = {Bio. J.},
  title        = {Risk-based decision making: Estimands for sequential prediction under interventions},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simulating data from marginal structural models for a
survival time outcome. <em>BIMJ</em>, <em>66</em>(8), e70010. (<a
href="https://doi.org/10.1002/bimj.70010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marginal structural models (MSMs) are often used to estimate causal effects of treatments on survival time outcomes from observational data when time-dependent confounding may be present. They can be fitted using, for example, inverse probability of treatment weighting (IPTW). It is important to evaluate the performance of statistical methods in different scenarios, and simulation studies are a key tool for such evaluations. In such simulation studies, it is common to generate data in such a way that the model of interest is correctly specified, but this is not always straightforward when the model of interest is for potential outcomes, as is an MSM. Methods have been proposed for simulating from MSMs for a survival outcome, but these methods impose restrictions on the data-generating mechanism. Here, we propose a method that overcomes these restrictions. The MSM can be, for example, a marginal structural logistic model for a discrete survival time or a Cox or additive hazards MSM for a continuous survival time. The hazard of the potential survival time can be conditional on baseline covariates, and the treatment variable can be discrete or continuous. We illustrate the use of the proposed simulation algorithm by carrying out a brief simulation study. This study compares the coverage of confidence intervals calculated in two different ways for causal effect estimates obtained by fitting an MSM via IPTW.},
  archive      = {J_BIMJ},
  author       = {Shaun R. Seaman and Ruth H. Keogh},
  doi          = {10.1002/bimj.70010},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70010},
  shortjournal = {Bio. J.},
  title        = {Simulating data from marginal structural models for a survival time outcome},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Τ-inflated beta regression model for estimating τ-restricted
means and event-free probabilities for censored time-to-event data.
<em>BIMJ</em>, <em>66</em>(8), e70009. (<a
href="https://doi.org/10.1002/bimj.70009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we propose analysis of τ $\tau$ -restricted censored time-to-event data via a τ $\tau$ -inflated beta regression ( τ $\tau$ -IBR) model. The outcome of interest is min ⁡ ( τ , T ) ${\rm min}(\tau,T)$ , where T $T$ and τ $\tau$ are the time-to-event and follow-up duration, respectively. Our analysis goals include estimation and inference related to τ $\tau$ -restricted mean survival time ( τ $\tau$ -RMST) values and event-free probabilities at τ $\tau$ that address the censored nature of the data. In this setting, it is common to observe many individuals with min ⁡ ( τ , T ) = τ ${\rm min}(\tau,T)=\tau$ , a point mass that is typically overlooked in τ $\tau$ -restricted event-time analyses. Our proposed τ $\tau$ -IBR model is based on a decomposition of min ⁡ ( τ , T ) ${\rm min}(\tau,T)$ into τ ⁢ [ I ⁡ ( T ≥ τ ) + ( T / τ ) ⁢ I ⁡ ( T &lt; τ ) ] $\tau [I(T \ge \tau) +(T/\tau) I(T &amp;lt;\tau)]$ . We model the mean of this latter expression using joint logistic and beta regression models that are fit using an expectation-maximization algorithm. An alternative multiple imputation (MI) algorithm for fitting the -IBR model has the additional advantage of producing uncensored datasets for analysis. Simulations indicate excellent performance of the -IBR model(s), and corresponding -RMST estimates, in independent and dependent censoring settings. We apply our method to the Azithromycin for Prevention of Chronic Obstructive Pulmonary Disease (COPD) Exacerbations Trial. In addition to -IBR model results providing a nuanced understanding of the treatment effect, visually appealing heatmaps of the -restricted event times based on our MI datasets are given, a visualization not typically available for censored time-to-event data.},
  archive      = {J_BIMJ},
  author       = {Yizhuo Wang and Susan Murray},
  doi          = {10.1002/bimj.70009},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70009},
  shortjournal = {Bio. J.},
  title        = {τ-inflated beta regression model for estimating τ-restricted means and event-free probabilities for censored time-to-event data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incompletely observed nonparametric factorial designs with
repeated measurements: A wild bootstrap approach. <em>BIMJ</em>,
<em>66</em>(8), e70008. (<a
href="https://doi.org/10.1002/bimj.70008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many life science experiments or medical studies, subjects are repeatedly observed and measurements are collected in factorial designs with multivariate data. The analysis of such multivariate data is typically based on multivariate analysis of variance (MANOVA) or mixed models, requiring complete data, and certain assumption on the underlying parametric distribution such as continuity or a specific covariance structure, for example, compound symmetry. However, these methods are usually not applicable when discrete data or even ordered categorical data are present. In such cases, nonparametric rank-based methods that do not require stringent distributional assumptions are the preferred choice. However, in the multivariate case, most rank-based approaches have only been developed for complete observations. It is the aim of this work to develop asymptotic correct procedures that are capable of handling missing values, allowing for singular covariance matrices and are applicable for ordinal or ordered categorical data. This is achieved by applying a wild bootstrap procedure in combination with quadratic form-type test statistics. Beyond proving their asymptotic correctness, extensive simulation studies validate their applicability for small samples. Finally, two real data examples are analyzed.},
  archive      = {J_BIMJ},
  author       = {Lubna Amro and Frank Konietschke and Markus Pauly},
  doi          = {10.1002/bimj.70008},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70008},
  shortjournal = {Bio. J.},
  title        = {Incompletely observed nonparametric factorial designs with repeated measurements: A wild bootstrap approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smoothed estimation on optimal treatment regime under
semisupervised setting in randomized trials. <em>BIMJ</em>,
<em>66</em>(8), e70006. (<a
href="https://doi.org/10.1002/bimj.70006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A treatment regime refers to the process of assigning the most suitable treatment to a patient based on their observed information. However, prevailing research on treatment regimes predominantly relies on labeled data, which may lead to the omission of valuable information contained within unlabeled data, such as historical records and healthcare databases. Current semisupervised works for deriving optimal treatment regimes either rely on model assumptions or struggle with high computational burdens for even moderate-dimensional covariates. To address this concern, we propose a semisupervised framework that operates within a model-free context to estimate the optimal treatment regime by leveraging the abundant unlabeled data. Our proposed approach encompasses three key steps. First, we employ a single-index model to achieve dimension reduction, followed by kernel regression to impute the missing outcomes in the unlabeled data. Second, we propose various forms of semisupervised value functions based on the imputed values, incorporating both labeled and unlabeled data components. Lastly, the optimal treatment regimes are derived by maximizing the semisupervised value functions. We establish the consistency and asymptotic normality of the estimators proposed in our framework. Furthermore, we introduce a perturbation resampling procedure to estimate the asymptotic variance. Simulations confirm the advantageous properties of incorporating unlabeled data in the estimation for optimal treatment regimes. A practical data example is also provided to illustrate the application of our methodology. This work is rooted in the framework of randomized trials, with additional discussions extending to observational studies.},
  archive      = {J_BIMJ},
  author       = {Xiaoqi Jiao and Mengjiao Peng and Yong Zhou},
  doi          = {10.1002/bimj.70006},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70006},
  shortjournal = {Bio. J.},
  title        = {Smoothed estimation on optimal treatment regime under semisupervised setting in randomized trials},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional variable screening for ultra-high dimensional
longitudinal data with time interactions. <em>BIMJ</em>, <em>66</em>(8),
e70005. (<a href="https://doi.org/10.1002/bimj.70005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, we have been able to gather large amounts of genomic data at a fast rate, creating situations where the number of variables greatly exceeds the number of observations. In these situations, most models that can handle a moderately high dimension will now become computationally infeasible or unstable. Hence, there is a need for a prescreening of variables to reduce the dimension efficiently and accurately to a more moderate scale. There has been much work to develop such screening procedures for independent outcomes. However, much less work has been done for high-dimensional longitudinal data in which the observations can no longer be assumed to be independent. In addition, it is of interest to capture possible interactions between the genomic variable and time in many of these longitudinal studies. In this work, we propose a novel conditional screening procedure that ranks variables according to the likelihood value at the maximum likelihood estimates in a marginal linear mixed model, where the genomic variable and its interaction with time are included in the model. This is to our knowledge the first conditional screening approach for clustered data. We prove that this approach enjoys the sure screening property, and assess the finite sample performance of the method through simulations.},
  archive      = {J_BIMJ},
  author       = {Andrea Bratsberg and Abhik Ghosh and Magne Thoresen},
  doi          = {10.1002/bimj.70005},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70005},
  shortjournal = {Bio. J.},
  title        = {Conditional variable screening for ultra-high dimensional longitudinal data with time interactions},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing class imbalance in bayesian classification
through posterior probability adjustment. <em>BIMJ</em>, <em>66</em>(8),
e70004. (<a href="https://doi.org/10.1002/bimj.70004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a known issue in classification tasks that can lead to predictive bias toward dominant classes. This paper introduces a novel straightforward Bayesian framework that adjusts posterior probabilities to counteract the bias introduced by imbalanced data sets. Instead of relying on the mean posterior distribution of class probabilities, we propose a method that scales the posterior probability of each class according to their representation in the training data.},
  archive      = {J_BIMJ},
  author       = {Vahid Nassiri and Fetene Tekle and Kanaka Tatikola and Helena Geys},
  doi          = {10.1002/bimj.70004},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70004},
  shortjournal = {Bio. J.},
  title        = {Addressing class imbalance in bayesian classification through posterior probability adjustment},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential adaptive design method for incorporating external
data. <em>BIMJ</em>, <em>66</em>(8), e70003. (<a
href="https://doi.org/10.1002/bimj.70003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {External data (e.g., real-world data (RWD) and historical data) have become more readily available. This has led to rapidly increasing interest in exploring and evaluating ways of utilizing external data to facilitate traditional clinical trials (TCT), especially for rare diseases with high unmet medical needs where a TCT would be impractical and/or unethical. In this article, we focus on hybrid studies that incorporate external data into randomized clinical trials to augment the control arm and explore a complex innovative design. A sequential adaptive design conducts multiple interim assessments to improve the accuracy of estimates of agreement between external data and current data. At each interim assessment, we apply the inverse probability weighted power prior (IPW-PP) method to adaptively borrow information from external data to account for confounding and heterogeneity. The randomization ratio is dynamically adjusted during the interim assessment based on accumulatively augmented information to reduce the sample size of the current trial. Additionally, the proposed design can be extended to allow interim analyses for early efficacy/futility stopping, that is, early assessment of trial success or failure based on accumulated data, potentially reducing ineffective treatment exposure and unnecessary time and resources. The performance of the proposed method and design is evaluated via extensive simulation studies. The sequential adaptive design and IPW-PP approach having desirable properties are implemented.},
  archive      = {J_BIMJ},
  author       = {Jinmei Chen and Lixin Li and Yuhao Feng and Shein-Chung Chow and Ming Tan and Jianhong Pan and Pingyan Chen and Ying Wu},
  doi          = {10.1002/bimj.70003},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70003},
  shortjournal = {Bio. J.},
  title        = {Sequential adaptive design method for incorporating external data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the sampling distribution of posterior decision
summaries in bayesian clinical trials. <em>BIMJ</em>, <em>66</em>(8),
e70002. (<a href="https://doi.org/10.1002/bimj.70002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference and the use of posterior or posterior predictive probabilities for decision making have become increasingly popular in clinical trials. The current practice in Bayesian clinical trials relies on a hybrid Bayesian-frequentist approach where the design and decision criteria are assessed with respect to frequentist operating characteristics such as power and type I error rate conditioning on a given set of parameters. These operating characteristics are commonly obtained via simulation studies. The utility of Bayesian measures, such as “assurance,” that incorporate uncertainty about model parameters in estimating the probabilities of various decisions in trials has been demonstrated. However, the computational burden remains an obstacle toward wider use of such criteria. In this article, we propose methodology which utilizes large sample theory of the posterior distribution to define parametric models for the sampling distribution of the posterior summaries used for decision making. The parameters of these models are estimated using a small number of simulation scenarios, thereby refining these models to capture the sampling distribution for small to moderate sample size. The proposed approach toward the assessment of conditional and marginal operating characteristics and sample size determination can be considered as simulation-assisted rather than simulation-based. It enables formal incorporation of uncertainty about the trial assumptions via a design prior and significantly reduces the computational burden for the design of Bayesian trials in general.},
  archive      = {J_BIMJ},
  author       = {Shirin Golchi and James J. Willard},
  doi          = {10.1002/bimj.70002},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70002},
  shortjournal = {Bio. J.},
  title        = {Estimating the sampling distribution of posterior decision summaries in bayesian clinical trials},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse-weighted quantile regression with partially
interval-censored data. <em>BIMJ</em>, <em>66</em>(8), e70001. (<a
href="https://doi.org/10.1002/bimj.70001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approach to estimating censored quantile regression using inverse probability of censoring weighted (IPCW) methodology, specifically tailored for data sets featuring partially interval-censored data. Such data sets, often encountered in HIV/AIDS and cancer biomedical research, may include doubly censored (DC) and partly interval-censored (PIC) endpoints. DC responses involve either left-censoring or right-censoring alongside some exact failure time observations, while PIC responses are subject to interval-censoring. Despite the existence of complex estimating techniques for interval-censored quantile regression, we propose a simple and intuitive IPCW-based method, easily implementable by assigning suitable inverse-probability weights to subjects with exact failure time observations. The resulting estimator exhibits asymptotic properties, such as uniform consistency and weak convergence, and we explore an augmented-IPCW (AIPCW) approach to enhance efficiency. In addition, our method can be adapted for multivariate partially interval-censored data. Simulation studies demonstrate the new procedure&#39;s strong finite-sample performance. We illustrate the practical application of our approach through an analysis of progression-free survival endpoints in a phase III clinical trial focusing on metastatic colorectal cancer.},
  archive      = {J_BIMJ},
  author       = {Yeji Kim and Taehwa Choi and Seohyeon Park and Sangbum Choi and Dipankar Bandyopadhyay},
  doi          = {10.1002/bimj.70001},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70001},
  shortjournal = {Bio. J.},
  title        = {Inverse-weighted quantile regression with partially interval-censored data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional bayesian semiparametric models for small
samples: A principled approach to the analysis of cytokine expression
data. <em>BIMJ</em>, <em>66</em>(8), e70000. (<a
href="https://doi.org/10.1002/bimj.70000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In laboratory medicine, due to the lack of sample availability and resources, measurements of many quantities of interest are commonly collected over a few samples, making statistical inference particularly challenging. In this context, several hypotheses can be tested, and studies are not often powered accordingly. We present a semiparametric Bayesian approach to effectively test multiple hypotheses applied to an experiment that aims to identify cytokines involved in Crohn&#39;s disease (CD) infection that may be ongoing in multiple tissues. We assume that the positive correlation commonly observed between cytokines is caused by latent groups of effects, which in turn result from a common cause. These clusters are effectively modeled through a Dirichlet Process (DP) that is one of the most popular choices as nonparametric prior in Bayesian statistics and has been proven to be a powerful tool for model-based clustering. We use a spike–slab distribution as the base measure of the DP. The nonparametric part has been included in an additive model whose parametric component is a Bayesian hierarchical model. We include simulations that empirically demonstrate the effectiveness of the proposed testing procedure in settings that mimic our application&#39;s sample size and data structure. Our CD data analysis shows strong evidence of a cytokine gradient in the external intestinal tissue.},
  archive      = {J_BIMJ},
  author       = {Giovanni Poli and Raffaele Argiento and Amedeo Amedei and Francesco C. Stingo},
  doi          = {10.1002/bimj.70000},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {e70000},
  shortjournal = {Bio. J.},
  title        = {High-dimensional bayesian semiparametric models for small samples: A principled approach to the analysis of cytokine expression data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate scalar on multidimensional distribution
regression with application to modeling the association between physical
activity and cognitive functions. <em>BIMJ</em>, <em>66</em>(7),
e202400042. (<a href="https://doi.org/10.1002/bimj.202400042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new method for multivariate scalar on multidimensional distribution regression. Traditional approaches typically analyze isolated univariate scalar outcomes or consider unidimensional distributional representations as predictors. However, these approaches are suboptimal because (i) they fail to utilize the dependence between the distributional predictors and (ii) neglect the correlation structure of the response. To overcome these limitations, we propose a multivariate distributional analysis framework that harnesses the power of multivariate density functions and multitask learning. We develop a computationally efficient semiparametric estimation method for modeling the effect of the latent joint density on the multivariate response of interest. Additionally, we introduce a new conformal prediction algorithm for quantifying the uncertainty of our multivariate predictions based on subject characteristics and individualized distributional predictors, providing valuable insights into the conditional distribution of the response. We validate the effectiveness of our proposed method through comprehensive numerical simulations, clearly demonstrating its superior performance compared to traditional methods. The application of the proposed method is demonstrated on triaxial accelerometer data from the National Health and Nutrition Examination Survey 2011–2014 for modeling the association between cognitive scores across various domains and distributional representation of physical activity among the older adult population. Our results highlight the advantages of the proposed approach, emphasizing the significance of incorporating multidimensional distributional information in the triaxial accelerometer data.},
  archive      = {J_BIMJ},
  author       = {Rahul Ghosal and Marcos Matabuena},
  doi          = {10.1002/bimj.202400042},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202400042},
  shortjournal = {Bio. J.},
  title        = {Multivariate scalar on multidimensional distribution regression with application to modeling the association between physical activity and cognitive functions},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing for sufficient follow-up in censored survival data
by using extremes. <em>BIMJ</em>, <em>66</em>(7), e202400033. (<a
href="https://doi.org/10.1002/bimj.202400033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis, it often happens that some individuals, referred to as cured individuals, never experience the event of interest. When analyzing time-to-event data with a cure fraction, it is crucial to check the assumption of “sufficient follow-up,” which means that the right extreme of the censoring time distribution is larger than that of the survival time distribution for the noncured individuals. However, the available methods to test this assumption are limited in the literature. In this article, we study the problem of testing whether follow-up is sufficient for light-tailed distributions and develop a simple novel test. The proposed test statistic compares an estimator of the noncure proportion under sufficient follow-up to one without the assumption of sufficient follow-up. A bootstrap procedure is employed to approximate the critical values of the test. We also carry out extensive simulations to evaluate the finite sample performance of the test and illustrate the practical use with applications to leukemia and breast cancer data sets.},
  archive      = {J_BIMJ},
  author       = {Ping Xie and Mikael Escobar-Bach and Ingrid Van Keilegom},
  doi          = {10.1002/bimj.202400033},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202400033},
  shortjournal = {Bio. J.},
  title        = {Testing for sufficient follow-up in censored survival data by using extremes},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel method for nonparametric statistical inference for
niche overlap in multiple species. <em>BIMJ</em>, <em>66</em>(7),
e202400013. (<a href="https://doi.org/10.1002/bimj.202400013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The understanding of species interactions and ecosystem dynamics hinges upon the study of ecological niches. Quantifying the overlap of Hutchinsonian-niches has garnered significant attention, with many recent publications addressing the issue. Prior work on estimating niche overlap often did not provide confidence intervals or assumed multivariate normality, seriously limiting applications in ecology, and biodiversity research. This paper extends a nonparametric approach, previously applied to the two-species case, to multiple species. For estimation, a consistent plug-in estimator based on rank sums is proposed and its asymptotic distribution is derived under weak conditions. The novel methodology is then applied to a study comparing the ecological niches of the Eurasian eagle owl, common buzzard, and red kite. These species share a habitat in Central Europe but exhibit distinct population trends. The analysis explores their breeding habitat preferences, considering the intricate competition dynamics and utilizing the nonparametric approach to niche overlap estimation. Our proposed method provides a valuable inferential tool for the quantitative evaluation of differences and overlap between niches.},
  archive      = {J_BIMJ},
  author       = {Patrick B. Langthaler and Kai-Philipp Gladow and Oliver Krüger and Jonas Beck},
  doi          = {10.1002/bimj.202400013},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202400013},
  shortjournal = {Bio. J.},
  title        = {A novel method for nonparametric statistical inference for niche overlap in multiple species},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Firth-type penalized methods of the modified poisson and
least-squares regression analyses for binary outcomes. <em>BIMJ</em>,
<em>66</em>(7), e202400004. (<a
href="https://doi.org/10.1002/bimj.202400004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modified Poisson and least-squares regression analyses for binary outcomes have been widely used as effective multivariable analysis methods to provide risk ratio and risk difference estimates in clinical and epidemiological studies. However, there is no certain evidence that assessed their operating characteristics under small and sparse data settings and no effective methods have been proposed for these regression analyses to address this issue. In this article, we show that the modified Poisson regression provides seriously biased estimates under small and sparse data settings. In addition, the modified least-squares regression provides unbiased estimates under these settings. We further show that the ordinary robust variance estimators for both of the methods have certain biases under situations that involve small or moderate sample sizes. To address these issues, we propose the Firth-type penalized methods for the modified Poisson and least-squares regressions. The adjustment methods lead to a more accurate and stable risk ratio estimator under small and sparse data settings, although the risk difference estimator is not invariant. In addition, to improve the inferences of the effect measures, we provide an improved robust variance estimator for these regression analyses. We conducted extensive simulation studies to assess the performances of the proposed methods under real-world conditions and found that the accuracies of the point and interval estimations were markedly improved by the proposed methods. We illustrate the effectiveness of these methods by applying them to a clinical study of epilepsy.},
  archive      = {J_BIMJ},
  author       = {Satoshi Uno and Hisashi Noma and Masahiko Gosho},
  doi          = {10.1002/bimj.202400004},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202400004},
  shortjournal = {Bio. J.},
  title        = {Firth-type penalized methods of the modified poisson and least-squares regression analyses for binary outcomes},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stakeholders’ perspectives on current issues in data
monitoring committees. <em>BIMJ</em>, <em>66</em>(7), e202300384. (<a
href="https://doi.org/10.1002/bimj.202300384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Monitoring Committees (DMCs) are groups of experts that review accumulating data from one or more ongoing clinical studies and advise the Sponsor regarding the continuing safety of study subjects along with the continuing validity and scientific merit of the study. Although DMCs are widely used, considerable variability exists in their conduct. This paper offers recommendations, derived from sessions given at the 2023 Central European Network International Biometric and Statisticians in the Pharmaceutical Industry Conferences&#39; and the authors&#39; experiences. We focus on four topics that are part of the DMC process and where there is unclarity and inconsistency in current practices: (1) Communication with the DMC—We reflect on the importance of effective, proper communication channels between the DMC and relevant stakeholders to foster collaboration and exchange of critical information while retaining study integrity throughout. (2) Open sessions—We discuss the benefits of incorporating open sessions in DMC meetings to enhance transparency, inclusivity, and the consideration of diverse perspectives, as well as pitfalls of open sessions. (3) Access to efficacy data—We highlight the need for appropriate access to efficacy data by DMCs and discuss how to implement this in practice and how to address potential concerns regarding multiplicity. (4) Interactive data displays—We outline the utilization of interactive data displays to facilitate a more intuitive understanding of study results by the DMC. By addressing these topics, we aim to provide comprehensive practical recommendations that bridge the gap between current practices and optimal DMC functionality.},
  archive      = {J_BIMJ},
  author       = {Michael J. Cartwright and Tim Friede and David Lawrence and Emma May and Tobias Mütze and Kit Roes},
  doi          = {10.1002/bimj.202300384},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202300384},
  shortjournal = {Bio. J.},
  title        = {Stakeholders&#39; perspectives on current issues in data monitoring committees},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Post-estimation shrinkage in full and selected linear
regression models in low-dimensional data revisited. <em>BIMJ</em>,
<em>66</em>(7), e202300368. (<a
href="https://doi.org/10.1002/bimj.202300368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fit of a regression model to new data is often worse due to overfitting. Analysts use variable selection techniques to develop parsimonious regression models, which may introduce bias into regression estimates. Shrinkage methods have been proposed to mitigate overfitting and reduce bias in estimates. Post-estimation shrinkage is an alternative to penalized methods. This study evaluates effectiveness of post-estimation shrinkage in improving prediction performance of full and selected models. Through a simulation study, results were compared with ordinary least squares (OLS) and ridge in full models, and best subset selection (BSS) and lasso in selected models. We focused on prediction errors and the number of selected variables. Additionally, we proposed a modified version of the parameter-wise shrinkage (PWS) approach named non-negative PWS (NPWS) to address weaknesses of PWS. Results showed that no method was superior in all scenarios. In full models, NPWS outperformed global shrinkage, whereas PWS was inferior to OLS. In low correlation with moderate-to-high signal-to-noise ratio (SNR), NPWS outperformed ridge, but ridge performed best in small sample sizes, high correlation, and low SNR. In selected models, all post-estimation shrinkage performed similarly, with global shrinkage slightly inferior. Lasso outperformed BSS and post-estimation shrinkage in small sample sizes, low SNR, and high correlation but was inferior when the opposite was true. Our study suggests that, with sufficient information, NPWS is more effective than global shrinkage in improving prediction accuracy of models. However, in high correlation, small sample sizes, and low SNR, penalized methods generally outperform post-estimation shrinkage methods.},
  archive      = {J_BIMJ},
  author       = {Edwin Kipruto and Willi Sauerbrei},
  doi          = {10.1002/bimj.202300368},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202300368},
  shortjournal = {Bio. J.},
  title        = {Post-estimation shrinkage in full and selected linear regression models in low-dimensional data revisited},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional data analysis: An introduction and recent
developments. <em>BIMJ</em>, <em>66</em>(7), e202300363. (<a
href="https://doi.org/10.1002/bimj.202300363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis (FDA) is a statistical framework that allows for the analysis of curves, images, or functions on higher dimensional domains. The goals of FDA, such as descriptive analyses, classification, and regression, are generally the same as for statistical analyses of scalar-valued or multivariate data, but FDA brings additional challenges due to the high- and infinite dimensionality of observations and parameters, respectively. This paper provides an introduction to FDA, including a description of the most common statistical analysis techniques, their respective software implementations, and some recent developments in the field. The paper covers fundamental concepts such as descriptives and outliers, smoothing, amplitude and phase variation, and functional principal component analysis. It also discusses functional regression, statistical inference with functional data, functional classification and clustering, and machine learning approaches for functional data analysis. The methods discussed in this paper are widely applicable in fields such as medicine, biophysics, neuroscience, and chemistry and are increasingly relevant due to the widespread use of technologies that allow for the collection of functional data. Sparse functional data methods are also relevant for longitudinal data analysis. All presented methods are demonstrated using available software in R by analyzing a dataset on human motion and motor control. To facilitate the understanding of the methods, their implementation, and hands-on application, the code for these practical examples is made available through a code and data supplement and on GitHub .},
  archive      = {J_BIMJ},
  author       = {Jan Gertheiss and David Rügamer and Bernard X. W. Liew and Sonja Greven},
  doi          = {10.1002/bimj.202300363},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202300363},
  shortjournal = {Bio. J.},
  title        = {Functional data analysis: An introduction and recent developments},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A network-constrain weibull AFT model for biomarkers
discovery. <em>BIMJ</em>, <em>66</em>(7), e202300272. (<a
href="https://doi.org/10.1002/bimj.202300272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose AFTNet, a novel network-constraint survival analysis method based on the Weibull accelerated failure time (AFT) model solved by a penalized likelihood approach for variable selection and estimation. When using the log-linear representation, the inference problem becomes a structured sparse regression problem for which we explicitly incorporate the correlation patterns among predictors using a double penalty that promotes both sparsity and grouping effect. Moreover, we establish the theoretical consistency for the AFTNet estimator and present an efficient iterative computational algorithm based on the proximal gradient descent method. Finally, we evaluate AFTNet performance both on synthetic and real data examples.},
  archive      = {J_BIMJ},
  author       = {Claudia Angelini and Daniela De Canditiis and Italia De Feis and Antonella Iuliano},
  doi          = {10.1002/bimj.202300272},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202300272},
  shortjournal = {Bio. J.},
  title        = {A network-constrain weibull AFT model for biomarkers discovery},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confirmatory adaptive designs for clinical trials with
multiple time-to-event outcomes in multi-state markov models.
<em>BIMJ</em>, <em>66</em>(7), e202300181. (<a
href="https://doi.org/10.1002/bimj.202300181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of multiple time-to-event outcomes in a randomized controlled clinical trial can be accomplished with existing methods. However, depending on the characteristics of the disease under investigation and the circumstances in which the study is planned, it may be of interest to conduct interim analyses and adapt the study design if necessary. Due to the expected dependency of the endpoints, the full available information on the involved endpoints may not be used for this purpose. We suggest a solution to this problem by embedding the endpoints in a multistate model. If this model is Markovian, it is possible to take the disease history of the patients into account and allow for data-dependent design adaptations. To this end, we introduce a flexible test procedure for a variety of applications, but are particularly concerned with the simultaneous consideration of progression-free survival (PFS) and overall survival (OS). This setting is of key interest in oncological trials. We conduct simulation studies to determine the properties for small sample sizes and demonstrate an application based on data from the NB2004-HR study.},
  archive      = {J_BIMJ},
  author       = {Moritz Fabian Danzer and Andreas Faldum and Thorsten Simon and Barbara Hero and Rene Schmidt},
  doi          = {10.1002/bimj.202300181},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202300181},
  shortjournal = {Bio. J.},
  title        = {Confirmatory adaptive designs for clinical trials with multiple time-to-event outcomes in multi-state markov models},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-analysis of diagnostic accuracy studies with multiple
thresholds: Comparison of approaches in a simulation study.
<em>BIMJ</em>, <em>66</em>(7), e202300101. (<a
href="https://doi.org/10.1002/bimj.202300101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of methods for the meta-analysis of diagnostic test accuracy (DTA) studies is still an active area of research. While methods for the standard case where each study reports a single pair of sensitivity and specificity are nearly routinely applied nowadays, methods to meta-analyze receiver operating characteristic (ROC) curves are not widely used. This situation is more complex, as each primary DTA study may report on several pairs of sensitivity and specificity, each corresponding to a different threshold. In a case study published earlier, we applied a number of methods for meta-analyzing DTA studies with multiple thresholds to a real-world data example (Zapf et al., Biometrical Journal . 2021; 63(4): 699–711). To date, no simulation study exists that systematically compares different approaches with respect to their performance in various scenarios when the truth is known. In this article, we aim to fill this gap and present the results of a simulation study that compares three frequentist approaches for the meta-analysis of ROC curves. We performed a systematic simulation study, motivated by an example from medical research. In the simulations, all three approaches worked partially well. The approach by Hoyer and colleagues was slightly superior in most scenarios and is recommended in practice.},
  archive      = {J_BIMJ},
  author       = {Antonia Zapf and Cornelia Frömke and Juliane Hardt and Gerta Rücker and Dina Voeltz and Annika Hoyer},
  doi          = {10.1002/bimj.202300101},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202300101},
  shortjournal = {Bio. J.},
  title        = {Meta-analysis of diagnostic accuracy studies with multiple thresholds: Comparison of approaches in a simulation study},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A flexible adaptive lasso cox frailty model based on the
full likelihood. <em>BIMJ</em>, <em>66</em>(7), e202300020. (<a
href="https://doi.org/10.1002/bimj.202300020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a method to regularize Cox frailty models is proposed that accommodates time-varying covariates and time-varying coefficients and is based on the full likelihood instead of the partial likelihood. A particular advantage of this framework is that the baseline hazard can be explicitly modeled in a smooth, semiparametric way, for example, via P-splines. Regularization for variable selection is performed via a lasso penalty and via group lasso for categorical variables while a second penalty regularizes wiggliness of smooth estimates of time-varying coefficients and the baseline hazard. Additionally, adaptive weights are included to stabilize the estimation. The method is implemented in the R function coxlasso , which is now integrated into the package PenCoxFrail , and will be compared to other packages for regularized Cox regression.},
  archive      = {J_BIMJ},
  author       = {Maike Hohberg and Andreas Groll},
  doi          = {10.1002/bimj.202300020},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {e202300020},
  shortjournal = {Bio. J.},
  title        = {A flexible adaptive lasso cox frailty model based on the full likelihood},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Random survival forests with competing events: A
subdistribution-based imputation approach. <em>BIMJ</em>,
<em>66</em>(6), e202400014. (<a
href="https://doi.org/10.1002/bimj.202400014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random survival forests (RSF) can be applied to many time-to-event research questions and are particularly useful in situations where the relationship between the independent variables and the event of interest is rather complex. However, in many clinical settings, the occurrence of the event of interest is affected by competing events, which means that a patient can experience an outcome other than the event of interest. Neglecting the competing event (i.e., regarding competing events as censoring) will typically result in biased estimates of the cumulative incidence function (CIF). A popular approach for competing events is Fine and Gray&#39;s subdistribution hazard model, which directly estimates the CIF by fitting a single-event model defined on a subdistribution timescale. Here, we integrate concepts from the subdistribution hazard modeling approach into the RSF. We develop several imputation strategies that use weights as in a discrete-time subdistribution hazard model to impute censoring times in cases where a competing event is observed. Our simulations show that the CIF is well estimated if the imputation already takes place outside the forest on the overall dataset. Especially in settings with a low rate of the event of interest or a high censoring rate, competing events must not be neglected, that is, treated as censoring. When applied to a real-world epidemiological dataset on chronic kidney disease, the imputation approach resulted in highly plausible predictor–response relationships and CIF estimates of renal events.},
  archive      = {J_BIMJ},
  author       = {Charlotte Behning and Alexander Bigerl and Marvin N. Wright and Peggy Sekula and Moritz Berger and Matthias Schmid},
  doi          = {10.1002/bimj.202400014},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202400014},
  shortjournal = {Bio. J.},
  title        = {Random survival forests with competing events: A subdistribution-based imputation approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factor-analytic variance–covariance structures for
prediction into a target population of environments. <em>BIMJ</em>,
<em>66</em>(6), e202400008. (<a
href="https://doi.org/10.1002/bimj.202400008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finlay–Wilkinson regression is a popular method for modeling genotype–environment interaction in plant breeding and crop variety testing. When environment is a random factor, this model may be cast as a factor-analytic variance–covariance structure, implying a regression on random latent environmental variables. This paper reviews such models with a focus on their use in the analysis of multi-environment trials for the purpose of making predictions in a target population of environments. We investigate the implication of random versus fixed effects assumptions, starting from basic analysis-of-variance models, then moving on to factor-analytic models and considering the transition to models involving observable environmental covariates, which promise to provide more accurate and targeted predictions than models with latent environmental variables.},
  archive      = {J_BIMJ},
  author       = {Hans-Peter Piepho and Emlyn Williams},
  doi          = {10.1002/bimj.202400008},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202400008},
  shortjournal = {Bio. J.},
  title        = {Factor-analytic Variance–Covariance structures for prediction into a target population of environments},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating the heterogeneity of “study twins.”
<em>BIMJ</em>, <em>66</em>(6), e202300387. (<a
href="https://doi.org/10.1002/bimj.202300387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analyses are commonly performed based on random-effects models, while in certain cases one might also argue in favor of a common-effect model. One such case may be given by the example of two “study twins” that are performed according to a common (or at least very similar) protocol. Here we investigate the particular case of meta-analysis of a pair of studies, for example, summarizing the results of two confirmatory clinical trials in phase III of a clinical development program. Thereby, we focus on the question of to what extent homogeneity or heterogeneity may be discernible and include an empirical investigation of published (“twin”) pairs of studies. A pair of estimates from two studies only provide very little evidence of homogeneity or heterogeneity of effects, and ad hoc decision criteria may often be misleading.},
  archive      = {J_BIMJ},
  author       = {Christian Röver and Tim Friede},
  doi          = {10.1002/bimj.202300387},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202300387},
  shortjournal = {Bio. J.},
  title        = {Investigating the heterogeneity of “Study twins”},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of nonconcurrent controls in adaptive platform
trials: Separating randomized and nonrandomized information.
<em>BIMJ</em>, <em>66</em>(6), e202300334. (<a
href="https://doi.org/10.1002/bimj.202300334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive platform trials allow treatments to be added or dropped during the study, meaning that the control arm may be active for longer than the experimental arms. This leads to nonconcurrent controls, which provide nonrandomized information that may increase efficiency but may introduce bias from temporal confounding and other factors. Various methods have been proposed to control confounding from nonconcurrent controls, based on adjusting for time period. We demonstrate that time adjustment is insufficient to prevent bias in some circumstances where nonconcurrent controls are present in adaptive platform trials, and we propose a more general analytical framework that accounts for nonconcurrent controls in such circumstances. We begin by defining nonconcurrent controls using the concept of a concurrently randomized cohort, which is a subgroup of participants all subject to the same randomized design. We then use cohort adjustment rather than time adjustment. Due to flexibilities in platform trials, more than one randomized design may be in force at any time, meaning that cohort-adjusted and time-adjusted analyses may be quite different. Using simulation studies, we demonstrate that time-adjusted analyses may be biased while cohort-adjusted analyses remove this bias. We also demonstrate that the cohort-adjusted analysis may be interpreted as a synthesis of randomized and indirect comparisons analogous to mixed treatment comparisons in network meta-analysis. This allows the use of network meta-analysis methodology to separate the randomized and nonrandomized components and to assess their consistency. Whenever nonconcurrent controls are used in platform trials, the separate randomized and indirect contributions to the treatment effect should be presented.},
  archive      = {J_BIMJ},
  author       = {Ian C. Marschner and I. Manjula Schou},
  doi          = {10.1002/bimj.202300334},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202300334},
  shortjournal = {Bio. J.},
  title        = {Analysis of nonconcurrent controls in adaptive platform trials: Separating randomized and nonrandomized information},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample size calculation under nonproportional hazards using
average hazard ratios. <em>BIMJ</em>, <em>66</em>(6), e202300271. (<a
href="https://doi.org/10.1002/bimj.202300271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clinical trials assess time-to-event endpoints. To describe the difference between groups in terms of time to event, we often employ hazard ratios. However, the hazard ratio is only informative in the case of proportional hazards (PHs) over time. There exist many other effect measures that do not require PHs. One of them is the average hazard ratio (AHR). Its core idea is to utilize a time-dependent weighting function that accounts for time variation. Though propagated in methodological research papers, the AHR is rarely used in practice. To facilitate its application, we unfold approaches for sample size calculation of an AHR test. We assess the reliability of the sample size calculation by extensive simulation studies covering various survival and censoring distributions with proportional as well as nonproportional hazards (N-PHs). The findings suggest that a simulation-based sample size calculation approach can be useful for designing clinical trials with N-PHs. Using the AHR can result in increased statistical power to detect differences between groups with more efficient sample sizes.},
  archive      = {J_BIMJ},
  author       = {Ina Dormuth and Markus Pauly and Geraldine Rauch and Carolin Herrmann},
  doi          = {10.1002/bimj.202300271},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202300271},
  shortjournal = {Bio. J.},
  title        = {Sample size calculation under nonproportional hazards using average hazard ratios},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new mixture model with cure rate applied to breast cancer
data. <em>BIMJ</em>, <em>66</em>(6), e202300257. (<a
href="https://doi.org/10.1002/bimj.202300257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new modelling for long-term survival models, assuming that the number of competing causes follows a mixture of Poisson and the Birnbaum-Saunders distribution. In this context, we present some statistical properties of our model and demonstrate that the promotion time model emerges as a limiting case. We delve into detailed discussions of specific models within this class. Notably, we examine the expected number of competing causes, which depends on covariates. This allows for direct modeling of the cure rate as a function of covariates. We present an Expectation-Maximization (EM) algorithm for parameter estimation, to discuss the estimation via maximum likelihood (ML) and provide insights into parameter inference for this model. Additionally, we outline sufficient conditions for ensuring the consistency and asymptotic normal distribution of ML estimators. To evaluate the performance of our estimation method, we conduct a Monte Carlo simulation to provide asymptotic properties and a power study of LR test by contrasting our methodology against the promotion time model. To demonstrate the practical applicability of our model, we apply it to a real medical dataset from a population-based study of incidence of breast cancer in São Paulo, Brazil. Our results illustrate that the proposed model can outperform traditional approaches in terms of model fitting, highlighting its potential utility in real-world scenarios.},
  archive      = {J_BIMJ},
  author       = {Diego I. Gallardo and Márcia Brandão and Jeremias Leão and Marcelo Bourguignon and Vinicius Calsavara},
  doi          = {10.1002/bimj.202300257},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202300257},
  shortjournal = {Bio. J.},
  title        = {A new mixture model with cure rate applied to breast cancer data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive multiple comparisons with the best. <em>BIMJ</em>,
<em>66</em>(6), e202300242. (<a
href="https://doi.org/10.1002/bimj.202300242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subset selection methods aim to choose a nonempty subset of populations including a best population with some prespecified probability. An example application involves location parameters that quantify yields in agriculture to select the best wheat variety. This is quite different from variable selection problems, for instance, in regression. Unfortunately, subset selection methods can become very conservative when the parameter configuration is not least favorable. This will lead to a selection of many non-best populations, making the set of selected populations less informative. To solve this issue, we propose less conservative adaptive approaches based on estimating the number of best populations. We also discuss variants of our adaptive approaches that are applicable when the sample sizes and/or variances differ between populations. Using simulations, we show that our methods yield a desirable performance. As an illustration of potential gains, we apply them to two real datasets, one on the yield of wheat varieties and the other obtained via genome sequencing of repeated samples.},
  archive      = {J_BIMJ},
  author       = {Haoyu Chen and Werner Brannath and Andreas Futschik},
  doi          = {10.1002/bimj.202300242},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202300242},
  shortjournal = {Bio. J.},
  title        = {Adaptive multiple comparisons with the best},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). False discovery rate control for lesion-symptom mapping with
heterogeneous data via weighted p-values. <em>BIMJ</em>, <em>66</em>(6),
e202300198. (<a href="https://doi.org/10.1002/bimj.202300198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lesion-symptom mapping studies provide insight into what areas of the brain are involved in different aspects of cognition. This is commonly done via behavioral testing in patients with a naturally occurring brain injury or lesions (e.g., strokes or brain tumors). This results in high-dimensional observational data where lesion status (present/absent) is nonuniformly distributed, with some voxels having lesions in very few (or no) subjects. In this situation, mass univariate hypothesis tests have severe power heterogeneity where many tests are known a priori to have little to no power. Recent advancements in multiple testing methodologies allow researchers to weigh hypotheses according to side information (e.g., information on power heterogeneity). In this paper, we propose the use of p -value weighting for voxel-based lesion-symptom mapping studies. The weights are created using the distribution of lesion status and spatial information to estimate different non-null prior probabilities for each hypothesis test through some common approaches. We provide a monotone minimum weight criterion, which requires minimum a priori power information. Our methods are demonstrated on dependent simulated data and an aphasia study investigating which regions of the brain are associated with the severity of language impairment among stroke survivors. The results demonstrate that the proposed methods have robust error control and can increase power. Further, we showcase how weights can be used to identify regions that are inconclusive due to lack of power.},
  archive      = {J_BIMJ},
  author       = {Siyu Zheng and Alexander C. McLain and Joshua Habiger and Christopher Rorden and Julius Fridriksson},
  doi          = {10.1002/bimj.202300198},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202300198},
  shortjournal = {Bio. J.},
  title        = {False discovery rate control for lesion-symptom mapping with heterogeneous data via weighted p-values},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Health care provider clustering using fusion penalty in
quasi-likelihood. <em>BIMJ</em>, <em>66</em>(6), e202300185. (<a
href="https://doi.org/10.1002/bimj.202300185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been growing research interest in developing methodology to evaluate the health care providers&#39; performance with respect to a patient outcome. Random and fixed effects models are traditionally used for such a purpose. We propose a new method, using a fusion penalty to cluster health care providers based on quasi-likelihood. Without any priori knowledge of grouping information, our method provides a desirable data-driven approach for automatically clustering health care providers into different groups based on their performance. Further, the quasi-likelihood is more flexible and robust than the regular likelihood in that no distributional assumption is needed. An efficient alternating direction method of multipliers algorithm is developed to implement the proposed method. We show that the proposed method enjoys the oracle properties; namely, it performs as well as if the true group structure were known in advance. The consistency and asymptotic normality of the estimators are established. Simulation studies and analysis of the national kidney transplant registry data demonstrate the utility and validity of our method.},
  archive      = {J_BIMJ},
  author       = {Lili Liu and Kevin He and Di Wang and Shujie Ma and Annie Qu and Yihui Luan and J. Philip Miller and Yizhe Song and Lei Liu},
  doi          = {10.1002/bimj.202300185},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202300185},
  shortjournal = {Bio. J.},
  title        = {Health care provider clustering using fusion penalty in quasi-likelihood},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MTML: An efficient multitrait multilocus GWAS method based
on the cauchy combination test. <em>BIMJ</em>, <em>66</em>(6),
e202300130. (<a href="https://doi.org/10.1002/bimj.202300130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genome-wide association study (GWAS) by measuring the joint effect of multiple loci on multiple traits, has recently attracted interest, due to the decreased costs of high-throughput genotyping and phenotyping technologies. Previous studies mainly focused on either multilocus models that identify associations with a single trait or multitrait models that scan a single marker at a time. Since these types of models cannot fully utilize the association information, the powers of the tests are usually low. To potentially address this problem, we present here a multitrait multilocus (MTML) modeling framework that implements in three steps: (1) simplify the complex calculation; (2) reduce the model dimension; (3) integrate the joint contribution of single markers to multiple traits by Cauchy combination. The performances of MTML are evaluated and compared with other three published methods by Monte Carlo simulations. Simulation results show that MTML is more powerful for quantitative trait nucleotide detection and robust for various numbers of traits. In the meanwhile, MTML can effectively control type I error rate at a reasonable level. Real data analysis of Arabidopsis thaliana shows that MTML identifies more pleiotropic genetic associations. Therefore, we conclude that MTML is an efficient GWAS method for joint analysis of multiple quantitative traits. The R package MTML, which facilitates the implementation of the proposed method, is publicly available on GitHub https://github.com/Guohongping/MTML .},
  archive      = {J_BIMJ},
  author       = {Hongping Guo and Tong Li and Yao Shi and Xiao Wang},
  doi          = {10.1002/bimj.202300130},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202300130},
  shortjournal = {Bio. J.},
  title        = {MTML: An efficient multitrait multilocus GWAS method based on the cauchy combination test},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric additive modeling of the restricted mean
survival time. <em>BIMJ</em>, <em>66</em>(6), e202200371. (<a
href="https://doi.org/10.1002/bimj.202200371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of the restricted mean survival time (RMST) has become increasingly common in biomedical studies during the last decade as a means of estimating treatment or covariate effects on survival. Advantages of RMST over the hazard ratio (HR) include increased interpretability and lack of reliance on the often tenuous proportional hazards assumption. Some authors have argued that RMST regression should generally be the frontline analysis as opposed to methods based on counting process increments. However, in order for the use of the RMST to be more mainstream, it is necessary to broaden the range of data structures to which pertinent methods can be applied. In this report, we address this issue from two angles. First, most of existing methodological development for directly modeling RMST has focused on multiplicative models. An additive model may be preferred due to goodness of fit and/or parameter interpretation. Second, many settings encountered nowadays feature high-dimensional categorical (nuisance) covariates, for which parameter estimation is best avoided. Motivated by these considerations, we propose stratified additive models for direct RMST analysis. The proposed methods feature additive covariate effects. Moreover, nuisance factors can be factored out of the estimation, akin to stratification in Cox regression, such that focus can be appropriately awarded to the parameters of chief interest. Large-sample properties of the proposed estimators are derived, and a simulation study is performed to assess finite-sample performance. In addition, we provide techniques for evaluating a fitted model with respect to risk discrimination and predictive accuracy. The proposed methods are then applied to liver transplant data to estimate the effects of donor characteristics on posttransplant survival time.},
  archive      = {J_BIMJ},
  author       = {Yuan Zhang and Douglas E. Schaubel},
  doi          = {10.1002/bimj.202200371},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {6},
  pages        = {e202200371},
  shortjournal = {Bio. J.},
  title        = {Semiparametric additive modeling of the restricted mean survival time},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust regression techniques for multiple method comparison
and transformation. <em>BIMJ</em>, <em>66</em>(5), e202400027. (<a
href="https://doi.org/10.1002/bimj.202400027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A generalization of Passing–Bablok regression is proposed for comparing multiple measurement methods simultaneously. Possible applications include assay migration studies or interlaboratory trials. When comparing only two methods, the method boils down to the usual Passing–Bablok estimator. It is close in spirit to reduced major axis regression, which is, however, not robust. To obtain a robust estimator, the major axis is replaced by the (hyper-)spherical median axis. This technique has been applied to compare SARS-CoV-2 serological tests, bilirubin in neonates, and an in vitro diagnostic test using different instruments, sample preparations, and reagent lots. In addition, plots similar to the well-known Bland–Altman plots have been developed to represent the variance structure.},
  archive      = {J_BIMJ},
  author       = {Florian Dufey},
  doi          = {10.1002/bimj.202400027},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202400027},
  shortjournal = {Bio. J.},
  title        = {Robust regression techniques for multiple method comparison and transformation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Years of life lost to COVID-19 and related mortality
indicators: An illustration in 30 countries. <em>BIMJ</em>,
<em>66</em>(5), e202300386. (<a
href="https://doi.org/10.1002/bimj.202300386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of (potential) years of life lost is a measure of premature mortality that can be used to compare the impacts of different specific causes of death. However, interpreting a given number of years of life lost at face value is more problematic because of the lack of a sensible reference value. In this paper, we propose three denominators to divide an excess years of life lost, thus obtaining three indicators, called average life lost , increase of life lost , and proportion of life lost , which should facilitate interpretation and comparisons. We study the links between these three indicators and classical mortality indicators, such as life expectancy and standardized mortality rate, introduce the concept of weighted standardized mortality rate , and calculate them in 30 countries to assess the impact of COVID-19 on mortality in the year 2020. Using any of the three indicators, a significant excess loss is found for both genders in 18 of the 30 countries.},
  archive      = {J_BIMJ},
  author       = {Valentin Rousson and Isabella Locatelli},
  doi          = {10.1002/bimj.202300386},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202300386},
  shortjournal = {Bio. J.},
  title        = {Years of life lost to COVID-19 and related mortality indicators: An illustration in 30 countries},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biostatistical aspects of whole genome sequencing studies:
Preprocessing and quality control. <em>BIMJ</em>, <em>66</em>(5),
e202300278. (<a href="https://doi.org/10.1002/bimj.202300278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advances in high-throughput DNA sequencing technologies have enabled large-scale whole genome sequencing (WGS) studies. Before performing association analysis between phenotypes and genotypes, preprocessing and quality control (QC) of the raw sequence data need to be performed. Because many biostatisticians have not been working with WGS data so far, we first sketch Illumina&#39;s short-read sequencing technology. Second, we explain the general preprocessing pipeline for WGS studies. Third, we provide an overview of important QC metrics, which are applied to WGS data: on the raw data, after mapping and alignment, after variant calling, and after multisample variant calling. Fourth, we illustrate the QC with the data from the GENEtic SequencIng Study Hamburg–Davos (GENESIS-HD), a study involving more than 9000 human whole genomes. All samples were sequenced on an Illumina NovaSeq 6000 with an average coverage of 35× using a PCR-free protocol. For QC, one genome in a bottle (GIAB) trio was sequenced in four replicates, and one GIAB sample was successfully sequenced 70 times in different runs. Fifth, we provide empirical data on the compression of raw data using the DRAGEN original read archive (ORA). The most important quality metrics in the application were genetic similarity, sample cross-contamination, deviations from the expected Het/Hom ratio, relatedness, and coverage. The compression ratio of the raw files using DRAGEN ORA was 5.6:1, and compression time was linear by genome coverage. In summary, the preprocessing, joint calling, and QC of large WGS studies are feasible within a reasonable time, and efficient QC procedures are readily available.},
  archive      = {J_BIMJ},
  author       = {Raphael O. Betschart and Cristian Riccio and Domingo Aguilera-Garcia and Stefan Blankenberg and Linlin Guo and Holger Moch and Dagmar Seidl and Hugo Solleder and Felix Thalén and Alexandre Thiéry and Raphael Twerenbold and Tanja Zeller and Martin Zoche and Andreas Ziegler},
  doi          = {10.1002/bimj.202300278},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202300278},
  shortjournal = {Bio. J.},
  title        = {Biostatistical aspects of whole genome sequencing studies: Preprocessing and quality control},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Penalized regression methods with modified cross-validation
and bootstrap tuning produce better prediction models. <em>BIMJ</em>,
<em>66</em>(5), e202300245. (<a
href="https://doi.org/10.1002/bimj.202300245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk prediction models fitted using maximum likelihood estimation (MLE) are often overfitted resulting in predictions that are too extreme and a calibration slope (CS) less than 1. Penalized methods, such as Ridge and Lasso, have been suggested as a solution to this problem as they tend to shrink regression coefficients toward zero, resulting in predictions closer to the average. The amount of shrinkage is regulated by a tuning parameter, commonly selected via cross-validation (“standard tuning”). Though penalized methods have been found to improve calibration on average, they often over-shrink and exhibit large variability in the selected and hence the CS. This is a problem, particularly for small sample sizes, but also when using sample sizes recommended to control overfitting. We consider whether these problems are partly due to selecting using cross-validation with “training” datasets of reduced size compared to the original development sample, resulting in an over-estimation of and, hence, excessive shrinkage. We propose a modified cross-validation tuning method (“modified tuning”), which estimates from a pseudo-development dataset obtained via bootstrapping from the original dataset, albeit of larger size, such that the resulting cross-validation training datasets are of the same size as the original dataset. Modified tuning can be easily implemented in standard software and is closely related to bootstrap selection of the tuning parameter (“bootstrap tuning”). We evaluated modified and bootstrap tuning for Ridge and Lasso in simulated and real data using recommended sample sizes, and sizes slightly lower and higher. They substantially improved the selection of , resulting in improved CS compared to the standard tuning method. They also improved predictions compared to MLE.},
  archive      = {J_BIMJ},
  author       = {Menelaos Pavlou and Rumana Z. Omar and Gareth Ambler},
  doi          = {10.1002/bimj.202300245},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202300245},
  shortjournal = {Bio. J.},
  title        = {Penalized regression methods with modified cross-validation and bootstrap tuning produce better prediction models},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A shared-frailty spatial scan statistic model for
time-to-event data. <em>BIMJ</em>, <em>66</em>(5), e202300200. (<a
href="https://doi.org/10.1002/bimj.202300200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial scan statistics are well-known methods widely used to detect spatial clusters of events. Furthermore, several spatial scan statistics models have been applied to the spatial analysis of time-to-event data. However, these models do not take account of potential correlations between the observations of individuals within the same spatial unit or potential spatial dependence between spatial units. To overcome this problem, we have developed a scan statistic based on a Cox model with shared frailty and that takes account of the spatial dependence between spatial units. In simulation studies, we found that (i) conventional models of spatial scan statistics for time-to-event data fail to maintain the type I error in the presence of a correlation between the observations of individuals within the same spatial unit and (ii) our model performed well in the presence of such correlation and spatial dependence. We have applied our method to epidemiological data and the detection of spatial clusters of mortality in patients with end-stage renal disease in northern France.},
  archive      = {J_BIMJ},
  author       = {Camille Frévent and Mohamed-Salem Ahmed and Sophie Dabo-Niang and Michaël Genin},
  doi          = {10.1002/bimj.202300200},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202300200},
  shortjournal = {Bio. J.},
  title        = {A shared-frailty spatial scan statistic model for time-to-event data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous inference of multiple binary endpoints in
biomedical research: Small sample properties of multiple marginal models
and a resampling approach. <em>BIMJ</em>, <em>66</em>(5), e202300197.
(<a href="https://doi.org/10.1002/bimj.202300197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical research, the simultaneous inference of multiple binary endpoints may be of interest. In such cases, an appropriate multiplicity adjustment is required that controls the family-wise error rate, which represents the probability of making incorrect test decisions. In this paper, we investigate two approaches that perform single-step p $p$ -value adjustments that also take into account the possible correlation between endpoints. A rather novel and flexible approach known as multiple marginal models is considered, which is based on stacking of the parameter estimates of the marginal models and deriving their joint asymptotic distribution. We also investigate a nonparametric vector-based resampling approach, and we compare both approaches with the Bonferroni method by examining the family-wise error rate and power for different parameter settings, including low proportions and small sample sizes. The results show that the resampling-based approach consistently outperforms the other methods in terms of power, while still controlling the family-wise error rate. The multiple marginal models approach, on the other hand, shows a more conservative behavior. However, it offers more versatility in application, allowing for more complex models or straightforward computation of simultaneous confidence intervals. The practical application of the methods is demonstrated using a toxicological dataset from the National Toxicology Program.},
  archive      = {J_BIMJ},
  author       = {Sören Budig and Klaus Jung and Mario Hasler and Frank Schaarschmidt},
  doi          = {10.1002/bimj.202300197},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202300197},
  shortjournal = {Bio. J.},
  title        = {Simultaneous inference of multiple binary endpoints in biomedical research: Small sample properties of multiple marginal models and a resampling approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A marginalized zero-inflated negative binomial model for
spatial data: Modeling COVID-19 deaths in georgia. <em>BIMJ</em>,
<em>66</em>(5), e202300182. (<a
href="https://doi.org/10.1002/bimj.202300182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial count data with an abundance of zeros arise commonly in disease mapping studies. Typically, these data are analyzed using zero-inflated models, which comprise a mixture of a point mass at zero and an ordinary count distribution, such as the Poisson or negative binomial. However, due to their mixture representation, conventional zero-inflated models are challenging to explain in practice because the parameter estimates have conditional latent-class interpretations. As an alternative, several authors have proposed marginalized zero-inflated models that simultaneously model the excess zeros and the marginal mean, leading to a parameterization that more closely aligns with ordinary count models. Motivated by a study examining predictors of COVID-19 death rates, we develop a spatiotemporal marginalized zero-inflated negative binomial model that directly models the marginal mean, thus extending marginalized zero-inflated models to the spatial setting. To capture the spatiotemporal heterogeneity in the data, we introduce region-level covariates, smooth temporal effects, and spatially correlated random effects to model both the excess zeros and the marginal mean. For estimation, we adopt a Bayesian approach that combines full-conditional Gibbs sampling and Metropolis–Hastings steps. We investigate features of the model and use the model to identify key predictors of COVID-19 deaths in the US state of Georgia during the 2021 calendar year.},
  archive      = {J_BIMJ},
  author       = {Fedelis Mutiso and John L. Pearce and Sara E. Benjamin-Neelon and Noel T. Mueller and Hong Li and Brian Neelon},
  doi          = {10.1002/bimj.202300182},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202300182},
  shortjournal = {Bio. J.},
  title        = {A marginalized zero-inflated negative binomial model for spatial data: Modeling COVID-19 deaths in georgia},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample size calculation for an individual stepped-wedge
randomized trial. <em>BIMJ</em>, <em>66</em>(5), e202300167. (<a
href="https://doi.org/10.1002/bimj.202300167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the individual stepped-wedge randomized trial (ISW-RT), subjects are allocated to sequences, each sequence being defined by a control period followed by an experimental period. The total follow-up time is the same for all sequences, but the duration of the control and experimental periods varies among sequences. To our knowledge, there is no validated sample size calculation formula for ISW-RTs unlike stepped-wedge cluster randomized trials (SW-CRTs). The objective of this study was to adapt the formula used for SW-CRTs to the case of individual randomization and to validate this adaptation using a Monte Carlo simulation study. The proposed sample size calculation formula for an ISW-RT design yielded satisfactory empirical power for most scenarios except scenarios with operating characteristic values near the boundary (i.e., smallest possible number of periods, very high or very low autocorrelation coefficient). Overall, the results provide useful insights into the sample size calculation for ISW-RTs.},
  archive      = {J_BIMJ},
  author       = {Aude Allemang-Trivalle and Annabel Maruani and Bruno Giraudeau},
  doi          = {10.1002/bimj.202300167},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202300167},
  shortjournal = {Bio. J.},
  title        = {Sample size calculation for an individual stepped-wedge randomized trial},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional multivariable logistic regression with an
application to HIV viral suppression prediction. <em>BIMJ</em>,
<em>66</em>(5), e202300081. (<a
href="https://doi.org/10.1002/bimj.202300081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by improving the prediction of the human immunodeficiency virus (HIV) suppression status using electronic health records (EHR) data, we propose a functional multivariable logistic regression model, which accounts for the longitudinal binary process and continuous process simultaneously. Specifically, the longitudinal measurements for either binary or continuous variables are modeled by functional principal components analysis, and their corresponding functional principal component scores are used to build a logistic regression model for prediction. The longitudinal binary data are linked to underlying Gaussian processes. The estimation is done using penalized spline for the longitudinal continuous and binary data. Group-lasso is used to select longitudinal processes, and the multivariate functional principal components analysis is proposed to revise functional principal component scores with the correlation. The method is evaluated via comprehensive simulation studies and then applied to predict viral suppression using EHR data for people living with HIV in South Carolina.},
  archive      = {J_BIMJ},
  author       = {Siyuan Guo and Jiajia Zhang and Yichao Wu and Alexander C. McLain and James W. Hardin and Bankole Olatosi and Xiaoming Li},
  doi          = {10.1002/bimj.202300081},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202300081},
  shortjournal = {Bio. J.},
  title        = {Functional multivariable logistic regression with an application to HIV viral suppression prediction},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining partial true discovery guarantee procedures.
<em>BIMJ</em>, <em>66</em>(5), e202300075. (<a
href="https://doi.org/10.1002/bimj.202300075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Closed testing has recently been shown to be optimal for simultaneous true discovery proportion control. It is, however, challenging to construct true discovery guarantee procedures in such a way that it focuses power on some feature sets chosen by users based on their specific interest or expertise. We propose a procedure that allows users to target power on prespecified feature sets, that is, “focus sets.” Still, the method also allows inference for feature sets chosen post hoc, that is, “nonfocus sets,” for which we deduce a true discovery lower confidence bound by interpolation. Our procedure is built from partial true discovery guarantee procedures combined with Holm&#39;s procedure and is a conservative shortcut to the closed testing procedure. A simulation study confirms that the statistical power of our method is relatively high for focus sets, at the cost of power for nonfocus sets, as desired. In addition, we investigate its power property for sets with specific structures, for example, trees and directed acyclic graphs. We also compare our method with AdaFilter in the context of replicability analysis. The application of our method is illustrated with a gene ontology analysis in gene expression data.},
  archive      = {J_BIMJ},
  author       = {Ningning Xu and Aldo Solari and Jelle J. Goeman},
  doi          = {10.1002/bimj.202300075},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {5},
  pages        = {e202300075},
  shortjournal = {Bio. J.},
  title        = {Combining partial true discovery guarantee procedures},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparative review of novel model-assisted designs for phase
i/II clinical trials. <em>BIMJ</em>, <em>66</em>(4), 2300398. (<a
href="https://doi.org/10.1002/bimj.202300398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, both model-based and model-assisted designs have emerged to efficiently determine the optimal biological dose (OBD) in phase I/II trials for immunotherapy and targeted cellular agents. Model-based designs necessitate repeated model fitting and computationally intensive posterior sampling for each dose-assignment decision, limiting their practical application in real trials. On the other hand, model-assisted designs employ simple statistical models and facilitate the precalculation of a decision table for use throughout the trial, eliminating the need for repeated model fitting. Due to their simplicity and transparency, model-assisted designs are often preferred in phase I/II trials. In this paper, we systematically evaluate and compare the operating characteristics of several recent model-assisted phase I/II designs, including TEPI, PRINTE, Joint i3+3, BOIN-ET, STEIN, uTPI, and BOIN12, in addition to the well-known model-based EffTox design, using comprehensive numerical simulations. To ensure an unbiased comparison, we generated 10,000 dosing scenarios using a random scenario generation algorithm for each predetermined OBD location. We thoroughly assess various performance metrics, such as the selection percentages, average patient allocation to OBD, and overdose percentages across the eight designs. Based on these assessments, we offer design recommendations tailored to different objectives, sample sizes, and starting dose locations.},
  archive      = {J_BIMJ},
  author       = {Haolun Shi and Ruitao Lin and Xiaolei Lin},
  doi          = {10.1002/bimj.202300398},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2300398},
  shortjournal = {Bio. J.},
  title        = {Comparative review of novel model-assisted designs for phase I/II clinical trials},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling tropical tuna shifts: An inflated power logit
regression approach. <em>BIMJ</em>, <em>66</em>(4), 2300288. (<a
href="https://doi.org/10.1002/bimj.202300288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of zero-or-one inflated power logit (IPL) regression models, which serve as a versatile tool for analyzing bounded continuous data with observations at a boundary. These models are applied to explore the effects of climate changes on the distribution of tropical tuna within the North Atlantic Ocean. Our findings suggest that our modeling approach is adequate and capable of handling the outliers in the data. It exhibited superior performance compared to rival models in both diagnostic analysis and regarding the inference robustness. We offer a user-friendly method for fitting IPL regression models in practical applications.},
  archive      = {J_BIMJ},
  author       = {Francisco F. Queiroz and Silvia L. P. Ferrari},
  doi          = {10.1002/bimj.202300288},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2300288},
  shortjournal = {Bio. J.},
  title        = {Modeling tropical tuna shifts: An inflated power logit regression approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian hierarchical hidden markov model for clustering
and gene selection: Application to kidney cancer gene expression data.
<em>BIMJ</em>, <em>66</em>(4), 2300173. (<a
href="https://doi.org/10.1002/bimj.202300173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a Bayesian approach for biclustering that accounts for the prior functional dependence between genes using hidden Markov models (HMMs). We utilize biological knowledge gathered from gene ontologies and the hidden Markov structure to capture the potential coexpression of neighboring genes. Our interpretable model-based clustering characterized each cluster of samples by three groups of features: overexpressed, underexpressed, and irrelevant features. The proposed methods have been implemented in an R package and are used to analyze both the simulated data and The Cancer Genome Atlas kidney cancer data.},
  archive      = {J_BIMJ},
  author       = {Thierry Chekouo and Himadri Mukherjee},
  doi          = {10.1002/bimj.202300173},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2300173},
  shortjournal = {Bio. J.},
  title        = {A bayesian hierarchical hidden markov model for clustering and gene selection: Application to kidney cancer gene expression data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting class switch recombination in b-cells from
antibody repertoire data. <em>BIMJ</em>, <em>66</em>(4), 2300171. (<a
href="https://doi.org/10.1002/bimj.202300171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical and machine learning methods have proved useful in many areas of immunology. In this paper, we address for the first time the problem of predicting the occurrence of class switch recombination (CSR) in B-cells, a problem of interest in understanding antibody response under immunological challenges. We propose a framework to analyze antibody repertoire data, based on clonal (CG) group representation in a way that allows us to predict CSR events using CG level features as input. We assess and compare the performance of several predicting models (logistic regression, LASSO logistic regression, random forest, and support vector machine) in carrying out this task. The proposed approach can obtain an unweighted average recall of with models based on variable region descriptors and measures of CG diversity during an immune challenge and, most notably, before an immune challenge.},
  archive      = {J_BIMJ},
  author       = {Lutecia Servius and Davide Pigoli and Joseph Ng and Franca Fraternali},
  doi          = {10.1002/bimj.202300171},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2300171},
  shortjournal = {Bio. J.},
  title        = {Predicting class switch recombination in B-cells from antibody repertoire data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference in the absence of positivity: The role of
overlap weights. <em>BIMJ</em>, <em>66</em>(4), 2300156. (<a
href="https://doi.org/10.1002/bimj.202300156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to analyze data when there is violation of the positivity assumption? Several possible solutions exist in the literature. In this paper, we consider propensity score (PS) methods that are commonly used in observational studies to assess causal treatment effects in the context where the positivity assumption is violated. We focus on and examine four specific alternative solutions to the inverse probability weighting (IPW) trimming and truncation: matching weight (MW), Shannon&#39;s entropy weight (EW), overlap weight (OW), and beta weight (BW) estimators. We first specify their target population, the population of patients for whom clinical equipoise, that is, where we have sufficient PS overlap. Then, we establish the nexus among the different corresponding weights (and estimators); this allows us to highlight the shared properties and theoretical implications of these estimators. Finally, we introduce their augmented estimators that take advantage of estimating both the propensity score and outcome regression models to enhance the treatment effect estimators in terms of bias and efficiency. We also elucidate the role of the OW estimator as the flagship of all these methods that target the overlap population. Our analytic results demonstrate that OW, MW, and EW are preferable to IPW and some cases of BW when there is a moderate or extreme (stochastic or structural) violation of the positivity assumption. We then evaluate, compare, and confirm the finite-sample performance of the aforementioned estimators via Monte Carlo simulations. Finally, we illustrate these methods using two real-world data examples marked by violations of the positivity assumption.},
  archive      = {J_BIMJ},
  author       = {Roland A. Matsouaka and Yunji Zhou},
  doi          = {10.1002/bimj.202300156},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2300156},
  shortjournal = {Bio. J.},
  title        = {Causal inference in the absence of positivity: The role of overlap weights},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonparametric proportional risk model to assess a
treatment effect in time-to-event data. <em>BIMJ</em>, <em>66</em>(4),
2300147. (<a href="https://doi.org/10.1002/bimj.202300147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-to-event analysis often relies on prior parametric assumptions, or, if a semiparametric approach is chosen, Cox&#39;s model. This is inherently tied to the assumption of proportional hazards, with the analysis potentially invalidated if this assumption is not fulfilled. In addition, most interpretations focus on the hazard ratio, that is often misinterpreted as the relative risk (RR), the ratio of the cumulative distribution functions. In this paper, we introduce an alternative to current methodology for assessing a treatment effect in a two-group situation, not relying on the proportional hazards assumption but assuming proportional risks. Precisely, we propose a new nonparametric model to directly estimate the RR of two groups to experience an event under the assumption that the risk ratio is constant over time. In addition to this relative measure, our model allows for calculating the number needed to treat as an absolute measure, providing the possibility of an easy and holistic interpretation of the data. We demonstrate the validity of the approach by means of a simulation study and present an application to data from a large randomized controlled trial investigating the effect of dapagliflozin on all-cause mortality.},
  archive      = {J_BIMJ},
  author       = {Lucia Ameis and Oliver Kuss and Annika Hoyer and Kathrin Möllenhoff},
  doi          = {10.1002/bimj.202300147},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2300147},
  shortjournal = {Bio. J.},
  title        = {A nonparametric proportional risk model to assess a treatment effect in time-to-event data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Valid instrumental variable selection method using negative
control outcomes and constructing efficient estimator. <em>BIMJ</em>,
<em>66</em>(4), 2300113. (<a
href="https://doi.org/10.1002/bimj.202300113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational studies, instrumental variable (IV) methods are commonly applied when there are unmeasured covariates. In Mendelian randomization, constructing an allele score using many single nucleotide polymorphisms is often implemented; however, estimating biased causal effects by including some invalid IVs poses some risks. Invalid IVs are those IV candidates that are associated with unobserved variables. To solve this problem, we developed a novel strategy using negative control outcomes (NCOs) as auxiliary variables. Using NCOs, we are able to select only valid IVs and exclude invalid IVs without knowing which of the instruments are invalid. We also developed a new two-step estimation procedure and proved the semiparametric efficiency of our estimator. The performance of our proposed method was superior to some previous methods through simulations. Subsequently, we applied the proposed method to the UK Biobank dataset. Our results demonstrate that the use of an auxiliary variable, such as an NCO, enables the selection of valid IVs with assumptions different from those used in previous methods.},
  archive      = {J_BIMJ},
  author       = {Shunichiro Orihara and Atsushi Goto and Masataka Taguri},
  doi          = {10.1002/bimj.202300113},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2300113},
  shortjournal = {Bio. J.},
  title        = {Valid instrumental variable selection method using negative control outcomes and constructing efficient estimator},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive predictor-set linear model: An imputation-free
method for linear regression prediction on data sets with missing
values. <em>BIMJ</em>, <em>66</em>(4), 2300090. (<a
href="https://doi.org/10.1002/bimj.202300090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear regression (LR) is vastly used in data analysis for continuous outcomes in biomedicine and epidemiology. Despite its popularity, LR is incompatible with missing data, which frequently occur in health sciences. For parameter estimation, this shortcoming is usually resolved by complete-case analysis or imputation. Both work-arounds, however, are inadequate for prediction, since they either fail to predict on incomplete records or ignore missingness-induced reduction in prediction accuracy and rely on (unrealistic) assumptions about the missing mechanism. Here, we derive adaptive predictor-set linear model (aps-lm), capable of making predictions for incomplete data without the need for imputation. It is derived by using a predictor-selection operation, the Moore–Penrose pseudoinverse, and the reduced QR decomposition. aps-lm is an LR generalization that inherently handles missing values. It is applied on a reference data set, where complete predictors and outcome are available, and yields a set of privacy-preserving parameters. In a second stage, these are shared for making predictions of the outcome on external data sets with missing entries for predictors without imputation. Moreover, aps-lm computes prediction errors that account for the pattern of missing values even under extreme missingness. We benchmark aps-lm in a simulation study. aps-lm showed greater prediction accuracy and reduced bias compared to popular imputation strategies under a wide range of scenarios including variation of sample size, goodness of fit, missing value type, and covariance structure. Finally, as a proof-of-principle, we apply aps-lm in the context of epigenetic aging clocks, linear models that predict a person&#39;s biological age from epigenetic data with promising clinical applications.},
  archive      = {J_BIMJ},
  author       = {Benjamin Planterose Jiménez and Manfred Kayser and Athina Vidaki and Amke Caliebe},
  doi          = {10.1002/bimj.202300090},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2300090},
  shortjournal = {Bio. J.},
  title        = {Adaptive predictor-set linear model: An imputation-free method for linear regression prediction on data sets with missing values},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A method for determining groups in cumulative incidence
curves in competing risk data. <em>BIMJ</em>, <em>66</em>(4), 2300084.
(<a href="https://doi.org/10.1002/bimj.202300084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cumulative incidence function is the standard method for estimating the marginal probability of a given event in the presence of competing risks. One basic but important goal in the analysis of competing risk data is the comparison of these curves, for which limited literature exists. We proposed a new procedure that lets us not only test the equality of these curves but also group them if they are not equal. The proposed method allows determining the composition of the groups as well as an automatic selection of their number. Simulation studies show the good numerical behavior of the proposed methods for finite sample size. The applicability of the proposed method is illustrated using real data.},
  archive      = {J_BIMJ},
  author       = {Marta Sestelo and Luís Meira-Machado and Nora M. Villanueva and Javier Roca-Pardiñas},
  doi          = {10.1002/bimj.202300084},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2300084},
  shortjournal = {Bio. J.},
  title        = {A method for determining groups in cumulative incidence curves in competing risk data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Sparse group penalties for bi-level variable selection.
<em>BIMJ</em>, <em>66</em>(4), 2200334. (<a
href="https://doi.org/10.1002/bimj.202200334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data sets exhibit a natural group structure due to contextual similarities or high correlations of variables, such as lipid markers that are interrelated based on biochemical principles. Knowledge of such groupings can be used through bi-level selection methods to identify relevant feature groups and highlight their predictive members. One of the best known approaches of this kind combines the classical Least Absolute Shrinkage and Selection Operator (LASSO) with the Group LASSO , resulting in the Sparse Group LASSO . We propose the Sparse Group Penalty (SGP) framework, which allows for a flexible combination of different SGL-style shrinkage conditions. Analogous to SGL, we investigated the combination of the Smoothly Clipped Absolute Deviation (SCAD), the Minimax Concave Penalty (MCP) and the Exponential Penalty (EP) with their group versions, resulting in the Sparse Group SCAD , the Sparse Group MCP , and the novel Sparse Group EP (SGE). Those shrinkage operators provide refined control of the effect of group formation on the selection process through a tuning parameter. In simulation studies, SGPs were compared with other bi-level selection methods (Group Bridge, composite MCP, and Group Exponential LASSO) for variable and group selection evaluated with the Matthews correlation coefficient. We demonstrated the advantages of the new SGE in identifying parsimonious models, but also identified scenarios that highlight the limitations of the approach. The performance of the techniques was further investigated in a real-world use case for the selection of regulated lipids in a randomized clinical trial.},
  archive      = {J_BIMJ},
  author       = {Gregor Buch and Andreas Schulz and Irene Schmidtmann and Konstantin Strauch and Philipp S. Wild},
  doi          = {10.1002/bimj.202200334},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {4},
  pages        = {2200334},
  shortjournal = {Bio. J.},
  title        = {Sparse group penalties for bi-level variable selection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian model-based reduced major axis regression.
<em>BIMJ</em>, <em>66</em>(3), 2300279. (<a
href="https://doi.org/10.1002/bimj.202300279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reduced major axis (RMA) regression, widely used in the fields of zoology, botany, ecology, biology, spectroscopy, and among others, outweighs the ordinary least square regression by relaxing the assumption that the covariates are without measurement errors. A Bayesian implementation of the RMA regression is presented in this paper, and the equivalence of the estimates of the parameters under the Bayesian and the frequentist frameworks is proved. This model-based Bayesian RMA method is advantageous since the posterior estimates, the standard deviations, as well as the credible intervals of the estimates can be obtained through Markov chain Monte Carlo methods directly. In addition, it is straightforward to extend to the multivariate RMA case. The performance of the Bayesian RMA approach is evaluated in the simulation study, and, finally, the proposed method is applied to analyze a dataset in the plantation.},
  archive      = {J_BIMJ},
  author       = {Zhihua Ma and Ming-Hui Chen},
  doi          = {10.1002/bimj.202300279},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2300279},
  shortjournal = {Bio. J.},
  title        = {A bayesian model-based reduced major axis regression},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample size planning for rank-based multiple contrast tests.
<em>BIMJ</em>, <em>66</em>(3), 2300240. (<a
href="https://doi.org/10.1002/bimj.202300240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rank methods are well-established tools for comparing two or multiple (independent) groups. Statistical planning methods for the computing the required sample size(s) to detect a specific alternative with predefined power are lacking. In the present paper, we develop numerical algorithms for sample size planning of pseudo-rank-based multiple contrast tests. We discuss the treatment effects and different ways to approximate variance parameters within the estimation scheme. We further compare pairwise with global rank methods in detail. Extensive simulation studies show that the sample size estimators are accurate. A real data example illustrates the application of the methods.},
  archive      = {J_BIMJ},
  author       = {Anna Pöhlmann and Edgar Brunner and Frank Konietschke},
  doi          = {10.1002/bimj.202300240},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2300240},
  shortjournal = {Bio. J.},
  title        = {Sample size planning for rank-based multiple contrast tests},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrap tests for simultaneous monotone ordering of
effects in a two-way ANOVA. <em>BIMJ</em>, <em>66</em>(3), 2300238. (<a
href="https://doi.org/10.1002/bimj.202300238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a two-way additive analysis of variance (ANOVA) model, we consider the problem of testing for homogeneity of both row and column effects against their simultaneous ordering. The error variances are assumed to be heterogeneous with unbalanced samples in each cell. Two simultaneous test procedures are developed—the first one using the likelihood ratio test (LRT) statistics of two independent hypotheses and another based on the consecutive pairwise differences of estimators of effects. The parametric bootstrap (PB) approach is used to find critical points of both the tests and the asymptotic accuracy of the bootstrap is established. An extensive simulation study shows that the proposed tests achieve the nominal size and have very good power performance. The robustness of the tests is also analyzed under deviation from normality. An “R” package is developed and shared on “GitHub” for ease of implementation of users. The proposed tests are illustrated using a real data set on the mortality due to alcoholic liver disease and it is shown that age and gender have a significant impact on the increasing incidence of mortality.},
  archive      = {J_BIMJ},
  author       = {Raju Dey and Anjana Mondal and Somesh Kumar},
  doi          = {10.1002/bimj.202300238},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2300238},
  shortjournal = {Bio. J.},
  title        = {Bootstrap tests for simultaneous monotone ordering of effects in a two-way ANOVA},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An exhaustive ADDIS principle for online FWER control.
<em>BIMJ</em>, <em>66</em>(3), 2300237. (<a
href="https://doi.org/10.1002/bimj.202300237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider online multiple testing with familywise error rate (FWER) control, where the probability of committing at least one type I error will remain under control while testing a possibly infinite sequence of hypotheses over time. Currently, adaptive-discard (ADDIS) procedures seem to be the most promising online procedures with FWER control in terms of power. Now, our main contribution is a uniform improvement of the ADDIS principle and thus of all ADDIS procedures. This means, the methods we propose reject as least as much hypotheses as ADDIS procedures and in some cases even more, while maintaining FWER control. In addition, we show that there is no other FWER controlling procedure that enlarges the event of rejecting any hypothesis. Finally, we apply the new principle to derive uniform improvements of the ADDIS-Spending and ADDIS-Graph.},
  archive      = {J_BIMJ},
  author       = {Lasse Fischer and Marta Bofill Roig and Werner Brannath},
  doi          = {10.1002/bimj.202300237},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2300237},
  shortjournal = {Bio. J.},
  title        = {An exhaustive ADDIS principle for online FWER control},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On repeated diagnostic testing in screening for a medical
condition: How often should the diagnostic test be repeated?
<em>BIMJ</em>, <em>66</em>(3), 2300175. (<a
href="https://doi.org/10.1002/bimj.202300175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In screening large populations a diagnostic test is frequently used repeatedly. An example is screening for bowel cancer using the fecal occult blood test (FOBT) on several occasions such as at 3 or 6 days. The question that is addressed here is how often should we repeat a diagnostic test when screening for a specific medical condition. Sensitivity is often used as a performance measure of a diagnostic test and is considered here for the individual application of the diagnostic test as well as for the overall screening procedure. The latter can involve an increasingly large number of repeated applications, but how many are sufficient? We demonstrate the issues involved in answering this question using real data on bowel cancer at St Vincents Hospital in Sydney. As data are only available for those testing positive at least once, an appropriate modeling technique is developed on the basis of the zero-truncated binomial distribution which allows for population heterogeneity. The latter is modeled using discrete nonparametric maximum likelihood. If we wish to achieve an overall sensitivity of 90%, the FOBT should be repeated for 2 weeks instead of the 1 week that was used at the time of the survey. A simulation study also shows consistency in the sense that bias and standard deviation for the estimated sensitivity decrease with an increasing number of repeated occasions as well as with increasing sample size.},
  archive      = {J_BIMJ},
  author       = {Patarawan Sangnawakij and Dankmar Böhning},
  doi          = {10.1002/bimj.202300175},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2300175},
  shortjournal = {Bio. J.},
  title        = {On repeated diagnostic testing in screening for a medical condition: How often should the diagnostic test be repeated?},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel nonparametric time-dependent precision–recall curve
estimator for right-censored survival data. <em>BIMJ</em>,
<em>66</em>(3), 2300135. (<a
href="https://doi.org/10.1002/bimj.202300135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to assess prognostic risk for individuals in precision health research, risk prediction models are increasingly used, in which statistical models are used to estimate the risk of future outcomes based on clinical and nonclinical characteristics. The predictive accuracy of a risk score must be assessed before it can be used in routine clinical decision making, where the receiver operator characteristic curves, precision–recall curves, and their corresponding area under the curves are commonly used metrics to evaluate the discriminatory ability of a continuous risk score. Among these the precision–recall curves have been shown to be more informative when dealing with unbalanced biomarker distribution between classes, which is common in rare event, even though except one, all existing methods are proposed for classic uncensored data. This paper is therefore to propose a novel nonparametric estimation approach for the time-dependent precision–recall curve and its associated area under the curve for right-censored data. A simulation is conducted to show the better finite sample property of the proposed estimator over the existing method and a real-world data from primary biliary cirrhosis trial is used to demonstrate the practical applicability of the proposed estimator.},
  archive      = {J_BIMJ},
  author       = {Kassu Mehari Beyene and Ding-Geng Chen and Yehenew Getachew Kifle},
  doi          = {10.1002/bimj.202300135},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2300135},
  shortjournal = {Bio. J.},
  title        = {A novel nonparametric time-dependent precision–recall curve estimator for right-censored survival data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beta spending function based on conditional power in group
sequential design. <em>BIMJ</em>, <em>66</em>(3), 2300094. (<a
href="https://doi.org/10.1002/bimj.202300094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional power (CP) serves as a widely utilized approach for futility monitoring in group sequential designs. However, adopting the CP methods may lead to inadequate control of the type II error rate at the desired level. In this study, we introduce a flexible beta spending function tailored to regulate the type II error rate while employing CP based on a predetermined standardized effect size for futility monitoring (a so-called CP-beta spending function). This function delineates the expenditure of type II error rate across the entirety of the trial. Unlike other existing beta spending functions, the CP-beta spending function seamlessly incorporates beta spending concept into the CP framework, facilitating precise stagewise control of the type II error rate during futility monitoring. In addition, the stopping boundaries derived from the CP-beta spending function can be calculated via integration akin to other traditional beta spending function methods. Furthermore, the proposed CP-beta spending function accommodates various thresholds on the CP-scale at different stages of the trial, ensuring its adaptability across different information time scenarios. These attributes render the CP-beta spending function competitive among other forms of beta spending functions, making it applicable to any trials in group sequential designs with straightforward implementation. Both simulation study and example from an acute ischemic stroke trial demonstrate that the proposed method accurately captures expected power, even when the initially determined sample size does not consider futility stopping, and exhibits a good performance in maintaining overall type I error rates for evident futility.},
  archive      = {J_BIMJ},
  author       = {Senmiao Ni and Zihang Zhong and Zhiwei Jiang and Yang Zhao and Jingwei Wu and Hao Yu and Jianling Bai},
  doi          = {10.1002/bimj.202300094},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2300094},
  shortjournal = {Bio. J.},
  title        = {Beta spending function based on conditional power in group sequential design},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework to select tuning parameters for nonparametric
derivative estimation. <em>BIMJ</em>, <em>66</em>(3), 2300039. (<a
href="https://doi.org/10.1002/bimj.202300039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a general framework to select tuning parameters for the nonparametric derivative estimation. The new framework broadens the scope of the previously proposed generalized C p $C_p$ criterion by replacing the empirical derivative with any other linear nonparametric smoother. We provide the theoretical support of the proposed derivative estimation in a random design and justify it through simulation studies. The practical application of the proposed framework is demonstrated in the study of the age effect on hippocampal gray matter volume in healthy adults from the IXI dataset and the study of the effect of age and body mass index on blood pressure from the Pima Indians dataset.},
  archive      = {J_BIMJ},
  author       = {Sisheng Liu and Xiaoli Kong},
  doi          = {10.1002/bimj.202300039},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2300039},
  shortjournal = {Bio. J.},
  title        = {A framework to select tuning parameters for nonparametric derivative estimation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mapping QTL controlling count traits with excess zeros and
ones using a zero-and-one-inflated generalized poisson regression model.
<em>BIMJ</em>, <em>66</em>(3), 2200342. (<a
href="https://doi.org/10.1002/bimj.202200342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on the quantitative trait locus (QTL) mapping of count data has aroused the wide attention of researchers. There are frequent problems in applied research that limit the application of the conventional Poisson model in the analysis of count phenotypes, which include the overdispersion and excess zeros and ones. In this article, a novel model, that is, the zero-and-one-inflated generalized Poisson (ZOIGP) model, is proposed to deal with these problems. Based on the proposed model, a score test is performed for the inflation parameter, in which the ZOIGP model with a constant proportion of excess zeros and ones is compared with a standard generalized Poisson model. To illustrate the practicability of the ZOIGP model, we extend it to the QTL interval mapping application that underpins count phenotype with excess zeros and excess ones. The genetic effects are estimated utilizing the expectation–maximization algorithm embedded with the Newton–Raphson algorithm, and the genome-wide scan and likelihood ratio test is performed to map and test the potential QTLs. The statistical properties exhibited by the proposed method are investigated through simulation. Finally, a real data analysis example is used to illustrate the utility of the proposed method for QTL mapping.},
  archive      = {J_BIMJ},
  author       = {Jinling Chi and Jimin Ye and Ying Zhou},
  doi          = {10.1002/bimj.202200342},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2200342},
  shortjournal = {Bio. J.},
  title        = {Mapping QTL controlling count traits with excess zeros and ones using a zero-and-one-inflated generalized poisson regression model},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recoverability and estimation of causal effects under
typical multivariable missingness mechanisms. <em>BIMJ</em>,
<em>66</em>(3), 2200326. (<a
href="https://doi.org/10.1002/bimj.202200326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of missing data, the identifiability or “recoverability” of the average causal effect (ACE) depends not only on the usual causal assumptions but also on missingness assumptions that can be depicted by adding variable-specific missingness indicators to causal diagrams, creating missingness directed acyclic graphs (m-DAGs). Previous research described canonical m-DAGs, representing typical multivariable missingness mechanisms in epidemiological studies, and examined mathematically the recoverability of the ACE in each case. However, this work assumed no effect modification and did not investigate methods for estimation across such scenarios. Here, we extend this research by determining the recoverability of the ACE in settings with effect modification and conducting a simulation study to evaluate the performance of widely used missing data methods when estimating the ACE using correctly specified g-computation. Methods assessed were complete case analysis (CCA) and various implementations of multiple imputation (MI) with varying degrees of compatibility with the outcome model used in g-computation. Simulations were based on an example from the Victorian Adolescent Health Cohort Study (VAHCS), where interest was in estimating the ACE of adolescent cannabis use on mental health in young adulthood. We found that the ACE is recoverable when no incomplete variable (exposure, outcome, or confounder) causes its own missingness, and nonrecoverable otherwise, in simplified versions of 10 canonical m-DAGs that excluded unmeasured common causes of missingness indicators. Despite this nonrecoverability, simulations showed that MI approaches that are compatible with the outcome model in g-computation may enable approximately unbiased estimation across all canonical m-DAGs considered, except when the outcome causes its own missingness or causes the missingness of a variable that causes its own missingness. In the latter settings, researchers may need to consider sensitivity analysis methods incorporating external information (e.g., delta-adjustment methods). The VAHCS case study illustrates the practical implications of these findings.},
  archive      = {J_BIMJ},
  author       = {Jiaxin Zhang and S. Ghazaleh Dashti and John B. Carlin and Katherine J. Lee and Margarita Moreno-Betancur},
  doi          = {10.1002/bimj.202200326},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2200326},
  shortjournal = {Bio. J.},
  title        = {Recoverability and estimation of causal effects under typical multivariable missingness mechanisms},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharing information across patient subgroups to draw
conclusions from sparse treatment networks. <em>BIMJ</em>,
<em>66</em>(3), 2200316. (<a
href="https://doi.org/10.1002/bimj.202200316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) usually provides estimates of the relative effects with the highest possible precision. However, sparse networks with few available studies and limited direct evidence can arise, threatening the robustness and reliability of NMA estimates. In these cases, the limited amount of available information can hamper the formal evaluation of the underlying NMA assumptions of transitivity and consistency. In addition, NMA estimates from sparse networks are expected to be imprecise and possibly biased as they rely on large-sample approximations that are invalid in the absence of sufficient data. We propose a Bayesian framework that allows sharing of information between two networks that pertain to different population subgroups. Specifically, we use the results from a subgroup with a lot of direct evidence (a dense network) to construct informative priors for the relative effects in the target subgroup (a sparse network). This is a two-stage approach where at the first stage, we extrapolate the results of the dense network to those expected from the sparse network. This takes place by using a modified hierarchical NMA model where we add a location parameter that shifts the distribution of the relative effects to make them applicable to the target population. At the second stage, these extrapolated results are used as prior information for the sparse network. We illustrate our approach through a motivating example of psychiatric patients. Our approach results in more precise and robust estimates of the relative effects and can adequately inform clinical practice in presence of sparse networks.},
  archive      = {J_BIMJ},
  author       = {Theodoros Evrenoglou and Silvia Metelli and Johannes-Schneider Thomas and Spyridon Siafis and Rebecca M. Turner and Stefan Leucht and Anna Chaimani},
  doi          = {10.1002/bimj.202200316},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {3},
  pages        = {2200316},
  shortjournal = {Bio. J.},
  title        = {Sharing information across patient subgroups to draw conclusions from sparse treatment networks},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explained variation and degrees of necessity and of
sufficiency for competing risks survival data. <em>BIMJ</em>,
<em>66</em>(2), 2300140. (<a
href="https://doi.org/10.1002/bimj.202300140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this contribution, the Schemper–Henderson measure of explained variation for survival outcomes is extended to accommodate competing events (CEs) in addition to events of interest. The extension is achieved by moving from the unconditional and conditional survival functions of the original measure to unconditional and conditional cumulative incidence functions, the latter obtained, for example, from Fine and Gray models. In the absence of CEs, the original measure is obtained as a special case. We define explained variation on the population level and provide two different types of estimates. Recently, the authors have achieved a multiplicative decomposition of explained variation into degrees of necessity and degrees of sufficiency. These measures are also extended to the case of competing risks survival data. A SAS macro and an R function are provided to facilitate application. Interesting empirical properties of the measures are explored on the population level and by an extensive simulation study. Advantages of the approach are exemplified by an Austrian study of breast cancer with a high proportion of CEs.},
  archive      = {J_BIMJ},
  author       = {Andreas Gleiss and Michael Gnant and Michael Schemper},
  doi          = {10.1002/bimj.202300140},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {2300140},
  shortjournal = {Bio. J.},
  title        = {Explained variation and degrees of necessity and of sufficiency for competing risks survival data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalized calibrated bayesian hierarchical modeling
approach to basket trials with multiple endpoints. <em>BIMJ</em>,
<em>66</em>(2), 2300122. (<a
href="https://doi.org/10.1002/bimj.202300122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A basket trial simultaneously evaluates a treatment in multiple cancer subtypes, offering an effective way to accelerate drug development in multiple indications. Many basket trials are designed and monitored based on a single efficacy endpoint, primarily the tumor response. For molecular targeted or immunotherapy agents, however, a single efficacy endpoint cannot adequately characterize the treatment effect. It is increasingly important to use more complex endpoints to comprehensively assess the risk–benefit profile of such targeted therapies. We extend the calibrated Bayesian hierarchical modeling approach to monitor phase II basket trials with multiple endpoints. We propose two generalizations, one based on the latent variable approach and the other based on the multinomial–normal hierarchical model, to accommodate different types of endpoints and dependence assumptions regarding information sharing. We introduce shrinkage parameters as functions of statistics measuring homogeneity among subgroups and propose a general calibration approach to determine the functional forms. Theoretical properties of the generalized hierarchical models are investigated. Simulation studies demonstrate that the monitoring procedure based on the generalized approach yields desirable operating characteristics.},
  archive      = {J_BIMJ},
  author       = {Xiaohan Chi and Ying Yuan and Zhangsheng Yu and Ruitao Lin},
  doi          = {10.1002/bimj.202300122},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {2300122},
  shortjournal = {Bio. J.},
  title        = {A generalized calibrated bayesian hierarchical modeling approach to basket trials with multiple endpoints},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Interpretability of bi-level variable selection methods.
<em>BIMJ</em>, <em>66</em>(2), 2300063. (<a
href="https://doi.org/10.1002/bimj.202300063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection is usually performed to increase interpretability, as sparser models are easier to understand than full models. However, a focus on sparsity is not always suitable, for example, when features are related due to contextual similarities or high correlations. Here, it may be more appropriate to identify groups and their predictive members, a task that can be accomplished with bi-level selection procedures. To investigate whether such techniques lead to increased interpretability, group exponential LASSO (GEL), sparse group LASSO (SGL), composite minimax concave penalty (cMCP), and least absolute shrinkage, and selection operator (LASSO) as reference methods were used to select predictors in time-to-event, regression, and classification tasks in bootstrap samples from a cohort of 1001 patients. Different groupings based on prior knowledge, correlation structure, and random assignment were compared in terms of selection relevance, group consistency, and collinearity tolerance. The results show that bi-level selection methods are superior to LASSO in all criteria. The cMCP demonstrated superiority in selection relevance, while SGL was convincing in group consistency. An all-round capacity was achieved by GEL: the approach jointly selected correlated and content-related predictors while maintaining high selection relevance. This method seems recommendable when variables are grouped, and interpretation is of primary interest.},
  archive      = {J_BIMJ},
  author       = {Gregor Buch and Andreas Schulz and Irene Schmidtmann and Konstantin Strauch and Philipp S. Wild},
  doi          = {10.1002/bimj.202300063},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {2300063},
  shortjournal = {Bio. J.},
  title        = {Interpretability of bi-level variable selection methods},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review on statistical and machine learning competing risks
methods. <em>BIMJ</em>, <em>66</em>(2), 2300060. (<a
href="https://doi.org/10.1002/bimj.202300060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When modeling competing risks (CR) survival data, several techniques have been proposed in both the statistical and machine learning literature. State-of-the-art methods have extended classical approaches with more flexible assumptions that can improve predictive performance, allow high-dimensional data and missing values, among others. Despite this, modern approaches have not been widely employed in applied settings. This article aims to aid the uptake of such methods by providing a condensed compendium of CR survival methods with a unified notation and interpretation across approaches. We highlight available software and, when possible, demonstrate their usage via reproducible R vignettes. Moreover, we discuss two major concerns that can affect benchmark studies in this context: the choice of performance metrics and reproducibility.},
  archive      = {J_BIMJ},
  author       = {Karla Monterrubio-Gómez and Nathan Constantine-Cooke and Catalina A. Vallejos},
  doi          = {10.1002/bimj.202300060},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {2300060},
  shortjournal = {Bio. J.},
  title        = {A review on statistical and machine learning competing risks methods},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse multiway canonical correlation analysis for
multimodal stroke recovery data. <em>BIMJ</em>, <em>66</em>(2), 2300037.
(<a href="https://doi.org/10.1002/bimj.202300037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional canonical correlation analysis (CCA) measures the association between two datasets and identifies relevant contributors. However, it encounters issues with execution and interpretation when the sample size is smaller than the number of variables or there are more than two datasets. Our motivating example is a stroke-related clinical study on pigs. The data are multimodal and consist of measurements taken at multiple time points and have many more variables than observations. This study aims to uncover important biomarkers and stroke recovery patterns based on physiological changes. To address the issues in the data, we develop two sparse CCA methods for multiple datasets. Various simulated examples are used to illustrate and contrast the performance of the proposed methods with that of the existing methods. In analyzing the pig stroke data, we apply the proposed sparse CCA methods along with dimension reduction techniques, interpret the recovery patterns, and identify influential variables in recovery.},
  archive      = {J_BIMJ},
  author       = {Subham Das and Franklin D. West and Cheolwoo Park},
  doi          = {10.1002/bimj.202300037},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {2300037},
  shortjournal = {Bio. J.},
  title        = {Sparse multiway canonical correlation analysis for multimodal stroke recovery data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pairwise fitting of piecewise mixed models for the joint
modeling of multivariate longitudinal outcomes, in a randomized
crossover trial. <em>BIMJ</em>, <em>66</em>(2), 2200333. (<a
href="https://doi.org/10.1002/bimj.202200333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many statistical models have been proposed in the literature for the analysis of longitudinal data. One may propose to model two or more correlated longitudinal processes simultaneously, with a goal of understanding their association over time. Joint modeling is then required to carefully study the association structure among the outcomes as well as drawing joint inferences about the different outcomes. In this study, we sought to model the associations among six nutrition outcomes while circumventing the computational challenge posed by their clustered and high-dimensional nature. We analyzed data from a 2 2 randomized crossover trial conducted in Kenya, to compare the effect of high-dose and low-dose iodine in household salt on systolic blood pressure (SBP) and diastolic blood pressure (DBP) in women of reproductive age and their household matching pair of school-aged children. Two additional outcomes, namely, urinary iodine concentration (UIC) in women and children were measured repeatedly to monitor the amount of iodine excreted through urine. We extended the model proposed by Mwangi et al. (2021, Communications in Statistics: Case Studies, Data Analysis and Applications , 7 (3), 413–431) allowing flexible piecewise joint models for six outcomes to depend on separate random effects, which are themselves correlated. This entailed fitting 15 bivariate general linear mixed models and deriving inference for the joint model using pseudo-likelihood theory. We analyzed the outcomes separately and jointly using piecewise linear mixed-effects (PLME) model and further validated the results using current state-of-the-art Jones and Kenward methodology (JKME model) used for analyzing randomized crossover trials. The results indicate that high-dose iodine in salt significantly reduced blood pressure (BP) compared to low-dose iodine in salt. Estimates for the random effects and residual error components showed that SBP and DBP had strong positive correlation, with effect of the random slope indicating that significantly related outcomes are strongly associated in their evolution. There was a moderately strong inverse relationship between evolutions of UIC and BP both in women and children. These findings confirmed the original hypothesis that high-dose iodine salt has significant lowering effect on BP. We further sought to evaluate the performance of our proposed PLME model against the widely used JKME model, within the multivariate joint modeling framework through a simulation study mimicking a crossover design. From our findings, the multivariate joint PLME model performed exceptionally well both in estimation of random-effects matrix (G) and Hessian matrix (H), allowing satisfactory model convergence during estimation. It allowed a more complex fit to the data with both random intercepts and slopes effects compared to the multivariate joint JKME model that allowed for random intercepts only. When a hierarchical viewpoint is adopted, in the sense that outcomes are specified conditionally upon random effects, the variance–covariance matrix of the random effects must be positive definite. In some cases, additional random effects could explain much variability in the data, thus improving precision in estimation of the estimands (effect size) parameters. The key highlight in this evaluation shows that multivariate joint JKME model is a powerful tool especially while fitting mixed models with random intercepts only, in crossover design settings. Addition of random slopes may lead to model complexities in most cases, resulting in unsatisfactory model convergence during estimation. To circumvent convergence pitfalls, extention of JKME model to PLME model allows a more flexible fit to the data (generated from crossover design settings), especially in the multivariate joint modeling framework.},
  archive      = {J_BIMJ},
  author       = {Moses Mwangi and Geert Molenberghs and Edmund Njeru Njagi and Samuel Mwalili and Roel Braekers and Alvaro Jose Florez and Susan Gachau and Zipporah N. Bukania and Geert Verbeke},
  doi          = {10.1002/bimj.202200333},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {2200333},
  shortjournal = {Bio. J.},
  title        = {Pairwise fitting of piecewise mixed models for the joint modeling of multivariate longitudinal outcomes, in a randomized crossover trial},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the proportion of true null hypotheses and
adaptive false discovery rate control in discrete paradigm.
<em>BIMJ</em>, <em>66</em>(2), 2200204. (<a
href="https://doi.org/10.1002/bimj.202200204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storey&#39;s estimator for the proportion of true null hypotheses, originally proposed under the continuous framework, has been modified in this work under the discrete framework. The modification results in improved estimation of the parameter of interest. The proposed estimator is used to formulate an adaptive version of the Benjamini–Hochberg procedure. Control over the false discovery rate by the proposed adaptive procedure has been proved analytically. The proposed estimate is also used to formulate an adaptive version of the Benjamini–Hochberg–Heyse procedure. Simulation experiments establish the conservative nature of this new adaptive procedure. Substantial amount of gain in power is observed for the new adaptive procedures over the standard procedures. For demonstration of the proposed method, two important real life gene expression data sets, one related to the study of HIV and the other related to methylation study, are used.},
  archive      = {J_BIMJ},
  author       = {Aniket Biswas and Gaurangadeb Chattopadhyay},
  doi          = {10.1002/bimj.202200204},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {2200204},
  shortjournal = {Bio. J.},
  title        = {Estimating the proportion of true null hypotheses and adaptive false discovery rate control in discrete paradigm},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric analysis of delayed treatment effects using
single-crossing constraints. <em>BIMJ</em>, <em>66</em>(2), 2200165. (<a
href="https://doi.org/10.1002/bimj.202200165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials involving novel immuno-oncology therapies frequently exhibit survival profiles which violate the proportional hazards assumption due to a delay in treatment effect, and, in such settings, the survival curves in the two treatment arms may have a crossing before the two curves eventually separate. To flexibly model such scenarios, we describe a nonparametric approach for estimating the treatment arm-specific survival functions which constrains these two survival functions to cross at most once without making any additional assumptions about how the survival curves are related. A main advantage of our approach is that it provides an estimate of a crossing time if such a crossing exists, and, moreover, our method generates interpretable measures of treatment benefit including crossing-conditional survival probabilities and crossing-conditional estimates of restricted residual mean life. Our estimates of these measures may be used together with efficacy measures from a primary analysis to provide further insight into differences in survival across treatment arms. We demonstrate the use and effectiveness of our approach with a large simulation study and an analysis of reconstructed outcomes from a recent combination therapy trial.},
  archive      = {J_BIMJ},
  author       = {Nicholas C. Henderson and Kijoeng Nam and Dai Feng},
  doi          = {10.1002/bimj.202200165},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {2200165},
  shortjournal = {Bio. J.},
  title        = {Nonparametric analysis of delayed treatment effects using single-crossing constraints},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of survival data with nonproportional hazards: A
comparison of propensity-score-weighted methods. <em>BIMJ</em>,
<em>66</em>(1), 202200099. (<a
href="https://doi.org/10.1002/bimj.202200099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most common ways researchers compare cancer survival outcomes across treatments from observational data is using Cox regression. This model depends on its underlying assumption of proportional hazards, but in some real-world cases, such as when comparing different classes of cancer therapies, substantial violations may occur. In this situation, researchers have several alternative methods to choose from, including Cox models with time-varying hazard ratios; parametric accelerated failure time models; Kaplan–Meier curves; and pseudo-observations. It is unclear which of these models are likely to perform best in practice. To fill this gap in the literature, we perform a neutral comparison study of candidate approaches. We examine clinically meaningful outcome measures that can be computed and directly compared across each method, namely, survival probability at time T , median survival, and restricted mean survival. To adjust for differences between treatment groups, we use inverse probability of treatment weighting based on the propensity score. We conduct simulation studies under a range of scenarios, and determine the biases, coverages, and standard errors of the average treatment effects for each method. We then demonstrate the use of these approaches using two published observational studies of survival after cancer treatment. The first examines chemotherapy in sarcoma, which has a late treatment effect (i.e., similar survival initially, but after 2 years the chemotherapy group shows a benefit). The other study is a comparison of surgical techniques for kidney cancer, where survival differences are attenuated over time.},
  archive      = {J_BIMJ},
  author       = {Elizabeth A. Handorf and Marc C. Smaldone and Sujana Movva and Nandita Mitra},
  doi          = {10.1002/bimj.202200099},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {202200099},
  shortjournal = {Bio. J.},
  title        = {Analysis of survival data with nonproportional hazards: A comparison of propensity-score-weighted methods},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online false discovery rate control for LORD++ and SAFFRON
under positive, local dependence. <em>BIMJ</em>, <em>66</em>(1),
2300177. (<a href="https://doi.org/10.1002/bimj.202300177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online testing procedures assume that hypotheses are observed in sequence, and allow the significance thresholds for upcoming tests to depend on the test statistics observed so far. Some of the most popular online methods include alpha investing, LORD++, and SAFFRON. These three methods have been shown to provide online control of the “modified” false discovery rate (mFDR) under a condition known as CS. However, to our knowledge, LORD++ and SAFFRON have only been shown to control the traditional false discovery rate (FDR) under an independence condition on the test statistics. Our work bolsters these results by showing that SAFFRON and LORD++ additionally ensure online control of the FDR under a “local” form of nonnegative dependence. Further, FDR control is maintained under certain types of adaptive stopping rules, such as stopping after a certain number of rejections have been observed. Because alpha investing can be recovered as a special case of the SAFFRON framework, our results immediately apply to alpha investing as well. In the process of deriving these results, we also formally characterize how the conditional super-uniformity assumption implicitly limits the allowed p -value dependencies. This implicit limitation is important not only to our proposed FDR result, but also to many existing mFDR results.},
  archive      = {J_BIMJ},
  author       = {Aaron Fisher},
  doi          = {10.1002/bimj.202300177},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2300177},
  shortjournal = {Bio. J.},
  title        = {Online false discovery rate control for LORD++ and SAFFRON under positive, local dependence},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian optimal stepped wedge design. <em>BIMJ</em>,
<em>66</em>(1), 2300168. (<a
href="https://doi.org/10.1002/bimj.202300168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been a growing interest in designing cluster trials using stepped wedge design (SWD). An SWD is a type of cluster–crossover design in which clusters of individuals are randomized unidirectional from a control to an intervention at certain time points. The intraclass correlation coefficient (ICC) that measures the dependency of subject within a cluster plays an important role in design and analysis of stepped wedge trials. In this paper, we discuss a Bayesian approach to address the dependency of SWD on the ICC and robust Bayesian SWDs are proposed. Bayesian design is shown to be more robust against the misspecification of the parameter values compared to the locally optimal design. Designs are obtained for the various choices of priors assigned to the ICC. A detailed sensitivity analysis is performed to assess the robustness of proposed optimal designs. The power superiority of Bayesian design against the commonly used balanced design is demonstrated numerically using hypothetical as well as real scenarios.},
  archive      = {J_BIMJ},
  author       = {Satya Prakash Singh},
  doi          = {10.1002/bimj.202300168},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2300168},
  shortjournal = {Bio. J.},
  title        = {Bayesian optimal stepped wedge design},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mediation analysis with case–control sampling:
Identification and estimation in the presence of a binary mediator.
<em>BIMJ</em>, <em>66</em>(1), 2300089. (<a
href="https://doi.org/10.1002/bimj.202300089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With reference to a stratified case–control (CC) procedure based on a binary variable of primary interest, we derive the expression of the distortion induced by the sampling design on the parameters of the logistic model of a secondary variable. This is particularly relevant when performing mediation analysis (possibly in a causal framework) with stratified case–control (SCC) data in settings where both the outcome and the mediator are binary. Despite being designed for parametric identification, our strategy is general and can be used also in a nonparametric context. With reference to parametric estimation, we derive the maximum likelihood (ML) estimator and the M-estimator of the joint outcome–mediator parameter vector. We then conduct a simulation study focusing on the main causal mediation quantities (i.e., natural effects) and comparing M- and ML estimation to existing methods, based on weighting. As an illustrative example, we reanalyze a German CC data set in order to investigate whether the effect of reduced immunocompetency on listeriosis onset is mediated by the intake of gastric acid suppressors.},
  archive      = {J_BIMJ},
  author       = {Marco Doretti and Minna Genbäck and Elena Stanghellini},
  doi          = {10.1002/bimj.202300089},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2300089},
  shortjournal = {Bio. J.},
  title        = {Mediation analysis with case–control sampling: Identification and estimation in the presence of a binary mediator},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comment on oberman &amp; vink: Should we fix or simulate the
complete data in simulation studies evaluating missing data methods?
<em>BIMJ</em>, <em>66</em>(1), 2300085. (<a
href="https://doi.org/10.1002/bimj.202300085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For simulation studies that evaluate methods of handling missing data, we argue that generating partially observed data by fixing the complete data and repeatedly simulating the missingness indicators is a superficially attractive idea but only rarely appropriate to use.},
  archive      = {J_BIMJ},
  author       = {Tim P. Morris and Ian R. White and Suzie Cro and Jonathan W. Bartlett and James R. Carpenter and Tra My Pham},
  doi          = {10.1002/bimj.202300085},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2300085},
  shortjournal = {Bio. J.},
  title        = {Comment on oberman &amp; vink: Should we fix or simulate the complete data in simulation studies evaluating missing data methods?},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple testing of composite null hypotheses for discrete
data using randomized p-values. <em>BIMJ</em>, <em>66</em>(1), 2300077.
(<a href="https://doi.org/10.1002/bimj.202300077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {P -values that are derived from continuously distributed test statistics are typically uniformly distributed on (0,1) under least favorable parameter configurations (LFCs) in the null hypothesis. Conservativeness of a p -value P (meaning that P is under the null hypothesis stochastically larger than uniform on (0,1)) can occur if the test statistic from which P is derived is discrete, or if the true parameter value under the null is not an LFC. To deal with both of these sources of conservativeness, we present two approaches utilizing randomized p -values. We illustrate their effectiveness for testing a composite null hypothesis under a binomial model. We also give an example of how the proposed p -values can be used to test a composite null in group testing designs. We find that the proposed randomized p -values are less conservative compared to nonrandomized p -values under the null hypothesis, but that they are stochastically not smaller under the alternative. The problem of establishing the validity of randomized p -values has received attention in previous literature. We show that our proposed randomized p -values are valid under various discrete statistical models, which are such that the distribution of the corresponding test statistic belongs to an exponential family. The behavior of the power function for the tests based on the proposed randomized p -values as a function of the sample size is also investigated. Simulations and a real data example are used to compare the different considered p -values.},
  archive      = {J_BIMJ},
  author       = {Daniel Ochieng and Anh-Tuan Hoang and Thorsten Dickhaus},
  doi          = {10.1002/bimj.202300077},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2300077},
  shortjournal = {Bio. J.},
  title        = {Multiple testing of composite null hypotheses for discrete data using randomized p-values},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate joint model under competing risks to predict
death of hospitalized patients for SARS-CoV-2 infection. <em>BIMJ</em>,
<em>66</em>(1), 2300049. (<a
href="https://doi.org/10.1002/bimj.202300049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the coronavirus disease 2019 (COVID-19) pandemic, several clinical prognostic scores have been proposed and evaluated in hospitalized patients, relying on variables available at admission. However, capturing data collected from the longitudinal follow-up of patients during hospitalization may improve prediction accuracy of a clinical outcome. To answer this question, 327 patients diagnosed with COVID-19 and hospitalized in an academic French hospital between January and July 2020 are included in the analysis. Up to 59 biomarkers were measured from the patient admission to the time to death or discharge from hospital. We consider a joint model with multiple linear or nonlinear mixed-effects models for biomarkers evolution, and a competing risks model involving subdistribution hazard functions for the risks of death and discharge. The links are modeled by shared random effects, and the selection of the biomarkers is mainly based on the significance of the link between the longitudinal and survival parts. Three biomarkers are retained: the blood neutrophil counts, the arterial pH, and the C-reactive protein. The predictive performances of the model are evaluated with the time-dependent area under the curve (AUC) for different landmark and horizon times, and compared with those obtained from a baseline model that considers only information available at admission. The joint modeling approach helps to improve predictions when sufficient information is available. For landmark 6 days and horizon of 30 days, we obtain AUC [95% CI] 0.73 [0.65, 0.81] and 0.81 [0.73, 0.89] for the baseline and joint model, respectively ( p = 0.04). Statistical inference is validated through a simulation study.},
  archive      = {J_BIMJ},
  author       = {Alexandra Lavalley-Morelle and Nathan Peiffer-Smadja and Simon B. Gressens and Bérénice Souhail and Alexandre Lahens and Agathe Bounhiol and François-Xavier Lescure and France Mentré and Jimmy Mullaert},
  doi          = {10.1002/bimj.202300049},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2300049},
  shortjournal = {Bio. J.},
  title        = {Multivariate joint model under competing risks to predict death of hospitalized patients for SARS-CoV-2 infection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing unmeasured confounders in cohort studies:
Instrumental variable method for a time-fixed exposure on an outcome
trajectory. <em>BIMJ</em>, <em>66</em>(1), 2200358. (<a
href="https://doi.org/10.1002/bimj.202200358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrumental variable methods, which handle unmeasured confounding by targeting the part of the exposure explained by an exogenous variable not subject to confounding, have gained much interest in observational studies. We consider the very frequent setting of estimating the unconfounded effect of an exposure measured at baseline on the subsequent trajectory of an outcome repeatedly measured over time. We didactically explain how to apply the instrumental variable method in such setting by adapting the two-stage classical methodology with (1) the prediction of the exposure according to the instrumental variable, (2) its inclusion into a mixed model to quantify the exposure association with the subsequent outcome trajectory, and (3) the computation of the estimated total variance. A simulation study illustrates the consequences of unmeasured confounding in classical analyses and the usefulness of the instrumental variable approach. The methodology is then applied to 6224 participants of the 3C cohort to estimate the association of type-2 diabetes with subsequent cognitive trajectory, using 42 genetic polymorphisms as instrumental variables. This contribution shows how to handle endogeneity when interested in repeated outcomes, along with a R implementation. However, it should still be used with caution as it relies on instrumental variable assumptions hardly testable in practice.},
  archive      = {J_BIMJ},
  author       = {Kateline Le Bourdonnec and Cécilia Samieri and Christophe Tzourio and Thibault Mura and Aniket Mishra and David-Alexandre Trégouët and Cécile Proust-Lima},
  doi          = {10.1002/bimj.202200358},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200358},
  shortjournal = {Bio. J.},
  title        = {Addressing unmeasured confounders in cohort studies: Instrumental variable method for a time-fixed exposure on an outcome trajectory},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite mixtures in capture–recapture surveys for modeling
residency patterns in marine wildlife populations. <em>BIMJ</em>,
<em>66</em>(1), 2200350. (<a
href="https://doi.org/10.1002/bimj.202200350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to show how prior knowledge about the structure of a heterogeneous animal population can be leveraged to improve the abundance estimation from capture–recapture survey data. We combine the Open Jolly-Seber model with finite mixtures and propose a parsimonious specification tailored to the residency patterns of the common bottlenose dolphin. We employ a Bayesian framework for our inference, discussing the appropriate choice of priors to mitigate label-switching and nonidentifiability issues, commonly associated with finite mixture models. We conduct a series of simulation experiments to illustrate the competitive advantage of our proposal over less specific alternatives. The proposed approach is applied to data collected on the common bottlenose dolphin population inhabiting the Tiber River estuary (Mediterranean Sea). Our results provide novel insights into this population&#39;s size and structure, shedding light on some of the ecological processes governing its dynamics.},
  archive      = {J_BIMJ},
  author       = {Gianmarco Caruso and Pierfrancesco Alaimo Di Loro and Marco Mingione and Luca Tardella and Daniela Silvia Pace and Giovanna Jona Lasinio},
  doi          = {10.1002/bimj.202200350},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200350},
  shortjournal = {Bio. J.},
  title        = {Finite mixtures in capture–recapture surveys for modeling residency patterns in marine wildlife populations},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parametric modal regression with error in covariates.
<em>BIMJ</em>, <em>66</em>(1), 2200348. (<a
href="https://doi.org/10.1002/bimj.202200348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An inference procedure is proposed to provide consistent estimators of parameters in a modal regression model with a covariate prone to measurement error. A score-based diagnostic tool exploiting parametric bootstrap is developed to assess adequacy of parametric assumptions imposed on the regression model. The proposed estimation method and diagnostic tool are applied to synthetic data generated from simulation experiments and data from real-world applications to demonstrate their implementation and performance. These empirical examples illustrate the importance of adequately accounting for measurement error in the error-prone covariate when inferring the association between a response and covariates based on a modal regression model that is especially suitable for skewed and heavy-tailed response data.},
  archive      = {J_BIMJ},
  author       = {Qingyang Liu and Xianzheng Huang},
  doi          = {10.1002/bimj.202200348},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200348},
  shortjournal = {Bio. J.},
  title        = {Parametric modal regression with error in covariates},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian hierarchical approach to account for evidence and
uncertainty in the modeling of infectious diseases: An application to
COVID-19. <em>BIMJ</em>, <em>66</em>(1), 2200341. (<a
href="https://doi.org/10.1002/bimj.202200341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infectious disease models can serve as critical tools to predict the development of cases and associated healthcare demand and to determine the set of nonpharmaceutical interventions (NPIs) that is most effective in slowing the spread of an infectious agent. Current approaches to estimate NPI effects typically focus on relatively short time periods and either on the number of reported cases, deaths, intensive care occupancy, or hospital occupancy as a single indicator of disease transmission. In this work, we propose a Bayesian hierarchical model that integrates multiple outcomes and complementary sources of information in the estimation of the true and unknown number of infections while accounting for time-varying underreporting and weekday-specific delays in reported cases and deaths, allowing us to estimate the number of infections on a daily basis rather than having to smooth the data. To address dynamic changes occurring over long periods of time, we account for the spread of new variants, seasonality, and time-varying differences in host susceptibility. We implement a Markov chain Monte Carlo algorithm to conduct Bayesian inference and illustrate the proposed approach with data on COVID-19 from 20 European countries. The approach shows good performance on simulated data and produces posterior predictions that show a good fit to reported cases, deaths, hospital, and intensive care occupancy.},
  archive      = {J_BIMJ},
  author       = {Raphael Rehms and Nicole Ellenbach and Eva Rehfuess and Jacob Burns and Ulrich Mansmann and Sabine Hoffmann},
  doi          = {10.1002/bimj.202200341},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200341},
  shortjournal = {Bio. J.},
  title        = {A bayesian hierarchical approach to account for evidence and uncertainty in the modeling of infectious diseases: An application to COVID-19},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Drug combinations screening using a bayesian ranking
approach based on dose–response models. <em>BIMJ</em>, <em>66</em>(1),
2200332. (<a href="https://doi.org/10.1002/bimj.202200332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug combinations have been of increasing interest in recent years for the treatment of complex diseases such as cancer, as they could reduce the risk of drug resistance. Moreover, in oncology, combining drugs may allow tackling tumor heterogeneity. Identifying potent combinations can be an arduous task since exploring the full dose–response matrix of candidate combinations over a large number of drugs is costly and sometimes unfeasible, as the quantity of available biological material is limited and may vary across patients. Our objective was to develop a rank-based screening approach for drug combinations in the setting of limited biological resources. A hierarchical Bayesian 4-parameter log-logistic (4PLL) model was used to estimate dose–response curves of dose–candidate combinations based on a parsimonious experimental design. We computed various activity ranking metrics, such as the area under the dose–response curve and Bliss synergy score, and we used the posterior distributions of ranks and the surface under the cumulative ranking curve to obtain a comprehensive final ranking of combinations. Based on simulations, our proposed method achieved good operating characteristics to identifying the most promising treatments in various scenarios with limited sample sizes and interpatient variability. We illustrate the proposed approach on real data from a combination screening experiment in acute myeloid leukemia.},
  archive      = {J_BIMJ},
  author       = {Luana Boumendil and Morgane Fontaine and Vincent Lévy and Kim Pacchiardi and Raphaël Itzykson and Lucie Biard},
  doi          = {10.1002/bimj.202200332},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200332},
  shortjournal = {Bio. J.},
  title        = {Drug combinations screening using a bayesian ranking approach based on dose–response models},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Surrogacy validation for time-to-event outcomes with
illness-death frailty models. <em>BIMJ</em>, <em>66</em>(1), 2200324.
(<a href="https://doi.org/10.1002/bimj.202200324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common practice in clinical trials is to evaluate a treatment effect on an intermediate outcome when the true outcome of interest would be difficult or costly to measure. We consider how to validate intermediate outcomes in a causally-valid way when the trial outcomes are time-to-event. Using counterfactual outcomes, those that would be observed if the counterfactual treatment had been given, the causal association paradigm assesses the relationship of the treatment effect on the surrogate outcome with the treatment effect on the true, primary outcome. In particular, we propose illness-death models to accommodate the censored and semicompeting risk structure of survival data. The proposed causal version of these models involves estimable and counterfactual frailty terms. Via these multistate models, we characterize what a valid surrogate would look like using a causal effect predictiveness plot. We evaluate the estimation properties of a Bayesian method using Markov chain Monte Carlo and assess the sensitivity of our model assumptions. Our motivating data source is a localized prostate cancer clinical trial where the two survival outcomes are time to distant metastasis and time to death.},
  archive      = {J_BIMJ},
  author       = {Emily K. Roberts and Michael R. Elliott and Jeremy M. G. Taylor},
  doi          = {10.1002/bimj.202200324},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200324},
  shortjournal = {Bio. J.},
  title        = {Surrogacy validation for time-to-event outcomes with illness-death frailty models},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust incorporation of historical information with known
type i error rate inflation. <em>BIMJ</em>, <em>66</em>(1), 2200322. (<a
href="https://doi.org/10.1002/bimj.202200322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian clinical trials can benefit from available historical information through the specification of informative prior distributions. Concerns are however often raised about the potential for prior-data conflict and the impact of Bayes test decisions on frequentist operating characteristics, with particular attention being assigned to inflation of type I error (TIE) rates. This motivates the development of principled borrowing mechanisms, that strike a balance between frequentist and Bayesian decisions. Ideally, the trust assigned to historical information defines the degree of robustness to prior-data conflict one is willing to sacrifice. However, such relationship is often not directly available when explicitly considering inflation of TIE rates. We build on available literature relating frequentist and Bayesian test decisions, and investigate a rationale for inflation of TIE rate which explicitly and linearly relates the amount of borrowing and the amount of TIE rate inflation in one-arm studies. A novel dynamic borrowing mechanism tailored to hypothesis testing is additionally proposed. We show that, while dynamic borrowing prevents the possibility to obtain a simple closed-form TIE rate computation, an explicit upper bound can still be enforced. Connections with the robust mixture prior approach, particularly in relation to the choice of the mixture weight and robust component, are made. Simulations are performed to show the properties of the approach for normal and binomial outcomes, and an exemplary application is demonstrated in a case study.},
  archive      = {J_BIMJ},
  author       = {Silvia Calderazzo and Manuel Wiesenfarth and Annette Kopp-Schneider},
  doi          = {10.1002/bimj.202200322},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200322},
  shortjournal = {Bio. J.},
  title        = {Robust incorporation of historical information with known type i error rate inflation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularized parametric survival modeling to improve risk
prediction models. <em>BIMJ</em>, <em>66</em>(1), 2200319. (<a
href="https://doi.org/10.1002/bimj.202200319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to combine the benefits of flexible parametric survival modeling and regularization to improve risk prediction modeling in the context of time-to-event data. Thereto, we introduce ridge, lasso, elastic net, and group lasso penalties for both log hazard and log cumulative hazard models. The log (cumulative) hazard in these models is represented by a flexible function of time that may depend on the covariates (i.e., covariate effects may be time-varying). We show that the optimization problem for the proposed models can be formulated as a convex optimization problem and provide a user-friendly R implementation for model fitting and penalty parameter selection based on cross-validation. Simulation study results show the advantage of regularization in terms of increased out-of-sample prediction accuracy and improved calibration and discrimination of predicted survival probabilities, especially when sample size was relatively small with respect to model complexity. An applied example illustrates the proposed methods. In summary, our work provides both a foundation for and an easily accessible implementation of regularized parametric survival modeling and suggests that it improves out-of-sample prediction performance.},
  archive      = {J_BIMJ},
  author       = {J. Hoogland and T. P. A. Debray and M. J. Crowther and R. D. Riley and J. IntHout and J. B. Reitsma and A. H. Zwinderman},
  doi          = {10.1002/bimj.202200319},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200319},
  shortjournal = {Bio. J.},
  title        = {Regularized parametric survival modeling to improve risk prediction models},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoupling power and type i error rate considerations when
incorporating historical control data using a test-then-pool approach.
<em>BIMJ</em>, <em>66</em>(1), 2200312. (<a
href="https://doi.org/10.1002/bimj.202200312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accelerate a randomized controlled trial, historical control data may be used after ensuring little heterogeneity between the historical and current trials. The test-then-pool approach is a simple frequentist borrowing method that assesses the similarity between historical and current control data using a two-sided test. A limitation of the conventional test-then-pool method is the inability to control the type I error rate and power for the primary hypothesis separately and flexibly for heterogeneity between trials. This is because the two-sided test focuses on the absolute value of the mean difference between the historical and current controls. In this paper, we propose a new test-then-pool method that splits the two-sided hypothesis of the conventional method into two one-sided hypotheses. Testing each one-sided hypothesis with different significance levels allows for the separate control of the type I error rate and power for heterogeneity between trials. We also propose a significance-level selection approach based on the maximum type I error rate and the minimum power. The proposed method prevented a decrease in power even when there was heterogeneity between trials while controlling type I error at a maximum tolerable type I error rate larger than the targeted type I error rate. The application of depression trial data and hypothetical trial data further supported the usefulness of the proposed method.},
  archive      = {J_BIMJ},
  author       = {Kazufumi Okada and Shiro Tanaka and Jun Matsubayashi and Keita Takahashi and Isao Yokota},
  doi          = {10.1002/bimj.202200312},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200312},
  shortjournal = {Bio. J.},
  title        = {Decoupling power and type i error rate considerations when incorporating historical control data using a test-then-pool approach},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing multicenter individually randomized group
treatment trials. <em>BIMJ</em>, <em>66</em>(1), 2200307. (<a
href="https://doi.org/10.1002/bimj.202200307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an individually randomized group treatment (IRGT) trial, participant outcomes can be positively correlated due to, for example, shared therapists in treatment delivery. Oftentimes, because of limited treatment resources or participants at one location, an IRGT trial can be carried out across multiple centers. This design can be subject to potential correlations in the participant outcomes between arms within the same center. While the design of a single-center IRGT trial has been studied, little is known about the planning of a multicenter IRGT trial. To address this gap, this paper provides analytical sample size formulas for designing multicenter IRGT trials with a continuous endpoint under the linear mixed model framework. We found that accounting for the additional center-level correlation at the design stage can lead to sample size reduction, and the magnitude of reduction depends on the amount of between-therapist correlation. However, if the variance components of therapist-level random effects are considered as input parameters in the design stage, accounting for the additional center-level variance component has no impact on the sample size estimation. We presented our findings through numeric illustrations and performed simulation studies to validate our sample size procedures under different scenarios. Optimal design configurations under the multicenter IRGT trials have also been discussed, and two real-world trial examples are drawn to illustrate the use of our method.},
  archive      = {J_BIMJ},
  author       = {Guangyu Tong and Jiaqi Tong and Fan Li},
  doi          = {10.1002/bimj.202200307},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200307},
  shortjournal = {Bio. J.},
  title        = {Designing multicenter individually randomized group treatment trials},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparison of strategies for selecting auxiliary variables
for multiple imputation. <em>BIMJ</em>, <em>66</em>(1), 2200291. (<a
href="https://doi.org/10.1002/bimj.202200291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation (MI) is a popular method for handling missing data. Auxiliary variables can be added to the imputation model(s) to improve MI estimates. However, the choice of which auxiliary variables to include is not always straightforward. Several data-driven auxiliary variable selection strategies have been proposed, but there has been limited evaluation of their performance. Using a simulation study we evaluated the performance of eight auxiliary variable selection strategies: (1, 2) two versions of selection based on correlations in the observed data; (3) selection using hypothesis tests of the “missing completely at random” assumption; (4) replacing auxiliary variables with their principal components; (5, 6) forward and forward stepwise selection; (7) forward selection based on the estimated fraction of missing information; and (8) selection via the least absolute shrinkage and selection operator (LASSO). A complete case analysis and an MI analysis using all auxiliary variables (the “full model”) were included for comparison. We also applied all strategies to a motivating case study. The full model outperformed all auxiliary variable selection strategies in the simulation study, with the LASSO strategy the best performing auxiliary variable selection strategy overall. All MI analysis strategies that we were able to apply to the case study led to similar estimates, although computational time was substantially reduced when variable selection was employed. This study provides further support for adopting an inclusive auxiliary variable strategy where possible. Auxiliary variable selection using the LASSO may be a promising alternative when the full model fails or is too burdensome.},
  archive      = {J_BIMJ},
  author       = {Rheanna M. Mainzer and Cattram D. Nguyen and John B. Carlin and Margarita Moreno-Betancur and Ian R. White and Katherine J. Lee},
  doi          = {10.1002/bimj.202200291},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200291},
  shortjournal = {Bio. J.},
  title        = {A comparison of strategies for selecting auxiliary variables for multiple imputation},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of odds ratio from group testing data with
misclassified exposure. <em>BIMJ</em>, <em>66</em>(1), 2200254. (<a
href="https://doi.org/10.1002/bimj.202200254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For low prevalence disease, we consider estimation of the odds ratio for two specified groups of individuals using group testing data. Broadly the two groups may be classified as “the exposed” and “the unexposed.” Often in observational studies, the exposure status is not correctly recorded. In addition, diagnostic tests are rarely completely accurate. The proposed model accounts for imperfect sensitivity and specificity of diagnostic tests along with the misclassification in the exposure status. For model identifiability, we make use of internal validation data, where a subsample of reasonably small size is selected from the original sample by simple random sampling without replacement. Pseudo-maximum likelihood method is employed for the estimation of the model parameters. The performance of group testing methodology is compared with individual testing for different parametric configurations. A limited data study related to COVID-19 prevalence is performed to illustrate the methodology.},
  archive      = {J_BIMJ},
  author       = {Surupa Roy and Sumanta Adhya and Subrata Rana},
  doi          = {10.1002/bimj.202200254},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200254},
  shortjournal = {Bio. J.},
  title        = {Estimation of odds ratio from group testing data with misclassified exposure},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining the optimistic performance evaluation of newly
proposed methods: A cross-design validation experiment. <em>BIMJ</em>,
<em>66</em>(1), 2200238. (<a
href="https://doi.org/10.1002/bimj.202200238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The constant development of new data analysis methods in many fields of research is accompanied by an increasing awareness that these new methods often perform better in their introductory paper than in subsequent comparison studies conducted by other researchers. We attempt to explain this discrepancy by conducting a systematic experiment that we call “cross-design validation of methods”. In the experiment, we select two methods designed for the same data analysis task, reproduce the results shown in each paper, and then reevaluate each method based on the study design (i.e., datasets, competing methods, and evaluation criteria) that was used to show the abilities of the other method. We conduct the experiment for two data analysis tasks, namely cancer subtyping using multiomic data and differential gene expression analysis. Three of the four methods included in the experiment indeed perform worse when they are evaluated on the new study design, which is mainly caused by the different datasets. Apart from illustrating the many degrees of freedom existing in the assessment of a method and their effect on its performance, our experiment suggests that the performance discrepancies between original and subsequent papers may not only be caused by the nonneutrality of the authors proposing the new method but also by differences regarding the level of expertise and field of application. Authors of new methods should thus focus not only on a transparent and extensive evaluation but also on comprehensive method documentation that enables the correct use of their methods in subsequent studies.},
  archive      = {J_BIMJ},
  author       = {Christina Nießl and Sabine Hoffmann and Theresa Ullmann and Anne-Laure Boulesteix},
  doi          = {10.1002/bimj.202200238},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200238},
  shortjournal = {Bio. J.},
  title        = {Explaining the optimistic performance evaluation of newly proposed methods: A cross-design validation experiment},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neutralise: An open science initiative for neutral
comparison of two-sample tests. <em>BIMJ</em>, <em>66</em>(1), 2200237.
(<a href="https://doi.org/10.1002/bimj.202200237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-sample problem is one of the earliest problems in statistics: given two samples, the question is whether or not the observations were sampled from the same distribution. Many statistical tests have been developed for this problem, and many tests have been evaluated in simulation studies, but hardly any study has tried to set up a neutral comparison study. In this paper, we introduce an open science initiative that potentially allows for neutral comparisons of two-sample tests. It is designed as an open-source R package, a repository, and an online R Shiny app. This paper describes the principles, the design of the system and illustrates the use of the system.},
  archive      = {J_BIMJ},
  author       = {Leyla Kodalci and Olivier Thas},
  doi          = {10.1002/bimj.202200237},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200237},
  shortjournal = {Bio. J.},
  title        = {Neutralise: An open science initiative for neutral comparison of two-sample tests},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A neutral comparison of statistical methods for analyzing
longitudinally measured ordinal outcomes in rare diseases.
<em>BIMJ</em>, <em>66</em>(1), 2200236. (<a
href="https://doi.org/10.1002/bimj.202200236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal data in a repeated measures design of a crossover study for rare diseases usually do not allow for the use of standard parametric methods, and hence, nonparametric methods should be considered instead. However, only limited simulation studies in settings with small sample sizes exist. Therefore, starting from an Epidermolysis Bullosa simplex trial with the above-mentioned design, a rank-based approach using the R package nparLD and different generalized pairwise comparisons (GPC) methods were compared impartially in a simulation study. The results revealed that there was not one single best method for this particular design, because a trade-off exists between achieving high power, accounting for period effects, and for missing data. Specifically, nparLD as well as the unmatched GPC approaches do not address crossover aspects, and the univariate GPC variants partly ignore the longitudinal information. The matched GPC approaches, on the other hand, take the crossover effect into account in the sense of incorporating the within-subject association. Overall, the prioritized unmatched GPC method achieved the highest power in the simulation scenarios, although this may be due to the specified prioritization. The rank-based approach yielded good power even at a sample size of , whereas the matched GPC method could not control the type I error.},
  archive      = {J_BIMJ},
  author       = {Martin Geroldinger and Johan Verbeeck and Konstantin E. Thiel and Geert Molenberghs and Arne C. Bathke and Martin Laimer and Georg Zimmermann},
  doi          = {10.1002/bimj.202200236},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200236},
  shortjournal = {Bio. J.},
  title        = {A neutral comparison of statistical methods for analyzing longitudinally measured ordinal outcomes in rare diseases},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phases of methodological research in biostatistics—building
the evidence base for new methods. <em>BIMJ</em>, <em>66</em>(1),
2200222. (<a href="https://doi.org/10.1002/bimj.202200222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although new biostatistical methods are published at a very high rate, many of these developments are not trustworthy enough to be adopted by the scientific community. We propose a framework to think about how a piece of methodological work contributes to the evidence base for a method. Similar to the well-known phases of clinical research in drug development, we propose to define four phases of methodological research. These four phases cover (I) proposing a new methodological idea while providing, for example, logical reasoning or proofs, (II) providing empirical evidence, first in a narrow target setting, then (III) in an extended range of settings and for various outcomes, accompanied by appropriate application examples, and (IV) investigations that establish a method as sufficiently well-understood to know when it is preferred over others and when it is not; that is, its pitfalls. We suggest basic definitions of the four phases to provoke thought and discussion rather than devising an unambiguous classification of studies into phases. Too many methodological developments finish before phase III/IV, but we give two examples with references. Our concept rebalances the emphasis to studies in phases III and IV, that is, carefully planned method comparison studies and studies that explore the empirical properties of existing methods in a wider range of problems.},
  archive      = {J_BIMJ},
  author       = {Georg Heinze and Anne-Laure Boulesteix and Michael Kammer and Tim P. Morris and Ian R. White},
  doi          = {10.1002/bimj.202200222},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200222},
  shortjournal = {Bio. J.},
  title        = {Phases of methodological research in biostatistics—Building the evidence base for new methods},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the role of benchmarking data sets and simulations in
method comparison studies. <em>BIMJ</em>, <em>66</em>(1), 2200212. (<a
href="https://doi.org/10.1002/bimj.202200212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Method comparisons are essential to provide recommendations and guidance for applied researchers, who often have to choose from a plethora of available approaches. While many comparisons exist in the literature, these are often not neutral but favor a novel method. Apart from the choice of design and a proper reporting of the findings, there are different approaches concerning the underlying data for such method comparison studies. Most manuscripts on statistical methodology rely on simulation studies and provide a single real-world data set as an example to motivate and illustrate the methodology investigated. In the context of supervised learning, in contrast, methods are often evaluated using so-called benchmarking data sets, that is, real-world data that serve as gold standard in the community. Simulation studies, on the other hand, are much less common in this context. The aim of this paper is to investigate differences and similarities between these approaches, to discuss their advantages and disadvantages, and ultimately to develop new approaches to the evaluation of methods picking the best of both worlds. To this aim, we borrow ideas from different contexts such as mixed methods research and Clinical Scenario Evaluation.},
  archive      = {J_BIMJ},
  author       = {Sarah Friedrich and Tim Friede},
  doi          = {10.1002/bimj.202200212},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200212},
  shortjournal = {Bio. J.},
  title        = {On the role of benchmarking data sets and simulations in method comparison studies},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection in linear regression models: Choosing the
best subset is not always the best choice. <em>BIMJ</em>,
<em>66</em>(1), 2200209. (<a
href="https://doi.org/10.1002/bimj.202200209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the question of variable selection in linear regressions, in the sense of identifying the correct direct predictors (those variables that have nonzero coefficients given all candidate predictors). Best subset selection (BSS) is often considered the “gold standard,” with its use being restricted only by its NP-hard nature. Alternatives such as the least absolute shrinkage and selection operator (Lasso) or the Elastic net (Enet) have become methods of choice in high-dimensional settings. A recent proposal represents BSS as a mixed-integer optimization problem so that large problems have become computationally feasible. We present an extensive neutral comparison assessing the ability to select the correct direct predictors of BSS compared to forward stepwise selection (FSS), Lasso, and Enet. The simulation considers a range of settings that are challenging regarding dimensionality (number of observations and variables), signal-to-noise ratios, and correlations between predictors. As fair measure of performance, we primarily used the best possible F1-score for each method, and results were confirmed by alternative performance measures and practical criteria for choosing the tuning parameters and subset sizes. Surprisingly, it was only in settings where the signal-to-noise ratio was high and the variables were uncorrelated that BSS reliably outperformed the other methods, even in low-dimensional settings. Furthermore, FSS performed almost identically to BSS. Our results shed new light on the usual presumption of BSS being, in principle, the best choice for selecting the correct direct predictors. Especially for correlated variables, alternatives like Enet are faster and appear to perform better in practical settings.},
  archive      = {J_BIMJ},
  author       = {Moritz Hanke and Louis Dijkstra and Ronja Foraita and Vanessa Didelez},
  doi          = {10.1002/bimj.202200209},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200209},
  shortjournal = {Bio. J.},
  title        = {Variable selection in linear regression models: Choosing the best subset is not always the best choice},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A neutral comparison of algorithms to minimize l0 penalties
for high-dimensional variable selection. <em>BIMJ</em>, <em>66</em>(1),
2200207. (<a href="https://doi.org/10.1002/bimj.202200207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection methods based on L 0 penalties have excellent theoretical properties to select sparse models in a high-dimensional setting. There exist modifications of the Bayesian Information Criterion (BIC) which either control the familywise error rate (mBIC) or the false discovery rate (mBIC2) in terms of which regressors are selected to enter a model. However, the minimization of L 0 penalties comprises a mixed-integer problem which is known to be NP-hard and therefore becomes computationally challenging with increasing numbers of regressor variables. This is one reason why alternatives like the LASSO have become so popular, which involve convex optimization problems that are easier to solve. The last few years have seen some real progress in developing new algorithms to minimize L 0 penalties. The aim of this article is to compare the performance of these algorithms in terms of minimizing L 0 -based selection criteria. Simulation studies covering a wide range of scenarios that are inspired by genetic association studies are used to compare the values of selection criteria obtained with different algorithms. In addition, some statistical characteristics of the selected models and the runtime of algorithms are compared. Finally, the performance of the algorithms is illustrated in a real data example concerned with expression quantitative trait loci (eQTL) mapping.},
  archive      = {J_BIMJ},
  author       = {Florian Frommlet},
  doi          = {10.1002/bimj.202200207},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200207},
  shortjournal = {Bio. J.},
  title        = {A neutral comparison of algorithms to minimize l0 penalties for high-dimensional variable selection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian dose escalation with overdose and underdose control
utilizing all toxicities in phase i/II clinical trials. <em>BIMJ</em>,
<em>66</em>(1), 2200189. (<a
href="https://doi.org/10.1002/bimj.202200189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Escalation with overdose control (EWOC) is a commonly used Bayesian adaptive design, which controls overdosing risk while estimating maximum tolerated dose (MTD) in cancer Phase I clinical trials. In 2010, Chen and his colleagues proposed a novel toxicity scoring system to fully utilize patients’ toxicity information by using a normalized equivalent toxicity score (NETS) in the range 0 to 1 instead of a binary indicator of dose limiting toxicity (DLT). Later in 2015, by adding underdosing control into EWOC, escalation with overdose and underdose control (EWOUC) design was proposed to guarantee patients the minimum therapeutic effect of drug in Phase I/II clinical trials. In this paper, the EWOUC-NETS design is developed by integrating the advantages of EWOUC and NETS in a Bayesian context. Moreover, both toxicity response and efficacy are treated as continuous variables to maximize trial efficiency. The dose escalation decision is based on the posterior distribution of both toxicity and efficacy outcomes, which are recursively updated with accumulated data. We compare the operation characteristics of EWOUC-NETS and existing methods through simulation studies under five scenarios. The study results show that EWOUC-NETS design treating toxicity and efficacy outcomes as continuous variables can increase accuracy in identifying the optimized utility dose (OUD) and provide better therapeutic effects.},
  archive      = {J_BIMJ},
  author       = {Jieqi Tu and Zhengjia Chen},
  doi          = {10.1002/bimj.202200189},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200189},
  shortjournal = {Bio. J.},
  title        = {Bayesian dose escalation with overdose and underdose control utilizing all toxicities in phase I/II clinical trials},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new method for clustered survival data: Estimation of
treatment effect heterogeneity and variable selection. <em>BIMJ</em>,
<em>66</em>(1), 2200178. (<a
href="https://doi.org/10.1002/bimj.202200178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We recently developed a new method random-intercept accelerated failure time model with Bayesian additive regression trees (riAFT-BART) to draw causal inferences about population treatment effect on patient survival from clustered and censored survival data while accounting for the multilevel data structure. The practical utility of this method goes beyond the estimation of population average treatment effect. In this work, we exposit how riAFT-BART can be used to solve two important statistical questions with clustered survival data: estimating the treatment effect heterogeneity and variable selection. Leveraging the likelihood-based machine learning, we describe a way in which we can draw posterior samples of the individual survival treatment effect from riAFT-BART model runs, and use the drawn posterior samples to perform an exploratory treatment effect heterogeneity analysis to identify subpopulations who may experience differential treatment effects than population average effects. There is sparse literature on methods for variable selection among clustered and censored survival data, particularly ones using flexible modeling techniques. We propose a permutation-based approach using the predictor&#39;s variable inclusion proportion supplied by the riAFT-BART model for variable selection. To address the missing data issue frequently encountered in health databases, we propose a strategy to combine bootstrap imputation and riAFT-BART for variable selection among incomplete clustered survival data. We conduct an expansive simulation study to examine the practical operating characteristics of our proposed methods, and provide empirical evidence that our proposed methods perform better than several existing methods across a wide range of data scenarios. Finally, we demonstrate the methods via a case study of predictors for in-hospital mortality among severe COVID-19 patients and estimating the heterogeneous treatment effects of three COVID-specific medications. The methods developed in this work are readily available in the package .},
  archive      = {J_BIMJ},
  author       = {Liangyuan Hu},
  doi          = {10.1002/bimj.202200178},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200178},
  shortjournal = {Bio. J.},
  title        = {A new method for clustered survival data: Estimation of treatment effect heterogeneity and variable selection},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A platform for comparing subgroup identification
methodologies. <em>BIMJ</em>, <em>66</em>(1), 2200164. (<a
href="https://doi.org/10.1002/bimj.202200164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the advent of the phrase “subgroup identification,” there has been an explosion of methodologies that seek to identify meaningful subgroups of patients with exceptional response in order to further the realization of personalized medicine. However, to perform fair comparison and understand what methods work best under different clinical trials situations, a common platform is needed for comparative effectiveness of these various approaches. In this paper, we describe a comprehensive project that created an extensive platform for evaluating subgroup identification methods as well as a publicly posted challenge that was used to elicit new approaches. We proposed a common data-generating model for creating virtual clinical trial datasets that contain subgroups of exceptional responders encompassing the many dimensions of the problem or null scenarios in which there are no such subgroups. Furthermore, we created a common scoring system for evaluating performance of purported methods for identifying subgroups. This makes it possible to benchmark methodologies in order to understand what methods work best under different clinical trial situations. The findings from this project produced considerable insights and allow us to make recommendations for how the statistical community can better compare and contrast old and new subgroup identification methodologies.},
  archive      = {J_BIMJ},
  author       = {Stephen Ruberg and Ying Zhang and Hollins Showalter and Lei Shen},
  doi          = {10.1002/bimj.202200164},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200164},
  shortjournal = {Bio. J.},
  title        = {A platform for comparing subgroup identification methodologies},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging baseline covariates to analyze small
cluster-randomized trials with a rare binary outcome. <em>BIMJ</em>,
<em>66</em>(1), 2200135. (<a
href="https://doi.org/10.1002/bimj.202200135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster-randomized trials (CRTs) involve randomizing entire groups of participants—called clusters—to treatment arms but are often comprised of a limited or fixed number of available clusters. While covariate adjustment can account for chance imbalances between treatment arms and increase statistical efficiency in individually randomized trials, analytical methods for individual-level covariate adjustment in small CRTs have received little attention to date. In this paper, we systematically investigate, through extensive simulations, the operating characteristics of propensity score weighting and multivariable regression as two individual-level covariate adjustment strategies for estimating the participant-average causal effect in small CRTs with a rare binary outcome and identify scenarios where each adjustment strategy has a relative efficiency advantage over the other to make practical recommendations. We also examine the finite-sample performance of the bias-corrected sandwich variance estimators associated with propensity score weighting and multivariable regression for quantifying the uncertainty in estimating the participant-average treatment effect. To illustrate the methods for individual-level covariate adjustment, we reanalyze a recent CRT testing a sedation protocol in 31 pediatric intensive care units.},
  archive      = {J_BIMJ},
  author       = {Angela Y. Zhu and Nandita Mitra and Karla Hemming and Michael O. Harhay and Fan Li},
  doi          = {10.1002/bimj.202200135},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200135},
  shortjournal = {Bio. J.},
  title        = {Leveraging baseline covariates to analyze small cluster-randomized trials with a rare binary outcome},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of likelihood penalization and variance
decomposition approaches for clinical prediction models: A simulation
study. <em>BIMJ</em>, <em>66</em>(1), 2200108. (<a
href="https://doi.org/10.1002/bimj.202200108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logistic regression is one of the most commonly used approaches to develop clinical risk prediction models. Developers of such models often rely on approaches that aim to minimize the risk of overfitting and improve predictive performance of the logistic model, such as through likelihood penalization and variance decomposition techniques. We present an extensive simulation study that compares the out-of-sample predictive performance of risk prediction models derived using the elastic net, with Lasso and ridge as special cases, and variance decomposition techniques, namely, incomplete principal component regression and incomplete partial least squares regression. We varied the expected events per variable, event fraction, number of candidate predictors, presence of noise predictors, and the presence of sparse predictors in a full-factorial design. Predictive performance was compared on measures of discrimination, calibration, and prediction error. Simulation metamodels were derived to explain the performance differences within model derivation approaches. Our results indicate that, on average, prediction models developed using penalization and variance decomposition approaches outperform models developed using ordinary maximum likelihood estimation, with penalization approaches being consistently superior over the variance decomposition approaches. Differences in performance were most pronounced on the calibration of the model. Performance differences regarding prediction error and concordance statistic outcomes were often small between approaches. The use of likelihood penalization and variance decomposition techniques methods was illustrated in the context of peripheral arterial disease.},
  archive      = {J_BIMJ},
  author       = {Anna Lohmann and Rolf H. H. Groenwold and Maarten van Smeden},
  doi          = {10.1002/bimj.202200108},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200108},
  shortjournal = {Bio. J.},
  title        = {Comparison of likelihood penalization and variance decomposition approaches for clinical prediction models: A simulation study},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward a standardized evaluation of imputation methodology.
<em>BIMJ</em>, <em>66</em>(1), 2200107. (<a
href="https://doi.org/10.1002/bimj.202200107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing new imputation methodology has become a very active field. Unfortunately, there is no consensus on how to perform simulation studies to evaluate the properties of imputation methods. In part, this may be due to different aims between fields and studies. For example, when evaluating imputation techniques aimed at prediction, different aims may be formulated than when statistical inference is of interest. The lack of consensus may also stem from different personal preferences or scientific backgrounds. All in all, the lack of common ground in evaluating imputation methodology may lead to suboptimal use in practice. In this paper, we propose a move toward a standardized evaluation of imputation methodology. To demonstrate the need for standardization, we highlight a set of possible pitfalls that bring forth a chain of potential problems in the objective assessment of the performance of imputation routines. Additionally, we suggest a course of action for simulating and evaluating missing data problems. Our suggested course of action is by no means meant to serve as a complete cookbook, but rather meant to incite critical thinking and a move to objective and fair evaluations of imputation methodology. We invite the readers of this paper to contribute to the suggested course of action.},
  archive      = {J_BIMJ},
  author       = {Hanne I. Oberman and Gerko Vink},
  doi          = {10.1002/bimj.202200107},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200107},
  shortjournal = {Bio. J.},
  title        = {Toward a standardized evaluation of imputation methodology},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Against the “one method fits all data sets” philosophy for
comparison studies in methodological research. <em>BIMJ</em>,
<em>66</em>(1), 2200104. (<a
href="https://doi.org/10.1002/bimj.202200104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many methodological comparison studies aim at identifying a single or a few “best performing” methods over a certain range of data sets. In this paper we take a different viewpoint by asking whether the research question of identifying the best performing method is what we should be striving for in the first place. We will argue that this research question implies assumptions which we do not consider warranted in methodological research, that a different research question would be more informative, and how this research question can be fruitfully investigated.},
  archive      = {J_BIMJ},
  author       = {Carolin Strobl and Friedrich Leisch},
  doi          = {10.1002/bimj.202200104},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200104},
  shortjournal = {Bio. J.},
  title        = {Against the “one method fits all data sets” philosophy for comparison studies in methodological research},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CITIES: Clinical trials with intercurrent events simulator.
<em>BIMJ</em>, <em>66</em>(1), 2200103. (<a
href="https://doi.org/10.1002/bimj.202200103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although clinical trials are often designed with randomization and well-controlled protocols, complications will inevitably arise in the presence of intercurrent events (ICEs) such as treatment discontinuation. These can lead to missing outcome data and possibly confounding causal inference when the missingness is a function of a latent stratification of patients defined by intermediate outcomes. The pharmaceutical industry has been focused on developing new methods that can yield pertinent causal inferences in trials with ICEs. However, it is difficult to compare the properties of different methods developed in this endeavor as real-life clinical trial data cannot be easily shared to provide benchmark data sets. Furthermore, different methods consider distinct assumptions for the underlying data-generating mechanisms, and simulation studies often are customized to specific situations or methods. We develop a novel, general simulation model and corresponding Shiny application in R for clinical trials with ICEs, aptly named the Clinical Trials with Intercurrent Events Simulator (CITIES). It is formulated under the Rubin Causal Model where the considered treatment effects account for ICEs in clinical trials with repeated measures. CITIES facilitates the effective generation of data that resemble real-life clinical trials with respect to their reported summary statistics, without requiring the use of the original trial data. We illustrate the utility of CITIES via two case studies involving real-life clinical trials that demonstrate how CITIES provides a comprehensive tool for practitioners in the pharmaceutical industry to compare methods for the analysis of clinical trials with ICEs on identical, benchmark settings that resemble real-life trials.},
  archive      = {J_BIMJ},
  author       = {Ahmad Hakeem Abdul Wahab and Yongming Qu and Hege Michiels and Junxiang Luo and Run Zhuang and Dominique McDaniel and Dong Xi and Elena Polverejan and Steven Gilbert and Stephen Ruberg and Arman Sabbaghi},
  doi          = {10.1002/bimj.202200103},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200103},
  shortjournal = {Bio. J.},
  title        = {CITIES: Clinical trials with intercurrent events simulator},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relative likelihood ratios for neutral comparisons of
statistical tests in simulation studies. <em>BIMJ</em>, <em>66</em>(1),
2200102. (<a href="https://doi.org/10.1002/bimj.202200102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When comparing the performance of two or more competing tests, simulation studies commonly focus on statistical power. However, if the size of the tests being compared are either different from one another or from the nominal size, comparing tests based on power alone may be misleading. By analogy with diagnostic accuracy studies, we introduce relative positive and negative likelihood ratios to factor in both power and size in the comparison of multiple tests. We derive sample size formulas for a comparative simulation study. As an example, we compared the performance of six statistical tests for small-study effects in meta-analyses of randomized controlled trials: Begg&#39;s rank correlation, Egger&#39;s regression, Schwarzer&#39;s method for sparse data, the trim-and-fill method, the arcsine-Thompson test, and Lin and Chu&#39;s combined test. We illustrate that comparing power alone, or power adjusted or penalized for size, can be misleading, and how the proposed likelihood ratio approach enables accurate comparison of the trade-off between power and size between competing tests.},
  archive      = {J_BIMJ},
  author       = {Qiuxi Huang and Ludovic Trinquart},
  doi          = {10.1002/bimj.202200102},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200102},
  shortjournal = {Bio. J.},
  title        = {Relative likelihood ratios for neutral comparisons of statistical tests in simulation studies},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing linear discriminant analysis and supervised
learning algorithms for binary classification—a method comparison study.
<em>BIMJ</em>, <em>66</em>(1), 2200098. (<a
href="https://doi.org/10.1002/bimj.202200098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In psychology, linear discriminant analysis (LDA) is the method of choice for two-group classification tasks based on questionnaire data. In this study, we present a comparison of LDA with several supervised learning algorithms. In particular, we examine to what extent the predictive performance of LDA relies on the multivariate normality assumption. As nonparametric alternatives, the linear support vector machine (SVM), classification and regression tree (CART), random forest (RF), probabilistic neural network (PNN), and the ensemble k conditional nearest neighbor (E k CNN) algorithms are applied. Predictive performance is determined using measures of overall performance, discrimination, and calibration, and is compared in two reference data sets as well as in a simulation study. The reference data are Likert-type data, and comprise 5 and 10 predictor variables, respectively. Simulations are based on the reference data and are done for a balanced and an unbalanced scenario in each case. In order to compare the algorithms&#39; performance, data are simulated from multivariate distributions with differing degrees of nonnormality. Results differ depending on the specific performance measure. The main finding is that LDA is always outperformed by RF in the bimodal data with respect to overall performance. Discriminative ability of the RF algorithm is often higher compared to LDA, but its model calibration is usually worse. Still LDA mostly ranges second in cases it is outperformed by another algorithm, or the differences are only marginal. In consequence, we still recommend LDA for this type of application.},
  archive      = {J_BIMJ},
  author       = {Ricarda Graf and Marina Zeldovich and Sarah Friedrich},
  doi          = {10.1002/bimj.202200098},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200098},
  shortjournal = {Bio. J.},
  title        = {Comparing linear discriminant analysis and supervised learning algorithms for binary classification—A method comparison study},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The bayesian simulation study (BASIS) framework for
simulation studies in statistical and methodological research.
<em>BIMJ</em>, <em>66</em>(1), 2200095. (<a
href="https://doi.org/10.1002/bimj.202200095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical simulation studies are becoming increasingly popular to demonstrate the performance or superiority of new computational procedures and algorithms. Despite this status quo, previous surveys of the literature have shown that the reporting of statistical simulation studies often lacks relevant information and structure. The latter applies in particular to Bayesian simulation studies, and in this paper the Bayesian simulation study framework (BASIS) is presented as a step towards improving the situation. The BASIS framework provides a structured skeleton for planning, coding, executing, analyzing, and reporting Bayesian simulation studies in biometrical research and computational statistics. It encompasses various features of previous proposals and recommendations in the methodological literature and aims to promote neutral comparison studies in statistical research. Computational aspects covered in the BASIS include algorithmic choices, Markov–chain-Monte-Carlo convergence diagnostics, sensitivity analyses, and Monte Carlo standard error calculations for Bayesian simulation studies. Although the BASIS framework focuses primarily on methodological research, it also provides useful guidance for researchers who rely on the results of Bayesian simulation studies or analyses, as current state-of-the-art guidelines for Bayesian analyses are incorporated into the BASIS.},
  archive      = {J_BIMJ},
  author       = {Riko Kelter},
  doi          = {10.1002/bimj.202200095},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200095},
  shortjournal = {Bio. J.},
  title        = {The bayesian simulation study (BASIS) framework for simulation studies in statistical and methodological research},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparative study of in vitro dose–response estimation
under extreme observations. <em>BIMJ</em>, <em>66</em>(1), 2200092. (<a
href="https://doi.org/10.1002/bimj.202200092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying drug potency, which requires an accurate estimation of dose–response relationship, is essential for drug development in biomedical research and life sciences. However, the standard estimation procedure of the median–effect equation to describe the dose–response curve is vulnerable to extreme observations in common experimental data. To facilitate appropriate statistical inference, many powerful estimation tools have been developed in R , including various dose–response packages based on the nonlinear least squares method with different optimization strategies. Recently, beta regression-based methods have also been introduced in estimation of the median–effect equation. In theory, they can overcome nonnormality, heteroscedasticity, and asymmetry and accommodate flexible robust frameworks and coefficients penalization. To identify a reliable estimation method(s) to estimate dose–response curves even with extreme observations, we conducted a comparative study to review 14 different tools in R and examine their robustness and efficiency via Monte Carlo simulation under a list of comprehensive scenarios. The simulation results demonstrate that penalized beta regression using the mgcv package outperforms other methods in terms of stable, accurate estimation, and reliable uncertainty quantification.},
  archive      = {J_BIMJ},
  author       = {Xinying Fang and Shouhao Zhou},
  doi          = {10.1002/bimj.202200092},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200092},
  shortjournal = {Bio. J.},
  title        = {A comparative study of in vitro dose–response estimation under extreme observations},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pitfalls and potentials in simulation studies: Questionable
research practices in comparative simulation studies allow for spurious
claims of superiority of any method. <em>BIMJ</em>, <em>66</em>(1),
2200091. (<a href="https://doi.org/10.1002/bimj.202200091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative simulation studies are workhorse tools for benchmarking statistical methods. As with other empirical studies, the success of simulation studies hinges on the quality of their design, execution, and reporting. If not conducted carefully and transparently, their conclusions may be misleading. In this paper, we discuss various questionable research practices, which may impact the validity of simulation studies, some of which cannot be detected or prevented by the current publication process in statistics journals. To illustrate our point, we invent a novel prediction method with no expected performance gain and benchmark it in a preregistered comparative simulation study. We show how easy it is to make the method appear superior over well-established competitor methods if questionable research practices are employed. Finally, we provide concrete suggestions for researchers, reviewers, and other academic stakeholders for improving the methodological quality of comparative simulation studies, such as preregistering simulation protocols, incentivizing neutral simulation studies, and code and data sharing.},
  archive      = {J_BIMJ},
  author       = {Samuel Pawel and Lucas Kook and Kelly Reeve},
  doi          = {10.1002/bimj.202200091},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200091},
  shortjournal = {Bio. J.},
  title        = {Pitfalls and potentials in simulation studies: Questionable research practices in comparative simulation studies allow for spurious claims of superiority of any method},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing algorithms for characterizing treatment effect
heterogeneity in randomized trials. <em>BIMJ</em>, <em>66</em>(1),
2100337. (<a href="https://doi.org/10.1002/bimj.202100337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification and estimation of heterogeneous treatment effects in biomedical clinical trials are challenging, because trials are typically planned to assess the treatment effect in the overall trial population. Nevertheless, the identification of how the treatment effect may vary across subgroups is of major importance for drug development. In this work, we review some existing simulation work and perform a simulation study to evaluate recent methods for identifying and estimating the heterogeneous treatments effects using various metrics and scenarios relevant for drug development. Our focus is not only on a comparison of the methods in general, but on how well these methods perform in simulation scenarios that reflect real clinical trials. We provide the R package benchtm that can be used to simulate synthetic biomarker distributions based on real clinical trial data and to create interpretable scenarios to benchmark methods for identification and estimation of treatment effect heterogeneity.},
  archive      = {J_BIMJ},
  author       = {Sophie Sun and Konstantinos Sechidis and Yao Chen and Jiarui Lu and Chong Ma and Ardalan Mirshani and David Ohlssen and Marc Vandemeulebroecke and Björn Bornkamp},
  doi          = {10.1002/bimj.202100337},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100337},
  shortjournal = {Bio. J.},
  title        = {Comparing algorithms for characterizing treatment effect heterogeneity in randomized trials},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact of the matching algorithm on the treatment effect
estimate: A neutral comparison study. <em>BIMJ</em>, <em>66</em>(1),
2100292. (<a href="https://doi.org/10.1002/bimj.202100292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity score matching is increasingly being used in the medical literature. Choice of matching algorithms, reporting quality, and estimands are oftentimes not discussed. We evaluated the impact of propensity score matching algorithms, based on a recent clinical dataset, with three commonly used outcomes. The resulting estimands for different strengths of treatment effects were compared in a neutral comparison study and based on a thoroughly designed simulation study. Different algorithms yielded different levels of balance after matching. Along with full matching and genetic matching with replacement, good balance was achieved with nearest neighbor matching with caliper but thereby more than one fifth of the treated units were discarded. Average marginal treatment effect estimates were least biased with genetic or nearest neighbor matching, both with replacement and full matching. Double adjustment yielded conditional treatment effects that were closer to the true values, throughout. The choice of the matching algorithm had an impact on covariate balance after matching as well as treatment effect estimates. In comparison, genetic matching with replacement yielded better covariate balance than all other matching algorithms. A literature review in the British Medical Journal including its subjournals revealed frequent use of propensity score matching; however, the use of different matching algorithms before treatment effect estimation was only reported in one out of 21 studies. Propensity score matching is a methodology for causal treatment effect estimation from observational data; however, the methodological difficulties and low reporting quality in applied medical research need to be addressed.},
  archive      = {J_BIMJ},
  author       = {Priska Heinz and Pedro David Wendel-Garcia and Ulrike Held},
  doi          = {10.1002/bimj.202100292},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100292},
  shortjournal = {Bio. J.},
  title        = {Impact of the matching algorithm on the treatment effect estimate: A neutral comparison study},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparison of full model specification and backward
elimination of potential confounders when estimating marginal and
conditional causal effects on binary outcomes from observational data.
<em>BIMJ</em>, <em>66</em>(1), 2100237. (<a
href="https://doi.org/10.1002/bimj.202100237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common view in epidemiology is that automated confounder selection methods, such as backward elimination, should be avoided as they can lead to biased effect estimates and underestimation of their variance. Nevertheless, backward elimination remains regularly applied. We investigated if and under which conditions causal effect estimation in observational studies can improve by using backward elimination on a prespecified set of potential confounders. An expression was derived that quantifies how variable omission relates to bias and variance of effect estimators. Additionally, 3960 scenarios were defined and investigated by simulations comparing bias and mean squared error (MSE) of the conditional log odds ratio, log(cOR), and the marginal log risk ratio, log(mRR), between full models including all prespecified covariates and backward elimination of these covariates. Applying backward elimination resulted in a mean bias of 0.03 for log(cOR) and 0.02 for log(mRR), compared to 0.56 and 0.52 for log(cOR) and log(mRR), respectively, for a model without any covariate adjustment, and no bias for the full model. In less than 3% of the scenarios considered, the MSE of the log(cOR) or log(mRR) was slightly lower (max 3%) when backward elimination was used compared to the full model. When an initial set of potential confounders can be specified based on background knowledge, there is minimal added value of backward elimination. We advise not to use it and otherwise to provide ample arguments supporting its use.},
  archive      = {J_BIMJ},
  author       = {Kim Luijken and Rolf H. H. Groenwold and Maarten van Smeden and Susanne Strohmaier and Georg Heinze},
  doi          = {10.1002/bimj.202100237},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100237},
  shortjournal = {Bio. J.},
  title        = {A comparison of full model specification and backward elimination of potential confounders when estimating marginal and conditional causal effects on binary outcomes from observational data},
  volume       = {66},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
