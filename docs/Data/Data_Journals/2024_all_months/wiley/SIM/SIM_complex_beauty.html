<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim---349">SIM - 349</h2>
<ul>
<li><details>
<summary>
(2024). Statistical inference for box–cox based receiver operating
characteristic curves. <em>SIM</em>, <em>43</em>(30), 6099–6122. (<a
href="https://doi.org/10.1002/sim.10252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Receiver operating characteristic (ROC) curve analysis is widely used in evaluating the effectiveness of a diagnostic test/biomarker or classifier score. A parametric approach for statistical inference on ROC curves based on a Box–Cox transformation to normality has frequently been discussed in the literature. Many investigators have highlighted the difficulty of taking into account the variability of the estimated transformation parameter when carrying out such an analysis. This variability is often ignored and inferences are made by considering the estimated transformation parameter as fixed and known. In this paper, we will review the literature discussing the use of the Box–Cox transformation for ROC curves and the methodology for accounting for the estimation of the Box–Cox transformation parameter in the context of ROC analysis, and detail its application to a number of problems. We present a general framework for inference on any functional of interest, including common measures such as the AUC, the Youden index, and the sensitivity at a given specificity (and vice versa). We further developed a new R package (named ‘rocbc’ ) that carries out all discussed approaches and is available in CRAN.},
  archive      = {J_SIM},
  author       = {Leonidas E. Bantis and Benjamin Brewer and Christos T. Nakas and Benjamin Reiser},
  doi          = {10.1002/sim.10252},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6099-6122},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for Box–Cox based receiver operating characteristic curves},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Double sampling for informatively missing data in electronic
health record-based comparative effectiveness research. <em>SIM</em>,
<em>43</em>(30), 6086–6098. (<a
href="https://doi.org/10.1002/sim.10298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data arise in most applied settings and are ubiquitous in electronic health records (EHR). When data are missing not at random (MNAR) with respect to measured covariates, sensitivity analyses are often considered. These solutions, however, are often unsatisfying in that they are not guaranteed to yield actionable conclusions. Motivated by an EHR-based study of long-term outcomes following bariatric surgery, we consider the use of double sampling as a means to mitigate MNAR outcome data when the statistical goals are estimation and inference regarding causal effects. We describe assumptions that are sufficient for the identification of the joint distribution of confounders, treatment, and outcome under this design. Additionally, we derive efficient and robust estimators of the average causal treatment effect under a nonparametric model and under a model assuming outcomes were, in fact, initially missing at random (MAR). We compare these in simulations to an approach that adaptively estimates based on evidence of violation of the MAR assumption. Finally, we also show that the proposed double sampling design can be extended to handle arbitrary coarsening mechanisms, and derive nonparametric efficient estimators of any smooth full data functional.},
  archive      = {J_SIM},
  author       = {Alexander W. Levis and Rajarshi Mukherjee and Rui Wang and Heidi Fischer and Sebastien Haneuse},
  doi          = {10.1002/sim.10298},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6086-6098},
  shortjournal = {Stat. Med.},
  title        = {Double sampling for informatively missing data in electronic health record-based comparative effectiveness research},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonparametric regression calibration for the accelerated
failure time model with measurement error. <em>SIM</em>,
<em>43</em>(30), 6073–6085. (<a
href="https://doi.org/10.1002/sim.10299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerated failure time models are appealing due to their intuitive interpretation. However, when covariates are subject to measurement errors, naive estimation becomes severely biased. To address this issue, the regression calibration (RC) approach is a widely applicable and effective method. Traditionally, the RC method requires a good predictor for the true covariate, which can be obtained through parametric distribution assumptions or validation datasets. Consequently, the performance of the estimator depends on the plausibility of these assumptions. In this work, we propose a novel method that utilizes error augmentation to duplicate covariates, facilitating nonparametric estimation. Our approach does not require a validation set or parametric distribution assumptions for the true covariate. Through simulation studies, we demonstrate that our approach is more robust and less impacted by heavy censoring rates compared to conventional analyses. Additionally, an analysis of a subset of a real dataset suggests that the conventional RC method may have a tendency to overcorrect the attenuation effect of measurement error.},
  archive      = {J_SIM},
  author       = {Yih-Huei Huang and Chien-Ying Wu},
  doi          = {10.1002/sim.10299},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6073-6085},
  shortjournal = {Stat. Med.},
  title        = {A nonparametric regression calibration for the accelerated failure time model with measurement error},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modelling of longitudinal measurements and
time-to-event outcomes with a cure fraction using functional principal
component analysis. <em>SIM</em>, <em>43</em>(30), 6059–6072. (<a
href="https://doi.org/10.1002/sim.10289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studying the association between clinical measurements and time-to-event outcomes within a cure model, utilizing repeated observations rather than solely baseline values may lead to more accurate estimation. However, there are two main challenges in this context. First, longitudinal measurements are usually observed at discrete time points and second, for diseases that respond well to treatment, a high censoring proportion may occur by the end of the trial. In this article, we propose a joint modelling approach to simultaneously study the longitudinal observations and time-to-event outcome with an assumed cure fraction. We employ the functional principal components analysis (FPCA) to model the longitudinal data, offering flexibility by not assuming a specific form for the longitudinal curve. We used a Cox&#39;s proportional hazards mixture cure model to study the survival outcome. To investigate the longitudinal binary observations, we adopt a quasi-likelihood method which builds pseudo normal distribution for the binary data and use the E-M algorithm to estimate the parameters. The tuning parameters are selected using the Akaike information criterion. Our proposed method is evaluated through extensive simulation studies and applied to a clinical trial data to study the relationship between the longitudinal prostate specific antigen (PSA) measurements and overall survival in men with metastatic prostate cancer.},
  archive      = {J_SIM},
  author       = {Siyuan Guo and Jiajia Zhang and Susan Halabi},
  doi          = {10.1002/sim.10289},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6059-6072},
  shortjournal = {Stat. Med.},
  title        = {Joint modelling of longitudinal measurements and time-to-event outcomes with a cure fraction using functional principal component analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian decision curve analysis with bayesdca.
<em>SIM</em>, <em>43</em>(30), 6042–6058. (<a
href="https://doi.org/10.1002/sim.10277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical decisions are often guided by clinical prediction models or diagnostic tests. Decision curve analysis (DCA) combines classical assessment of predictive performance with the consequences of using these strategies for clinical decision-making. In DCA, the best decision strategy is the one that maximizes the net benefit: the net number of true positives (or negatives) provided by a given strategy. Here, we employ Bayesian approaches to DCA, addressing four fundamental concerns when evaluating clinical decision strategies: (i) which strategies are clinically useful, (ii) what is the best available decision strategy, (iii) which of two competing strategies is better, and (iv) what is the expected net benefit loss associated with the current level of uncertainty. While often consistent with frequentist point estimates, fully Bayesian DCA allows for an intuitive probabilistic interpretation framework and the incorporation of prior evidence. We evaluate the methods using simulation and provide a comprehensive case study. Software implementation is available in the bayesDCA R package. Ultimately, the Bayesian DCA workflow may help clinicians and health policymakers adopt better-informed decisions.},
  archive      = {J_SIM},
  author       = {Giuliano Netto Flores Cruz and Keegan Korthauer},
  doi          = {10.1002/sim.10277},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6042-6058},
  shortjournal = {Stat. Med.},
  title        = {Bayesian decision curve analysis with bayesdca},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient risk assessment of time-to-event targets with
adaptive information transfer. <em>SIM</em>, <em>43</em>(30), 6026–6041.
(<a href="https://doi.org/10.1002/sim.10290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using informative sources to enhance statistical analysis in target studies has become an increasingly popular research topic. However, cohorts with time-to-event outcomes have not received sufficient attention, and external studies often encounter issues of incomparability due to population heterogeneity and unmeasured risk factors. To improve individualized risk assessments, we propose a novel methodology that adaptively borrows information from multiple incomparable sources. By extracting aggregate statistics through transitional models applied to both the external sources and the target population, we incorporate this information efficiently using the control variate technique. This approach eliminates the need to load individual-level records from sources directly, resulting in low computational complexity and strong privacy protection. Asymptotically, our estimators of both relative and baseline risks are more efficient than traditional results, and the power of covariate effects testing is much enhanced. We demonstrate the practical performance of our method via extensive simulations and a real case study.},
  archive      = {J_SIM},
  author       = {Jie Ding and Jialiang Li and Ping Xie and Xiaoguang Wang},
  doi          = {10.1002/sim.10290},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6026-6041},
  shortjournal = {Stat. Med.},
  title        = {Efficient risk assessment of time-to-event targets with adaptive information transfer},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensitivity analysis for effects of multiple exposures in
the presence of unmeasured confounding: Non-gaussian and time-to-event
outcomes. <em>SIM</em>, <em>43</em>(30), 5996–6025. (<a
href="https://doi.org/10.1002/sim.10293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In epidemiological studies, evaluating the health impacts stemming from multiple exposures is one of the important goals. To analyze the effects of multiple exposures on discrete or time-to-event health outcomes, researchers often employ generalized linear models, Cox proportional hazards models, and machine learning methods. However, observational studies are prone to unmeasured confounding factors, which can introduce the potential for substantial bias in the multiple exposure effects. To address this issue, we propose a novel outcome model-based sensitivity analysis method for non-Gaussian and time-to-event outcomes with multiple exposures. All the proposed sensitivity analysis problems are formulated as linear programming problems with quadratic and linear constraints, which can be solved efficiently. Analytic solutions are provided for some optimization problems, and a numerical study is performed to examine how the proposed sensitivity analysis behaves in finite samples. We illustrate the proposed method using two real data examples.},
  archive      = {J_SIM},
  author       = {Seungjae Lee and Boram Jeong and Donghwan Lee and Woojoo Lee},
  doi          = {10.1002/sim.10293},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5996-6025},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity analysis for effects of multiple exposures in the presence of unmeasured confounding: Non-gaussian and time-to-event outcomes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized estimating equations for survival data with
dependent censoring. <em>SIM</em>, <em>43</em>(30), 5983–5995. (<a
href="https://doi.org/10.1002/sim.10296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Independent censoring is usually assumed in survival data analysis. However, dependent censoring, where the survival time is dependent on the censoring time, is often seen in real data applications. In this project, we model the vector of survival time and censoring time marginally through semiparametric heteroscedastic accelerated failure time models and model their association by the vector of errors in the model. We show that this semiparametric model is identified, and the generalized estimating equation approach is extended to estimate the parameters in this model. It is shown that the estimators of the model parameters are consistent and asymptotically normal. Simulation studies are conducted to compare it with the estimation method under a parametric model. A real dataset from a prostate cancer study is used for illustration of the new proposed method.},
  archive      = {J_SIM},
  author       = {Lili Yu and Liang Liu},
  doi          = {10.1002/sim.10296},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5983-5995},
  shortjournal = {Stat. Med.},
  title        = {Generalized estimating equations for survival data with dependent censoring},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian nonparametric model for heterogeneous treatment
effects with zero-inflated data. <em>SIM</em>, <em>43</em>(30),
5968–5982. (<a href="https://doi.org/10.1002/sim.10266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One goal of precision medicine is to develop effective treatments for patients by tailoring to their individual demographic, clinical, and/or genetic characteristics. To achieve this goal, statistical models must be developed that can identify and evaluate potentially heterogeneous treatment effects in a robust manner. The oft-cited existing methods for assessing treatment effect heterogeneity are based upon parametric models with interactions or conditioning on covariate values, the performance of which is sensitive to the omission of important covariates and/or the choice of their values. We propose a new Bayesian nonparametric (BNP) method for estimating heterogeneous causal effects in studies with zero-inflated outcome data, which arise commonly in health-related studies. We employ the enriched Dirichlet process (EDP) mixture in our BNP approach, establishing a connection between an outcome DP mixture and a covariate DP mixture. This enables us to estimate posterior distributions concurrently, facilitating flexible inference regarding individual causal effects. We show in a set of simulation studies that the proposed method outperforms two other BNP methods in terms of bias and mean squared error (MSE) of the conditional average treatment effect estimates. In particular, the proposed model has the advantage of appropriately reflecting uncertainty in regions where the overlap condition is violated compared to other competing models. We apply the proposed method to a study of the relationship between heart radiation dose parameters and the blood level of high-sensitivity cardiac troponin T (hs-cTnT) to examine if the effect of a high mean heart radiation dose on hs-cTnT varies by baseline characteristics.},
  archive      = {J_SIM},
  author       = {Chanmin Kim and Yisheng Li and Ting Xu and Zhongxing Liao},
  doi          = {10.1002/sim.10266},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5968-5982},
  shortjournal = {Stat. Med.},
  title        = {Bayesian nonparametric model for heterogeneous treatment effects with zero-inflated data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic treatment regimes on dyadic networks. <em>SIM</em>,
<em>43</em>(30), 5944–5967. (<a
href="https://doi.org/10.1002/sim.10278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying interventions that are optimally tailored to each individual is of significant interest in various fields, in particular precision medicine. Dynamic treatment regimes (DTRs) employ sequences of decision rules that utilize individual patient information to recommend treatments. However, the assumption that an individual&#39;s treatment does not impact the outcomes of others, known as the no interference assumption, is often challenged in practical settings. For example, in infectious disease studies, the vaccine status of individuals in close proximity can influence the likelihood of infection. Imposing this assumption when it, in fact, does not hold, may lead to biased results and impact the validity of the resulting DTR optimization. We extend the estimation method of dynamic weighted ordinary least squares (dWOLS), a doubly robust and easily implemented approach for estimating optimal DTRs, to incorporate the presence of interference within dyads (i.e., pairs of individuals). We formalize an appropriate outcome model and describe the estimation of an optimal decision rule in the dyadic-network context. Through comprehensive simulations and analysis of the Population Assessment of Tobacco and Health (PATH) data, we demonstrate the improved performance of the proposed joint optimization strategy compared to the current state-of-the-art conditional optimization methods in estimating the optimal treatment assignments when within-dyad interference exists.},
  archive      = {J_SIM},
  author       = {Marizeh Mussavi Rizi and Joel A. Dubin and Micheal P. Wallace},
  doi          = {10.1002/sim.10278},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5944-5967},
  shortjournal = {Stat. Med.},
  title        = {Dynamic treatment regimes on dyadic networks},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying overdiagnosis for multicancer detection tests: A
novel method. <em>SIM</em>, <em>43</em>(30), 5935–5943. (<a
href="https://doi.org/10.1002/sim.10285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicancer detection (MCD) tests use blood specimens to detect preclinical cancers. A major concern is overdiagnosis, the detection of preclinical cancer on screening that would not have developed into symptomatic cancer in the absence of screening. Because overdiagnosis can lead to unnecessary and harmful treatments, its quantification is important. A key metric is the screen overdiagnosis fraction (SOF), the probability of overdiagnosis at screen detection. Estimating SOF is notoriously difficult because overdiagnosis is not observed. This estimation is more challenging with MCD tests because short-term results are needed as the technology is rapidly changing. To estimate average SOF for a program of yearly MCD tests, I introduce a novel method that requires at least two yearly MCD tests given to persons having a wide range of ages and applies only to cancers for which there is no conventional screening. The method assumes an exponential distribution for the sojourn time in an operational screen-detectable preclinical cancer (OPC) state, defined as once screen-detectable (positive screen and work-up), always screen-detectable. Because this assumption appears in only one term in the SOF formula, the results are robust to violations of the assumption. An SOF plot graphs average SOF versus mean sojourn time. With lung cancer screening data and synthetic data, SOF plots distinguished small from moderate levels of SOF. With its unique set of assumptions, the SOF plot would complement other modeling approaches for estimating SOF once sufficient short-term observational data on MCD tests become available.},
  archive      = {J_SIM},
  author       = {Stuart G. Baker},
  doi          = {10.1002/sim.10285},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5935-5943},
  shortjournal = {Stat. Med.},
  title        = {Quantifying overdiagnosis for multicancer detection tests: A novel method},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The win ratio approach in bayesian monitoring for two-arm
phase II clinical trial designs with multiple time-to-event endpoints.
<em>SIM</em>, <em>43</em>(30), 5922–5934. (<a
href="https://doi.org/10.1002/sim.10282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To assess the preliminary therapeutic impact of a novel treatment, futility monitoring is commonly employed in Phase II clinical trials to facilitate informed decisions regarding the early termination of trials. Given the rapid evolution in cancer treatment development, particularly with new agents like immunotherapeutic agents, the focus has often shifted from objective response to time-to-event endpoints. In trials involving multiple time-to-event endpoints, existing monitoring designs typically select one as the primary endpoint or employ a composite endpoint as the time to the first occurrence of any event. However, relying on a single efficacy endpoint may not adequately evaluate an experimental treatment. Additionally, the time-to-first-event endpoint treats all events equally, ignoring their differences in clinical priorities. To tackle these issues, we propose a Bayesian futility monitoring design for a two-arm randomized Phase II trial, which incorporates the win ratio approach to account for the clinical priority of multiple time-to-event endpoints. A joint lognormal distribution was assumed to model the time-to-event variables for the estimation. We conducted simulation studies to assess the operating characteristics of the proposed monitoring design and compared them to those of conventional methods. The proposed design allows for early termination for futility if the endpoint with higher clinical priority (e.g., death) deteriorates in the treatment arm, compared to the time-to-first-event approach. Meanwhile, it prevents an aggressive early termination if the endpoint with lower clinical priority (e.g., cancer recurrence) shows deterioration in the treatment arm, offering a more tailored approach to decision-making in clinical trials with multiple time-to-event endpoints.},
  archive      = {J_SIM},
  author       = {Xinran Huang and Jian Wang and Jing Ning},
  doi          = {10.1002/sim.10282},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5922-5934},
  shortjournal = {Stat. Med.},
  title        = {The win ratio approach in bayesian monitoring for two-arm phase II clinical trial designs with multiple time-to-event endpoints},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Response-adaptive randomization procedure in clinical trials
with surrogate endpoints. <em>SIM</em>, <em>43</em>(30), 5911–5921. (<a
href="https://doi.org/10.1002/sim.10286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, subjects are usually recruited sequentially. According to the outcomes amassed thus far in a trial, the response-adaptive randomization (RAR) design has been shown to be an advantageous treatment assignment procedure that skews the treatment allocation proportion to pre-specified objectives, such as sending more patients to a more promising treatment. Unfortunately, there are circumstances under which very few data of the primary endpoints are collected in the recruitment period, such as circumstances relating to public health emergencies and chronic diseases, and RAR is thus difficult to apply in allocating treatments using available outcomes. To overcome this problem, if an informative surrogate endpoint can be acquired much earlier than the primary endpoint, the surrogate endpoint can be used as a substitute for the primary endpoint in the RAR procedure. In this paper, we propose an RAR procedure that relies only on surrogate endpoints. The validity of the statistical inference on the primary endpoint and the patient benefit of this approach are justified by both theory and simulation. Furthermore, different types of surrogate endpoint and primary endpoint are considered. The results reassure that RAR with surrogate endpoints can be a viable option in some cases for clinical trials when primary endpoints are unavailable for adaptation.},
  archive      = {J_SIM},
  author       = {Jingya Gao and Feifang Hu and Wei Ma},
  doi          = {10.1002/sim.10286},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5911-5921},
  shortjournal = {Stat. Med.},
  title        = {Response-adaptive randomization procedure in clinical trials with surrogate endpoints},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlocking cognitive analysis potential in alzheimer’s
disease clinical trials: Investigating hierarchical linear models for
analyzing novel measurement burst design data. <em>SIM</em>,
<em>43</em>(30), 5898–5910. (<a
href="https://doi.org/10.1002/sim.10292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement burst designs typically administer brief cognitive tests four times per day for 1 week, resulting in a maximum of 28 data points per week per test for every 6 months. In Alzheimer&#39;s disease clinical trials, utilizing measurement burst designs holds great promise for boosting statistical power by collecting huge amount of data. However, appropriate methods for analyzing these complex datasets are not well investigated. Furthermore, the large amount of burst design data also poses tremendous challenges for traditional computational procedures such as SAS mixed or Nlmixed. We propose to analyze burst design data using novel hierarchical linear mixed effects models or hierarchical mixed models for repeated measures. Through simulations and real-world data applications using the novel SAS procedure Hpmixed, we demonstrate these hierarchical models&#39; efficiency over traditional models. Our sample simulation and analysis code can serve as a catalyst to facilitate the methodology development for burst design data.},
  archive      = {J_SIM},
  author       = {Guoqiao Wang and Jason Hassenstab and Yan Li and Andrew J. Aschenbrenner and Eric M. McDade and Jorge Llibre-Guerra and Randall J. Bateman and Chengjie Xiong},
  doi          = {10.1002/sim.10292},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5898-5910},
  shortjournal = {Stat. Med.},
  title        = {Unlocking cognitive analysis potential in alzheimer&#39;s disease clinical trials: Investigating hierarchical linear models for analyzing novel measurement burst design data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Q-learning in dynamic treatment regimes with misclassified
binary outcome. <em>SIM</em>, <em>43</em>(30), 5885–5897. (<a
href="https://doi.org/10.1002/sim.10223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of precision medicine involves dynamic treatment regimes (DTRs), which are sequences of treatment decision rules recommended based on patient-level information. The primary goal of the DTR study is to identify an optimal DTR, a sequence of treatment decision rules that optimizes the clinical outcome across multiple decision points. Statistical methods have been developed in recent years to estimate an optimal DTR, including Q-learning, a regression-based method in the DTR literature. Although there are many studies concerning Q-learning, little attention has been paid in the presence of noisy data, such as misclassified outcomes. In this article, we investigate the effect of outcome misclassification on identifying optimal DTRs using Q-learning and propose a correction method to accommodate the misclassification effect on DTR. Simulation studies are conducted to demonstrate the satisfactory performance of the proposed method. We illustrate the proposed method using two examples from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study and the Population Assessment of Tobacco and Health Study.},
  archive      = {J_SIM},
  author       = {Dan Liu and Wenqing He},
  doi          = {10.1002/sim.10223},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5885-5897},
  shortjournal = {Stat. Med.},
  title        = {Q-learning in dynamic treatment regimes with misclassified binary outcome},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regression trees with fused leaves. <em>SIM</em>,
<em>43</em>(30), 5872–5884. (<a
href="https://doi.org/10.1002/sim.10272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel regression tree method named “TreeFuL,” an abbreviation for ‘Tree with Fused Leaves.’ TreeFuL innovatively combines recursive partitioning with fused regularization, offering a distinct approach to the conventional pruning method. One of TreeFuL&#39;s noteworthy advantages is its capacity for cross-validated amalgamation of non-neighboring terminal nodes. This is facilitated by a leaf coloring scheme that supports tree shearing and node amalgamation. As a result, TreeFuL facilitates the development of more parsimonious tree models without compromising predictive accuracy. The refined model offers enhanced interpretability, making it particularly well-suited for biomedical applications of decision trees, such as disease diagnosis and prognosis. We demonstrate the practical advantages of our proposed method through simulation studies and an analysis of data collected in an obesity study.},
  archive      = {J_SIM},
  author       = {Xiaogang Su and Lei Liu and Lili Liu and Ruiwen Zhou and Guoqiao Wang and Elise Dusseldorp and Tianni Zhou},
  doi          = {10.1002/sim.10272},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5872-5884},
  shortjournal = {Stat. Med.},
  title        = {Regression trees with fused leaves},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skewness-corrected confidence intervals for predictive
values in enrichment studies. <em>SIM</em>, <em>43</em>(30), 5862–5871.
(<a href="https://doi.org/10.1002/sim.10283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The positive predictive value (PPV) and negative predictive value (NPV) can be expressed as functions of disease prevalence ( ρ $$ \rho $$ ) and the ratios of two binomial proportions ( ϕ $$ \phi $$ ), where ϕ ppv = 1 − specificity sensitivity $$ {\phi}_{ppv}=\frac{1- specificity}{sensitivity} $$ and ϕ npv = 1 − sensitivity specificity $$ {\phi}_{npv}=\frac{1- sensitivity}{specificity} $$ . In prospective studies, where the proportion of subjects with the disease in the study cohort is an unbiased estimate of the disease prevalence, the confidence intervals (CIs) of PPV and NPV can be estimated using established methods for single proportion. However, in enrichment studies, such as case–control studies, where the proportion of diseased subjects significantly differs from disease prevalence, estimating CIs for PPV and NPV remains a challenge in terms of skewness and overall coverage, especially under extreme conditions (e.g., ). In this article, we extend the method adopted by Li, where CIs for PPV and NPV were derived from those of . We explored additional CI methods for , including those by Gart &amp; Nam (GN), MoverJ, and Walter and convert their corresponding CIs for PPV and NPV. Through simulations, we compared these methods with established CI methods, Fieller, Pepe, and Delta in terms of skewness and overall coverage. While no method proves universally optimal, GN and MoverJ methods generally emerge as recommended choices.},
  archive      = {J_SIM},
  author       = {Dadong Zhang and Jingye Wang and Suqin Cai and Johan Surtihadi},
  doi          = {10.1002/sim.10283},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5862-5871},
  shortjournal = {Stat. Med.},
  title        = {Skewness-corrected confidence intervals for predictive values in enrichment studies},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference for counting processes under shape
heterogeneity. <em>SIM</em>, <em>43</em>(30), 5849–5861. (<a
href="https://doi.org/10.1002/sim.10280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proportional rate models are among the most popular methods for analyzing recurrent event data. Although providing a straightforward rate-ratio interpretation of covariate effects, the proportional rate assumption implies that covariates do not modify the shape of the rate function. When the proportionality assumption fails to hold, we propose to characterize covariate effects on the rate function through two types of parameters: the shape parameters and the size parameters. The former allows the covariates to flexibly affect the shape of the rate function, and the latter retains the interpretability of covariate effects on the magnitude of the rate function. To overcome the challenges in simultaneously estimating the two sets of parameters, we propose a conditional pseudolikelihood approach to eliminate the size parameters in shape estimation, followed by an event count projection approach for size estimation. The proposed estimators are asymptotically normal with a root- convergence rate. Simulation studies and an analysis of recurrent hospitalizations using SEER-Medicare data are conducted to illustrate the proposed methods.},
  archive      = {J_SIM},
  author       = {Ying Sheng and Yifei Sun},
  doi          = {10.1002/sim.10280},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5849-5861},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for counting processes under shape heterogeneity},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforced borrowing framework: Leveraging auxiliary data
for individualized inference. <em>SIM</em>, <em>43</em>(30), 5837–5848.
(<a href="https://doi.org/10.1002/sim.10267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly during the past decade, researchers have sought to leverage auxiliary data for enhancing individualized inference. Many existing methods, such as multisource exchangeability models (MEM), have been developed to borrow information from multiple supplemental sources to support parameter inference in a primary source. MEM and its alternatives decide how much information to borrow based on the exchangeability of the primary and supplemental sources, where exchangeability is defined as equality of the target parameter. Other information that may also help determine the exchangeability of sources is ignored. In this article, we propose a generalized reinforced borrowing framework (RBF) leveraging auxiliary data for enhancing individualized inference using a distance-embedded prior which uses data not only about the target parameter but also uses different types of auxiliary information sources to “reinforce” inference on the target parameter. RBF improves inference with minimal additional computational burden. We demonstrate the application of RBF to a study investigating the impact of the COVID-19 pandemic on individual activity and transportation behaviors, where RBF achieves 20%–40% lower MSE compared with existing methods.},
  archive      = {J_SIM},
  author       = {Ziyu Ji and Julian Wolfson},
  doi          = {10.1002/sim.10267},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5837-5848},
  shortjournal = {Stat. Med.},
  title        = {Reinforced borrowing framework: Leveraging auxiliary data for individualized inference},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instrumental variable model average with applications in
nonlinear causal inference. <em>SIM</em>, <em>43</em>(30), 5814–5836.
(<a href="https://doi.org/10.1002/sim.10269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The instrumental variable method is widely used in causal inference research to improve the accuracy of estimating causal effects. However, the weak correlation between instruments and exposure, as well as the direct impact of instruments on the outcome, can lead to biased estimates. To mitigate the bias introduced by such instruments in nonlinear causal inference, we propose a two-stage nonlinear causal effect estimation based on model averaging. The model uses different subsets of instruments in the first stage to predict exposure after a nonlinear transformation with the help of sliced inverse regression. In the second stage, adaptive Lasso penalty is applied to instruments to obtain the estimation of causal effect. We prove that the proposed estimator exhibits favorable asymptotic properties and evaluate its performance through a series of numerical studies, demonstrating its effectiveness in identifying nonlinear causal effects and its capability to handle scenarios with weak and invalid instruments. We apply the proposed method to the Atherosclerosis Risk in Communities dataset to investigate the relationship between BMI and hypertension.},
  archive      = {J_SIM},
  author       = {Dong Chen and Yuquan Wang and Dapeng Shi and Yunlong Cao and Yue-Qing Hu},
  doi          = {10.1002/sim.10269},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5814-5836},
  shortjournal = {Stat. Med.},
  title        = {Instrumental variable model average with applications in nonlinear causal inference},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving survey inference using administrative records
without releasing individual-level continuous data. <em>SIM</em>,
<em>43</em>(30), 5803–5813. (<a
href="https://doi.org/10.1002/sim.10270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probability surveys are challenged by increasing nonresponse rates, resulting in biased statistical inference. Auxiliary information about populations can be used to reduce bias in estimation. Often continuous auxiliary variables in administrative records are first discretized before releasing to the public to avoid confidentiality breaches. This may weaken the utility of the administrative records in improving survey estimates, particularly when there is a strong relationship between continuous auxiliary information and the survey outcome. In this paper, we propose a two-step strategy, where the confidential continuous auxiliary data in the population are first utilized to estimate the response propensity score of the survey sample by statistical agencies, which is then included in a modified population data for data users. In the second step, data users who do not have access to confidential continuous auxiliary data conduct predictive survey inference by including discretized continuous variables and the propensity score as predictors using splines in a Bayesian model. We show by simulation that the proposed method performs well, yielding more efficient estimates of population means with 95% credible intervals providing better coverage than alternative approaches. We illustrate the proposed method using the Ohio Army National Guard Mental Health Initiative (OHARNG-MHI). The methods developed in this work are readily available in the R package AuxSurvey .},
  archive      = {J_SIM},
  author       = {Sharifa Z. Williams and Jungang Zou and Yutao Liu and Yajuan Si and Sandro Galea and Qixuan Chen},
  doi          = {10.1002/sim.10270},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5803-5813},
  shortjournal = {Stat. Med.},
  title        = {Improving survey inference using administrative records without releasing individual-level continuous data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Powerful test of heterogeneity in two-sample summary-data
mendelian randomization. <em>SIM</em>, <em>43</em>(30), 5791–5802. (<a
href="https://doi.org/10.1002/sim.10279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Kai Wang and Steven Y. Alberding},
  doi          = {10.1002/sim.10279},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5791-5802},
  shortjournal = {Stat. Med.},
  title        = {Powerful test of heterogeneity in two-sample summary-data mendelian randomization},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A partially randomized patient preference, sequential,
multiple-assignment, randomized trial design analyzed via weighted and
replicated frequentist and bayesian methods. <em>SIM</em>,
<em>43</em>(30), 5777–5790. (<a
href="https://doi.org/10.1002/sim.10276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Results from randomized control trials (RCTs) may not be representative when individuals refuse to be randomized or are excluded for having a preference for which treatment they receive. If trial designs do not allow for participant treatment preferences, trials can suffer in accrual, adherence, retention, and external validity of results. Thus, there is interest surrounding clinical trial designs that incorporate participant treatment preferences. We propose a Partially Randomized, Patient Preference, Sequential, Multiple Assignment, Randomized Trial (PRPP-SMART) which combines a Partially Randomized, Patient Preference (PRPP) design with a Sequential, Multiple Assignment, Randomized Trial (SMART) design. This novel PRPP-SMART design is a multi-stage clinical trial design where, at each stage, participants either receive their preferred treatment, or if they do not have a preferred treatment, they are randomized. This paper focuses on the clinical trial design for PRPP-SMARTs and the development of Bayesian and frequentist weighted and replicated regression models (WRRMs) to analyze data from such trials. We propose a two-stage PRPP-SMART with binary end of stage outcomes and estimate the embedded dynamic treatment regimes (DTRs). Our WRRMs use data from both randomized and non-randomized participants for efficient estimation of the DTR effects. We compare our method to a more traditional PRPP analysis which only considers participants randomized to treatment. Our Bayesian and frequentist methods produce more efficient DTR estimates with negligible bias despite the inclusion of non-randomized participants in the analysis. The proposed PRPP-SMART design and analytic method is a promising approach to incorporate participant treatment preferences into clinical trial design.},
  archive      = {J_SIM},
  author       = {Marianthie Wank and Sarah Medley and Roy N. Tamura and Thomas M. Braun and Kelley M. Kidwell},
  doi          = {10.1002/sim.10276},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5777-5790},
  shortjournal = {Stat. Med.},
  title        = {A partially randomized patient preference, sequential, multiple-assignment, randomized trial design analyzed via weighted and replicated frequentist and bayesian methods},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sieve maximum likelihood estimation of partially linear
transformation models with interval-censored data. <em>SIM</em>,
<em>43</em>(30), 5765–5776. (<a
href="https://doi.org/10.1002/sim.10225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially linear models provide a valuable tool for modeling failure time data with nonlinear covariate effects. Their applicability and importance in survival analysis have been widely acknowledged. To date, numerous inference methods for such models have been developed under traditional right censoring. However, the existing studies seldom target interval-censored data, which provide more coarse information and frequently occur in many scientific studies involving periodical follow-up. In this work, we propose a flexible class of partially linear transformation models to examine parametric and nonparametric covariate effects for interval-censored outcomes. We consider the sieve maximum likelihood estimation approach that approximates the cumulative baseline hazard function and nonparametric covariate effect with the monotone splines and -splines, respectively. We develop an easy-to-implement expectation-maximization algorithm coupled with three-stage data augmentation to facilitate maximization. We establish the consistency of the proposed estimators and the asymptotic distribution of parametric components based on the empirical process techniques. Numerical results from extensive simulation studies indicate that our proposed method performs satisfactorily in finite samples. An application to a study on hypobaric decompression sickness suggests that the variable TR360 exhibits a significant dynamic and nonlinear effect on the risk of developing hypobaric decompression sickness.},
  archive      = {J_SIM},
  author       = {Changhui Yuan and Shishun Zhao and Shuwei Li and Xinyuan Song},
  doi          = {10.1002/sim.10225},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5765-5776},
  shortjournal = {Stat. Med.},
  title        = {Sieve maximum likelihood estimation of partially linear transformation models with interval-censored data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian approach to modeling variance of intensive
longitudinal biomarker data as a predictor of health outcomes.
<em>SIM</em>, <em>43</em>(30), 5748–5764. (<a
href="https://doi.org/10.1002/sim.10281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensive longitudinal biomarker data are increasingly common in scientific studies that seek temporally granular understanding of the role of behavioral and physiological factors in relation to outcomes of interest. Intensive longitudinal biomarker data, such as those obtained from wearable devices, are often obtained at a high frequency typically resulting in several hundred to thousand observations per individual measured over minutes, hours, or days. Often in longitudinal studies, the primary focus is on relating the means of biomarker trajectories to an outcome, and the variances are treated as nuisance parameters, although they may also be informative for the outcomes. In this paper, we propose a Bayesian hierarchical model to jointly model a cross-sectional outcome and the intensive longitudinal biomarkers. To model the variability of biomarkers and deal with the high intensity of data, we develop subject-level cubic B-splines and allow the sharing of information across individuals for both the residual variability and the random effects variability. Then different levels of variability are extracted and incorporated into an outcome submodel for inferential and predictive purposes. We demonstrate the utility of the proposed model via an application involving bio-monitoring of hertz-level heart rate information from a study on social stress.},
  archive      = {J_SIM},
  author       = {Mingyan Yu and Zhenke Wu and Margaret Hicken and Michael R. Elliott},
  doi          = {10.1002/sim.10281},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5748-5764},
  shortjournal = {Stat. Med.},
  title        = {A bayesian approach to modeling variance of intensive longitudinal biomarker data as a predictor of health outcomes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ℓ1-penalized multinomial regression: Estimation, inference,
and prediction, with an application to risk factor identification for
different dementia subtypes. <em>SIM</em>, <em>43</em>(30), 5711–5747.
(<a href="https://doi.org/10.1002/sim.10263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional multinomial regression models are very useful in practice but have received less research attention than logistic regression models, especially from the perspective of statistical inference. In this work, we analyze the estimation and prediction error of the contrast-based ℓ 1 $$ {\ell}_1 $$ -penalized multinomial regression model and extend the debiasing method to the multinomial case, providing a valid confidence interval for each coefficient and value of the individual hypothesis test. We also examine cases of model misspecification and non-identically distributed data to demonstrate the robustness of our method when some assumptions are violated. We apply the debiasing method to identify important predictors in the progression into dementia of different subtypes. Results from extensive simulations show the superiority of the debiasing method compared to other inference methods.},
  archive      = {J_SIM},
  author       = {Ye Tian and Henry Rusinek and Arjun V. Masurkar and Yang Feng},
  doi          = {10.1002/sim.10263},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5711-5747},
  shortjournal = {Stat. Med.},
  title        = {ℓ1-penalized multinomial regression: Estimation, inference, and prediction, with an application to risk factor identification for different dementia subtypes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shape mediation analysis in alzheimer’s disease studies.
<em>SIM</em>, <em>43</em>(30), 5698–5710. (<a
href="https://doi.org/10.1002/sim.10265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial tool in neuroscience, mediation analysis has been developed and widely adopted to elucidate the role of intermediary variables derived from neuroimaging data. Typically, structural equation models (SEMs) are employed to investigate the influences of exposures on outcomes, with model coefficients being interpreted as causal effects. While existing SEMs have proven to be effective tools for mediation analysis involving various neuroimaging-related mediators, limited research has explored scenarios where these mediators are derived from the shape space. In addition, the linear relationship assumption adopted in existing SEMs may lead to substantial efficiency losses and decreased predictive accuracy in real-world applications. To address these challenges, we introduce a novel framework for shape mediation analysis, designed to explore the causal relationships between genetic exposures and clinical outcomes, whether mediated or unmediated by shape-related factors while accounting for potential confounding variables. Within our framework, we apply the square-root velocity function to extract elastic shape representations, which reside within the linear Hilbert space of square-integrable functions. Subsequently, we introduce a two-layer shape regression model to characterize the relationships among neurocognitive outcomes, elastic shape mediators, genetic exposures, and clinical confounders. Both estimation and inference procedures are established for unknown parameters along with the corresponding causal estimands. The asymptotic properties of estimated quantities are investigated as well. Both simulated studies and real-data analyses demonstrate the superior performance of our proposed method in terms of estimation accuracy and robustness when compared to existing approaches for estimating causal estimands.},
  archive      = {J_SIM},
  author       = {Xingcai Zhou and Miyeon Yeon and Jiangyan Wang and Shengxian Ding and Kaizhou Lei and Yanyong Zhao and Rongjie Liu and Chao Huang},
  doi          = {10.1002/sim.10265},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5698-5710},
  shortjournal = {Stat. Med.},
  title        = {Shape mediation analysis in alzheimer&#39;s disease studies},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformed ROC curve for biomarker evaluation.
<em>SIM</em>, <em>43</em>(30), 5681–5697. (<a
href="https://doi.org/10.1002/sim.10268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To complement the conventional area under the ROC curve (AUC) which cannot fully describe the diagnostic accuracy of some non-standard biomarkers, we introduce a transformed ROC curve and its associated transformed AUC (TAUC) in this article, and show that TAUC can relate the original improper biomarker to a proper biomarker after a non-monotone transformation. We then provide nonparametric estimation of the non-monotone transformation and TAUC, and establish their consistency and asymptotic normality. We conduct extensive simulation studies to assess the performance of the proposed TAUC method and compare with the traditional methods. Case studies on real biomedical data are provided to illustrate the proposed TAUC method. We are able to identify more important biomarkers that tend to escape the traditional screening method.},
  archive      = {J_SIM},
  author       = {Jianping Yang and Pei-Fen Kuan and Xiangyu Li and Jialiang Li and Xiao-Hua Zhou},
  doi          = {10.1002/sim.10268},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5681-5697},
  shortjournal = {Stat. Med.},
  title        = {Transformed ROC curve for biomarker evaluation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical bayesian model for estimating age-specific
COVID-19 infection fatality rates in developing countries. <em>SIM</em>,
<em>43</em>(30), 5667–5680. (<a
href="https://doi.org/10.1002/sim.10259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 infection fatality rate (IFR) is the proportion of individuals infected with SARS-CoV-2 who subsequently die. As COVID-19 disproportionately affects older individuals, age-specific IFR estimates are imperative to facilitate comparisons of the impact of COVID-19 between locations and prioritize distribution of scarce resources. However, there lacks a coherent method to synthesize available data to create estimates of IFR and seroprevalence that vary continuously with age and adequately reflect uncertainties inherent in the underlying data. In this article, we introduce a novel Bayesian hierarchical model to estimate IFR as a continuous function of age that acknowledges heterogeneity in population age structure across locations and accounts for uncertainty in the estimates due to seroprevalence sampling variability and the imperfect serology test assays. Our approach simultaneously models test assay characteristics, serology, and death data, where the serology and death data are often available only for binned age groups. Information is shared across locations through hierarchical modeling to improve estimation of the parameters with limited data. Modeling data from 26 developing country locations during the first year of the COVID-19 pandemic, we found seroprevalence did not change dramatically with age, and the IFR at age 60 was above the high-income country estimate for most locations.},
  archive      = {J_SIM},
  author       = {Sierra Pugh and Andrew T. Levin and Gideon Meyerowitz-Katz and Satej Soman and Nana Owusu-Boaitey and Anthony B. Zwi and Anup Malani and Ander Wilson and Bailey K. Fosdick},
  doi          = {10.1002/sim.10259},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5667-5680},
  shortjournal = {Stat. Med.},
  title        = {A hierarchical bayesian model for estimating age-specific COVID-19 infection fatality rates in developing countries},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general framework for the multiple nonparametric
behrens–fisher problem with dependent replicates. <em>SIM</em>,
<em>43</em>(30), 5650–5666. (<a
href="https://doi.org/10.1002/sim.10262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many trials and experiments, subjects are not only observed once but multiple times, resulting in a cluster of possibly correlated observations (e.g., brain regions per patient). Observations often do not fulfill model assumptions of mixed models and require the use of nonparametric methods. In this article, we develop and present a purely nonparametric rank-based procedure that flexibly allows the unbiased and consistent estimation of the Wilcoxon–Mann–Whitney effect P ⁡ ( X &lt; Y ) + 1 2 ⁢ P ⁡ ( X = Y ) $$ P\left(X&lt;Y\right)+\frac{1}{2}P\left(X=Y\right) $$ in clustered data designs. Compared with existing methods, we allow flexible weights to be used in effect estimation. Additionally, we develop global and multiple contrast test procedures to test null hypotheses formulated regarding the generalized Mann–Whitney effects and for the computation of range-preserving simultaneous confidence intervals in a unified way. Extensive simulation studies show that these methods control the type-I error rate well and have reasonable power to detect alternatives in various situations.},
  archive      = {J_SIM},
  author       = {Erin Sprünken and Robert Mertens and Frank Konietschke},
  doi          = {10.1002/sim.10262},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5650-5666},
  shortjournal = {Stat. Med.},
  title        = {A general framework for the multiple nonparametric Behrens–Fisher problem with dependent replicates},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized fused lasso for treatment pooling in network
meta-analysis. <em>SIM</em>, <em>43</em>(30), 5635–5649. (<a
href="https://doi.org/10.1002/sim.10253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work develops a generalized fused lasso (GFL) approach to fitting contrast-based network meta-analysis (NMA) models. The GFL method penalizes all pairwise differences between treatment effects, resulting in the pooling of treatments that are not sufficiently different. This approach offers an intriguing avenue for potentially mitigating biases in treatment rankings and reducing sparsity in networks. To fit contrast-based NMA models within the GFL framework, we formulate the models as generalized least squares problems, where the precision matrix depends on the standard error in the data, the estimated between-study heterogeneity and the correlation between contrasts in multi-arm studies. By utilizing a Cholesky decomposition of the precision matrix, we linearly transform the data vector and design matrix to frame NMA within the GFL framework. We demonstrate how to construct the GFL penalty such that every pairwise difference is penalized similarly. The model is straightforward to implement in R via the “genlasso” package, and runs instantaneously, contrary to other regularization approaches that are Bayesian. A two-step GFL-NMA approach is recommended to obtain measures of uncertainty associated with the (pooled) relative treatment effects. Two simulation studies confirm the GFL approach&#39;s ability to pool treatments that have the same (or similar) effects while also revealing when incorrect pooling may occur, and its potential benefits against alternative methods. The novel GFL-NMA method is successfully applied to a real-world dataset on diabetes where the standard NMA model was not favored compared to the best-fitting GFL-NMA model with AICc selection of the tuning parameter ( .},
  archive      = {J_SIM},
  author       = {Xiangshan Kong and Caitlin H. Daly and Audrey Béliveau},
  doi          = {10.1002/sim.10253},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5635-5649},
  shortjournal = {Stat. Med.},
  title        = {Generalized fused lasso for treatment pooling in network meta-analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A brief introduction on latent variable based ordinal
regression models with an application to survey data. <em>SIM</em>,
<em>43</em>(29), 5618–5634. (<a
href="https://doi.org/10.1002/sim.10208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of survey data is a frequently arising issue in clinical trials, particularly when capturing quantities which are difficult to measure. Typical examples are questionnaires about patient&#39;s well-being, pain, or consent to an intervention. In these, data is captured on a discrete scale containing only a limited number of possible answers, from which the respondent has to pick the answer which fits best his/her personal opinion. This data is generally located on an ordinal scale as answers can usually be arranged in an ascending order, for example, “bad”, “neutral”, “good” for well-being. Since responses are usually stored numerically for data processing purposes, analysis of survey data using ordinary linear regression models are commonly applied. However, assumptions of these models are often not met as linear regression requires a constant variability of the response variable and can yield predictions out of the range of response categories. By using linear models, one only gains insights about the mean response which may affect representativeness. In contrast, ordinal regression models can provide probability estimates for all response categories and yield information about the full response scale beyond the mean. In this work, we provide a concise overview of the fundamentals of latent variable based ordinal models, applications to a real data set, and outline the use of state-of-the-art-software for this purpose. Moreover, we discuss strengths, limitations and typical pitfalls. This is a companion work to a current vignette-based structured interview study in pediatric anesthesia.},
  archive      = {J_SIM},
  author       = {Johannes Wieditz and Clemens Miller and Jan Scholand and Marcus Nemeth},
  doi          = {10.1002/sim.10208},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5618-5634},
  shortjournal = {Stat. Med.},
  title        = {A brief introduction on latent variable based ordinal regression models with an application to survey data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Drug efficacy estimation for follow-on companion diagnostic
devices through external studies. <em>SIM</em>, <em>43</em>(29),
5605–5617. (<a href="https://doi.org/10.1002/sim.10231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A therapeutic product is usually not suitable for all patients but for only a subpopulation. The safe and effective use of such a therapeutic product requires the co-approval of a companion diagnostic device which can be used to identify suitable patients. While the first-of-a-kind companion diagnostic device is often developed in conjunction with its intended therapeutic product and simultaneously validated through a randomized clinical trial, there remains room for the innovation of new and improved follow-on companion diagnostic devices designed for the same therapeutic product. However, conducting a new randomized trial or a bridging study for the follow on companion devices may be unethical, expensive or unpractical. Hence, there arises a need for an external study to evaluate the concordance between the FDA-approved comparator companion diagnostic device (CCD) and the subsequent follow-on companion diagnostic devices (FCD), indirectly validating the latter. In this article, we introduce a novel external study design, referred to as the targeted treatment design, as an extension of the existing concordance design. Additionally, we present corresponding statistical analysis methods. Our approach combines the CCD randomized trial data and the FCD external study data, enabling the estimation of drug efficacy within the FCD+ and FCD- subpopulations—the parameters crucial for the validation of the FCD. Theoretical results and simulation studies validate the proposed methods and we further illustrate the proposed methods through an application in a real example of non-small-cell lung cancer.},
  archive      = {J_SIM},
  author       = {Jiarui Sun and Wenjie Hu and Xiao-Hua Zhou},
  doi          = {10.1002/sim.10231},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5605-5617},
  shortjournal = {Stat. Med.},
  title        = {Drug efficacy estimation for follow-on companion diagnostic devices through external studies},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Permutation test for image-on-scalar regression with an
application to breast cancer. <em>SIM</em>, <em>43</em>(29), 5596–5604.
(<a href="https://doi.org/10.1002/sim.10242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image based screening is now routinely available for early detection of cancer and other diseases. Quantitative analysis for effects of risk factors on digital images is important to extract biological insights for modifiable factors in prevention studies and understand pathways for targets in preventive drugs. However, current approaches are restricted to summary measures within the image with the assumption that all relevant features needed to characterize an image can be identified and appropriately quantified. Motivated by data challenges in breast cancer, we propose a nonparametric statistical framework for risk factor screening that uses the whole mammogram image as outcome. The proposed permutation test allows assessment of whether a set of scalar risk factors is associated with the whole image in the presence of correlated residuals across the spatial domain. We provide extensive simulation studies and illustrate an application to the Joanne Knight Breast Health Cohort using the mammogram imaging data.},
  archive      = {J_SIM},
  author       = {Shu Jiang and Graham A. Colditz},
  doi          = {10.1002/sim.10242},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5596-5604},
  shortjournal = {Stat. Med.},
  title        = {Permutation test for image-on-scalar regression with an application to breast cancer},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian safety and futility monitoring in phase II trials
using one utility-based rule. <em>SIM</em>, <em>43</em>(29), 5583–5595.
(<a href="https://doi.org/10.1002/sim.10254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For phase II clinical trials that determine the acceptability of an experimental treatment based on ordinal toxicity and ordinal response, most monitoring methods require each ordinal outcome to be dichotomized using a selected cut-point. This allows two early stopping rules to be constructed that compare marginal probabilities of toxicity and response to respective upper and lower limits. Important problems with this approach are loss of information due to dichotomization, dependence of treatment acceptability decisions on precisely how each ordinal variable is dichotomized, and ignoring association between the two outcomes. To address these problems, we propose a new Bayesian method, which we call U-Bayes, that exploits elicited numerical utilities of the joint ordinal outcomes to construct one early stopping rule that compares the mean utility to a lower limit. U-Bayes avoids the problems noted above by using the entire joint distribution of the ordinal outcomes, and not dichotomizing the outcomes. A step-by-step algorithm is provided for constructing a U-Bayes rule based on elicited utilities and elicited limits on marginal outcome probabilities. A simulation study shows that U-Bayes greatly improves the probability of determining treatment acceptability compared to conventional designs that use two monitoring rules based on marginal probabilities.},
  archive      = {J_SIM},
  author       = {Juhee Lee and Peter F. Thall},
  doi          = {10.1002/sim.10254},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5583-5595},
  shortjournal = {Stat. Med.},
  title        = {Bayesian safety and futility monitoring in phase II trials using one utility-based rule},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing interpretable regression analysis for binary data:
A novel distributed algorithm approach. <em>SIM</em>, <em>43</em>(29),
5573–5582. (<a href="https://doi.org/10.1002/sim.10250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse data bias, where there is a lack of sufficient cases, is a common problem in data analysis, particularly when studying rare binary outcomes. Although a two-step meta-analysis approach may be used to lessen the bias by combining the summary statistics to increase the number of cases from multiple studies, this method does not completely eliminate bias in effect estimation. In this paper, we propose a one-shot distributed algorithm for estimating relative risk using a modified Poisson regression for binary data, named ODAP-B. We evaluate the performance of our method through both simulation studies and real-world case analyses of postacute sequelae of SARS-CoV-2 infection in children using data from 184 501 children across eight national academic medical centers. Compared with the meta-analysis method, our method provides closer estimates of the relative risk for all outcomes considered including syndromic and systemic outcomes. Our method is communication-efficient and privacy-preserving, requiring only aggregated data to obtain relatively unbiased effect estimates compared with two-step meta-analysis methods. Overall, ODAP-B is an effective distributed learning algorithm for Poisson regression to study rare binary outcomes. The method provides inference on adjusted relative risk with a robust variance estimator.},
  archive      = {J_SIM},
  author       = {Jiayi Tong and Lu Li and Jenna Marie Reps and Vitaly Lorman and Naimin Jing and Mackenzie Edmondson and Xiwei Lou and Ravi Jhaveri and Kelly J. Kelleher and Nathan M. Pajor and Christopher B. Forrest and Jiang Bian and Haitao Chu and Yong Chen},
  doi          = {10.1002/sim.10250},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5573-5582},
  shortjournal = {Stat. Med.},
  title        = {Advancing interpretable regression analysis for binary data: A novel distributed algorithm approach},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical sandwich variance estimator for iterated
conditional expectation g-computation. <em>SIM</em>, <em>43</em>(29),
5562–5572. (<a href="https://doi.org/10.1002/sim.10255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iterated conditional expectation (ICE) g-computation is an estimation approach for addressing time-varying confounding for both longitudinal and time-to-event data. Unlike other g-computation implementations, ICE avoids the need to specify models for each time-varying covariate. For variance estimation, previous work has suggested the bootstrap. However, bootstrapping can be computationally intense. Here, we present ICE g-computation as a set of stacked estimating equations. Therefore, the variance for the ICE g-computation estimator can be consistently estimated using the empirical sandwich variance estimator. Performance of the variance estimator was evaluated empirically with a simulation study. The proposed approach is also demonstrated with an illustrative example on the effect of cigarette smoking on the prevalence of hypertension. In the simulation study, the empirical sandwich variance estimator appropriately estimated the variance. When comparing runtimes between the sandwich variance estimator and the bootstrap for the applied example, the sandwich estimator was substantially faster, even when bootstraps were run in parallel. The empirical sandwich variance estimator is a viable option for variance estimation with ICE g-computation.},
  archive      = {J_SIM},
  author       = {Paul N. Zivich and Rachael K. Ross and Bonnie E. Shook-Sa and Stephen R. Cole and Jessie K. Edwards},
  doi          = {10.1002/sim.10255},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5562-5572},
  shortjournal = {Stat. Med.},
  title        = {Empirical sandwich variance estimator for iterated conditional expectation g-computation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A comparison of variance estimators for logistic regression
models estimated using generalized estimating equations (GEE) in the
context of observational health services research. <em>SIM</em>,
<em>43</em>(29), 5548–5561. (<a
href="https://doi.org/10.1002/sim.10260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational health services research, researchers often use clustered data to estimate the independent association between individual outcomes and several cluster-level covariates after adjusting for individual-level characteristics. Generalized estimating equations are a popular method for estimating generalized linear models using clustered data. The conventional Liang–Zeger variance estimator is known to result in estimated standard errors that are biased low when the number of clusters in small. Alternative variance estimators have been proposed for use when the number of clusters is low. Previous studies focused on these alternative variance estimators in the context of cluster randomized trials, which are often characterized by a small number of clusters and by an outcomes regression model that often consists of a single cluster-level variable (the treatment/exposure variable). We addressed the following questions: (i) which estimator is preferred for estimating the standard errors of cluster-level covariates for logistic regression models with multiple binary and continuous cluster-level variables in addition to subject-level variables; (ii) in such settings, how many clusters are required for the Liang–Zeger variance estimator to have acceptable performance for estimating the standard errors of cluster-level covariates. We suggest that when estimating standard errors: (i) when the number of clusters is &lt; 15 use the Kauermann–Carroll estimator; (ii) when the number of clusters is between 15 and 40 use the Fay–Graubard estimator; (iii) when the number of clusters exceeds 40, use the Liang–Zeger estimator or the Fay–Graubard estimator. When estimating confidence intervals, we suggest using the Mancl–DeRouen estimator with a t -distribution.},
  archive      = {J_SIM},
  author       = {Peter C. Austin},
  doi          = {10.1002/sim.10260},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5548-5561},
  shortjournal = {Stat. Med.},
  title        = {A comparison of variance estimators for logistic regression models estimated using generalized estimating equations (GEE) in the context of observational health services research},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dose individualization for phase i cancer trials with
broadened eligibility. <em>SIM</em>, <em>43</em>(29), 5534–5547. (<a
href="https://doi.org/10.1002/sim.10264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broadening eligibility criteria in cancer trials has been advocated to represent the intended patient population more accurately. The advantages are clear in terms of generalizability and recruitment, however there are some important considerations in terms of design for efficiency and patient safety. While toxicity may be expected to be homogeneous across these subpopulations, designs should be able to recommend safe and precise doses if subpopulations with different toxicity profiles exist. Dose-finding designs accounting for patient heterogeneity have been proposed, but existing methods assume that the source of heterogeneity is known. We propose a broadened eligibility dose-finding design to address the situation of unknown patient heterogeneity in phase I cancer clinical trials where eligibility is expanded, and multiple eligibility criteria could potentially lead to different optimal doses for patient subgroups. The design offers a two-in-one approach to dose-finding by simultaneously selecting patient criteria that differentiate the maximum tolerated dose (MTD), using stochastic search variable selection, and recommending the subpopulation-specific MTD if needed. Our simulation study compares the proposed design to the naive approach of assuming patient homogeneity and demonstrates favorable operating characteristics across a wide range of scenarios, allocating patients more often to their true MTD during the trial, recommending more than one MTD when needed, and identifying criteria that differentiate the patient population. The proposed design highlights the advantages of adding more variability at an early stage and demonstrates how assuming patient homogeneity can lead to unsafe or sub-therapeutic dose recommendations.},
  archive      = {J_SIM},
  author       = {Rebecca B. Silva and Bin Cheng and Richard D. Carvajal and Shing M. Lee},
  doi          = {10.1002/sim.10264},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5534-5547},
  shortjournal = {Stat. Med.},
  title        = {Dose individualization for phase i cancer trials with broadened eligibility},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimands and cumulative incidence function regression in
clinical trials: Some new results on interpretability and robustness.
<em>SIM</em>, <em>43</em>(29), 5513–5533. (<a
href="https://doi.org/10.1002/sim.10236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression analyses based on transformations of cumulative incidence functions are often adopted when modeling and testing for treatment effects in clinical trial settings involving competing and semi-competing risks. Common frameworks include the Fine–Gray model and models based on direct binomial regression. Using large sample theory we derive the limiting values of treatment effect estimators based on such models when the data are generated according to multiplicative intensity-based models, and show that the estimand is sensitive to several process features. The rejection rates of hypothesis tests based on cumulative incidence function regression models are also examined for null hypotheses of different types, based on which a robustness property is established. In such settings supportive secondary analyses of treatment effects are essential to ensure a full understanding of the nature of treatment effects. An application to a palliative study of individuals with breast cancer metastatic to bone is provided for illustration.},
  archive      = {J_SIM},
  author       = {Alexandra Bühler and Richard J. Cook and Jerald F. Lawless},
  doi          = {10.1002/sim.10236},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5513-5533},
  shortjournal = {Stat. Med.},
  title        = {Estimands and cumulative incidence function regression in clinical trials: Some new results on interpretability and robustness},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous mediation analysis for cox proportional
hazards model with multiple mediators. <em>SIM</em>, <em>43</em>(29),
5497–5512. (<a href="https://doi.org/10.1002/sim.10239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a heterogeneous mediation analysis for survival data that accommodates multiple mediators and sparsity of the predictors. We introduce a joint modeling approach that links the mediation regression and proportional hazards models through Bayesian additive regression trees with shared typologies. The shared tree component is motivated by the fact that confounders and effect modifiers on the causal pathways linked by different mediators often overlap. A sparsity-inducing prior is incorporated to capture the most relevant confounders and effect modifiers on different causal pathways. The individual-specific interventional direct and indirect effects are derived on the scale of the logarithm of hazards and survival function. A Bayesian approach with an efficient Markov chain Monte Carlo algorithm is developed to estimate the conditional interventional effects through the Monte Carlo implementation of the mediation formula. Simulation studies are conducted to verify the empirical performance of the proposed method. An application to the ACTG175 study further demonstrates the method&#39;s utility in causal discovery and heterogeneity quantification.},
  archive      = {J_SIM},
  author       = {Rongqian Sun and Xinyuan Song},
  doi          = {10.1002/sim.10239},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5497-5512},
  shortjournal = {Stat. Med.},
  title        = {Heterogeneous mediation analysis for cox proportional hazards model with multiple mediators},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modified debiased inverse-variance weighted estimator in
two-sample summary-data mendelian randomization. <em>SIM</em>,
<em>43</em>(29), 5484–5496. (<a
href="https://doi.org/10.1002/sim.10245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization uses genetic variants as instrumental variables to estimate the causal effect of exposure on outcome from observational data. A common challenge in Mendelian randomization is that many genetic variants are only modestly or even weakly associated with the exposure of interest, a setting known as many weak instruments. Conventional methods, such as the popular inverse-variance weighted (IVW) estimator, could be heavily biased toward zero when the instrument strength is weak. To address this issue, the debiased IVW (dIVW) estimator and the penalized IVW (pIVW) estimator, which are shown to be robust to many weak instruments, were recently proposed. However, we find that the dIVW estimator tends to produce an exaggerated estimate of the causal effect, especially when the effective sample size is small. Although the pIVW estimator has better statistical properties, it is slightly more complex, and the idea behind this method is also a bit less intuitive. Therefore, we propose a modified debiased IVW (mdIVW) estimator that directly multiplies a shrinkage factor with the original dIVW estimator. After this simple modification, we prove that the mdIVW estimator not only has second-order bias with respect to the effective sample size, but also has smaller variance and mean squared error than the preceding two estimators. We then extend the proposed method to account for the presence of instrumental variable selection and balanced horizontal pleiotropy. We demonstrate the improvement of the mdIVW estimator over the competing ones through extensive simulation studies and real data analysis.},
  archive      = {J_SIM},
  author       = {Youpeng Su and Siqi Xu and Yilei Ma and Ping Yin and Xingjie Hao and Jiyuan Zhou and Wing Kam Fung and Hongwei Jiang and Peng Wang},
  doi          = {10.1002/sim.10245},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5484-5496},
  shortjournal = {Stat. Med.},
  title        = {A modified debiased inverse-variance weighted estimator in two-sample summary-data mendelian randomization},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New quadratic discriminant analysis algorithms for
correlated audiometric data. <em>SIM</em>, <em>43</em>(29), 5473–5483.
(<a href="https://doi.org/10.1002/sim.10257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paired organs like eyes, ears, and lungs in humans exhibit similarities, and data from these organs often display remarkable correlations. Accounting for these correlations could enhance classification models used in predicting disease phenotypes. To our knowledge, there is limited, if any, literature addressing this topic, and existing methods do not exploit such correlations. For example, the conventional approach treats each ear as an independent observation when predicting audiometric phenotypes and is agnostic about the correlation of data from the two ears of the same person. This approach may lead to information loss and reduce the model performance. In response to this gap, particularly in the context of audiometric phenotype prediction, this paper proposes new quadratic discriminant analysis (QDA) algorithms that appropriately deal with the dependence between ears. We propose two-stage analysis strategies: (1) conducting data transformations to reduce data dimensionality before applying QDA; and (2) developing new QDA algorithms to partially utilize the dependence between phenotypes of two ears. We conducted simulation studies to compare different transformation methods and to assess the performance of different QDA algorithms. The empirical results suggested that the transformation may only be beneficial when the sample size is relatively small. Moreover, our proposed new QDA algorithms performed better than the conventional approach in both person-level and ear-level accuracy. As an illustration, we applied them to audiometric data from the Medical University of South Carolina Longitudinal Cohort Study of Age-related Hearing Loss. In addition, we developed an R package, PairQDA , to implement the proposed algorithms.},
  archive      = {J_SIM},
  author       = {Fuyu Guo and David M. Zucker and Kenneth I. Vaden and Sharon Curhan and Judy R. Dubno and Molin Wang},
  doi          = {10.1002/sim.10257},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5473-5483},
  shortjournal = {Stat. Med.},
  title        = {New quadratic discriminant analysis algorithms for correlated audiometric data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian empirical likelihood regression for semiparametric
estimation of optimal dynamic treatment regimes. <em>SIM</em>,
<em>43</em>(28), 5461–5472. (<a
href="https://doi.org/10.1002/sim.10251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a semiparametric approach to Bayesian modeling of dynamic treatment regimes that is built on a Bayesian likelihood-based regression estimation framework. Methods based on this framework exhibit a probabilistic coherence property that leads to accurate estimation of the optimal dynamic treatment regime. Unlike most Bayesian estimation methods, our proposed method avoids strong distributional assumptions for the intermediate and final outcomes by utilizing empirical likelihoods. Our proposed method allows for either linear, or more flexible forms of mean functions for the stagewise outcomes. A variational Bayes approximation is used for computation to avoid common pitfalls associated with Markov Chain Monte Carlo approaches coupled with empirical likelihood. Through simulations and analysis of the STAR*D sequential randomized trial data, our proposed method demonstrates superior accuracy over Q-learning and parametric Bayesian likelihood-based regression estimation, particularly when the parametric assumptions of regression error distributions may be potentially violated.},
  archive      = {J_SIM},
  author       = {Weichang Yu and Howard Bondell},
  doi          = {10.1002/sim.10251},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5461-5472},
  shortjournal = {Stat. Med.},
  title        = {Bayesian empirical likelihood regression for semiparametric estimation of optimal dynamic treatment regimes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate cluster point process to quantify and explore
multi-entity configurations: Application to biofilm image data.
<em>SIM</em>, <em>43</em>(28), 5446–5460. (<a
href="https://doi.org/10.1002/sim.10261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clusters of similar or dissimilar objects are encountered in many fields. Frequently used approaches treat each cluster&#39;s central object as latent. Yet, often objects of one or more types cluster around objects of another type. Such arrangements are common in biomedical images of cells, in which nearby cell types likely interact. Quantifying spatial relationships may elucidate biological mechanisms. Parent-offspring statistical frameworks can be usefully applied even when central objects (“parents”) differ from peripheral ones (“offspring”). We propose the novel multivariate cluster point process (MCPP) to quantify multi-object (e.g., multi-cellular) arrangements. Unlike commonly used approaches, the MCPP exploits locations of the central parent object in clusters. It accounts for possibly multilayered, multivariate clustering. The model formulation requires specification of which object types function as cluster centers and which reside peripherally. If such information is unknown, the relative roles of object types may be explored by comparing fit of different models via the deviance information criterion (DIC). In simulated data, we compared a series of models&#39; DIC; the MCPP correctly identified simulated relationships. It also produced more accurate and precise parameter estimates than the classical univariate Neyman–Scott process model. We also used the MCPP to quantify proposed configurations and explore new ones in human dental plaque biofilm image data. MCPP models quantified simultaneous clustering of Streptococcus and Porphyromonas around Corynebacterium and of Pasteurellaceae around Streptococcus and successfully captured hypothesized structures for all taxa. Further exploration suggested the presence of clustering between Fusobacterium and Leptotrichia , a previously unreported relationship.},
  archive      = {J_SIM},
  author       = {Suman Majumder and Brent A. Coull and Jessica L. Mark Welch and Patrick J. La Riviere and Floyd E. Dewhirst and Jacqueline R. Starr and Kyu Ha Lee},
  doi          = {10.1002/sim.10261},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5446-5460},
  shortjournal = {Stat. Med.},
  title        = {Multivariate cluster point process to quantify and explore multi-entity configurations: Application to biofilm image data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional principal component analysis for continuous
non-gaussian, truncated, and discrete functional data. <em>SIM</em>,
<em>43</em>(28), 5431–5445. (<a
href="https://doi.org/10.1002/sim.10240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile health studies often collect multiple within-day self-reported assessments of participants&#39; behavior and well-being on different scales such as physical activity (continuous scale), pain levels (truncated scale), mood states (ordinal scale), and the occurrence of daily life events (binary scale). These assessments, when indexed by time of day, can be treated and analyzed as functional data corresponding to their respective types: continuous, truncated, ordinal, and binary. Motivated by these examples, we develop a functional principal component analysis that deals with all four types of functional data in a unified manner. It employs a semiparametric Gaussian copula model, assuming a generalized latent non-paranormal process as the underlying generating mechanism for these four types of functional data. We specify latent temporal dependence using a covariance estimated through Kendall&#39;s bridging method, incorporating smoothness in the bridging process. The approach is then extended with methods for handling both dense and sparse sampling designs, calculating subject-specific latent representations of observed data, latent principal components and principal component scores. Simulation studies demonstrate the method&#39;s competitive performance under both dense and sparse sampling designs. The method is applied to data from 497 participants in the National Institute of Mental Health Family Study of Mood Spectrum Disorders to characterize differences in within-day temporal patterns of mood in individuals with the major mood disorder subtypes, including Major Depressive Disorder and Type 1 and 2 Bipolar Disorder. Software implementation of the proposed method is provided in an R-package.},
  archive      = {J_SIM},
  author       = {Debangan Dey and Rahul Ghosal and Kathleen Merikangas and Vadim Zipunnikov},
  doi          = {10.1002/sim.10240},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5431-5445},
  shortjournal = {Stat. Med.},
  title        = {Functional principal component analysis for continuous non-gaussian, truncated, and discrete functional data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Genetic prediction modeling in large cohort studies via
boosting targeted loss functions. <em>SIM</em>, <em>43</em>(28),
5412–5430. (<a href="https://doi.org/10.1002/sim.10249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polygenic risk scores (PRS) aim to predict a trait from genetic information, relying on common genetic variants with low to medium effect sizes. As genotype data are high-dimensional in nature, it is crucial to develop methods that can be applied to large-scale data (large n $$ n $$ and large p $$ p $$ ). Many PRS tools aggregate univariate summary statistics from genome-wide association studies into a single score. Recent advancements allow simultaneous modeling of variant effects from individual-level genotype data. In this context, we introduced snpboost, an algorithm that applies statistical boosting on individual-level genotype data to estimate PRS via multivariable regression models. By processing variants iteratively in batches, snpboost can deal with large-scale cohort data. Having solved the technical obstacles due to data dimensionality, the methodological scope can now be broadened—focusing on key objectives for the clinical application of PRS. Similar to most methods in this context, snpboost has, so far, been restricted to quantitative and binary traits. Now, we incorporate more advanced alternatives—targeted to the particular aim and outcome. Adapting the loss function extends the snpboost framework to further data situations such as time-to-event and count data. Furthermore, alternative loss functions for continuous outcomes allow us to focus not only on the mean of the conditional distribution but also on other aspects that may be more helpful in the risk stratification of individual patients and can quantify prediction uncertainty, for example, median or quantile regression. This work enhances PRS fitting across multiple model classes previously unfeasible for this data type.},
  archive      = {J_SIM},
  author       = {Hannah Klinkhammer and Christian Staerk and Carlo Maj and Peter M. Krawitz and Andreas Mayr},
  doi          = {10.1002/sim.10249},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5412-5430},
  shortjournal = {Stat. Med.},
  title        = {Genetic prediction modeling in large cohort studies via boosting targeted loss functions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phase i/II design for selecting subgroup-specific optimal
biological doses for prespecified subgroups. <em>SIM</em>,
<em>43</em>(28), 5401–5411. (<a
href="https://doi.org/10.1002/sim.10256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a phase I/II trial design to support dose-finding when the optimal biological dose (OBD) may differ in two prespecified patient subgroups. The proposed design uses a utility function to quantify efficacy-toxicity trade-offs, and a Bayesian model with spike and slab prior distributions for the subgroup effect on toxicity and efficacy to guide dosing and to facilitate identifying either subgroup-specific OBDs or a common OBD depending on the resulting trial data. In a simulation study, we find the proposed design performs nearly as well as a design that ignores subgroups when the dose-toxicity and dose-efficacy relationships are the same in both subgroups, and nearly as well as a design with independent dose-finding within each subgroup when these relationships differ across subgroups. In other words, the proposed adaptive design performs similarly to the design that would be chosen if investigators possessed foreknowledge about whether the dose-toxicity and/or dose-efficacy relationship differs across two prespecified subgroups. Thus, the proposed design may be effective for OBD selection when uncertainty exists about whether the OBD differs in two prespecified subgroups.},
  archive      = {J_SIM},
  author       = {Sydney Porter and Thomas A. Murray and Anne Eaton},
  doi          = {10.1002/sim.10256},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5401-5411},
  shortjournal = {Stat. Med.},
  title        = {Phase I/II design for selecting subgroup-specific optimal biological doses for prespecified subgroups},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference for continuous multiple time point
interventions. <em>SIM</em>, <em>43</em>(28), 5380–5400. (<a
href="https://doi.org/10.1002/sim.10246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are limited options to estimate the treatment effects of variables which are continuous and measured at multiple time points, particularly if the true dose–response curve should be estimated as closely as possible. However, these situations may be of relevance: in pharmacology, one may be interested in how outcomes of people living with—and treated for—HIV, such as viral failure, would vary for time-varying interventions such as different drug concentration trajectories. A challenge for doing causal inference with continuous interventions is that the positivity assumption is typically violated. To address positivity violations, we develop projection functions, which reweigh and redefine the estimand of interest based on functions of the conditional support for the respective interventions. With these functions, we obtain the desired dose–response curve in areas of enough support, and otherwise a meaningful estimand that does not require the positivity assumption. We develop -computation type plug-in estimators for this case. Those are contrasted with g-computation estimators which are applied to continuous interventions without specifically addressing positivity violations, which we propose to be presented with diagnostics. The ideas are illustrated with longitudinal data from HIV positive children treated with an efavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children years in Zambia/Uganda. Simulations show in which situations a standard g-computation approach is appropriate, and in which it leads to bias and how the proposed weighted estimation approach then recovers the alternative estimand of interest.},
  archive      = {J_SIM},
  author       = {Michael Schomaker and Helen McIlleron and Paolo Denti and Iván Díaz},
  doi          = {10.1002/sim.10246},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5380-5400},
  shortjournal = {Stat. Med.},
  title        = {Causal inference for continuous multiple time point interventions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonparametric global win probability approach to the
analysis and sizing of randomized controlled trials with multiple
endpoints of different scales and missing data: Beyond
o’brien–wei–lachin. <em>SIM</em>, <em>43</em>(28), 5366–5379. (<a
href="https://doi.org/10.1002/sim.10247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple primary endpoints are commonly used in randomized controlled trials to assess treatment effects. When the endpoints are measured on different scales, the O&#39;Brien rank-sum test or the Wei–Lachin test for stochastic ordering may be used for hypothesis testing. However, the O&#39;Brien–Wei–Lachin (OWL) approach is unable to handle missing data and adjust for baseline measurements. We present a nonparametric approach for data analysis that encompasses the OWL approach as a special case. Our approach is based on quantifying an endpoint-specific treatment effect using the probability that a participant in the treatment group has a better score than (or a win over) a participant in the control group. The average of the endpoint-specific win probabilities (WinPs), termed the global win probability (gWinP), is used to quantify the global treatment effect, with the null hypothesis gWinP = 0.50. Our approach involves converting the data for each endpoint to endpoint-specific win fractions, and modeling the win fractions using multivariate linear mixed models to obtain estimates of the endpoint-specific WinPs and the associated variance–covariance matrix. Focusing on confidence interval estimation for the gWinP, we derive sample size formulas for clinical trial design. Simulation results demonstrate that our approach performed well in terms of bias, interval coverage percentage, and assurance of achieving a pre-specified precision for the gWinP. Illustrative code for implementing the methods using SAS PROC RANK and PROC MIXED is provided.},
  archive      = {J_SIM},
  author       = {Guangyong Zou and Lily Zou},
  doi          = {10.1002/sim.10247},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5366-5379},
  shortjournal = {Stat. Med.},
  title        = {A nonparametric global win probability approach to the analysis and sizing of randomized controlled trials with multiple endpoints of different scales and missing data: Beyond O&#39;Brien–Wei–Lachin},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Variability in causal effects and noncompliance in a
multisite trial: A bivariate hierarchical generalized random
coefficients model for a binary outcome. <em>SIM</em>, <em>43</em>(28),
5353–5365. (<a href="https://doi.org/10.1002/sim.10229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within each of 170 physicians, patients were randomized to access e-assist, an online program that aimed to increase colorectal cancer screening (CRCS), or control. Compliance was partial: of the experimental patients accessed e-assist while no controls were provided the access. Of interest are the average causal effect of assignment to treatment and the complier average causal effect as well as the variation of these causal effects across physicians. Each physician generates probabilities of screening for experimental compliers (experimental patients who accessed e-assist), control compliers (controls who would have accessed e-assist had they been assigned to e-assist), and never takers (patients who would have avoided e-assist no matter what). Estimating physician-specific probabilities jointly over physicians poses novel challenges. We address these challenges by maximum likelihood, factoring a “complete-data likelihood” uniquely into the conditional distribution of screening and partially observed compliance given random effects and the distribution of random effects. We marginalize this likelihood using adaptive Gauss–Hermite quadrature. The approach is doubly iterative in that the conditional distribution defies analytic evaluation. Because the small sample size per physician constrains estimability of multiple random effects, we reduce their dimensionality using a shared random effects model having a factor analytic structure. We assess estimators and recommend sample sizes to produce reasonably accurate and precise estimates by simulation, and analyze data from a trial of a CRCS intervention.},
  archive      = {J_SIM},
  author       = {Xinxin Sun and Yongyun Shin and Jennifer Elston Lafata and Stephen W. Raudenbush},
  doi          = {10.1002/sim.10229},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5353-5365},
  shortjournal = {Stat. Med.},
  title        = {Variability in causal effects and noncompliance in a multisite trial: A bivariate hierarchical generalized random coefficients model for a binary outcome},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering functional data with measurement errors: A
simulation-based approach. <em>SIM</em>, <em>43</em>(28), 5344–5352. (<a
href="https://doi.org/10.1002/sim.10238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering analysis of functional data, which comprises observations that evolve continuously over time or space, has gained increasing attention across various scientific disciplines. Practical applications often involve functional data that are contaminated with measurement errors arising from imprecise instruments, sampling errors, or other sources. These errors can significantly distort the inherent data structure, resulting in erroneous clustering outcomes. In this article, we propose a simulation-based approach designed to mitigate the impact of measurement errors. Our proposed method estimates the distribution of functional measurement errors through repeated measurements. Subsequently, the clustering algorithm is applied to simulated data generated from the conditional distribution of the unobserved true functional data given the observed contaminated functional data, accounting for the adjustments made to rectify measurement errors. We illustrate through simulations show that the proposed method has improved numerical performance than the naive methods that neglect such errors. Our proposed method was applied to a childhood obesity study, giving more reliable clustering results.},
  archive      = {J_SIM},
  author       = {Tingyu Zhu and Lan Xue and Carmen Tekwe and Keith Diaz and Mark Benden and Roger Zoh},
  doi          = {10.1002/sim.10238},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5344-5352},
  shortjournal = {Stat. Med.},
  title        = {Clustering functional data with measurement errors: A simulation-based approach},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep neural network-based accelerated failure time models
using rank loss. <em>SIM</em>, <em>43</em>(28), 5331–5343. (<a
href="https://doi.org/10.1002/sim.10235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accelerated failure time (AFT) model assumes a log-linear relationship between failure times and a set of covariates. In contrast to other popular survival models that work on hazard functions, the effects of covariates are directly on failure times, the interpretation of which is intuitive. The semiparametric AFT model that does not specify the error distribution is sufficiently flexible and robust to depart from the distributional assumption. Owing to its desirable features, this class of model has been considered a promising alternative to the popular Cox model in the analysis of censored failure time data. However, in these AFT models, a linear predictor for the mean is typically assumed. Little research has addressed the non-linearity of predictors when modeling the mean. Deep neural networks (DNNs) have received much attention over the past few decades and have achieved remarkable success in a variety of fields. DNNs have a number of notable advantages and have been shown to be particularly useful in addressing non-linearity. Here, we propose applying a DNN to fit AFT models using Gehan-type loss combined with a sub-sampling technique. Finite sample properties of the proposed DNN and rank-based AFT model (DeepR-AFT) were investigated via an extensive simulation study. The DeepR-AFT model showed superior performance over its parametric and semiparametric counterparts when the predictor was nonlinear. For linear predictors, DeepR-AFT performed better when the dimensions of the covariates were large. The superior performance of the proposed DeepR-AFT was demonstrated using three real datasets.},
  archive      = {J_SIM},
  author       = {Gwangsu Kim and Jeongho Park and Sangwook Kang},
  doi          = {10.1002/sim.10235},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5331-5343},
  shortjournal = {Stat. Med.},
  title        = {Deep neural network-based accelerated failure time models using rank loss},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing similarity of parametric competing risks models for
identifying potentially similar pathways in healthcare. <em>SIM</em>,
<em>43</em>(28), 5316–5330. (<a
href="https://doi.org/10.1002/sim.10243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of similar patient pathways is a crucial task in healthcare analytics. A flexible tool to address this issue are parametric competing risks models, where transition intensities may be specified by a variety of parametric distributions, thus in particular being possibly time-dependent. We assess the similarity between two such models by examining the transitions between different health states. This research introduces a method to measure the maximum differences in transition intensities over time, leading to the development of a test procedure for assessing similarity. We propose a parametric bootstrap approach for this purpose and provide a proof to confirm the validity of this procedure. The performance of our proposed method is evaluated through a simulation study, considering a range of sample sizes, differing amounts of censoring, and various thresholds for similarity. Finally, we demonstrate the practical application of our approach with a case study from urological clinical routine practice, which inspired this research.},
  archive      = {J_SIM},
  author       = {Kathrin Möllenhoff and Nadine Binder and Holger Dette},
  doi          = {10.1002/sim.10243},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5316-5330},
  shortjournal = {Stat. Med.},
  title        = {Testing similarity of parametric competing risks models for identifying potentially similar pathways in healthcare},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel bayesian spatio-temporal surveillance metric to
predict emerging infectious disease areas of high disease risk.
<em>SIM</em>, <em>43</em>(28), 5300–5315. (<a
href="https://doi.org/10.1002/sim.10227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of areas of high disease risk has been one of the top goals for infectious disease public health surveillance. Accurate prediction of these regions leads to effective resource allocation and faster intervention. This paper proposes a novel prediction surveillance metric based on a Bayesian spatio-temporal model for infectious disease outbreaks. Exceedance probability, which has been commonly used for cluster detection in statistical epidemiology, was extended to predict areas of high risk. The proposed metric consists of three components: the area&#39;s risk profile, temporal risk trend, and spatial neighborhood influence. We also introduce a weighting scheme to balance these three components, which accommodates the characteristics of the infectious disease outbreak, spatial properties, and disease trends. Thorough simulation studies were conducted to identify the optimal weighting scheme and evaluate the performance of the proposed prediction surveillance metric. Results indicate that the area&#39;s own risk and the neighborhood influence play an important role in making a highly sensitive metric, and the risk trend term is important for the specificity and accuracy of prediction. The proposed prediction metric was applied to the COVID-19 case data of South Carolina from March 12, 2020, and the subsequent 30 weeks of data.},
  archive      = {J_SIM},
  author       = {Joanne Kim and Andrew B. Lawson and Brian Neelon and Jeffrey E. Korte and Jan M. Eberth and Gerardo Chowell},
  doi          = {10.1002/sim.10227},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5300-5315},
  shortjournal = {Stat. Med.},
  title        = {A novel bayesian spatio-temporal surveillance metric to predict emerging infectious disease areas of high disease risk},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Does remdesivir lower COVID-19 mortality? A subgroup
analysis of hospitalized adults receiving supplemental oxygen.
<em>SIM</em>, <em>43</em>(28), 5285–5299. (<a
href="https://doi.org/10.1002/sim.10241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first Adaptive COVID-19 Treatment Trial (ACTT-1) showed that remdesivir improved COVID-19 recovery time compared with placebo in hospitalized adults. The secondary outcome of mortality was almost significant overall ( p = 0.07) and highly significant for people receiving supplemental oxygen at enrollment ( p = 0.002), suggesting a mortality benefit concentrated in this group. We explore analysis methods that are helpful when a single subgroup benefits from treatment and apply them to ACTT-1, using baseline oxygen use to define subgroups. We consider two questions: (1) is the remdesivir effect for people receiving supplemental oxygen real, and (2) does this effect differ from the overall effect? For Question 1, we apply a Bonferroni adjustment to subgroup-specific hypothesis tests and the Westfall and Young permutation test, which is valid when small cell counts preclude normally distributed test statistics (a frequently unexamined condition in subgroup analyses). For Question 2, we introduce Q max , the largest standardized difference between subgroup-specific effects and the overall effect. Q max simultaneously tests whether any subgroup effect differs from the overall effect and identifies the subgroup benefitting most. We demonstrate that Q max strongly controls the familywise error rate (FWER) when test statistics are normally distributed with no mean–variance relationship. We compare Q max to a related permutation test, SEAMOS, which was previously proposed but not extensively applied or tested. We show that SEAMOS can have inflated Type 1 error under the global null when control arm event rates differ between subgroups. Our results support a mortality benefit from remdesivir in people receiving supplemental oxygen.},
  archive      = {J_SIM},
  author       = {Gail E. Potter and Michael A. Proschan},
  doi          = {10.1002/sim.10241},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5285-5299},
  shortjournal = {Stat. Med.},
  title        = {Does remdesivir lower COVID-19 mortality? a subgroup analysis of hospitalized adults receiving supplemental oxygen},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling chronic disease mortality by methods from
accelerated life testing. <em>SIM</em>, <em>43</em>(28), 5273–5284. (<a
href="https://doi.org/10.1002/sim.10233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a parametric model for describing chronic disease mortality from cohort data and illustrate its use for Type 2 diabetes. The model uses ideas from accelerated life testing in reliability theory and conceptualizes the occurrence of a chronic disease as putting the observational unit to an enhanced stress level, which is supposed to shorten its lifetime. It further addresses the issue of semi-competing risk, that is, the asymmetry of death and diagnosis of disease, where the disease can be diagnosed before death, but not after. With respect to the cohort structure of the data, late entry into the cohort is taken into account and prevalent as well as incident cases inform the analysis. We finally give an extension of the model that allows age at disease diagnosis to be observed not exactly, but only partially within an interval. Model parameters can be straightforwardly estimated by Maximum Likelihood, using the assumption of a Gompertz distribution we show in a small simulation study that this works well. Data of the Cardiovascular Disease, Living and Ageing in Halle (CARLA) study, a population-based cohort in the city of Halle (Saale) in the eastern part of Germany, are used for illustration.},
  archive      = {J_SIM},
  author       = {Marina Zamsheva and Alexander Kluttig and Andreas Wienke and Oliver Kuss},
  doi          = {10.1002/sim.10233},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5273-5284},
  shortjournal = {Stat. Med.},
  title        = {Modeling chronic disease mortality by methods from accelerated life testing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survival causal rule ensemble method considering the main
effect for estimating heterogeneous treatment effects. <em>SIM</em>,
<em>43</em>(27), 5234–5271. (<a
href="https://doi.org/10.1002/sim.10180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an increasing focus on precision medicine in medical research, numerous studies have been conducted in recent years to clarify the relationship between treatment effects and patient characteristics. The treatment effects for patients with different characteristics are always heterogeneous, and therefore, various heterogeneous treatment effect machine learning estimation methods have been proposed owing to their flexibility and high estimation accuracy. However, most machine learning methods rely on black-box models, preventing direct interpretation of the relationship between patient characteristics and treatment effects. Moreover, most of these studies have focused on continuous or binary outcomes, although survival outcomes are also important in medical research. To address these challenges, we propose a heterogeneous treatment effect estimation method for survival data based on RuleFit, an interpretable machine learning method. Numerical simulation results confirmed that the prediction performance of the proposed method was comparable to that of existing methods. We also applied a dataset from an HIV study, the AIDS Clinical Trials Group Protocol 175 dataset, to illustrate the interpretability of the proposed method using real data. Consequently, the proposed survival causal rule ensemble method provides an interpretable model with sufficient estimation accuracy.},
  archive      = {J_SIM},
  author       = {Ke Wan and Kensuke Tanioka and Toshio Shimokawa},
  doi          = {10.1002/sim.10180},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5234-5271},
  shortjournal = {Stat. Med.},
  title        = {Survival causal rule ensemble method considering the main effect for estimating heterogeneous treatment effects},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A complex meta-regression model to identify effective
features of interventions from multi-arm, multi-follow-up trials.
<em>SIM</em>, <em>43</em>(27), 5217–5233. (<a
href="https://doi.org/10.1002/sim.10237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) combines evidence from multiple trials to compare the effectiveness of a set of interventions. In many areas of research, interventions are often complex, made up of multiple components or features. This makes it difficult to define a common set of interventions on which to perform the analysis. One approach to this problem is component network meta-analysis (CNMA) which uses a meta-regression framework to define each intervention as a subset of components whose individual effects combine additively. In this article, we are motivated by a systematic review of complex interventions to prevent obesity in children. Due to considerable heterogeneity across the trials, these interventions cannot be expressed as a subset of components but instead are coded against a framework of characteristic features. To analyse these data, we develop a bespoke CNMA-inspired model that allows us to identify the most important features of interventions. We define a meta-regression model with covariates on three levels: intervention, study, and follow-up time, as well as flexible interaction terms. By specifying different regression structures for trials with and without a control arm, we relax the assumption from previous CNMA models that a control arm is the absence of intervention components. Furthermore, we derive a correlation structure that accounts for trials with multiple intervention arms and multiple follow-up times. Although, our model was developed for the specifics of the obesity data set, it has wider applicability to any set of complex interventions that can be coded according to a set of shared features.},
  archive      = {J_SIM},
  author       = {Annabel L. Davies and Julian P. T. Higgins},
  doi          = {10.1002/sim.10237},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5217-5233},
  shortjournal = {Stat. Med.},
  title        = {A complex meta-regression model to identify effective features of interventions from multi-arm, multi-follow-up trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging external aggregated information for the marginal
accelerated failure time model. <em>SIM</em>, <em>43</em>(27),
5203–5216. (<a href="https://doi.org/10.1002/sim.10224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is becoming increasingly common for researchers to consider leveraging information from external sources to enhance the analysis of small-scale studies. While much attention has focused on univariate survival data, correlated survival data are prevalent in epidemiological investigations. In this article, we propose a unified framework to improve the estimation of the marginal accelerated failure time model with correlated survival data by integrating additional information given in the form of covariate effects evaluated in a reduced accelerated failure time model. Such auxiliary information can be summarized by using valid estimating equations and hence can then be combined with the internal linear rank-estimating equations via the generalized method of moments. We investigate the asymptotic properties of the proposed estimator and show that it is more efficient than the conventional estimator using internal data only. When population heterogeneity exists, we revise the proposed estimation procedure and present a shrinkage estimator to protect against bias and loss of efficiency. Moreover, the proposed estimation procedure can be further refined to accommodate the non-negligible uncertainty in the auxiliary information, leading to more trustable inference conclusions. Simulation results demonstrate the finite sample performance of the proposed methods, and empirical application on the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial substantiates its practical relevance.},
  archive      = {J_SIM},
  author       = {Ping Xie and Jie Ding and Xiaoguang Wang},
  doi          = {10.1002/sim.10224},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5203-5216},
  shortjournal = {Stat. Med.},
  title        = {Leveraging external aggregated information for the marginal accelerated failure time model},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference over a subpopulation: The effect of malaria
vaccine in women during pregnancy. <em>SIM</em>, <em>43</em>(27),
5193–5202. (<a href="https://doi.org/10.1002/sim.10228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preventing malaria during pregnancy is of critical importance, yet there are no approved malaria vaccines for pregnant women due to lack of efficacy results within this population. Conducting a randomized trial in pregnant women throughout the entire duration of pregnancy is impractical. Instead, a randomized trial was conducted among women of childbearing potential (WOCBP), and some participants became pregnant during the 2-year study. We explore a statistical method for estimating vaccine effect within the target subpopulation—women who can naturally become pregnant, namely, women who can become pregnant under a placebo condition—within the causal inference framework. Two vaccine effect estimators are employed to effectively utilize baseline characteristics and account for the fact that certain baseline characteristics were only available from pregnant participants. The first estimator considers all participants but can only utilize baseline variables collected from the entire participant pool. In contrast, the second estimator, which includes only pregnant participants, utilizes all available baseline information. Both estimators are evaluated numerically through simulation studies and applied to the WOCBP trial to assess vaccine effect against pregnancy malaria.},
  archive      = {J_SIM},
  author       = {Zonghui Hu and Dean Follmann},
  doi          = {10.1002/sim.10228},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5193-5202},
  shortjournal = {Stat. Med.},
  title        = {Causal inference over a subpopulation: The effect of malaria vaccine in women during pregnancy},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining biomarkers to improve diagnostic accuracy in
detecting diseases with group-tested data. <em>SIM</em>,
<em>43</em>(27), 5182–5192. (<a
href="https://doi.org/10.1002/sim.10230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of combining multiple biomarkers to improve the diagnostic accuracy of detecting a disease when only group-tested data on the disease status are available. There are several challenges in addressing this problem, including unavailable individual disease statuses, differential misclassification depending on group size and number of diseased individuals in the group, and extensive computation due to a large number of possible combinations of multiple biomarkers. To tackle these issues, we propose a pairwise model fitting approach to estimating the distribution of the optimal linear combination of biomarkers and its diagnostic accuracy under the assumption of a multivariate normal distribution. The approach is evaluated in simulation studies and applied to data on chlamydia detection and COVID-19 diagnosis.},
  archive      = {J_SIM},
  author       = {Jin Yang and Wei Zhang and Paul S. Albert and Aiyi Liu and Zhen Chen},
  doi          = {10.1002/sim.10230},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5182-5192},
  shortjournal = {Stat. Med.},
  title        = {Combining biomarkers to improve diagnostic accuracy in detecting diseases with group-tested data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating time-varying exposure effects through
continuous-time modelling in mendelian randomization. <em>SIM</em>,
<em>43</em>(27), 5166–5181. (<a
href="https://doi.org/10.1002/sim.10222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization is an instrumental variable method that utilizes genetic information to investigate the causal effect of a modifiable exposure on an outcome. In most cases, the exposure changes over time. Understanding the time-varying causal effect of the exposure can yield detailed insights into mechanistic effects and the potential impact of public health interventions. Recently, a growing number of Mendelian randomization studies have attempted to explore time-varying causal effects. However, the proposed approaches oversimplify temporal information and rely on overly restrictive structural assumptions, limiting their reliability in addressing time-varying causal problems. This article considers a novel approach to estimate time-varying effects through continuous-time modelling by combining functional principal component analysis and weak-instrument-robust techniques. Our method effectively utilizes available data without making strong structural assumptions and can be applied in general settings where the exposure measurements occur at different timepoints for different individuals. We demonstrate through simulations that our proposed method performs well in estimating time-varying effects and provides reliable inference when the time-varying effect form is correctly specified. The method could theoretically be used to estimate arbitrarily complex time-varying effects. However, there is a trade-off between model complexity and instrument strength. Estimating complex time-varying effects requires instruments that are unrealistically strong. We illustrate the application of this method in a case study examining the time-varying effects of systolic blood pressure on urea levels.},
  archive      = {J_SIM},
  author       = {Haodong Tian and Ashish Patel and Stephen Burgess},
  doi          = {10.1002/sim.10222},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5166-5181},
  shortjournal = {Stat. Med.},
  title        = {Estimating time-varying exposure effects through continuous-time modelling in mendelian randomization},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regression approaches to assess effect of treatments that
arrest progression of symptoms. <em>SIM</em>, <em>43</em>(27),
5155–5165. (<a href="https://doi.org/10.1002/sim.10219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a small sample example in neonatal onset multisystem inflammatory disease (NOMID), we propose a method that can be used when the interest is testing for an association between a changes in disease progression with start of treatment compared to historical disease progression prior to treatment. Our method estimates the longitudinal trajectory of the outcome variable and adds an interaction term between an intervention indicator variable and the time since initiation of the intervention. This method is appropriate for a situation in which the intervention slows or arrests the effect of the disease on the outcome, as is the case in our motivating example. By simulation in small samples and restricted sets of treatment initiation times, we show that the generalized estimating equations (GEE) formulation with small sample adjustments can bound the Type I error rate better than GEE and linear mixed models without small sample adjustments. Permutation tests (permuting the time of treatment initiation) is another valid approach that can also be useful. We illustrate the methodology through an application to a prospective cohort of NOMID patients enrolled at the NIH clinical center.},
  archive      = {J_SIM},
  author       = {Ana M. Ortega-Villa and Martha C. Nason and Michael P. Fay and Sara Alehashemi and Raphaela Goldbach-Mansky and Dean A. Follmann},
  doi          = {10.1002/sim.10219},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5155-5165},
  shortjournal = {Stat. Med.},
  title        = {Regression approaches to assess effect of treatments that arrest progression of symptoms},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pairwise accelerated failure time regression models for
infectious disease transmission in close-contact groups with external
sources of infection. <em>SIM</em>, <em>43</em>(27), 5138–5154. (<a
href="https://doi.org/10.1002/sim.10226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many important questions in infectious disease epidemiology involve associations between covariates (e.g., age or vaccination status) and infectiousness or susceptibility. Because disease transmission produces dependent outcomes, these questions are difficult or impossible to address using standard regression models from biostatistics. Pairwise survival analysis handles dependent outcomes by calculating likelihoods in terms of contact interval distributions in ordered pairs of individuals. The contact interval in the ordered pair is the time from the onset of infectiousness in to infectious contact from to , where an infectious contact is sufficient to infect if they are susceptible. Here, we introduce a pairwise accelerated failure time regression model for infectious disease transmission that allows the rate parameter of the contact interval distribution to depend on individual-level infectiousness covariates for , individual-level susceptibility covariates for , and pair-level covariates (e.g., type of relationship). This model can simultaneously handle internal infections (caused by transmission between individuals under observation) and external infections (caused by environmental or community sources of infection). We show that this model produces consistent and asymptotically normal parameter estimates. In a simulation study, we evaluate bias and confidence interval coverage probabilities, explore the role of epidemiologic study design, and investigate the effects of model misspecification. We use this regression model to analyze household data from Los Angeles County during the 2009 influenza A (H1N1) pandemic, where we find that the ability to account for external sources of infection increases the statistical power to estimate the effect of antiviral prophylaxis.},
  archive      = {J_SIM},
  author       = {Yushuf Sharker and Zaynab Diallo and Wasiur R. KhudaBukhsh and Eben Kenah},
  doi          = {10.1002/sim.10226},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5138-5154},
  shortjournal = {Stat. Med.},
  title        = {Pairwise accelerated failure time regression models for infectious disease transmission in close-contact groups with external sources of infection},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent archetypes of the spatial patterns of cancer.
<em>SIM</em>, <em>43</em>(27), 5115–5137. (<a
href="https://doi.org/10.1002/sim.10232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cancer atlas edited by several countries is the main resource for the analysis of the geographic variation of cancer risk. Correlating the observed spatial patterns with known or hypothesized risk factors is time-consuming work for epidemiologists who need to deal with each cancer separately, breaking down the patterns according to sex and race. The recent literature has proposed to study more than one cancer simultaneously looking for common spatial risk factors. However, this previous work has two constraints: they consider only a very small (2–4) number of cancers previously known to share risk factors. In this article, we propose an exploratory method to search for latent spatial risk factors of a large number of supposedly unrelated cancers. The method is based on the singular value decomposition and nonnegative matrix factorization, it is computationally efficient, scaling easily with the number of regions and cancers. We carried out a simulation study to evaluate the method&#39;s performance and apply it to cancer atlas from the USA, England, France, Australia, Spain, and Brazil. We conclude that with very few latent maps, which can represent a reduction of up to 90% of atlas maps, most of the spatial variability is conserved. By concentrating on the epidemiological analysis of these few latent maps a substantial amount of work is saved and, at the same time, high-level explanations affecting many cancers simultaneously can be reached.},
  archive      = {J_SIM},
  author       = {Thaís Pacheco Menezes and Marcos Oliveira Prates and Renato Assunção and Mônica Silva Monteiro De Castro},
  doi          = {10.1002/sim.10232},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5115-5137},
  shortjournal = {Stat. Med.},
  title        = {Latent archetypes of the spatial patterns of cancer},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted expectile regression neural networks for right
censored data. <em>SIM</em>, <em>43</em>(27), 5100–5114. (<a
href="https://doi.org/10.1002/sim.10221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a favorable alternative to the censored quantile regression, censored expectile regression has been popular in survival analysis due to its flexibility in modeling the heterogeneous effect of covariates. The existing weighted expectile regression (WER) method assumes that the censoring variable and covariates are independent, and that the covariates effects has a global linear structure. However, these two assumptions are too restrictive to capture the complex and nonlinear pattern of the underlying covariates effects. In this article, we developed a novel weighted expectile regression neural networks (WERNN) method by incorporating the deep neural network structure into the censored expectile regression framework. To handle the random censoring, we employ the inverse probability of censoring weighting (IPCW) technique in the expectile loss function. The proposed WERNN method is flexible enough to fit nonlinear patterns and therefore achieves more accurate prediction performance than the existing WER method for right censored data. Our findings are supported by extensive Monte Carlo simulation studies and a real data application.},
  archive      = {J_SIM},
  author       = {Feipeng Zhang and Xi Chen and Peng Liu and Caiyun Fan},
  doi          = {10.1002/sim.10221},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5100-5114},
  shortjournal = {Stat. Med.},
  title        = {Weighted expectile regression neural networks for right censored data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A precision mixture risk model to identify adverse drug
events in subpopulations using a case-crossover design. <em>SIM</em>,
<em>43</em>(27), 5088–5099. (<a
href="https://doi.org/10.1002/sim.10216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the success of pharmacovigilance studies in detecting signals of adverse drug events (ADEs) from real-world data, the risks of ADEs in subpopulations warrant increased scrutiny to prevent them in vulnerable individuals. Recently, the case-crossover design has been implemented to leverage large-scale administrative claims data for ADE detection, while controlling both observed confounding effects and short-term fixed unobserved confounding effects. Additionally, as the case-crossover design only includes cases, subpopulations can be conveniently derived. In this manuscript, we propose a precision mixture risk model (PMRM) to identify ADE signals from subpopulations under the case-crossover design. The proposed model is able to identify signals from all ADE-subpopulation-drug combinations, while controlling for false discovery rate (FDR) and confounding effects. We applied the PMRM to an administrative claims data. We identified ADE signals in subpopulations defined by demographic variables, comorbidities, and detailed diagnosis codes. Interestingly, certain drugs were associated with a higher risk of ADE only in subpopulations, while these drugs had a neutral association with ADE in the general population. Additionally, the PMRM could control FDR at a desired level and had a higher probability to detect true ADE signals than the widely used McNemar&#39;s test. In conclusion, the PMRM is able to identify subpopulation-specific ADE signals from a tremendous number of ADE-subpopulation-drug combinations, while controlling for both FDR and confounding effects.},
  archive      = {J_SIM},
  author       = {Yi Shi and Michael T. Eadon and Yao Chen and Anna Sun and Yuedi Yang and Chienwei Chiang and Macarius Donneyong and Jing Su and Pengyue Zhang},
  doi          = {10.1002/sim.10216},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5088-5099},
  shortjournal = {Stat. Med.},
  title        = {A precision mixture risk model to identify adverse drug events in subpopulations using a case-crossover design},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric biomarker based treatment selection with
reproducibility data. <em>SIM</em>, <em>43</em>(27), 5077–5087. (<a
href="https://doi.org/10.1002/sim.10218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider evaluating biomarkers for treatment selection under assay modification. Survival outcome, treatment, and Affymetrix gene expression data were attained from cancer patients. Consider migrating a gene expression biomarker to the Illumina platform. A recent novel approach allows a quick evaluation of the migrated biomarker with only a reproducibility study needed to compare the two platforms, achieved by treating the original biomarker as an error-contaminated observation of the migrated biomarker. However, its assumptions of a classical measurement error model and a linear predictor for the outcome may not hold. Ignoring such model deviations may lead to sub-optimal treatment selection or failure to identify effective biomarkers. To overcome such limitations, we adopt a nonparametric logistic regression to model the relationship between the event rate and the biomarker, and the deduced marker-based treatment selection is optimal. We further assume a nonparametric relationship between the migrated and original biomarkers and show that the error-contaminated biomarker leads to sub-optimal treatment selection compared to the error-free biomarker. We obtain the estimation via B-spline approximation. The approach is assessed by simulation studies and demonstrated through application to lung cancer data.},
  archive      = {J_SIM},
  author       = {Sara Byers and Xiao Song},
  doi          = {10.1002/sim.10218},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5077-5087},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric biomarker based treatment selection with reproducibility data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotic confidence interval, sample size formulas and
comparison test for the agreement intra-class correlation coefficient in
inter-rater reliability studies. <em>SIM</em>, <em>43</em>(27),
5060–5076. (<a href="https://doi.org/10.1002/sim.10217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The agreement intra-class correlation coefficient (ICCa) is a suitable statistical index for inter-rater reliability studies. With balanced Gaussian data, we prove the explicit form of ICCa asymptotic normality (ASN), valid both with analysis of variance (ANOVA), maximum likelihood (ML), or restricted ML (REML) estimates. An asymptotic confidence interval is then derived and its performances are examined by simulation compared to the most commonly used methods, under small, moderate and large sample size designs. Then, we deduce sample size calculation formulas, for the number of subjects and observers needed, to achieve a desired confidence interval width or an acceptable ICCa value test power and give concrete examples of their use. Finally, we propose a likelihood ratio test (LRT) to compare two ICCa&#39;s from two distinct subpopulations of patients (or raters) and study by simulation its first order risk and power properties. These methods are illustrated using data from two inter-rater reliability studies, one in physiotherapy with 42 patients and 10 raters and the second in neonatology with 80 subjects and 14 raters. In conclusion, we made recommendations to employ the proposed confidence interval for medium to large samples combined with the quantification of the minimal required sample size at the planning step, or the posterior-power at the analysis step, using simple dedicated formulas. Furthermore, with sufficient sizes, the proposed LRT seems suitable to compare inter-rater reliability between two patient subpopulations. Used wisely, this proposed methods toolbox can remedy common current issues in inter-rater reliability studies.},
  archive      = {J_SIM},
  author       = {Abderrahmane Bourredjem and Hervé Cardot and Hervé Devilliers},
  doi          = {10.1002/sim.10217},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5060-5076},
  shortjournal = {Stat. Med.},
  title        = {Asymptotic confidence interval, sample size formulas and comparison test for the agreement intra-class correlation coefficient in inter-rater reliability studies},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group response-adaptive randomization with delayed and
missing responses. <em>SIM</em>, <em>43</em>(27), 5047–5059. (<a
href="https://doi.org/10.1002/sim.10220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Response-adaptive randomization (RAR) procedures have been extensively studied in the literature, but most of the procedures rely on updating the randomization after each response, which is impractical in many clinical trials. In this article, we propose a new family of RAR procedures that dynamically update based on the responses of a group of individuals, either when available or at fixed time intervals (weekly or biweekly). We show that the proposed design retains the essential theoretical properties of Hu and Zhang&#39;s doubly adaptive biased coin designs (DBCD), and performs well in scenarios involving delayed and randomly missing responses. Numerical studies have been conducted to demonstrate that the new proposed group doubly adaptive biased coin design has similar properties to the Hu and Zhang&#39;s DBCDs in different situations. We also apply the new design to a real clinical trial, highlighting its advantages and practicality. Our findings open the door to studying the properties of other group response adaptive designs, such as urn models, and facilitate the application of response-adaptive randomized clinical trials in practice.},
  archive      = {J_SIM},
  author       = {Guannan Zhai and Yang Li and Lixin Zhang and Feifang Hu},
  doi          = {10.1002/sim.10220},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5047-5059},
  shortjournal = {Stat. Med.},
  title        = {Group response-adaptive randomization with delayed and missing responses},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Response to nadarajah. <em>SIM</em>, <em>43</em>(26), 5046.
(<a href="https://doi.org/10.1002/sim.10234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Leonhard Held},
  doi          = {10.1002/sim.10234},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5046},
  shortjournal = {Stat. Med.},
  title        = {Response to nadarajah},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Letter to the editors. <em>SIM</em>, <em>43</em>(26),
5043–5045. (<a href="https://doi.org/10.1002/sim.10159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Saralees Nadarajah},
  doi          = {10.1002/sim.10159},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5043-5045},
  shortjournal = {Stat. Med.},
  title        = {Letter to the editors},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Beyond the two-trials rule. <em>SIM</em>, <em>43</em>(26),
5023–5042. (<a href="https://doi.org/10.1002/sim.10055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-trials rule for drug approval requires “at least two adequate and well-controlled studies, each convincing on its own, to establish effectiveness.” This is usually implemented by requiring two significant pivotal trials and is the standard regulatory requirement to provide evidence for a new drug&#39;s efficacy. However, there is need to develop suitable alternatives to this rule for a number of reasons, including the possible availability of data from more than two trials. I consider the case of up to three studies and stress the importance to control the partial Type-I error rate, where only some studies have a true null effect, while maintaining the overall Type-I error rate of the two-trials rule, where all studies have a null effect. Some less-known P $$ P $$ -value combination methods are useful to achieve this: Pearson&#39;s method, Edgington&#39;s method and the recently proposed harmonic mean -test. I study their properties and discuss how they can be extended to a sequential assessment of success while still ensuring overall Type-I error control. I compare the different methods in terms of partial Type-I error rate, project power and the expected number of studies required. Edgington&#39;s method is eventually recommended as it is easy to implement and communicate, has only moderate partial Type-I error rate inflation but substantially increased project power.},
  archive      = {J_SIM},
  author       = {Leonhard Held},
  doi          = {10.1002/sim.10055},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5023-5042},
  shortjournal = {Stat. Med.},
  title        = {Beyond the two-trials rule},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trivariate joint modeling for family data with longitudinal
counts, recurrent events and a terminal event with application to lynch
syndrome. <em>SIM</em>, <em>43</em>(26), 5000–5022. (<a
href="https://doi.org/10.1002/sim.10210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trivariate joint modeling for longitudinal count data, recurrent events, and a terminal event for family data has increased interest in medical studies. For example, families with Lynch syndrome (LS) are at high risk of developing colorectal cancer (CRC), where the number of polyps and the frequency of colonoscopy screening visits are highly associated with the risk of CRC among individuals and families. To assess how screening visits influence polyp detection, which in turn influences time to CRC, we propose a clustered trivariate joint model. The proposed model facilitates longitudinal count data that are zero-inflated and over-dispersed and invokes individual-specific and family-specific random effects to account for dependence among individuals and families. We formulate our proposed model as a latent Gaussian model to use the Bayesian estimation approach with the integrated nested Laplace approximation algorithm and evaluate its performance using simulation studies. Our trivariate joint model is applied to a series of 18 families from Newfoundland, with the occurrence of CRC taken as the terminal event, the colonoscopy screening visits as recurrent events, and the number of polyps detected at each visit as zero-inflated count data with overdispersion. We showed that our trivariate model fits better than alternative bivariate models and that the cluster effects should not be ignored when analyzing family data. Finally, the proposed model enables us to quantify heterogeneity across families and individuals in polyp detection and CRC risk, thus helping to identify individuals and families who would benefit from more intensive screening visits.},
  archive      = {J_SIM},
  author       = {Jingwei Lu and Grace Y. Yi and Denis Rustand and Patrick Parfrey and Laurent Briollais and Yun-hee Choi},
  doi          = {10.1002/sim.10210},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5000-5022},
  shortjournal = {Stat. Med.},
  title        = {Trivariate joint modeling for family data with longitudinal counts, recurrent events and a terminal event with application to lynch syndrome},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing the implementation challenge of risk prediction
model due to missing risk factors: The submodel approximation approach.
<em>SIM</em>, <em>43</em>(26), 4984–4999. (<a
href="https://doi.org/10.1002/sim.10184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical prediction models have been widely acknowledged as informative tools providing evidence-based support for clinical decision making. However, prediction models are often underused in clinical practice due to many reasons including missing information upon real-time risk calculation in electronic health records (EHR) system. Existing literature to address this challenge focuses on statistical comparison of various approaches while overlooking the feasibility of their implementation in EHR. In this article, we propose a novel and feasible submodel approach to address this challenge for prediction models developed using the model approximation (also termed “preconditioning”) method. The proposed submodel coefficients are equivalent to the corresponding original prediction model coefficients plus a correction factor. Comprehensive simulations were conducted to assess the performance of the proposed method and compared with the existing “one-step-sweep” approach as well as the imputation approach. In general, the simulation results show the preconditioning-based submodel approach is robust to various heterogeneity scenarios and is comparable to the imputation-based approach, while the “one-step-sweep” approach is less robust under certain heterogeneity scenarios. The proposed method was applied to facilitate real-time implementation of a prediction model to identify emergency department patients with acute heart failure who can be safely discharged home.},
  archive      = {J_SIM},
  author       = {Tianyi Sun and Allison B. McCoy and Alan B. Storrow and Dandan Liu},
  doi          = {10.1002/sim.10184},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4984-4999},
  shortjournal = {Stat. Med.},
  title        = {Addressing the implementation challenge of risk prediction model due to missing risk factors: The submodel approximation approach},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The spike-and-slab quantile LASSO for robust variable
selection in cancer genomics studies. <em>SIM</em>, <em>43</em>(26),
4928–4983. (<a href="https://doi.org/10.1002/sim.10196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data irregularity in cancer genomics studies has been widely observed in the form of outliers and heavy-tailed distributions in the complex traits. In the past decade, robust variable selection methods have emerged as powerful alternatives to the nonrobust ones to identify important genes associated with heterogeneous disease traits and build superior predictive models. In this study, to keep the remarkable features of the quantile LASSO and fully Bayesian regularized quantile regression while overcoming their disadvantage in the analysis of high-dimensional genomics data, we propose the spike-and-slab quantile LASSO through a fully Bayesian spike-and-slab formulation under the robust likelihood by adopting the asymmetric Laplace distribution (ALD). The proposed robust method has inherited the prominent properties of selective shrinkage and self-adaptivity to the sparsity pattern from the spike-and-slab LASSO (Roc̆ková and George, J Am Stat Associat , 2018, 113(521): 431–444). Furthermore, the spike-and-slab quantile LASSO has a computational advantage to locate the posterior modes via soft-thresholding rule guided Expectation-Maximization (EM) steps in the coordinate descent framework, a phenomenon rarely observed for robust regularization with nondifferentiable loss functions. We have conducted comprehensive simulation studies with a variety of heavy-tailed errors in both homogeneous and heterogeneous model settings to demonstrate the superiority of the spike-and-slab quantile LASSO over its competing methods. The advantage of the proposed method has been further demonstrated in case studies of the lung adenocarcinomas (LUAD) and skin cutaneous melanoma (SKCM) data from The Cancer Genome Atlas (TCGA).},
  archive      = {J_SIM},
  author       = {Yuwen Liu and Jie Ren and Shuangge Ma and Cen Wu},
  doi          = {10.1002/sim.10196},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4928-4983},
  shortjournal = {Stat. Med.},
  title        = {The spike-and-slab quantile LASSO for robust variable selection in cancer genomics studies},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selection of number of clusters and warping penalty in
clustering functional electrocardiogram. <em>SIM</em>, <em>43</em>(26),
4913–4927. (<a href="https://doi.org/10.1002/sim.10192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering functional data aims to identify unique functional patterns in the entire domain, but this can be challenging due to phase variability that distorts the observed patterns. Curve registration can be used to remove this variability, but determining the appropriate level of warping flexibility can be complicated. Curve registration also requires a target to which a functional object is aligned, typically the cross-sectional mean of functional objects within the same cluster. However, this mean is unknown prior to clustering. Furthermore, there is a trade-off between flexible warping and the number of resulting clusters. Removing more phase variability through curve registration can lead to fewer remaining variations in the functional data, resulting in a smaller number of clusters. Thus, the optimal number of clusters and warping flexibility cannot be uniquely identified. We propose to use external information to solve the identification issue. We define a cross validated Kullback-Leibler information criterion to select the number of clusters and the warping penalty. The criterion is derived from the predictive classification likelihood considering the joint distribution of both the functional data and external variable and penalizes the uncertainty in the cluster membership. We evaluate our method through simulation and apply it to electrocardiographic data collected in the Chronic Renal Insufficiency Cohort study. We identify two distinct clusters of electrocardiogram (ECG) profiles, with the second cluster exhibiting ST segment depression, an indication of cardiac ischemia, compared to the normal ECG profiles in the first cluster.},
  archive      = {J_SIM},
  author       = {Wei Yang and Harold I. Feldman and Wensheng Guo},
  doi          = {10.1002/sim.10192},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4913-4927},
  shortjournal = {Stat. Med.},
  title        = {Selection of number of clusters and warping penalty in clustering functional electrocardiogram},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional principal component analysis as an alternative to
mixed-effect models for describing sparse repeated measures in presence
of missing data. <em>SIM</em>, <em>43</em>(26), 4899–4912. (<a
href="https://doi.org/10.1002/sim.10214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing longitudinal data in health studies is challenging due to sparse and error-prone measurements, strong within-individual correlation, missing data and various trajectory shapes. While mixed-effect models (MM) effectively address these challenges, they remain parametric models and may incur computational costs. In contrast, functional principal component analysis (FPCA) is a non-parametric approach developed for regular and dense functional data that flexibly describes temporal trajectories at a potentially lower computational cost. This article presents an empirical simulation study evaluating the behavior of FPCA with sparse and error-prone repeated measures and its robustness under different missing data schemes in comparison with MM. The results show that FPCA is well-suited in the presence of missing at random data caused by dropout, except in scenarios involving most frequent and systematic dropout. Like MM, FPCA fails under missing not at random mechanism. The FPCA was applied to describe the trajectories of four cognitive functions before clinical dementia and contrast them with those of matched controls in a case-control study nested in a population-based aging cohort. The average cognitive declines of future dementia cases showed a sudden divergence from those of their matched controls with a sharp acceleration 5 to 2.5 years prior to diagnosis.},
  archive      = {J_SIM},
  author       = {Corentin Ségalas and Catherine Helmer and Robin Genuer and Cécile Proust-Lima},
  doi          = {10.1002/sim.10214},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4899-4912},
  shortjournal = {Stat. Med.},
  title        = {Functional principal component analysis as an alternative to mixed-effect models for describing sparse repeated measures in presence of missing data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mediation analysis with multiple exposures and multiple
mediators. <em>SIM</em>, <em>43</em>(25), 4887–4898. (<a
href="https://doi.org/10.1002/sim.10215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mediation analysis approach is proposed for multiple exposures, multiple mediators, and a continuous scalar outcome under the linear structural equation modeling framework. It assumes that there exist orthogonal components that demonstrate parallel mediation mechanisms on the outcome, and thus is named principal component mediation analysis (PCMA). Likelihood-based estimators are introduced for simultaneous estimation of the component projections and effect parameters. The asymptotic distribution of the estimators is derived for low-dimensional data. A bootstrap procedure is introduced for inference. Simulation studies illustrate the superior performance of the proposed approach. Applied to a proteomics-imaging dataset from the Alzheimer&#39;s disease neuroimaging initiative (ADNI), the proposed framework identifies protein deposition – brain atrophy – memory deficit mechanisms consistent with existing knowledge and suggests potential AD pathology by integrating data collected from different modalities.},
  archive      = {J_SIM},
  author       = {Yi Zhao},
  doi          = {10.1002/sim.10215},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4887-4898},
  shortjournal = {Stat. Med.},
  title        = {Mediation analysis with multiple exposures and multiple mediators},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximate maximum likelihood estimation in cure models
using aggregated data, with application to HPV vaccine completion.
<em>SIM</em>, <em>43</em>(25), 4872–4886. (<a
href="https://doi.org/10.1002/sim.10174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research into vaccine hesitancy is a critical component of the public health enterprise, as rates of communicable diseases preventable by routine childhood immunization have been increasing in recent years. It is therefore important to estimate proportions of “never-vaccinators” in various subgroups of the population in order to successfully target interventions to improve childhood vaccination rates. However, due to privacy issues, it may be difficult to obtain individual patient data (IPD) needed to perform the appropriate time-to-event analyses: state-level immunization information services may only be willing to share aggregated data with researchers. We propose statistical methodology for the analysis of aggregated survival data that can accommodate a cured fraction based on a polynomial approximation of the mixture cure model log-likelihood function relying only on summary statistics. We study the performance of the method through simulation studies and apply it to a real-world data set from a study examining reminder/recall approaches to improve human papillomavirus (HPV) vaccination uptake. The proposed methods may be generalized for use when there is interest in fitting complex likelihood-based models but IPD is unavailable due to data privacy or other concerns.},
  archive      = {J_SIM},
  author       = {John D. Rice and Allison Kempe},
  doi          = {10.1002/sim.10174},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4872-4886},
  shortjournal = {Stat. Med.},
  title        = {Approximate maximum likelihood estimation in cure models using aggregated data, with application to HPV vaccine completion},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing the performance of machine learning methods
trained on public health observational data: A case study from COVID-19.
<em>SIM</em>, <em>43</em>(25), 4861–4871. (<a
href="https://doi.org/10.1002/sim.10211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From early in the coronavirus disease 2019 (COVID-19) pandemic, there was interest in using machine learning methods to predict COVID-19 infection status based on vocal audio signals, for example, cough recordings. However, early studies had limitations in terms of data collection and of how the performances of the proposed predictive models were assessed. This article describes how these limitations have been overcome in a study carried out by the Turing-RSS Health Data Laboratory and the UK Health Security Agency. As part of the study, the UK Health Security Agency collected a dataset of acoustic recordings, SARS-CoV-2 infection status and extensive study participant meta-data. This allowed us to rigorously assess state-of-the-art machine learning techniques to predict SARS-CoV-2 infection status based on vocal audio signals. The lessons learned from this project should inform future studies on statistical evaluation methods to assess the performance of machine learning techniques for public health tasks.},
  archive      = {J_SIM},
  author       = {Davide Pigoli and Kieran Baker and Jobie Budd and Lorraine Butler and Harry Coppock and Sabrina Egglestone and Steven G. Gilmour and Chris Holmes and David Hurley and Radka Jersakova and Ivan Kiskin and Vasiliki Koutra and Jonathon Mellor and George Nicholson and Joe Packham and Selina Patel and Richard Payne and Stephen J. Roberts and Björn W. Schuller and Ana Tendero-Cañadas and Tracey Thornley and Alexander Titcomb},
  doi          = {10.1002/sim.10211},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4861-4871},
  shortjournal = {Stat. Med.},
  title        = {Assessing the performance of machine learning methods trained on public health observational data: A case study from COVID-19},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A causal mediation approach to account for interaction of
treatment and intercurrent events: Using hypothetical strategy.
<em>SIM</em>, <em>43</em>(25), 4850–4860. (<a
href="https://doi.org/10.1002/sim.10212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypothetical strategy is a common strategy for handling intercurrent events (IEs). No current guideline or study considers treatment–IE interaction to target the estimand in any one IE-handling strategy. Based on the hypothetical strategy, we aimed to (1) assess the performance of three estimators with different considerations for the treatment–IE interaction in a simulation and (2) compare the estimation of these estimators in a real trial. Simulation data were generalized based on realistic clinical trials of Alzheimer&#39;s disease. The estimand of interest was the effect of treatment with no IE occurring under the hypothetical strategy. Three estimators, namely, G-estimation with and without interaction and IE-ignored estimation, were compared in scenarios where the treatment–IE interaction effect was set as −50% to 50% of the main effect. Bias was the key performance measure. The real case was derived from a randomized trial of methadone maintenance treatment. Only G-estimation with interaction exhibited unbiased estimations regardless of the existence, direction or magnitude of the treatment–IE interaction in those scenarios. Neglecting the interaction and ignoring the IE would introduce a bias as large as 0.093 and 0.241 (true value, −1.561) if the interaction effect existed. In the real case, compared with G-estimation with interaction, G-estimation without interaction and IE-ignored estimation increased the estimand of interest by 33.55% and 34.36%, respectively. This study highlights the importance of considering treatment–IE interaction in the estimand framework. In practice, it would be better to include the interaction in the estimator by default.},
  archive      = {J_SIM},
  author       = {Kunpeng Wu and Xiangliang Zhang and Meng Zheng and Jianghui Zhang and Wen Chen},
  doi          = {10.1002/sim.10212},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4850-4860},
  shortjournal = {Stat. Med.},
  title        = {A causal mediation approach to account for interaction of treatment and intercurrent events: Using hypothetical strategy},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional overdispersed generalized factor model with
application to single-cell sequencing data analysis. <em>SIM</em>,
<em>43</em>(25), 4836–4849. (<a
href="https://doi.org/10.1002/sim.10213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current high-dimensional linear factor models fail to account for the different types of variables, while high-dimensional nonlinear factor models often overlook the overdispersion present in mixed-type data. However, overdispersion is prevalent in practical applications, particularly in fields like biomedical and genomics studies. To address this practical demand, we propose an overdispersed generalized factor model (OverGFM) for performing high-dimensional nonlinear factor analysis on overdispersed mixed-type data. Our approach incorporates an additional error term to capture the overdispersion that cannot be accounted for by factors alone. However, this introduces significant computational challenges due to the involvement of two high-dimensional latent random matrices in the nonlinear model. To overcome these challenges, we propose a novel variational EM algorithm that integrates Laplace and Taylor approximations. This algorithm provides iterative explicit solutions for the complex variational parameters and is proven to possess excellent convergence properties. We also develop a criterion based on the singular value ratio to determine the optimal number of factors. Numerical results demonstrate the effectiveness of this criterion. Through comprehensive simulation studies, we show that OverGFM outperforms state-of-the-art methods in terms of estimation accuracy and computational efficiency. Furthermore, we demonstrate the practical merit of our method through its application to two datasets from genomics. To facilitate its usage, we have integrated the implementation of OverGFM into the R package GFM .},
  archive      = {J_SIM},
  author       = {Jinyu Nie and Zhilong Qin and Wei Liu},
  doi          = {10.1002/sim.10213},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4836-4849},
  shortjournal = {Stat. Med.},
  title        = {High-dimensional overdispersed generalized factor model with application to single-cell sequencing data analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performance of mixed effects models and generalized
estimating equations for continuous outcomes in partially clustered
trials including both independent and paired data. <em>SIM</em>,
<em>43</em>(25), 4819–4835. (<a
href="https://doi.org/10.1002/sim.10201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clinical trials involve partially clustered data, where some observations belong to a cluster and others can be considered independent. For example, neonatal trials may include infants from single or multiple births. Sample size and analysis methods for these trials have received limited attention. A simulation study was conducted to (1) assess whether existing power formulas based on generalized estimating equations (GEEs) provide an adequate approximation to the power achieved by mixed effects models, and (2) compare the performance of mixed models vs GEEs in estimating the effect of treatment on a continuous outcome. We considered clusters that exist prior to randomization with a maximum cluster size of 2, three methods of randomizing the clustered observations, and simulated datasets with uninformative cluster size and the sample size required to achieve 80% power according to GEE-based formulas with an independence or exchangeable working correlation structure. The empirical power of the mixed model approach was close to the nominal level when sample size was calculated using the exchangeable GEE formula, but was often too high when the sample size was based on the independence GEE formula. The independence GEE always converged and performed well in all scenarios. Performance of the exchangeable GEE and mixed model was also acceptable under cluster randomization, though under-coverage and inflated type I error rates could occur with other methods of randomization. Analysis of partially clustered trials using GEEs with an independence working correlation structure may be preferred to avoid the limitations of mixed models and exchangeable GEEs.},
  archive      = {J_SIM},
  author       = {Kylie M. Lange and Thomas R. Sullivan and Jessica Kasza and Lisa N. Yelland},
  doi          = {10.1002/sim.10201},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4819-4835},
  shortjournal = {Stat. Med.},
  title        = {Performance of mixed effects models and generalized estimating equations for continuous outcomes in partially clustered trials including both independent and paired data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating analytic models for individually randomized group
treatment trials with complex clustering in nested and crossed designs.
<em>SIM</em>, <em>43</em>(25), 4796–4818. (<a
href="https://doi.org/10.1002/sim.10206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many individually randomized group treatment (IRGT) trials randomly assign individuals to study arms but deliver treatments via shared agents, such as therapists, surgeons, or trainers. Post-randomization interactions induce correlations in outcome measures between participants sharing the same agent. Agents can be nested in or crossed with trial arm, and participants may interact with a single agent or with multiple agents. These complications have led to ambiguity in choice of models but there have been no systematic efforts to identify appropriate analytic models for these study designs. To address this gap, we undertook a simulation study to examine the performance of candidate analytic models in the presence of complex clustering arising from multiple membership, single membership, and single agent settings, in both nested and crossed designs and for a continuous outcome. With nested designs, substantial type I error rate inflation was observed when analytic models did not account for multiple membership and when analytic model weights characterizing the association with multiple agents did not match the data generating mechanism. Conversely, analytic models for crossed designs generally maintained nominal type I error rates unless there was notable imbalance in the number of participants that interact with each agent.},
  archive      = {J_SIM},
  author       = {Jonathan C. Moyer and Fan Li and Andrea J. Cook and Patrick J. Heagerty and Sherri L. Pals and Elizabeth L. Turner and Rui Wang and Yunji Zhou and Qilu Yu and Xueqi Wang and David M. Murray},
  doi          = {10.1002/sim.10206},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4796-4818},
  shortjournal = {Stat. Med.},
  title        = {Evaluating analytic models for individually randomized group treatment trials with complex clustering in nested and crossed designs},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilevel longitudinal functional principal component
model. <em>SIM</em>, <em>43</em>(25), 4781–4795. (<a
href="https://doi.org/10.1002/sim.10207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor devices, such as accelerometers, are widely used for measuring physical activity (PA). These devices provide outputs at fine granularity (e.g., 10–100 Hz or minute-level), which while providing rich data on activity patterns, also pose computational challenges with multilevel densely sampled data, resulting in PA records that are measured continuously across multiple days and visits. On the other hand, a scalar health outcome (e.g., BMI) is usually observed only at the individual or visit level. This leads to a discrepancy in numbers of nested levels between the predictors (PA) and outcomes, raising analytic challenges. To address this issue, we proposed a multilevel longitudinal functional principal component analysis (mLFPCA) model to directly model multilevel functional PA inputs in a longitudinal study, and then implemented a longitudinal functional principal component regression (FPCR) to explore the association between PA and obesity-related health outcomes. Additionally, we conducted a comprehensive simulation study to examine the impact of imbalanced multilevel data on both mLFPCA and FPCR performance and offer guidelines for selecting optimal methods.},
  archive      = {J_SIM},
  author       = {Wenyi Lin and Jingjing Zou and Chongzhi Di and Cheryl L. Rock and Loki Natarajan},
  doi          = {10.1002/sim.10207},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4781-4795},
  shortjournal = {Stat. Med.},
  title        = {Multilevel longitudinal functional principal component model},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Letter to the editor: The use of fast approximate random
forests using subsampling (rfsrc.fast) does not support the conclusion
on sample sizes needed for random survival forests. <em>SIM</em>,
<em>43</em>(24), 4778–4780. (<a
href="https://doi.org/10.1002/sim.10204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Elena Albu and Ben Van Calster and Laure Wynants},
  doi          = {10.1002/sim.10204},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4778-4780},
  shortjournal = {Stat. Med.},
  title        = {Letter to the editor: The use of fast approximate random forests using subsampling (rfsrc.fast) does not support the conclusion on sample sizes needed for random survival forests},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection and correction in dense functional data
within electronic medical records. <em>SIM</em>, <em>43</em>(24),
4768–4777. (<a href="https://doi.org/10.1002/sim.10209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical research, the accuracy of data from electronic medical records (EMRs) is critical, particularly when analyzing dense functional data, where anomalies can severely compromise research integrity. Anomalies in EMRs often arise from human errors in data measurement and entry, and increase in frequency with the volume of data. Despite the established methods in computer science, anomaly detection in medical applications remains underdeveloped. We address this deficiency by introducing a novel tool for identifying and correcting anomalies specifically in dense functional EMR data. Our approach utilizes studentized residuals from a mean-shift model, and therefore assumes that the data adheres to a smooth functional trajectory. Additionally, our method is tailored to be conservative, focusing on anomalies that signify actual errors in the data collection process while controlling for false discovery rates and type II errors. To support widespread implementation, we provide a comprehensive R package, ensuring that our methods can be applied in diverse settings. Our methodology&#39;s efficacy has been validated through rigorous simulation studies and real-world applications, confirming its ability to accurately identify and correct errors, thus enhancing the reliability and quality of medical data analysis.},
  archive      = {J_SIM},
  author       = {Daren Kuwaye and Hyunkeun Ryan Cho},
  doi          = {10.1002/sim.10209},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4768-4777},
  shortjournal = {Stat. Med.},
  title        = {Anomaly detection and correction in dense functional data within electronic medical records},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simulation study of the performance of statistical models
for count outcomes with excessive zeros. <em>SIM</em>, <em>43</em>(24),
4752–4767. (<a href="https://doi.org/10.1002/sim.10198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background : Outcome measures that are count variables with excessive zeros are common in health behaviors research. Examples include the number of standard drinks consumed or alcohol-related problems experienced over time. There is a lack of empirical data about the relative performance of prevailing statistical models for assessing the efficacy of interventions when outcomes are zero-inflated, particularly compared with recently developed marginalized count regression approaches for such data. Methods : The current simulation study examined five commonly used approaches for analyzing count outcomes, including two linear models (with outcomes on raw and log-transformed scales, respectively) and three prevailing count distribution-based models (ie, Poisson, negative binomial, and zero-inflated Poisson (ZIP) models). We also considered the marginalized zero-inflated Poisson (MZIP) model, a novel alternative that estimates the overall effects on the population mean while adjusting for zero-inflation. Motivated by alcohol misuse prevention trials, extensive simulations were conducted to evaluate and compare the statistical power and Type I error rate of the statistical models and approaches across data conditions that varied in sample size ( to 500), zero rate (0.2 to 0.8), and intervention effect sizes. Results : Under zero-inflation, the Poisson model failed to control the Type I error rate, resulting in higher than expected false positive results. When the intervention effects on the zero (vs. non-zero) and count parts were in the same direction, the MZIP model had the highest statistical power, followed by the linear model with outcomes on the raw scale, negative binomial model, and ZIP model. The performance of the linear model with a log-transformed outcome variable was unsatisfactory. Conclusions : The MZIP model demonstrated better statistical properties in detecting true intervention effects and controlling false positive results for zero-inflated count outcomes. This MZIP model may serve as an appealing analytical approach to evaluating overall intervention effects in studies with count outcomes marked by excessive zeros.},
  archive      = {J_SIM},
  author       = {Zhengyang Zhou and Dateng Li and David Huh and Minge Xie and Eun-Young Mun},
  doi          = {10.1002/sim.10198},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4752-4767},
  shortjournal = {Stat. Med.},
  title        = {A simulation study of the performance of statistical models for count outcomes with excessive zeros},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid approach to sample size re-estimation in cluster
randomized trials with continuous outcomes. <em>SIM</em>,
<em>43</em>(24), 4736–4751. (<a
href="https://doi.org/10.1002/sim.10205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a hybrid (Bayesian-frequentist) approach to sample size re-estimation (SSRE) for cluster randomised trials with continuous outcome data, allowing for uncertainty in the intra-cluster correlation (ICC). In the hybrid framework, pre-trial knowledge about the ICC is captured by placing a Truncated Normal prior on it, which is then updated at an interim analysis using the study data, and used in expected power control. On average, both the hybrid and frequentist approaches mitigate against the implications of misspecifying the ICC at the trial&#39;s design stage. In addition, both frameworks lead to SSRE designs with approximate control of the type I error-rate at the desired level. It is clearly demonstrated how the hybrid approach is able to reduce the high variability in the re-estimated sample size observed within the frequentist framework, based on the informativeness of the prior. However, misspecification of a highly informative prior can cause significant power loss. In conclusion, a hybrid approach could offer advantages to cluster randomised trials using SSRE. Specifically, when there is available data or expert opinion to help guide the choice of prior for the ICC, the hybrid approach can reduce the variance of the re-estimated required sample size compared to a frequentist approach. As SSRE is unlikely to be employed when there is substantial amounts of such data available (ie, when a constructed prior is highly informative), the greatest utility of a hybrid approach to SSRE likely lies when there is low-quality evidence available to guide the choice of prior.},
  archive      = {J_SIM},
  author       = {Samuel K Sarkodie and James MS Wason and Michael J Grayling},
  doi          = {10.1002/sim.10205},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4736-4751},
  shortjournal = {Stat. Med.},
  title        = {A hybrid approach to sample size re-estimation in cluster randomized trials with continuous outcomes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating causes of maternal death in data-sparse contexts.
<em>SIM</em>, <em>43</em>(24), 4702–4735. (<a
href="https://doi.org/10.1002/sim.10199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the underlying causes of maternal death across all regions of the world is essential to inform policies and resource allocation to reduce the mortality burden. However, in many countries there exists very little data on the causes of maternal death, and data that do exist do not capture the entire population at risk. In this article, we present a Bayesian hierarchical multinomial model to estimate maternal cause of death distributions globally, regionally, and for all countries worldwide. The framework combines data from various sources to inform estimates, including data from civil registration and vital systems, smaller-scale surveys and studies, and high-quality data from confidential enquiries and surveillance systems. The framework accounts for varying data quality and coverage, and allows for situations where one or more causes of death are missing. We illustrate the results of the model on three case-study countries that have different data availability situations.},
  archive      = {J_SIM},
  author       = {Michael Y. C. Chong and Marija Pejchinovska and Monica Alexander},
  doi          = {10.1002/sim.10199},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4702-4735},
  shortjournal = {Stat. Med.},
  title        = {Estimating causes of maternal death in data-sparse contexts},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling multiple-criterion diagnoses by
heterogeneous-instance logistic regression. <em>SIM</em>,
<em>43</em>(24), 4684–4701. (<a
href="https://doi.org/10.1002/sim.10202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mild cognitive impairment (MCI) is a prodromal stage of Alzheimer&#39;s disease (AD) that causes a significant burden in caregiving and medical costs. Clinically, the diagnosis of MCI is determined by the impairment statuses of five cognitive domains. If one of these cognitive domains is impaired, the patient is diagnosed with MCI, and if two out of the five domains are impaired, the patient is diagnosed with AD. In medical records, most of the time, the diagnosis of MCI/AD is given, but not the statuses of the five domains. We may treat the domain statuses as missing variables. This diagnostic procedure relates MCI/AD status modeling to multiple-instance learning, where each domain resembles an instance. However, traditional multiple-instance learning assumes common predictors among instances, but in our case, each domain is associated with different predictors. In this article, we generalized the multiple-instance logistic regression to accommodate the heterogeneity in predictors among different instances. The proposed model is dubbed heterogeneous-instance logistic regression and is estimated via the expectation-maximization algorithm because of the presence of the missing variables. We also derived two variants of the proposed model for the MCI and AD diagnoses. The proposed model is validated in terms of its estimation accuracy, latent status prediction, and robustness via extensive simulation studies. Finally, we analyzed the National Alzheimer&#39;s Coordinating Center-Uniform Data Set using the proposed model and demonstrated its potential.},
  archive      = {J_SIM},
  author       = {Chun-Hao Yang and Ming-Han Li and Shu-Fang Wen and Sheng-Mao Chang},
  doi          = {10.1002/sim.10202},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4684-4701},
  shortjournal = {Stat. Med.},
  title        = {Modeling multiple-criterion diagnoses by heterogeneous-instance logistic regression},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group sequential designs for clinical trials when the
maximum sample size is uncertain. <em>SIM</em>, <em>43</em>(24),
4667–4683. (<a href="https://doi.org/10.1002/sim.10203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the experience of COVID-19 trials, we consider clinical trials in the setting of an emerging disease in which the uncertainty of natural disease course and potential treatment effects makes advance specification of a sample size challenging. One approach to such a challenge is to use a group sequential design to allow the trial to stop on the basis of interim analysis results as soon as a conclusion regarding the effectiveness of the treatment under investigation can be reached. As such a trial may be halted before a formal stopping boundary is reached, we consider the final analysis under such a scenario, proposing alternative methods for when the decision to halt the trial is made with or without knowledge of interim analysis results. We address the problems of ensuring that the type I error rate neither exceeds nor falls unnecessarily far below the nominal level. We also propose methods in which there is no maximum sample size, the trial continuing either until the stopping boundary is reached or it is decided to halt the trial.},
  archive      = {J_SIM},
  author       = {Amin Yarahmadi and Lori E. Dodd and Thomas Jaki and Peter Horby and Nigel Stallard},
  doi          = {10.1002/sim.10203},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4667-4683},
  shortjournal = {Stat. Med.},
  title        = {Group sequential designs for clinical trials when the maximum sample size is uncertain},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal subsampling for semi-parametric accelerated failure
time models with massive survival data using a rank-based approach.
<em>SIM</em>, <em>43</em>(24), 4650–4666. (<a
href="https://doi.org/10.1002/sim.10200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling is a practical strategy for analyzing vast survival data, which are progressively encountered across diverse research domains. While the optimal subsampling method has been applied to inferences for Cox models and parametric accelerated failure time (AFT) models, its application to semi-parametric AFT models with rank-based estimation have received limited attention. The challenges arise from the non-smooth estimating function for regression coefficients and the seemingly zero contribution from censored observations in estimating functions in the commonly seen form. To address these challenges, we develop optimal subsampling probabilities for both event and censored observations by expressing the estimating functions through a well-defined stochastic process. Meanwhile, we apply an induced smoothing procedure to the non-smooth estimating functions. As the optimal subsampling probabilities depend on the unknown regression coefficients, we employ a two-step procedure to obtain a feasible estimation method. An additional benefit of the method is its ability to resolve the issue of underestimation of the variance when the subsample size approaches the full sample size. We validate the performance of our estimators through a simulation study and apply the methods to analyze the survival time of lymphoma patients in the surveillance, epidemiology, and end results program.},
  archive      = {J_SIM},
  author       = {Zehan Yang and HaiYing Wang and Jun Yan},
  doi          = {10.1002/sim.10200},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4650-4666},
  shortjournal = {Stat. Med.},
  title        = {Optimal subsampling for semi-parametric accelerated failure time models with massive survival data using a rank-based approach},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Principal quantile treatment effect estimation using
principal scores. <em>SIM</em>, <em>43</em>(24), 4635–4649. (<a
href="https://doi.org/10.1002/sim.10178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intercurrent events and estimands play a key role in defining the treatment effects of interest precisely. Sometimes the median or other quantiles of outcomes in a principal stratum according to potential occurrence of intercurrent events are of interest in randomized clinical trials. Naïve analyses such as those based on the observed occurrence of the intercurrent events lead to biased results. Therefore, we propose principal quantile treatment effect estimators that can nonparametrically estimate the distribution of potential outcomes by principal score weighting without relying on the exclusion restriction assumption. Our simulation studies show that the proposed method works in situations where the median or quantiles may be regarded as the preferred population-level summary over the mean. We illustrate our proposed method by using data from a randomized controlled trial conducted on patients with nonerosive reflux disease.},
  archive      = {J_SIM},
  author       = {Kotaro Mizuma and Takamasa Hashimoto and Sho Sakui and Shingo Kuroda},
  doi          = {10.1002/sim.10178},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4635-4649},
  shortjournal = {Stat. Med.},
  title        = {Principal quantile treatment effect estimation using principal scores},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic path analysis for exploring treatment effect
mediation processes in clinical trials with time-to-event endpoints.
<em>SIM</em>, <em>43</em>(23), 4614–4634. (<a
href="https://doi.org/10.1002/sim.10191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Why does a beneficial treatment effect on a longitudinal biomarker not translate into overall treatment benefit on survival, when the biomarker is in fact a prognostic factor of survival? In a recent exploratory data analysis in oncology, we were faced with this seemingly paradoxical result. To address this problem, we applied a theoretically principled methodology called dynamic path analysis, which allows us to perform mediation analysis with a longitudinal mediator and survival outcome. The aim of the analysis is to decompose the total treatment effect into a direct treatment effect and an indirect treatment effect mediated through a carefully constructed mediation path. The dynamic nature of the underlying methodology enables us to describe how these effects evolve over time, which can add to the mechanistic understanding of the underlying processes. In this paper, we present a detailed description of the dynamic path analysis framework and illustrate its application to survival mediation analysis using simulated and real data. The use case analysis provides clarity on the specific exploratory question of interest while the methodology generalizes to a wide range of applications in drug development where time-to-event is the primary clinical outcome of interest.},
  archive      = {J_SIM},
  author       = {Matthias Kormaksson and Markus Reiner Lange and David Demanse and Susanne Strohmaier and Jiawei Duan and Qing Xie and Mariana Carbini and Claudia Bossen and Achim Guettner and Antonella Maniero},
  doi          = {10.1002/sim.10191},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4614-4634},
  shortjournal = {Stat. Med.},
  title        = {Dynamic path analysis for exploring treatment effect mediation processes in clinical trials with time-to-event endpoints},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel block-coordinate gradient descent algorithm for
simultaneous grouped selection of fixed and random effects in joint
modeling. <em>SIM</em>, <em>43</em>(23), 4595–4613. (<a
href="https://doi.org/10.1002/sim.10193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models for longitudinal and time-to-event data are receiving increasing attention owing to its capability of capturing the possible association between these two types of data. Typically, a joint model consists of a longitudinal submodel for longitudinal processes and a survival submodel for the time-to-event response, and links two submodels by common covariates that may carry both fixed and random effects. However, research gaps still remain on how to simultaneously select fixed and random effects from the two submodels under the joint modeling framework efficiently and effectively. In this article, we propose a novel block-coordinate gradient descent (BCGD) algorithm to simultaneously select multiple longitudinal covariates that may carry fixed and random effects in the joint model. Specifically, for the multiple longitudinal processes, a linear mixed effect model is adopted where random intercepts and slopes serve as essential covariates of the trajectories, and for the survival submodel, the popular proportional hazard model is employed. A penalized likelihood estimation is used to control the dimensionality of covariates in the joint model and estimate the unknown parameters, especially when estimating the covariance matrix of random effects. The proposed BCGD method can successfully capture the useful covariates of both fixed and random effects with excellent selection power, and efficiently provide a relatively accurate estimate of fixed and random effects empirically. The simulation results show excellent performance of the proposed method and support its effectiveness. The proposed BCGD method is further applied on two real data sets, and we examine the risk factors for the effects of different heart valves, differing on type of tissue, implanted in the aortic position and the risk factors for the diagnosis of primary biliary cholangitis.},
  archive      = {J_SIM},
  author       = {Shuyan Chen and Zhiqing Fang and Zhong Li and Xin Liu},
  doi          = {10.1002/sim.10193},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4595-4613},
  shortjournal = {Stat. Med.},
  title        = {A novel block-coordinate gradient descent algorithm for simultaneous grouped selection of fixed and random effects in joint modeling},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing methods for assessing the reliability of health
care quality measures. <em>SIM</em>, <em>43</em>(23), 4575–4594. (<a
href="https://doi.org/10.1002/sim.10197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality measurement plays an increasing role in U.S. health care. Measures inform quality improvement efforts, public reporting of variations in quality of care across providers and hospitals, and high-stakes financial decisions. To be meaningful in these contexts, measures should be reliable and not heavily impacted by chance variations in sampling or measurement. Several different methods are used in practice by measure developers and endorsers to evaluate reliability; however, there is uncertainty and debate over differences between these methods and their interpretations. We review methods currently used in practice, pointing out differences that can lead to disparate reliability estimates. We compare estimates from 14 different methods in the case of two sets of mental health quality measures within a large health system. We find that estimates can differ substantially and that these discrepancies widen when sample size is reduced.},
  archive      = {J_SIM},
  author       = {Kenneth J. Nieser and Alex H. S. Harris},
  doi          = {10.1002/sim.10197},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4575-4594},
  shortjournal = {Stat. Med.},
  title        = {Comparing methods for assessing the reliability of health care quality measures},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalar-on-function regression: Estimation and inference
under complex survey designs. <em>SIM</em>, <em>43</em>(23), 4559–4574.
(<a href="https://doi.org/10.1002/sim.10194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly, large, nationally representative health and behavioral surveys conducted under a multistage stratified sampling scheme collect high dimensional data with correlation structured along some domain (eg, wearable sensor data measured continuously and correlated over time, imaging data with spatiotemporal correlation) with the goal of associating these data with health outcomes. Analysis of this sort requires novel methodologic work at the intersection of survey statistics and functional data analysis. Here, we address this crucial gap in the literature by proposing an estimation and inferential framework for generalizable scalar-on-function regression models for data collected under a complex survey design. We propose to: (1) estimate functional regression coefficients using weighted score equations; and (2) perform inference using novel functional balanced repeated replication and survey-weighted bootstrap for multistage survey designs. This is the first frequentist study to discuss the estimation of scalar-on-function regression models in the context of complex survey studies and to assess the validity of various inferential techniques based on re-sampling methods via a comprehensive simulation study. We implement our methods to predict mortality using diurnal activity profiles measured via wearable accelerometers using the National Health and Nutrition Examination Survey 2003-2006 data. The proposed computationally efficient methods are implemented in R software package surveySoFR.},
  archive      = {J_SIM},
  author       = {Ekaterina Smirnova and Erjia Cui and Lucia Tabacu and Andrew Leroux},
  doi          = {10.1002/sim.10194},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4559-4574},
  shortjournal = {Stat. Med.},
  title        = {Scalar-on-function regression: Estimation and inference under complex survey designs},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modeling framework for the analysis of the SARS-CoV2
transmission dynamics. <em>SIM</em>, <em>43</em>(23), 4542–4558. (<a
href="https://doi.org/10.1002/sim.10195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the progress in medical data collection the actual burden of SARS-CoV-2 remains unknown due to under-ascertainment of cases. This was apparent in the acute phase of the pandemic and the use of reported deaths has been pointed out as a more reliable source of information, likely less prone to under-reporting. Since daily deaths occur from past infections weighted by their probability of death, one may infer the total number of infections accounting for their age distribution, using the data on reported deaths. We adopt this framework and assume that the dynamics generating the total number of infections can be described by a continuous time transmission model expressed through a system of nonlinear ordinary differential equations where the transmission rate is modeled as a diffusion process allowing to reveal both the effect of control strategies and the changes in individuals behavior. We develop this flexible Bayesian tool in Stan and study 3 pairs of European countries, estimating the time-varying reproduction number ( ) as well as the true cumulative number of infected individuals. As we estimate the true number of infections we offer a more accurate estimate of . We also provide an estimate of the daily reporting ratio and discuss the effects of changes in mobility and testing on the inferred quantities.},
  archive      = {J_SIM},
  author       = {Anastasia Chatzilena and Nikolaos Demiris and Konstantinos Kalogeropoulos},
  doi          = {10.1002/sim.10195},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4542-4558},
  shortjournal = {Stat. Med.},
  title        = {A modeling framework for the analysis of the SARS-CoV2 transmission dynamics},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Path-specific causal decomposition analysis with multiple
correlated mediator variables. <em>SIM</em>, <em>43</em>(23), 4519–4541.
(<a href="https://doi.org/10.1002/sim.10182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A causal decomposition analysis allows researchers to determine whether the difference in a health outcome between two groups can be attributed to a difference in each group&#39;s distribution of one or more modifiable mediator variables. With this knowledge, researchers and policymakers can focus on designing interventions that target these mediator variables. Existing methods for causal decomposition analysis either focus on one mediator variable or assume that each mediator variable is conditionally independent given the group label and the mediator-outcome confounders. In this article, we propose a flexible causal decomposition analysis method that can accommodate multiple correlated and interacting mediator variables, which are frequently seen in studies of health behaviors and studies of environmental pollutants. We extend a Monte Carlo-based causal decomposition analysis method to this setting by using a multivariate mediator model that can accommodate any combination of binary and continuous mediator variables. Furthermore, we state the causal assumptions needed to identify both joint and path-specific decomposition effects through each mediator variable. To illustrate the reduction in bias and confidence interval width of the decomposition effects under our proposed method, we perform a simulation study. We also apply our approach to examine whether differences in smoking status and dietary inflammation score explain any of the Black-White differences in incident diabetes using data from a national cohort study.},
  archive      = {J_SIM},
  author       = {Melissa J. Smith and Leslie A. McClure and D. Leann Long},
  doi          = {10.1002/sim.10182},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4519-4541},
  shortjournal = {Stat. Med.},
  title        = {Path-specific causal decomposition analysis with multiple correlated mediator variables},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exact test and exact confidence interval for the cox model.
<em>SIM</em>, <em>43</em>(23), 4499–4518. (<a
href="https://doi.org/10.1002/sim.10189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox proportional hazards model is commonly used to analyze time-to-event data in clinical trials. Standard inference procedures for the Cox model are based on asymptotic approximations and may perform poorly when there are few events in one or both treatment groups, as may be the case when the event of interest is rare or when the experimental treatment is highly efficacious. In this article, we propose an exact test of equivalence and efficacy under a proportional hazard model with treatment effect as the only fixed effect, together with an exact confidence interval that is obtained by inverting the exact test. The proposed test is based on a conditional error method originally proposed for sample size reestimation problems. In the present context, the conditional error method is used to combine information from a sequence of hypergeometric distributions, one at each observed event time. The proposed procedures are evaluated in simulation studies and illustrated using real data from an HIV prevention trial. A companion R package “ExactCox” is available for download on CRAN.},
  archive      = {J_SIM},
  author       = {Yongwu Shao and Zhishen Ye and Zhiwei Zhang},
  doi          = {10.1002/sim.10189},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4499-4518},
  shortjournal = {Stat. Med.},
  title        = {Exact test and exact confidence interval for the cox model},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating individualized treatment effect predictions: A
model-based perspective on discrimination and calibration assessment.
<em>SIM</em>, <em>43</em>(23), 4481–4498. (<a
href="https://doi.org/10.1002/sim.10186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing interest in the prediction of individualized treatment effects. While there is a rapidly growing literature on the development of such models, there is little literature on the evaluation of their performance. In this paper, we aim to facilitate the validation of prediction models for individualized treatment effects. The estimands of interest are defined based on the potential outcomes framework, which facilitates a comparison of existing and novel measures. In particular, we examine existing measures of discrimination for benefit (variations of the c-for-benefit), and propose model-based extensions to the treatment effect setting for discrimination and calibration metrics that have a strong basis in outcome risk prediction. The main focus is on randomized trial data with binary endpoints and on models that provide individualized treatment effect predictions and potential outcome predictions. We use simulated data to provide insight into the characteristics of the examined discrimination and calibration statistics under consideration, and further illustrate all methods in a trial of acute ischemic stroke treatment. The results show that the proposed model-based statistics had the best characteristics in terms of bias and accuracy. While resampling methods adjusted for the optimism of performance estimates in the development data, they had a high variance across replications that limited their accuracy. Therefore, individualized treatment effect models are best validated in independent data. To aid implementation, a software implementation of the proposed methods was made available in R .},
  archive      = {J_SIM},
  author       = {J. Hoogland and O. Efthimiou and T. L. Nguyen and T. P. A. Debray},
  doi          = {10.1002/sim.10186},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4481-4498},
  shortjournal = {Stat. Med.},
  title        = {Evaluating individualized treatment effect predictions: A model-based perspective on discrimination and calibration assessment},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How estimating nuisance parameters can reduce the variance
(with consistent variance estimation). <em>SIM</em>, <em>43</em>(23),
4456–4480. (<a href="https://doi.org/10.1002/sim.10164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We often estimate a parameter of interest ψ $$ \psi $$ when the identifying conditions involve a finite-dimensional nuisance parameter θ ∈ ℝ d $$ \theta \in {\mathbb{R}}^d $$ . Examples from causal inference are inverse probability weighting, marginal structural models and structural nested models, which all lead to unbiased estimating equations. This article presents a consistent sandwich estimator for the variance of estimators ψ ^ $$ \hat{\psi} $$ that solve unbiased estimating equations including θ $$ \theta $$ which is also estimated by solving unbiased estimating equations. This article presents four additional results for settings where θ ^ $$ \hat{\theta} $$ solves (partial) score equations and ψ $$ \psi $$ does not depend on θ $$ \theta $$ . This includes many causal inference settings where θ $$ \theta $$ describes the treatment probabilities, missing data settings where θ $$ \theta $$ describes the missingness probabilities, and measurement error settings where describes the error distribution. These four additional results are: (1) Counter-intuitively, the asymptotic variance of is typically smaller when is estimated. (2) If estimating is ignored, the sandwich estimator for the variance of is conservative. (3) A consistent sandwich estimator for the variance of . (4) If with the true plugged in is efficient, the asymptotic variance of does not depend on whether is estimated. To illustrate we use observational data to calculate confidence intervals for (1) the effect of cazavi versus colistin on bacterial infections and (2) how the effect of antiretroviral treatment depends on its initiation time in HIV-infected patients.},
  archive      = {J_SIM},
  author       = {Judith J. Lok},
  doi          = {10.1002/sim.10164},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4456-4480},
  shortjournal = {Stat. Med.},
  title        = {How estimating nuisance parameters can reduce the variance (with consistent variance estimation)},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation and comparison of covariate balance metrics in
studies with time-dependent confounding. <em>SIM</em>, <em>43</em>(23),
4437–4455. (<a href="https://doi.org/10.1002/sim.10188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marginal structural models have been increasingly used by analysts in recent years to account for confounding bias in studies with time-varying treatments. The parameters of these models are often estimated using inverse probability of treatment weighting. To ensure that the estimated weights adequately control confounding, it is possible to check for residual imbalance between treatment groups in the weighted data. Several balance metrics have been developed and compared in the cross-sectional case but have not yet been evaluated and compared in longitudinal studies with time-varying treatment. We have first extended the definition of several balance metrics to the case of a time-varying treatment, with or without censoring. We then compared the performance of these balance metrics in a simulation study by assessing the strength of the association between their estimated level of imbalance and bias. We found that the Mahalanobis balance performed best. Finally, the method was illustrated for estimating the cumulative effect of statins exposure over one year on the risk of cardiovascular disease or death in people aged 65 and over in population-wide administrative data. This illustration confirms the feasibility of employing our proposed metrics in large databases with multiple time-points.},
  archive      = {J_SIM},
  author       = {David Adenyo and Jason R. Guertin and Bernard Candas and Caroline Sirois and Denis Talbot},
  doi          = {10.1002/sim.10188},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4437-4455},
  shortjournal = {Stat. Med.},
  title        = {Evaluation and comparison of covariate balance metrics in studies with time-dependent confounding},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modern approaches for evaluating treatment effect
heterogeneity from clinical trials and observational data. <em>SIM</em>,
<em>43</em>(22), 4388–4436. (<a
href="https://doi.org/10.1002/sim.10167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we review recent advances in statistical methods for the evaluation of the heterogeneity of treatment effects (HTE), including subgroup identification and estimation of individualized treatment regimens, from randomized clinical trials and observational studies. We identify several types of approaches using the features introduced in Lipkovich et al ( Stat Med 2017;36: 136-196) that distinguish the recommended principled methods from basic methods for HTE evaluation that typically rely on rules of thumb and general guidelines (the methods are often referred to as common practices). We discuss the advantages and disadvantages of various principled methods as well as common measures for evaluating their performance. We use simulated data and a case study based on a historical clinical trial to illustrate several new approaches to HTE evaluation.},
  archive      = {J_SIM},
  author       = {Ilya Lipkovich and David Svensson and Bohdana Ratitch and Alex Dmitrienko},
  doi          = {10.1002/sim.10167},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4388-4436},
  shortjournal = {Stat. Med.},
  title        = {Modern approaches for evaluating treatment effect heterogeneity from clinical trials and observational data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification and estimation of causal effects in the
presence of confounded principal strata. <em>SIM</em>, <em>43</em>(22),
4372–4387. (<a href="https://doi.org/10.1002/sim.10175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal stratification has become a popular tool to address a broad class of causal inference questions, particularly in dealing with non-compliance and truncation by death problems. The causal effects within principal strata, which are determined by joint potential values of the intermediate variable, also known as the principal causal effects, are often of interest in these studies. The analysis of principal causal effects from observational studies mostly relies on the ignorability assumption of treatment assignment, which requires practitioners to accurately measure as many covariates as possible so that all potential sources of confounders are captured. However, in practice, collecting all potential confounding factors can be challenging and costly, rendering the ignorability assumption questionable. In this paper, we consider the identification and estimation of causal effects when treatment and principal stratification are confounded by unmeasured confounding. Specifically, we establish the nonparametric identification of principal causal effects using a pair of negative controls to mitigate unmeasured confounding, requiring they have no direct effect on the outcome variable. We also provide an estimation method for principal causal effects. Extensive simulations and a leukemia study are employed for illustration.},
  archive      = {J_SIM},
  author       = {Shanshan Luo and Wei Li and Wang Miao and Yangbo He},
  doi          = {10.1002/sim.10175},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4372-4387},
  shortjournal = {Stat. Med.},
  title        = {Identification and estimation of causal effects in the presence of confounded principal strata},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-agent dose-finding in phase i clinical trial—an
extension of rapid enrollment design. <em>SIM</em>, <em>43</em>(22),
4361–4371. (<a href="https://doi.org/10.1002/sim.10185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dual-agent treatment has become more and more popular in clinical trials. We have developed an approach called rapid enrollment dual-agent design (REDD) for dose-finding in Phase I clinical trials. This approach aims to administer treatment to patients using a dose combination that is highly probable to be the target dose combination. Unlike other non-model-based designs, rapid enrollment designs (RED and REDD) do not require waiting for all patients to complete an assessment before the assignment of the next participant. Simulations showed that across several scenarios, the average performance of REDD is comparable to that of the Bayesian optimal interval (BOIN) design and the partial order continual reassessment method (POCRM). The simulation results of REDD for late-onset toxicity assessments demonstrated that assigning patients to a dose combination as they are being enrolled, without waiting for the most recent cohort of patients to complete their follow-up, does not significantly compromise the quality of the maximum tolerated dose (MTD) estimation. Instead, it saves a considerable amount of time in clinical trial enrollment. User-friendly online applications have also been created to further facilitate the adoption of rapid enrollment designs in Phase I trials. In summary, being similar to BOIN and POCRM in performance, REDD is an approach that is easily comprehensible, straightforward to implement and offers an advantage of enrolling patients without having to wait for all current patients to complete their follow-ups for toxicity.},
  archive      = {J_SIM},
  author       = {Yunfei Wang},
  doi          = {10.1002/sim.10185},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4361-4371},
  shortjournal = {Stat. Med.},
  title        = {Dual-agent dose-finding in phase i clinical trial—An extension of rapid enrollment design},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Validation of predicted individual treatment effects in out
of sample respondents. <em>SIM</em>, <em>43</em>(22), 4349–4360. (<a
href="https://doi.org/10.1002/sim.10187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized medicine promises the ability to improve patient outcomes by tailoring treatment recommendations to the likelihood that any given patient will respond well to a given treatment. It is important that predictions of treatment response be validated and replicated in independent data to support their use in clinical practice. In this paper, we propose and test an approach for validating predictions of individual treatment effects with continuous outcomes across samples that uses matching in a test (validation) sample to match individuals in the treatment and control arms based on their predicted treatment response and their predicted response under control. To examine the proposed validation approach, we conducted simulations where test data is generated from either an identical, similar, or unrelated process to the training data. We also examined the impact of nuisance variables. To demonstrate the use of this validation procedure in the context of predicting individual treatment effects in the treatment of alcohol use disorder, we apply our validation procedure using data from a clinical trial of combined behavioral and pharmacotherapy treatments. We find that the validation algorithm accurately confirms validation and lack of validation, and also provides insights into cases where test data were generated under similar, but not identical conditions. We also show that the presence of nuisance variables detrimentally impacts algorithm performance, which can be partially reduced though the use of variable selection methods. An advantage of the approach is that it can be widely applied to different predictive methods.},
  archive      = {J_SIM},
  author       = {Alena Kuhlemeier and Thomas Jaki and Katie Witkiewitz and Elizabeth A. Stuart and M. Lee Van Horn},
  doi          = {10.1002/sim.10187},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4349-4360},
  shortjournal = {Stat. Med.},
  title        = {Validation of predicted individual treatment effects in out of sample respondents},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal model averaging for partially linear models with
missing response variables and error-prone covariates. <em>SIM</em>,
<em>43</em>(22), 4328–4348. (<a
href="https://doi.org/10.1002/sim.10176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of optimal model averaging for partially linear models when the responses are missing at random and some covariates are measured with error. A novel weight choice criterion based on the Mallows-type criterion is proposed for the weight vector to be used in the model averaging. The resulting model averaging estimator for the partially linear models is shown to be asymptotically optimal under some regularity conditions in terms of achieving the smallest possible squared loss. In addition, the existence of a local minimizing weight vector and its convergence rate to the risk-based optimal weight vector are established. Simulation studies suggest that the proposed model averaging method generally outperforms existing methods. As an illustration, the proposed method is applied to analyze an HIV-CD4 dataset.},
  archive      = {J_SIM},
  author       = {Zhongqi Liang and Suojin Wang and Li Cai},
  doi          = {10.1002/sim.10176},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4328-4348},
  shortjournal = {Stat. Med.},
  title        = {Optimal model averaging for partially linear models with missing response variables and error-prone covariates},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Separable and controlled direct effects for competing
events: Estimation of component specific effects on sickness absence.
<em>SIM</em>, <em>43</em>(22), 4305–4327. (<a
href="https://doi.org/10.1002/sim.10179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many settings, it is reasonable to think of treatment as consisting of a number of components, either because this is the case in practice or because it is conceptually possible to decompose treatment into separate components due to the way in which it exerts effects on the outcome of interest. For competing events, the treatment decomposition idea has recently been suggested to separate effects of treatments on the outcome of interest from effects mediated through competing events using so-called separable effects. Like the idea of separating effects of exposure, it has been pointed out that ideas from mediation analysis generally may help to clarify the interpretation of existing estimands used in competing events settings. One example is the use of the controlled direct effect, to conceptualize the effects of interventions preventing the competing event from occurring. In this article, we identify the controlled direct effect as a component specific effect and discuss the merits of this estimand when the prevented event is non terminal and other methods of effects separation are problematic. Our motivating example is the study of a policy initiative, introduced in 2001, aimed at reducing long term sickness absence (SA) in Norway. The initiative consists of different components, one being to encourage use of graded SA, which is considered a key tool in the Nordic countries to reduce long term SA. The analysis makes use of longitudinal registry data for 113 808 individuals, followed from the time of first SA.},
  archive      = {J_SIM},
  author       = {Niklas N. Maltzahn and Ingrid Sivesind Mehlum and Jon Michael Gran},
  doi          = {10.1002/sim.10179},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4305-4327},
  shortjournal = {Stat. Med.},
  title        = {Separable and controlled direct effects for competing events: Estimation of component specific effects on sickness absence},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shortest path or random walks? A framework for path weights
in network meta-analysis. <em>SIM</em>, <em>43</em>(22), 4287–4304. (<a
href="https://doi.org/10.1002/sim.10177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying the contributions, or weights, of comparisons or single studies to the estimates in a network meta-analysis (NMA) is an active area of research. We extend this work to include the contributions of paths of evidence. We present a general framework, based on the path-design matrix, that describes the problem of finding path contributions as a linear equation. The resulting solutions may have negative coefficients. We show that two known approaches, called shortestpath and randomwalk, are special solutions of this equation, and both meet an optimization criterion, as they minimize the sum of absolute path contributions. In general, there is an infinite set of solutions, which can be identified using the generalized inverse (Moore-Penrose pseudoinverse). We consider two further special approaches. For large networks we find that shortestpath is superior with respect to run time and variability, compared to the other approaches, and is thus recommended in practice. The path-weights framework also has the potential to answer more general research questions in NMA.},
  archive      = {J_SIM},
  author       = {Gerta Rücker and Theodoros Papakonstantinou and Adriani Nikolakopoulou and Guido Schwarzer and Tobias Galla and Annabel L. Davies},
  doi          = {10.1002/sim.10177},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4287-4304},
  shortjournal = {Stat. Med.},
  title        = {Shortest path or random walks? a framework for path weights in network meta-analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using a monotonic density ratio model to increase the power
of the goodness-of-fit test for logistic regression models with
case-control data. <em>SIM</em>, <em>43</em>(22), 4272–4286. (<a
href="https://doi.org/10.1002/sim.10183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logistic regression models are widely used in case-control data analysis, and testing the goodness-of-fit of their parametric model assumption is a fundamental research problem. In this article, we propose to enhance the power of the goodness-of-fit test by exploiting a monotonic density ratio model, in which the ratio of case and control densities is assumed to be a monotone function. We show that such a monotonic density ratio model is naturally induced by the retrospective case-control sampling design under the alternative hypothesis. The pool-adjacent-violator algorithm is adapted to solve for the constrained nonparametric maximum likelihood estimator under the alternative hypothesis. By measuring the discrepancy between this estimator and the semiparametric maximum likelihood estimator under the null hypothesis, we develop a new Kolmogorov-Smirnov-type statistic to test the goodness-of-fit for logistic regression models with case-control data. A bootstrap resampling procedure is suggested to approximate the -value of the proposed test. Simulation results show that the type I error of the proposed test is well controlled and the power improvement is substantial in many cases. Three real data applications are also included for illustration.},
  archive      = {J_SIM},
  author       = {Chunlin Wang and Zheyu Liu and Xinyu Wang},
  doi          = {10.1002/sim.10183},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4272-4286},
  shortjournal = {Stat. Med.},
  title        = {Using a monotonic density ratio model to increase the power of the goodness-of-fit test for logistic regression models with case-control data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using instruments for selection to adjust for selection bias
in mendelian randomization. <em>SIM</em>, <em>43</em>(22), 4250–4271.
(<a href="https://doi.org/10.1002/sim.10173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selection bias is a common concern in epidemiologic studies. In the literature, selection bias is often viewed as a missing data problem. Popular approaches to adjust for bias due to missing data, such as inverse probability weighting, rely on the assumption that data are missing at random and can yield biased results if this assumption is violated. In observational studies with outcome data missing not at random, Heckman&#39;s sample selection model can be used to adjust for bias due to missing data. In this paper, we review Heckman&#39;s method and a similar approach proposed by Tchetgen Tchetgen and Wirth (2017). We then discuss how to apply these methods to Mendelian randomization analyses using individual-level data, with missing data for either the exposure or outcome or both. We explore whether genetic variants associated with participation can be used as instruments for selection. We then describe how to obtain missingness-adjusted Wald ratio, two-stage least squares and inverse variance weighted estimates. The two methods are evaluated and compared in simulations, with results suggesting that they can both mitigate selection bias but may yield parameter estimates with large standard errors in some settings. In an illustrative real-data application, we investigate the effects of body mass index on smoking using data from the Avon Longitudinal Study of Parents and Children.},
  archive      = {J_SIM},
  author       = {Apostolos Gkatzionis and Eric J. Tchetgen Tchetgen and Jon Heron and Kate Northstone and Kate Tilling},
  doi          = {10.1002/sim.10173},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4250-4271},
  shortjournal = {Stat. Med.},
  title        = {Using instruments for selection to adjust for selection bias in mendelian randomization},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transportability of model-based estimands in evidence
synthesis. <em>SIM</em>, <em>43</em>(22), 4217–4249. (<a
href="https://doi.org/10.1002/sim.10111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In evidence synthesis, effect modifiers are typically described as variables that induce treatment effect heterogeneity at the individual level, through treatment-covariate interactions in an outcome model parametrized at such level. As such, effect modification is defined with respect to a conditional measure, but marginal effect estimates are required for population-level decisions in health technology assessment. For noncollapsible measures, purely prognostic variables that are not determinants of treatment response at the individual level may modify marginal effects, even where there is individual-level treatment effect homogeneity. With heterogeneity, marginal effects for measures that are not directly collapsible cannot be expressed in terms of marginal covariate moments, and generally depend on the joint distribution of conditional effect measure modifiers and purely prognostic variables. There are implications for recommended practices in evidence synthesis. Unadjusted anchored indirect comparisons can be biased in the absence of individual-level treatment effect heterogeneity, or when marginal covariate moments are balanced across studies. Covariate adjustment may be necessary to account for cross-study imbalances in joint covariate distributions involving purely prognostic variables. In the absence of individual patient data for the target, covariate adjustment approaches are inherently limited in their ability to remove bias for measures that are not directly collapsible. Directly collapsible measures would facilitate the transportability of marginal effects between studies by: (1) reducing dependence on model-based covariate adjustment where there is individual-level treatment effect homogeneity or marginal covariate moments are balanced; and (2) facilitating the selection of baseline covariates for adjustment where there is individual-level treatment effect heterogeneity.},
  archive      = {J_SIM},
  author       = {Antonio Remiro-Azócar},
  doi          = {10.1002/sim.10111},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4217-4249},
  shortjournal = {Stat. Med.},
  title        = {Transportability of model-based estimands in evidence synthesis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calibrating machine learning approaches for probability
estimation: A short expansion. <em>SIM</em>, <em>43</em>(21), 4212–4215.
(<a href="https://doi.org/10.1002/sim.10051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Francisco M. Ojeda and Stuart G. Baker and Andreas Ziegler},
  doi          = {10.1002/sim.10051},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4212-4215},
  shortjournal = {Stat. Med.},
  title        = {Calibrating machine learning approaches for probability estimation: A short expansion},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An augmented illness-death model for semi-competing risks
with clinically immediate terminal events. <em>SIM</em>,
<em>43</em>(21), 4194–4211. (<a
href="https://doi.org/10.1002/sim.10181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preeclampsia is a pregnancy-associated condition posing risks of both fetal and maternal mortality and morbidity that can only resolve following delivery and removal of the placenta. Because in its typical form preeclampsia can arise before delivery, but not after, these two events exemplify the time-to-event setting of “semi-competing risks” in which a non-terminal event of interest is subject to the occurrence of a terminal event of interest. The semi-competing risks framework presents a valuable opportunity to simultaneously address two clinically meaningful risk modeling tasks: (i) characterizing risk of developing preeclampsia, and (ii) characterizing time to delivery after onset of preeclampsia. However, some people with preeclampsia deliver immediately upon diagnosis, while others are admitted and monitored for an extended period before giving birth, resulting in two distinct trajectories following the non-terminal event, which we call “clinically immediate” and “non-immediate” terminal events. Though such phenomena arise in many clinical contexts, to-date there have not been methods developed to acknowledge the complex dependencies between such outcomes, nor leverage these phenomena to gain new insight into individualized risk. We address this gap by proposing a novel augmented frailty-based illness-death model with a binary submodel to distinguish risk of immediate terminal event following the non-terminal event. The model admits direct dependence of the terminal event on the non-terminal event through flexible regression specification, as well as indirect dependence via a shared frailty term linking each submodel. We develop an efficient Bayesian sampler for estimation and corresponding model fit metrics, and derive formulae for dynamic risk prediction. In an extended example using pregnancy outcome data from an electronic health record, we demonstrate the proposed model&#39;s direct applicability to address a broad range of clinical questions.},
  archive      = {J_SIM},
  author       = {Harrison T. Reeder and Kyu Ha Lee and Stefania I. Papatheodorou and Sebastien Haneuse},
  doi          = {10.1002/sim.10181},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4194-4211},
  shortjournal = {Stat. Med.},
  title        = {An augmented illness-death model for semi-competing risks with clinically immediate terminal events},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian modeling of spatial ordinal data from health
surveys. <em>SIM</em>, <em>43</em>(21), 4178–4193. (<a
href="https://doi.org/10.1002/sim.10166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health surveys allow exploring health indicators that are of great value from a public health point of view and that cannot normally be studied from regular health registries. These indicators are usually coded as ordinal variables and may depend on covariates associated with individuals. In this article, we propose a Bayesian individual-level model for small-area estimation of survey-based health indicators. A categorical likelihood is used at the first level of the model hierarchy to describe the ordinal data, and spatial dependence among small areas is taken into account by using a conditional autoregressive distribution. Post-stratification of the results of the proposed individual-level model allows extrapolating the results to any administrative areal division, even for small areas. We apply this methodology to describe the geographical distribution of a self-perceived health indicator from the Health Survey of the Region of Valencia (Spain) for the year 2016.},
  archive      = {J_SIM},
  author       = {Miguel Ángel Beltrán-Sánchez and Miguel-Angel Martinez-Beneito and Ana Corberán-Vallet},
  doi          = {10.1002/sim.10166},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4178-4193},
  shortjournal = {Stat. Med.},
  title        = {Bayesian modeling of spatial ordinal data from health surveys},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A latent variable approach to jointly modeling longitudinal
and cumulative event data using a weighted two-stage method.
<em>SIM</em>, <em>43</em>(21), 4163–4177. (<a
href="https://doi.org/10.1002/sim.10171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ecological momentary assessment (EMA), a data collection method commonly employed in mHealth studies, allows for repeated real-time sampling of individuals&#39; psychological, behavioral, and contextual states. Due to the frequent measurements, data collected using EMA are useful for understanding both the temporal dynamics in individuals&#39; states and how these states relate to adverse health events. Motivated by data from a smoking cessation study, we propose a joint model for analyzing longitudinal EMA data to determine whether certain latent psychological states are associated with repeated cigarette use. Our method consists of a longitudinal submodel—a dynamic factor model—that models changes in the time-varying latent states and a cumulative risk submodel—a Poisson regression model—that connects the latent states with the total number of events. In the motivating data, both the predictors—the underlying psychological states—and the event outcome—the number of cigarettes smoked—are partially unobservable; we account for this incomplete information in our proposed model and estimation method. We take a two-stage approach to estimation that leverages existing software and uses importance sampling-based weights to reduce potential bias. We demonstrate that these weights are effective at reducing bias in the cumulative risk submodel parameters via simulation. We apply our method to a subset of data from a smoking cessation study to assess the association between psychological state and cigarette smoking. The analysis shows that above-average intensities of negative mood are associated with increased cigarette use.},
  archive      = {J_SIM},
  author       = {Madeline R. Abbott and Inbal Nahum-Shani and Cho Y. Lam and Lindsey N. Potter and David W. Wetter and Walter H. Dempsey},
  doi          = {10.1002/sim.10171},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4163-4177},
  shortjournal = {Stat. Med.},
  title        = {A latent variable approach to jointly modeling longitudinal and cumulative event data using a weighted two-stage method},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extending the DeLong algorithm for comparing areas under
correlated receiver operating characteristic curves with missing data.
<em>SIM</em>, <em>43</em>(21), 4148–4162. (<a
href="https://doi.org/10.1002/sim.10172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nonparametric method proposed by DeLong et al in 1988 for comparing areas under correlated receiver operating characteristic curves is used widely in practice. However, the DeLong method as implemented in popular software quietly deletes individuals with any missing values, yielding potentially invalid and/or inefficient results. We simplify the DeLong algorithm using ranks and extend it to accommodate missing data by using a mixed model approach for multivariate data. Simulation results demonstrate the validity and efficiency of our procedure for data missing at random. We illustrate our proposed procedure in SAS, Stata, and R using the original DeLong data.},
  archive      = {J_SIM},
  author       = {Lily Zou and Yun-Hee Choi and Leonardo Guizzetti and Di Shu and Joshua Zou and Guangyong Zou},
  doi          = {10.1002/sim.10172},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4148-4162},
  shortjournal = {Stat. Med.},
  title        = {Extending the DeLong algorithm for comparing areas under correlated receiver operating characteristic curves with missing data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic undirected graphical models for time-varying
clinical symptom and neuroimaging networks. <em>SIM</em>,
<em>43</em>(21), 4131–4147. (<a
href="https://doi.org/10.1002/sim.10143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose methods to examine how the complex interrelationships between clinical symptoms and, separately, brain imaging biomarkers change over time leading up to the diagnosis of a disease in subjects with a known genetic near-certainty of disease. We propose a time-dependent undirected graphical model that ensures temporal and structural smoothness across time-specific networks to examine the trajectories of interactions between markers aligned at the time of disease onset. Specifically, we anchor subjects relative to the time of disease diagnosis (anchoring time) as in a revival process, and we estimate networks at each time point of interest relative to the anchoring time. To use all available data, we apply kernel weights to borrow information across observations that are close to the time of interest. Adaptive lasso weights are introduced to encourage temporal smoothness in edge strength, while a novel elastic fused- penalty removes spurious edges and encourages temporal smoothness in network structure. Our approach can handle practical complications such as unbalanced visit times. We conduct simulation studies to compare our approach with existing methods. We then apply our method to data from PREDICT-HD, a large prospective observational study of pre-manifest Huntington&#39;s disease (HD) patients, to identify symptom and imaging network changes that precede clinical diagnosis of HD.},
  archive      = {J_SIM},
  author       = {Erin I. McDonnell and Shanghong Xie and Karen Marder and Fanyu Cui and Yuanjia Wang},
  doi          = {10.1002/sim.10143},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4131-4147},
  shortjournal = {Stat. Med.},
  title        = {Dynamic undirected graphical models for time-varying clinical symptom and neuroimaging networks},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prefiltered component-based greedy (PreCoG) scan method.
<em>SIM</em>, <em>43</em>(21), 4113–4130. (<a
href="https://doi.org/10.1002/sim.10170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial distribution of disease cases can provide important insights into disease spread and its potential risk factors. Identifying disease clusters correctly can help us discover new risk factors and inform interventions to control and prevent the spread of disease as quickly as possible. In this study, we propose a novel scan method, the Prefiltered Component-based Greedy (PreCoG) scan method, which efficiently and accurately detects irregularly shaped clusters using a prefiltered component-based algorithm. The PreCoG scan method&#39;s flexibility allows it to perform well in detecting both regularly and irregularly-shaped clusters. Additionally, it is fast to apply while providing high power, sensitivity, and positive predictive value for the detected clusters compared to other scan methods. To confirm the effectiveness of the PreCoG method, we compare its performance to many other scan methods. Additionally, we have implemented this method in the smerc R package to make it publicly available to other researchers. Our proposed PreCoG scan method presents a unique and innovative process for detecting disease clusters and can improve the accuracy of disease surveillance systems.},
  archive      = {J_SIM},
  author       = {Joshua P. French and Mohammad Meysami and Ettie M. Lipner},
  doi          = {10.1002/sim.10170},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4113-4130},
  shortjournal = {Stat. Med.},
  title        = {Prefiltered component-based greedy (PreCoG) scan method},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Determining sample size in a personalized randomized
controlled (PRACTical) trial. <em>SIM</em>, <em>43</em>(21), 4098–4112.
(<a href="https://doi.org/10.1002/sim.10168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical settings with no commonly accepted standard-of-care, multiple treatment regimens are potentially useful, but some treatments may not be appropriate for some patients. A personalized randomized controlled trial (PRACTical) design has been proposed for this setting. For a network of treatments, each patient is randomized only among treatments which are appropriate for them. The aim is to produce treatment rankings that can inform clinical decisions about treatment choices for individual patients. Here we propose methods for determining sample size in a PRACTical design, since standard power-based methods are not applicable. We derive a sample size by evaluating information gained from trials of varying sizes. For a binary outcome, we quantify how many adverse outcomes would be prevented by choosing the top-ranked treatment for each patient based on trial results rather than choosing a random treatment from the appropriate personalized randomization list. In simulations, we evaluate three performance measures: mean reduction in adverse outcomes using sample information, proportion of simulated patients for whom the top-ranked treatment performed as well or almost as well as the best appropriate treatment, and proportion of simulated trials in which the top-ranked treatment performed better than a randomly chosen treatment. We apply the methods to a trial evaluating eight different combination antibiotic regimens for neonatal sepsis (NeoSep1), in which a PRACTical design addresses varying patterns of antibiotic choice based on disease characteristics and resistance. Our proposed approach produces results that are more relevant to complex decision making by clinicians and policy makers.},
  archive      = {J_SIM},
  author       = {Rebecca M. Turner and Kim May Lee and A. Sarah Walker and Sally Ellis and Michael Sharland and Julia A. Bielicki and Wolfgang Stöhr and Ian R. White},
  doi          = {10.1002/sim.10168},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4098-4112},
  shortjournal = {Stat. Med.},
  title        = {Determining sample size in a personalized randomized controlled (PRACTical) trial},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting relationship directionality to enhance
statistical modeling of peer-influence across social networks.
<em>SIM</em>, <em>43</em>(21), 4073–4097. (<a
href="https://doi.org/10.1002/sim.10169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risky-prescribing is the excessive or inappropriate prescription of drugs that singly or in combination pose significant risks of adverse health outcomes. In the United States, prescribing of opioids and other “risky” drugs is a national public health concern. We use a novel data framework—a directed network connecting physicians who encounter the same patients in a sequence of visits—to investigate if risky-prescribing diffuses across physicians through a process of peer-influence. Using a shared-patient network of 10 661 Ohio-based physicians constructed from Medicare claims data over 2014-2015, we extract information on the order in which patients encountered physicians to derive a directed patient-sharing network. This enables the novel decomposition of peer-effects of a medical practice such as risky-prescribing into directional (outbound and inbound) and bidirectional (mutual) relationship components. Using this framework, we develop models of peer-effects for contagion in risky-prescribing behavior as well as spillover effects. The latter is measured in terms of adverse health events suspected to be related to risky-prescribing in patients of peer-physicians. Estimated peer-effects were strongest when the patient-sharing relationship was mutual as opposed to directional. Using simulations we confirmed that our modeling and estimation strategies allows simultaneous estimation of each type of peer-effect (mutual and directional) with accuracy and precision. We also show that failing to account for these distinct mechanisms (a form of model mis-specification) produces misleading results, demonstrating the importance of retaining directional information in the construction of physician shared-patient networks. These findings suggest network-based interventions for reducing risky-prescribing.},
  archive      = {J_SIM},
  author       = {Xin Ran and Nancy E. Morden and Ellen Meara and Erika L. Moen and Daniel N. Rockmore and A. James O&#39;Malley},
  doi          = {10.1002/sim.10169},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4073-4097},
  shortjournal = {Stat. Med.},
  title        = {Exploiting relationship directionality to enhance statistical modeling of peer-influence across social networks},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sequential, multiple assignment, randomized trial design
with a tailoring function. <em>SIM</em>, <em>43</em>(21), 4055–4072. (<a
href="https://doi.org/10.1002/sim.10161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a trial design for sequential multiple assignment randomized trials (SMARTs) that use a tailoring function instead of a binary tailoring variable allowing for simultaneous development of the tailoring variable and estimation of dynamic treatment regimens (DTRs). We apply methods for developing DTRs from observational data: tree-based regression learning and Q-learning. We compare this to a balanced randomized SMART with equal re-randomization probabilities and a typical SMART design where re-randomization depends on a binary tailoring variable and DTRs are analyzed with weighted and replicated regression. This project addresses a gap in clinical trial methodology by presenting SMARTs where second stage treatment is based on a continuous outcome removing the need for a binary tailoring variable. We demonstrate that data from a SMART using a tailoring function can be used to efficiently estimate DTRs and is more flexible under varying scenarios than a SMART using a tailoring variable.},
  archive      = {J_SIM},
  author       = {Holly Hartman and Matthew Schipper and Kelley Kidwell},
  doi          = {10.1002/sim.10161},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4055-4072},
  shortjournal = {Stat. Med.},
  title        = {A sequential, multiple assignment, randomized trial design with a tailoring function},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian semi-parametric scalar-on-function regression
with measurement error using instrumental variables. <em>SIM</em>,
<em>43</em>(21), 4043–4054. (<a
href="https://doi.org/10.1002/sim.10165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable devices such as the ActiGraph are now commonly used in research to monitor or track physical activity. This trend corresponds with the growing need to assess the relationships between physical activity and health outcomes, such as obesity, accurately. Device-based physical activity measures are best treated as functions when assessing their associations with scalar-valued outcomes such as body mass index. Scalar-on-function regression (SoFR) is a suitable regression model in this setting. Most estimation approaches in SoFR assume that the measurement error in functional covariates is white noise. Violating this assumption can lead to underestimating model parameters. There are limited approaches to correcting measurement errors for frequentist methods and none for Bayesian methods in this area. We present a non-parametric Bayesian measurement error-corrected SoFR model that relaxes all the constraining assumptions often involved with these models. Our estimation relies on an instrumental variable allowing a time-varying biasing factor, a significant departure from the current generalized method of moment (GMM) approach. Our proposed method also permits model-based grouping of the functional covariate following measurement error correction. This grouping of the measurement error-corrected functional covariate allows additional ease of interpretation of how the different groups differ. Our method is easy to implement, and we demonstrate its finite sample properties in extensive simulations. Finally, we applied our method to data from the National Health and Examination Survey to assess the relationship between wearable device-based measures of physical activity and body mass index in adults in the United States.},
  archive      = {J_SIM},
  author       = {Roger S Zoh and Yuanyuan Luan and Lan Xue and David B Allison and Carmen D Tekwe},
  doi          = {10.1002/sim.10165},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4043-4054},
  shortjournal = {Stat. Med.},
  title        = {A bayesian semi-parametric scalar-on-function regression with measurement error using instrumental variables},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariate-adjusted generalized pairwise comparisons in small
samples. <em>SIM</em>, <em>43</em>(21), 4027–4042. (<a
href="https://doi.org/10.1002/sim.10140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semiparametric probabilistic index models allow for the comparison of two groups of observations, whilst adjusting for covariates, thereby fitting nicely within the framework of generalized pairwise comparisons (GPC). As with most regression approaches in this setting, the limited amount of data results in invalid inference as the asymptotic normality assumption is not met. In addition, separation issues might arise when considering small samples. In this article, we show that the parameters of the probabilistic index model can be estimated using generalized estimating equations, for which adjustments exist that lead to estimators of the sandwich variance-covariance matrix with improved finite sample properties and that can deal with bias due to separation. In this way, appropriate inference can be performed as is shown through extensive simulation studies. The known relationships between the probabilistic index and other GPC statistics allow to also provide valid inference for example, the net treatment benefit or the success odds.},
  archive      = {J_SIM},
  author       = {Stijn Jaspers and Johan Verbeeck and Olivier Thas},
  doi          = {10.1002/sim.10140},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4027-4042},
  shortjournal = {Stat. Med.},
  title        = {Covariate-adjusted generalized pairwise comparisons in small samples},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BHAFT: Bayesian heredity-constrained accelerated failure
time models for detecting gene-environment interactions in survival
analysis. <em>SIM</em>, <em>43</em>(21), 4013–4026. (<a
href="https://doi.org/10.1002/sim.10145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In addition to considering the main effects, understanding gene-environment (G × E) interactions is imperative for determining the etiology of diseases and the factors that affect their prognosis. In the existing statistical framework for censored survival outcomes, there are several challenges in detecting G × E interactions, such as handling high-dimensional omics data, diverse environmental factors, and algorithmic complications in survival analysis. The effect heredity principle has widely been used in studies involving interaction identification because it incorporates the dependence of the main and interaction effects. However, Bayesian survival models that incorporate the assumption of this principle have not been developed. Therefore, we propose Bayesian heredity-constrained accelerated failure time (BHAFT) models for identifying main and interaction (M-I) effects with novel spike-and-slab or regularized horseshoe priors to incorporate the assumption of effect heredity principle. The R package rstan was used to fit the proposed models. Extensive simulations demonstrated that BHAFT models had outperformed other existing models in terms of signal identification, coefficient estimation, and prognosis prediction. Biologically plausible G × E interactions associated with the prognosis of lung adenocarcinoma were identified using our proposed model. Notably, BHAFT models incorporating the effect heredity principle could identify both main and interaction effects, which are highly useful in exploring G × E interactions in high-dimensional survival analysis. The code and data used in our paper are available at https://github.com/SunNa-bayesian/BHAFT .},
  archive      = {J_SIM},
  author       = {Na Sun and Jiadong Chu and Qida He and Yu Wang and Qiang Han and Nengjun Yi and Ruyang Zhang and Yueping Shen},
  doi          = {10.1002/sim.10145},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4013-4026},
  shortjournal = {Stat. Med.},
  title        = {BHAFT: Bayesian heredity-constrained accelerated failure time models for detecting gene-environment interactions in survival analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Erratum to “joint modeling for censored predictors due to
detection limits with applications to metabolites data.” <em>SIM</em>,
<em>43</em>(20), 4011. (<a
href="https://doi.org/10.1002/sim.10190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.10190},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4011},
  shortjournal = {Stat. Med.},
  title        = {Erratum to “Joint modeling for censored predictors due to detection limits with applications to metabolites data”},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian survival analysis with INLA. <em>SIM</em>,
<em>43</em>(20), 3975–4010. (<a
href="https://doi.org/10.1002/sim.10160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This tutorial shows how various Bayesian survival models can be fitted using the integrated nested Laplace approximation in a clear, legible, and comprehensible manner using the INLA and INLAjoint R-packages. Such models include accelerated failure time, proportional hazards, mixture cure, competing risks, multi-state, frailty, and joint models of longitudinal and survival data, originally presented in the article “ Bayesian survival analysis with BUGS .” In addition, we illustrate the implementation of a new joint model for a longitudinal semicontinuous marker, recurrent events, and a terminal event. Our proposal aims to provide the reader with syntax examples for implementing survival models using a fast and accurate approximate Bayesian inferential approach.},
  archive      = {J_SIM},
  author       = {Danilo Alvares and Janet van Niekerk and Elias Teixeira Krainski and Håvard Rue and Denis Rustand},
  doi          = {10.1002/sim.10160},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3975-4010},
  shortjournal = {Stat. Med.},
  title        = {Bayesian survival analysis with INLA},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian non-stationary heteroskedastic time series model
for multivariate critical care data. <em>SIM</em>, <em>43</em>(20),
3958–3974. (<a href="https://doi.org/10.1002/sim.10154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multivariate GARCH model for non-stationary health time series by modifying the observation-level variance of the standard state space model. The proposed model provides an intuitive and novel way of dealing with heteroskedastic data using the conditional nature of state-space models. We follow the Bayesian paradigm to perform the inference procedure. In particular, we use Markov chain Monte Carlo methods to obtain samples from the resultant posterior distribution. We use the forward filtering backward sampling algorithm to efficiently obtain samples from the posterior distribution of the latent state. The proposed model also handles missing data in a fully Bayesian fashion. We validate our model on synthetic data and analyze a data set obtained from an intensive care unit in a Montreal hospital and the MIMIC dataset. We further show that our proposed models offer better performance, in terms of WAIC than standard state space models. The proposed model provides a new way to model multivariate heteroskedastic non-stationary time series data. Model comparison can then be easily performed using the WAIC.},
  archive      = {J_SIM},
  author       = {Zayd Omar and David A. Stephens and Alexandra M. Schmidt and David L. Buckeridge},
  doi          = {10.1002/sim.10154},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3958-3974},
  shortjournal = {Stat. Med.},
  title        = {A bayesian non-stationary heteroskedastic time series model for multivariate critical care data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent classification model for censored longitudinal binary
outcome. <em>SIM</em>, <em>43</em>(20), 3943–3957. (<a
href="https://doi.org/10.1002/sim.10156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent classification model is a class of statistical methods for identifying unobserved class membership among the study samples using some observed data. In this study, we proposed a latent classification model that takes a censored longitudinal binary outcome variable and uses its changing pattern over time to predict individuals&#39; latent class membership. Assuming the time-dependent outcome variables follow a continuous-time Markov chain, the proposed method has two primary goals: (1) estimate the distribution of the latent classes and predict individuals&#39; class membership, and (2) estimate the class-specific transition rates and rate ratios. To assess the model&#39;s performance, we conducted a simulation study and verified that our algorithm produces accurate model estimates (ie, small bias) with reasonable confidence intervals (ie, achieving approximately 95% coverage probability). Furthermore, we compared our model to four other existing latent class models and demonstrated that our approach yields higher prediction accuracies for latent classes. We applied our proposed method to analyze the COVID-19 data in Houston, Texas, US collected between January first 2021 and December 31st 2021. Early reports on the COVID-19 pandemic showed that the severity of a SARS-CoV-2 infection tends to vary greatly by cases. We found that while demographic characteristics explain some of the differences in individuals&#39; experience with COVID-19, some unaccounted-for latent variables were associated with the disease.},
  archive      = {J_SIM},
  author       = {Jacky C. Kuo and Wenyaw Chan and Luis Leon-Novelo and David R. Lairson and Armand Brown and Kayo Fujimoto},
  doi          = {10.1002/sim.10156},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3943-3957},
  shortjournal = {Stat. Med.},
  title        = {Latent classification model for censored longitudinal binary outcome},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploratory subgroup identification in the heterogeneous cox
model: A relatively simple procedure. <em>SIM</em>, <em>43</em>(20),
3921–3942. (<a href="https://doi.org/10.1002/sim.10163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For survival analysis applications we propose a novel procedure for identifying subgroups with large treatment effects, with focus on subgroups where treatment is potentially detrimental. The approach, termed forest search, is relatively simple and flexible. All-possible subgroups are screened and selected based on hazard ratio thresholds indicative of harm with assessment according to the standard Cox model. By reversing the role of treatment one can seek to identify substantial benefit. We apply a splitting consistency criteria to identify a subgroup considered “maximally consistent with harm.” The type-1 error and power for subgroup identification can be quickly approximated by numerical integration. To aid inference we describe a bootstrap bias-corrected Cox model estimator with variance estimated by a Jacknife approximation. We provide a detailed evaluation of operating characteristics in simulations and compare to virtual twins and generalized random forests where we find the proposal to have favorable performance. In particular, in our simulation setting, we find the proposed approach favorably controls the type-1 error for falsely identifying heterogeneity with higher power and classification accuracy for substantial heterogeneous effects. Two real data applications are provided for publicly available datasets from a clinical trial in oncology, and HIV.},
  archive      = {J_SIM},
  author       = {Larry F. León and Thomas Jemielita and Zifang Guo and Rachel Marceau West and Keaven M. Anderson},
  doi          = {10.1002/sim.10163},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3921-3942},
  shortjournal = {Stat. Med.},
  title        = {Exploratory subgroup identification in the heterogeneous cox model: A relatively simple procedure},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing dispersion in mis-measured multivariate binomial
outcomes: A novel statistical approach for detecting differentially
methylated regions in bisulfite sequencing data. <em>SIM</em>,
<em>43</em>(20), 3899–3920. (<a
href="https://doi.org/10.1002/sim.10149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a DNA methylation application, this article addresses the problem of fitting and inferring a multivariate binomial regression model for outcomes that are contaminated by errors and exhibit extra-parametric variations, also known as dispersion. While dispersion in univariate binomial regression has been extensively studied, addressing dispersion in the context of multivariate outcomes remains a complex and relatively unexplored task. The complexity arises from a noteworthy data characteristic observed in our motivating dataset: non-constant yet correlated dispersion across outcomes. To address this challenge and account for possible measurement error, we propose a novel hierarchical quasi-binomial varying coefficient mixed model, which enables flexible dispersion patterns through a combination of additive and multiplicative dispersion components. To maximize the Laplace-approximated quasi-likelihood of our model, we further develop a specialized two-stage expectation-maximization (EM) algorithm, where a plug-in estimate for the multiplicative scale parameter enhances the speed and stability of the EM iterations. Simulations demonstrated that our approach yields accurate inference for smooth covariate effects and exhibits excellent power in detecting non-zero effects. Additionally, we applied our proposed method to investigate the association between DNA methylation, measured across the genome through targeted custom capture sequencing of whole blood, and levels of anti-citrullinated protein antibodies (ACPA), a preclinical marker for rheumatoid arthritis (RA) risk. Our analysis revealed 23 significant genes that potentially contribute to ACPA-related differential methylation, highlighting the relevance of cell signaling and collagen metabolism in RA. We implemented our method in the R Bioconductor package called “SOMNiBUS.”},
  archive      = {J_SIM},
  author       = {Kaiqiong Zhao and Karim Oualkacha and Yixiao Zeng and Cathy Shen and Kathleen Klein and Lajmi Lakhal-Chaieb and Aurélie Labbe and Tomi Pastinen and Marie Hudson and Inés Colmegna and Sasha Bernatsky and Celia M. T. Greenwood},
  doi          = {10.1002/sim.10149},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3899-3920},
  shortjournal = {Stat. Med.},
  title        = {Addressing dispersion in mis-measured multivariate binomial outcomes: A novel statistical approach for detecting differentially methylated regions in bisulfite sequencing data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Review of weighted exponential random graph models
frameworks applied to neuroimaging. <em>SIM</em>, <em>43</em>(20),
3881–3898. (<a href="https://doi.org/10.1002/sim.10162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuro-imaging data can often be represented as statistical networks, especially for functional magnetic resonance imaging (fMRI) data, where brain regions are defined as nodes and the functional interactions between those regions are taken as edges. Such networks are commonly divided into classes depending on the type of edges, namely binary or weighted. A binary network means edges can either be present or absent. Whereas the edges of a weighted network are associated with weight values, and fMRI networks belong to weighted networks. Statistical methods are often adopted to analyse such networks, among which, the exponential random graph model (ERGM) is an important network analysis approach. Typically ERGMs are applied to binary networks, and weighted networks often need to be binarised by arbitrarily selecting a threshold value to define the presence of the edges, which can lead to non-robustness and loss of valuable edge weight information representing the strength of fMRI interaction in fMRI networks. While it is therefore important to gain deeper insight in adopting ERGM on weighted networks, there only exists a few different ERGM frameworks for weighted networks; some of these are not directly implementable on fMRI networks based on their original proposal. We systematically review, implement, analyse and compare five such frameworks via a simulation study and provide guidelines on each modelling framework as well as conclude the suitability of them on fMRI networks based on a range of criteria. We concluded that Multi-Layered ERGM is currently the most suitable framework.},
  archive      = {J_SIM},
  author       = {Yefeng Fan and Simon R. White},
  doi          = {10.1002/sim.10162},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3881-3898},
  shortjournal = {Stat. Med.},
  title        = {Review of weighted exponential random graph models frameworks applied to neuroimaging},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multivariate to multivariate approach for voxel-wise
genome-wide association analysis. <em>SIM</em>, <em>43</em>(20),
3862–3880. (<a href="https://doi.org/10.1002/sim.10101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint analysis of imaging-genetics data facilitates the systematic investigation of genetic effects on brain structures and functions with spatial specificity. We focus on voxel-wise genome-wide association analysis, which may involve trillions of single nucleotide polymorphism (SNP)-voxel pairs. We attempt to identify underlying organized association patterns of SNP-voxel pairs and understand the polygenic and pleiotropic networks on brain imaging traits. We propose a bi-clique graph structure (ie, a set of SNPs highly correlated with a cluster of voxels) for the systematic association pattern. Next, we develop computational strategies to detect latent SNP-voxel bi-cliques and an inference model for statistical testing. We further provide theoretical results to guarantee the accuracy of our computational algorithms and statistical inference. We validate our method by extensive simulation studies, and then apply it to the whole genome genetic and voxel-level white matter integrity data collected from 1052 participants of the human connectome project. The results demonstrate multiple genetic loci influencing white matter integrity measures on splenium and genu of the corpus callosum.},
  archive      = {J_SIM},
  author       = {Qiong Wu and Yuan Zhang and Xiaoqi Huang and Tianzhou Ma and L. Elliot Hong and Peter Kochunov and Shuo Chen},
  doi          = {10.1002/sim.10101},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3862-3880},
  shortjournal = {Stat. Med.},
  title        = {A multivariate to multivariate approach for voxel-wise genome-wide association analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential network knockoff filter with application to
brain connectivity analysis. <em>SIM</em>, <em>43</em>(20), 3830–3861.
(<a href="https://doi.org/10.1002/sim.10155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain functional connectivity can typically be represented as a brain functional network, where nodes represent regions of interest (ROIs) and edges symbolize their connections. Studying group differences in brain functional connectivity can help identify brain regions and recover the brain functional network linked to neurodegenerative diseases. This process, known as differential network analysis focuses on the differences between estimated precision matrices for two groups. Current methods struggle with individual heterogeneity in measuring the brain connectivity, false discovery rate (FDR) control, and accounting for confounding factors, resulting in biased estimates and diminished power. To address these issues, we present a two-stage FDR-controlled feature selection method for differential network analysis using functional magnetic resonance imaging (fMRI) data. First, we create individual brain connectivity measures using a high-dimensional precision matrix estimation technique. Next, we devise a penalized logistic regression model that employs individual brain connectivity data and integrates a new knockoff filter for FDR control when detecting significant differential edges. Through extensive simulations, we showcase the superiority of our approach compared to other methods. Additionally, we apply our technique to fMRI data to identify differential edges between Alzheimer&#39;s disease and control groups. Our results are consistent with prior experimental studies, emphasizing the practical applicability of our method.},
  archive      = {J_SIM},
  author       = {Jiadong Ji and Zhendong Hou and Yong He and Lei Liu and Fuzhong Xue and Hao Chen and Zhongshang Yuan},
  doi          = {10.1002/sim.10155},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3830-3861},
  shortjournal = {Stat. Med.},
  title        = {Differential network knockoff filter with application to brain connectivity analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Propensity score weighted multi-source exchangeability
models for incorporating external control data in randomized clinical
trials. <em>SIM</em>, <em>43</em>(20), 3815–3829. (<a
href="https://doi.org/10.1002/sim.10158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among clinical trialists, there has been a growing interest in using external data to improve decision-making and accelerate drug development in randomized clinical trials (RCTs). Here we propose a novel approach that combines the propensity score weighting (PW) and the multi-source exchangeability modelling (MEM) approaches to augment the control arm of a RCT in the rare disease setting. First, propensity score weighting is used to construct weighted external controls that have similar observed pre-treatment characteristics as the current trial population. Next, the MEM approach evaluates the similarity in outcome distributions between the weighted external controls and the concurrent control arm. The amount of external data we borrow is determined by the similarities in pretreatment characteristics and outcome distributions. The proposed approach can be applied to binary, continuous and count data. We evaluate the performance of the proposed PW-MEM method and several competing approaches based on simulation and re-sampling studies. Our results show that the PW-MEM approach improves the precision of treatment effect estimates while reducing the biases associated with borrowing data from external sources.},
  archive      = {J_SIM},
  author       = {Wei Wei and Yunxuan Zhang and Satrajit Roychoudhury},
  doi          = {10.1002/sim.10158},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3815-3829},
  shortjournal = {Stat. Med.},
  title        = {Propensity score weighted multi-source exchangeability models for incorporating external control data in randomized clinical trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrative deep learning with prior assisted feature
selection. <em>SIM</em>, <em>43</em>(20), 3792–3814. (<a
href="https://doi.org/10.1002/sim.10148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrative analysis has emerged as a prominent tool in biomedical research, offering a solution to the “small n $$ n $$ and large p $$ p $$ ” challenge. Leveraging the powerful capabilities of deep learning in extracting complex relationship between genes and diseases, our objective in this study is to incorporate deep learning into the framework of integrative analysis. Recognizing the redundancy within candidate features, we introduce a dedicated feature selection layer in the proposed integrative deep learning method. To further improve the performance of feature selection, the rich previous researches are utilized by an ensemble learning method to identify “prior information”. This leads to the proposed prior assisted integrative deep learning (PANDA) method. We demonstrate the superiority of the PANDA method through a series of simulation studies, showing its clear advantages over competing approaches in both feature selection and outcome prediction. Finally, a skin cutaneous melanoma (SKCM) dataset is extensively analyzed by the PANDA method to show its practical application.},
  archive      = {J_SIM},
  author       = {Feifei Wang and Ke Jia and Yang Li},
  doi          = {10.1002/sim.10148},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3792-3814},
  shortjournal = {Stat. Med.},
  title        = {Integrative deep learning with prior assisted feature selection},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust inference methods for meta-analysis involving
influential outlying studies. <em>SIM</em>, <em>43</em>(20), 3778–3791.
(<a href="https://doi.org/10.1002/sim.10157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis is an essential tool to comprehensively synthesize and quantitatively evaluate results of multiple clinical studies in evidence-based medicine. In many meta-analyses, the characteristics of some studies might markedly differ from those of the others, and these outlying studies can generate biases and potentially yield misleading results. In this article, we provide effective robust statistical inference methods using generalized likelihoods based on the density power divergence. The robust inference methods are designed to adjust the influences of outliers through the use of modified estimating equations based on a robust criterion, even when multiple and serious influential outliers are present. We provide the robust estimators, statistical tests, and confidence intervals via the generalized likelihoods for the fixed-effect and random-effects models of meta-analysis. We also assess the contribution rates of individual studies to the robust overall estimators that indicate how the influences of outlying studies are adjusted. Through simulations and applications to two recently published systematic reviews, we demonstrate that the overall conclusions and interpretations of meta-analyses can be markedly changed if the robust inference methods are applied and that only the conventional inference methods might produce misleading evidence. These methods would be recommended to be used at least as a sensitivity analysis method in the practice of meta-analysis. We have also developed an R package, robustmeta , that implements the robust inference methods.},
  archive      = {J_SIM},
  author       = {Hisashi Noma and Shonosuke Sugasawa and Toshi A. Furukawa},
  doi          = {10.1002/sim.10157},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3778-3791},
  shortjournal = {Stat. Med.},
  title        = {Robust inference methods for meta-analysis involving influential outlying studies},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Renewable risk assessment of heterogeneous streaming
time-to-event cohorts. <em>SIM</em>, <em>43</em>(20), 3761–3777. (<a
href="https://doi.org/10.1002/sim.10146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of streaming time-to-event cohorts has garnered significant research attention. Most existing methods require observed cohorts from a study sequence to be independent and identically sampled from a common model. This assumption may be easily violated in practice. Our methodology operates within the framework of online data updating, where risk estimates for each cohort of interest are continuously refreshed using the latest observations and historical summary statistics. At each streaming stage, we introduce parameters to quantify the potential discrepancy between batch-specific effects from adjacent cohorts. We then employ penalized estimation techniques to identify nonzero discrepancy parameters, allowing us to adaptively adjust risk estimates based on current data and historical trends. We illustrate our proposed method through extensive empirical simulations and a lung cancer data analysis.},
  archive      = {J_SIM},
  author       = {Jie Ding and Jialiang Li and Xiaoguang Wang},
  doi          = {10.1002/sim.10146},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3761-3777},
  shortjournal = {Stat. Med.},
  title        = {Renewable risk assessment of heterogeneous streaming time-to-event cohorts},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to “propensity score weighting for covariate
adjustment in randomized clinical trials.” <em>SIM</em>,
<em>43</em>(19), 3759. (<a
href="https://doi.org/10.1002/sim.10080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Shuxi Zeng and Fan Li and Rui Wang},
  doi          = {10.1002/sim.10080},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3759},
  shortjournal = {Stat. Med.},
  title        = {Correction to “Propensity score weighting for covariate adjustment in randomized clinical trials”},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric empirical bayes biomarker imputation and
estimation. <em>SIM</em>, <em>43</em>(19), 3742–3758. (<a
href="https://doi.org/10.1002/sim.10150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomarkers are often measured in bulk to diagnose patients, monitor patient conditions, and research novel drug pathways. The measurement of these biomarkers often suffers from detection limits that result in missing and untrustworthy measurements. Frequently, missing biomarkers are imputed so that down-stream analysis can be conducted with modern statistical methods that cannot normally handle data subject to informative censoring. This work develops an empirical Bayes g $$ g $$ -modeling method for imputing and denoising biomarker measurements. We establish superior estimation properties compared to popular methods in simulations and with real data, providing the useful biomarker measurement estimations for down-stream analysis.},
  archive      = {J_SIM},
  author       = {Alton Barbehenn and Sihai Dave Zhao},
  doi          = {10.1002/sim.10150},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3742-3758},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric empirical bayes biomarker imputation and estimation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian mixture modelling with ranked set samples.
<em>SIM</em>, <em>43</em>(19), 3723–3741. (<a
href="https://doi.org/10.1002/sim.10144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the Bayesian estimation of the parameters of a finite mixture model from independent order statistics arising from imperfect ranked set sampling designs. As a cost-effective method, ranked set sampling enables us to incorporate easily attainable characteristics, as ranking information, into data collection and Bayesian estimation. To handle the special structure of the ranked set samples, we develop a Bayesian estimation approach exploiting the Expectation-Maximization (EM) algorithm in estimating the ranking parameters and Metropolis within Gibbs Sampling to estimate the parameters of the underlying mixture model. Our findings show that the proposed RSS-based Bayesian estimation method outperforms the commonly used Bayesian counterpart using simple random sampling. The developed method is finally applied to estimate the bone disorder status of women aged 50 and older.},
  archive      = {J_SIM},
  author       = {Amirhossein Alvandi and Sedigheh Omidvar and Armin Hatefi and Mohammad Jafari Jozani and Omer Ozturk and Nader Nematollahi},
  doi          = {10.1002/sim.10144},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3723-3741},
  shortjournal = {Stat. Med.},
  title        = {Bayesian mixture modelling with ranked set samples},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data fusion for predicting long-term program impacts.
<em>SIM</em>, <em>43</em>(19), 3702–3722. (<a
href="https://doi.org/10.1002/sim.10147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policymakers often require information on programs&#39; long-term impacts that is not available when decisions are made. For example, while rigorous evidence from the Oregon Health Insurance Experiment (OHIE) shows that having health insurance influences short-term health and financial measures, the impact on long-term outcomes, such as mortality, will not be known for many years following the program&#39;s implementation. We demonstrate how data fusion methods may be used address the problem of missing final outcomes and predict long-run impacts of interventions before the requisite data are available. We implement this method by concatenating data on an intervention (such as the OHIE) with auxiliary long-term data and then imputing missing long-term outcomes using short-term surrogate outcomes while approximating uncertainty with replication methods. We use simulations to examine the performance of the methodology and apply the method in a case study. Specifically, we fuse data on the OHIE with data from the National Longitudinal Mortality Study and estimate that being eligible to apply for subsidized health insurance will lead to a statistically significant improvement in long-term mortality.},
  archive      = {J_SIM},
  author       = {Michael W. Robbins and Sebastian Bauhoff and Lane Burgette},
  doi          = {10.1002/sim.10147},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3702-3722},
  shortjournal = {Stat. Med.},
  title        = {Data fusion for predicting long-term program impacts},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepAFT: A nonlinear accelerated failure time model with
artificial neural network. <em>SIM</em>, <em>43</em>(19), 3689–3701. (<a
href="https://doi.org/10.1002/sim.10152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox regression model or accelerated failure time regression models are often used for describing the relationship between survival outcomes and potential explanatory variables. These models assume the studied covariates are connected to the survival time or its distribution or their transformations through a function of a linear regression form. In this article, we propose nonparametric, nonlinear algorithms (deepAFT methods) based on deep artificial neural networks to model survival outcome data in the broad distribution family of accelerated failure time models. The proposed methods predict survival outcomes directly and tackle the problem of censoring via an imputation algorithm as well as re-weighting and transformation techniques based on the inverse probabilities of censoring. Through extensive simulation studies, we confirm that the proposed deepAFT methods achieve accurate predictions. They outperform the existing regression models in prediction accuracy, while being flexible and robust in modeling covariate effects of various nonlinear forms. Their prediction performance is comparable to other established deep learning methods such as deepSurv and random survival forest methods. Even though the direct output is the expected survival time, the proposed AFT methods also provide predictions for distributional functions such as the cumulative hazard and survival functions without additional learning efforts. For situations where the popular Cox regression model may not be appropriate, the deepAFT methods provide useful and effective alternatives, as shown in simulations, and demonstrated in applications to a lymphoma clinical trial study.},
  archive      = {J_SIM},
  author       = {Patrick A. Norman and Wanlu Li and Wenyu Jiang and Bingshu E. Chen},
  doi          = {10.1002/sim.10152},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3689-3701},
  shortjournal = {Stat. Med.},
  title        = {DeepAFT: A nonlinear accelerated failure time model with artificial neural network},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensitivity analysis for principal ignorability violation in
estimating complier and noncomplier average causal effects.
<em>SIM</em>, <em>43</em>(19), 3664–3688. (<a
href="https://doi.org/10.1002/sim.10153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important strategy for identifying principal causal effects (popular estimands in settings with noncompliance) is to invoke the principal ignorability (PI) assumption. As PI is untestable, it is important to gauge how sensitive effect estimates are to its violation. We focus on this task for the common one-sided noncompliance setting where there are two principal strata, compliers and noncompliers. Under PI, compliers and noncompliers share the same outcome-mean-given-covariates function under the control condition. For sensitivity analysis, we allow this function to differ between compliers and noncompliers in several ways, indexed by an odds ratio, a generalized odds ratio, a mean ratio, or a standardized mean difference sensitivity parameter. We tailor sensitivity analysis techniques (with any sensitivity parameter choice) to several types of PI-based main analysis methods, including outcome regression, influence function (IF) based and weighting methods. We discuss range selection for the sensitivity parameter. We illustrate the sensitivity analyses with several outcome types from the JOBS II study. This application estimates nuisance functions parametrically – for simplicity and accessibility. In addition, we establish rate conditions on nonparametric nuisance estimation for IF-based estimators to be asymptotically normal – with a view to inform nonparametric inference.},
  archive      = {J_SIM},
  author       = {Trang Quynh Nguyen and Elizabeth A. Stuart and Daniel O. Scharfstein and Elizabeth L. Ogburn},
  doi          = {10.1002/sim.10153},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3664-3688},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity analysis for principal ignorability violation in estimating complier and noncomplier average causal effects},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved mortality analysis in early-phase dose-ranging
clinical trials for emergency medical diseases using bayesian
time-to-event models with active comparators. <em>SIM</em>,
<em>43</em>(19), 3649–3663. (<a
href="https://doi.org/10.1002/sim.10141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency medical diseases (EMDs) are the leading cause of death worldwide. A time-to-death analysis is needed to accurately identify the risks and describe the pattern of an EMD because the mortality rate can peak early and then decline. Dose-ranging Phase II clinical trials are essential for developing new therapies for EMDs. However, most dose-finding trials do not analyze mortality as a time-to-event endpoint. We propose three Bayesian dose-response time-to-event models for a secondary mortality analysis of a clinical trial: a two-group (active treatment vs control) model, a three-parameter sigmoid EMAX model, and a hierarchical EMAX model. The study also incorporates one specific active treatment as an active comparator in constructing three new models. We evaluated the performance of these six models and a very popular independent model using simulated data motivated by a randomized Phase II clinical trial focused on identifying the most effective hyperbaric oxygen dose to achieve favorable functional outcomes in patients with severe traumatic brain injury. The results show that the three-group, EMAX, and EMAX model with an active comparator produce the smallest averaged mean squared errors and smallest mean absolute biases. We provide a new approach for time-to-event analysis in early-phase dose-ranging clinical trials for EMDs. The EMAX model with an active comparator can provide valuable insights into the mortality analysis of new EMDs or other conditions that have changing risks over time. The restricted mean survival time, a function of the model&#39;s hazards, is recommended for displaying treatment effects for EMD research.},
  archive      = {J_SIM},
  author       = {Xiaosong Shi and Jo A. Wick and Renee&#39; L. Martin and Jonathan Beall and Robert Silbergleit and Gaylan L. Rockswold and William G. Barsan and Frederick K. Korley and Sarah Rockswold and Byron J. Gajewski},
  doi          = {10.1002/sim.10141},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3649-3663},
  shortjournal = {Stat. Med.},
  title        = {Improved mortality analysis in early-phase dose-ranging clinical trials for emergency medical diseases using bayesian time-to-event models with active comparators},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sparse factor model for clustering high-dimensional
longitudinal data. <em>SIM</em>, <em>43</em>(19), 3633–3648. (<a
href="https://doi.org/10.1002/sim.10151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in engineering technologies have enabled the collection of a large number of longitudinal features. This wealth of information presents unique opportunities for researchers to investigate the complex nature of diseases and uncover underlying disease mechanisms. However, analyzing such kind of data can be difficult due to its high dimensionality, heterogeneity and computational challenges. In this article, we propose a Bayesian nonparametric mixture model for clustering high-dimensional mixed-type (eg, continuous, discrete and categorical) longitudinal features. We employ a sparse factor model on the joint distribution of random effects and the key idea is to induce clustering at the latent factor level instead of the original data to escape the curse of dimensionality. The number of clusters is estimated through a Dirichlet process prior. An efficient Gibbs sampler is developed to estimate the posterior distribution of the model parameters. Analysis of real and simulated data is presented and discussed. Our study demonstrates that the proposed model serves as a useful analytical tool for clustering high-dimensional longitudinal data.},
  archive      = {J_SIM},
  author       = {Zihang Lu and Noirrit Kiran Chandra},
  doi          = {10.1002/sim.10151},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3633-3648},
  shortjournal = {Stat. Med.},
  title        = {A sparse factor model for clustering high-dimensional longitudinal data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-arm multi-stage platform design that allows
preplanned addition of arms while still controlling the family-wise
error. <em>SIM</em>, <em>43</em>(19), 3613–3632. (<a
href="https://doi.org/10.1002/sim.10135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in platform trials that allow for adding of new treatment arms as the trial progresses as well as being able to stop treatments part way through the trial for either lack of benefit/futility or for superiority. In some situations, platform trials need to guarantee that error rates are controlled. This paper presents a multi-stage design, that allows additional arms to be added in a platform trial in a preplanned fashion, while still controlling the family-wise error rate, under the assumption of known number and timing of treatments to be added, and no time trends. A method is given to compute the sample size required to achieve a desired level of power and we show how the distribution of the sample size and the expected sample size can be found. We focus on power under the least favorable configuration which is the power of finding the treatment with a clinically relevant effect out of a set of treatments while the rest have an uninteresting treatment effect. A motivating trial is presented which focuses on two settings, with the first being a set number of stages per active treatment arm and the second being a set total number of stages, with treatments that are added later getting fewer stages. Compared to Bonferroni, the savings in the total maximum sample size are modest in a trial with three arms, &lt;1% of the total sample size. However, the savings are more substantial in trials with more arms.},
  archive      = {J_SIM},
  author       = {Peter Greenstreet and Thomas Jaki and Alun Bedding and Chris Harbron and Pavel Mozgunov},
  doi          = {10.1002/sim.10135},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3613-3632},
  shortjournal = {Stat. Med.},
  title        = {A multi-arm multi-stage platform design that allows preplanned addition of arms while still controlling the family-wise error},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assurance methods for designing a clinical trial with a
delayed treatment effect. <em>SIM</em>, <em>43</em>(19), 3595–3612. (<a
href="https://doi.org/10.1002/sim.10136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An assurance calculation is a Bayesian alternative to a power calculation. One may be performed to aid the planning of a clinical trial, specifically setting the sample size or to support decisions about whether or not to perform a study. Immuno-oncology is a rapidly evolving area in the development of anticancer drugs. A common phenomenon that arises in trials of such drugs is one of delayed treatment effects, that is, there is a delay in the separation of the survival curves. To calculate assurance for a trial in which a delayed treatment effect is likely to be present, uncertainty about key parameters needs to be considered. If uncertainty is not considered, the number of patients recruited may not be enough to ensure we have adequate statistical power to detect a clinically relevant treatment effect and the risk of an unsuccessful trial is increased. We present a new elicitation technique for when a delayed treatment effect is likely and show how to compute assurance using these elicited prior distributions. We provide an example to illustrate how this can be used in practice and develop open-source software to implement our methods. Our methodology has the potential to improve the success rate and efficiency of Phase III trials in immuno-oncology and for other treatments where a delayed treatment effect is expected to occur.},
  archive      = {J_SIM},
  author       = {James A. Salsbury and Jeremy E. Oakley and Steven A. Julious and Lisa V. Hampson},
  doi          = {10.1002/sim.10136},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3595-3612},
  shortjournal = {Stat. Med.},
  title        = {Assurance methods for designing a clinical trial with a delayed treatment effect},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized single index modeling of longitudinal data with
multiple binary responses. <em>SIM</em>, <em>43</em>(19), 3578–3594. (<a
href="https://doi.org/10.1002/sim.10139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In health and clinical research, medical indices (eg, BMI) are commonly used for monitoring and/or predicting health outcomes of interest. While single-index modeling can be used to construct such indices, methods to use single-index models for analyzing longitudinal data with multiple correlated binary responses are underdeveloped, although there are abundant applications with such data (eg, prediction of multiple medical conditions based on longitudinally observed disease risk factors). This article aims to fill the gap by proposing a generalized single-index model that can incorporate multiple single indices and mixed effects for describing observed longitudinal data of multiple binary responses. Compared to the existing methods focusing on constructing marginal models for each response, the proposed method can make use of the correlation information in the observed data about different responses when estimating different single indices for predicting response variables. Estimation of the proposed model is achieved by using a local linear kernel smoothing procedure, together with methods designed specifically for estimating single-index models and traditional methods for estimating generalized linear mixed models. Numerical studies show that the proposed method is effective in various cases considered. It is also demonstrated using a dataset from the English Longitudinal Study of Aging project.},
  archive      = {J_SIM},
  author       = {Zibo Tian and Peihua Qiu},
  doi          = {10.1002/sim.10139},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3578-3594},
  shortjournal = {Stat. Med.},
  title        = {Generalized single index modeling of longitudinal data with multiple binary responses},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing two hazard curves when there is a treatment
time-lag effect. <em>SIM</em>, <em>43</em>(19), 3563–3577. (<a
href="https://doi.org/10.1002/sim.10142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cancer and other medical studies, time-to-event (eg, death) data are common. One major task to analyze time-to-event (or survival) data is usually to compare two medical interventions (eg, a treatment and a control) regarding their effect on patients&#39; hazard to have the event in concern. In such cases, we need to compare two hazard curves of the two related patient groups. In practice, a medical treatment often has a time-lag effect, that is, the treatment effect can only be observed after a time period since the treatment is applied. In such cases, the two hazard curves would be similar in an initial time period, and the traditional testing procedures, such as the log-rank test, would be ineffective in detecting the treatment effect because the similarity between the two hazard curves in the initial time period would attenuate the difference between the two hazard curves that is reflected in the related testing statistics. In this paper, we suggest a new method for comparing two hazard curves when there is a potential treatment time-lag effect based on a weighted log-rank test with a flexible weighting scheme. The new method is shown to be more effective than some representative existing methods in various cases when a treatment time-lag effect is present.},
  archive      = {J_SIM},
  author       = {Xiaoxi Zhang and Somnath Datta and Peihua Qiu},
  doi          = {10.1002/sim.10142},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3563-3577},
  shortjournal = {Stat. Med.},
  title        = {Comparing two hazard curves when there is a treatment time-lag effect},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian transition models for ordinal longitudinal
outcomes. <em>SIM</em>, <em>43</em>(18), 3539–3561. (<a
href="https://doi.org/10.1002/sim.10133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal longitudinal outcomes are becoming common in clinical research, particularly in the context of COVID-19 clinical trials. These outcomes are information-rich and can increase the statistical efficiency of a study when analyzed in a principled manner. We present Bayesian ordinal transition models as a flexible modeling framework to analyze ordinal longitudinal outcomes. We develop the theory from first principles and provide an application using data from the Adaptive COVID-19 Treatment Trial (ACTT-1) with code examples in R. We advocate that researchers use ordinal transition models to analyze ordinal longitudinal outcomes when appropriate alongside standard methods such as time-to-event modeling.},
  archive      = {J_SIM},
  author       = {Maximilian D. Rohde and Benjamin French and Thomas G. Stewart and Frank E. Harrell},
  doi          = {10.1002/sim.10133},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3539-3561},
  shortjournal = {Stat. Med.},
  title        = {Bayesian transition models for ordinal longitudinal outcomes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-parametric inference on calibration of predicted risks.
<em>SIM</em>, <em>43</em>(18), 3524–3538. (<a
href="https://doi.org/10.1002/sim.10138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moderate calibration, the expected event probability among observations with predicted probability z being equal to z, is a desired property of risk prediction models. Current graphical and numerical techniques for evaluating moderate calibration of risk prediction models are mostly based on smoothing or grouping the data. As well, there is no widely accepted inferential method for the null hypothesis that a model is moderately calibrated. In this work, we discuss recently-developed, and propose novel, methods for the assessment of moderate calibration for binary responses. The methods are based on the limiting distributions of functions of standardized partial sums of prediction errors converging to the corresponding laws of Brownian motion. The novel method relies on well-known properties of the Brownian bridge which enables joint inference on mean and moderate calibration, leading to a unified “bridge” test for detecting miscalibration. Simulation studies indicate that the bridge test is more powerful, often substantially, than the alternative test. As a case study we consider a prediction model for short-term mortality after a heart attack, where we provide suggestions on graphical presentation and the interpretation of results. Moderate calibration can be assessed without requiring arbitrary grouping of data or using methods that require tuning of parameters.},
  archive      = {J_SIM},
  author       = {Mohsen Sadatsafavi and John Petkau},
  doi          = {10.1002/sim.10138},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3524-3538},
  shortjournal = {Stat. Med.},
  title        = {Non-parametric inference on calibration of predicted risks},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional score approaches to errors-in-variables
competing risks data in discrete time. <em>SIM</em>, <em>43</em>(18),
3503–3523. (<a href="https://doi.org/10.1002/sim.10098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of competing risks data has been an important topic in survival analysis due to the need to account for the dependence among the competing events. Also, event times are often recorded on discrete time scales, rendering the models tailored for discrete-time nature useful in the practice of survival analysis. In this work, we focus on regression analysis with discrete-time competing risks data, and consider the errors-in-variables issue where the covariates are prone to measurement errors. Viewing the true covariate value as a parameter, we develop the conditional score methods for various discrete-time competing risks models, including the cause-specific and subdistribution hazards models that have been popular in competing risks data analysis. The proposed estimators can be implemented by efficient computation algorithms, and the associated large sample theories can be simply obtained. Simulation results show satisfactory finite sample performances, and the application with the competing risks data from the scleroderma lung study reveals the utility of the proposed methods.},
  archive      = {J_SIM},
  author       = {Chi-Chung Wen and Yi-Hau Chen},
  doi          = {10.1002/sim.10098},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3503-3523},
  shortjournal = {Stat. Med.},
  title        = {Conditional score approaches to errors-in-variables competing risks data in discrete time},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). REDOMA: Bayesian random-effects dose-optimization
meta-analysis using spike-and-slab priors. <em>SIM</em>,
<em>43</em>(18), 3484–3502. (<a
href="https://doi.org/10.1002/sim.10107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of cutting-edge precision cancer treatments has led to a growing significance of the optimal biological dose (OBD) in modern oncology trials. These trials now prioritize the consideration of both toxicity and efficacy simultaneously when determining the most desirable dosage for treatment. Traditional approaches in early-phase oncology trials have conventionally relied on the assumption of a monotone relationship between treatment efficacy and dosage. However, this assumption may not hold valid for novel oncology therapies. In reality, the dose-efficacy curve of such treatments may reach a plateau at a specific dose, posing challenges for conventional methods in accurately identifying the OBD. Furthermore, achieving reliable identification of the OBD is typically not possible based on a single small-sample trial. With data from multiple phase I and phase I/II trials, we propose a novel Bayesian random-effects dose-optimization meta-analysis (REDOMA) approach to identify the OBD by synthesizing toxicity and efficacy data from each trial. The REDOMA method can address trials with heterogeneous characteristics. We adopt a curve-free approach based on a Gamma process prior to model the average dose-toxicity relationship. In addition, we utilize a Bayesian model selection framework that uses the spike-and-slab prior as an automatic variable selection technique to eliminate monotonic constraints on the dose-efficacy curve. The good performance of the REDOMA method is confirmed by extensive simulation studies.},
  archive      = {J_SIM},
  author       = {Cheng-Han Yang and Evan Kwiatkowski and J. Jack Lee and Ruitao Lin},
  doi          = {10.1002/sim.10107},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3484-3502},
  shortjournal = {Stat. Med.},
  title        = {REDOMA: Bayesian random-effects dose-optimization meta-analysis using spike-and-slab priors},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Categorical linkage-data analysis. <em>SIM</em>,
<em>43</em>(18), 3463–3483. (<a
href="https://doi.org/10.1002/sim.10134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of integrated data often requires record linkage in order to join together the data residing in separate sources. In case linkage errors cannot be avoided, due to the lack a unique identity key that can be used to link the records unequivocally, standard statistical techniques may produce misleading inference if the linked data are treated as if they were true observations. In this paper, we propose methods for categorical data analysis based on linked data that are not prepared by the analyst, such that neither the match-key variables nor the unlinked records are available. The adjustment is based on the proportion of false links in the linked file and our approach allows the probabilities of correct linkage to vary across the records without requiring that one is able to estimate this probability for each individual record. It accommodates also the general situation where unmatched records that cannot possibly be correctly linked exist in all the sources. The proposed methods are studied by simulation and applied to real data.},
  archive      = {J_SIM},
  author       = {Li-Chun Zhang and Tiziana Tuoto},
  doi          = {10.1002/sim.10134},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3463-3483},
  shortjournal = {Stat. Med.},
  title        = {Categorical linkage-data analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adding experimental treatment arms to multi-arm multi-stage
platform trials in progress. <em>SIM</em>, <em>43</em>(18), 3447–3462.
(<a href="https://doi.org/10.1002/sim.10090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-arm multi-stage (MAMS) platform trials efficiently compare several treatments with a common control arm. Crucially MAMS designs allow for adjustment for multiplicity if required. If for example, the active treatment arms in a clinical trial relate to different dose levels or different routes of administration of a drug, the strict control of the family-wise error rate (FWER) is paramount. Suppose a further treatment becomes available, it is desirable to add this to the trial already in progress; to access both the practical and statistical benefits of the MAMS design. In any setting where control of the error rate is required, we must add corresponding hypotheses without compromising the validity of the testing procedure.To strongly control the FWER, MAMS designs use pre-planned decision rules that determine the recruitment of the next stage of the trial based on the available data. The addition of a treatment arm presents an unplanned change to the design that we must account for in the testing procedure. We demonstrate the use of the conditional error approach to add hypotheses to any testing procedure that strongly controls the FWER. We use this framework to add treatments to a MAMS trial in progress. Simulations illustrate the possible characteristics of such procedures.},
  archive      = {J_SIM},
  author       = {Thomas Burnett and Franz König and Thomas Jaki},
  doi          = {10.1002/sim.10090},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3447-3462},
  shortjournal = {Stat. Med.},
  title        = {Adding experimental treatment arms to multi-arm multi-stage platform trials in progress},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian hierarchical profile regression for binary
covariates. <em>SIM</em>, <em>43</em>(18), 3432–3446. (<a
href="https://doi.org/10.1002/sim.10119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dysphagia, a common result of other medical conditions, is caused by malfunctions in swallowing physiology resulting in difficulty eating and drinking. The Modified Barium Swallow Study (MBSS), the most commonly used diagnostic tool for evaluating dysphagia, can be assessed using the Modified Barium Swallow Impairment Profile (MBSImP™). The MBSImP assessment tool consists of a hierarchical grouped data structure with multiple domains, a set of components within each domain which characterize specific swallowing physiologies, and a set of tasks scored on a discrete scale within each component. We lack sophisticated approaches to extract patterns of physiologic swallowing impairment from the MBSImP task scores within a component while still recognizing the nested structure of components within a domain. We propose a Bayesian hierarchical profile regression model, which uses a Bayesian profile regression model in conjunction with a hierarchical Dirichlet process mixture model to (1) cluster subjects into impairment profile patterns while respecting the hierarchical grouped data structure of the MBSImP, and (2) simultaneously determine associations between latent profile cluster membership for all components and the outcome of dysphagia severity. We apply our approach to a cohort of patients referred for an MBSS and assessed using the MBSImP. Our research results can be used to inform appropriate intervention strategies, and provide tools for clinicians to make better multidimensional management and treatment decisions for patients with dysphagia.},
  archive      = {J_SIM},
  author       = {Jonathan Beall and Hong Li and Bonnie Martin-Harris and Brian Neelon and Jordan Elm and Evan Graboyes and Elizabeth Hill},
  doi          = {10.1002/sim.10119},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3432-3446},
  shortjournal = {Stat. Med.},
  title        = {Bayesian hierarchical profile regression for binary covariates},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Familywise error for multiple time-to-event endpoints in a
group sequential design. <em>SIM</em>, <em>43</em>(18), 3417–3431. (<a
href="https://doi.org/10.1002/sim.10132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the familywise error rate (FWER) for time-to-event endpoints evaluated using a group sequential design with a hierarchical testing procedure for secondary endpoints. We show that, in this setup, the correlation between the log-rank test statistics at interim and at end of study is not congruent with the canonical correlation derived for normal-distributed endpoints. We show, both theoretically and by simulation, that the correlation also depends on the level of censoring, the hazard rates of the endpoints, and the hazard ratio. To optimize operating characteristics in this complex scenario, we propose a simulation-based method to assess the FWER which, better than the alpha-spending approach, can inform the choice of critical values for testing secondary endpoints.},
  archive      = {J_SIM},
  author       = {Henrik F. Thomsen and Nanna L. Lausvig and Christian B. Pipper and Søren Andersen and Lars H. Damgaard and Scott S. Emerson and Henrik Ravn},
  doi          = {10.1002/sim.10132},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3417-3431},
  shortjournal = {Stat. Med.},
  title        = {Familywise error for multiple time-to-event endpoints in a group sequential design},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based bioequivalence approach for sparse
pharmacokinetic bioequivalence studies: Model selection or model
averaging? <em>SIM</em>, <em>43</em>(18), 3403–3416. (<a
href="https://doi.org/10.1002/sim.10088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional pharmacokinetic (PK) bioequivalence (BE) studies aim to compare the rate and extent of drug absorption from a test (T) and reference (R) product using non-compartmental analysis (NCA) and the two one-sided test (TOST). Recently published regulatory guidance recommends alternative model-based (MB) approaches for BE assessment when NCA is challenging, as for long-acting injectables and products which require sparse PK sampling. However, our previous research on MB-TOST approaches showed that model misspecification can lead to inflated type I error. The objective of this research was to compare the performance of model selection (MS) on R product arm data and model averaging (MA) from a pool of candidate structural PK models in MBBE studies with sparse sampling. Our simulation study was inspired by a real case BE study using a two-way crossover design. PK data were simulated using three structural models under the null hypothesis and one model under the alternative hypothesis. MB-TOST was applied either using each of the five candidate models or following MS and MA with or without the simulated model in the pool. Assuming T and R have the same PK model, our simulation shows that following MS and MA, MB-TOST controls type I error rates at or below 0.05 and attains similar or even higher power than when using the simulated model. Thus, we propose to use MS prior to MB-TOST for BE studies with sparse PK sampling and to consider MA when candidate models have similar Akaike information criterion.},
  archive      = {J_SIM},
  author       = {Morgane Philipp and Adrien Tessier and Mark Donnelly and Lanyan Fang and Kairui Feng and Liang Zhao and Stella Grosser and Guoying Sun and Wanjie Sun and France Mentré and Julie Bertrand},
  doi          = {10.1002/sim.10088},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3403-3416},
  shortjournal = {Stat. Med.},
  title        = {Model-based bioequivalence approach for sparse pharmacokinetic bioequivalence studies: Model selection or model averaging?},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A seamless phase II/III design with dose optimization for
oncology drug development. <em>SIM</em>, <em>43</em>(18), 3383–3402. (<a
href="https://doi.org/10.1002/sim.10129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The US FDA&#39;s Project Optimus initiative that emphasizes dose optimization prior to marketing approval represents a pivotal shift in oncology drug development. It has a ripple effect for rethinking what changes may be made to conventional pivotal trial designs to incorporate a dose optimization component. Aligned with this initiative, we propose a novel seamless phase II/III design with dose optimization (SDDO framework). The proposed design starts with dose optimization in a randomized setting, leading to an interim analysis focused on optimal dose selection, trial continuation decisions, and sample size re-estimation (SSR). Based on the decision at interim analysis, patient enrollment continues for both the selected dose arm and control arm, and the significance of treatment effects will be determined at final analysis. The SDDO framework offers increased flexibility and cost-efficiency through sample size adjustment, while stringently controlling the Type I error. This proposed design also facilitates both accelerated approval (AA) and regular approval in a “one-trial” approach. Extensive simulation studies confirm that our design reliably identifies the optimal dosage and makes preferable decisions with a reduced sample size while retaining statistical power.},
  archive      = {J_SIM},
  author       = {Yuhan Li and Yiding Zhang and Gu Mi and Ji Lin},
  doi          = {10.1002/sim.10129},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3383-3402},
  shortjournal = {Stat. Med.},
  title        = {A seamless phase II/III design with dose optimization for oncology drug development},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage randomized clinical trials with a right-censored
endpoint: Comparison of frequentist and bayesian adaptive designs.
<em>SIM</em>, <em>43</em>(18), 3364–3382. (<a
href="https://doi.org/10.1002/sim.10130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive randomized clinical trials are of major interest when dealing with a time-to-event outcome in a prolonged observation window. No consensus exists either to define stopping boundaries or to combine p $$ p $$ values or test statistics in the terminal analysis in the case of a frequentist design and sample size adaptation. In a one-sided setting, we compared three frequentist approaches using stopping boundaries relying on α $$ \alpha $$ -spending functions and a Bayesian monitoring setting with boundaries based on the posterior distribution of the log-hazard ratio. All designs comprised a single interim analysis with an efficacy stopping rule and the possibility of sample size adaptation at this interim step. Three frequentist approaches were defined based on the terminal analysis: combination of stagewise statistics (Wassmer) or of values (Desseaux), or on patientwise splitting (Jörgens), and we compared the results with those of the Bayesian monitoring approach (Freedman). These different approaches were evaluated in a simulation study and then illustrated on a real dataset from a randomized clinical trial conducted in elderly patients with chronic lymphocytic leukemia. All approaches controlled for the type I error rate, except for the Bayesian monitoring approach, and yielded satisfactory power. It appears that the frequentist approaches are the best in underpowered trials. The power of all the approaches was affected by the violation of the proportional hazards (PH) assumption. For adaptive designs with a survival endpoint and a one-sided alternative hypothesis, the Wassmer and Jörgens approaches after sample size adaptation should be preferred, unless violation of PH is suspected.},
  archive      = {J_SIM},
  author       = {Luana Boumendil and Sylvie Chevret and Vincent Lévy and Lucie Biard},
  doi          = {10.1002/sim.10130},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3364-3382},
  shortjournal = {Stat. Med.},
  title        = {Two-stage randomized clinical trials with a right-censored endpoint: Comparison of frequentist and bayesian adaptive designs},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian method to detect drug-drug interaction using
external information for spontaneous reporting system. <em>SIM</em>,
<em>43</em>(18), 3353–3363. (<a
href="https://doi.org/10.1002/sim.10137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the insufficiency of safety assessments of clinical trials for drugs, further assessments are required for post-marketed drugs. In addition to adverse drug reactions (ADRs) induced by one drug, drug-drug interaction (DDI)-induced ADR should also be investigated. The spontaneous reporting system (SRS) is a powerful tool for evaluating the safety of drugs continually. In this study, we propose a novel Bayesian method for detecting potential DDIs in a database collected by the SRS. By applying a power prior, the proposed method can borrow information from similar drugs for a drug assessed DDI to increase sensitivity of detection. The proposed method can also adjust the amount of the information borrowed by tuning the parameters in power prior. In the simulation study, we demonstrate the aforementioned increase in sensitivity. Depending on the scenarios, approximately 20 points of sensitivity of the proposed method increase from an existing method to a maximum. We also indicate the possibility of early detection of potential DDIs by the proposed method through analysis of the database shared by the Food and Drug Administration. In conclusion, the proposed method has a higher sensitivity and a novel criterion to detect potential DDIs early, provided similar drugs have similar observed-expected ratios to the drug under assessment.},
  archive      = {J_SIM},
  author       = {Keisuke Tada and Kazushi Maruo and Masahiko Gosho},
  doi          = {10.1002/sim.10137},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3353-3363},
  shortjournal = {Stat. Med.},
  title        = {A bayesian method to detect drug-drug interaction using external information for spontaneous reporting system},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust analysis of stepped wedge trials using composite
likelihood models. <em>SIM</em>, <em>43</em>(17), 3326–3352. (<a
href="https://doi.org/10.1002/sim.10120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge trials (SWTs) are a type of cluster randomized trial that involve repeated measures on clusters and design-induced confounding between time and treatment. Although mixed models are commonly used to analyze SWTs, they are susceptible to misspecification particularly for cluster-longitudinal designs such as SWTs. Mixed model estimation leverages both “horizontal” or within-cluster information and “vertical” or between-cluster information. To use horizontal information in a mixed model, both the mean model and correlation structure must be correctly specified or accounted for, since time is confounded with treatment and measurements are likely correlated within clusters. Alternative non-parametric methods have been proposed that use only vertical information; these are more robust because between-cluster comparisons in a SWT preserve randomization, but these non-parametric methods are not very efficient. We propose a composite likelihood method that focuses on vertical information, but has the flexibility to recover efficiency by using additional horizontal information. We compare the properties and performance of various methods, using simulations based on COVID-19 data and a demonstration of application to the LIRE trial. We found that a vertical composite likelihood model that leverages baseline data is more robust than traditional methods, and more efficient than methods that use only vertical information. We hope that these results demonstrate the potential value of model-based vertical methods for SWTs with a large number of clusters, and that these new tools are useful to researchers who are concerned about misspecification of traditional models.},
  archive      = {J_SIM},
  author       = {Emily C. Voldal and Avi Kenny and Fan Xia and Patrick Heagerty and James P. Hughes},
  doi          = {10.1002/sim.10120},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3326-3352},
  shortjournal = {Stat. Med.},
  title        = {Robust analysis of stepped wedge trials using composite likelihood models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding an impact of patient enrollment pattern on
predictability of central (unstratified) randomization in a multi-center
clinical trial. <em>SIM</em>, <em>43</em>(17), 3313–3325. (<a
href="https://doi.org/10.1002/sim.10117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a multi-center randomized controlled trial (RCT) with competitive recruitment, eligible patients are enrolled sequentially by different study centers and are randomized to treatment groups using the chosen randomization method. Given the stochastic nature of the recruitment process, some centers may enroll more patients than others, and in some instances, a center may enroll multiple patients in a row, for example, on a given day. If the study is open-label, the investigators might be able to make intelligent guesses on upcoming treatment assignments in the randomization sequence, even if the trial is centrally randomized and not stratified by center. In this paper, we use enrollment data inspired by a real multi-center RCT to quantify the susceptibility of two restricted randomization procedures, the permuted block design and the big stick design, to selection bias under the convergence strategy of Blackwell and Hodges (1957) applied at the center level. We provide simulation evidence that the expected proportion of correct guesses may be greater than 50% (i.e., an increased risk of selection bias) and depends on the chosen randomization method and the number of study patients recruited by a given center that takes consecutive positions on the central allocation schedule. We propose some strategies for ensuring stronger encryption of the randomization sequence to mitigate the risk of selection bias.},
  archive      = {J_SIM},
  author       = {Johannes Krisam and Yevgen Ryeznik and Kerstine Carter and Olga Kuznetsova and Oleksandr Sverdlov},
  doi          = {10.1002/sim.10117},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3313-3325},
  shortjournal = {Stat. Med.},
  title        = {Understanding an impact of patient enrollment pattern on predictability of central (unstratified) randomization in a multi-center clinical trial},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting responsible nodes in differential bayesian
networks. <em>SIM</em>, <em>43</em>(17), 3294–3312. (<a
href="https://doi.org/10.1002/sim.10125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To study the roles that different nodes play in differentiating Bayesian networks under two states, such as control versus disease, we formulate two node-specific scores to facilitate such assessment. The first score is motivated by the prediction invariance property of a causal model. The second score results from modifying an existing score constructed for differential analysis of undirected networks. We develop strategies based on these scores to identify nodes responsible for topological differences between two Bayesian networks. Synthetic data and real-life data from designed experiments are used to demonstrate the efficacy of the proposed methods in detecting responsible nodes.},
  archive      = {J_SIM},
  author       = {Xianzheng Huang and Hongmei Zhang},
  doi          = {10.1002/sim.10125},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3294-3312},
  shortjournal = {Stat. Med.},
  title        = {Detecting responsible nodes in differential bayesian networks},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging the gap between two-stage and joint models: The
case of tumor growth inhibition and overall survival models.
<em>SIM</em>, <em>43</em>(17), 3280–3293. (<a
href="https://doi.org/10.1002/sim.10128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clinical trials generate both longitudinal biomarker and time-to-event data. We might be interested in their relationship, as in the case of tumor size and overall survival in oncology drug development. Many well-established methods exist for analyzing such data either sequentially (two-stage models) or simultaneously (joint models). Two-stage modeling (2stgM) has been challenged (i) for not acknowledging that biomarkers are endogenous covariable to the survival submodel and (ii) for not propagating the uncertainty of the longitudinal biomarker submodel to the survival submodel. On the other hand, joint modeling (JM), which properly circumvents both problems, has been criticized for being time-consuming, and difficult to use in practice. In this paper, we explore a third approach, referred to as a novel two-stage modeling (N2stgM). This strategy reduces the model complexity without compromising the parameter estimate accuracy. The three approaches (2stgM, JM, and N2stgM) are formulated, and a Bayesian framework is considered for their implementation. Both real and simulated data were used to analyze the performance of such approaches. In all scenarios, our proposal estimated the parameters approximately as JM but without being computationally expensive, while 2stgM produced biased results.},
  archive      = {J_SIM},
  author       = {Danilo Alvares and François Mercier},
  doi          = {10.1002/sim.10128},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3280-3293},
  shortjournal = {Stat. Med.},
  title        = {Bridging the gap between two-stage and joint models: The case of tumor growth inhibition and overall survival models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). The effect of number of clusters and magnitude of
within-cluster homogeneity in outcomes on the performance of four
variance estimators for a marginal multivariable cox regression model
fit to clustered data in the context of observational research.
<em>SIM</em>, <em>43</em>(17), 3264–3279. (<a
href="https://doi.org/10.1002/sim.10126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers often estimate the association between the hazard of a time-to-event outcome and the characteristics of individuals and the clusters in which individuals are nested. Lin and Wei&#39;s robust variance estimator is often used with a Cox regression model fit to clustered data. Recently, alternative variance estimators have been proposed: the Fay–Graubard estimator, the Kauermann–Carroll estimator, and the Mancl–DeRouen estimator. Using Monte Carlo simulations, we found that, when fitting a marginal Cox regression model with both individual-level and cluster-level covariates: (i) in the presence of weak to moderate within-cluster homogeneity of outcomes, the Lin–Wei variance estimator can result in estimates of the SE with moderate bias when the number of clusters is fewer than 20–30, while in the presence of strong within-cluster homogeneity, it can result in biased estimation even when the number of clusters is as large as 100; (ii) when the number of clusters was less than approximately 20, the Fay–Graubard variance estimator tended to result in estimates of SE with the lowest bias; (iii) when the number of clusters exceeded approximately 20, the Mancl–DeRouen estimator tended to result in estimated standard errors with the lowest bias; (iv) the Mancl–DeRouen estimator used with a t -distribution tended to result in 95% confidence that had the best performance of the estimators; (v) when the magnitude of within-cluster homogeneity in outcomes was strong or very strong, all methods resulted in confidence intervals with lower than advertised coverage rates even when the number of clusters was very large.},
  archive      = {J_SIM},
  author       = {Peter C. Austin},
  doi          = {10.1002/sim.10126},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3264-3279},
  shortjournal = {Stat. Med.},
  title        = {The effect of number of clusters and magnitude of within-cluster homogeneity in outcomes on the performance of four variance estimators for a marginal multivariable cox regression model fit to clustered data in the context of observational research},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling intra-individual inter-trial EEG response
variability in autism. <em>SIM</em>, <em>43</em>(17), 3239–3263. (<a
href="https://doi.org/10.1002/sim.10131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (autism) is a prevalent neurodevelopmental condition characterized by early emerging impairments in social behavior and communication. EEG represents a powerful and non-invasive tool for examining functional brain differences in autism. Recent EEG evidence suggests that greater intra-individual trial-to-trial variability across EEG responses in stimulus-related tasks may characterize brain differences in autism. Traditional analysis of EEG data largely focuses on mean trends of the trial-averaged data, where trial-level analysis is rarely performed due to low neural signal to noise ratio. We propose to use nonlinear (shape-invariant) mixed effects (NLME) models to study intra-individual inter-trial EEG response variability using trial-level EEG data. By providing more precise metrics of response variability, this approach could enrich our understanding of neural disparities in autism and potentially aid the identification of objective markers. The proposed multilevel NLME models quantify variability in the signal&#39;s interpretable and widely recognized features (e.g., latency and amplitude) while also regularizing estimation based on noisy trial-level data. Even though NLME models have been studied for more than three decades, existing methods cannot scale up to large data sets. We propose computationally feasible estimation and inference methods via the use of a novel minorization-maximization (MM) algorithm. Extensive simulations are conducted to show the efficacy of the proposed procedures. Applications to data from a large national consortium find that children with autism have larger intra-individual inter-trial variability in P1 latency in a visual evoked potential (VEP) task, compared to their neurotypical peers.},
  archive      = {J_SIM},
  author       = {Mingfei Dong and Donatello Telesca and Michele Guindani and Catherine Sugar and Sara J. Webb and Shafali Jeste and Abigail Dickinson and April R. Levin and Frederick Shic and Adam Naples and Susan Faja and Geraldine Dawson and James C. McPartland and Damla Şentürk},
  doi          = {10.1002/sim.10131},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3239-3263},
  shortjournal = {Stat. Med.},
  title        = {Modeling intra-individual inter-trial EEG response variability in autism},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical methods for predicting e-cigarette use events
based on beat-to-beat interval (BBI) data collected from wearable
devices. <em>SIM</em>, <em>43</em>(17), 3227–3238. (<a
href="https://doi.org/10.1002/sim.10124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of e-cigarette use among young adults in the USA is high (14%). Although the majority of users plan to quit vaping, the motivation to make a quit attempt is low and available support during a quit attempt is limited. Using wearable sensors to collect physiological data (eg, heart rate) holds promise for capturing the right timing to deliver intervention messages. This study aims to fill the current knowledge gap by proposing statistical methods to (1) de-noise beat-to-beat interval (BBI) data from smartwatches worn by 12 young adult regular e-cigarette users for 7 days; and (2) summarize the de-noised data by event and control segments. We also conducted a comprehensive review of conventional methods for summarizing heart rate variability (HRV) and compared their performance with the proposed method. The results show that the proposed singular spectrum analysis (SSA) can effectively de-noise the highly variable BBI data, as well as quantify the proportion of total variation extracted. Compared to existing HRV methods, the proposed second order polynomial model yields the highest area under the curve (AUC) value of 0.76 and offers better interpretability. The findings also indicate that the average heart rate before vaping is higher and there is an increasing trend in the heart rate before the vaping event. Importantly, the development of increasing heart rate observed in this study implies that there may be time to intervene as this physiological signal emerges. This finding, if replicated in a larger scale study, may inform optimal timings for delivering messages in future intervention.},
  archive      = {J_SIM},
  author       = {James J. Yang and Megan E. Piper and Premananda Indic and Anne Buu},
  doi          = {10.1002/sim.10124},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3227-3238},
  shortjournal = {Stat. Med.},
  title        = {Statistical methods for predicting e-cigarette use events based on beat-to-beat interval (BBI) data collected from wearable devices},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fractional accumulative calibration-free odds (f-aCFO)
design for delayed toxicity in phase i clinical trials. <em>SIM</em>,
<em>43</em>(17), 3210–3226. (<a
href="https://doi.org/10.1002/sim.10127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The calibration-free odds (CFO) design has been demonstrated to be robust, model-free, and practically useful but faces challenges when dealing with late-onset toxicity. The emergence of the time-to-event (TITE) method and fractional method leads to the development of TITE-CFO and fractional CFO (fCFO) designs to accumulate delayed toxicity. Nevertheless, existing CFO-type designs have untapped potential because they primarily consider dose information from the current position and its two neighboring positions. To incorporate information from all doses, we propose the accumulative CFO (aCFO) design by utilizing data at all dose levels similar to a tug-of-war game where players distant from the center also contribute their strength. This approach enhances full information utilization while still preserving the model-free and calibration-free characteristics. Extensive simulation studies demonstrate performance improvement over the original CFO design, emphasizing the advantages of incorporating information from a broader range of dose levels. Furthermore, we propose to incorporate late-onset outcomes into the TITE-aCFO and f-aCFO designs, with f-aCFO displaying superior performance over existing methods in both fixed and random simulation scenarios. In conclusion, the aCFO and f-aCFO designs can be considered robust, efficient, and user-friendly approaches for conducting phase I trials without or with late-onsite toxicity.},
  archive      = {J_SIM},
  author       = {Jialu Fang and Guosheng Yin},
  doi          = {10.1002/sim.10127},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3210-3226},
  shortjournal = {Stat. Med.},
  title        = {Fractional accumulative calibration-free odds (f-aCFO) design for delayed toxicity in phase i clinical trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing heterogeneity in surrogacy using censored data.
<em>SIM</em>, <em>43</em>(17), 3184–3209. (<a
href="https://doi.org/10.1002/sim.10122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining whether a surrogate marker can be used to replace a primary outcome in a clinical study is complex. While many statistical methods have been developed to formally evaluate a surrogate marker, they generally do not provide a way to examine heterogeneity in the utility of a surrogate marker. Similar to treatment effect heterogeneity, where the effect of a treatment varies based on a patient characteristic, heterogeneity in surrogacy means that the strength or utility of the surrogate marker varies based on a patient characteristic. The few methods that have been recently developed to examine such heterogeneity cannot accommodate censored data. Studies with a censored outcome are typically the studies that could most benefit from a surrogate because the follow-up time is often long. In this paper, we develop a robust nonparametric approach to assess heterogeneity in the utility of a surrogate marker with respect to a baseline variable in a censored time-to-event outcome setting. In addition, we propose and evaluate a testing procedure to formally test for heterogeneity at a single time point or across multiple time points simultaneously. Finite sample performance of our estimation and testing procedure are examined in a simulation study. We use our proposed method to investigate the complex relationship between change in fasting plasma glucose, diabetes, and sex hormones using data from the diabetes prevention program study.},
  archive      = {J_SIM},
  author       = {Layla Parast and Lu Tian and Tianxi Cai},
  doi          = {10.1002/sim.10122},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3184-3209},
  shortjournal = {Stat. Med.},
  title        = {Assessing heterogeneity in surrogacy using censored data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structured learning in time-dependent cox models.
<em>SIM</em>, <em>43</em>(17), 3164–3183. (<a
href="https://doi.org/10.1002/sim.10116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cox models with time-dependent coefficients and covariates are widely used in survival analysis. In high-dimensional settings, sparse regularization techniques are employed for variable selection, but existing methods for time-dependent Cox models lack flexibility in enforcing specific sparsity patterns (ie, covariate structures). We propose a flexible framework for variable selection in time-dependent Cox models, accommodating complex selection rules. Our method can adapt to arbitrary grouping structures, including interaction selection, temporal, spatial, tree, and directed acyclic graph structures. It achieves accurate estimation with low false alarm rates. We develop the sox package, implementing a network flow algorithm for efficiently solving models with complex covariate structures. sox offers a user-friendly interface for specifying grouping structures and delivers fast computation. Through examples, including a case study on identifying predictors of time to all-cause death in atrial fibrillation patients, we demonstrate the practical application of our method with specific selection rules.},
  archive      = {J_SIM},
  author       = {Guanbo Wang and Yi Lian and Archer Y. Yang and Robert W. Platt and Rui Wang and Sylvie Perreault and Marc Dorais and Mireille E. Schnitzer},
  doi          = {10.1002/sim.10116},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3164-3183},
  shortjournal = {Stat. Med.},
  title        = {Structured learning in time-dependent cox models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balancing versus modelling in weighted analysis of
non-randomised studies with survival outcomes: A simulation study.
<em>SIM</em>, <em>43</em>(17), 3140–3163. (<a
href="https://doi.org/10.1002/sim.10110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighting methods are widely used for causal effect estimation in non-randomised studies. In general, these methods use the propensity score (PS), the probability of receiving the treatment given the covariates, to arrive at the respective weights. All of these “modelling” methods actually optimize prediction of the respective outcome, which is, in the PS model, treatment assignment. However, this does not match with the actual aim of weighting, which is eliminating the association between covariates and treatment assignment. In the “balancing” approach, covariates are thus balanced directly by solving systems of numerical equations, explicitly without fitting a PS model. To compare modelling, balancing and hybrid approaches to weighting we performed a large simulation study for a binary treatment and a survival outcome. For maximal practical relevance all simulation parameters were selected after a systematic review of medical studies that used PS methods for analysis. We also introduce a new hybrid method that uses the idea of the covariate balancing propensity score and matching weights, thus avoiding extreme weights. In addition, we present a corrected robust variance estimator for some of the methods. Overall, our simulations results indicate that balancing approach methods work worse than expected. However, among the considered balancing methods, entropy balancing consistently outperforms the variance balancing approach. All methods estimating the average treatment effect in the overlap population perform well with very little bias and small standard errors even in settings with misspecified propensity score models. Finally, the coverage using the standard robust variance estimator was too high for all methods, with the proposed corrected robust variance estimator improving coverage in a variety of settings.},
  archive      = {J_SIM},
  author       = {Tim Filla and Holger Schwender and Oliver Kuss},
  doi          = {10.1002/sim.10110},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3140-3163},
  shortjournal = {Stat. Med.},
  title        = {Balancing versus modelling in weighted analysis of non-randomised studies with survival outcomes: A simulation study},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An enhanced cross-sectional HIV incidence estimator that
incorporates prior HIV test results. <em>SIM</em>, <em>43</em>(17),
3125–3139. (<a href="https://doi.org/10.1002/sim.10112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incidence estimation of HIV infection can be performed using recent infection testing algorithm (RITA) results from a cross-sectional sample. This allows practitioners to understand population trends in the HIV epidemic without having to perform longitudinal follow-up on a cohort of individuals. The utility of the approach is limited by its precision, driven by the (low) sensitivity of the RITA at identifying recent infection. By utilizing results of previous HIV tests that individuals may have taken, we consider an enhanced RITA with increased sensitivity (and specificity). We use it to propose an enhanced estimator for incidence estimation. We prove the theoretical properties of the enhanced estimator and illustrate its numerical performance in simulation studies. We apply the estimator to data from a cluster-randomized trial to study the effect of community-level HIV interventions on HIV incidence. We demonstrate that the enhanced estimator provides a more precise estimate of HIV incidence compared to the standard estimator.},
  archive      = {J_SIM},
  author       = {Marlena Bannick and Deborah Donnell and Richard Hayes and Oliver Laeyendecker and Fei Gao},
  doi          = {10.1002/sim.10112},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3125-3139},
  shortjournal = {Stat. Med.},
  title        = {An enhanced cross-sectional HIV incidence estimator that incorporates prior HIV test results},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of randomized clinical trials with a binary endpoint:
Conditional versus unconditional analyses of a two-by-two table.
<em>SIM</em>, <em>43</em>(16), 3109–3123. (<a
href="https://doi.org/10.1002/sim.10115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When designing a randomized clinical trial to compare two treatments, the sample size required to have desired power with a specified type 1 error depends on the hypothesis testing procedure. With a binary endpoint (e.g., response), the trial results can be displayed in a 2 × 2 table. If one does the analysis conditional on the number of positive responses, then using Fisher&#39;s exact test has an actual type 1 error less than or equal to the specified nominal type 1 error. Alternatively, one can use one of many unconditional “exact” tests that also preserve the type 1 error and are less conservative than Fisher&#39;s exact test. In particular, the unconditional test of Boschloo is always at least as powerful as Fisher&#39;s exact test, leading to smaller required sample sizes for clinical trials. However, many statisticians have argued over the years that the conditional analysis with Fisher&#39;s exact test is the only appropriate procedure. Since having smaller clinical trials is an extremely important consideration, we review the general arguments given for the conditional analysis of a 2 × 2 table in the context of a randomized clinical trial. We find the arguments not relevant in this context, or, if relevant, not completely convincing, suggesting the sample-size advantage of the unconditional tests should lead to their recommended use. We also briefly suggest that since designers of clinical trials practically always have target null and alternative response rates, there is the possibility of using this information to improve the power of the unconditional tests.},
  archive      = {J_SIM},
  author       = {Edward L. Korn and Boris Freidlin},
  doi          = {10.1002/sim.10115},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3109-3123},
  shortjournal = {Stat. Med.},
  title        = {Design of randomized clinical trials with a binary endpoint: Conditional versus unconditional analyses of a two-by-two table},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Standardization and other approaches to meta-analyze
differences in means. <em>SIM</em>, <em>43</em>(16), 3092–3108. (<a
href="https://doi.org/10.1002/sim.10114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysts often use standardized mean differences (SMD) to combine mean effects from studies in which the dependent variable has been measured with different instruments or scales. In this tutorial we show how the SMD is properly calculated as the difference in means divided by a between-subject reference-group, control-group, or pooled pre-intervention SD, usually free of measurement error. When combining mean effects from controlled trials and crossovers, most meta-analysts have divided by either the pooled SD of change scores, the pooled SD of post-intervention scores, or the pooled SD of pre- and post-intervention scores, resulting in SMDs that are biased and difficult to interpret. The frequent use of such inappropriate standardizing SDs by meta-analysts in three medical journals we surveyed is due to misleading advice in peer-reviewed publications and meta-analysis packages. Even with an appropriate standardizing SD, meta-analysis of SMDs increases heterogeneity artifactually via differences in the standardizing SD between settings. Furthermore, the usual magnitude thresholds for standardized mean effects are not thresholds for clinically important differences. We therefore explain how to use other approaches to combining mean effects of disparate measures: log transformation of factor effects (response ratios) and of percent effects converted to factors; rescaling of psychometrics to percent of maximum range; and rescaling with minimum clinically important differences. In the absence of clinically important differences, we explain how standardization after meta-analysis with appropriately transformed or rescaled pre-intervention SDs can be used to assess magnitudes of a meta-analyzed mean effect in different settings.},
  archive      = {J_SIM},
  author       = {Will G. Hopkins and David S. Rowlands},
  doi          = {10.1002/sim.10114},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3092-3108},
  shortjournal = {Stat. Med.},
  title        = {Standardization and other approaches to meta-analyze differences in means},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible cost-penalized bayesian model selection: Developing
inclusion paths with an application to diagnosis of heart disease.
<em>SIM</em>, <em>43</em>(16), 3073–3091. (<a
href="https://doi.org/10.1002/sim.10113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian model selection approach that allows medical practitioners to select among predictor variables while taking their respective costs into account. Medical procedures almost always incur costs in time and/or money. These costs might exceed their usefulness for modeling the outcome of interest. We develop Bayesian model selection that uses flexible model priors to penalize costly predictors a priori and select a subset of predictors useful relative to their costs. Our approach (i) gives the practitioner control over the magnitude of cost penalization, (ii) enables the prior to scale well with sample size, and (iii) enables the creation of our proposed inclusion path visualization, which can be used to make decisions about individual candidate predictors using both probabilistic and visual tools. We demonstrate the effectiveness of our inclusion path approach and the importance of being able to adjust the magnitude of the prior&#39;s cost penalization through a dataset pertaining to heart disease diagnosis in patients at the Cleveland Clinic Foundation, where several candidate predictors with various costs were recorded for patients, and through simulated data.},
  archive      = {J_SIM},
  author       = {Erica M. Porter and Christopher T. Franck and Stephen Adams},
  doi          = {10.1002/sim.10113},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3073-3091},
  shortjournal = {Stat. Med.},
  title        = {Flexible cost-penalized bayesian model selection: Developing inclusion paths with an application to diagnosis of heart disease},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample size determination for prediction models via
learning-type curves. <em>SIM</em>, <em>43</em>(16), 3062–3072. (<a
href="https://doi.org/10.1002/sim.10121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with sample size determination methodology for prediction models. We propose to combine the individual calculations via learning-type curves. We suggest two distinct ways of doing so, a deterministic skeleton of a learning curve and a Gaussian process centered upon its deterministic counterpart. We employ several learning algorithms for modeling the primary endpoint and distinct measures for trial efficacy. We find that the performance may vary with the sample size, but borrowing information across sample size universally improves the performance of such calculations. The Gaussian process-based learning curve appears more robust and statistically efficient, while computational efficiency is comparable. We suggest that anchoring against historical evidence when extrapolating sample sizes should be adopted when such data are available. The methods are illustrated on binary and survival endpoints.},
  archive      = {J_SIM},
  author       = {Alimu Dayimu and Nikola Simidjievski and Nikolaos Demiris and Jean Abraham},
  doi          = {10.1002/sim.10121},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3062-3072},
  shortjournal = {Stat. Med.},
  title        = {Sample size determination for prediction models via learning-type curves},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient matrix profile computation with euclidean distance
using eigen transformation: Performance evaluation based on beat-to-beat
interval (BBI) data. <em>SIM</em>, <em>43</em>(16), 3051–3061. (<a
href="https://doi.org/10.1002/sim.10123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The matrix profile serves as a fundamental tool to provide insights into similar patterns within time series. Existing matrix profile algorithms have been primarily developed for the normalized Euclidean distance, which may not be a proper distance measure in many settings. The methodology work of this paper was motivated by statistical analysis of beat-to-beat interval (BBI) data collected from smartwatches to monitor e-cigarette users&#39; heart rate change patterns for which the original Euclidean distance ( L 2 $$ {L}_2 $$ -norm) would be a more suitable choice. Yet, incorporating the Euclidean distance into existing matrix profile algorithms turned out to be computationally challenging, especially when the time series is long with extended query sequences. We propose a novel methodology to efficiently compute matrix profile for long time series data based on the Euclidean distance. This methodology involves four key steps including (1) projection of the time series onto eigenspace; (2) enhancing singular value decomposition (SVD) computation; (3) early abandon strategy; and (4) determining lower bounds based on the first left singular vector. Simulation studies based on BBI data from the motivating example have demonstrated remarkable reductions in computational time, ranging from one-fourth to one-twentieth of the time required by the conventional method. Unlike the conventional method of which the performance deteriorates sharply as the time series length or the query sequence length increases, the proposed method consistently performs well across a wide range of the time series length or the query sequence length.},
  archive      = {J_SIM},
  author       = {James J. Yang and Anne Buu},
  doi          = {10.1002/sim.10123},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3051-3061},
  shortjournal = {Stat. Med.},
  title        = {Efficient matrix profile computation with euclidean distance using eigen transformation: Performance evaluation based on beat-to-beat interval (BBI) data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prognostic score-based methods for estimating center effects
based on survival probability: Application to post-kidney transplant
survival. <em>SIM</em>, <em>43</em>(16), 3036–3050. (<a
href="https://doi.org/10.1002/sim.10092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In evaluating the performance of different facilities or centers on survival outcomes, the standardized mortality ratio (SMR), which compares the observed to expected mortality has been widely used, particularly in the evaluation of kidney transplant centers. Despite its utility, the SMR may exaggerate center effects in settings where survival probability is relatively high. An example is one-year graft survival among U.S. kidney transplant recipients. We propose a novel approach to estimate center effects in terms of differences in survival probability (ie, each center versus a reference population). An essential component of the method is a prognostic score weighting technique, which permits accurately evaluating centers without necessarily specifying a correct survival model. Advantages of our approach over existing facility-profiling methods include a metric based on survival probability (greater clinical relevance than ratios of counts/rates); direct standardization (valid to compare between centers, unlike indirect standardization based methods, such as the SMR); and less reliance on correct model specification (since the assumed model is used to generate risk classes as opposed to fitted-value based ‘expected’ counts). We establish the asymptotic properties of the proposed weighted estimator and evaluate its finite-sample performance under a diverse set of simulation settings. The method is then applied to evaluate U.S. kidney transplant centers with respect to graft survival probability.},
  archive      = {J_SIM},
  author       = {Youjin Lee and Peter P. Reese and Amelia H. Tran and Douglas E. Schaubel},
  doi          = {10.1002/sim.10092},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3036-3050},
  shortjournal = {Stat. Med.},
  title        = {Prognostic score-based methods for estimating center effects based on survival probability: Application to post-kidney transplant survival},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust inference for causal mediation analysis of recurrent
event data. <em>SIM</em>, <em>43</em>(16), 3020–3035. (<a
href="https://doi.org/10.1002/sim.10118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events, including cardiovascular events, are commonly observed in biomedical studies. Understanding the effects of various treatments on recurrent events and investigating the underlying mediation mechanisms by which treatments may reduce the frequency of recurrent events are crucial tasks for researchers. Although causal inference methods for recurrent event data have been proposed, they cannot be used to assess mediation. This study proposed a novel methodology of causal mediation analysis that accommodates recurrent outcomes of interest in a given individual. A formal definition of causal estimands (direct and indirect effects) within a counterfactual framework is given, and empirical expressions for these effects are identified. To estimate these effects, a semiparametric estimator with triple robustness against model misspecification was developed. The proposed methodology was demonstrated in a real-world application. The method was applied to measure the effects of two diabetes drugs on the recurrence of cardiovascular disease and to examine the mediating role of kidney function in this process.},
  archive      = {J_SIM},
  author       = {Yan-Lin Chen and Yan-Hong Chen and Pei-Fang Su and Huang-Tz Ou and An-Shun Tai},
  doi          = {10.1002/sim.10118},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3020-3035},
  shortjournal = {Stat. Med.},
  title        = {Robust inference for causal mediation analysis of recurrent event data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HIV estimation using population-based surveys with
non-response: A partial identification approach. <em>SIM</em>,
<em>43</em>(16), 3005–3019. (<a
href="https://doi.org/10.1002/sim.10108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HIV estimation using data from the demographic and health surveys (DHS) is limited by the presence of non-response and test refusals. Conventional adjustments such as imputation require the data to be missing at random. Methods that use instrumental variables allow the possibility that prevalence is different between the respondents and non-respondents, but their performance depends critically on the validity of the instrument. Using Manski&#39;s partial identification approach, we form instrumental variable bounds for HIV prevalence from a pool of candidate instruments. Our method does not require all candidate instruments to be valid. We use a simulation study to evaluate and compare our method against its competitors. We illustrate the proposed method using DHS data from Zambia, Malawi and Kenya. Our simulations show that imputation leads to seriously biased results even under mild violations of non-random missingness. Using worst case identification bounds that do not make assumptions about the non-response mechanism is robust but not informative. By taking the union of instrumental variable bounds balances informativeness of the bounds and robustness to inclusion of some invalid instruments. Non-response and refusals are ubiquitous in population based HIV data such as those collected under the DHS. Partial identification bounds provide a robust solution to HIV prevalence estimation without strong assumptions. Union bounds are significantly more informative than the worst case bounds without sacrificing credibility.},
  archive      = {J_SIM},
  author       = {Oyelola A. Adegboye and Tomoki Fujii and Denis Heng-Yan Leung and Li Siyu},
  doi          = {10.1002/sim.10108},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3005-3019},
  shortjournal = {Stat. Med.},
  title        = {HIV estimation using population-based surveys with non-response: A partial identification approach},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel non-linear models for clinical trial analysis with
longitudinal data: A tutorial using SAS for both frequentist and
bayesian methods. <em>SIM</em>, <em>43</em>(15), 2987–3004. (<a
href="https://doi.org/10.1002/sim.10089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal data from clinical trials are commonly analyzed using mixed models for repeated measures (MMRM) when the time variable is categorical or linear mixed-effects models (ie, random effects model) when the time variable is continuous. In these models, statistical inference is typically based on the absolute difference in the adjusted mean change (for categorical time) or the rate of change (for continuous time). Previously, we proposed a novel approach: modeling the percentage reduction in disease progression associated with the treatment relative to the placebo decline using proportional models. This concept of proportionality provides an innovative and flexible method for simultaneously modeling different cohorts, multivariate endpoints, and jointly modeling continuous and survival endpoints. Through simulated data, we demonstrate the implementation of these models using SAS procedures in both frequentist and Bayesian approaches. Additionally, we introduce a novel method for implementing MMRM models (ie, analysis of response profile) using the nlmixed procedure.},
  archive      = {J_SIM},
  author       = {Guoqiao Wang and Whedy Wang and Brian Mangal and Yijie Liao and Lon Schneider and Yan Li and Chengjie Xiong and Eric McDade and Richard Kennedy and Randall Bateman and Gary Cutter},
  doi          = {10.1002/sim.10089},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2987-3004},
  shortjournal = {Stat. Med.},
  title        = {Novel non-linear models for clinical trial analysis with longitudinal data: A tutorial using SAS for both frequentist and bayesian methods},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and sample size determination for multiple-dose
randomized phase II trials for dose optimization. <em>SIM</em>,
<em>43</em>(15), 2972–2986. (<a
href="https://doi.org/10.1002/sim.10093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The U.S. Food and Drug Administration (FDA) has launched Project Optimus to shift dose selection from the maximum tolerated dose (MTD) to the dose that produces the optimal risk-benefit tradeoff. One approach highlighted in the FDA&#39;s guidance involves conducting a randomized phase II trial following the completion of a phase I trial, where multiple doses (typically including the MTD and one or two doses lower than the MTD) are compared to identify the optimal dose that maximizes the benefit-risk tradeoff. This article focuses on the design of such a multiple-dose randomized trial, specifically the determination of the sample size. We generalized the standard definitions of type I error and power to accommodate the unique characteristics of dose optimization and derived a decision rule along with an algorithm to determine the optimal sample size. The resulting design is referred to as MERIT (Multiple-dosE RandomIzed Trial design for dose optimization based on toxicity and efficacy). Simulation studies demonstrate that MERIT has desirable operating characteristics, and a sample size between 20 and 40 per dosage arm often offers reasonable power and type I errors to ensure patient safety and benefit. To facilitate the implementation of the MERIT design, we provide software, available at https://www.trialdesign.org .},
  archive      = {J_SIM},
  author       = {Peng Yang and Daniel Li and Ruitao Lin and Bo Huang and Ying Yuan},
  doi          = {10.1002/sim.10093},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2972-2986},
  shortjournal = {Stat. Med.},
  title        = {Design and sample size determination for multiple-dose randomized phase II trials for dose optimization},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Random effects models of tumour growth for investigating
interval breast cancer. <em>SIM</em>, <em>43</em>(15), 2957–2971. (<a
href="https://doi.org/10.1002/sim.10105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Nordic countries and across Europe, breast cancer screening participation is high. However, a significant number of breast cancer cases are still diagnosed due to symptoms between screening rounds, termed “interval cancers”. Radiologists use the interval cancer proportion as a proxy for the screening false negative rate (ie, 1-sensitivity). Our objective is to enhance our understanding of interval cancers by applying continuous tumour growth models to data from a study involving incident invasive breast cancer cases. Building upon previous findings regarding stationary distributions of tumour size and growth rate distributions in non-screened populations, we develop an analytical expression for the proportion of interval breast cancer cases among regularly screened women. Our approach avoids relying on estimated background cancer rates. We make specific parametric assumptions concerning tumour growth and detection processes (screening or symptoms), but our framework easily accommodates alternative assumptions. We also show how our developed analytical expression for the proportion of interval breast cancers within a screened population can be incorporated into an approach for fitting tumour growth models to incident case data. We fit a model on 3493 cases diagnosed in Sweden between 2001 and 2008. Our methodology allows us to estimate the distribution of tumour sizes at the most recent screening for interval cancers. Importantly, we find that our model-based expected incidence of interval breast cancers aligns closely with observed patterns in our study and in a large Nordic screening cohort. Finally, we evaluate the association between screening interval length and the interval cancer proportion. Our analytical expression represents a useful tool for gaining insights into the performance of population-based breast cancer screening programs.},
  archive      = {J_SIM},
  author       = {Letizia Orsini and Kamila Czene and Keith Humphreys},
  doi          = {10.1002/sim.10105},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2957-2971},
  shortjournal = {Stat. Med.},
  title        = {Random effects models of tumour growth for investigating interval breast cancer},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample size calculation for comparing two screening tests
when the gold standard is missing at random. <em>SIM</em>,
<em>43</em>(15), 2944–2956. (<a
href="https://doi.org/10.1002/sim.10109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample size formulas have been proposed for comparing two sensitivities (specificities) in the presence of verification bias under a paired design. However, the existing sample size formulas involve lengthy calculations of derivatives and are too complicated to implement. In this paper, we propose alternative sample size formulas for each of three existing tests, two Wald tests and one weighted McNemar&#39;s test. The proposed sample size formulas are more intuitive and simpler to implement than their existing counterparts. Furthermore, by comparing the sample sizes calculated based on the three tests, we can show that the three tests have similar sample sizes even though the weighted McNemar&#39;s test only use the data from discordant pairs whereas the two Wald tests also use the additional data from accordant pairs.},
  archive      = {J_SIM},
  author       = {Yougui Wu},
  doi          = {10.1002/sim.10109},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2944-2956},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation for comparing two screening tests when the gold standard is missing at random},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified bayesian framework for bias adjustment in multiple
comparisons from clinical trials. <em>SIM</em>, <em>43</em>(15),
2928–2943. (<a href="https://doi.org/10.1002/sim.10064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, multiple comparisons arising from various treatments/doses, subgroups, or endpoints are common. Typically, trial teams focus on the comparison showing the largest observed treatment effect, often involving a specific treatment pair and endpoint within a subgroup. These findings frequently lead to follow-up pivotal studies, many of which do not confirm the initial positive results. Selection bias occurs when the most promising treatment, subgroup, or endpoint is chosen for further development, potentially skewing subsequent investigations. Such bias can be defined as the deviation in the observed treatment effects from the underlying truth. In this article, we propose a general and unified Bayesian framework to address selection bias in clinical trials with multiple comparisons. Our approach does not require a priori specification of a parametric distribution for the prior, offering a more flexible and generalized solution. The proposed method facilitates a more accurate interpretation of clinical trial results by adjusting for such selection bias. Through simulation studies, we compared several methods and demonstrated their superior performance over the normal shrinkage estimator. We recommended the use of Bayesian Model Averaging estimator averaging over Gaussian Mixture Models as the prior distribution based on its performance and flexibility. We applied the method to a multicenter, randomized, double-blind, placebo-controlled study investigating the cardiovascular effects of dulaglutide.},
  archive      = {J_SIM},
  author       = {Yu Du and Jianghao Li and Sohini Raha and Yongming Qu},
  doi          = {10.1002/sim.10064},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2928-2943},
  shortjournal = {Stat. Med.},
  title        = {A unified bayesian framework for bias adjustment in multiple comparisons from clinical trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast bootstrap algorithm for causal inference with large
data. <em>SIM</em>, <em>43</em>(15), 2894–2927. (<a
href="https://doi.org/10.1002/sim.10075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating causal effects from large experimental and observational data has become increasingly prevalent in both industry and research. The bootstrap is an intuitive and powerful technique used to construct standard errors and confidence intervals of estimators. Its application however can be prohibitively demanding in settings involving large data. In addition, modern causal inference estimators based on machine learning and optimization techniques exacerbate the computational burden of the bootstrap. The bag of little bootstraps has been proposed in non-causal settings for large data but has not yet been applied to evaluate the properties of estimators of causal effects. In this article, we introduce a new bootstrap algorithm called causal bag of little bootstraps for causal inference with large data. The new algorithm significantly improves the computational efficiency of the traditional bootstrap while providing consistent estimates and desirable confidence interval coverage. We describe its properties, provide practical considerations, and evaluate the performance of the proposed algorithm in terms of bias, coverage of the true 95% confidence intervals, and computational time in a simulation study. We apply it in the evaluation of the effect of hormone therapy on the average time to coronary heart disease using a large observational data set from the Women&#39;s Health Initiative.},
  archive      = {J_SIM},
  author       = {Matthew Kosko and Lin Wang and Michele Santacatterina},
  doi          = {10.1002/sim.10075},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2894-2927},
  shortjournal = {Stat. Med.},
  title        = {A fast bootstrap algorithm for causal inference with large data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal mediation analysis with a three-dimensional image
mediator. <em>SIM</em>, <em>43</em>(15), 2869–2893. (<a
href="https://doi.org/10.1002/sim.10106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis is increasingly abundant in biology, psychology, and epidemiology studies and so forth. In particular, with the advent of the big data era, the issue of high-dimensional mediators is becoming more prevalent. In neuroscience, with the widespread application of magnetic resonance technology in the field of brain imaging, studies on image being a mediator emerged. In this study, a novel causal mediation analysis method with a three-dimensional image mediator is proposed. We define the average casual effects under the potential outcome framework, explore several sufficient conditions for the valid identification, and develop techniques for estimation and inference. To verify the effectiveness of the proposed method, a series of simulations under various scenarios is performed. Finally, the proposed method is applied to a study on the causal effect of mother&#39;s delivery mode on child&#39;s IQ development. It is found that cesarean section may have a negative effect on intellectual performance and that this effect is mediated by white matter development. Additional prospective and longitudinal studies may be necessary to validate these emerging findings.},
  archive      = {J_SIM},
  author       = {Minghao Chen and Yingchun Zhou},
  doi          = {10.1002/sim.10106},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2869-2893},
  shortjournal = {Stat. Med.},
  title        = {Causal mediation analysis with a three-dimensional image mediator},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). G-formula for observational studies under stratified
interference, with application to bed net use on malaria. <em>SIM</em>,
<em>43</em>(15), 2853–2868. (<a
href="https://doi.org/10.1002/sim.10102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing population-level effects of vaccines and other infectious disease prevention measures is important to the field of public health. In infectious disease studies, one person&#39;s treatment may affect another individual&#39;s outcome, that is, there may be interference between units. For example, the use of bed nets to prevent malaria by one individual may have an indirect effect on other individuals living in close proximity. In some settings, individuals may form groups or clusters where interference only occurs within groups, that is, there is partial interference. Inverse probability weighted estimators have previously been developed for observational studies with partial interference. Unfortunately, these estimators are not well suited for studies with large clusters. Therefore, in this paper, the parametric g-formula is extended to allow for partial interference. G-formula estimators are proposed for overall effects, effects when treated, and effects when untreated. The proposed estimators can accommodate large clusters and do not suffer from the g-null paradox that may occur in the absence of interference. The large sample properties of the proposed estimators are derived assuming no unmeasured confounders and that the partial interference takes a particular form (referred to as ‘weak stratified interference’). Simulation studies are presented demonstrating the finite-sample performance of the proposed estimators. The Demographic and Health Survey from the Democratic Republic of the Congo is then analyzed using the proposed g-formula estimators to assess the effects of bed net use on malaria.},
  archive      = {J_SIM},
  author       = {Kayla W. Kilpatrick and Chanhwa Lee and Michael G. Hudgens},
  doi          = {10.1002/sim.10102},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2853-2868},
  shortjournal = {Stat. Med.},
  title        = {G-formula for observational studies under stratified interference, with application to bed net use on malaria},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calibration plots for multistate risk prediction models.
<em>SIM</em>, <em>43</em>(14), 2830–2852. (<a
href="https://doi.org/10.1002/sim.10094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Alexander Pate and Matthew Sperrin and Richard D. Riley and Niels Peek and Tjeerd Van Staa and Jamie C. Sergeant and Mamas A. Mamas and Gregory Y. H. Lip and Martin O&#39;Flaherty and Michael Barrowman and Iain Buchan and Glen P. Martin},
  doi          = {10.1002/sim.10094},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2830-2852},
  shortjournal = {Stat. Med.},
  title        = {Calibration plots for multistate risk prediction models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive designs in public health: Vaccine and cluster
randomized trials go bayesian. <em>SIM</em>, <em>43</em>(14), 2811–2829.
(<a href="https://doi.org/10.1002/sim.10104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials in public health—particularly those conducted in low- and middle-income countries—often involve communicable and non-communicable diseases with high disease burden and unmet needs. Trials conducted in these regions often are faced with resource limitations, so improving the efficiencies of these trials is critical. Adaptive trial designs have the potential to save trial time and resources and reduce the number of patients receiving ineffective interventions. In this paper, we provide a detailed account of the implementation of vaccine and cluster randomized trials within the framework of Bayesian adaptive trials, with emphasis on computational efficiency and flexibility with regard to stopping rules and allocation ratios. We offer an educated approach to selecting prior distributions and a data-driven empirical Bayes method for plug-in estimates for nuisance parameters.},
  archive      = {J_SIM},
  author       = {Ofir Harari and Jay J. H. Park and Prince Kumar Lat and Edward J. Mills},
  doi          = {10.1002/sim.10104},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2811-2829},
  shortjournal = {Stat. Med.},
  title        = {Adaptive designs in public health: Vaccine and cluster randomized trials go bayesian},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Propensity score matching for estimating a marginal hazard
ratio. <em>SIM</em>, <em>43</em>(14), 2783–2810. (<a
href="https://doi.org/10.1002/sim.10103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity score matching is commonly used to draw causal inference from observational survival data. However, its asymptotic properties have yet to be established, and variance estimation is still open to debate. We derive the statistical properties of the propensity score matching estimator of the marginal causal hazard ratio based on matching with replacement and a fixed number of matches. We also propose a double-resampling technique for variance estimation that takes into account the uncertainty due to propensity score estimation prior to matching.},
  archive      = {J_SIM},
  author       = {Tongrong Wang and Honghe Zhao and Shu Yang and Shuhan Tang and Zhanglin Cui and Li Li and Douglas E. Faries},
  doi          = {10.1002/sim.10103},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2783-2810},
  shortjournal = {Stat. Med.},
  title        = {Propensity score matching for estimating a marginal hazard ratio},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning optimal biomarker-guided treatment policy for
chronic disorders. <em>SIM</em>, <em>43</em>(14), 2765–2782. (<a
href="https://doi.org/10.1002/sim.10099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) provides noninvasive measures of brain activity and is found to be valuable for the diagnosis of some chronic disorders. Specifically, pre-treatment EEG signals in the alpha and theta frequency bands have demonstrated some association with antidepressant response, which is well-known to have a low response rate. We aim to design an integrated pipeline that improves the response rate of patients with major depressive disorder by developing a treatment policy guided by the resting state pre-treatment EEG recordings and other treatment effects modifiers. First, we design an innovative automatic site-specific EEG preprocessing pipeline to extract features with stronger signals than raw data. We then estimate the conditional average treatment effect (CATE) using causal forests and use a doubly robust technique to improve efficiency in the estimation of the average treatment effect. We present evidence of heterogeneity in the treatment effect and the modifying power of the EEG features, as well as a significant average treatment effect, a result that cannot be obtained with conventional methods. Finally, we employ an efficient policy learning algorithm to learn an optimal depth-2 treatment assignment decision tree and compare its performance with Q-Learning and outcome-weighted learning via simulation studies and an application to a large multi-site, double-blind, randomized controlled clinical trial, EMBARC.},
  archive      = {J_SIM},
  author       = {Bin Yang and Xingche Guo and Ji Meng Loh and Qinxia Wang and Yuanjia Wang},
  doi          = {10.1002/sim.10099},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2765-2782},
  shortjournal = {Stat. Med.},
  title        = {Learning optimal biomarker-guided treatment policy for chronic disorders},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Negative variance components and intercept-slope
correlations greater than one in magnitude: How do such “non-regular”
random intercept and slope models arise, and what should be done when
they do? <em>SIM</em>, <em>43</em>(14), 2747–2764. (<a
href="https://doi.org/10.1002/sim.10070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models with random intercepts and slopes (RIAS models) are commonly used to analyze longitudinal data. Fitting such models sometimes results in negative estimates of variance components or estimates on parameter space boundaries. This can be an unlucky chance occurrence, but can also occur because certain marginal distributions are mathematically identical to those from RIAS models with negative intercept and/or slope variance components and/or intercept-slope correlations greater than one in magnitude. We term such parameters “pseudo-variances” and “pseudo-correlations,” and the models “non-regular.” We use eigenvalue theory to explore how and when such non-regular RIAS models arise, showing: (i) A small number of measurements, short follow-up, and large residual variance increase the parameter space for which data (with a positive semidefinite marginal variance-covariance matrix) are compatible with non-regular RIAS models. (ii) Non-regular RIAS models can arise from model misspecification, when non-linearity in fixed effects is ignored or when random effects are omitted. (iii) A non-regular RIAS model can sometimes be interpreted as a regular linear mixed model with one or more additional random effects, which may not be identifiable from the data. (iv) Particular parameterizations of non-regular RIAS models have no generality for all possible numbers of measurements over time. Because of this lack of generality, we conclude that non-regular RIAS models can only be regarded as plausible data-generating mechanisms in some situations. Nevertheless, fitting a non-regular RIAS model can be acceptable, allowing unbiased inference on fixed effects where commonly recommended alternatives such as dropping the random slope result in bias.},
  archive      = {J_SIM},
  author       = {Helen Bridge and Katy E. Morgan and Chris Frost},
  doi          = {10.1002/sim.10070},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2747-2764},
  shortjournal = {Stat. Med.},
  title        = {Negative variance components and intercept-slope correlations greater than one in magnitude: How do such “non-regular” random intercept and slope models arise, and what should be done when they do?},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online causal inference with application to near real-time
post-market vaccine safety surveillance. <em>SIM</em>, <em>43</em>(14),
2734–2746. (<a href="https://doi.org/10.1002/sim.10095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming data routinely generated by social networks, mobile or web applications, e-commerce, and electronic health records present new opportunities to monitor the impact of an intervention on an outcome via causal inference methods. However, most existing causal inference methods have been focused on and applied to static data, that is, a fixed data set in which observations are pooled and stored before performing statistical analysis. There is thus a pressing need to turn static causal inference into online causal learning to support near real-time monitoring of treatment effects. In this paper, we present a framework for online estimation and inference of treatment effects that can incorporate new information as it becomes available without revisiting prior observations. We show that, under mild regularity conditions, the proposed online estimator is asymptotically equivalent to the offline oracle estimator obtained by pooling all data. Our proposal is motivated by the need for near real-time vaccine effectiveness and safety monitoring, and our proposed method is applied to a case study on COVID-19 vaccine safety surveillance.},
  archive      = {J_SIM},
  author       = {Lan Luo and Malcolm Risk and Xu Shi},
  doi          = {10.1002/sim.10095},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2734-2746},
  shortjournal = {Stat. Med.},
  title        = {Online causal inference with application to near real-time post-market vaccine safety surveillance},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-varying dynamic bayesian network learning for an fMRI
study of emotion processing. <em>SIM</em>, <em>43</em>(14), 2713–2733.
(<a href="https://doi.org/10.1002/sim.10096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel method for learning time-varying dynamic Bayesian networks. The proposed method breaks down the dynamic Bayesian network learning problem into a sequence of regression inference problems and tackles each problem using the Markov neighborhood regression technique. Notably, the method demonstrates scalability concerning data dimensionality, accommodates time-varying network structure, and naturally handles multi-subject data. The proposed method exhibits consistency and offers superior performance compared to existing methods in terms of estimation accuracy and computational efficiency, as supported by extensive numerical experiments. To showcase its effectiveness, we apply the proposed method to an fMRI study investigating the effective connectivity among various regions of interest (ROIs) during an emotion-processing task. Our findings reveal the pivotal role of the subcortical-cerebellum in emotion processing.},
  archive      = {J_SIM},
  author       = {Lizhe Sun and Aiying Zhang and Faming Liang},
  doi          = {10.1002/sim.10096},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2713-2733},
  shortjournal = {Stat. Med.},
  title        = {Time-varying dynamic bayesian network learning for an fMRI study of emotion processing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mediation analysis using incomplete information from
publicly available data sources. <em>SIM</em>, <em>43</em>(14),
2695–2712. (<a href="https://doi.org/10.1002/sim.10076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work was motivated by the question whether, and to what extent, well-established risk factors mediate the racial disparity observed for colorectal cancer (CRC) incidence in the United States. Mediation analysis examines the relationships between an exposure, a mediator and an outcome. All available methods require access to a single complete data set with these three variables. However, because population-based studies usually include few non-White participants, these approaches have limited utility in answering our motivating question. Recently, we developed novel methods to integrate several data sets with incomplete information for mediation analysis. These methods have two limitations: (i) they only consider a single mediator and (ii) they require a data set containing individual-level data on the mediator and exposure (and possibly confounders) obtained by independent and identically distributed sampling from the target population. Here, we propose a new method for mediation analysis with several different data sets that accommodates complex survey and registry data, and allows for multiple mediators. The proposed approach yields unbiased causal effects estimates and confidence intervals with nominal coverage in simulations. We apply our method to data from U.S. cancer registries, a U.S.-population-representative survey and summary level odds-ratio estimates, to rigorously evaluate what proportion of the difference in CRC risk between non-Hispanic Whites and Blacks is mediated by three potentially modifiable risk factors (CRC screening history, body mass index, and regular aspirin use).},
  archive      = {J_SIM},
  author       = {Andriy Derkach and Elizabeth D. Kantor and Joshua N. Sampson and Ruth M. Pfeiffer},
  doi          = {10.1002/sim.10076},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2695-2712},
  shortjournal = {Stat. Med.},
  title        = {Mediation analysis using incomplete information from publicly available data sources},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On variance estimation of the inverse
probability-of-treatment weighting estimator: A tutorial for different
types of propensity score weights. <em>SIM</em>, <em>43</em>(13),
2672–2694. (<a href="https://doi.org/10.1002/sim.10078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity score methods, such as inverse probability-of-treatment weighting (IPTW), have been increasingly used for covariate balancing in both observational studies and randomized trials, allowing the control of both systematic and chance imbalances. Approaches using IPTW are based on two steps: (i) estimation of the individual propensity scores (PS), and (ii) estimation of the treatment effect by applying PS weights. Thus, a variance estimator that accounts for both steps is crucial for correct inference. Using a variance estimator which ignores the first step leads to overestimated variance when the estimand is the average treatment effect (ATE), and to under or overestimated estimates when targeting the average treatment effect on the treated (ATT). In this article, we emphasize the importance of using an IPTW variance estimator that correctly considers the uncertainty in PS estimation. We present a comprehensive tutorial to obtain unbiased variance estimates, by proposing and applying a unifying formula for different types of PS weights (ATE, ATT, matching and overlap weights). This can be derived either via the linearization approach or M-estimation. Extensive R code is provided along with the corresponding large-sample theory. We perform simulation studies to illustrate the behavior of the estimators under different treatment and outcome prevalences and demonstrate appropriate behavior of the analytical variance estimator. We also use a reproducible analysis of observational lung cancer data as an illustrative example, estimating the effect of receiving a PET-CT scan on the receipt of surgery.},
  archive      = {J_SIM},
  author       = {Andriana Kostouraki and David Hajage and Bernard Rachet and Elizabeth J. Williamson and Guillaume Chauvet and Aurélien Belot and Clémence Leyrat},
  doi          = {10.1002/sim.10078},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2672-2694},
  shortjournal = {Stat. Med.},
  title        = {On variance estimation of the inverse probability-of-treatment weighting estimator: A tutorial for different types of propensity score weights},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic hierarchical state space forecasting. <em>SIM</em>,
<em>43</em>(13), 2655–2671. (<a
href="https://doi.org/10.1002/sim.10097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to both borrow information from existing units and incorporate the target unit&#39;s history data in time series forecasting. We consider a situation when we have time series data from multiple units that share similar patterns when aligned in terms of an internal time. The internal time is defined as an index according to evolving features of interest. When mapped back to the calendar time, these time series can span different time intervals that can include the future calendar time of the targeted unit, over which we can borrow the information from other units in forecasting the targeted unit. We first build a hierarchical state space model for the multiple time series data in terms of the internal time, where the shared components capture the similarities among different units while allowing for unit-specific deviations. A conditional state space model is then constructed to incorporate the information of existing units as the prior information in forecasting the targeted unit. By running the Kalman filtering based on the conditional state space model on the targeted unit, we incorporate both the information from the other units and the history of the targeted unit. The forecasts are then transformed from internal time back into calendar time for ease of interpretation. A simulation study is conducted to evaluate the finite sample performance. Forecasting state-level new COVID-19 cases in United States is used for illustration.},
  archive      = {J_SIM},
  author       = {Ziyue Liu and Wensheng Guo},
  doi          = {10.1002/sim.10097},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2655-2671},
  shortjournal = {Stat. Med.},
  title        = {Dynamic hierarchical state space forecasting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cumulative incidence of cardiac surgery associated with
exposure to benfluorex: A retrospective analysis based on compensation
claims data. <em>SIM</em>, <em>43</em>(13), 2641–2654. (<a
href="https://doi.org/10.1002/sim.10100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data on retrospective compensation claims for injuries caused by pharmaceutical drugs are prone to selection and reporting biases. Nevertheless, this case study of the antidiabetic drug benfluorex shows that such data can be used to estimate the cumulative incidence of drug-related injury, and to provide insights into its epidemiology. To this end, we develop a modelling framework for under-reporting of retrospective claims for compensation arising from drug damage. The model involves a longitudinal component related to attrition of cases over time, and a cross-sectional component related to incomplete reporting. We apply this model to cardiac valve surgery necessitated by exposure to benfluorex. Benfluorex was marketed in France between 1976 and 2009, when it was withdrawn because it caused valvular heart disease. A scandal erupted in 2010 over the scale of the damage caused by the drug. Since then, no further estimates of cumulative incidence have been published, though thousands of claims for compensation have been processed. The analysis combines compensation claims data and sociological survey data on benfluorex users, together with data on benfluorex sales and duration of treatment. We find a threshold of toxicity at about 6 months&#39; exposure, and that at least 1690 individuals (95% CI 1290 to 2320) needed heart surgery to replace or repair valves damaged by exposure to benfluorex in France: a cumulative incidence of 3.68 per 10,000 (95% CI 2.68 to 5.34) benfluorex users or 3.22 per 10,000 (95% CI 2.48 to 4.39) person-years at risk above the exposure threshold. While these findings are tentative, they are consistent with those obtained previously using very different methods.},
  archive      = {J_SIM},
  author       = {Paddy Farrington and Solène Lellinger},
  doi          = {10.1002/sim.10100},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2641-2654},
  shortjournal = {Stat. Med.},
  title        = {Cumulative incidence of cardiac surgery associated with exposure to benfluorex: A retrospective analysis based on compensation claims data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributional imputation for the analysis of censored
recurrent events. <em>SIM</em>, <em>43</em>(13), 2622–2640. (<a
href="https://doi.org/10.1002/sim.10087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal clinical trials for which recurrent events endpoints are of interest are commonly subject to missing event data. Primary analyses in such trials are often performed assuming events are missing at random, and sensitivity analyses are necessary to assess robustness of primary analysis conclusions to missing data assumptions. Control-based imputation is an attractive approach in superiority trials for imposing conservative assumptions on how data may be missing not at random. A popular approach to implementing control-based assumptions for recurrent events is multiple imputation (MI), but Rubin&#39;s variance estimator is often biased for the true sampling variability of the point estimator in the control-based setting. We propose distributional imputation (DI) with corresponding wild bootstrap variance estimation procedure for control-based sensitivity analyses of recurrent events. We apply control-based DI to a type I diabetes trial. In the application and simulation studies, DI produced more reasonable standard error estimates than MI with Rubin&#39;s combining rules in control-based sensitivity analyses of recurrent events.},
  archive      = {J_SIM},
  author       = {Sarah R. Fairfax and Shu Yang},
  doi          = {10.1002/sim.10087},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2622-2640},
  shortjournal = {Stat. Med.},
  title        = {Distributional imputation for the analysis of censored recurrent events},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing long-term survival prediction with two short-term
events: Landmarking with a flexible varying coefficient model.
<em>SIM</em>, <em>43</em>(13), 2607–2621. (<a
href="https://doi.org/10.1002/sim.10086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with cardiovascular diseases who experience disease-related short-term events, such as hospitalizations, often exhibit diverse long-term survival outcomes compared to others. In this study, we aim to improve the prediction of long-term survival probability by incorporating two short-term events using a flexible varying coefficient landmark model. Our objective is to predict the long-term survival among patients who survived up to a pre-specified landmark time since the initial admission. Inverse probability weighting estimation equations are formed based on the information of the short-term outcomes before the landmark time. The kernel smoothing method with the use of cross-validation for bandwidth selection is employed to estimate the time-varying coefficients. The predictive performance of the proposed model is evaluated and compared using predictive measures: area under the receiver operating characteristic curve and Brier score. Simulation studies confirm that parameters under the landmark models can be estimated accurately and the predictive performance of the proposed method consistently outperforms existing methods that either do not incorporate or only partially incorporate information from two short-term events. We demonstrate the practical application of our model using a community-based cohort from the Atherosclerosis Risk in Communities (ARIC) study.},
  archive      = {J_SIM},
  author       = {Wen Li and Qian Wang and Jing Ning and Jing Zhang and Zhouxuan Li and Sean I. Savitz and Amirali Tahanan and Mohammad H. Rahbar},
  doi          = {10.1002/sim.10086},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2607-2621},
  shortjournal = {Stat. Med.},
  title        = {Enhancing long-term survival prediction with two short-term events: Landmarking with a flexible varying coefficient model},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible parametrization of graph-theoretical features from
individual-specific networks for prediction. <em>SIM</em>,
<em>43</em>(13), 2592–2606. (<a
href="https://doi.org/10.1002/sim.10091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical techniques are needed to analyze data structures with complex dependencies such that clinically useful information can be extracted. Individual-specific networks, which capture dependencies in complex biological systems, are often summarized by graph-theoretical features. These features, which lend themselves to outcome modeling, can be subject to high variability due to arbitrary decisions in network inference and noise. Correlation-based adjacency matrices often need to be sparsified before meaningful graph-theoretical features can be extracted, requiring the data analysts to determine an optimal threshold. To address this issue, we propose to incorporate a flexible weighting function over the full range of possible thresholds to capture the variability of graph-theoretical features over the threshold domain. The potential of this approach, which extends concepts from functional data analysis to a graph-theoretical setting, is explored in a plasmode simulation study using real functional magnetic resonance imaging (fMRI) data from the Autism Brain Imaging Data Exchange (ABIDE) Preprocessed initiative. The simulations show that our modeling approach yields accurate estimates of the functional form of the weight function, improves inference efficiency, and achieves a comparable or reduced root mean square prediction error compared to competitor modeling approaches. This assertion holds true in settings where both complex functional forms underlie the outcome-generating process and a universal threshold value is employed. We demonstrate the practical utility of our approach by using resting-state fMRI data to predict biological age in children. Our study establishes the flexible modeling approach as a statistically principled, serious competitor to ad-hoc methods with superior performance.},
  archive      = {J_SIM},
  author       = {Mariella Gregorich and Sean L. Simpson and Georg Heinze},
  doi          = {10.1002/sim.10091},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2592-2606},
  shortjournal = {Stat. Med.},
  title        = {Flexible parametrization of graph-theoretical features from individual-specific networks for prediction},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighting estimation in the cause-specific cox regression
with partially missing causes of failure. <em>SIM</em>, <em>43</em>(13),
2575–2591. (<a href="https://doi.org/10.1002/sim.10084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex diseases are often analyzed using disease subtypes classified by multiple biomarkers to study pathogenic heterogeneity. In such molecular pathological epidemiology research, we consider a weighted Cox proportional hazard model to evaluate the effect of exposures on various disease subtypes under competing-risk settings in the presence of partially or completely missing biomarkers. The asymptotic properties of the inverse and augmented inverse probability-weighted estimating equation methods are studied with a general pattern of missing data. Simulation studies have been conducted to demonstrate the double robustness of the estimators. For illustration, we applied this method to examine the association between pack-years of smoking before the age of 30 and the incidence of colorectal cancer subtypes defined by a combination of four tumor molecular biomarkers (statuses of microsatellite instability, CpG island methylator phenotype, BRAF mutation, and KRAS mutation) in the Nurses&#39; Health Study cohort.},
  archive      = {J_SIM},
  author       = {Jooyoung Lee and Shuji Ogino and Molin Wang},
  doi          = {10.1002/sim.10084},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2575-2591},
  shortjournal = {Stat. Med.},
  title        = {Weighting estimation in the cause-specific cox regression with partially missing causes of failure},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variance-components tests for genetic association with
multiple interval-censored outcomes. <em>SIM</em>, <em>43</em>(13),
2560–2574. (<a href="https://doi.org/10.1002/sim.10081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive genetic compendiums such as the UK Biobank have become an invaluable resource for identifying genetic variants that are associated with complex diseases. Due to the difficulties of massive data collection, a common practice of these compendiums is to collect interval-censored data. One challenge in analyzing such data is the lack of methodology available for genetic association studies with interval-censored data. Genetic effects are difficult to detect because of their rare and weak nature, and often the time-to-event outcomes are transformed to binary phenotypes for access to more powerful signal detection approaches. However transforming the data to binary outcomes can result in loss of valuable information. To alleviate such challenges, this work develops methodology to associate genetic variant sets with multiple interval-censored outcomes. Testing sets of variants such as genes or pathways is a common approach in genetic association settings to lower the multiple testing burden, aggregate small effects, and improve interpretations of results. Instead of performing inference with only a single outcome, utilizing multiple outcomes can increase statistical power by aggregating information across multiple correlated phenotypes. Simulations show that the proposed strategy can offer significant power gains over a single outcome approach. We apply the proposed test to the investigation that motivated this study, a search for the genes that perturb risks of bone fractures and falls in the UK Biobank.},
  archive      = {J_SIM},
  author       = {Jaihee Choi and Zhichao Xu and Ryan Sun},
  doi          = {10.1002/sim.10081},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2560-2574},
  shortjournal = {Stat. Med.},
  title        = {Variance-components tests for genetic association with multiple interval-censored outcomes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Population-average mediation analysis for zero-inflated
count outcomes. <em>SIM</em>, <em>43</em>(13), 2547–2559. (<a
href="https://doi.org/10.1002/sim.10085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is an increasingly popular statistical method for explaining causal pathways to inform intervention. While methods have increased, there is still a dearth of robust mediation methods for count outcomes with excess zeroes. Current mediation methods addressing this issue are computationally intensive, biased, or challenging to interpret. To overcome these limitations, we propose a new mediation methodology for zero-inflated count outcomes using the marginalized zero-inflated Poisson (MZIP) model and the counterfactual approach to mediation. This novel work gives population-average mediation effects whose variance can be estimated rapidly via delta method. This methodology is extended to cases with exposure-mediator interactions. We apply this novel methodology to explore if diabetes diagnosis can explain BMI differences in healthcare utilization and test model performance via simulations comparing the proposed MZIP method to existing zero-inflated and Poisson methods. We find that our proposed method minimizes bias and computation time compared to alternative approaches while allowing for straight-forward interpretations.},
  archive      = {J_SIM},
  author       = {Andrew Sims and D. Leann Long and Hemant K. Tiwari and Jinhong Cui and Dustin M. Long and Todd M. Brown and Melissa J. Smith and Emily B. Levitan},
  doi          = {10.1002/sim.10085},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2547-2559},
  shortjournal = {Stat. Med.},
  title        = {Population-average mediation analysis for zero-inflated count outcomes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiblock partial least squares and rank aggregation:
Applications to detection of bacteriophages associated with
antimicrobial resistance in the presence of potential confounding
factors. <em>SIM</em>, <em>43</em>(13), 2527–2546. (<a
href="https://doi.org/10.1002/sim.10058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban environments, characterized by bustling mass transit systems and high population density, host a complex web of microorganisms that impact microbial interactions. These urban microbiomes, influenced by diverse demographics and constant human movement, are vital for understanding microbial dynamics. We explore urban metagenomics, utilizing an extensive dataset from the Metagenomics &amp; Metadesign of Subways &amp; Urban Biomes (MetaSUB) consortium, and investigate antimicrobial resistance (AMR) patterns. In this pioneering research, we delve into the role of bacteriophages, or “phages”–viruses that prey on bacteria and can facilitate the exchange of antibiotic resistance genes (ARGs) through mechanisms like horizontal gene transfer (HGT). Despite their potential significance, existing literature lacks a consensus on their significance in ARG dissemination. We argue that they are an important consideration. We uncover that environmental variables, such as those on climate, demographics, and landscape, can obscure phage-resistome relationships. We adjust for these potential confounders and clarify these relationships across specific and overall antibiotic classes with precision, identifying several key phages. Leveraging machine learning tools and validating findings through clinical literature, we uncover novel associations, adding valuable insights to our comprehension of AMR development.},
  archive      = {J_SIM},
  author       = {Shoumi Sarkar and Samuel Anyaso-Samuel and Peihua Qiu and Somnath Datta},
  doi          = {10.1002/sim.10058},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2527-2546},
  shortjournal = {Stat. Med.},
  title        = {Multiblock partial least squares and rank aggregation: Applications to detection of bacteriophages associated with antimicrobial resistance in the presence of potential confounding factors},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Order selection for heterogeneous semiparametric hidden
markov models. <em>SIM</em>, <em>43</em>(13), 2501–2526. (<a
href="https://doi.org/10.1002/sim.10069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models (HMMs), which can characterize dynamic heterogeneity, are valuable tools for analyzing longitudinal data. The order of HMMs (ie, the number of hidden states) is typically assumed to be known or predetermined by some model selection criterion in conventional analysis. As prior information about the order frequently lacks, pairwise comparisons under criterion-based methods become computationally expensive with the model space growing. A few studies have conducted order selection and parameter estimation simultaneously, but they only considered homogeneous parametric instances. This study proposes a Bayesian double penalization (BDP) procedure for simultaneous order selection and parameter estimation of heterogeneous semiparametric HMMs. To overcome the difficulties in updating the order, we create a brand-new Markov chain Monte Carlo algorithm coupled with an effective adjust-bound reversible jump strategy. Simulation results reveal that the proposed BDP procedure performs well in estimation and works noticeably better than the conventional criterion-based approaches. Application of the suggested method to the Alzheimer&#39;s Disease Neuroimaging Initiative research further supports its usefulness.},
  archive      = {J_SIM},
  author       = {Yudan Zou and Xinyuan Song and Qian Zhao},
  doi          = {10.1002/sim.10069},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2501-2526},
  shortjournal = {Stat. Med.},
  title        = {Order selection for heterogeneous semiparametric hidden markov models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding the best subgroup with differential treatment effect
with multiple outcomes. <em>SIM</em>, <em>43</em>(13), 2487–2500. (<a
href="https://doi.org/10.1002/sim.10083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine aims to identify specific patient subgroups that may benefit the most from a particular treatment than the whole population. Existing definitions for the best subgroup in subgroup analysis are based on a single outcome and do not consider multiple outcomes; specifically, outcomes of different types. In this article, we introduce a definition for the best subgroup under a multiple-outcome setting with continuous, binary, and censored time-to-event outcomes. Our definition provides a trade-off between the subgroup size and the conditional average treatment effects (CATE) in the subgroup with respect to each of the outcomes while taking the relative contribution of the outcomes into account. We conduct simulations to illustrate the proposed definition. By examining the outcomes of urinary tract infection and renal scarring in the RIVUR clinical trial, we identify a subgroup of children that would benefit the most from long-term antimicrobial prophylaxis.},
  archive      = {J_SIM},
  author       = {Beibo Zhao and Jason Fine and Anastasia Ivanova},
  doi          = {10.1002/sim.10083},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2487-2500},
  shortjournal = {Stat. Med.},
  title        = {Finding the best subgroup with differential treatment effect with multiple outcomes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical considerations in model-based dose finding for
binary responses under model uncertainty. <em>SIM</em>, <em>43</em>(12),
2472–2485. (<a href="https://doi.org/10.1002/sim.10082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The statistical methodology for model-based dose finding under model uncertainty has attracted increasing attention in recent years. While the underlying principles are simple and easy to understand, developing and implementing an efficient approach for binary responses can be a formidable task in practice. Motivated by the statistical challenges encountered in a phase II dose finding study, we explore several key design and analysis issues related to the hybrid testing-modeling approaches for binary responses. The issues include candidate model selection and specifications, optimal design and efficient sample size allocations, and, notably, the methods for dose-response testing and estimation. Specifically, we consider a class of generalized linear models suited for the candidate set and establish D-optimal designs for these models. Additionally, we propose using permutation-based tests for dose-response testing to avoid asymptotic normality assumptions typically required for contrast-based tests. We perform trial simulations to enhance our understanding of these issues.},
  archive      = {J_SIM},
  author       = {Zhiwu Yan and Min Yang},
  doi          = {10.1002/sim.10082},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2472-2485},
  shortjournal = {Stat. Med.},
  title        = {Statistical considerations in model-based dose finding for binary responses under model uncertainty},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A discrete approximation method for modeling
interval-censored multistate data. <em>SIM</em>, <em>43</em>(12),
2452–2471. (<a href="https://doi.org/10.1002/sim.10079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many longitudinal studies are designed to monitor participants for major events related to the progression of diseases. Data arising from such longitudinal studies are usually subject to interval censoring since the events are only known to occur between two monitoring visits. In this work, we propose a new method to handle interval-censored multistate data within a proportional hazards model framework where the hazard rate of events is modeled by a nonparametric function of time and the covariates affect the hazard rate proportionally. The main idea of this method is to simplify the likelihood functions of a discrete-time multistate model through an approximation and the application of data augmentation techniques, where the assumed presence of censored information facilitates a simpler parameterization. Then the expectation-maximization algorithm is used to estimate the parameters in the model. The performance of the proposed method is evaluated by numerical studies. Finally, the method is employed to analyze a dataset on tracking the advancement of coronary allograft vasculopathy following heart transplantation.},
  archive      = {J_SIM},
  author       = {Lu You and Xiang Liu and Jeffrey Krischer},
  doi          = {10.1002/sim.10079},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2452-2471},
  shortjournal = {Stat. Med.},
  title        = {A discrete approximation method for modeling interval-censored multistate data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian platform trial design with hybrid control based
on multisource exchangeability modelling. <em>SIM</em>, <em>43</em>(12),
2439–2451. (<a href="https://doi.org/10.1002/sim.10077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enrolling patients to the standard of care (SOC) arm in randomized clinical trials, especially for rare diseases, can be very challenging due to the lack of resources, restricted patient population availability, and ethical considerations. As the therapeutic effect for the SOC is often well documented in historical trials, we propose a Bayesian platform trial design with hybrid control based on the multisource exchangeability modelling (MEM) framework to harness historical control data. The MEM approach provides a computationally efficient method to formally evaluate the exchangeability of study outcomes between different data sources and allows us to make better informed data borrowing decisions based on the exchangeability between historical and concurrent data. We conduct extensive simulation studies to evaluate the proposed hybrid design. We demonstrate the proposed design leads to significant sample size reduction for the internal control arm and borrows more information compared to competing Bayesian approaches when historical and internal data are compatible.},
  archive      = {J_SIM},
  author       = {Wei Wei and Ondrej Blaha and Denise Esserman and Daniel Zelterman and Michael Kane and Rachael Liu and Jianchang Lin},
  doi          = {10.1002/sim.10077},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2439-2451},
  shortjournal = {Stat. Med.},
  title        = {A bayesian platform trial design with hybrid control based on multisource exchangeability modelling},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian federated inference for estimating statistical
models based on non-shared multicenter data sets. <em>SIM</em>,
<em>43</em>(12), 2421–2438. (<a
href="https://doi.org/10.1002/sim.10072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying predictive factors for an outcome of interest via a multivariable analysis is often difficult when the data set is small. Combining data from different medical centers into a single (larger) database would alleviate this problem, but is in practice challenging due to regulatory and logistic problems. Federated learning (FL) is a machine learning approach that aims to construct from local inferences in separate data centers what would have been inferred had the data sets been merged. It seeks to harvest the statistical power of larger data sets without actually creating them. The FL strategy is not always efficient and precise. Therefore, in this paper we refine and implement an alternative Bayesian federated inference (BFI) framework for multicenter data with the same aim as FL. The BFI framework is designed to cope with small data sets by inferring locally not only the optimal parameter values, but also additional features of the posterior parameter distribution, capturing information beyond what is used in FL. BFI has the additional benefit that a single inference cycle across the centers is sufficient, whereas FL needs multiple cycles. We quantify the performance of the proposed methodology on simulated and real life data.},
  archive      = {J_SIM},
  author       = {Marianne A Jonker and Hassan Pazira and Anthony CC Coolen},
  doi          = {10.1002/sim.10072},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2421-2438},
  shortjournal = {Stat. Med.},
  title        = {Bayesian federated inference for estimating statistical models based on non-shared multicenter data sets},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Individualized empirical null estimation for exact tests of
healthcare quality. <em>SIM</em>, <em>43</em>(12), 2403–2420. (<a
href="https://doi.org/10.1002/sim.10074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {United States federal agencies evaluate healthcare providers to identify, flag, and potentially penalize those that deliver low-quality care compared to national expectations. In practice, evaluation metrics are inevitably impacted by unobserved confounding factors, which reduce flagging accuracy and cause the statistics to be overdispersed relative to the theoretical null distributions. In response to this issue, several authors have proposed individualized empirical null (IEN) methods to estimate an appropriate null distribution for each provider&#39;s evaluation statistic while taking into account the provider&#39;s effective size. However, existing IEN methods require that the statistics asymptotically follow normal distributions, which often does not hold in applications with small providers or misspecified models. In this article, we develop an IEN framework for exact hypothesis tests that accounts for the impact of unobserved confounding without making any asymptotic assumptions. Simulations show that the proposed IEN method has greater flagging accuracy compared to conventional approaches. We apply these methods to evaluate dialysis facilities and transplant centers that are monitored by the Centers for Medicare and Medicaid Services.},
  archive      = {J_SIM},
  author       = {Nicholas Hartman and Kevin He},
  doi          = {10.1002/sim.10074},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2403-2420},
  shortjournal = {Stat. Med.},
  title        = {Individualized empirical null estimation for exact tests of healthcare quality},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A joint frailty model for recurrent and competing terminal
events: Application to delirium in the ICU. <em>SIM</em>,
<em>43</em>(12), 2389–2402. (<a
href="https://doi.org/10.1002/sim.10053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models linking longitudinal biomarkers or recurrent event processes with a terminal event, for example, mortality, have been studied extensively. Motivated by studies of recurrent delirium events in patients receiving care in an intensive care unit (ICU), we devise a joint model for a recurrent event process and multiple terminal events. Being discharged alive from the ICU or experiencing mortality may be associated with a patient&#39;s hazard of delirium, violating the assumption of independent censoring. Moreover, the direction of the association between the hazards of delirium and mortality may be opposite of the direction of association between the hazards of delirium and ICU discharge. Hence treating either terminal event as independent censoring may bias inferences. We propose a competing joint model that uses a latent frailty to link a patient&#39;s recurrent and competing terminal event processes. We fit our model to data from a completed placebo-controlled clinical trial, which studied whether Haloperidol could prevent death and delirium among ICU patients. The clinical trial served as a foundation for a simulation study, in which we evaluate the properties, for example, bias and confidence interval coverage, of the competing joint model. As part of the simulation study, we demonstrate the shortcomings of using a joint model with a recurrent delirium process and a single terminal event to study delirium in the ICU. Lastly, we discuss limitations and possible extensions for the competing joint model. The competing joint model has been added to frailtypack , an R package for fitting an assortment of joint models.},
  archive      = {J_SIM},
  author       = {Lacey H. Etzkorn and Quentin Le Coënt and Mark van den Boogaard and Virginie Rondeau and Elizabeth Colantuoni},
  doi          = {10.1002/sim.10053},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2389-2402},
  shortjournal = {Stat. Med.},
  title        = {A joint frailty model for recurrent and competing terminal events: Application to delirium in the ICU},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-stage group-sequential design for delayed treatment
responses with the possibility of trial restart. <em>SIM</em>,
<em>43</em>(12), 2368–2388. (<a
href="https://doi.org/10.1002/sim.10061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common statistical theory applicable to confirmatory phase III trial designs usually assumes that patients are enrolled simultaneously and there is no time gap between enrollment and outcome observation. However, in practice, patients are enrolled successively and there is a lag between the enrollment of a patient and the measurement of the primary outcome. For single-stage designs, the difference between theory and practice only impacts on the trial duration but not on the statistical analysis and its interpretation. For designs with interim analyses, however, the number of patients already enrolled into the trial and the number of patients with available outcome measurements differ, which can cause issues regarding the statistical analyses of the data. The main issue is that current methodologies either imply that at the time of the interim analysis there are so-called pipeline patients whose data are not used to make a statistical decision (like stopping early for efficacy) or the enrollment into the trial needs to be at least paused for interim analysis to avoid pipeline patients. There are methods for delayed responses available that introduced error-spending stopping boundaries for the enrollment of patients followed by critical values to reject the null hypothesis in case the stopping boundaries have been crossed beforehand. Here, we will discuss other solutions, considering different boundary determination algorithms using conditional power and introducing a design allowing for recruitment restart while keeping the type I error rate controlled.},
  archive      = {J_SIM},
  author       = {Stephen Schüürhuis and Frank Konietschke and Cornelia Ursula Kunz},
  doi          = {10.1002/sim.10061},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2368-2388},
  shortjournal = {Stat. Med.},
  title        = {A two-stage group-sequential design for delayed treatment responses with the possibility of trial restart},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence intervals for odds ratio from multistage
randomized phase II trials. <em>SIM</em>, <em>43</em>(12), 2359–2367.
(<a href="https://doi.org/10.1002/sim.10073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-stage randomized trial design can significantly improve efficiency by allowing early termination of the trial when the experimental arm exhibits either low or high efficacy compared to the control arm during the study. However, proper inference methods are necessary because the underlying distribution of the target statistic changes due to the multi-stage structure. This article focuses on multi-stage randomized phase II trials with a dichotomous outcome, such as treatment response, and proposes exact conditional confidence intervals for the odds ratio. The usual single-stage confidence intervals are invalid when used in multi-stage trials. To address this issue, we propose a linear ordering of all possible outcomes. This ordering is conditioned on the total number of responders in each stage and utilizes the exact conditional distribution function of the outcomes. This approach enables the estimation of an exact confidence interval accounting for the multi-stage designs.},
  archive      = {J_SIM},
  author       = {Shiwei Cao and Sin-Ho Jung},
  doi          = {10.1002/sim.10073},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2359-2367},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals for odds ratio from multistage randomized phase II trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximate balancing weights for clustered observational
study designs. <em>SIM</em>, <em>43</em>(12), 2332–2358. (<a
href="https://doi.org/10.1002/sim.10054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a clustered observational study, a treatment is assigned to groups and all units within the group are exposed to the treatment. We develop a new method for statistical adjustment in clustered observational studies using approximate balancing weights, a generalization of inverse propensity score weights that solve a convex optimization problem to find a set of weights that directly minimize a measure of covariate imbalance, subject to an additional penalty on the variance of the weights. We tailor the approximate balancing weights optimization problem to the clustered observational study setting by deriving an upper bound on the mean square error and finding weights that minimize this upper bound, linking the level of covariate balance to a bound on the bias. We implement the procedure by specializing the bound to a random cluster-level effects model, leading to a variance penalty that incorporates the signal-to-noise ratio and penalizes the weight on individuals and the total weight on groups differently according to the the intra-class correlation.},
  archive      = {J_SIM},
  author       = {Eli Ben-Michael and Lindsay Page and Luke Keele},
  doi          = {10.1002/sim.10054},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2332-2358},
  shortjournal = {Stat. Med.},
  title        = {Approximate balancing weights for clustered observational study designs},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing efficacy in non-inferiority trials with
non-adherence to interventions: Are intention-to-treat and per-protocol
analyses fit for purpose? <em>SIM</em>, <em>43</em>(12), 2314–2331. (<a
href="https://doi.org/10.1002/sim.10067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Matthew Dodd and James Carpenter and Jennifer A. Thompson and Elizabeth Williamson and Katherine Fielding and Diana Elbourne},
  doi          = {10.1002/sim.10067},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2314-2331},
  shortjournal = {Stat. Med.},
  title        = {Assessing efficacy in non-inferiority trials with non-adherence to interventions: Are intention-to-treat and per-protocol analyses fit for purpose?},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal mediation analysis with mediator values below an
assay limit. <em>SIM</em>, <em>43</em>(12), 2299–2313. (<a
href="https://doi.org/10.1002/sim.10065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal indirect and direct effects provide an interpretable method for decomposing the total effect of an exposure on an outcome into the indirect effect through a mediator and the direct effect through all other pathways. A natural choice for a mediator in a randomized clinical trial is the treatment&#39;s targeted biomarker. However, when the mediator is a biomarker, values can be subject to an assay lower limit. The mediator is affected by the treatment and is a putative cause of the outcome, so the assay lower limit presents a compounded problem in mediation analysis. We propose two approaches to estimate indirect and direct effects with a mediator subject to an assay limit: (1) extrapolation and (2) numerical optimization and integration of the observed likelihood. Since these estimation methods solely rely on the so-called Mediation Formula, they apply to most approaches to causal mediation analysis: natural, separable, and organic indirect, and direct effects. A simulation study compares the two estimation approaches to imputing with half the assay limit. Using HIV interruption study data from the AIDS Clinical Trials Group described in Li et al 2016, AIDS; Lok and Bosch 2021, Epidemiology, we illustrate our methods by estimating the organic/pure indirect effect of a hypothetical HIV curative treatment on viral suppression mediated by two HIV persistence measures: cell-associated HIV-RNA and single-copy plasma HIV-RNA.},
  archive      = {J_SIM},
  author       = {Ariel Chernofsky and Ronald J. Bosch and Judith J. Lok},
  doi          = {10.1002/sim.10065},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2299-2313},
  shortjournal = {Stat. Med.},
  title        = {Causal mediation analysis with mediator values below an assay limit},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information-incorporated sparse hierarchical cancer
heterogeneity analysis. <em>SIM</em>, <em>43</em>(11), 2280–2297. (<a
href="https://doi.org/10.1002/sim.10071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer heterogeneity analysis is essential for precision medicine. Most of the existing heterogeneity analyses only consider a single type of data and ignore the possible sparsity of important features. In cancer clinical practice, it has been suggested that two types of data, pathological imaging and omics data, are commonly collected and can produce hierarchical heterogeneous structures, in which the refined sub-subgroup structure determined by omics features can be nested in the rough subgroup structure determined by the imaging features. Moreover, sparsity pursuit has extraordinary significance and is more challenging for heterogeneity analysis, because the important features may not be the same in different subgroups, which is ignored by the existing heterogeneity analyses. Fortunately, rich information from previous literature (for example, those deposited in PubMed) can be used to assist feature selection in the present study. Advancing from the existing analyses, in this study, we propose a novel sparse hierarchical heterogeneity analysis framework, which can integrate two types of features and incorporate prior knowledge to improve feature selection. The proposed approach has satisfactory statistical properties and competitive numerical performance. A TCGA real data analysis demonstrates the practical value of our approach in analyzing data heterogeneity and sparsity.},
  archive      = {J_SIM},
  author       = {Wei Han and Sanguo Zhang and Shuangge Ma and Mingyang Ren},
  doi          = {10.1002/sim.10071},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2280-2297},
  shortjournal = {Stat. Med.},
  title        = {Information-incorporated sparse hierarchical cancer heterogeneity analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative inference for treatment effect with
distributed data-sharing management in multicenter studies.
<em>SIM</em>, <em>43</em>(11), 2263–2279. (<a
href="https://doi.org/10.1002/sim.10068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sharing barriers present paramount challenges arising from multicenter clinical studies where multiple data sources are stored and managed in a distributed fashion at different local study sites. Merging such data sources into a common data storage for a centralized statistical analysis requires a data use agreement, which is often time-consuming. Data merging may become more burdensome when propensity score modeling is involved in the analysis because combining many confounding variables, and systematic incorporation of this additional modeling in a meta-analysis has not been thoroughly investigated in the literature. Motivated from a multicenter clinical trial of basal insulin treatment for reducing the risk of post-transplantation diabetes mellitus, we propose a new inference framework that avoids the merging of subject-level raw data from multiple sites at a centralized facility but needs only the sharing of summary statistics. Unlike the architecture of federated learning, the proposed collaborative inference does not need a center site to combine local results and thus enjoys maximal protection of data privacy and minimal sensitivity to unbalanced data distributions across data sources. We show theoretically and numerically that the new distributed inference approach has little loss of statistical power compared to the centralized method that requires merging the entire data. We present large-sample properties and algorithms for the proposed method. We illustrate its performance by simulation experiments and the motivating example on the differential average treatment effect of basal insulin to lower risk of diabetes among kidney-transplant patients compared to the standard-of-care.},
  archive      = {J_SIM},
  author       = {Mengtong Hu and Xu Shi and Peter X.-K. Song},
  doi          = {10.1002/sim.10068},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2263-2279},
  shortjournal = {Stat. Med.},
  title        = {Collaborative inference for treatment effect with distributed data-sharing management in multicenter studies},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How could a pooled testing policy have performed in managing
the early stages of the COVID-19 pandemic? Results from a simulation
study. <em>SIM</em>, <em>43</em>(11), 2239–2262. (<a
href="https://doi.org/10.1002/sim.10062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A coordinated testing policy is an essential tool for responding to emerging epidemics, as was seen with COVID-19. However, it is very difficult to agree on the best policy when there are multiple conflicting objectives. A key objective is minimizing cost, which is why pooled testing (a method that involves pooling samples taken from multiple individuals and analyzing this with a single diagnostic test) has been suggested. In this article, we present results from an extensive and realistic simulation study comparing testing policies based on individually testing subjects with symptoms (a policy resembling the UK strategy at the start of the COVID-19 pandemic), individually testing subjects at random or pools of subjects randomly combined and tested. To compare these testing methods, a dynamic model compromised of a relationship network and an extended SEIR model is used. In contrast to most existing literature, testing capacity is considered as fixed and limited rather than unbounded. This article then explores the impact of the proportion of symptomatic infections on the expected performance of testing policies. Symptomatic testing performs better than pooled testing unless a low proportion of infections are symptomatic. Additionally, we include the novel feature for testing of non-compliance and perform a sensitivity analysis for different compliance assumptions. Our results suggest for the pooled testing scheme to be superior to testing symptomatic people individually, only a small proportion of the population ( ) needs to not comply with the testing procedure.},
  archive      = {J_SIM},
  author       = {Bethany Heath and Sofía S. Villar and David S. Robertson},
  doi          = {10.1002/sim.10062},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2239-2262},
  shortjournal = {Stat. Med.},
  title        = {How could a pooled testing policy have performed in managing the early stages of the COVID-19 pandemic? results from a simulation study},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonparametric relative treatment effect for direct
comparisons of censored paired survival outcomes. <em>SIM</em>,
<em>43</em>(11), 2216–2238. (<a
href="https://doi.org/10.1002/sim.10063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A frequently addressed issue in clinical trials is the comparison of censored paired survival outcomes, for example, when individuals were matched based on their characteristics prior to the analysis. In this regard, a proper incorporation of the dependence structure of the paired censored outcomes is required and, up to now, appropriate methods are only rarely available in the literature. Moreover, existing methods are not motivated by the strive for insights by means of an easy-to-interpret parameter. Hence, we seek to develop a new estimand-driven method to compare the effectiveness of two treatments in the context of right-censored survival data with matched pairs. With the help of competing risks techniques, the so-called relative treatment effect is estimated. This estimand describes the probability that individuals under Treatment 1 have a longer lifetime than comparable individuals under Treatment 2. We derive hypothesis tests and confidence intervals based on a studentized version of the estimator, where resampling-based inference is established by means of a randomization method. In a simulation study, we demonstrate for numerous sample sizes and different amounts of censoring that the developed test exhibits a good power. Finally, we apply the methodology to a well-known benchmark data set from a trial with patients suffering from diabetic retinopathy.},
  archive      = {J_SIM},
  author       = {Dennis Dobler and Kathrin Möllenhoff},
  doi          = {10.1002/sim.10063},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2216-2238},
  shortjournal = {Stat. Med.},
  title        = {A nonparametric relative treatment effect for direct comparisons of censored paired survival outcomes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample size adaptation designs and efficiency comparison
with group sequential designs. <em>SIM</em>, <em>43</em>(11), 2203–2215.
(<a href="https://doi.org/10.1002/sim.10066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is to give a systematic account of sample size adaptation designs (SSADs) and to provide direct proof of the efficiency advantage of general SSADs over group sequential designs (GSDs) from a different perspective. For this purpose, a class of sample size mapping functions to define SSADs is introduced. Under the two-stage adaptive clinical trial setting, theorems are developed to describe the properties of SSADs. Sufficient conditions are derived and used to prove analytically that SSADs based on the weighted combination test can be uniformly more efficient than GSDs in a range of likely values of the true treatment difference δ $$ \delta $$ . As shown in various scenarios, given a GSD, a fully adaptive SSAD can be obtained that has sufficient statistical power similar to that of the GSD but has a smaller average sample size for all in the range. The associated sample size savings can be substantial. A practical design example and suggestions on the steps to find efficient SSADs are also provided.},
  archive      = {J_SIM},
  author       = {Lu Cui},
  doi          = {10.1002/sim.10066},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2203-2215},
  shortjournal = {Stat. Med.},
  title        = {Sample size adaptation designs and efficiency comparison with group sequential designs},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating generalized propensity scores with survey and
attrition weighted data. <em>SIM</em>, <em>43</em>(11), 2183–2202. (<a
href="https://doi.org/10.1002/sim.10039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior work in causal inference has shown that using survey sampling weights in the propensity score estimation stage and the outcome model stage for binary treatments can result in a more robust estimator of the effect of the binary treatment being analyzed. However, to date, extending this work to continuous treatments and exposures has not been explored nor has consideration been given for how to handle attrition weights in the propensity score model. Nonetheless, generalized propensity score (GPS) analyses are being used for estimating continuous treatment effects on outcomes when researchers have observational data, and those data sets often have survey or attrition weights that need to be accounted for in the analysis. Here, we extend prior work and show with analytic results that using survey sampling or attrition weights in the GPS estimation stage and the outcome model stage for continuous treatments can result in a more robust estimator than one that does not. Simulation study results show that, although using weights in both estimation stages is sufficient for robust estimation, it is not necessary and unbiased estimation is possible in some cases under various approaches to using weights in estimation. Analysts do not know if the conditions of our simulation studies hold, so use of weights in both estimation stages might provide insurance for reducing potential bias. We discuss the implications of our results in the context of an empirical example.},
  archive      = {J_SIM},
  author       = {Daniel F. McCaffrey and Beth Ann Griffin and Michael Robbins and Yajnaseni Chakraborti and Donna L. Coffman and Brian Vegetabile},
  doi          = {10.1002/sim.10039},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2183-2202},
  shortjournal = {Stat. Med.},
  title        = {Estimating generalized propensity scores with survey and attrition weighted data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-agnostic explanations for survival prediction models.
<em>SIM</em>, <em>43</em>(11), 2161–2182. (<a
href="https://doi.org/10.1002/sim.10057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced machine learning methods capable of capturing complex and nonlinear relationships can be used in biomedical research to accurately predict time-to-event outcomes. However, these methods have been criticized as “black boxes” that are not interpretable and thus are difficult to trust in making important clinical decisions. Explainable machine learning proposes the use of model-agnostic explainers that can be applied to predictions from any complex model. These explainers describe how a patient&#39;s characteristics are contributing to their prediction, and thus provide insight into how the model is arriving at that prediction. The specific application of these explainers to survival prediction models can be used to obtain explanations for (i) survival predictions at particular follow-up times, and (ii) a patient&#39;s overall predicted survival curve. Here, we present a model-agnostic approach for obtaining these explanations from any survival prediction model. We extend the local interpretable model-agnostic explainer framework for classification outcomes to survival prediction models. Using simulated data, we assess the performance of the proposed approaches under various settings. We illustrate application of the new methodology using prostate cancer data.},
  archive      = {J_SIM},
  author       = {Krithika Suresh and Carsten Görg and Debashis Ghosh},
  doi          = {10.1002/sim.10057},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2161-2182},
  shortjournal = {Stat. Med.},
  title        = {Model-agnostic explanations for survival prediction models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monitoring epidemic processes under political measures.
<em>SIM</em>, <em>43</em>(11), 2122–2160. (<a
href="https://doi.org/10.1002/sim.10042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical modeling of epidemiological curves to capture the course of epidemic processes and to implement a signaling system for detecting significant changes in the process is a challenging task, especially when the process is affected by political measures. As previous monitoring approaches are subject to various problems, we develop a practical and flexible tool that is well suited for monitoring epidemic processes under political measures. This tool enables monitoring across different epochs using a single statistical model that constantly adapts to the underlying process, and therefore allows both retrospective and on-line monitoring of epidemic processes. It is able to detect essential shifts and to identify anomaly conditions in the epidemic process, and it provides decision-makers a reliable method for rapidly learning from trends in the epidemiological curves. Moreover, it is a tool to evaluate the effectivity of political measures and to detect the transition from pandemic to endemic. This research is based on a comprehensive COVID-19 study on infection rates under political measures in line with the reporting of the Robert Koch Institute covering the entire period of the pandemic in Germany.},
  archive      = {J_SIM},
  author       = {Nataliya Chukhrova and Oskar Plate and Arne Johannssen},
  doi          = {10.1002/sim.10042},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2122-2160},
  shortjournal = {Stat. Med.},
  title        = {Monitoring epidemic processes under political measures},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting the multivariate zero-inflated counts: A novel
model averaging method under pearson loss. <em>SIM</em>,
<em>43</em>(11), 2096–2121. (<a
href="https://doi.org/10.1002/sim.10052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Excessive zeros in multivariate count data are often observed in scenarios of biomedicine and public health. To provide a better analysis on this type of data, we first develop a marginalized multivariate zero-inflated Poisson (MZIP) regression model to directly interpret the overall exposure effects on marginal means. Then, we define a multiple Pearson residual for our newly developed MZIP regression model by simultaneously taking heterogeneity and correlation into consideration. Furthermore, a new model averaging prediction method is introduced based on the multiple Pearson residual, and the asymptotical optimality of this model averaging prediction is proved. Simulations and two empirical applications in medicine are used to illustrate the effectiveness of the proposed method.},
  archive      = {J_SIM},
  author       = {Yin Liu and Ziwen Gao},
  doi          = {10.1002/sim.10052},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2096-2121},
  shortjournal = {Stat. Med.},
  title        = {Predicting the multivariate zero-inflated counts: A novel model averaging method under pearson loss},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Categorisation of continuous covariates for stratified
randomisation: How should we adjust? <em>SIM</em>, <em>43</em>(11),
2083–2095. (<a href="https://doi.org/10.1002/sim.10060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To obtain valid inference following stratified randomisation, treatment effects should be estimated with adjustment for stratification variables. Stratification sometimes requires categorisation of a continuous prognostic variable (eg, age), which raises the question: should adjustment be based on randomisation categories or underlying continuous values? In practice, adjustment for randomisation categories is more common. We reviewed trials published in general medical journals and found none of the 32 trials that stratified randomisation based on a continuous variable adjusted for continuous values in the primary analysis. Using data simulation, this article evaluates the performance of different adjustment strategies for continuous and binary outcomes where the covariate-outcome relationship (via the link function) was either linear or non-linear. Given the utility of covariate adjustment for addressing missing data, we also considered settings with complete or missing outcome data. Analysis methods included linear or logistic regression with no adjustment for the stratification variable, adjustment for randomisation categories, or adjustment for continuous values assuming a linear covariate-outcome relationship or allowing for non-linearity using fractional polynomials or restricted cubic splines. Unadjusted analysis performed poorly throughout. Adjustment approaches that misspecified the underlying covariate-outcome relationship were less powerful and, alarmingly, biased in settings where the stratification variable predicted missing outcome data. Adjustment for randomisation categories tends to involve the highest degree of misspecification, and so should be avoided in practice. To guard against misspecification, we recommend use of flexible approaches such as fractional polynomials and restricted cubic splines when adjusting for continuous stratification variables in randomised trials.},
  archive      = {J_SIM},
  author       = {Thomas R. Sullivan and Tim P. Morris and Brennan C. Kahan and Alana R. Cuthbert and Lisa N. Yelland},
  doi          = {10.1002/sim.10060},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2083-2095},
  shortjournal = {Stat. Med.},
  title        = {Categorisation of continuous covariates for stratified randomisation: How should we adjust?},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new and unified method for regression analysis of
interval-censored failure time data under semiparametric transformation
models with missing covariates. <em>SIM</em>, <em>43</em>(11),
2062–2082. (<a href="https://doi.org/10.1002/sim.10035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses regression analysis of interval-censored failure time data arising from semiparametric transformation models in the presence of missing covariates. Although some methods have been developed for the problem, they either apply only to limited situations or may have some computational issues. Corresponding to these, we propose a new and unified two-step inference procedure that can be easily implemented using the existing or standard software. The proposed method makes use of a set of working models to extract partial information from incomplete observations and yields a consistent estimator of regression parameters assuming missing at random. An extensive simulation study is conducted and indicates that it performs well in practical situations. Finally, we apply the proposed approach to an Alzheimer&#39;s Disease study that motivated this study.},
  archive      = {J_SIM},
  author       = {Yichen Lou and Yuqing Ma and Mingyue Du},
  doi          = {10.1002/sim.10035},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2062-2082},
  shortjournal = {Stat. Med.},
  title        = {A new and unified method for regression analysis of interval-censored failure time data under semiparametric transformation models with missing covariates},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do machine learning methods lead to similar individualized
treatment rules? A comparison study on real data. <em>SIM</em>,
<em>43</em>(11), 2043–2061. (<a
href="https://doi.org/10.1002/sim.10059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying patients who benefit from a treatment is a key aspect of personalized medicine, which allows the development of individualized treatment rules (ITRs). Many machine learning methods have been proposed to create such rules. However, to what extent the methods lead to similar ITRs, that is, recommending the same treatment for the same individuals is unclear. In this work, we compared 22 of the most common approaches in two randomized control trials. Two classes of methods can be distinguished. The first class of methods relies on predicting individualized treatment effects from which an ITR is derived by recommending the treatment evaluated to the individuals with a predicted benefit. In the second class, methods directly estimate the ITR without estimating individualized treatment effects. For each trial, the performance of ITRs was assessed by various metrics, and the pairwise agreement between all ITRs was also calculated. Results showed that the ITRs obtained via the different methods generally had considerable disagreements regarding the patients to be treated. A better concordance was found among akin methods. Overall, when evaluating the performance of ITRs in a validation sample, all methods produced ITRs with limited performance, suggesting a high potential for optimism. For non-parametric methods, this optimism was likely due to overfitting. The different methods do not lead to similar ITRs and are therefore not interchangeable. The choice of the method strongly influences for which patients a certain treatment is recommended, drawing some concerns about their practical use.},
  archive      = {J_SIM},
  author       = {Florie Bouvier and Etienne Peyrot and Alan Balendran and Corentin Ségalas and Ian Roberts and François Petit and Raphaël Porcher},
  doi          = {10.1002/sim.10059},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2043-2061},
  shortjournal = {Stat. Med.},
  title        = {Do machine learning methods lead to similar individualized treatment rules? a comparison study on real data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Penalized weighted smoothed quantile regression for
high-dimensional longitudinal data. <em>SIM</em>, <em>43</em>(10),
2007–2042. (<a href="https://doi.org/10.1002/sim.10056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression, known as a robust alternative to linear regression, has been widely used in statistical modeling and inference. In this paper, we propose a penalized weighted convolution-type smoothed method for variable selection and robust parameter estimation of the quantile regression with high dimensional longitudinal data. The proposed method utilizes a twice-differentiable and smoothed loss function instead of the check function in quantile regression without penalty, and can select the important covariates consistently using the efficient gradient-based iterative algorithms when the dimension of covariates is larger than the sample size. Moreover, the proposed method can circumvent the influence of outliers in the response variable and/or the covariates. To incorporate the correlation within each subject and enhance the accuracy of the parameter estimation, a two-step weighted estimation method is also established. Furthermore, we prove the oracle properties of the proposed method under some regularity conditions. Finally, the performance of the proposed method is demonstrated by simulation studies and two real examples.},
  archive      = {J_SIM},
  author       = {Yanan Song and Haohui Han and Liya Fu and Ting Wang},
  doi          = {10.1002/sim.10056},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2007-2042},
  shortjournal = {Stat. Med.},
  title        = {Penalized weighted smoothed quantile regression for high-dimensional longitudinal data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A class of computational methods to reduce selection bias
when designing phase 3 clinical trials. <em>SIM</em>, <em>43</em>(10),
1993–2006. (<a href="https://doi.org/10.1002/sim.10041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When designing confirmatory Phase 3 studies, one usually evaluates one or more efficacious and safe treatment option(s) based on data from previous studies. However, several retrospective research articles reported the phenomenon of “diminished treatment effect in Phase 3” based on many case studies. Even under basic assumptions, it was shown that the commonly used estimator could substantially overestimate the efficacy of selected group(s). As alternatives, we propose a class of computational methods to reduce estimation bias and mean squared error with a broader scope of multiple treatment groups and flexibility to accommodate summary results by group as input. Based on simulation studies and a real data example, we provide practical implementation guidance for this class of methods under different scenarios. For more complicated problems, our framework can serve as a starting point with additional layers built in. Proposed methods can also be widely applied to other selection problems.},
  archive      = {J_SIM},
  author       = {Tianyu Zhan},
  doi          = {10.1002/sim.10041},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1993-2006},
  shortjournal = {Stat. Med.},
  title        = {A class of computational methods to reduce selection bias when designing phase 3 clinical trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the distribution of the power function for the scale
parameter of exponential families. <em>SIM</em>, <em>43</em>(10),
1973–1992. (<a href="https://doi.org/10.1002/sim.10043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expected value of the standard power function of a test, computed with respect to a design prior distribution, is often used to evaluate the probability of success of an experiment. However, looking only at the expected value might be reductive. Instead, the whole probability distribution of the power function induced by the design prior can be exploited. In this article we consider one-sided testing for the scale parameter of exponential families and we derive general unifying expressions for cumulative distribution and density functions of the random power. Sample size determination criteria based on alternative summaries of these functions are discussed. The study sheds light on the relevance of the choice of the design prior in order to construct a successful experiment.},
  archive      = {J_SIM},
  author       = {Fulvio De Santis and Stefania Gubbiotti},
  doi          = {10.1002/sim.10043},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1973-1992},
  shortjournal = {Stat. Med.},
  title        = {On the distribution of the power function for the scale parameter of exponential families},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Estimation and reduction of bias in self-controlled case
series with non-rare event dependent outcomes and heterogeneous
populations. <em>SIM</em>, <em>43</em>(10), 1955–1972. (<a
href="https://doi.org/10.1002/sim.10033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The self-controlled case series (SCCS) is a commonly adopted study design in the assessment of vaccine and drug safety. Recurrent event data collected from SCCS studies are typically analyzed using the conditional Poisson model which assumes event times are independent within-cases. This assumption is violated in the presence of event dependence, where the occurrence of an event influences the probability and timing of subsequent events. When event dependence is suspected in an SCCS study, the standard recommendation is to include only the first event from each case in the analysis. However, first event analysis can still yield biased estimates of the exposure relative incidence if the outcome event is not rare. We first demonstrate that the bias in first event analysis can be even higher than previously assumed when subpopulations with different baseline incidence rates are present and describe an improved method for estimating this bias. Subsequently, we propose a novel partitioned analysis method and demonstrate how it can reduce this bias. We provide a recommendation to guide the number of partitions to use with the partitioned analysis, illustrate this recommendation with an example SCCS study of the association between beta-blockers and acute myocardial infarction, and compare the partitioned analysis against other SCCS analysis methods by simulation.},
  archive      = {J_SIM},
  author       = {Kenneth Menglin Lee and Yin Bun Cheung},
  doi          = {10.1002/sim.10033},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1955-1972},
  shortjournal = {Stat. Med.},
  title        = {Estimation and reduction of bias in self-controlled case series with non-rare event dependent outcomes and heterogeneous populations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference under superspreading: Determinants of SARS-CoV-2
transmission in germany. <em>SIM</em>, <em>43</em>(10), 1933–1954. (<a
href="https://doi.org/10.1002/sim.10046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superspreading, under-reporting, reporting delay, and confounding complicate statistical inference on determinants of disease transmission. A model that accounts for these factors within a Bayesian framework is estimated using German Covid-19 surveillance data. Compartments based on date of symptom onset, location, and age group allow to identify age-specific changes in transmission, adjusting for weather, reported prevalence, and testing and tracing. Several factors were associated with a reduction in transmission: public awareness rising, information on local prevalence, testing and tracing, high temperature, stay-at-home orders, and restaurant closures. However, substantial uncertainty remains for other interventions including school closures and mandatory face coverings. The challenge of disentangling the effects of different determinants is discussed and examined through a simulation study. On a broader perspective, the study illustrates the potential of surveillance data with demographic information and date of symptom onset to improve inference in the presence of under-reporting and reporting delay.},
  archive      = {J_SIM},
  author       = {Patrick W. Schmidt},
  doi          = {10.1002/sim.10046},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1933-1954},
  shortjournal = {Stat. Med.},
  title        = {Inference under superspreading: Determinants of SARS-CoV-2 transmission in germany},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Use of win time for ordered composite endpoints in clinical
trials. <em>SIM</em>, <em>43</em>(10), 1920–1932. (<a
href="https://doi.org/10.1002/sim.10045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the choice of outcome for overall treatment benefit in a clinical trial which measures the first time to each of several clinical events. We describe several new variants of the win ratio that incorporate the time spent in each clinical state over the common follow-up, where clinical state means the worst clinical event that has occurred by that time. One version allows restriction so that death during follow-up is most important, while time spent in other clinical states is still accounted for. Three other variants are described; one is based on the average pairwise win time, one creates a continuous outcome for each participant based on expected win times against a reference distribution and another that uses the estimated distributions of clinical state to compare the treatment arms. Finally, a combination testing approach is described to give robust power for detecting treatment benefit across a broad range of alternatives. These new methods are designed to be closer to the overall treatment benefit/harm from a patient&#39;s perspective, compared to the ordinary win ratio. The new methods are compared to the composite event approach and the ordinary win ratio. Simulations show that when overall treatment benefit on death is substantial, the variants based on either the participants&#39; expected win times (EWTs) against a reference distribution or estimated clinical state distributions have substantially higher power than either the pairwise comparison or composite event methods. The methods are illustrated by re-analysis of the trial heart failure: a controlled trial investigating outcomes of exercise training.},
  archive      = {J_SIM},
  author       = {James F. Troendle and Eric S. Leifer and Song Yang and Neal Jeffries and Dong-Yun Kim and Jungnam Joo and Christopher M. O&#39;Connor},
  doi          = {10.1002/sim.10045},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1920-1932},
  shortjournal = {Stat. Med.},
  title        = {Use of win time for ordered composite endpoints in clinical trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian nonparametric meta-analysis model for estimating
the reference interval. <em>SIM</em>, <em>43</em>(10), 1905–1919. (<a
href="https://doi.org/10.1002/sim.10001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reference interval represents the normative range for measurements from a healthy population. It plays an important role in laboratory testing, as well as in differentiating healthy from diseased patients. The reference interval based on a single study might not be applicable to a broader population. Meta-analysis can provide a more generalizable reference interval based on the combined population by synthesizing results from multiple studies. However, the assumptions of normally distributed underlying study-specific means and equal within-study variances, which are commonly used in existing methods, are strong and may not hold in practice. We propose a Bayesian nonparametric model with more flexible assumptions to extend random effects meta-analysis for estimating reference intervals. We illustrate through simulation studies and two real data examples the performance of our proposed approach when the assumptions of normally distributed study means and equal within-study variances do not hold.},
  archive      = {J_SIM},
  author       = {Wenhao Cao and Haitao Chu and Timothy Hanson and Lianne Siegel},
  doi          = {10.1002/sim.10001},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1905-1919},
  shortjournal = {Stat. Med.},
  title        = {A bayesian nonparametric meta-analysis model for estimating the reference interval},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage stratified designs with survival outcomes and
adjustment for misclassification in predictive biomarkers. <em>SIM</em>,
<em>43</em>(10), 1883–1904. (<a
href="https://doi.org/10.1002/sim.10048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomarker stratified clinical trial designs are versatile tools to assess biomarker clinical utility and address its relationship with clinical endpoints. Due to imperfect assays and/or classification rules, biomarker status is prone to errors. To account for biomarker misclassification, we consider a two-stage stratified design for survival outcomes with an adjustment for misclassification in predictive biomarkers. Compared to continuous and/or binary outcomes, the test statistics for survival outcomes with an adjustment for biomarker misclassification is much more complicated and needs to take special care. We propose to use the information from the observed biomarker status strata to construct adjusted log-rank statistics for true biomarker status strata. These adjusted log-rank statistics are then used to develop sequential tests for the global (composite) hypothesis and component-wise hypothesis. We discuss the power analysis with the control of the type-I error rate by using the correlations between the adjusted log-rank statistics within and between the design stages. Our method is illustrated with examples of the recent successful development of immunotherapy in nonsmall-cell lung cancer.},
  archive      = {J_SIM},
  author       = {Yanping Chen and Yong Lin and Shou-En Lu and Weichung J. Shih and Hui Quan},
  doi          = {10.1002/sim.10048},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1883-1904},
  shortjournal = {Stat. Med.},
  title        = {Two-stage stratified designs with survival outcomes and adjustment for misclassification in predictive biomarkers},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting changes in the transmission rate of a stochastic
epidemic model. <em>SIM</em>, <em>43</em>(10), 1867–1882. (<a
href="https://doi.org/10.1002/sim.10050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Throughout the course of an epidemic, the rate at which disease spreads varies with behavioral changes, the emergence of new disease variants, and the introduction of mitigation policies. Estimating such changes in transmission rates can help us better model and predict the dynamics of an epidemic, and provide insight into the efficacy of control and intervention strategies. We present a method for likelihood-based estimation of parameters in the stochastic susceptible-infected-removed model under a time-inhomogeneous transmission rate comprised of piecewise constant components. In doing so, our method simultaneously learns change points in the transmission rate via a Markov chain Monte Carlo algorithm. The method targets the exact model posterior in a difficult missing data setting given only partially observed case counts over time. We validate performance on simulated data before applying our approach to data from an Ebola outbreak in Western Africa and COVID-19 outbreak on a university campus.},
  archive      = {J_SIM},
  author       = {Jenny Huang and Raphaël Morsomme and David Dunson and Jason Xu},
  doi          = {10.1002/sim.10050},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1867-1882},
  shortjournal = {Stat. Med.},
  title        = {Detecting changes in the transmission rate of a stochastic epidemic model},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RMST-based multiple contrast tests in general factorial
designs. <em>SIM</em>, <em>43</em>(10), 1849–1866. (<a
href="https://doi.org/10.1002/sim.10017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several methods in survival analysis are based on the proportional hazards assumption. However, this assumption is very restrictive and often not justifiable in practice. Therefore, effect estimands that do not rely on the proportional hazards assumption are highly desirable in practical applications. One popular example for this is the restricted mean survival time (RMST). It is defined as the area under the survival curve up to a prespecified time point and, thus, summarizes the survival curve into a meaningful estimand. For two-sample comparisons based on the RMST, previous research found the inflation of the type I error of the asymptotic test for small samples and, therefore, a two-sample permutation test has already been developed. The first goal of the present paper is to further extend the permutation test for general factorial designs and general contrast hypotheses by considering a Wald-type test statistic and its asymptotic behavior. Additionally, a groupwise bootstrap approach is considered. Moreover, when a global test detects a significant difference by comparing the RMSTs of more than two groups, it is of interest which specific RMST differences cause the result. However, global tests do not provide this information. Therefore, multiple tests for the RMST are developed in a second step to infer several null hypotheses simultaneously. Hereby, the asymptotically exact dependence structure between the local test statistics is incorporated to gain more power. Finally, the small sample performance of the proposed global and multiple testing procedures is analyzed in simulations and illustrated in a real data example.},
  archive      = {J_SIM},
  author       = {Merle Munko and Marc Ditzhaus and Dennis Dobler and Jon Genuneit},
  doi          = {10.1002/sim.10017},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1849-1866},
  shortjournal = {Stat. Med.},
  title        = {RMST-based multiple contrast tests in general factorial designs},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter estimation and forecasting with quantified
uncertainty for ordinary differential equation models using
QuantDiffForecast: A MATLAB toolbox and tutorial. <em>SIM</em>,
<em>43</em>(9), 1826–1848. (<a
href="https://doi.org/10.1002/sim.10036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematical models based on systems of ordinary differential equations (ODEs) are frequently applied in various scientific fields to assess hypotheses, estimate key model parameters, and generate predictions about the system&#39;s state. To support their application, we present a comprehensive, easy-to-use, and flexible MATLAB toolbox, QuantDiffForecast , and associated tutorial to estimate parameters and generate short-term forecasts with quantified uncertainty from dynamical models based on systems of ODEs. We provide software ( https://github.com/gchowell/paramEstimation_forecasting_ODEmodels/ ) and detailed guidance on estimating parameters and forecasting time-series trajectories that are characterized using ODEs with quantified uncertainty through a parametric bootstrapping approach. It includes functions that allow the user to infer model parameters and assess forecasting performance for different ODE models specified by the user, using different estimation methods and error structures in the data. The tutorial is intended for a diverse audience, including students training in dynamic systems, and will be broadly applicable to estimate parameters and generate forecasts from models based on ODEs. The functions included in the toolbox are illustrated using epidemic models with varying levels of complexity applied to data from the 1918 influenza pandemic in San Francisco. A tutorial video that demonstrates the functionality of the toolbox is included.},
  archive      = {J_SIM},
  author       = {Gerardo Chowell and Amanda Bleichrodt and Ruiyan Luo},
  doi          = {10.1002/sim.10036},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1826-1848},
  shortjournal = {Stat. Med.},
  title        = {Parameter estimation and forecasting with quantified uncertainty for ordinary differential equation models using QuantDiffForecast: A MATLAB toolbox and tutorial},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical plasmode simulations–potentials, challenges and
recommendations. <em>SIM</em>, <em>43</em>(9), 1804–1825. (<a
href="https://doi.org/10.1002/sim.10012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical data simulation is essential in the development of statistical models and methods as well as in their performance evaluation. To capture complex data structures, in particular for high-dimensional data, a variety of simulation approaches have been introduced including parametric and the so-called plasmode simulations. While there are concerns about the realism of parametrically simulated data, it is widely claimed that plasmodes come very close to reality with some aspects of the “truth” known. However, there are no explicit guidelines or state-of-the-art on how to perform plasmode data simulations. In the present paper, we first review existing literature and introduce the concept of statistical plasmode simulation. We then discuss advantages and challenges of statistical plasmodes and provide a step-wise procedure for their generation, including key steps to their implementation and reporting. Finally, we illustrate the concept of statistical plasmodes as well as the proposed plasmode generation procedure by means of a public real RNA data set on breast carcinoma patients.},
  archive      = {J_SIM},
  author       = {Nicholas Schreck and Alla Slynko and Maral Saadati and Axel Benner},
  doi          = {10.1002/sim.10012},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1804-1825},
  shortjournal = {Stat. Med.},
  title        = {Statistical plasmode simulations–Potentials, challenges and recommendations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust best linear weighted estimator with missing
covariates in survival analysis. <em>SIM</em>, <em>43</em>(9),
1790–1803. (<a href="https://doi.org/10.1002/sim.10044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data in covariates can result in biased estimates and loss of power to detect associations. We consider Cox regression in which some covariates are subject to missing. The inverse probability weighted approach is often applied to regression analysis with missing covariates. Inverse probability weighted estimators typically are less efficient than likelihood-based estimators, but in general are more robust against model misspecification. In this article, we propose a robust best linear weighted estimator for Cox regression with missing covariates. Our proposed estimator is the projection of the simple inverse probability weighted estimator onto the orthogonal complement of the score space based on a working regression model of the observed data. The efficiency gain is from the use of the association between the survival outcome variable and the available covariates, which is the working regression model. The asymptotic distribution is derived, and the finite sample performance of the proposed estimator is examined via extensive simulation studies. The methods are applied to a colorectal cancer study to assess the association of the microsatellite instability status with colorectal cancer-specific mortality.},
  archive      = {J_SIM},
  author       = {Ching-Yun Wang and Li Hsu and Tabitha Harrison},
  doi          = {10.1002/sim.10044},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1790-1803},
  shortjournal = {Stat. Med.},
  title        = {Robust best linear weighted estimator with missing covariates in survival analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal ensemble construction for multistudy prediction with
applications to mortality estimation. <em>SIM</em>, <em>43</em>(9),
1774–1789. (<a href="https://doi.org/10.1002/sim.10006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is increasingly common to encounter prediction tasks in the biomedical sciences for which multiple datasets are available for model training. Common approaches such as pooling datasets before model fitting can produce poor out-of-study prediction performance when datasets are heterogeneous. Theoretical and applied work has shown multistudy ensembling to be a viable alternative that leverages the variability across datasets in a manner that promotes model generalizability. Multistudy ensembling uses a two-stage stacking strategy which fits study-specific models and estimates ensemble weights separately. This approach ignores, however, the ensemble properties at the model-fitting stage, potentially resulting in performance losses. Motivated by challenges in the estimation of COVID-attributable mortality, we propose optimal ensemble construction , an approach to multistudy stacking whereby we jointly estimate ensemble weights and parameters associated with study-specific models. We prove that limiting cases of our approach yield existing methods such as multistudy stacking and pooling datasets before model fitting. We propose an efficient block coordinate descent algorithm to optimize the loss function. We use our method to perform multicountry COVID-19 baseline mortality prediction. We show that when little data is available for a country before the onset of the pandemic, leveraging data from other countries can substantially improve prediction accuracy. We further compare and characterize the method&#39;s performance in data-driven simulations and other numerical experiments. Our method remains competitive with or outperforms multistudy stacking and other earlier methods in the COVID-19 data application and in a range of simulation settings.},
  archive      = {J_SIM},
  author       = {Gabriel Loewinger and Rolando Acosta Nunez and Rahul Mazumder and Giovanni Parmigiani},
  doi          = {10.1002/sim.10006},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1774-1789},
  shortjournal = {Stat. Med.},
  title        = {Optimal ensemble construction for multistudy prediction with applications to mortality estimation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of trajectory of protective efficacy in
infectious disease prevention trials using recurrent event times.
<em>SIM</em>, <em>43</em>(9), 1759–1773. (<a
href="https://doi.org/10.1002/sim.10049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies of infectious disease prevention, the level of protective efficacy of medicinal products such as vaccines and prophylactic drugs tends to vary over time. Many products require administration of multiple doses at scheduled times, as opposed to one-off or continual intervention. Accurate information on the trajectory of the level of protective efficacy over time facilitates informed clinical recommendations and implementation strategies, for example, with respect to the timing of administration of the doses. Based on concepts from pharmacokinetic and pharmacodynamic modeling, we propose a non-linear function for modeling the trajectory after each dose. The cumulative effect of multiple doses of the products is captured by an additive series of the function. The model has the advantages of parsimony and interpretability, while remaining flexible in capturing features of the trajectories. We incorporate this series into the Andersen-Gill model for analysis of recurrent event time data and compare it with alternative parametric and non-parametric functions. We use data on clinical malaria disease episodes from a trial of four doses of an anti-malarial drug combination for chemoprevention to illustrate, and evaluate the performance of the methods using simulation. The proposed method out-performed the alternatives in the analysis of real data in terms of Akaike and Bayesian Information Criterion. It also accurately captured the features of the protective efficacy trajectory such as the area under curve in simulations. The proposed method has strong potential to enhance the evaluation of disease prevention measures and improve their implementation strategies.},
  archive      = {J_SIM},
  author       = {Yin Bun Cheung and Xiangmei Ma and K. F. Lam and Chee Fu Yung and Paul Milligan},
  doi          = {10.1002/sim.10049},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1759-1773},
  shortjournal = {Stat. Med.},
  title        = {Estimation of trajectory of protective efficacy in infectious disease prevention trials using recurrent event times},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doubly adaptive biased coin design to improve bayesian
clinical trials with time-to-event endpoints. <em>SIM</em>,
<em>43</em>(9), 1743–1758. (<a
href="https://doi.org/10.1002/sim.10047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trialists often face the challenge of balancing scientific questions with other design features, such as improving efficiency, minimizing exposure to inferior treatments, and simultaneously comparing multiple treatments. While Bayesian response adaptive randomization (RAR) is a popular and effective method for achieving these objectives, it is known to have large variability and a lack of explicit theoretical results, making its use in clinical trials a subject of concern. It is desirable to propose a design that targets the same allocation proportion as Bayesian RAR and achieves the above objectives but addresses the concerns over Bayesian RAR. We propose the frequentist doubly adaptive biased coin designs (DBCD) targeting ethical allocation proportions from the Bayesian framework to satisfy different objectives in clinical trials with time-to-event endpoints. We derive the theoretical properties of the proposed adaptive randomization design and show through comprehensive numerical simulations that it can achieve ethical objectives without sacrificing efficiency. Our combined theoretical and numerical results offer a strong foundation for the practical use of RAR in real clinical trials.},
  archive      = {J_SIM},
  author       = {Wenhao Cao and Hongjian Zhu and Li Wang and Lixin Zhang and Jun Yu},
  doi          = {10.1002/sim.10047},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1743-1758},
  shortjournal = {Stat. Med.},
  title        = {Doubly adaptive biased coin design to improve bayesian clinical trials with time-to-event endpoints},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Martingale-residual-based greedy model averaging for
high-dimensional current status data. <em>SIM</em>, <em>43</em>(9),
1726–1742. (<a href="https://doi.org/10.1002/sim.10037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current status data are a type of failure time data that arise when the failure time of study subject cannot be determined precisely but is known only to occur before or after a random monitoring time. Variable selection methods for the failure time data have been discussed extensively in the literature. However, the statistical inference of the model selected based on the variable selection method ignores the uncertainty caused by model selection. To enhance the prediction accuracy for risk quantities such as survival probability, we propose two optimal model averaging methods under semiparametric additive hazards models. Specifically, based on martingale residuals processes, a delete-one cross-validation (CV) process is defined, and two new CV functional criteria are derived for choosing model weights. Furthermore, we present a greedy algorithm for the implementation of the techniques, and the asymptotic optimality of the proposed model averaging approaches is established, along with the convergence of the greedy averaging algorithms. A series of simulation experiments demonstrate the effectiveness and superiority of the proposed methods. Finally, a real-data example is provided as an illustration.},
  archive      = {J_SIM},
  author       = {Chang Wang and Mingyue Du},
  doi          = {10.1002/sim.10037},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1726-1742},
  shortjournal = {Stat. Med.},
  title        = {Martingale-residual-based greedy model averaging for high-dimensional current status data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handling missing disease information due to death in
diseases that need two visits to diagnose. <em>SIM</em>, <em>43</em>(9),
1708–1725. (<a href="https://doi.org/10.1002/sim.10038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies that assess disease status periodically, time of disease onset is interval censored between visits. Participants who die between two visits may have unknown disease status after their last visit. In this work, we consider an additional scenario where diagnosis requires two consecutive positive tests, such that disease status can also be unknown at the last visit preceding death. We show that this impacts the choice of censoring time for those who die without an observed disease diagnosis. We investigate two classes of models that quantify the effect of risk factors on disease outcome: a Cox proportional hazards model with death as a competing risk and an illness death model that treats disease as a possible intermediate state. We also consider four censoring strategies: participants without observed disease are censored at death (Cox model only), the last visit, the last visit with a negative test, or the second last visit. We evaluate the performance of model and censoring strategy combinations on simulated data with a binary risk factor and illustrate with a real data application. We find that the illness death model with censoring at the second last visit shows the best performance in all simulation settings. Other combinations show bias that varies in magnitude and direction depending on the differential mortality between diseased and disease-free subjects, the gap between visits, and the choice of the censoring time.},
  archive      = {J_SIM},
  author       = {Le Thi Phuong Thao and Rory Wolfe and Stephane Heritier and Ronald Geskus},
  doi          = {10.1002/sim.10038},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1708-1725},
  shortjournal = {Stat. Med.},
  title        = {Handling missing disease information due to death in diseases that need two visits to diagnose},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are the tests overpowered or underpowered? A unified
solution to correctly specify type i errors in design of clinical trials
for two sample proportions. <em>SIM</em>, <em>43</em>(9), 1688–1707. (<a
href="https://doi.org/10.1002/sim.10005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most commonly used data types, methods in testing or designing a trial for binary endpoints from two independent populations are still being developed until recently. However, the power and the minimum required sample size comparisons between different tests may not be valid if their type I errors are not controlled at the same level. In this article, we unify all related testing procedures into a decision framework, including both frequentist and Bayesian methods. Sufficient conditions of the type I error attained at the boundary of hypotheses are derived, which help reduce the magnitude of the exact calculations and lay out a foundation for developing computational algorithms to correctly specify the actual type I error. The efficient algorithms are thus proposed to calculate the cutoff value in a deterministic decision rule and the probability value in a randomized decision rule, such that the actual type I error is under but closest to, or equal to, the intended level, respectively. The algorithm may also be used to calculate the sample size to achieve the prespecified type I error and power. The usefulness of the proposed methodology is further demonstrated in the power calculation for designing superiority and noninferiority trials.},
  archive      = {J_SIM},
  author       = {Peiran Liu and Ming-Hui Chen and Susie Sinks and Peng Sun},
  doi          = {10.1002/sim.10005},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1688-1707},
  shortjournal = {Stat. Med.},
  title        = {Are the tests overpowered or underpowered? a unified solution to correctly specify type i errors in design of clinical trials for two sample proportions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shape restricted additive hazards models: Monotone,
unimodal, and u-shape hazard functions. <em>SIM</em>, <em>43</em>(9),
1671–1687. (<a href="https://doi.org/10.1002/sim.10040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimation of the semiparametric additive hazards model with an unspecified baseline hazard function where the effect of a continuous covariate has a specific shape but otherwise unspecified. Such estimation is particularly useful for a unimodal hazard function, where the hazard is monotone increasing and monotone decreasing with an unknown mode. A popular approach of the proportional hazards model is limited in such setting due to the complicated structure of the partial likelihood. Our model defines a quadratic loss function, and its simple structure allows a global Hessian matrix that does not involve parameters. Thus, once the global Hessian matrix is computed, a standard quadratic programming method can be applicable by profiling all possible locations of the mode. However, the quadratic programming method may be inefficient to handle a large global Hessian matrix in the profiling algorithm due to a large dimensionality, where the dimension of the global Hessian matrix and number of hypothetical modes are the same order as the sample size. We propose the quadratic pool adjacent violators algorithm to reduce computational costs. The proposed algorithm is extended to the model with a time-dependent covariate with monotone or U-shape hazard function. In simulation studies, our proposed method improves computational speed compared to the quadratic programming method, with bias and mean square error reductions. We analyze data from a recent cardiovascular study.},
  archive      = {J_SIM},
  author       = {Yunro Chung and Anastasia Ivanova and Jason P. Fine},
  doi          = {10.1002/sim.10040},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1671-1687},
  shortjournal = {Stat. Med.},
  title        = {Shape restricted additive hazards models: Monotone, unimodal, and U-shape hazard functions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to “using principal stratification in analysis of
clinical trials.” <em>SIM</em>, <em>43</em>(8), 1669. (<a
href="https://doi.org/10.1002/sim.10034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Ilya Lipkovich and Bohdana Ratitch and Yongming Qu and Xiang Zhang and Mingyang Shan and Craig Mallinckrodt},
  doi          = {10.1002/sim.10034},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1669},
  shortjournal = {Stat. Med.},
  title        = {Correction to “Using principal stratification in analysis of clinical trials”},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Modeling correlated pairs of mammogram images.
<em>SIM</em>, <em>43</em>(8), 1660–1668. (<a
href="https://doi.org/10.1002/sim.10002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammography remains the primary screening strategy for breast cancer, which continues to be the most prevalent cancer diagnosis among women globally. Because screening mammograms capture both the left and right breast, there is a nonnegligible correlation between the pair of images. Previous studies have explored the concept of averaging between the pair of images after proper image registration; however, no comparison has been made in directly utilizing the paired images. In this paper, we extend the bivariate functional principal component analysis over triangulations to jointly characterize the pair of imaging data bounded in an irregular domain and then nest the extracted features within the survival model to predict the onset of breast cancer. The method is applied to our motivating data from the Joanne Knight Breast Health Cohort at Siteman Cancer Center. Our findings indicate that there was no statistically significant difference in model discrimination performance between averaging the pair of images and jointly modeling the two images. Although the breast cancer study did not reveal any significant difference, it is worth noting that the methods proposed here can be readily extended to other studies involving paired or multivariate imaging data.},
  archive      = {J_SIM},
  author       = {Shu Jiang and Graham A. Colditz},
  doi          = {10.1002/sim.10002},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1660-1668},
  shortjournal = {Stat. Med.},
  title        = {Modeling correlated pairs of mammogram images},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utilizing local likelihood in regression discontinuity
design: Investigating the impact of antiretroviral therapy eligibility
on retention in clinical HIV care in south africa. <em>SIM</em>,
<em>43</em>(8), 1640–1659. (<a
href="https://doi.org/10.1002/sim.10028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The regression discontinuity (RD) design is a widely utilized approach for assessing treatment effects. It involves assigning treatment based on the value of an observed covariate in relation to a fixed threshold. Although the RD design has been widely employed across various problems, its application to specific data types has received limited attention. For instance, there has been little research on utilizing the RD design when the outcome variable exhibits zero-inflation. This study introduces a novel RD estimator using local likelihood, which overcomes the limitations of the local linear regression model, a popular approach for estimating treatment effects in RD design, by considering the data type of the outcome variable. To determine the optimal bandwidth, we propose a modified Ludwig-Miller cross validation method. A set of simulations is carried out, involving binary, count, and zero-inflated outcome variables, to showcase the superior performance of the suggested method over local linear regression models. Subsequently, the proposed local likelihood model is employed on HIV care data, where antiretroviral therapy eligibility is determined by a CD4 count threshold. A comparison is made between the results obtained using the local likelihood model and those obtained using local linear regression.},
  archive      = {J_SIM},
  author       = {Jaehyun Seo and Chanmin Kim},
  doi          = {10.1002/sim.10028},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1640-1659},
  shortjournal = {Stat. Med.},
  title        = {Utilizing local likelihood in regression discontinuity design: Investigating the impact of antiretroviral therapy eligibility on retention in clinical HIV care in south africa},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Methods for the estimation of direct and indirect
vaccination effects by combining data from individual- and
cluster-randomized trials. <em>SIM</em>, <em>43</em>(8), 1627–1639. (<a
href="https://doi.org/10.1002/sim.10030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both individually and cluster randomized study designs have been used for vaccine trials to assess the effects of vaccine on reducing the risk of disease or infection. The choice between individually and cluster randomized designs is often driven by the target estimand of interest (eg, direct versus total), statistical power, and, importantly, logistic feasibility. To combat emerging infectious disease threats, especially when the number of events from one single trial may not be adequate to obtain vaccine effect estimates with a desired level of precision, it may be necessary to combine information across multiple trials. In this article, we propose a model formulation to estimate the direct, indirect, total, and overall vaccine effects combining data from trials with two types of study designs: individual-randomization and cluster-randomization, based on a Cox proportional hazards model, where the hazard of infection depends on both vaccine status of the individual as well as the vaccine status of the other individuals in the same cluster. We illustrate the use of the proposed model and assess the potential efficiency gain from combining data from multiple trials, compared to using data from each individual trial alone, through two simulation studies, one of which is designed based on a cholera vaccine trial previously carried out in Matlab, Bangladesh.},
  archive      = {J_SIM},
  author       = {Rui Wang and Mengqi Cen and Yunda Huang and George Qian and Natalie E. Dean and Susan S. Ellenberg and Thomas R. Fleming and Wenbin Lu and Ira M. Longini},
  doi          = {10.1002/sim.10030},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1627-1639},
  shortjournal = {Stat. Med.},
  title        = {Methods for the estimation of direct and indirect vaccination effects by combining data from individual- and cluster-randomized trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scaled kernel density estimation prior for dynamic
borrowing of historical information with application to clinical trial
design. <em>SIM</em>, <em>43</em>(8), 1615–1626. (<a
href="https://doi.org/10.1002/sim.10032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating historical data into a current data analysis can improve estimation of parameters shared across both datasets and increase the power to detect associations of interest while reducing the time and cost of new data collection. Several methods for prior distribution elicitation have been introduced to allow for the data-driven borrowing of historical information within a Bayesian analysis of the current data. We propose scaled Gaussian kernel density estimation (SGKDE) prior distributions as potentially more flexible alternatives. SGKDE priors directly use posterior samples collected from a historical data analysis to approximate probability density functions, whose variances depend on the degree of similarity between the historical and current datasets, which are used as prior distributions in the current data analysis. We compare the performances of the SGKDE priors with some existing approaches using a simulation study. Data from a recently completed phase III clinical trial of a maternal vaccine for respiratory syncytial virus are used to further explore the properties of SGKDE priors when designing a new clinical trial while incorporating historical data. Overall, both studies suggest that the new approach results in improved parameter estimation and power in the current data analysis compared to the considered existing methods.},
  archive      = {J_SIM},
  author       = {Joshua L. Warren and Qi Wang and Maria M. Ciarleglio},
  doi          = {10.1002/sim.10032},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1615-1626},
  shortjournal = {Stat. Med.},
  title        = {A scaled kernel density estimation prior for dynamic borrowing of historical information with application to clinical trial design},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distribution–free hyperrectangular tolerance regions for
setting multivariate reference regions in laboratory medicine.
<em>SIM</em>, <em>43</em>(8), 1604–1614. (<a
href="https://doi.org/10.1002/sim.10019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reference regions are important in laboratory medicine to interpret the test results of patients, and usually given by tolerance regions. Tolerance regions of p ( ≥ 2 ) $$ p\;\left(\ge 2\right) $$ dimensions are highly desirable when the test results contains p $$ p $$ outcome measures. Nonparametric hyperrectangular tolerance regions are attractive in real problems due to their robustness with respect to the underlying distribution of the measurements and ease of intepretation, and methods to construct them have been recently provided by Young and Mathew [Stat Methods Med Res. 2020;29:3569-3585]. However, their validity is supported by a simulation study only. In this paper, nonparametric hyperrectangular tolerance regions are constructed by using Tukey&#39;s [Ann Math Stat. 1947;18:529-539; Ann Math Stat. 1948;19:30-39] elegant results of equivalence blocks. The validity of these new tolerance regions is proven mathematically in [Ann Math Stat. 1947;18:529-539; Ann Math Stat. 1948;19:30-39] under the only assumption that the underlying distribution of the measurements is continuous. The methodology is applied to analyze the kidney function problem considered in Young and Mathew [Stat Methods Med Res. 2020;29:3569-3585].},
  archive      = {J_SIM},
  author       = {Wei Liu and Frank Bretz and Mario Cortina–Borja},
  doi          = {10.1002/sim.10019},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1604-1614},
  shortjournal = {Stat. Med.},
  title        = {Distribution–free hyperrectangular tolerance regions for setting multivariate reference regions in laboratory medicine},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point estimation, confidence intervals, and p-values for
optimal adaptive two-stage designs with normal endpoints. <em>SIM</em>,
<em>43</em>(8), 1577–1603. (<a
href="https://doi.org/10.1002/sim.10020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the dependency structure in the sampling process, adaptive trial designs create challenges in point and interval estimation and in the calculation of P -values. Optimal adaptive designs, which are designs where the parameters governing the adaptivity are chosen to maximize some performance criterion, suffer from the same problem. Various analysis methods which are able to handle this dependency structure have already been developed. In this work, we aim to give a comprehensive summary of these methods and show how they can be applied to the class of designs with planned adaptivity, of which optimal adaptive designs are an important member. The defining feature of these kinds of designs is that the adaptive elements are completely prespecified. This allows for explicit descriptions of the calculations involved, which makes it possible to evaluate different methods in a fast and accurate manner. We will explain how to do so, and present an extensive comparison of the performance characteristics of various estimators between an optimal adaptive design and its group-sequential counterpart.},
  archive      = {J_SIM},
  author       = {Jan Meis and Maximilian Pilz and Björn Bokelmann and Carolin Herrmann and Geraldine Rauch and Meinhard Kieser},
  doi          = {10.1002/sim.10020},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1577-1603},
  shortjournal = {Stat. Med.},
  title        = {Point estimation, confidence intervals, and P-values for optimal adaptive two-stage designs with normal endpoints},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized functional linear model with a point process
predictor. <em>SIM</em>, <em>43</em>(8), 1564–1576. (<a
href="https://doi.org/10.1002/sim.10023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point process data have become increasingly popular these days. For example, many of the data captured in electronic health records (EHR) are in the format of point process data. It is of great interest to study the association between a point process predictor and a scalar response using generalized functional linear regression models. Various generalized functional linear regression models have been developed under different settings in the past decades. However, existing methods can only deal with functional or longitudinal predictors, not point process predictors. In this article, we propose a novel generalized functional linear regression model for a point process predictor. Our proposed model is based on the joint modeling framework, where we adopt a log-Gaussian Cox process model for the point process predictor and a generalized linear regression model for the outcome. We also develop a new algorithm for fast model estimation based on the Gaussian variational approximation method. We conduct extensive simulation studies to evaluate the performance of our proposed method and compare it to competing methods. The performance of our proposed method is further demonstrated on an EHR dataset of patients admitted into the intensive care units of the Beth Israel Deaconess Medical Center between 2001 and 2008.},
  archive      = {J_SIM},
  author       = {Jiehuan Sun and Kuang-Yao Lee},
  doi          = {10.1002/sim.10023},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1564-1576},
  shortjournal = {Stat. Med.},
  title        = {Generalized functional linear model with a point process predictor},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensitivity analysis with iterative outlier detection for
systematic reviews and meta-analyses. <em>SIM</em>, <em>43</em>(8),
1549–1563. (<a href="https://doi.org/10.1002/sim.10008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis is a widely used tool for synthesizing results from multiple studies. The collected studies are deemed heterogeneous when they do not share a common underlying effect size; thus, the factors attributable to the heterogeneity need to be carefully considered. A critical problem in meta-analyses and systematic reviews is that outlying studies are frequently included, which can lead to invalid conclusions and affect the robustness of decision-making. Outliers may be caused by several factors such as study selection criteria, low study quality, small-study effects, and so on. Although outlier detection is well-studied in the statistical community, limited attention has been paid to meta-analysis. The conventional outlier detection method in meta-analysis is based on a leave-one-study-out procedure. However, when calculating a potentially outlying study&#39;s deviation, other outliers could substantially impact its result. This article proposes an iterative method to detect potential outliers, which reduces such an impact that could confound the detection. Furthermore, we adopt bagging to provide valid inference for sensitivity analyses of excluding outliers. Based on simulation studies, the proposed iterative method yields smaller bias and heterogeneity after performing a sensitivity analysis to remove the identified outliers. It also provides higher accuracy on outlier detection. Two case studies are used to illustrate the proposed method&#39;s real-world performance.},
  archive      = {J_SIM},
  author       = {Zhuo Meng and Jingshen Wang and Lifeng Lin and Chong Wu},
  doi          = {10.1002/sim.10008},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1549-1563},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity analysis with iterative outlier detection for systematic reviews and meta-analyses},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate probit linear mixed models for multivariate
longitudinal binary data. <em>SIM</em>, <em>43</em>(8), 1527–1548. (<a
href="https://doi.org/10.1002/sim.10029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When analyzing multivariate longitudinal binary data, we estimate the effects on the responses of the covariates while accounting for three types of complex correlations present in the data. These include the correlations within separate responses over time, cross-correlations between different responses at different times, and correlations between different responses at each time point. The number of parameters thus increases quadratically with the dimension of the correlation matrix, making parameter estimation difficult; the estimated correlation matrix must also meet the positive definiteness constraint. The correlation matrix may additionally be heteroscedastic; however, the matrix structure is commonly considered to be homoscedastic and constrained, such as exchangeable or autoregressive with order one. These assumptions are overly strong, resulting in skewed estimates of the covariate effects on the responses. Hence, we propose probit linear mixed models for multivariate longitudinal binary data, where the correlation matrix is estimated using hypersphere decomposition instead of the strong assumptions noted above. Simulations and real examples are used to demonstrate the proposed methods. An open source R package, BayesMGLM , is made available on GitHub at https://github.com/kuojunglee/BayesMGLM/ with full documentation to produce the results.},
  archive      = {J_SIM},
  author       = {Kuo-Jung Lee and Chanmin Kim and Jae Keun Yoo and Keunbaik Lee},
  doi          = {10.1002/sim.10029},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1527-1548},
  shortjournal = {Stat. Med.},
  title        = {Multivariate probit linear mixed models for multivariate longitudinal binary data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous variable selection and estimation for survival
data via the gaussian seamless-l0 penalty. <em>SIM</em>, <em>43</em>(8),
1509–1526. (<a href="https://doi.org/10.1002/sim.10031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new simultaneous variable selection and estimation procedure with the Gaussian seamless- L 0 $$ {L}_0 $$ (GSELO) penalty for Cox proportional hazard model and additive hazards model. The GSELO procedure shows good potential to improve the existing variable selection methods by taking strength from both best subset selection (BSS) and regularization. In addition, we develop an iterative algorithm to implement the proposed procedure in a computationally efficient way. Theoretically, we establish the convergence properties of the algorithm and asymptotic theoretical properties of the proposed procedure. Since parameter tuning is crucial to the performance of the GSELO procedure, we also propose an extended Bayesian information criteria (EBIC) parameter selector for the GSELO procedure. Simulated and real data studies have demonstrated the prediction performance and effectiveness of the proposed method over several state-of-the-art methods.},
  archive      = {J_SIM},
  author       = {Zili Liu and Hong Wang},
  doi          = {10.1002/sim.10031},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1509-1526},
  shortjournal = {Stat. Med.},
  title        = {Simultaneous variable selection and estimation for survival data via the gaussian seamless-l0 penalty},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Targeted learning in observational studies with multi-valued
treatments: An evaluation of antipsychotic drug treatment safety.
<em>SIM</em>, <em>43</em>(8), 1489–1508. (<a
href="https://doi.org/10.1002/sim.10003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate estimation of causal effects of multiple competing (multi-valued) treatments in the absence of randomization. Our work is motivated by an intention-to-treat study of the relative cardiometabolic risk of assignment to one of six commonly prescribed antipsychotic drugs in a cohort of nearly 39 000 adults with serious mental illnesses. Doubly-robust estimators, such as targeted minimum loss-based estimation (TMLE), require correct specification of either the treatment model or outcome model to ensure consistent estimation; however, common TMLE implementations estimate treatment probabilities using multiple binomial regressions rather than multinomial regression. We implement a TMLE estimator that uses multinomial treatment assignment and ensemble machine learning to estimate average treatment effects. Our multinomial implementation improves coverage, but does not necessarily reduce bias, relative to the binomial implementation in simulation experiments with varying treatment propensity overlap and event rates. Evaluating the causal effects of the antipsychotics on 3-year diabetes risk or death, we find a safety benefit of moving from a second-generation drug considered among the safest of the second-generation drugs to an infrequently prescribed first-generation drug known for having low cardiometabolic risk.},
  archive      = {J_SIM},
  author       = {Jason Poulos and Marcela Horvitz-Lennon and Katya Zelevinsky and Tudor Cristea-Platon and Thomas Huijskens and Pooja Tyagi and Jiaju Yan and Jordi Diaz and Sharon-Lise Normand},
  doi          = {10.1002/sim.10003},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1489-1508},
  shortjournal = {Stat. Med.},
  title        = {Targeted learning in observational studies with multi-valued treatments: An evaluation of antipsychotic drug treatment safety},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaled average bioequivalence methods for highly variable
drugs: Leveling-off soft limits and the EMA’s 2010 guideline (some ways
to improve its type i error control). <em>SIM</em>, <em>43</em>(7),
1475–1488. (<a href="https://doi.org/10.1002/sim.10021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The regulatory EMA&#39;s reference scaled average bioequivalence (RSABE) approach for highly variable drugs suffers from some type I error control problems at the neighborhood of the 30% coefficient of variation (CV), where the bioequivalence (BE) limits change from constant to linearly scaled. This paper analyses BE inference methods based on the “Leveling-off” (LO) soft sigmoid expanding BE limits that were proposed as an appealing surrogate for the EMA&#39;s limits and compares both approaches, on the replicated and partially replicated crossover designs. The initially proposed version of the LO method also has type I error inflation problems, albeit attenuated. But given its more mathematically regular character, it is more suitable for analytical corrections. Here we introduce two improvements over LO, one based on the application of Howe&#39;s method and the other based on correcting the estimation error. They further reduce the type I error inflation, although it does not disappear completely. Finally, the effect of heteroscedasticity on the above results is studied. It leads to inflation or deflation of the type I error, depending on the design and the type of heteroscedasticity (variability of the test product greater than that of the reference product or the opposite). The replicated design is much more stable against these effects than the partially replicated design and maintains these improvements much better.},
  archive      = {J_SIM},
  author       = {Joel Muñoz and Jordi Ocaña and Rolando Suárez and Carolina Millapán},
  doi          = {10.1002/sim.10021},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1475-1488},
  shortjournal = {Stat. Med.},
  title        = {Scaled average bioequivalence methods for highly variable drugs: Leveling-off soft limits and the EMA&#39;s 2010 guideline (some ways to improve its type i error control)},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiply robust generalized estimating equations for cluster
randomized trials with missing outcomes. <em>SIM</em>, <em>43</em>(7),
1458–1474. (<a href="https://doi.org/10.1002/sim.10027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized estimating equations (GEEs) provide a useful framework for estimating marginal regression parameters based on data from cluster randomized trials (CRTs), but they can result in inaccurate parameter estimates when some outcomes are informatively missing. Existing techniques to handle missing outcomes in CRTs rely on correct specification of a propensity score model, a covariate-conditional mean outcome model, or require at least one of these two models to be correct, which can be challenging in practice. In this article, we develop new weighted GEEs to simultaneously estimate the marginal mean, scale, and correlation parameters in CRTs with missing outcomes, allowing for multiple propensity score models and multiple covariate-conditional mean models to be specified. The resulting estimators are consistent provided that any one of these models is correct. An iterative algorithm is provided for implementing this more robust estimator and practical considerations for specifying multiple models are discussed. We evaluate the performance of the proposed method through Monte Carlo simulations and apply the proposed multiply robust estimator to analyze the Botswana Combination Prevention Project, a large HIV prevention CRT designed to evaluate whether a combination of HIV-prevention measures can reduce HIV incidence.},
  archive      = {J_SIM},
  author       = {Dustin J. Rabideau and Fan Li and Rui Wang},
  doi          = {10.1002/sim.10027},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1458-1474},
  shortjournal = {Stat. Med.},
  title        = {Multiply robust generalized estimating equations for cluster randomized trials with missing outcomes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling variation in mixture effects over space with a
bayesian spatially varying mixture model. <em>SIM</em>, <em>43</em>(7),
1441–1457. (<a href="https://doi.org/10.1002/sim.10022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture analysis is an emerging statistical tool in epidemiological research that seeks to estimate the health effects associated with mixtures of several exposures. This approach acknowledges that individuals experience many simultaneous exposures and it can estimate the relative importance of components in the mixture. Health effects due to mixtures may vary over space driven by to political, demographic, environmental, or other differences. In such cases, estimating a global mixture effect without accounting for spatial variation would induce bias in effect estimates and potentially lower statistical power. To date, no methods have been developed to estimate spatially varying chemical mixture effects. We developed a Bayesian spatially varying mixture model that estimates spatially varying mixture effects and the importance weights of components in the mixture, while adjusting for covariates. We demonstrate the efficacy of the model through a simulation study that varies the number of mixtures (one and two) and spatial pattern (global, one-dimensional, radial) and magnitude of mixture effects, showing that the model is able to accurately reproduce the spatial pattern of mixture effects across a diverse set of scenarios. Finally, we apply our model to a multi-center case-control study of non-Hodgkin lymphoma (NHL) in Detroit, Iowa, Los Angeles, and Seattle. We identify significant spatially varying positive and inverse associations with NHL for two mixtures of pesticides in Iowa and do not find strong spatial effects at the other three centers. In conclusion, the Bayesian spatially varying mixture model represents a novel method for modeling spatial variation in mixture effects.},
  archive      = {J_SIM},
  author       = {Joseph Boyle and Mary H. Ward and James R. Cerhan and Nat Rothman and David C. Wheeler},
  doi          = {10.1002/sim.10022},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1441-1457},
  shortjournal = {Stat. Med.},
  title        = {Modeling variation in mixture effects over space with a bayesian spatially varying mixture model},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference on qualitative differences in the
magnitude of an effect. <em>SIM</em>, <em>43</em>(7), 1419–1440. (<a
href="https://doi.org/10.1002/sim.10025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Qualitative interactions occur when a treatment effect or measure of association varies in sign by sub-population. Of particular interest in many biomedical settings are absence/presence qualitative interactions, which occur when an effect is present in one sub-population but absent in another. Absence/presence interactions arise in emerging applications in precision medicine, where the objective is to identify a set of predictive biomarkers that have prognostic value for clinical outcomes in some sub-population but not others. They also arise naturally in gene regulatory network inference, where the goal is to identify differences in networks corresponding to diseased and healthy individuals, or to different subtypes of disease; such differences lead to identification of network-based biomarkers for diseases. In this paper, we argue that while the absence/presence hypothesis is important, developing a statistical test for this hypothesis is an intractable problem. To overcome this challenge, we approximate the problem in a novel inference framework. In particular, we propose to make inferences about absence/presence interactions by quantifying the relative difference in effect size, reasoning that when the relative difference is large, an absence/presence interaction occurs. The proposed methodology is illustrated through a simulation study as well as an analysis of breast cancer data from the Cancer Genome Atlas.},
  archive      = {J_SIM},
  author       = {Aaron Hudson and Ali Shojaie},
  doi          = {10.1002/sim.10025},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1419-1440},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference on qualitative differences in the magnitude of an effect},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety signal detection with control of latent factors.
<em>SIM</em>, <em>43</em>(7), 1397–1418. (<a
href="https://doi.org/10.1002/sim.10015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Postmarket drug safety database like vaccine adverse event reporting system (VAERS) collect thousands of spontaneous reports annually, with each report recording occurrences of any adverse events (AEs) and use of vaccines. We hope to identify signal vaccine-AE pairs, for which certain vaccines are statistically associated with certain adverse events (AE), using such data. Thus, the outcomes of interest are multiple AEs, which are binary outcomes and could be correlated because they might share certain latent factors; and the primary covariates are vaccines. Appropriately accounting for the complex correlation among AEs could improve the sensitivity and specificity of identifying signal vaccine-AE pairs. We propose a two-step approach in which we first estimate the shared latent factors among AEs using a working multivariate logistic regression model, and then use univariate logistic regression model to examine the vaccine-AE associations after controlling for the latent factors. Our simulation studies show that this approach outperforms current approaches in terms of sensitivity and specificity. We apply our approach in analyzing VAERS data and report our findings.},
  archive      = {J_SIM},
  author       = {Xianming Tan and William Wang and Donglin Zeng and Guanghan F. Liu and Guoqing Diao and Niusha Jafari and Ethan M. Alt and Joseph G. Ibrahim},
  doi          = {10.1002/sim.10015},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1397-1418},
  shortjournal = {Stat. Med.},
  title        = {Safety signal detection with control of latent factors},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective sample size: A measure of individual uncertainty
in predictions. <em>SIM</em>, <em>43</em>(7), 1384–1396. (<a
href="https://doi.org/10.1002/sim.10018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical prediction models are estimated using a sample of limited size from the target population, leading to uncertainty in predictions, even when the model is correctly specified. Generally, not all patient profiles are observed uniformly in model development. As a result, sampling uncertainty varies between individual patients&#39; predictions. We aimed to develop an intuitive measure of individual prediction uncertainty. The variance of a patient&#39;s prediction can be equated to the variance of the sample mean outcome in n ∗ $$ {n}_{\ast } $$ hypothetical patients with the same predictor values. This hypothetical sample size n ∗ $$ {n}_{\ast } $$ can be interpreted as the number of similar patients n eff $$ {n}_{\mathrm{eff}} $$ that the prediction is effectively based on, given that the model is correct. For generalized linear models, we derived analytical expressions for the effective sample size. In addition, we illustrated the concept in patients with acute myocardial infarction. In model development, can be used to balance accuracy versus uncertainty of predictions. In a validation sample, the distribution of indicates which patients were more and less represented in the development data, and whether predictions might be too uncertain for some to be practically meaningful. In a clinical setting, the effective sample size may facilitate communication of uncertainty about predictions. We propose the effective sample size as a clinically interpretable measure of uncertainty in individual predictions. Its implications should be explored further for the development, validation and clinical implementation of prediction models.},
  archive      = {J_SIM},
  author       = {Doranne Thomassen and Saskia le Cessie and Hans C. van Houwelingen and Ewout W. Steyerberg},
  doi          = {10.1002/sim.10018},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1384-1396},
  shortjournal = {Stat. Med.},
  title        = {Effective sample size: A measure of individual uncertainty in predictions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Likelihood ratio combination of multiple biomarkers via
smoothing spline estimated densities. <em>SIM</em>, <em>43</em>(7),
1372–1383. (<a href="https://doi.org/10.1002/sim.10026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnostic accuracy of multiple biomarkers in medical research is crucial for detecting diseases and predicting patient outcomes. An optimal method for combining these biomarkers is essential to maximize the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC). Although the optimality of the likelihood ratio has been proven by Neyman and Pearson, challenges persist in estimating the likelihood ratio, primarily due to the estimation of multivariate density functions. In this study, we propose a non-parametric approach for estimating multivariate density functions by utilizing Smoothing Spline density estimation to approximate the full likelihood function for both diseased and non-diseased groups, which compose the likelihood ratio. Simulation results demonstrate the efficiency of our method compared to other biomarker combination techniques under various settings for generated biomarker values. Additionally, we apply the proposed method to a real-world study aimed at detecting childhood autism spectrum disorder (ASD), showcasing its practical relevance and potential for future applications in medical research.},
  archive      = {J_SIM},
  author       = {Zhiyuan Du and Pang Du and Aiyi Liu},
  doi          = {10.1002/sim.10026},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1372-1383},
  shortjournal = {Stat. Med.},
  title        = {Likelihood ratio combination of multiple biomarkers via smoothing spline estimated densities},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric estimation of linear personalized diagnostics
rules via efficient grid algorithm. <em>SIM</em>, <em>43</em>(7),
1354–1371. (<a href="https://doi.org/10.1002/sim.10016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many diseases are heterogeneous, comprised of multiple disease subgroups. It is of great interest but highly unlikely to find a single biomarker that can accurately detect such heterogeneous diseases across different subgroups. In this article, we propose to estimate a personalized diagnostic rule (PDR) to tailor more effective biomarkers to each individual according to a linear combination of his or her profiles. A standard grid search algorithm can be used to estimate the optimal linear PDR that maximizes the area under the receiver operating characteristics curve (AUC) among all the linear PDRs, but it is time-consuming especially when the number of variables is large. Alternatively, we developed an efficient grid rotation algorithm that provides a nearly suboptimal solution and studied its variation to find the optimal solution. We implemented the cross-validated forward variable selection method to find a subset of useful variables while avoid overfitting. Extensive simulations show that our proposed method reduces bias and variance. Analysis of a gastric cancer biomarker study and censored survival outcome data illustrates the practical utility of our proposed method. The proposed method is implemented in the open-source R package persDx.},
  archive      = {J_SIM},
  author       = {Yaliang Zhang and Yunro Chung},
  doi          = {10.1002/sim.10016},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1354-1371},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric estimation of linear personalized diagnostics rules via efficient grid algorithm},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing subject heterogeneity in time-dependent
discrimination for biomarker evaluation. <em>SIM</em>, <em>43</em>(7),
1341–1353. (<a href="https://doi.org/10.1002/sim.10024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate discrimination has been the central goal in identifying biomarkers for monitoring disease progression and early detection. Acknowledging the fact that discrimination accuracy of biomarkers for a time-to-event outcome often changes over time, local measures such as the time-dependent receiver operating characteristic curve and its area under the curve (AUC) are used to assess time-dependent predictive discrimination. However, such measures do not address subject heterogeneity, although the impact of covariates including demographics, disease-related characteristics, and other clinical information on the discriminatory performance of biomarkers needs to be investigated before their clinical use. We propose the covariate-specific time-dependent AUC, a measure for covariate-adjusted discrimination. We develop a regression model on the covariate-specific time-dependent AUC to understand how and in what magnitude the covariates influence biomarker performance. Then we construct a pseudo partial-likelihood for estimation and inference. This is followed by our establishing the asymptotic properties of the proposed estimators and provide variance estimation. The simulation studies and application to the AIDS Clinical Trials Group 175 data demonstrate that the proposed method offers an informative tool for inferring covariate-specific and time-dependent predictive discrimination.},
  archive      = {J_SIM},
  author       = {Xinyang Jiang and Wen Li and Ruosha Li and Jing Ning},
  doi          = {10.1002/sim.10024},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1341-1353},
  shortjournal = {Stat. Med.},
  title        = {Addressing subject heterogeneity in time-dependent discrimination for biomarker evaluation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classified functional mixed effects model prediction.
<em>SIM</em>, <em>43</em>(7), 1329–1340. (<a
href="https://doi.org/10.1002/sim.10007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In nowadays biomedical research, there has been a growing demand for making accurate prediction at subject levels. In many of these situations, data are collected as longitudinal curves and display distinct individual characteristics. Thus, prediction mechanisms accommodated with functional mixed effects models (FMEM) are useful. In this paper, we developed a classified functional mixed model prediction (CFMMP) method, which adapts classified mixed model prediction (CMMP) to the framework of FMEM. Performance of CFMMP against functional regression prediction based on simulation studies and the consistency property of CFMMP estimators are explored. Real-world applications of CFMMP are illustrated using real world examples including data from the hormone research menstrual cycles and the diffusion tensor imaging.},
  archive      = {J_SIM},
  author       = {Xiaoyan Liu and Jiming Jiang},
  doi          = {10.1002/sim.10007},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1329-1340},
  shortjournal = {Stat. Med.},
  title        = {Classified functional mixed effects model prediction},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing dynamic predictions from joint models using super
learning. <em>SIM</em>, <em>43</em>(7), 1315–1328. (<a
href="https://doi.org/10.1002/sim.10010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models for longitudinal and time-to-event data are often employed to calculate dynamic individualized predictions used in numerous applications of precision medicine. Two components of joint models that influence the accuracy of these predictions are the shape of the longitudinal trajectories and the functional form linking the longitudinal outcome history to the hazard of the event. Finding a single well-specified model that produces accurate predictions for all subjects and follow-up times can be challenging, especially when considering multiple longitudinal outcomes. In this work, we use the concept of super learning and avoid selecting a single model. In particular, we specify a weighted combination of the dynamic predictions calculated from a library of joint models with different specifications. The weights are selected to optimize a predictive accuracy metric using V-fold cross-validation. We use as predictive accuracy measures the expected quadratic prediction error and the expected predictive cross-entropy. In a simulation study, we found that the super learning approach produces results very similar to the Oracle model, which was the model with the best performance in the test datasets. All proposed methodology is implemented in the freely available R package JMbayes2 .},
  archive      = {J_SIM},
  author       = {Dimitris Rizopoulos and Jeremy M. G. Taylor},
  doi          = {10.1002/sim.10010},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1315-1328},
  shortjournal = {Stat. Med.},
  title        = {Optimizing dynamic predictions from joint models using super learning},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of methods that combine multiple randomized
trials to estimate heterogeneous treatment effects. <em>SIM</em>,
<em>43</em>(7), 1291–1314. (<a
href="https://doi.org/10.1002/sim.9955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment decisions can improve health outcomes, but using data to make these decisions in a reliable, precise, and generalizable way is challenging with a single dataset. Leveraging multiple randomized controlled trials allows for the combination of datasets with unconfounded treatment assignment to better estimate heterogeneous treatment effects. This article discusses several nonparametric approaches for estimating heterogeneous treatment effects using data from multiple trials. We extend single-study methods to a scenario with multiple trials and explore their performance through a simulation study, with data generation scenarios that have differing levels of cross-trial heterogeneity. The simulations demonstrate that methods that directly allow for heterogeneity of the treatment effect across trials perform better than methods that do not, and that the choice of single-study method matters based on the functional form of the treatment effect. Finally, we discuss which methods perform well in each setting and then apply them to four randomized controlled trials to examine effect heterogeneity of treatments for major depressive disorder.},
  archive      = {J_SIM},
  author       = {Carly Lupton Brantner and Trang Quynh Nguyen and Tengjie Tang and Congwen Zhao and Hwanhee Hong and Elizabeth A. Stuart},
  doi          = {10.1002/sim.9955},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1291-1314},
  shortjournal = {Stat. Med.},
  title        = {Comparison of methods that combine multiple randomized trials to estimate heterogeneous treatment effects},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence distributions for treatment effects in clinical
trials: Posteriors without priors. <em>SIM</em>, <em>43</em>(6),
1271–1289. (<a href="https://doi.org/10.1002/sim.10000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An attractive feature of using a Bayesian analysis for a clinical trial is that knowledge and uncertainty about the treatment effect is summarized in a posterior probability distribution. Researchers often find probability statements about treatment effects highly intuitive and the fact that this is not accommodated in frequentist inference is a disadvantage. At the same time, the requirement to specify a prior distribution in order to obtain a posterior distribution is sometimes an artificial process that may introduce subjectivity or complexity into the analysis. This paper considers a compromise involving confidence distributions, which are probability distributions that summarize uncertainty about the treatment effect without the need for a prior distribution and in a way that is fully compatible with frequentist inference. The concept of a confidence distribution provides a posterior–like probability distribution that is distinct from, but exists in tandem with, the relative frequency interpretation of probability used in frequentist inference. Although they have been discussed for decades, confidence distributions are not well known among clinical trial statisticians and the goal of this paper is to discuss their use in analyzing treatment effects from randomized trials. As well as providing an introduction to confidence distributions, some illustrative examples relevant to clinical trials are presented, along with various case studies based on real clinical trials. It is recommended that trial statisticians consider presenting confidence distributions for treatment effects when reporting analyses of clinical trials.},
  archive      = {J_SIM},
  author       = {Ian C. Marschner},
  doi          = {10.1002/sim.10000},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1271-1289},
  shortjournal = {Stat. Med.},
  title        = {Confidence distributions for treatment effects in clinical trials: Posteriors without priors},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian response adaptive randomization design with a
composite endpoint of mortality and morbidity. <em>SIM</em>,
<em>43</em>(6), 1256–1270. (<a
href="https://doi.org/10.1002/sim.10014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Allocating patients to treatment arms during a trial based on the observed responses accumulated up to the decision point, and sequential adaptation of this allocation, could minimize the expected number of failures or maximize total benefits to patients. In this study, we developed a Bayesian response-adaptive randomization (RAR) design targeting the endpoint of organ support-free days (OSFD) for patients admitted to the intensive care units. The OSFD is a mixture of mortality and morbidity assessed by the number of days of free of organ support within a predetermined post-randomization time-window. In the past, researchers treated OSFD as an ordinal outcome variable where the lowest category is death. We propose a novel RAR design for a composite endpoint of mortality and morbidity, for example, OSFD, by using a Bayesian mixture model with a Markov chain Monte Carlo sampling to estimate the posterior probability distribution of OSFD and determine treatment allocation ratios at each interim. Simulations were conducted to compare the performance of our proposed design under various randomization rules and different alpha spending functions. The results show that our RAR design using Bayesian inference allocated more patients to the better performing arm(s) compared to other existing adaptive rules while assuring adequate power and type I error rate control across a range of plausible clinical scenarios.},
  archive      = {J_SIM},
  author       = {Zhongying Xu and Tianzhou Ma and Lu Tang and Victor B. Talisa and Chung-Chou H. Chang},
  doi          = {10.1002/sim.10014},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1256-1270},
  shortjournal = {Stat. Med.},
  title        = {Bayesian response adaptive randomization design with a composite endpoint of mortality and morbidity},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple imputation strategies for missing event times in a
multi-state model analysis. <em>SIM</em>, <em>43</em>(6), 1238–1255. (<a
href="https://doi.org/10.1002/sim.10011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical studies, multi-state model (MSM) analysis is often used to describe the sequence of events that patients experience, enabling better understanding of disease progression. A complicating factor in many MSM studies is that the exact event times may not be known. Motivated by a real dataset of patients who received stem cell transplants, we considered the setting in which some event times were exactly observed and some were missing. In our setting, there was little information about the time intervals in which the missing event times occurred and missingness depended on the event type, given the analysis model covariates. These additional challenges limited the usefulness of some missing data methods (maximum likelihood, complete case analysis, and inverse probability weighting). We show that multiple imputation (MI) of event times can perform well in this setting. MI is a flexible method that can be used with any complete data analysis model. Through an extensive simulation study, we show that MI by predictive mean matching (PMM), in which sampling is from a set of observed times without reliance on a specific parametric distribution, has little bias when event times are missing at random, conditional on the observed data. Applying PMM separately for each sub-group of patients with a different pathway through the MSM tends to further reduce bias and improve precision. We recommend MI using PMM methods when performing MSM analysis with Markov models and partially observed event times.},
  archive      = {J_SIM},
  author       = {Elinor Curnow and Rachael A. Hughes and Kate Birnie and Kate Tilling and Michael J. Crowther},
  doi          = {10.1002/sim.10011},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1238-1255},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation strategies for missing event times in a multi-state model analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical performance review on diagnosis of leukemia,
glaucoma and diabetes mellitus using AI. <em>SIM</em>, <em>43</em>(6),
1227–1237. (<a href="https://doi.org/10.1002/sim.10004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of artificial intelligence (AI) in the healthcare industry tremendously increases the patient outcomes by reshaping the way we diagnose, treat and monitor patient s . AI-based innovation in healthcare include exploration of drugs, personalized medicine, clinical diagnosis investigations, robotic-assisted surgery, verified prescriptions, pregnancy care for women, radiology, and reviewed patient information analytics. However, prediction of AI-based solutions are depends mainly on the implementation of statistical algorithms and input data set. In this article, statistical performance review on various algorithms, Accuracy, Precision, Recall and F1-Score used to predict the diagnosis of leukemia, glaucoma, and diabetes mellitus is presented. Review on statistical algorithms&#39; performance, used for individual disease diagnosis gives a complete picture of various research efforts during the last two decades. At the end of statistical review on each disease diagnosis, we have discussed our inferences that will give future directions for the new researchers on selection of AI statistical algorithm as well as the input data set.},
  archive      = {J_SIM},
  author       = {Rengaraju Perumalraja and B. Felcia Logan&#39;s Deshna and N. Swetha},
  doi          = {10.1002/sim.10004},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1227-1237},
  shortjournal = {Stat. Med.},
  title        = {Statistical performance review on diagnosis of leukemia, glaucoma and diabetes mellitus using AI},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient estimation of cox model with random change point.
<em>SIM</em>, <em>43</em>(6), 1213–1226. (<a
href="https://doi.org/10.1002/sim.9987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical studies, the risk of a disease may dramatically change when some biological indexes of the human body exceed some thresholds. Furthermore, the differences in individual characteristics of patients such as physical and psychological experience may lead to subject-specific thresholds or change points. Although a large literature has been established for regression analysis of failure time data with change points, most of the existing methods assume the same, fixed change point for all study subjects. In this paper, we consider the situation where there exists a subject-specific change point and two Cox type models are presented. The proposed models also offer a framework for subgroup analysis. For inference, a sieve maximum likelihood estimation procedure is proposed and the asymptotic properties of the resulting estimators are established. An extensive simulation study is conducted to assess the empirical performance of the proposed method and indicates that it works well in practical situations. Finally the proposed approach is applied to a set of breast cancer data.},
  archive      = {J_SIM},
  author       = {Xuerong Chen and Yalu Ping and Jianguo Sun},
  doi          = {10.1002/sim.9987},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1213-1226},
  shortjournal = {Stat. Med.},
  title        = {Efficient estimation of cox model with random change point},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Steady-state statistical properties and implementation of
randomization designs with maximum tolerated imbalance restriction for
two-arm equal allocation clinical trials. <em>SIM</em>, <em>43</em>(6),
1194–1212. (<a href="https://doi.org/10.1002/sim.10013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, several randomization designs have been proposed in the literature as better alternatives to the traditional permuted block design (PBD), providing higher allocation randomness under the same restriction of the maximum tolerated imbalance (MTI). However, PBD remains the most frequently used method for randomizing subjects in clinical trials. This status quo may reflect an inadequate awareness and appreciation of the statistical properties of these randomization designs, and a lack of simple methods for their implementation. This manuscript presents the analytic results of statistical properties for five randomization designs with MTI restriction based on their steady-state probabilities of the treatment imbalance Markov chain and compares them to those of the PBD. A unified framework for randomization sequence generation and real-time on-demand treatment assignment is proposed for the straightforward implementation of randomization algorithms with explicit formulas of conditional allocation probabilities. Topics associated with the evaluation, selection, and implementation of randomization designs are discussed. It is concluded that for two-arm equal allocation trials, several randomization designs offer stronger protection against selection bias than the PBD does, and their implementation is not necessarily more difficult than the implementation of the PBD.},
  archive      = {J_SIM},
  author       = {Wenle Zhao and Kerstine Carter and Oleksandr Sverdlov and Annika Scheffold and Yevgen Ryeznik and Christy Cassarly and Vance W. Berger},
  doi          = {10.1002/sim.10013},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1194-1212},
  shortjournal = {Stat. Med.},
  title        = {Steady-state statistical properties and implementation of randomization designs with maximum tolerated imbalance restriction for two-arm equal allocation clinical trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Τ-inflated beta regression model for censored recurrent
events. <em>SIM</em>, <em>43</em>(6), 1170–1193. (<a
href="https://doi.org/10.1002/sim.9999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces a multivariate τ $$ \tau $$ -inflated beta regression ( τ $$ \tau $$ -IBR) modeling approach for the analysis of censored recurrent event data that is particularly useful when there is a mixture of (a) individuals who are generally less susceptible to recurrent events and (b) heterogeneity in duration of event-free periods amongst those who experience events. The modeling approach is applied to a restructured version of the recurrent event data that consists of censored longitudinal times-to-first-event in τ $$ \tau $$ length follow-up windows that potentially overlap. Multiple imputation (MI) and expectation-solution (ES) approaches appropriate for censored data are developed as part of the model fitting process. A suite of useful analysis outputs are provided from the τ $$ \tau $$ -IBR model that include parameter estimates to help interpret the (a) and (b) mixture of event times in the data, estimates of mean -restricted event-free duration in a -length follow-up window based on a patient&#39;s covariate profile, and heat maps of raw -restricted event-free durations observed in the data with censored observations augmented via averages across MI datasets. Simulations indicate good statistical performance of the proposed -IBR approach to modeling censored recurrent event data. An example is given based on the Azithromycin for Prevention of COPD Exacerbations Trial.},
  archive      = {J_SIM},
  author       = {Yizhuo Wang and Susan Murray},
  doi          = {10.1002/sim.9999},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1170-1193},
  shortjournal = {Stat. Med.},
  title        = {τ-inflated beta regression model for censored recurrent events},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian framework for modeling COVID-19 case numbers
through longitudinal monitoring of SARS-CoV-2 RNA in wastewater.
<em>SIM</em>, <em>43</em>(6), 1153–1169. (<a
href="https://doi.org/10.1002/sim.10009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wastewater-based surveillance has become an important tool for research groups and public health agencies investigating and monitoring the COVID-19 pandemic and other public health emergencies including other pathogens and drug abuse. While there is an emerging body of evidence exploring the possibility of predicting COVID-19 infections from wastewater signals, there remain significant challenges for statistical modeling. Longitudinal observations of viral copies in municipal wastewater can be influenced by noisy datasets and missing values with irregular and sparse samplings. We propose an integrative Bayesian framework to predict daily positive cases from weekly wastewater observations with missing values via functional data analysis techniques. In a unified procedure, the proposed analysis models severe acute respiratory syndrome coronavirus-2 RNA wastewater signals as a realization of a smooth process with error and combines the smooth process with COVID-19 cases to evaluate the prediction of positive cases. We demonstrate that the proposed framework can achieve these objectives with high predictive accuracies through simulated and observed real data.},
  archive      = {J_SIM},
  author       = {Xiaotian Dai and Nicole Acosta and Xuewen Lu and Casey R. J. Hubert and Jangwoo Lee and Kevin Frankowski and Maria A. Bautista and Barbara J. Waddell and Kristine Du and Janine McCalder and Jon Meddings and Norma Ruecker and Tyler Williamson and Danielle A. Southern and Jordan Hollman and Gopal Achari and M. Cathryn Ryan and Steve E. Hrudey and Bonita E. Lee and Xiaoli Pang and Rhonda G. Clark and Michael D. Parkins and Thierry Chekouo},
  doi          = {10.1002/sim.10009},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1153-1169},
  shortjournal = {Stat. Med.},
  title        = {A bayesian framework for modeling COVID-19 case numbers through longitudinal monitoring of SARS-CoV-2 RNA in wastewater},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modeling of association networks and longitudinal
biomarkers: An application to childhood obesity. <em>SIM</em>,
<em>43</em>(6), 1135–1152. (<a
href="https://doi.org/10.1002/sim.9994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of chronic non-communicable diseases such as obesity has noticeably increased in the last decade. The study of these diseases in early life is of paramount importance in determining their course in adult life and in supporting clinical interventions. Recently, attention has been drawn to approaches that study the alteration of metabolic pathways in obese children. In this work, we propose a novel joint modeling approach for the analysis of growth biomarkers and metabolite associations, to unveil metabolic pathways related to childhood obesity. Within a Bayesian framework, we flexibly model the temporal evolution of growth trajectories and metabolic associations through the specification of a joint nonparametric random effect distribution, with the main goal of clustering subjects, thus identifying risk sub-groups. Growth profiles as well as patterns of metabolic associations determine the clustering structure. Inclusion of risk factors is straightforward through the specification of a regression term. We demonstrate the proposed approach on data from the Growing Up in Singapore Towards healthy Outcomes cohort study, based in Singapore. Posterior inference is obtained via a tailored MCMC algorithm, involving a nonparametric prior with mixed support. Our analysis has identified potential key pathways in obese children that allow for the exploration of possible molecular mechanisms associated with childhood obesity.},
  archive      = {J_SIM},
  author       = {Andrea Cremaschi and Maria De Iorio and Narasimhan Kothandaraman and Fabian Yap and Mya Thway Tint and Johan Eriksson},
  doi          = {10.1002/sim.9994},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1135-1152},
  shortjournal = {Stat. Med.},
  title        = {Joint modeling of association networks and longitudinal biomarkers: An application to childhood obesity},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparison of hyperparameter tuning procedures for
clinical prediction models: A simulation study. <em>SIM</em>,
<em>43</em>(6), 1119–1134. (<a
href="https://doi.org/10.1002/sim.9932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuning hyperparameters, such as the regularization parameter in Ridge or Lasso regression, is often aimed at improving the predictive performance of risk prediction models. In this study, various hyperparameter tuning procedures for clinical prediction models were systematically compared and evaluated in low-dimensional data. The focus was on out-of-sample predictive performance (discrimination, calibration, and overall prediction error) of risk prediction models developed using Ridge, Lasso, Elastic Net, or Random Forest. The influence of sample size, number of predictors and events fraction on performance of the hyperparameter tuning procedures was studied using extensive simulations. The results indicate important differences between tuning procedures in calibration performance, while generally showing similar discriminative performance. The one-standard-error rule for tuning applied to cross-validation (1SE CV) often resulted in severe miscalibration. Standard non-repeated and repeated cross-validation (both 5-fold and 10-fold) performed similarly well and outperformed the other tuning procedures. Bootstrap showed a slight tendency to more severe miscalibration than standard cross-validation-based tuning procedures. Differences between tuning procedures were larger for smaller sample sizes, lower events fractions and fewer predictors. These results imply that the choice of tuning procedure can have a profound influence on the predictive performance of prediction models. The results support the application of standard 5-fold or 10-fold cross-validation that minimizes out-of-sample prediction error. Despite an increased computational burden, we found no clear benefit of repeated over non-repeated cross-validation for hyperparameter tuning. We warn against the potentially detrimental effects on model calibration of the popular 1SE CV rule for tuning prediction models in low-dimensional settings.},
  archive      = {J_SIM},
  author       = {Zoë S. Dunias and Ben Van Calster and Dirk Timmerman and Anne-Laure Boulesteix and Maarten van Smeden},
  doi          = {10.1002/sim.9932},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1119-1134},
  shortjournal = {Stat. Med.},
  title        = {A comparison of hyperparameter tuning procedures for clinical prediction models: A simulation study},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence sets for a level set in linear regression.
<em>SIM</em>, <em>43</em>(6), 1103–1118. (<a
href="https://doi.org/10.1002/sim.9996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression modeling is the workhorse of statistics and there is a vast literature on estimation of the regression function. It has been realized in recent years that in regression analysis the ultimate aim may be the estimation of a level set of the regression function, ie, the set of covariate values for which the regression function exceeds a predefined level, instead of the estimation of the regression function itself. The published work on estimation of the level set has thus far focused mainly on nonparametric regression, especially on point estimation. In this article, the construction of confidence sets for the level set of linear regression is considered. In particular, 1 − α $$ 1-\alpha $$ level upper, lower and two-sided confidence sets are constructed for the normal-error linear regression. It is shown that these confidence sets can be easily constructed from the corresponding level simultaneous confidence bands. It is also pointed out that the construction method is readily applicable to other parametric regression models where the mean response depends on a linear predictor through a monotonic link function, which include generalized linear models, linear mixed models and generalized linear mixed models. Therefore, the method proposed in this article is widely applicable. Simulation studies with both linear and generalized linear models are conducted to assess the method and real examples are used to illustrate the method.},
  archive      = {J_SIM},
  author       = {Fang Wan and Wei Liu and Frank Bretz},
  doi          = {10.1002/sim.9996},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1103-1118},
  shortjournal = {Stat. Med.},
  title        = {Confidence sets for a level set in linear regression},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An information-theoretic approach for the assessment of a
continuous outcome as a surrogate for a binary true endpoint based on
causal inference: Application to vaccine evaluation. <em>SIM</em>,
<em>43</em>(6), 1083–1102. (<a
href="https://doi.org/10.1002/sim.9997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the causal association paradigm, a method is proposed to assess the validity of a continuous outcome as a surrogate for a binary true endpoint. The methodology is based on a previously introduced information-theoretic definition of surrogacy and has two main steps. In the first step, a new model is proposed to describe the joint distribution of the potential outcomes associated with the putative surrogate and the true endpoint of interest. The identifiability issues inherent to this type of models are handled via sensitivity analysis. In the second step, a metric of surrogacy new to this setting, the so-called individual causal association is presented. The methodology is studied in detail using theoretical considerations, some simulations, and data from a randomized clinical trial evaluating an inactivated quadrivalent influenza vaccine. A user-friendly R package Surrogate is provided to carry out the evaluation exercise.},
  archive      = {J_SIM},
  author       = {Ariel Alonso Abad and Fenny Ong and Florian Stijven and Wim Van der Elst and Geert Molenberghs and Ingrid Van Keilegom and Geert Verbeke and Andrea Callegaro},
  doi          = {10.1002/sim.9997},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1083-1102},
  shortjournal = {Stat. Med.},
  title        = {An information-theoretic approach for the assessment of a continuous outcome as a surrogate for a binary true endpoint based on causal inference: Application to vaccine evaluation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling multiple correlated end-organ disease trajectories:
A tutorial for multistate and joint models with applications in diabetes
complications. <em>SIM</em>, <em>43</em>(5), 1048–1082. (<a
href="https://doi.org/10.1002/sim.9984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art biostatistics methods allow for the simultaneous modeling of several correlated non-fatal disease processes over time, but there is no clear guidance on the optimal analysis in most settings. An example occurs in diabetes, where it is not known with certainty how microvascular complications of the eyes, kidneys, and nerves co-develop over time. In this article, we propose and contrast two general model frameworks for studying complications (sequential state and parallel trajectory frameworks) and review multivariate methods for their analysis, focusing on multistate and joint modeling. We illustrate these methods in a tutorial format using the long-term follow-up from the Diabetes Control and Complications Trial and Epidemiology of Diabetes Interventions and Complications study public data repository. A formal comparison of prediction error and discrimination is included. Multistate models are particularly advantageous for determining the order and timing of complications, but require discretization of the longitudinal outcomes and possibly a very complex state space process. Intermittent observation of the states must be accounted for, and discretization is a probable disadvantage in this setting. In contrast, joint models can account for variations of continuous biomarkers over time and are particularly designed for modeling complex association structures between the complications and for performing dynamic predictions of an outcome of interest to inform clinical decisions (eg, a late-stage complication). We found that both models have helpful features that can better-inform our understanding of the complex trajectories that complications may take and can therefore help with decision making for patients presenting with diabetes complications.},
  archive      = {J_SIM},
  author       = {Leif Erik Lovblom and Laurent Briollais and Bruce A. Perkins and George Tomlinson},
  doi          = {10.1002/sim.9984},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1048-1082},
  shortjournal = {Stat. Med.},
  title        = {Modeling multiple correlated end-organ disease trajectories: A tutorial for multistate and joint models with applications in diabetes complications},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fetal growth analysis from ultrasound videos based on
different biometrics using optimal segmentation and hybrid classifier.
<em>SIM</em>, <em>43</em>(5), 1019–1047. (<a
href="https://doi.org/10.1002/sim.9995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Birth defects and their associated deaths, high health and financial costs of maternal care and associated morbidity are major contributors to infant mortality. If permitted by law, prenatal diagnosis allows for intrauterine care, more complicated hospital deliveries, and termination of pregnancy. During pregnancy, a set of measurements is commonly used to monitor the fetal health, including fetal head circumference, crown-rump length, abdominal circumference, and femur length. Because of the intricate interactions between the biological tissues and the US waves mother and fetus, analyzing fetal US images from a specialized perspective is difficult. Artifacts include acoustic shadows, speckle noise, motion blur, and missing borders. The fetus moves quickly, body structures close, and the weeks of pregnancy vary greatly. In this work, we propose a fetal growth analysis through US image of head circumference biometry using optimal segmentation and hybrid classifier. First, we introduce a hybrid whale with oppositional fruit fly optimization (WOFF) algorithm for optimal segmentation of segment fetal head which improves the detection accuracy. Next, an improved U-Net design is utilized for the hidden feature (head circumference biometry) extraction which extracts features from the segmented extraction. Then, we design a modified Boosting arithmetic optimization (MBAO) algorithm for feature optimization to selects optimal best features among multiple features for the reduction of data dimensionality issues. Furthermore, a hybrid deep learning technique called bi-directional LSTM with convolutional neural network (B-LSTM-CNN) for fetal growth analysis to compute the fetus growth and health. Finally, we validate our proposed method through the open benchmark datasets are HC18 (Ultrasound image) and oxford university research archive (ORA-data) (Ultrasound video frames). We compared the simulation results of our proposed algorithm with the existing state-of-art techniques in terms of various metrics.},
  archive      = {J_SIM},
  author       = {B. Devisri and M. Kavitha},
  doi          = {10.1002/sim.9995},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1019-1047},
  shortjournal = {Stat. Med.},
  title        = {Fetal growth analysis from ultrasound videos based on different biometrics using optimal segmentation and hybrid classifier},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing risk factors for post-acute recovery in older
adults with alzheimer’s disease and related dementia: A new
semi-parametric model for large-scale medicare claims. <em>SIM</em>,
<em>43</em>(5), 1003–1018. (<a
href="https://doi.org/10.1002/sim.9982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearly 300,000 older adults experience a hip fracture every year, the majority of which occur following a fall. Unfortunately, recovery after fall-related trauma such as hip fracture is poor, where older adults diagnosed with Alzheimer&#39;s disease and related dementia (ADRD) spend a particularly long time in hospitals or rehabilitation facilities during the post-operative recuperation period. Because older adults value functional recovery and spending time at home versus facilities as key outcomes after hospitalization, identifying factors that influence days spent at home after hospitalization is imperative. While several individual-level factors have been identified, the characteristics of the treating hospital have recently been identified as contributors. However, few methodological rigorous approaches are available to help overcome potential sources of bias such as hospital-level unmeasured confounders, informative hospital size, and loss to follow-up due to death. This article develops a useful tool equipped with unsupervised learning to simultaneously handle statistical complexities that are often encountered in health services research, especially when using large administrative claims databases. The proposed estimator has a closed form, thus only requiring light computation load in a large-scale study. We further develop its asymptotic properties with stabilized inference assisted by unsupervised clustering. Extensive simulation studies demonstrate superiority of the proposed estimator compared to existing estimators.},
  archive      = {J_SIM},
  author       = {Biyi Shen and Haoyu Ren and Michelle Shardell and Jason Falvey and Chixiang Chen},
  doi          = {10.1002/sim.9982},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1003-1018},
  shortjournal = {Stat. Med.},
  title        = {Analyzing risk factors for post-acute recovery in older adults with alzheimer&#39;s disease and related dementia: A new semi-parametric model for large-scale medicare claims},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated bayesian framework for multi-omics prediction
and classification. <em>SIM</em>, <em>43</em>(5), 983–1002. (<a
href="https://doi.org/10.1002/sim.9953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing commonality of multi-omics datasets, there is now increasing evidence that integrated omics profiles lead to more efficient discovery of clinically actionable biomarkers that enable better disease outcome prediction and patient stratification. Several methods exist to perform host phenotype prediction from cross-sectional, single-omics data modalities but decentralized frameworks that jointly analyze multiple time-dependent omics data to highlight the integrative and dynamic impact of repeatedly measured biomarkers are currently limited. In this article, we propose a novel Bayesian ensemble method to consolidate prediction by combining information across several longitudinal and cross-sectional omics data layers. Unlike existing frequentist paradigms, our approach enables uncertainty quantification in prediction as well as interval estimation for a variety of quantities of interest based on posterior summaries. We apply our method to four published multi-omics datasets and demonstrate that it recapitulates known biology in addition to providing novel insights while also outperforming existing methods in estimation, prediction, and uncertainty quantification. Our open-source software is publicly available at https://github.com/himelmallick/IntegratedLearner .},
  archive      = {J_SIM},
  author       = {Himel Mallick and Anupreet Porwal and Satabdi Saha and Piyali Basak and Vladimir Svetnik and Erina Paul},
  doi          = {10.1002/sim.9953},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {983-1002},
  shortjournal = {Stat. Med.},
  title        = {An integrated bayesian framework for multi-omics prediction and classification},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using leave-one-out cross validation (LOO) in a multilevel
regression and poststratification (MRP) workflow: A cautionary tale.
<em>SIM</em>, <em>43</em>(5), 953–982. (<a
href="https://doi.org/10.1002/sim.9964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, multilevel regression and poststratification (MRP) has surged in popularity for population inference. However, the validity of the estimates can depend on details of the model, and there is currently little research on validation. We explore how leave-one-out cross validation (LOO) can be used to compare Bayesian models for MRP. We investigate two approximate calculations of LOO: Pareto smoothed importance sampling (PSIS-LOO) and a survey-weighted alternative (WTD-PSIS-LOO). Using two simulation designs, we examine how accurately these two criteria recover the correct ordering of model goodness at predicting population and small-area estimands. Focusing first on variable selection, we find that neither PSIS-LOO nor WTD-PSIS-LOO correctly recovers the models&#39; order for an MRP population estimand, although both criteria correctly identify the best and worst model. When considering small-area estimation, the best model differs for different small areas, highlighting the complexity of MRP validation. When considering different priors, the models&#39; order seems slightly better at smaller-area levels. These findings suggest that, while not terrible, PSIS-LOO-based ranking techniques may not be suitable to evaluate MRP as a method. We suggest this is due to the aggregation stage of MRP, where individual-level prediction errors average out. We validate these results by applying to the real world National Health and Nutrition Examination Survey (NHANES) data in the United States. Altogether, these results show that PSIS-LOO-based model validation tools need to be used with caution and might not convey the full story when validating MRP as a method.},
  archive      = {J_SIM},
  author       = {Swen Kuh and Lauren Kennedy and Qixuan Chen and Andrew Gelman},
  doi          = {10.1002/sim.9964},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {953-982},
  shortjournal = {Stat. Med.},
  title        = {Using leave-one-out cross validation (LOO) in a multilevel regression and poststratification (MRP) workflow: A cautionary tale},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Properties of the full random-effect modeling approach with
missing covariate data. <em>SIM</em>, <em>43</em>(5), 935–952. (<a
href="https://doi.org/10.1002/sim.9979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During drug development, a key step is the identification of relevant covariates predicting between-subject variations in drug response. The full random effects model (FREM) is one of the full-covariate approaches used to identify relevant covariates in nonlinear mixed effects models. Here we explore the ability of FREM to handle missing (both missing completely at random (MCAR) and missing at random (MAR)) covariate data and compare it to the full fixed-effects model (FFEM) approach, applied either with complete case analysis or mean imputation. A global health dataset (20 421 children) was used to develop a FREM describing the changes of height for age Z-score (HAZ) over time. Simulated datasets (n = 1000) were generated with variable rates of missing (MCAR) covariate data (0%-90%) and different proportions of missing (MAR) data condition on either observed covariates or predicted HAZ. The three methods were used to re-estimate model and compared in terms of bias and precision which showed that FREM had only minor increases in bias and minor loss of precision at increasing percentages of missing (MCAR) covariate data and performed similarly in the MAR scenarios. Conversely, the FFEM approaches either collapsed at 70% of missing (MCAR) covariate data (FFEM complete case analysis) or had large bias increases and loss of precision (FFEM with mean imputation). Our results suggest that FREM is an appropriate approach to covariate modeling for datasets with missing (MCAR and MAR) covariate data, such as in global health studies.},
  archive      = {J_SIM},
  author       = {Joakim Nyberg and E. Niclas Jonsson and Mats O. Karlsson and Jonas Häggström},
  doi          = {10.1002/sim.9979},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {935-952},
  shortjournal = {Stat. Med.},
  title        = {Properties of the full random-effect modeling approach with missing covariate data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dealing with time-dependent exposures and confounding when
defining and estimating attributable fractions—revisiting estimands and
estimators. <em>SIM</em>, <em>43</em>(5), 912–934. (<a
href="https://doi.org/10.1002/sim.9988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The population-attributable fraction (PAF) is commonly interpreted as the proportion of events that can be ascribed to a certain exposure in a certain population. Its estimation is sensitive to common forms of time-dependent bias in the face of a time-dependent exposure. Predominant estimation approaches based on multistate modeling fail to fully eliminate such bias and, as a result, do not permit a causal interpretation, even in the absence of confounding. While recently proposed multistate modeling approaches can successfully eliminate residual time-dependent bias, and moreover succeed to adjust for time-dependent confounding by means of inverse probability of censoring weighting, inadequate application, and misinterpretation prevails in the medical literature. In this paper, we therefore revisit recent work on previously proposed PAF estimands and estimators in settings with time-dependent exposures and competing events and extend this work in several ways. First, we critically revisit the interpretation and applied terminology of these estimands. Second, we further formalize the assumptions under which a causally interpretable PAF estimand can be identified and provide analogous weighting-based representations of the identifying functionals of other proposed estimands. This representation aims to enhance the applied statistician&#39;s understanding of different sources of bias that may arise when the aim is to obtain a valid estimate of a causally interpretable PAF. To illustrate and compare these representations, we present a real-life application to observational data from the Ghent University Hospital ICUs to estimate the fraction of ICU deaths attributable to hospital-acquired infections.},
  archive      = {J_SIM},
  author       = {Johan Steen and Paweł Morzywołek and Wim Van Biesen and Johan Decruyenaere and Stijn Vansteelandt},
  doi          = {10.1002/sim.9988},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {912-934},
  shortjournal = {Stat. Med.},
  title        = {Dealing with time-dependent exposures and confounding when defining and estimating attributable fractions—Revisiting estimands and estimators},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Planning stepped wedge cluster randomized trials to detect
treatment effect heterogeneity. <em>SIM</em>, <em>43</em>(5), 890–911.
(<a href="https://doi.org/10.1002/sim.9990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge design is a popular research design that enables a rigorous evaluation of candidate interventions by using a staggered cluster randomization strategy. While analytical methods were developed for designing stepped wedge trials, the prior focus has been solely on testing for the average treatment effect. With a growing interest on formal evaluation of the heterogeneity of treatment effects across patient subpopulations, trial planning efforts need appropriate methods to accurately identify sample sizes or design configurations that can generate evidence for both the average treatment effect and variations in subgroup treatment effects. To fill in that important gap, this article derives novel variance formulas for confirmatory analyses of treatment effect heterogeneity, that are applicable to both cross-sectional and closed-cohort stepped wedge designs. We additionally point out that the same framework can be used for more efficient average treatment effect analyses via covariate adjustment, and allows the use of familiar power formulas for average treatment effect analyses to proceed. Our results further sheds light on optimal design allocations of clusters to maximize the weighted precision for assessing both the average and heterogeneous treatment effects. We apply the new methods to the Lumbar Imaging with Reporting of Epidemiology Trial, and carry out a simulation study to validate our new methods.},
  archive      = {J_SIM},
  author       = {Fan Li and Xinyuan Chen and Zizhong Tian and Rui Wang and Patrick J. Heagerty},
  doi          = {10.1002/sim.9990},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {890-911},
  shortjournal = {Stat. Med.},
  title        = {Planning stepped wedge cluster randomized trials to detect treatment effect heterogeneity},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating joint confidence region of hypervolume under ROC
manifold and generalized youden index. <em>SIM</em>, <em>43</em>(5),
869–889. (<a href="https://doi.org/10.1002/sim.9998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomarker evaluation/diagnostic studies, the hypervolume under the receiver operating characteristic manifold ( HUM K $$ {\mathrm{HUM}}_K $$ ) and the generalized Youden index ( J K $$ {J}_K $$ ) are the most popular measures for assessing classification accuracy under multiple classes. While HUM K $$ {\mathrm{HUM}}_K $$ is frequently used to evaluate the overall accuracy, J K $$ {J}_K $$ provides direct measure of accuracy at the optimal cut-points. Simultaneous evaluation of HUM K $$ {\mathrm{HUM}}_K $$ and J K $$ {J}_K $$ provides a comprehensive picture about the classification accuracy of the biomarker/diagnostic test under consideration. This article studies both parametric and non-parametric approaches for estimating the confidence region of HUM K $$ {\mathrm{HUM}}_K $$ and J K $$ {J}_K $$ for a single biomarker. The performances of the proposed methods are investigated by an extensive simulation study and are applied to a real data set from the Alzheimer&#39;s Disease Neuroimaging Initiative.},
  archive      = {J_SIM},
  author       = {Jia Wang and Jingjing Yin and Lili Tian},
  doi          = {10.1002/sim.9998},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {869-889},
  shortjournal = {Stat. Med.},
  title        = {Evaluating joint confidence region of hypervolume under ROC manifold and generalized youden index},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating a shrinkage estimator for the treatment effect in
clinical trials. <em>SIM</em>, <em>43</em>(5), 855–868. (<a
href="https://doi.org/10.1002/sim.9992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of most clinical trials is to estimate the effect of some treatment compared to a control condition. We define the signal-to-noise ratio (SNR) as the ratio of the true treatment effect to the SE of its estimate. In a previous publication in this journal, we estimated the distribution of the SNR among the clinical trials in the Cochrane Database of Systematic Reviews (CDSR). We found that the SNR is often low, which implies that the power against the true effect is also low in many trials. Here we use the fact that the CDSR is a collection of meta-analyses to quantitatively assess the consequences. Among trials that have reached statistical significance we find considerable overoptimism of the usual unbiased estimator and under-coverage of the associated confidence interval. Previously, we have proposed a novel shrinkage estimator to address this “winner&#39;s curse.” We compare the performance of our shrinkage estimator to the usual unbiased estimator in terms of the root mean squared error, the coverage and the bias of the magnitude. We find superior performance of the shrinkage estimator both conditionally and unconditionally on statistical significance.},
  archive      = {J_SIM},
  author       = {Erik W. van Zwet and Lu Tian and Robert Tibshirani},
  doi          = {10.1002/sim.9992},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {855-868},
  shortjournal = {Stat. Med.},
  title        = {Evaluating a shrinkage estimator for the treatment effect in clinical trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite sample corrections for average equivalence testing.
<em>SIM</em>, <em>43</em>(5), 833–854. (<a
href="https://doi.org/10.1002/sim.9993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Average (bio)equivalence tests are used to assess if a parameter, like the mean difference in treatment response between two conditions for example, lies within a given equivalence interval, hence allowing to conclude that the conditions have “equivalent” means. The two one-sided tests (TOST) procedure, consisting in testing whether the target parameter is respectively significantly greater and lower than some pre-defined lower and upper equivalence limits, is typically used in this context, usually by checking whether the confidence interval for the target parameter lies within these limits. This intuitive and visual procedure is however known to be conservative, especially in the case of highly variable drugs, where it shows a rapid power loss, often reaching zero, hence making it impossible to conclude for equivalence when it is actually true. Here, we propose a finite sample correction of the TOST procedure, the -TOST, which consists in a correction of the significance level of the TOST allowing to guarantee a test size (or type-I error rate) of . This new procedure essentially corresponds to a finite sample and variability correction of the TOST procedure. We show that this procedure is uniformly more powerful than the TOST, easy to compute, and that its operating characteristics outperform the ones of its competitors. A case study about econazole nitrate deposition in porcine skin is used to illustrate the benefits of the proposed method and its advantages compared to other available procedures.},
  archive      = {J_SIM},
  author       = {Younes Boulaguiem and Julie Quartier and Maria Lapteva and Yogeshvar N. Kalia and Maria-Pia Victoria-Feser and Stéphane Guerrier and Dominique-Laurent Couturier},
  doi          = {10.1002/sim.9993},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {833-854},
  shortjournal = {Stat. Med.},
  title        = {Finite sample corrections for average equivalence testing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling basal body temperature data using horseshoe process
regression. <em>SIM</em>, <em>43</em>(5), 817–832. (<a
href="https://doi.org/10.1002/sim.9991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical data often exhibit jumps or abrupt changes. For example, women&#39;s basal body temperature may jump at ovulation, menstruation, implantation, and miscarriage. These sudden changes make these data challenging to model: many methods will oversmooth the sharp changes or overfit in response to measurement error. We develop horseshoe process regression (HPR) to address this problem. We define a horseshoe process as a stochastic process in which each increment is horseshoe-distributed. We use the horseshoe process as a nonparametric Bayesian prior for modeling a potentially nonlinear association between an outcome and its continuous predictor, which we implement via Stan and in the R package HPR. We provide guidance and extensions to advance HPR&#39;s use in applied practice: we introduce a Bayesian imputation scheme to allow for interpolation at unobserved values of the predictor within the HPR; include additional covariates via a partial linear model framework; and allow for monotonicity constraints. We find that HPR performs well when fitting functions that have sharp changes. We apply HPR to model women&#39;s basal body temperatures over the course of the menstrual cycle.},
  archive      = {J_SIM},
  author       = {Elizabeth C. Chase and Jeremy M. G. Taylor and Philip S. Boonstra},
  doi          = {10.1002/sim.9991},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {817-832},
  shortjournal = {Stat. Med.},
  title        = {Modeling basal body temperature data using horseshoe process regression},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusing trial data for treatment comparisons: Single vs
multi-span bridging. <em>SIM</em>, <em>43</em>(4), 793–815. (<a
href="https://doi.org/10.1002/sim.9989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While randomized controlled trials (RCTs) are critical for establishing the efficacy of new therapies, there are limitations regarding what comparisons can be made directly from trial data. RCTs are limited to a small number of comparator arms and often compare a new therapeutic to a standard of care which has already proven efficacious. It is sometimes of interest to estimate the efficacy of the new therapy relative to a treatment that was not evaluated in the same trial, such as a placebo or an alternative therapy that was evaluated in a different trial. Such dual-study comparisons are challenging because of potential differences between trial populations that can affect the outcome. In this article, two bridging estimators are considered that allow for comparisons of treatments evaluated in different trials, accounting for measured differences in trial populations. A “multi-span” estimator leverages a shared arm between two trials, while a “single-span” estimator does not require a shared arm. A diagnostic statistic that compares the outcome in the standardized shared arms is provided. The two estimators are compared in simulations, where both estimators demonstrate minimal empirical bias and nominal confidence interval coverage when the identification assumptions are met. The estimators are applied to data from the AIDS Clinical Trials Group 320 and 388 to compare the efficacy of two-drug vs four-drug antiretroviral therapy on CD4 cell counts among persons with advanced HIV. The single-span approach requires weaker identification assumptions and was more efficient in simulations and the application.},
  archive      = {J_SIM},
  author       = {Bonnie E. Shook-Sa and Paul N. Zivich and Samuel P. Rosin and Jessie K. Edwards and Adaora A. Adimora and Michael G. Hudgens and Stephen R. Cole},
  doi          = {10.1002/sim.9989},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {793-815},
  shortjournal = {Stat. Med.},
  title        = {Fusing trial data for treatment comparisons: Single vs multi-span bridging},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible evaluation of surrogate markers with bayesian model
averaging. <em>SIM</em>, <em>43</em>(4), 774–792. (<a
href="https://doi.org/10.1002/sim.9986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When long-term follow up is required for a primary endpoint in a randomized clinical trial, a valid surrogate marker can help to estimate the treatment effect and accelerate the decision process. Several model-based methods have been developed to evaluate the proportion of the treatment effect that is explained by the treatment effect on the surrogate marker. More recently, a nonparametric approach has been proposed allowing for more flexibility by avoiding the restrictive parametric model assumptions required in the model-based methods. While the model-based approaches suffer from potential mis-specification of the models, the nonparametric method fails to give desirable estimates when the sample size is small, or when the range of the data does not follow certain conditions. In this paper, we propose a Bayesian model averaging approach to estimate the proportion of treatment effect explained by the surrogate marker. Our procedure offers a compromise between the model-based approach and the nonparametric approach by introducing model flexibility via averaging over several candidate models and maintains the strength of parametric models with respect to inference. We compare our approach with previous model-based methods and the nonparametric method. Simulation studies demonstrate the advantage of our method when surrogate supports are inconsistent and sample sizes are small. We illustrate our method using data from the Diabetes Prevention Program study to examine hemoglobin A1c as a surrogate marker for fasting glucose.},
  archive      = {J_SIM},
  author       = {Yunshan Duan and Layla Parast},
  doi          = {10.1002/sim.9986},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {774-792},
  shortjournal = {Stat. Med.},
  title        = {Flexible evaluation of surrogate markers with bayesian model averaging},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multistep estimators of the between-study covariance matrix
under the multivariate random-effects model for meta-analysis.
<em>SIM</em>, <em>43</em>(4), 756–773. (<a
href="https://doi.org/10.1002/sim.9985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wide variety of methods are available to estimate the between-study variance under the univariate random-effects model for meta-analysis. Some, but not all, of these estimators have been extended so that they can be used in the multivariate setting. We begin by extending the univariate generalised method of moments, which immediately provides a wider class of multivariate methods than was previously available. However, our main proposal is to use this new type of estimator to derive multivariate multistep estimators of the between-study covariance matrix. We then use the connection between the univariate multistep and Paule–Mandel estimators to motivate taking the limit, where the number of steps tends toward infinity. We illustrate our methodology using two contrasting examples and investigate its properties in a simulation study. We conclude that the proposed methodology is a fully viable alternative to existing estimation methods, is well suited to sensitivity analyses that explore the use of alternative estimators, and should be used instead of the existing DerSimonian and Laird-type moments based estimator in application areas where data are expected to be heterogeneous. However, multistep estimators do not seem to outperform the existing estimators when the data are more homogeneous. Advantages of the new multivariate multistep estimator include its semi-parametric nature and that it is computationally feasible in high dimensions. Our proposed estimation methods are also applicable for multivariate random-effects meta-regression, where study-level covariates are included in the model.},
  archive      = {J_SIM},
  author       = {Dan Jackson and Wolfgang Viechtbauer and Robbie C. M. van Aert},
  doi          = {10.1002/sim.9985},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {756-773},
  shortjournal = {Stat. Med.},
  title        = {Multistep estimators of the between-study covariance matrix under the multivariate random-effects model for meta-analysis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining mendelian randomization with the sibling
comparison design. <em>SIM</em>, <em>43</em>(4), 731–755. (<a
href="https://doi.org/10.1002/sim.9983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) is a popular epidemiologic study design that uses genetic variants as instrumental variables (IVs) to estimate causal effects, while accounting for unmeasured confounding. The validity of the MR design hinges on certain IV assumptions, which may sometimes be violated due to dynastic effects, population stratification, or assortative mating. Since these mechanisms act through parental factors it was recently suggested that the bias resulting from violations of the IV assumptions can be reduced by combing the MR design with the sibling comparison design, which implicitly controls for all factors that are constant within families. In this article, we provide a formal discussion of this combined MR-sibling design. We derive conditions under which the MR-sibling design is unbiased, and we relate these to the corresponding conditions for the standard MR and sibling comparison designs. We proceed by considering scenarios where all three designs are biased to some extent, and discuss under which conditions the MR-sibling design can be expected to have less bias than the other two designs. We finally illustrate the theoretical results and conclusions with an application to real data, in a study of low-density lipoprotein and diastolic blood pressure using data from the Swedish Twin Registry.},
  archive      = {J_SIM},
  author       = {Arvid Sjölander and Thomas Frisell and Sara Öberg and Yunzhang Wang and Sara Hägg},
  doi          = {10.1002/sim.9983},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {731-755},
  shortjournal = {Stat. Med.},
  title        = {Combining mendelian randomization with the sibling comparison design},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric estimation of the random effects distribution
for the risk or rate ratio in rare events meta-analysis with the
arm-based and contrast-based approaches. <em>SIM</em>, <em>43</em>(4),
706–730. (<a href="https://doi.org/10.1002/sim.9981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rare events are events which occur with low frequencies. These often arise in clinical trials or cohort studies where the data are arranged in binary contingency tables. In this article, we investigate the estimation of effect heterogeneity for the risk-ratio parameter in meta-analysis of rare events studies through two likelihood-based nonparametric mixture approaches: an arm-based and a contrast-based model. Maximum likelihood estimation is achieved using the EM algorithm. Special attention is given to the choice of initial values. Inspired by the classification likelihood, a strategy is implemented which repeatably uses random allocation of the studies to the mixture components as choice of initial values. The likelihoods under the contrast-based and arm-based approaches are compared and differences are highlighted. We use simulations to assess the performance of these two methods. Under the design of sampling studies with nested treatment groups, the results show that the nonparametric mixture model based on the contrast-based approach is more appropriate in terms of model selection criteria such as AIC and BIC. Under the arm-based design the results from the arm-based model performs well although in some cases it is also outperformed by the contrast-based model. Comparisons of the estimators are provided in terms of bias and mean squared error. Also included in the comparison is the mixed Poisson regression model as well as the classical DerSimonian-Laird model (using the Mantel-Haenszel estimator for the common effect). Using simulation, estimating effect heterogeneity in the case of the contrast-based method appears to behave better than the compared methods although differences become negligible for large within-study sample sizes. We illustrate the methodologies using several meta-analytic data sets in medicine.},
  archive      = {J_SIM},
  author       = {Patarawan Sangnawakij and Dankmar Böhning and Heinz Holling and Katrin Jansen},
  doi          = {10.1002/sim.9981},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {706-730},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric estimation of the random effects distribution for the risk or rate ratio in rare events meta-analysis with the arm-based and contrast-based approaches},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An extended bayesian semi-mechanistic dose-finding design
for phase i oncology trials using pharmacokinetic and pharmacodynamic
information. <em>SIM</em>, <em>43</em>(4), 689–705. (<a
href="https://doi.org/10.1002/sim.9980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model-based, semi-mechanistic dose-finding (SDF) design for phase I oncology trials that incorporates pharmacokinetic/pharmacodynamic (PK/PD) information when modeling the dose-toxicity relationship. This design is motivated by a phase Ib/II clinical trial of anti-CD20/CD3 T cell therapy in non-Hodgkin lymphoma patients; it extends a recently proposed SDF model framework by incorporating measurements of a PD biomarker relevant to the primary dose-limiting toxicity (DLT). We propose joint Bayesian modeling of the PK, PD, and DLT outcomes. Our extensive simulation studies show that on average the proposed design outperforms some common phase I trial designs, including modified toxicity probability interval (mTPI) and Bayesian optimal interval (BOIN) designs, the continual reassessment method (CRM), as well as an SDF design assuming a latent PD biomarker (SDF-woPD), in terms of the percentage of correct selection of maximum tolerated dose (MTD) and average number of patients allocated to MTD, under a variety of dose-toxicity scenarios. When the working PK model and the class of link function between the cumulative PD effect and DLT probability is correctly specified, the proposed design also yields better estimated dose-toxicity curves than CRM and SDF-woPD. Our sensitivity analyses suggest that the design&#39;s performance is reasonably robust to prior specification for the parameter in the link function, as well as misspecification of the PK model and class of the link function.},
  archive      = {J_SIM},
  author       = {Chao Yang and Yisheng Li},
  doi          = {10.1002/sim.9980},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {689-705},
  shortjournal = {Stat. Med.},
  title        = {An extended bayesian semi-mechanistic dose-finding design for phase i oncology trials using pharmacokinetic and pharmacodynamic information},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modeling approaches for censored predictors due to
detection limits with applications to metabolites data. <em>SIM</em>,
<em>43</em>(4), 674–688. (<a
href="https://doi.org/10.1002/sim.9978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measures of substance concentration in urine, serum or other biological matrices often have an assay limit of detection. When concentration levels fall below the limit, exact measures cannot be obtained, and thus are left censored. The problem becomes more challenging when the censored data come from heterogeneous populations consisting of exposed and non-exposed subjects. If the censored data come from non-exposed subjects, their measures are always zero and hence censored, forming a latent class governed by a distinct censoring mechanism compared with the exposed subjects. The exposed group&#39;s censored measurements are always greater than zero, but less than the detection limit. It is very often that the exposed and non-exposed subjects may have different disease traits or different relationships with outcomes of interest, so we need to disentangle the two different populations for valid inference. In this article, we aim to fill the methodological gaps in the literature by developing a novel joint modeling approach to not only address the censoring issue in predictors, but also untangle different relationships of exposed and non-exposed subjects with the outcome. Simulation studies are performed to assess the numerical performance of our proposed approach when the sample size is small to moderate. The joint modeling approach is also applied to examine associations between plasma metabolites and blood pressure in Bogalusa Heart Study, and identify new metabolites that are highly associated with blood pressure.},
  archive      = {J_SIM},
  author       = {Peng Ye and Shuo Bai and Wan Tang and Han Feng and Xinhua Qiao and Shengjia Tu and Hua He},
  doi          = {10.1002/sim.9978},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {674-688},
  shortjournal = {Stat. Med.},
  title        = {Joint modeling approaches for censored predictors due to detection limits with applications to metabolites data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiply robust estimation of natural indirect effects with
multiple ordered mediators. <em>SIM</em>, <em>43</em>(4), 656–673. (<a
href="https://doi.org/10.1002/sim.9977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple mediation analysis is a powerful methodology to assess causal effects in the presence of multiple mediators. Several methodologies, such as G-computation and inverse-probability-weighting, have been widely used to draw inferences about natural indirect effects (NIEs). However, a limitation of these methods is their potential for model misspecification. Although powerful semiparametric methods with high robustness and consistency have been developed for inferring average causal effects and for analyzing the effects of a single mediator, a comparably robust method for multiple mediation analysis is still lacking. Therefore, this theoretical study proposes a method of using multiply robust estimators of NIEs in the presence of multiple ordered mediators. We show that the proposed estimators not only enjoy the multiply robustness to model misspecification, they are also consistent and asymptotically normal under regular conditions. We also performed simulations for empirical comparisons of the finite-sample properties between our multiply robust estimators and existing methods. In an illustrative example, a dataset for liver disease patients in Taiwan is used to examine the mediating roles of liver damage and liver cancer in the pathway from hepatitis B/C virus infection to mortality. The model is implemented in the open-source R package “MedMR.”},
  archive      = {J_SIM},
  author       = {An-Shun Tai and Sheng-Hsuan Lin},
  doi          = {10.1002/sim.9977},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {656-673},
  shortjournal = {Stat. Med.},
  title        = {Multiply robust estimation of natural indirect effects with multiple ordered mediators},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Association analysis of self-reported outcomes with a
validated subset. <em>SIM</em>, <em>43</em>(4), 642–655. (<a
href="https://doi.org/10.1002/sim.9976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In health-science research, outcomes ascertained through surveys and interviews are subject to potential bias with respect to the true outcome status, which is only ascertainable with clinical and laboratory assessment. This measurement error may lead to biased inference when evaluating associations between exposures and outcomes of interest. Here, we consider a cohort study in which the outcome of interest is ascertained via questionnaire, subject to imperfect ascertainment, but where a subset of participants also have a clinically assessed, validated outcome available. This presents a methodological opportunity to address potential bias. Specifically, we constructed the likelihood in two parts, one using the validated subset and the other using a subset without validation. This work expands on that proposed by Pepe and enables inference with standard statistical software. Weighted generalized linear model estimates for our method and maximum likelihood estimates (MLE) for Pepe&#39;s method were computed, and the statistical inference was based on the standard large-sample likelihood theory. We compare the finite sample performance of two approaches through Monte Carlo simulations. This methodological work was motivated by a large cohort study of long-term childhood cancer survivors, allowing us to provide a relevant application example where we examined the association between clinical factors and chronic health conditions.},
  archive      = {J_SIM},
  author       = {Sedigheh Mirzaei and José M. Martínez and Eric J. Chow and Kirsten K. Ness and Melissa M. Hudson and Gregory T. Armstrong and Yutaka Yasui},
  doi          = {10.1002/sim.9976},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {642-655},
  shortjournal = {Stat. Med.},
  title        = {Association analysis of self-reported outcomes with a validated subset},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference with large-scale trait imputation.
<em>SIM</em>, <em>43</em>(4), 625–641. (<a
href="https://doi.org/10.1002/sim.9975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently a nonparametric method called LS-imputation has been proposed for large-scale trait imputation based on a GWAS summary dataset and a large set of genotyped individuals. The imputed trait values, along with the genotypes, can be treated as an individual-level dataset for downstream genetic analyses, including those that cannot be done with GWAS summary data. However, since the covariance matrix of the imputed trait values is often too large to calculate, the current method imposes a working assumption that the imputed trait values are identically and independently distributed, which is incorrect in truth. Here we propose a “divide and conquer/combine” strategy to estimate and account for the covariance matrix of the imputed trait values via batches, thus relaxing the incorrect working assumption. Applications of the methods to the UK Biobank data for marginal association analysis showed some improvement by the new method in some cases, but overall the original method performed well, which was explained by nearly constant variances of and mostly weak correlations among imputed trait values.},
  archive      = {J_SIM},
  author       = {Jingchen Ren and Wei Pan},
  doi          = {10.1002/sim.9975},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {625-641},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference with large-scale trait imputation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cutoff estimation and construction of their confidence
intervals for continuous biomarkers under ternary umbrella and tree
stochastic ordering settings. <em>SIM</em>, <em>43</em>(3), 606–623. (<a
href="https://doi.org/10.1002/sim.9974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuberculosis (TB) studies often involve four different states under consideration, namely, “healthy,” “latent infection,” “pulmonary active disease,” and “extra-pulmonary active disease.” While highly accurate clinical diagnosis tests do exist, they are expensive and generally not accessible in regions where they are most needed; thus, there is an interest in assessing the accuracy of new and easily obtainable biomarkers. For some such biomarkers, the typical stochastic ordering assumption might not be justified for all disease classes under study, and usual ROC methodologies that involve ROC surfaces and hypersurfaces are inadequate. Different types of orderings may be appropriate depending on the setting, and these may involve a number of ambiguously ordered groups that stochastically exhibit larger (or lower) marker scores than the remaining groups. Recently, there has been scientific interest on ROC methods that can accommodate these so-called “tree” or “umbrella” orderings. However, there is limited work discussing the estimation of cutoffs in such settings. In this article, we discuss the estimation and inference around optimized cutoffs when accounting for such configurations. We explore different cutoff alternatives and provide parametric, flexible parametric, and non-parametric kernel-based approaches for estimation and inference. We evaluate our approaches using simulations and illustrate them through a real data set that involves TB patients.},
  archive      = {J_SIM},
  author       = {Benjamin C. Brewer and Leonidas E. Bantis},
  doi          = {10.1002/sim.9974},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {606-623},
  shortjournal = {Stat. Med.},
  title        = {Cutoff estimation and construction of their confidence intervals for continuous biomarkers under ternary umbrella and tree stochastic ordering settings},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accommodating misclassification effects on optimizing
dynamic treatment regimes with q-learning. <em>SIM</em>, <em>43</em>(3),
578–605. (<a href="https://doi.org/10.1002/sim.9973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on dynamic treatment regimes has enticed extensive interest. Many methods have been proposed in the literature, which, however, are vulnerable to the presence of misclassification in covariates. In particular, although Q-learning has received considerable attention, its applicability to data with misclassified covariates is unclear. In this article, we investigate how ignoring misclassification in binary covariates can impact the determination of optimal decision rules in randomized treatment settings, and demonstrate its deleterious effects on Q-learning through empirical studies. We present two correction methods to address misclassification effects on Q-learning. Numerical studies reveal that misclassification in covariates induces non-negligible estimation bias and that the correction methods successfully ameliorate bias in parameter estimation.},
  archive      = {J_SIM},
  author       = {Yasin Khadem Charvadeh and Grace Y. Yi},
  doi          = {10.1002/sim.9973},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {578-605},
  shortjournal = {Stat. Med.},
  title        = {Accommodating misclassification effects on optimizing dynamic treatment regimes with Q-learning},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian inference for prediction of survival probability in
prime-boost vaccination regimes. <em>SIM</em>, <em>43</em>(3), 560–577.
(<a href="https://doi.org/10.1002/sim.9972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on Bayesian inference for survival probabilities in a prime-boost vaccination regime in the development of an Ebola vaccine. We are interested in the heterologous prime-boost regimen (unmatched vaccine deliverys using the same antigen) due to its demonstrated durable immunity, well-tolerated safety profile, and suitability as a population vaccination strategy. Our research is motivated by the need to estimate the survival probability given the administered dosage. To do so, we establish two key relationships. Firstly, we model the connection between the designed dose concentration and the induced antibody count using a Bayesian response surface model. Secondly, we model the association between the antibody count and the probability of survival when experimental subjects are exposed to the Ebola virus in a controlled setting using a Bayesian probability of survival model. Finally, we employ a combination of the two models with dose concentration as the predictor of the survival probability for a future vaccinated population. We implement our two-level Bayesian model in Stan, and illustrate its use with simulated and real-world data. Performance of this model is evaluated via simulation. Our work offers a new application of drug synergy models to examine prime-boost vaccine efficacy, and does so using a hierarchical Bayesian framework that allows us to use dose concentration to predict survival probability.},
  archive      = {J_SIM},
  author       = {Yuelin Lu and Bradley P. Carlin and John W. Seaman},
  doi          = {10.1002/sim.9972},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {560-577},
  shortjournal = {Stat. Med.},
  title        = {Bayesian inference for prediction of survival probability in prime-boost vaccination regimes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian hierarchical modeling in interim futility analysis
for two parallel clinical trials. <em>SIM</em>, <em>43</em>(3), 548–559.
(<a href="https://doi.org/10.1002/sim.9971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating interim analysis into a trial design is gaining popularity in the field of confirmatory clinical trials, where two studies may be conducted in parallel (ie, twin studies) in order to provide substantial evidence per the requirement of FDA guidance. Interim futility analysis provides a chance to check for the &quot;disaster&quot; scenario when the treatment has a high probability to be not more efficacious than the control. Therefore, it is an efficient tool to mitigate risk of running a complete and expansive trial under such scenario. There is no agreement among trial designers that interim analysis should be based on individual study data or pooled data under the twin study scenario. In fact, it is a dilemma for most scientists when specifying the interim analysis strategy at the design stage as the true treatment effects of the twin studies are unknown no matter how similar they are intended to be. To address the issue, we developed a Bayesian hierarchical modeling method to allow dynamic data borrowing between twin studies and demonstrated a favorable characteristic of the new method over the separate and pooled analyses. We evaluated a wide spectrum of the heterogeneity hyperparameters and visualized its critical impact on the Bayesian model&#39;s characteristic. Based on the evaluation, we made a suggestion on the heterogeneity hyperparameter selection independent of any a priori knowledge. We also applied our method to a case study where predictive powers of different methods are compared.},
  archive      = {J_SIM},
  author       = {Hao Li and Dooti Roy and Qiqi Deng},
  doi          = {10.1002/sim.9971},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {548-559},
  shortjournal = {Stat. Med.},
  title        = {Bayesian hierarchical modeling in interim futility analysis for two parallel clinical trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse probability of treatment weighting with generalized
linear outcome models for doubly robust estimation. <em>SIM</em>,
<em>43</em>(3), 534–547. (<a
href="https://doi.org/10.1002/sim.9969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are now many options for doubly robust estimation; however, there is a concerning trend in the applied literature to believe that the combination of a propensity score and an adjusted outcome model automatically results in a doubly robust estimator and/or to misuse more complex established doubly robust estimators. A simple alternative, canonical link generalized linear models (GLM) fit via inverse probability of treatment (propensity score) weighted maximum likelihood estimation followed by standardization (the g $$ g $$ -formula) for the average causal effect, is a doubly robust estimation method. Our aim is for the reader not just to be able to use this method, which we refer to as IPTW GLM, for doubly robust estimation, but to fully understand why it has the doubly robust property. For this reason, we define clearly, and in multiple ways, all concepts needed to understand the method and why it is doubly robust. In addition, we want to make very clear that the mere combination of propensity score weighting and an adjusted outcome model does not generally result in a doubly robust estimator. Finally, we hope to dispel the misconception that one can adjust for residual confounding remaining after propensity score weighting by adjusting in the outcome model for what remains ‘unbalanced’ even when using doubly robust estimators. We provide R code for our simulations and real open-source data examples that can be followed step-by-step to use and hopefully understand the IPTW GLM method. We also compare to a much better-known but still simple doubly robust estimator.},
  archive      = {J_SIM},
  author       = {Erin E. Gabriel and Michael C. Sachs and Torben Martinussen and Ingeborg Waernbaum and Els Goetghebeur and Stijn Vansteelandt and Arvid Sjölander},
  doi          = {10.1002/sim.9969},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {534-547},
  shortjournal = {Stat. Med.},
  title        = {Inverse probability of treatment weighting with generalized linear outcome models for doubly robust estimation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple imputation of incomplete multilevel data using
heckman selection models. <em>SIM</em>, <em>43</em>(3), 514–533. (<a
href="https://doi.org/10.1002/sim.9965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is a common problem in medical research, and is commonly addressed using multiple imputation. Although traditional imputation methods allow for valid statistical inference when data are missing at random (MAR), their implementation is problematic when the presence of missingness depends on unobserved variables, that is, the data are missing not at random (MNAR). Unfortunately, this MNAR situation is rather common, in observational studies, registries and other sources of real-world data. While several imputation methods have been proposed for addressing individual studies when data are MNAR, their application and validity in large datasets with multilevel structure remains unclear. We therefore explored the consequence of MNAR data in hierarchical data in-depth, and proposed a novel multilevel imputation method for common missing patterns in clustered datasets. This method is based on the principles of Heckman selection models and adopts a two-stage meta-analysis approach to impute binary and continuous variables that may be outcomes or predictors and that are systematically or sporadically missing. After evaluating the proposed imputation model in simulated scenarios, we illustrate it use in a cross-sectional community survey to estimate the prevalence of malaria parasitemia in children aged 2-10 years in five regions in Uganda.},
  archive      = {J_SIM},
  author       = {Johanna Muñoz and Orestis Efthimiou and Vincent Audigier and Valentijn M. T. de Jong and Thomas P. A. Debray},
  doi          = {10.1002/sim.9965},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {514-533},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation of incomplete multilevel data using heckman selection models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A flexible multi-metric bayesian framework for
decision-making in phase II multi-arm multi-stage studies. <em>SIM</em>,
<em>43</em>(3), 501–513. (<a
href="https://doi.org/10.1002/sim.9961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multi-metric flexible Bayesian framework to support efficient interim decision-making in multi-arm multi-stage phase II clinical trials. Multi-arm multi-stage phase II studies increase the efficiency of drug development, but early decisions regarding the futility or desirability of a given arm carry considerable risk since sample sizes are often low and follow-up periods may be short. Further, since intermediate outcomes based on biomarkers of treatment response are rarely perfect surrogates for the primary outcome and different trial stakeholders may have different levels of risk tolerance, a single hypothesis test is insufficient for comprehensively summarizing the state of the collected evidence. We present a Bayesian framework comprised of multiple metrics based on point estimates, uncertainty, and evidence towards desired thresholds (a Target Product Profile) for (1) ranking of arms and (2) comparison of each arm against an internal control. Using a large public-private partnership targeting novel TB arms as a motivating example, we find via simulation study that our multi-metric framework provides sufficient confidence for decision-making with sample sizes as low as 30 patients per arm, even when intermediate outcomes have only moderate correlation with the primary outcome. Our reframing of trial design and the decision-making procedure has been well-received by research partners and is a practical approach to more efficient assessment of novel therapeutics.},
  archive      = {J_SIM},
  author       = {Suzanne M. Dufault and Angela M. Crook and Katie Rolfe and Patrick P. J. Phillips},
  doi          = {10.1002/sim.9961},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {501-513},
  shortjournal = {Stat. Med.},
  title        = {A flexible multi-metric bayesian framework for decision-making in phase II multi-arm multi-stage studies},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal weighted bonferroni tests and their graphical
extensions. <em>SIM</em>, <em>43</em>(3), 475–500. (<a
href="https://doi.org/10.1002/sim.9958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regulatory guidelines mandate the strong control of the familywise error rate in confirmatory clinical trials with primary and secondary objectives. Bonferroni tests are one of the popular choices for multiple comparison procedures and are building blocks of more advanced procedures. It is usually of interest to find the optimal weighted Bonferroni split for multiple hypotheses. We consider two popular quantities as the optimization objectives, which are the disjunctive power and the conjunctive power. The former is the probability to reject at least one false hypothesis and the latter is the probability to reject all false hypotheses. We investigate the behavior of each of them as a function of different Bonferroni splits, given assumptions about the alternative hypotheses and correlations between test statistics. Under independent tests, unique optimal Bonferroni weights exist; under dependence, optimal Bonferroni weights may not be unique based on a fine grid search. In general, we propose an optimization algorithm based on constrained nonlinear optimization and multiple starting points. The proposed algorithm efficiently identifies optimal Bonferroni weights to maximize the disjunctive or conjunctive power. In addition, we apply the proposed algorithm to graphical approaches, which include many Bonferroni-based multiple comparison procedures. Utilizing the closed testing principle, we adopt a two-step approach to find optimal graphs using the disjunctive power. We also identify a class of closed test procedures that optimize the conjunctive power. We apply the proposed algorithm to a case study to illustrate the utility of optimal graphical approaches that reflect study objectives.},
  archive      = {J_SIM},
  author       = {Dong Xi and Yao Chen},
  doi          = {10.1002/sim.9958},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {475-500},
  shortjournal = {Stat. Med.},
  title        = {Optimal weighted bonferroni tests and their graphical extensions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating average treatment effects for clustered RCTs with
recruitment bias. <em>SIM</em>, <em>43</em>(3), 452–474. (<a
href="https://doi.org/10.1002/sim.9957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clustered randomized controlled trials (RCTs), sample recruitment is often conducted after cluster randomization. This timing can lead to recruitment bias if access to the intervention affects the composition of study-eligible cluster entrants and study consenters. This article develops a potential outcomes framework in such settings that yields a causal estimand that pertains to the always-recruited in either research condition. A consistent inverse probability weighting (IPW) estimator is developed using data on recruits only, and a generalized estimating equations approach is used to obtain robust clustered SE estimators that adjust for estimation error in the IPW weights. A simple data collection strategy is discussed to improve the predictive accuracy of the logit propensity score models. Simulations show that the IPW estimator achieves nominal confidence interval coverage under the assumed identification conditions. An empirical application demonstrates the methods using data from an RCT testing the effects of a behavioral health intervention in schools. An R program for estimation is available for download.},
  archive      = {J_SIM},
  author       = {Peter Z. Schochet},
  doi          = {10.1002/sim.9957},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {452-474},
  shortjournal = {Stat. Med.},
  title        = {Estimating average treatment effects for clustered RCTs with recruitment bias},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of the yearly transition function in measles
disease modeling. <em>SIM</em>, <em>43</em>(3), 435–451. (<a
href="https://doi.org/10.1002/sim.9951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globally, there were an estimated 9.8 million measles cases and 207 500 measles deaths in 2019. As the effort to eliminate measles around the world continues, modeling remains a valuable tool for public health decision-makers and program implementers. This study presents a novel approach to the use of a yearly transition function that formulates mathematically the vaccine schedules for different age groups while accounting for the effects of the age of vaccination, the timing of vaccination, and disease seasonality on the yearly number of measles cases in a country. The methodology presented adds to an existing modeling framework and expands its analysis, making its utilization more adjustable for the user and contributing to its conceptual clarity. This article also adjusts for the temporal interaction between vaccination and exposure to disease, applying adjustments to estimated yearly counts of cases and the number of vaccines administered that increase population immunity. These new model features provide the ability to forecast and compare the effects of different vaccination timing scenarios and seasonality of transmission on the expected disease incidence. Although the work presented is applied to the example of measles, it has potential relevance to modeling other vaccine-preventable diseases.},
  archive      = {J_SIM},
  author       = {C. S. Davila-Payan and A. Hill and L. Kayembe and J. P. Alexander and M. Lynch and S. W. Pallas},
  doi          = {10.1002/sim.9951},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {435-451},
  shortjournal = {Stat. Med.},
  title        = {Analysis of the yearly transition function in measles disease modeling},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust estimation of mean-variance relation. <em>SIM</em>,
<em>43</em>(2), 419–434. (<a
href="https://doi.org/10.1002/sim.9970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate assessment of the mean-variance relation can benefit subsequent analysis in biomedical research. However, in most biomedical data, both the true mean and the true variance are unavailable. Instead, raw data are typically used to allow forming sample mean and sample variance in practice. In addition, different experimental conditions sometimes cause a slightly different mean-variance relation from the majority of the data in the same data set. To address these issues, we propose a semiparametric estimator, where we treat the uncertainty in the sample mean as a measurement error problem, the uncertainty in the sample variance as model error, and use a mixture model to account for different mean-variance relations. Asymptotic normality of the proposed method is established and its finite sample properties are demonstrated by simulation studies. The data application shows that the proposed method produces sensible results compared with methods either ignoring the uncertainty in the sample means or ignoring the potential different mean-variance relations.},
  archive      = {J_SIM},
  author       = {Mushan Li and Yanyuan Ma},
  doi          = {10.1002/sim.9970},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {419-434},
  shortjournal = {Stat. Med.},
  title        = {Robust estimation of mean-variance relation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian safety surveillance with adaptive bias correction.
<em>SIM</em>, <em>43</em>(2), 395–418. (<a
href="https://doi.org/10.1002/sim.9968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Postmarket safety surveillance is an integral part of mass vaccination programs. Typically relying on sequential analysis of real-world health data as they accrue, safety surveillance is challenged by sequential multiple testing and by biases induced by residual confounding in observational data. The current standard approach based on the maximized sequential probability ratio test (MaxSPRT) fails to satisfactorily address these practical challenges and it remains a rigid framework that requires prespecification of the surveillance schedule. We develop an alternative Bayesian surveillance procedure that addresses both aforementioned challenges using a more flexible framework. To mitigate bias, we jointly analyze a large set of negative control outcomes that are adverse events with no known association with the vaccines in order to inform an empirical bias distribution, which we then incorporate into estimating the effect of vaccine exposure on the adverse event of interest through a Bayesian hierarchical model. To address multiple testing and improve on flexibility, at each analysis timepoint, we update a posterior probability in favor of the alternative hypothesis that vaccination induces higher risks of adverse events, and then use it for sequential detection of safety signals. Through an empirical evaluation using six US observational healthcare databases covering more than 360 million patients, we benchmark the proposed procedure against MaxSPRT on testing errors and estimation accuracy, under two epidemiological designs, the historical comparator and the self-controlled case series. We demonstrate that our procedure substantially reduces Type 1 error rates, maintains high statistical power and fast signal detection, and provides considerably more accurate estimation than MaxSPRT. Given the extensiveness of the empirical study which yields more than 7 million sets of results, we present all results in a public R ShinyApp. As an effort to promote open science, we provide full implementation of our method in the open-source R package EvidenceSynthesis .},
  archive      = {J_SIM},
  author       = {Fan Bu and Martijn J. Schuemie and Akihiko Nishimura and Louisa H. Smith and Kristin Kostka and Thomas Falconer and Jody-Ann McLeggon and Patrick B. Ryan and George Hripcsak and Marc A. Suchard},
  doi          = {10.1002/sim.9968},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {395-418},
  shortjournal = {Stat. Med.},
  title        = {Bayesian safety surveillance with adaptive bias correction},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-phase generalized raking and multiple imputation
estimators to address error-prone data. <em>SIM</em>, <em>43</em>(2),
379–394. (<a href="https://doi.org/10.1002/sim.9967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Validation studies are often used to obtain more reliable information in settings with error-prone data. Validated data on a subsample of subjects can be used together with error-prone data on all subjects to improve estimation. In practice, more than one round of data validation may be required, and direct application of standard approaches for combining validation data into analyses may lead to inefficient estimators since the information available from intermediate validation steps is only partially considered or even completely ignored. In this paper, we present two novel extensions of multiple imputation and generalized raking estimators that make full use of all available data. We show through simulations that incorporating information from intermediate steps can lead to substantial gains in efficiency. This work is motivated by and illustrated in a study of contraceptive effectiveness among 83 671 women living with HIV, whose data were originally extracted from electronic medical records, of whom 4732 had their charts reviewed, and a subsequent 1210 also had a telephone interview to validate key study variables.},
  archive      = {J_SIM},
  author       = {Gustavo Amorim and Ran Tao and Sarah Lotspeich and Pamela A. Shaw and Thomas Lumley and Rena C. Patel and Bryan E. Shepherd},
  doi          = {10.1002/sim.9967},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {379-394},
  shortjournal = {Stat. Med.},
  title        = {Three-phase generalized raking and multiple imputation estimators to address error-prone data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing individually randomized group treatment trials
with repeated outcome measurements using generalized estimating
equations. <em>SIM</em>, <em>43</em>(2), 358–378. (<a
href="https://doi.org/10.1002/sim.9966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individually randomized group treatment (IRGT) trials, in which the clustering of outcome is induced by group-based treatment delivery, are increasingly popular in public health research. IRGT trials frequently incorporate longitudinal measurements, of which the proper sample size calculations should account for correlation structures reflecting both the treatment-induced clustering and repeated outcome measurements. Given the relatively sparse literature on designing longitudinal IRGT trials, we propose sample size procedures for continuous and binary outcomes based on the generalized estimating equations approach, employing the block exchangeable correlation structures with different correlation parameters for the treatment arm and for the control arm, and surveying five marginal mean models with different assumptions of time effect: no-time constant treatment effect, linear-time constant treatment effect, categorical-time constant treatment effect, linear time by treatment interaction, and categorical time by treatment interaction. Closed-form sample size formulas are derived for continuous outcomes, which depends on the eigenvalues of the correlation matrices; detailed numerical sample size procedures are proposed for binary outcomes. Through simulations, we demonstrate that the empirical power agrees well with the predicted power, for as few as eight groups formed in the treatment arm, when data are analyzed using the matrix-adjusted estimating equations for the correlation parameters with a bias-corrected sandwich variance estimator.},
  archive      = {J_SIM},
  author       = {Xueqi Wang and Elizabeth L. Turner and Fan Li},
  doi          = {10.1002/sim.9966},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {358-378},
  shortjournal = {Stat. Med.},
  title        = {Designing individually randomized group treatment trials with repeated outcome measurements using generalized estimating equations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage targeted maximum likelihood estimation for mixed
aggregate and individual participant data analysis with an application
to multidrug resistant tuberculosis. <em>SIM</em>, <em>43</em>(2),
342–357. (<a href="https://doi.org/10.1002/sim.9963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we develop a new method for the meta-analysis of mixed aggregate data (AD) and individual participant data (IPD). The method is an adaptation of inverse probability weighted targeted maximum likelihood estimation (IPW-TMLE), which was initially proposed for two-stage sampled data. Our methods are motivated by a systematic review investigating treatment effectiveness for multidrug resistant tuberculosis (MDR-TB) where the available data include IPD from some studies but only AD from others. One complication in this application is that participants with MDR-TB are typically treated with multiple antimicrobial agents where many such medications were not observed in all studies considered in the meta-analysis. We focus here on the estimation of the expected potential outcome while intervening on a specific medication but not intervening on any others. Our method involves the implementation of a TMLE that transports the estimation from studies where the treatment is observed to the full target population. A second weighting component adjusts for the studies with missing (inaccessible) IPD. We demonstrate the properties of the proposed method and contrast it with alternative approaches in a simulation study. We finally apply this method to estimate treatment effectiveness in the MDR-TB case study.},
  archive      = {J_SIM},
  author       = {Arman Alam Siddique and Mireille E. Schnitzer and Narayanaswamy Balakrishnan and Giovanni Sotgiu and Mario H. Vargas and Dick Menzies and Andrea Benedetti},
  doi          = {10.1002/sim.9963},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {342-357},
  shortjournal = {Stat. Med.},
  title        = {Two-stage targeted maximum likelihood estimation for mixed aggregate and individual participant data analysis with an application to multidrug resistant tuberculosis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group sequential two-stage preference designs. <em>SIM</em>,
<em>43</em>(2), 315–341. (<a
href="https://doi.org/10.1002/sim.9962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-stage preference design (TSPD) enables inference for treatment efficacy while allowing for incorporation of patient preference to treatment. It can provide unbiased estimates for selection and preference effects, where a selection effect occurs when patients who prefer one treatment respond differently than those who prefer another, and a preference effect is the difference in response caused by an interaction between the patient&#39;s preference and the actual treatment they receive. One potential barrier to adopting TSPD in practice, however, is the relatively large sample size required to estimate selection and preference effects with sufficient power. To address this concern, we propose a group sequential two-stage preference design (GS-TSPD), which combines TSPD with sequential monitoring for early stopping. In the GS-TSPD, pre-planned sequential monitoring allows investigators to conduct repeated hypothesis tests on accumulated data prior to full enrollment to assess study eligibility for early trial termination without inflating type I error rates. Thus, the procedure allows investigators to terminate the study when there is sufficient evidence of treatment, selection, or preference effects during an interim analysis, thereby reducing the design resource in expectation. To formalize such a procedure, we verify the independent increments assumption for testing the selection and preference effects and apply group sequential stopping boundaries from the approximate sequential density functions. Simulations are then conducted to investigate the operating characteristics of our proposed GS-TSPD compared to the traditional TSPD. We demonstrate the applicability of the design using a study of Hepatitis C treatment modality.},
  archive      = {J_SIM},
  author       = {Ruyi Liu and Fan Li and Denise Esserman and Mary M. Ryan},
  doi          = {10.1002/sim.9962},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {315-341},
  shortjournal = {Stat. Med.},
  title        = {Group sequential two-stage preference designs},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cox regression with linked data. <em>SIM</em>,
<em>43</em>(2), 296–314. (<a
href="https://doi.org/10.1002/sim.9960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Record linkage is increasingly used, especially in medical studies, to combine data from different databases that refer to the same entities. The linked data can bring analysts novel and valuable knowledge that is impossible to obtain from a single database. However, linkage errors are usually unavoidable, regardless of record linkage methods, and ignoring these errors may lead to biased estimates. While different methods have been developed to deal with the linkage errors in the generalized linear model, there is not much interest on Cox regression model, although this is one of the most important statistical models in clinical and epidemiological research. In this work, we propose an adjusted estimating equation for secondary Cox regression analysis, where linked data have been prepared by a third-party operator, and no information on matching variables is available to the analyst. Through a Monte Carlo simulation study, the proposed method is shown to lead to substantial bias reductions in the estimation of the parameters of the Cox model caused by false links. An asymptotically unbiased variance estimator for the adjusted estimators of Cox regression coefficients is also proposed. Finally, the proposed method is applied to a linked database from the Brest stroke registry in France.},
  archive      = {J_SIM},
  author       = {Thanh Huan Vo and Valérie Garès and Li-Chun Zhang and André Happe and Emmanuel Oger and Stéphane Paquelet and Guillaume Chauvet},
  doi          = {10.1002/sim.9960},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {296-314},
  shortjournal = {Stat. Med.},
  title        = {Cox regression with linked data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MERIT: Controlling monte-carlo error rate in large-scale
monte-carlo hypothesis testing. <em>SIM</em>, <em>43</em>(2), 279–295.
(<a href="https://doi.org/10.1002/sim.9959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Monte-Carlo (MC) p $$ p $$ -values when testing the significance of a large number of hypotheses is now commonplace. In large-scale hypothesis testing, we will typically encounter at least some p $$ p $$ -values near the threshold of significance, which require a larger number of MC replicates than p $$ p $$ -values that are far from the threshold. As a result, some incorrect conclusions can be reached due to MC error alone; for hypotheses near the threshold, even a very large number (eg, 1 ⁢ 0 6 $$ 1{0}^6 $$ ) of MC replicates may not be enough to guarantee conclusions reached using MC p $$ p $$ -values. Gandy and Hahn (GH) 6-8 have developed the only method that directly addresses this problem. They defined a Monte-Carlo error rate (MCER) to be the probability that any decisions on accepting or rejecting a hypothesis based on MC -values are different from decisions based on ideal -values; their method then makes decisions by controlling the MCER. Unfortunately, the GH method is frequently very conservative, often making no rejections at all and leaving a large number of hypotheses “undecided”. In this article, we propose MERIT, a method for large-scale MC hypothesis testing that also controls the MCER but is more statistically efficient than the GH method. Through extensive simulation studies, we demonstrate that MERIT controls the MCER while making more decisions that agree with the ideal -values than GH does. We also illustrate our method by an analysis of gene expression data from a prostate cancer study.},
  archive      = {J_SIM},
  author       = {Yunxiao Li and Yi-Juan Hu and Glen A. Satten},
  doi          = {10.1002/sim.9959},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {279-295},
  shortjournal = {Stat. Med.},
  title        = {MERIT: Controlling monte-carlo error rate in large-scale monte-carlo hypothesis testing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embedded multilevel regression and poststratification:
Model-based inference with incomplete auxiliary information.
<em>SIM</em>, <em>43</em>(2), 256–278. (<a
href="https://doi.org/10.1002/sim.9956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health disparity research often evaluates health outcomes across demographic subgroups. Multilevel regression and poststratification (MRP) is a popular approach for small subgroup estimation as it can stabilize estimates by fitting multilevel models and adjust for selection bias by poststratifying on auxiliary variables, which are population characteristics predictive of the analytic outcome. However, the granularity and quality of the estimates produced by MRP are limited by the availability of the auxiliary variables&#39; joint distribution; data analysts often only have access to the marginal distributions. To overcome this limitation, we embed the estimation of population cell counts needed for poststratification into the MRP workflow: embedded MRP (EMRP). Under EMRP, we generate synthetic populations of the auxiliary variables before implementing MRP. All sources of estimation uncertainty are propagated with a fully Bayesian framework. Through simulation studies, we compare different methods of generating the synthetic populations and demonstrate EMRP&#39;s improvements over alternatives on the bias-variance tradeoff to yield valid subpopulation inferences of interest. We apply EMRP to the Longitudinal Survey of Wellbeing and estimate food insecurity prevalence among vulnerable groups in New York City. We find that all EMRP estimators can correct for the bias in classical MRP while maintaining lower standard errors and narrower confidence intervals than directly imputing with the weighted finite population Bayesian bootstrap (WFPBB) and design-based estimates. Performances from the EMRP estimators do not differ substantially from each other, though we would generally recommend using the WFPBB-MRP for its consistently high coverage rates.},
  archive      = {J_SIM},
  author       = {Katherine Li and Yajuan Si},
  doi          = {10.1002/sim.9956},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {256-278},
  shortjournal = {Stat. Med.},
  title        = {Embedded multilevel regression and poststratification: Model-based inference with incomplete auxiliary information},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A flexible model based on piecewise linear approximation for
the analysis of left truncated right censored data with covariates, and
applications to worcester heart attack study data and channing house
data. <em>SIM</em>, <em>43</em>(2), 233–255. (<a
href="https://doi.org/10.1002/sim.9954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Left truncated right censored (LTRC) data arise quite commonly from survival studies. In this article, a model based on piecewise linear approximation is proposed for the analysis of LTRC data with covariates. Specifically, the model involves a piecewise linear approximation for the cumulative baseline hazard function of the proportional hazards model. The principal advantage of the proposed model is that it does not depend on restrictive parametric assumptions while being flexible and data-driven. Likelihood inference for the model is developed. Through detailed simulation studies, the robustness property of the model is studied by fitting it to LTRC data generated from different processes covering a wide range of lifetime distributions. A sensitivity analysis is also carried out by fitting the model to LTRC data generated from a process with a piecewise constant baseline hazard. It is observed that the performance of the model is quite satisfactory in all those cases. Analyses of two real LTRC datasets by using the model are provided as illustrative examples. Applications of the model in some practical prediction issues are discussed. In summary, the proposed model provides a comprehensive and flexible approach to model a general structure for LTRC lifetime data.},
  archive      = {J_SIM},
  author       = {Ayon Ganguly and Debanjan Mitra and Narayanaswamy Balakrishnan and Debasis Kundu},
  doi          = {10.1002/sim.9954},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {233-255},
  shortjournal = {Stat. Med.},
  title        = {A flexible model based on piecewise linear approximation for the analysis of left truncated right censored data with covariates, and applications to worcester heart attack study data and channing house data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference for time-to-event data in
non-randomized cohorts with selective attrition. <em>SIM</em>,
<em>43</em>(2), 216–232. (<a
href="https://doi.org/10.1002/sim.9952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-season clinical trials with a randomize-once strategy, patients enrolled from previous seasons who stay alive and remain in the study will be treated according to the initial randomization in subsequent seasons. To address the potentially selective attrition from earlier seasons for the non-randomized cohorts, we develop an inverse probability of treatment weighting method using season-specific propensity scores to produce unbiased estimates of survival functions or hazard ratios. Bootstrap variance estimators are used to account for the randomness in the estimated weights and the potential correlations in repeated events within each patient from season to season. Simulation studies show that the weighting procedure and bootstrap variance estimator provide unbiased estimates and valid inferences in Kaplan-Meier estimates and Cox proportional hazard models. Finally, data from the INVESTED trial are analyzed to illustrate the proposed method.},
  archive      = {J_SIM},
  author       = {Tuo Wang and Lu Mao and Aldo Cocco and KyungMann Kim},
  doi          = {10.1002/sim.9952},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {216-232},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for time-to-event data in non-randomized cohorts with selective attrition},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating tests for cluster-randomized trials with few
clusters under generalized linear mixed models with covariate
adjustment: A simulation study. <em>SIM</em>, <em>43</em>(2), 201–215.
(<a href="https://doi.org/10.1002/sim.9950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized linear mixed models (GLMM) are commonly used to analyze clustered data, but when the number of clusters is small to moderate, standard statistical tests may produce elevated type I error rates. Small-sample corrections have been proposed for continuous or binary outcomes without covariate adjustment. However, appropriate tests to use for count outcomes or under covariate-adjusted models remains unknown. An important setting in which this issue arises is in cluster-randomized trials (CRTs). Because many CRTs have just a few clusters (eg, clinics or health systems), covariate adjustment is particularly critical to address potential chance imbalance and/or low power (eg, adjustment following stratified randomization or for the baseline value of the outcome). We conducted simulations to evaluate GLMM-based tests of the treatment effect that account for the small (10) or moderate (20) number of clusters under a parallel-group CRT setting across scenarios of covariate adjustment (including adjustment for one or more person-level or cluster-level covariates) for both binary and count outcomes. We find that when the intraclass correlation is non-negligible ( 0.01) and the number of covariates is small ( 2), likelihood ratio tests with a between-within denominator degree of freedom have type I error rates close to the nominal level. When the number of covariates is moderate ( 5), across our simulation scenarios, the relative performance of the tests varied considerably and no method performed uniformly well. Therefore, we recommend adjusting for no more than a few covariates and using likelihood ratio tests with a between-within denominator degree of freedom.},
  archive      = {J_SIM},
  author       = {Hongxiang Qiu and Andrea J. Cook and Jennifer F. Bobb},
  doi          = {10.1002/sim.9950},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {201-215},
  shortjournal = {Stat. Med.},
  title        = {Evaluating tests for cluster-randomized trials with few clusters under generalized linear mixed models with covariate adjustment: A simulation study},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling the multi-state natural history of rare diseases
with heterogeneous individual patient data: A simulation study.
<em>SIM</em>, <em>43</em>(1), 184–200. (<a
href="https://doi.org/10.1002/sim.9949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-state survival models are used to represent the natural history of a disease, forming the basis of a health technology assessment comparing a novel treatment to current practice. Constructing such models for rare diseases is problematic, since evidence sources are typically much sparser and more heterogeneous. This simulation study investigated different one-stage and two-stage approaches to meta-analyzing individual patient data (IPD) in a multi-state survival setting when the number and size of studies being meta-analyzed are small. The objective was to assess methods of different complexity to see when they are accurate, when they are inaccurate and when they struggle to converge due to the sparsity of data. Biologically plausible multi-state IPD were simulated from study- and transition-specific hazard functions. One-stage frailty and two-stage stratified models were estimated, and compared to a base case model that did not account for study heterogeneity. Convergence and the bias/coverage of population-level transition probabilities to, and lengths of stay in, each state were used to assess model performance. A real-world application to Duchenne Muscular Dystrophy, a neuromuscular rare disease, was conducted, and a software demonstration is provided. Models not accounting for study heterogeneity were consistently out-performed by two-stage models. Frailty models struggled to converge, particularly in scenarios of low heterogeneity, and predictions from models that did converge were also subject to bias. Stratified models may be better suited to meta-analyzing disparate sources of IPD in rare disease natural history/economic modeling, as they converge more consistently and produce less biased predictions of lengths of stay.},
  archive      = {J_SIM},
  author       = {Jonathan Broomfield and Keith R. Abrams and Suzanne Freeman and Nicholas Latimer and Mark J. Rutherford and Michael J. Crowther},
  doi          = {10.1002/sim.9949},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {184-200},
  shortjournal = {Stat. Med.},
  title        = {Modeling the multi-state natural history of rare diseases with heterogeneous individual patient data: A simulation study},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian phase II proof-of-concept design for clinical
trials with longitudinal endpoints. <em>SIM</em>, <em>43</em>(1),
173–183. (<a href="https://doi.org/10.1002/sim.9948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing phase II clinical trial designs focus on a single scalar endpoint, such as a binary, continuous, or survival endpoint. In some clinical trials, such as pain management studies, the efficacy endpoint of interest is measured longitudinally. We propose a Bayesian phase II design for such clinical trials. We model the longitudinal measurement process using Bayesian hierarchical model, where subject-specific trajectory shrinks toward the population trajectory to borrow information across subjects. The Bayesian penalized spline is used to model subject-specific and population trajectories without making strong parametric assumption on their shapes. We use the area under the curve of the trajectory as the summary of the treatment effect over time. The design takes a group sequential approach and takes into account both statistical significance and clinical relevance. Bayesian criteria is proposed to make interim and final decisions based on the evidence of statistical significance and clinical relevance. The proposed design is highly flexible and can accommodate trials with one or multiple longitudinal endpoints, as well as a longitudinal primary endpoint with a secondary endpoint. Simulation study shows that the proposed design is robust with desirable operating characteristics.},
  archive      = {J_SIM},
  author       = {Wen Zhang and Glen Laird and Josh Chen and Ying Yuan},
  doi          = {10.1002/sim.9948},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {173-183},
  shortjournal = {Stat. Med.},
  title        = {A bayesian phase II proof-of-concept design for clinical trials with longitudinal endpoints},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A frequentist design for basket trials using adaptive lasso.
<em>SIM</em>, <em>43</em>(1), 156–172. (<a
href="https://doi.org/10.1002/sim.9947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A basket trial aims to expedite the drug development process by evaluating a new therapy in multiple populations within the same clinical trial. Each population, referred to as a “basket”, can be defined by disease type, biomarkers, or other patient characteristics. The objective of a basket trial is to identify the subset of baskets for which the new therapy shows promise. The conventional approach would be to analyze each of the baskets independently. Alternatively, several Bayesian dynamic borrowing methods have been proposed that share data across baskets when responses appear similar. These methods can achieve higher power than independent testing in exchange for a risk of some inflation in the type 1 error rate. In this paper we propose a frequentist approach to dynamic borrowing for basket trials using adaptive lasso. Through simulation studies we demonstrate adaptive lasso can achieve similar power and type 1 error to the existing Bayesian methods. The proposed approach has the benefit of being easier to implement and faster than existing methods. In addition, the adaptive lasso approach is very flexible: it can be extended to basket trials with any number of treatment arms and any type of endpoint.},
  archive      = {J_SIM},
  author       = {Lauren Kanapka and Anastasia Ivanova},
  doi          = {10.1002/sim.9947},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {156-172},
  shortjournal = {Stat. Med.},
  title        = {A frequentist design for basket trials using adaptive lasso},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian compositional generalized linear models for
analyzing microbiome data. <em>SIM</em>, <em>43</em>(1), 141–155. (<a
href="https://doi.org/10.1002/sim.9946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crucial impact of the microbiome on human health and disease has gained significant scientific attention. Researchers seek to connect microbiome features with health conditions, aiming to predict diseases and develop personalized medicine strategies. However, the practicality of conventional models is restricted due to important aspects of microbiome data. Specifically, the data observed is compositional, as the counts within each sample are bound by a fixed-sum constraint. Moreover, microbiome data often exhibits high dimensionality, wherein the number of variables surpasses the available samples. In addition, microbiome features exhibiting phenotypical similarity usually have similar influence on the response variable. To address the challenges posed by these aspects of the data structure, we proposed Bayesian compositional generalized linear models for analyzing microbiome data (BCGLM) with a structured regularized horseshoe prior for the compositional coefficients and a soft sum-to-zero restriction on coefficients through the prior distribution. We fitted the proposed models using Markov Chain Monte Carlo (MCMC) algorithms with R package rstan . The performance of the proposed method was assessed by extensive simulation studies. The simulation results show that our approach outperforms existing methods with higher accuracy of coefficient estimates and lower prediction error. We also applied the proposed method to microbiome study to find microorganisms linked to inflammatory bowel disease (IBD). To make this work reproducible, the code and data used in this article are available at https://github.com/Li-Zhang28/BCGLM .},
  archive      = {J_SIM},
  author       = {Li Zhang and Xinyan Zhang and Nengjun Yi},
  doi          = {10.1002/sim.9946},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {141-155},
  shortjournal = {Stat. Med.},
  title        = {Bayesian compositional generalized linear models for analyzing microbiome data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian zero-inflated beta-binomial model for
longitudinal data with group-specific changepoints. <em>SIM</em>,
<em>43</em>(1), 125–140. (<a
href="https://doi.org/10.1002/sim.9945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timeline followback (TLFB) is often used in addiction research to monitor recent substance use, such as the number of abstinent days in the past week. TLFB data usually take the form of binomial counts that exhibit overdispersion and zero inflation. Motivated by a 12-week randomized trial evaluating the efficacy of varenicline tartrate for smoking cessation among adolescents, we propose a Bayesian zero-inflated beta-binomial model for the analysis of longitudinal, bounded TLFB data. The model comprises a mixture of a point mass that accounts for zero inflation and a beta-binomial distribution for the number of days abstinent in the past week. Because treatment effects appear to level off during the study, we introduce random changepoints for each study group to reflect group-specific changes in treatment efficacy over time. The model also includes fixed and random effects that capture group- and subject-level slopes before and after the changepoints. Using the model, we can accurately estimate the mean trend for each study group, test whether the groups experience changepoints simultaneously, and identify critical windows of treatment efficacy. For posterior computation, we propose an efficient Markov chain Monte Carlo algorithm that relies on easily sampled Gibbs and Metropolis–Hastings steps. Our application shows that the varenicline group has a short-term positive effect on abstinence that tapers off after week 9.},
  archive      = {J_SIM},
  author       = {Chun-Che Wen and Nathaniel Baker and Rajib Paul and Elizabeth Hill and Kelly Hunt and Hong Li and Kevin Gray and Brian Neelon},
  doi          = {10.1002/sim.9945},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {125-140},
  shortjournal = {Stat. Med.},
  title        = {A bayesian zero-inflated beta-binomial model for longitudinal data with group-specific changepoints},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing latent classes in gut microbiome data using
generalized poisson regression models. <em>SIM</em>, <em>43</em>(1),
102–124. (<a href="https://doi.org/10.1002/sim.9944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human microbiome research has gained increasing importance due to its critical roles in comprehending human health and disease. Within the realm of microbiome research, the data generated often involves operational taxonomic unit counts, which can frequently present challenges such as over-dispersion and zero-inflation. To address dispersion-related concerns, the generalized Poisson model offers a flexible solution, effectively handling data characterized by over-dispersion, equi-dispersion, and under-dispersion. Furthermore, the realm of zero-inflated generalized Poisson models provides a strategic avenue to simultaneously tackle both over-dispersion and zero-inflation. The phenomenon of zero-inflation frequently stems from the heterogeneous nature of study populations. It emerges when specific microbial taxa fail to thrive in the microbial community of certain subjects, consequently resulting in a consistent count of zeros for these individuals. This subset of subjects represents a latent class, where their zeros originate from the genuine absence of the microbial taxa. In this paper, we introduce a novel testing methodology designed to uncover such latent classes within generalized Poisson regression models. We establish a closed-form test statistic and deduce its asymptotic distribution based on estimating equations. To assess its efficacy, we conduct an extensive array of simulation studies, and further apply the test to detect latent classes in human gut microbiome data from the Bogalusa Heart Study.},
  archive      = {J_SIM},
  author       = {Xinhui Qiao and Hua He and Liuquan Sun and Shuo Bai and Peng Ye},
  doi          = {10.1002/sim.9944},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {102-124},
  shortjournal = {Stat. Med.},
  title        = {Testing latent classes in gut microbiome data using generalized poisson regression models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A latent variable mixed-effects location scale model that
also considers between-person differences in the autocorrelation.
<em>SIM</em>, <em>43</em>(1), 89–101. (<a
href="https://doi.org/10.1002/sim.9943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In public health research an increasing number of studies is conducted in which intensive longitudinal data is collected in an experience sampling or a daily diary design. Typically, the resulting data is analyzed with a mixed-effects model or mixed-effects location scale model because they allow one to examine a host of interesting longitudinal research questions. Here, we introduce an extension of the mixed-effects location scale model in which measurement error of the observed variables is considered by a latent factor model and in which—in addition to the mean-or location-related effects—the residual variance of the latent factor and the parameters of the autoregressive process of this latent factor can differ between persons. We show how to estimate the parameters of the model with a maximum likelihood approach, whose performance is also compared with a Bayesian approach in a small simulation study. We illustrate the models using a real data example and end with a discussion in which we suggest questions for future research.},
  archive      = {J_SIM},
  author       = {Steffen Nestler and Shelley A. Blozis},
  doi          = {10.1002/sim.9943},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {89-101},
  shortjournal = {Stat. Med.},
  title        = {A latent variable mixed-effects location scale model that also considers between-person differences in the autocorrelation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local false discovery rate estimation with competition-based
procedures for variable selection. <em>SIM</em>, <em>43</em>(1), 61–88.
(<a href="https://doi.org/10.1002/sim.9942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple hypothesis testing has been widely applied to problems dealing with high-dimensional data, for example, the selection of important variables or features from a large number of candidates while controlling the error rate. The most prevailing measure of error rate used in multiple hypothesis testing is the false discovery rate (FDR). In recent years, the local false discovery rate (fdr) has drawn much attention, due to its advantage of accessing the confidence of individual hypotheses. However, most methods estimate fdr through P $$ P $$ -values or statistics with known null distributions, which are sometimes unavailable or unreliable. Adopting the innovative methodology of competition-based procedures, for example, the knockoff filter, this paper proposes a new approach, named TDfdr, to fdr estimation, which is free of -values or known null distributions. Extensive simulation studies demonstrate that TDfdr can accurately estimate the fdr with two competition-based procedures. We applied the TDfdr method to two real biomedical tasks. One is to identify significantly differentially expressed proteins related to the COVID-19 disease, and the other is to detect mutations in the genotypes of HIV-1 that are associated with drug resistance. Higher discovery power was observed compared to existing popular methods.},
  archive      = {J_SIM},
  author       = {Xiaoya Sun and Yan Fu},
  doi          = {10.1002/sim.9942},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {61-88},
  shortjournal = {Stat. Med.},
  title        = {Local false discovery rate estimation with competition-based procedures for variable selection},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Cluster randomized trial designs for modeling time-varying
intervention effects. <em>SIM</em>, <em>43</em>(1), 49–60. (<a
href="https://doi.org/10.1002/sim.9941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped-wedge cluster randomized trials (SW-CRTs) are typically analyzed assuming a constant intervention effect. In practice, the intervention effect may vary as a function of exposure time, leading to biased results. The estimation of time-on-intervention (TOI) effects specifies separate discrete intervention effects for each elapsed period of exposure time since the intervention was first introduced. It has been demonstrated to produce results with minimum bias and nominal coverage probabilities in the analysis of SW-CRTs. Due to the design&#39;s staggered crossover, TOI effect variances are heteroskedastic in a SW-CRT. Accordingly, we hypothesize that alternative CRT designs will be more efficient at modeling certain TOI effects. We derive and compare the variance estimators of TOI effects between a SW-CRT, parallel CRT (P-CRT), parallel CRT with baseline (PB-CRT), and novel parallel CRT with baseline and an all-exposed period (PBAE-CRT). We also prove that the time-averaged TOI effect variance and point estimators are identical to that of the constant intervention effect in both P-CRTs and PB-CRTs . We then use data collected from a hospital disinvestment study to simulate and compare the differences in TOI effect estimates between the different CRT designs. Our results reveal that the SW-CRT has the most efficient estimator for the early TOI effect, whereas the PB-CRT typically has the most efficient estimator for the long-term and time-averaged TOI effects. Overall, the PB-CRT with TOI effects can be a more appropriate choice of CRT design for modeling intervention effects that vary by exposure time.},
  archive      = {J_SIM},
  author       = {Kenneth Menglin Lee and Yin Bun Cheung},
  doi          = {10.1002/sim.9941},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {49-60},
  shortjournal = {Stat. Med.},
  title        = {Cluster randomized trial designs for modeling time-varying intervention effects},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Principal stratification for quantile causal effects under
partial compliance. <em>SIM</em>, <em>43</em>(1), 34–48. (<a
href="https://doi.org/10.1002/sim.9940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the principal stratification framework in causal inference, the majority of the literature has focused on binary compliance with an intervention and modelling means. Yet in some research areas, compliance is partial, and research questions—and hence analyses—are concerned with causal effects on (possibly high) quantiles rather than on shifts in average outcomes. Modelling partial compliance is challenging because it can suffer from lack of identifiability. We develop an approach to estimate quantile causal effects within a principal stratification framework, where principal strata are defined by the bivariate vector of (partial) compliance to the two levels of a binary intervention. We propose a conditional copula approach to impute the missing potential compliance and estimate the principal quantile treatment effect surface at high quantiles, allowing the copula association parameter to vary with the covariates. A bootstrap procedure is used to estimate the parameter to account for inflation due to imputation of missing compliance. Moreover, we describe precise assumptions on which the proposed approach is based, and investigate the finite sample behavior of our method by a simulation study. The proposed approach is used to study the 90th principal quantile treatment effect of executive stay-at-home orders on mitigating the risk of COVID-19 transmission in the United States.},
  archive      = {J_SIM},
  author       = {Shuo Sun and Johanna G. Nešlehová and Erica E. M. Moodie},
  doi          = {10.1002/sim.9940},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {34-48},
  shortjournal = {Stat. Med.},
  title        = {Principal stratification for quantile causal effects under partial compliance},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mixed model approach to estimate the survivor average
causal effect in cluster-randomized trials. <em>SIM</em>,
<em>43</em>(1), 16–33. (<a
href="https://doi.org/10.1002/sim.9939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many medical studies, the outcome measure (such as quality of life, QOL) for some study participants becomes informatively truncated (censored, missing, or unobserved) due to death or other forms of dropout, creating a nonignorable missing data problem. In such cases, the use of a composite outcome or imputation methods that fill in unmeasurable QOL values for those who died rely on strong and untestable assumptions and may be conceptually unappealing to certain stakeholders when estimating a treatment effect. The survivor average causal effect (SACE) is an alternative causal estimand that surmounts some of these issues. While principal stratification has been applied to estimate the SACE in individually randomized trials, methods for estimating the SACE in cluster-randomized trials are currently limited. To address this gap, we develop a mixed model approach along with an expectation–maximization algorithm to estimate the SACE in cluster-randomized trials. We model the continuous outcome measure with a random intercept to account for intracluster correlations due to cluster-level randomization, and model the principal strata membership both with and without a random intercept. In simulations, we compare the performance of our approaches with an existing fixed-effects approach to illustrate the importance of accounting for clustering in cluster-randomized trials. The methodology is then illustrated using a cluster-randomized trial of telecare and assistive technology on health-related QOL in the elderly.},
  archive      = {J_SIM},
  author       = {Wei Wang and Guangyu Tong and Shashivadan P. Hirani and Stanton P. Newman and Scott D. Halpern and Dylan S. Small and Fan Li and Michael O. Harhay},
  doi          = {10.1002/sim.9939},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {16-33},
  shortjournal = {Stat. Med.},
  title        = {A mixed model approach to estimate the survivor average causal effect in cluster-randomized trials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel cox partially linear regression: Building predictive
models for cancer patients’ survival. <em>SIM</em>, <em>43</em>(1),
1–15. (<a href="https://doi.org/10.1002/sim.9938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wide heterogeneity exists in cancer patients&#39; survival, ranging from a few months to several decades. To accurately predict clinical outcomes, it is vital to build an accurate predictive model that relates the patients&#39; molecular profiles with the patients&#39; survival. With complex relationships between survival and high-dimensional molecular predictors, it is challenging to conduct nonparametric modeling and irrelevant predictors removing simultaneously. In this article, we build a kernel Cox proportional hazards semi-parametric model and propose a novel regularized garrotized kernel machine (RegGKM) method to fit the model. We use the kernel machine method to describe the complex relationship between survival and predictors, while automatically removing irrelevant parametric and nonparametric predictors through a LASSO penalty. An efficient high-dimensional algorithm is developed for the proposed method. Comparison with other competing methods in simulation shows that the proposed method always has better predictive accuracy. We apply this method to analyze a multiple myeloma dataset and predict the patients&#39; death burden based on their gene expressions. Our results can help classify patients into groups with different death risks, facilitating treatment for better clinical outcomes.},
  archive      = {J_SIM},
  author       = {Yaohua Rong and Sihai Dave Zhao and Xia Zheng and Yi Li},
  doi          = {10.1002/sim.9938},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Med.},
  title        = {Kernel cox partially linear regression: Building predictive models for cancer patients&#39; survival},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
