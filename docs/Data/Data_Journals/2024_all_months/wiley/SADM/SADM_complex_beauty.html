<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SADM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sadm---69">SADM - 69</h2>
<ul>
<li><details>
<summary>
(2024). Nonparametric expectile regression meets deep neural
networks: A robust nonlinear variable selection method. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(6), e70002. (<a
href="https://doi.org/10.1002/sam.70002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper investigates the variable selection problem in expectile regression under nonparametric conditions. In practical scenarios, data often exhibits heterogeneity and has a heavy-tailed distribution. Expectile regression combines the advantages of mean regression and quantile regression, and can show the distribution characteristics of data at different expectile values. For actual data obtained, not all variables are important, selecting important variables can significantly reduce the cost of data collection and also reduce computational overhead. To extract representative feature subsets, various variable selection methods have been proposed for linear expectile regression models. However, in practice, there may be complex nonparametric relationships between explanatory and response variables. This paper extends the nonlinear variable selection method based on deep neural network DFS to nonparametric expectile regression to study the distribution correlation between explanatory and response variables in nonparametric situations. We validated the effectiveness and feasibility of the proposed method in numerical simulations and applied it to the used car transaction price dataset.},
  archive  = {J},
  author   = {Rui Yang and Yunquan Song},
  doi      = {10.1002/sam.70002},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {e70002},
  title    = {Nonparametric expectile regression meets deep neural networks: A robust nonlinear variable selection method},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convolutional sparse coding for time series via a ℓ0
penalty: An efficient algorithm with statistical guarantees.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(6), e70000. (<a
href="https://doi.org/10.1002/sam.70000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Identifying characteristic patterns in time series, such as heartbeats or brain responses to a stimulus, is critical to understanding the physical or physiological phenomena monitored with sensors. Convolutional sparse coding (CSC) methods, which aim to approximate signals by a sparse combination of short signal templates (also called atoms), are well-suited for this task. However, enforcing sparsity leads to non-convex and untractable optimization problems. This article proposes finding the optimal solution to the original and non-convex CSC problem when the atoms do not overlap. Specifically, we show that the reconstruction error satisfies a simple recursive relationship in this setting, which leads to an efficient detection algorithm. We prove that our method correctly estimates the number of patterns and their localization, up to a detection margin that depends on a certain measure of the signal-to-noise ratio. In a thorough empirical study, with simulated and real-world physiological data sets, our method is shown to be more accurate than existing algorithms at detecting the patterns&#39; onsets.},
  archive  = {J},
  author   = {Charles Truong and Thomas Moreau},
  doi      = {10.1002/sam.70000},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {e70000},
  title    = {Convolutional sparse coding for time series via a ℓ0 penalty: An efficient algorithm with statistical guarantees},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On algorithms and approximations for progressively type-i
censoring schemes. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(6), e11717. (<a
href="https://doi.org/10.1002/sam.11717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This research aims to study the estimation of parameters for a Burr-XII distribution and to investigate optimal sampling plans under progressive Type-I censoring (PTIC). For point estimation, we employed the maximum-likelihood estimation (MLE) method using two numerical approaches: Newton–Raphson and the Expectation–Maximization algorithm. We also utilized Bayesian estimation with squared error loss and linear exponential loss functions. Specifically, two approximate Bayesian methods, the Lindley and Tierney-Kadane methods were examined. Additionally, Bayesian numerical estimation was performed using Markov Chain Monte Carlo with the Metropolis-Hastings (MH) algorithm. For interval estimation, we constructed asymptotic confidence intervals for MLE and the highest posterior density method within the Bayesian framework. The practical study involved Monte Carlo simulations to assess the efficiency and accuracy of the proposed estimation methods across different PTIC schemes. A real data analysis is also provided to illustrate the practical application of these methodologies in analyzing a clinical trial dataset. Data from a clinical trial using a PTIC scheme reveals patterns in pain relief, aiding in evaluating the antibiotic ointment&#39;s effectiveness. The study further investigates optimal sampling plans for the Burr-XII distribution under PTIC.},
  archive  = {J},
  author   = {Ahmed R. El-Saeed and Ehab M. Almetwally},
  doi      = {10.1002/sam.11717},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {e11717},
  title    = {On algorithms and approximations for progressively type-I censoring schemes},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian posterior interval calibration to improve the
interpretability of observational studies. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>17</em>(6), e11715.
(<a href="https://doi.org/10.1002/sam.11715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Observational healthcare data offer the potential to estimate causal effects of medical products on a large scale. However, the confidence intervals and p values produced by observational studies only account for random error and fail to account for systematic error. Therefore, operating characteristics such as confidence interval coverage and Type I error rates often deviate sharply from their nominal values and render interpretation impossible. While there is a longstanding awareness of systematic error in observational studies, analytic approaches to empirically account for systematic error are relatively new. Several authors have proposed approaches using negative controls (also known as “falsification hypotheses”) and positive controls. The basic idea is to adjust confidence intervals and p values in light of the bias (if any) detected in the analyses of the negative and positive control. In this work, we propose a Bayesian statistical procedure for posterior interval calibration that uses negative and positive controls. We show that the posterior interval calibration procedure restores nominal characteristics, such as 95% coverage of the true effect size by the 95% posterior interval.},
  archive  = {J},
  author   = {Jami J. Mulgrave and David Madigan and George Hripcsak},
  doi      = {10.1002/sam.11715},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {e11715},
  title    = {Bayesian posterior interval calibration to improve the interpretability of observational studies},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A conversational assistant for democratization of data
visualization: A comparative study of two approaches of interaction.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(6), e11714. (<a
href="https://doi.org/10.1002/sam.11714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data visualization is an important aspect of exploratory data analysis which requires expertise, posing a challenge to many users. In this work, we present a conversational system designed to make data visualization accessible, where users can generate visualizations of data by natural language conversational interaction with the system. We conducted a between-subject study to compare two versions of our system, where users were asked to perform data exploration tasks. We did not find significant differences as concerns user assessments of their interactions with the system; however, we uncovered that users explore different properties of the data and reach different conclusions when using the two systems. We will discuss the general implications of our findings as regards the usage of conversational interfaces for visualization.},
  archive  = {J},
  author   = {Abari Bhattacharya and Barbara Di Eugenio and Veronica Grosso and Andrew Johnson and Roderick Tabalba and Nurit Kirshenbaum and Jason Leigh and Moira Zellner},
  doi      = {10.1002/sam.11714},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {e11714},
  title    = {A conversational assistant for democratization of data visualization: A comparative study of two approaches of interaction},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient filtering approach for model estimation in
sparse regression. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(5), e11713. (<a
href="https://doi.org/10.1002/sam.11713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As technology advances, the scale of data generated is growing exponentially, bringing huge challenges to data storage and computation. To facilitate the computational cost while maintaining model estimation accuracy, subdata selection become important. Conventional methods, such as LASSO and ridge regression, often focus on feature selection. In contrast, methods of subsampling aim at specifying data points to be extracted. However, these subsampling methods often overlook the full consideration of the role of the response variable and its relationship with predictor variables. In this work, we propose a so-called filtering approach for model estimation (FAME) method to perform subsampling in combination with feature screening. Compared with existing methods, the proposed method can result in the subdata being smaller in size both in terms of the number of features and observations, and also the computational complexity does not increase. The proposed method can be extended to situations when the predictor is binary, the response is binary, or both are binary. The performance of FAME is evaluated in both numerical studies and real data examples.},
  archive  = {J},
  author   = {Yanran Wei and William Myers and Xinwei Deng},
  doi      = {10.1002/sam.11713},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {e11713},
  title    = {An efficient filtering approach for model estimation in sparse regression},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Node centrality inference via hypothesis testing.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(5), e11712. (<a
href="https://doi.org/10.1002/sam.11712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Central nodes, also known as vital or critical nodes, play a unique role in networks of various domains. To date, numerous centrality evaluation methods have been proposed to identify central nodes from different types of networks. However, most of existing methods did not address this issue from a rigorous significance testing perspective. As a result, it is hard for us to judge whether an identified central node is true or not and control the quality of reported nodes in a statistically sound manner. In this paper, we propose a centrality evaluation method based on hypothesis testing, where the test statistic is the degree sum of the neighbors of the node. We further derive an analytical p value and its upper bound under the Erdös–Rényi model, which can be employed as the centrality score for vital node recognition. To demonstrate the effectiveness of our method, extensive experiments are conducted on both simulated and real networks. The experimental results show that (1) our testing-based centrality measure in terms of p value is capable of distinguishing central nodes from non-central ones effectively and (2) our method is able to identify central nodes more accurately than those classic centrality measures and state-of-the-art (SOTA) centrality evaluation methods.},
  archive  = {J},
  author   = {Zengyou He and Jun Lou and Yan Liu and Lianyu Hu and Mudi Jiang},
  doi      = {10.1002/sam.11712},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {e11712},
  title    = {Node centrality inference via hypothesis testing},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On an empirical likelihood based solution to the approximate
bayesian computation problem. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>17</em>(5), e11711. (<a
href="https://doi.org/10.1002/sam.11711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Approximate Bayesian computation (ABC) methods are applicable to statistical models specified by generative processes with analytically intractable likelihoods. These methods try to approximate the posterior density of a model parameter by comparing the observed data with additional process-generated simulated data sets. For computational benefit, only the values of certain well-chosen summary statistics are usually compared, instead of the whole data set. Most ABC procedures are computationally expensive, justified only heuristically, and have poor asymptotic properties. In this article, we introduce a new empirical likelihood-based approach to the ABC paradigm called ABCel. The proposed procedure is computationally tractable and approximates the target log posterior of the parameter as a sum of two functions of the data—namely, the mean of the optimal log-empirical likelihood weights and the estimated differential entropy of the summary functions. We rigorously justify the procedure via direct and reverse information projections onto appropriate classes of probability densities. Past applications of empirical likelihood in ABC demanded constraints based on analytically tractable estimating functions that involve both the data and the parameter; although by the nature of the ABC problem such functions may not be available in general. In contrast, we use constraints that are functions of the summary statistics only. Equally importantly, we show that our construction directly connects to the reverse information projection and estimate the relevant differential entropy by a k-NN estimator. We show that ABCel is posterior consistent and has highly favorable asymptotic properties. Its construction justifies the use of simple summary statistics like moments, quantiles, and so forth, which in practice produce accurate approximation of the posterior density. We illustrate the performance of the proposed procedure in a range of applications.},
  archive  = {J},
  author   = {Sanjay Chaudhuri and Subhroshekhar Ghosh and Kim Cuc Pham},
  doi      = {10.1002/sam.11711},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {e11711},
  title    = {On an empirical likelihood based solution to the approximate bayesian computation problem},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformal multi-target hyperrectangles. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(5), e11710. (<a
href="https://doi.org/10.1002/sam.11710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose conformal hyperrectangular prediction regions for multi-target regression. We propose split conformal prediction algorithms for both point and quantile regression to form hyperrectangular prediction regions, which allow for easy marginal interpretation and do not require covariance estimation. In practice, it is preferable that a prediction region is balanced, that is, having identical marginal prediction coverage, since prediction accuracy is generally equally important across components of the response vector. The proposed algorithms possess two desirable properties, namely, tight asymptotic overall nominal coverage as well as asymptotic balance, that is, identical asymptotic marginal coverage, under mild conditions. We then compare our methods to some existing methods on both simulated and real data sets. Our simulation results and real data analysis show that our methods outperform existing methods while achieving the desired nominal coverage and good balance between dimensions.},
  archive  = {J},
  author   = {Max Sampson and Kung-Sik Chan},
  doi      = {10.1002/sam.11710},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {e11710},
  title    = {Conformal multi-target hyperrectangles},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying epistemic uncertainty in binary classification
via accuracy gain. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(5), e11709. (<a
href="https://doi.org/10.1002/sam.11709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, a surge of interest has been given to quantifying epistemic uncertainty (EU), the reducible portion of uncertainty due to lack of data. We propose a novel EU estimator in the binary classification setting, as the posterior expected value of the empirical gain in accuracy between the current prediction and the optimal prediction. In order to validate the performance of our EU estimator, we introduce an experimental procedure where we take an existing dataset, remove a set of points, and compare the estimated EU with the observed change in accuracy. Through real and simulated data experiments, we demonstrate the effectiveness of our proposed EU estimator.},
  archive  = {J},
  author   = {Christopher Qian and Tyler Ganter and Joshua Michalenko and Feng Liang and Jason Adams},
  doi      = {10.1002/sam.11709},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {e11709},
  title    = {Quantifying epistemic uncertainty in binary classification via accuracy gain},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new logarithmic multiplicative distortion for correlation
analysis. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(4), e11708. (<a
href="https://doi.org/10.1002/sam.11708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We study the Pearson correlation coefficient in a logarithmic manner under the presence of multiplicative distortion measurement errors. In this context, the observed variables with logarithmic transformation are distorted in multiplicative fashions by an observed confounding variable. The proposed multiplicative distortion model in this paper is applied to analyze positive variables. We utilize the conditional mean calibration and the conditional absolute mean calibration methods to obtain the calibrated variables. Furthermore, we propose confidence intervals based on asymptotic normality, empirical likelihood, and jackknife empirical likelihood. Simulation studies demonstrate the effectiveness of the proposed estimation procedure, and a real-world example is analyzed to illustrate its practical application.},
  archive  = {J},
  author   = {Siming Deng and Jun Zhang},
  doi      = {10.1002/sam.11708},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {e11708},
  title    = {A new logarithmic multiplicative distortion for correlation analysis},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting winnow: A modified online feature selection
algorithm for efficient binary classification. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>17</em>(4),
e11707. (<a href="https://doi.org/10.1002/sam.11707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Winnow is an efficient binary classification algorithm that effectively learns from data even in the presence of a large number of irrelevant attributes. It is specifically designed for online learning scenarios. Unlike the Perceptron algorithm, Winnow employs a multiplicative weight update function, which leads to fewer mistakes and faster convergence. However, the original Winnow algorithm has several limitations. They include, it only works on binary data, and the weight updates are constant and do not depend on the input features. In this article, we propose a modified version of the Winnow algorithm that addresses these limitations. The proposed algorithm is capable of handling real-valued data, updates the learning function based on the input feature vector. To evaluate the performance of our proposed algorithm, we compare it with seven existing variants of the Winnow algorithm on datasets of varying sizes. We employ various evaluation metrics and parameters to assess and compare the performance of the algorithms. The experimental results demonstrate that our proposed algorithm outperforms all the other algorithms used for comparison, highlighting its effectiveness in classification tasks.},
  archive  = {J},
  author   = {Y. Narasimhulu and Pralhad Kolambkar and Venkaiah V. China},
  doi      = {10.1002/sam.11707},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {e11707},
  title    = {Revisiting winnow: A modified online feature selection algorithm for efficient binary classification},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characterizing climate pathways using feature importance on
echo state networks. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(4), e11706. (<a
href="https://doi.org/10.1002/sam.11706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The 2022 National Defense Strategy of the United States listed climate change as a serious threat to national security. Climate intervention methods, such as stratospheric aerosol injection, have been proposed as mitigation strategies, but the downstream effects of such actions on a complex climate system are not well understood. The development of algorithmic techniques for quantifying relationships between source and impact variables related to a climate event (i.e., a climate pathway) would help inform policy decisions. Data-driven deep learning models have become powerful tools for modeling highly nonlinear relationships and may provide a route to characterize climate variable relationships. In this paper, we explore the use of an echo state network (ESN) for characterizing climate pathways. ESNs are a computationally efficient neural network variation designed for temporal data, and recent work proposes ESNs as a useful tool for forecasting spatiotemporal climate data. However, ESNs are noninterpretable black-box models along with other neural networks. The lack of model transparency poses a hurdle for understanding variable relationships. We address this issue by developing feature importance methods for ESNs in the context of spatiotemporal data to quantify variable relationships captured by the model. We conduct a simulation study to assess and compare the feature importance techniques, and we demonstrate the approach on reanalysis climate data. In the climate application, we consider a time period that includes the 1991 volcanic eruption of Mount Pinatubo. This event was a significant stratospheric aerosol injection, which acts as a proxy for an anthropogenic stratospheric aerosol injection. We are able to use the proposed approach to characterize relationships between pathway variables associated with this event that agree with relationships previously identified by climate scientists.},
  archive  = {J},
  author   = {Katherine Goode and Daniel Ries and Kellie McClernon},
  doi      = {10.1002/sam.11706},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {e11706},
  title    = {Characterizing climate pathways using feature importance on echo state networks},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A random forest approach for interval selection in
functional regression. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(4), e11705. (<a
href="https://doi.org/10.1002/sam.11705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we focus on the problem of variable selection in a functional regression framework. This question is motivated by practical applications in the field of agronomy: In this field, identifying the temporal periods during which weather measurements have the greatest impact on yield is critical for guiding agriculture practices in a changing environment. From a methodological point of view, our goal is to identify consecutive measurement points in the definition domain of the functional predictors, which correspond to the most important intervals for the prediction of a numeric output from the functional variables. We propose an approach based on the versatile random forest method that benefits from its good performances for variable selection and prediction. Our method builds in three steps (interval creation, summary, and selection). Different variants for each of the steps are proposed and compared on both simulated and real-life datasets. The performances of our method compared to alternative approaches highlight its usefulness to select relevant intervals while maintaining good prediction capabilities. All variants of our method are available in the R package SISIR.},
  archive  = {J},
  author   = {Rémi Servien and Nathalie Vialaneix},
  doi      = {10.1002/sam.11705},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {e11705},
  title    = {A random forest approach for interval selection in functional regression},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural interval-censored survival regression with feature
selection. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>17</em>(4), e11704. (<a
href="https://doi.org/10.1002/sam.11704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Survival analysis is a fundamental area of focus in biomedical research, particularly in the context of personalized medicine. This prominence is due to the increasing prevalence of large and high-dimensional datasets, such as omics and medical image data. However, the literature on nonlinear regression algorithms and variable selection techniques for interval-censoring is either limited or nonexistent, particularly in the context of neural networks. Our objective is to introduce a novel predictive framework tailored for interval-censored regression tasks, rooted in Accelerated Failure Time (AFT) models. Our strategy comprises two key components: (i) a variable selection phase leveraging recent advances on sparse neural network architectures; (ii) a regression model targeting prediction of the interval-censored response. To assess the performance of our novel algorithm, we conducted a comprehensive evaluation through both numerical experiments and real-world applications that encompass scenarios related to diabetes and physical activity. Our results outperform traditional AFT algorithms, particularly in scenarios featuring nonlinear relationships.},
  archive  = {J},
  author   = {Carlos García Meixide and Marcos Matabuena and Louis Abraham and Michael R. Kosorok},
  doi      = {10.1002/sam.11704},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {e11704},
  title    = {Neural interval-censored survival regression with feature selection},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-sample testing for random graphs. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(4), e11703. (<a
href="https://doi.org/10.1002/sam.11703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The employment of two-sample hypothesis testing in examining random graphs has been a prevalent approach in diverse fields such as social sciences, neuroscience, and genetics. We advance a spectral-based two-sample hypothesis testing methodology to test the latent position random graphs. We propose two distinct asymptotic normal statistics, each optimally designed for two different models—the elementary Erdős–Rényi model and the more complex latent position random graph model. For the latter, the spectral embedding of the adjacency matrix was utilized to estimate the test statistic. The proposed method exhibited superior efficacy as it accomplished higher power than the conventional method of mean estimation. To validate our hypothesis testing procedure, we applied it to empirical biological data to discern structural variances in gene co-expression networks between COVID-19 patients and individuals who remained unaffected by the disease.},
  archive  = {J},
  author   = {Xiaoyi Wen},
  doi      = {10.1002/sam.11703},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {e11703},
  title    = {Two-sample testing for random graphs},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-sensitive classification with time constraint on
incomplete data. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>17</em>(3), e11702. (<a
href="https://doi.org/10.1002/sam.11702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Missing values are common, but dealing with them by inappropriate method may lead to large classification errors. Empirical evidences show that the tree-based classification algorithms such as classification and regression tree (CART) can benefit from imputation, especially multiple imputation. Nevertheless, less attention has been paid to incorporating multiple imputation into cost-sensitive decision tree induction. This study focuses on the treatment of missing data based on a time-constrained minimal-cost tree algorithm. We introduce various approaches to handle incomplete data into the algorithm including complete-case analysis, missing-value branch, single imputation, feature acquisition, and multiple imputation. A simulation study under different scenarios examines the predictive performances of the proposed strategies. The simulation results show that the combination of the algorithm with multiple imputation can assure classification accuracy under the budget. A real medical data example provides insights into the problem of missing values in cost-sensitive learning and the advantages of the proposed methods.},
  archive  = {J},
  author   = {Yong-Shiuan Lee and Chia-Chi Wu},
  doi      = {10.1002/sam.11702},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {7},
  number   = {3},
  pages    = {e11702},
  title    = {Cost-sensitive classification with time constraint on incomplete data},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessment of the real-time pattern recognition capability
of machine learning algorithms. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>17</em>(3), e11701. (<a
href="https://doi.org/10.1002/sam.11701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays data streams from different sources, like blockchain-based and traditional financial transactions, social networks, and interconnected Internet of Things (IoT) devices, are becoming increasingly large in volume and the need to recognize patterns in real time from these streams, while adapting to their velocity and veracity, is emerging. Established machine learning algorithms used for pattern recognition methods have not been designed taking under account the volume, velocity, diversity, and accuracy of the data streams. This research contributes with an approach for assessing the pattern recognition capabilities of established machine learning algorithms when handling volatile data in real time and proposes a system that adapts the algorithms to the requirements of data streams, as well as assesses their pattern recognition capabilities based on established criteria. The system was applied for assessing five machine learning algorithms with input from a data stream from Bluetooth beacons tracking consumers in a retail store. This research can support future data scientists and analysts who need to reveal data patterns in big, volatile data streams in real time in order to support effective decision-making in the respective application domain. Copyright © 2024 John Wiley &amp; Sons, Ltd.},
  archive  = {J},
  author   = {Elias Polytarchos and Cleopatra Bardaki and Katerina Pramatari},
  doi      = {10.1002/sam.11701},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11701},
  title    = {Assessment of the real-time pattern recognition capability of machine learning algorithms},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multitask learning in high dimensions under memory
constraint. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>17</em>(3), e11700. (<a
href="https://doi.org/10.1002/sam.11700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We investigate multitask learning in the context of multivariate linear regression with high dimensional covariates and heavy-tailed noise, while under the constraint of limited memory. To tackle the computational complexity arising from the non-smoothness of the quantile loss, we reformulate it as an equivalent least squares loss, which yields robust solutions even in the presence of heavy-tailed noise. We incorporate a group lasso penalty into the quantile loss to produce sparse solutions, and an accelerated proximal sub-gradient descent algorithm to speed up the computation while ensuring explicit forms for penalized solutions at each iteration. The proposed algorithm is general and can be applied to similar optimization problems. Moreover, we introduce a communication-efficient distributed algorithm that guarantees optimal convergence rates after finite communication rounds in cases where computing resources such as memory are insufficient. We also study the theoretical properties of the resultant estimate and relax the widely used model selection consistency assumption on the initial estimate. We demonstrate the effectiveness of our proposal through extensive numerical studies.},
  archive  = {J},
  author   = {Canyi Chen and Bingzhen Chen and Lingchen Kong and Liping Zhu},
  doi      = {10.1002/sam.11700},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11700},
  title    = {Robust multitask learning in high dimensions under memory constraint},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian process selections in semiparametric multi-kernel
machine regression for multi-pathway analysis. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>17</em>(3),
e11699. (<a href="https://doi.org/10.1002/sam.11699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Analyzing correlated high-dimensional data is a challenging problem in genomics, proteomics, and other related areas. For example, it is important to identify significant genetic pathway effects associated with biomarkers in which a gene pathway is a set of genes that functionally works together to regulate a certain biological process. A pathway-based analysis can detect a subtle change in expression level that cannot be found using a gene-based analysis. Here, we refer to pathway as a set and gene as an element in a set. However, it is challenging to select automatically which pathways are highly associated to the outcome when there are multiple pathways. In this paper, we propose a semiparametric multikernel regression model to study the effects of fixed covariates (e.g., clinical variables) and sets of elements (e.g., pathways of genes) to address a problem of detecting signal sets associated to biomarkers. We model the unknown high-dimension functions of multi-sets via multiple Gaussian kernel machines to consider the possibility that elements within the same set interact with each other. Hence, our variable set selection can be considered a Gaussian process set selection. We develop our Gaussian process set selection under the Bayesian variance component-selection framework. We incorporate prior knowledge for structural sets by imposing an Ising prior on the model. Our approach can be easily applied in high-dimensional spaces where the sample size is smaller than the number of variables. An efficient variational Bayes algorithm is developed. We demonstrate the advantages of our approach through simulation studies and through a type II diabetes genetic-pathway analysis.},
  archive  = {J},
  author   = {Jiali Lin and Inyoung Kim},
  doi      = {10.1002/sam.11699},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11699},
  title    = {Gaussian process selections in semiparametric multi-kernel machine regression for multi-pathway analysis},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian batch optimization for molybdenum versus tungsten
inertial confinement fusion double shell target design. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(3), e11698. (<a
href="https://doi.org/10.1002/sam.11698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Access to reliable, clean energy sources is a major concern for national security. Much research is focused on the “grand challenge” of producing energy via controlled fusion reactions in a laboratory setting. For fusion experiments, specifically inertial confinement fusion (ICF), to produce sufficient energy, the fusion reactions in the ICF fuel need to become self-sustaining and burn deuterium-tritium (DT) fuel efficiently. The recent record-breaking NIF ignition shot was able to achieve this goal as well as produce more energy than used to drive the experiment. This achievement brings self-sustaining fusion-based power systems closer than ever before, capable of providing humans with access to secure, renewable energy. In order to further progress toward the actualization of such power systems, more ICF experiments need to be conducted at large laser facilities such as the United States&#39;s National Ignition Facility (NIF) or France&#39;s Laser Mega-Joule. The high cost per shot and limited number of shots that are possible per year make it prohibitive to perform large numbers of experiments. As such, experimental design relies heavily on complex predictive physics simulations for high-fidelity “preshot” analysis. These multidimensional, multi-physics, high-fidelity simulations have to account for a variety of input parameters as well as modeling the extreme conditions (pressures and densities) present at ignition. Such simulations (especially in 3D) can become computationally prohibitive to turn around for each ICF experiment. In this work, we explore using Bayesian optimization with Gaussian processes (GPs) to find optimal designs for ICF double shell targets, while keeping computational costs to manageable levels. These double shell targets have an inner shell that grades from beryllium on the outer surface to the higher Z material molybdenum, as opposed to the nominally used tungsten, on the inside in order to trade off between the high performance associated with high density inner shells and capsule stability. We describe our results for “capsule-only” xRAGE simulations to study the physics between different capsule designs, inner shell materials, and potential for future experiments.},
  archive  = {J},
  author   = {Nomita N. Vazirani and Ryan Sacks and Brian M. Haines and Michael J. Grosskopf and David J. Stark and Paul A. Bradley},
  doi      = {10.1002/sam.11698},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11698},
  title    = {Bayesian batch optimization for molybdenum versus tungsten inertial confinement fusion double shell target design},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential metamodel-based approaches to level-set
estimation under heteroscedasticity. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>17</em>(3), e11697. (<a
href="https://doi.org/10.1002/sam.11697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper proposes two sequential metamodel-based methods for level-set estimation (LSE) that leverage the uniform bound built on stochastic kriging: predictive variance reduction (PVR) and expected classification improvement (ECI). We show that PVR and ECI possess desirable theoretical performance guarantees and provide closed-form expressions for their respective sequential sampling criteria to seek the next design point for performing simulation runs, allowing computationally efficient one-iteration look-ahead updates. To enhance understanding, we reveal the connection between PVR and ECI&#39;s sequential sampling criteria. Additionally, we propose integrating a budget allocation feature with PVR and ECI, which improves computational efficiency and potentially enhances robustness to the impacts of heteroscedasticity. Numerical studies demonstrate the superior performance of the proposed methods compared to state-of-the-art benchmarking approaches when given a fixed simulation budget, highlighting their effectiveness in addressing LSE problems.},
  archive  = {J},
  author   = {Yutong Zhang and Xi Chen},
  doi      = {10.1002/sam.11697},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11697},
  title    = {Sequential metamodel-based approaches to level-set estimation under heteroscedasticity},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards accelerating particle-resolved direct numerical
simulation with neural operators. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>17</em>(3), e11690. (<a
href="https://doi.org/10.1002/sam.11690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present our ongoing work aimed at accelerating a particle-resolved direct numerical simulation model designed to study aerosol–cloud–turbulence interactions. The dynamical model consists of two main components—a set of fluid dynamics equations for air velocity, temperature, and humidity, coupled with a set of equations for particle (i.e., cloud droplet) tracing. Rather than attempting to replace the original numerical solution method in its entirety with a machine learning (ML) method, we consider developing a hybrid approach. We exploit the potential of neural operator learning to yield fast and accurate surrogate models and, in this study, develop such surrogates for the velocity and vorticity fields. We discuss results from numerical experiments designed to assess the performance of ML architectures under consideration as well as their suitability for capturing the behavior of relevant dynamical systems.},
  archive  = {J},
  author   = {Mohammad Atif and Vanessa López-Marrero and Tao Zhang and Abdullah Al Muti Sharfuddin and Kwangmin Yu and Jiaqi Yang and Fan Yang and Foluso Ladeinde and Yangang Liu and Meifeng Lin and Lingda Li},
  doi      = {10.1002/sam.11690},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11690},
  title    = {Towards accelerating particle-resolved direct numerical simulation with neural operators},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric mean and variance adaptive classification rule
for high-dimensional data with heteroscedastic variances.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(3), e11689. (<a
href="https://doi.org/10.1002/sam.11689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this study, we introduce an innovative methodology aimed at enhancing Fisher&#39;s Linear Discriminant Analysis (LDA) in the context of high-dimensional data classification scenarios, specifically addressing situations where each feature exhibits distinct variances. Our approach leverages Nonparametric Maximum Likelihood Estimation (NPMLE) techniques to estimate both the mean and variance parameters. By accommodating varying variances among features, our proposed method leads to notable improvements in classification performance. In particular, unlike numerous prior studies that assume the distribution of heterogeneous variances follows a right-skewed inverse gamma distribution, our proposed method demonstrates excellent performance even when the distribution of heterogeneous variances takes on left-skewed, symmetric, or right-skewed forms. We conducted a series of rigorous experiments to empirically validate the effectiveness of our approach. The results of these experiments demonstrate that our proposed methodology excels in accurately classifying high-dimensional data characterized by heterogeneous variances.},
  archive  = {J},
  author   = {Seungyeon Oh and Hoyoung Park},
  doi      = {10.1002/sam.11689},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11689},
  title    = {Nonparametric mean and variance adaptive classification rule for high-dimensional data with heteroscedastic variances},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric estimation of average treatment effects in
observational studies. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(3), e11688. (<a
href="https://doi.org/10.1002/sam.11688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a semiparametric method to estimate average treatment effects in observational studies based on the assumption of unconfoundedness. Assume that the propensity score model and outcome model are a general single index model, which are estimated by the kernel method and the unknown index parameter is estimated via linearized maximum rank correlation method. The proposed estimator is computationally tractable, allows for large dimension covariates and not involves the approximation of link functions. We showed that the proposed estimator is consistent and asymptotically normally distributed. In general, the proposed estimator is superior to existing methods when the model is incorrectly specified. We also provide an empirical analysis on the average treatment effect and average treatment effect on the treated of 401(k) eligibility on net financial assets.},
  archive  = {J},
  author   = {Jun Wang and Yujiao Guo},
  doi      = {10.1002/sam.11688},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11688},
  title    = {Semiparametric estimation of average treatment effects in observational studies},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Density estimation via measure transport: Outlook for
applications in the biological sciences. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>17</em>(3), e11687.
(<a href="https://doi.org/10.1002/sam.11687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One among several advantages of measure transport methods is that they allow or a unified framework for processing and analysis of data distributed according to a wide class of probability measures. Within this context, we present results from computational studies aimed at assessing the potential of measure transport techniques, specifically, the use of triangular transport maps, as part of a workflow intended to support research in the biological sciences. Scenarios characterized by the availability of limited amount of sample data, which are common in domains such as radiation biology, are of particular interest. We find that when estimating a distribution density function given limited amount of sample data, adaptive transport maps are advantageous. In particular, statistics gathered from computing series of adaptive transport maps, trained on a series of randomly chosen subsets of the set of available data samples, leads to uncovering information hidden in the data. As a result, in the radiation biology application considered here, this approach provides a tool for generating hypotheses about gene relationships and their dynamics under radiation exposure.},
  archive  = {J},
  author   = {Vanessa López-Marrero and Patrick R. Johnstone and Gilchan Park and Xihaier Luo},
  doi      = {10.1002/sam.11687},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11687},
  title    = {Density estimation via measure transport: Outlook for applications in the biological sciences},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The analysis of association rules: Latent class analysis.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(3), e11686. (<a
href="https://doi.org/10.1002/sam.11686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Association rules are used to extract information from transactional databases with a collection of items also called “tokens” or “words.” The aim of association rule analysis is to indicate what and how items go with what items in a set of transactions called “documents.” This approach is used in the analysis of text records, of blogs in social media and of shopping baskets. We present here an approach to analyze documents using latent class analysis (LCA) clustering of document term matrices. A document term matrix (DTM) consists of rows referring to documents and columns corresponding to items. In binary weights, “1” indicates the presence of a term in a document and “0” otherwise. The clustering of similar documents provides stratified data sets used to enhance the interpretability of measures of interest such as lift, odds ratios and relative linkage disequilibrium. The article demonstrates the approach with two case studies. A first example consists of comments recorded in a survey aimed at pet owners. A second, much larger example, is based on online reviews to crocs sandals. Association rules describe combinations of terms in the pet survey and crocs reviews. In Section 3, we compute, for these case studies, association rule measures of interest defined in Section 2. We first introduce the case studies to motivate the methods proposed here. In Section 4, we provide a new approach with an enhanced interpretations of measures such as lift by comparing them across clusters derived from an LCA of the DTM. A key result is the application of clustered data in analyzing observational data. This enhances generalizability and interpretability of findings from text analytics. The article concludes with a discussion in Section 5.},
  archive  = {J},
  author   = {Ron S. Kenett and Chris Gotwalt},
  doi      = {10.1002/sam.11686},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11686},
  title    = {The analysis of association rules: Latent class analysis},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prior effective sample size for exponential family
distributions with multiple parameters. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>17</em>(3), e11685.
(<a href="https://doi.org/10.1002/sam.11685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The setting of priors is an important issue in Bayesian analysis. In particular, when external information is applied, a prior with too much information can dominate the posterior inferences. To prevent this effect, the effective sample size (ESS) can be used. Various ESSs have been proposed recently; however, all have the problem of limiting the applicable prior distributions. For example, one ESS can only be used with a prior that can be approximated by a normal distribution, and another ESS cannot be applied when the parameters are multidimensional. We propose an ESS to be applied to more prior distributions when the sampling model belongs to an exponential family (including the normal model and logistic regression models). This ESS has the predictive consistency and can be used with multidimensional parameters. It is confirmed from normally distributed data with the Student&#39;s- t priors that this ESS behaves as well as an existing predictively consistent ESS for one-parameter exponential families. As examples of multivariate parameters, ESSs for linear and logistic regression models are also discussed.},
  archive  = {J},
  author   = {Ryota Tamanoi},
  doi      = {10.1002/sam.11685},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11685},
  title    = {Prior effective sample size for exponential family distributions with multiple parameters},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Individualized image region detection with total variation.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(3), e11684. (<a
href="https://doi.org/10.1002/sam.11684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Medical image data have emerged to be an indispensable component of modern medicine. Different from many general image problems that focus on outcome prediction or image recognition, medical image analysis pays more attention to model interpretation. For instance, given a list of medical images and corresponding labels of patients&#39; health status, it is often of greater importance to identify the image regions that could differentiate the outcome status, compared to simply predicting labels of new images. Moreover, medical image data often demonstrate strong individual heterogeneity. In other words, the image regions associated with an outcome could be different across patients. As a consequence, the traditional one-model-fits-all approach not only omits patient heterogeneity but also possibly leads to misleading or even wrong conclusions. In this article, we introduce a novel statistical framework to detect individualized regions that are associated with a binary outcome, that is, whether a patient has a certain disease or not. Moreover, we propose a total variation-based penalization for individualized image region detection under a local label-free scenario. Considering that local labeling is often difficult to obtain for medical image data, our approach may potentially have a wider range of applications in medical research. The effectiveness of our proposed approach is validated by two real histopathology databases: Colon Cancer and Camelyon16.},
  archive  = {J},
  author   = {Sanyou Wu and Fuying Wang and Long Feng},
  doi      = {10.1002/sam.11684},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {e11684},
  title    = {Individualized image region detection with total variation},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian relative composite quantile regression approach of
ordinal latent regression model with l1/2 regularization.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(2), e11683. (<a
href="https://doi.org/10.1002/sam.11683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Ordinal data frequently occur in various fields such as knowledge level assessment, credit rating, clinical disease diagnosis, and psychological evaluation. The classic models including cumulative logistic regression or probit regression are often used to model such ordinal data. But these modeling approaches conditionally depict the mean characteristic of response variable on a cluster of predictive variables, which often results in non-robust estimation results. As a considerable alternative, composite quantile regression (CQR) approach is usually employed to gain more robust and relatively efficient results. In this paper, we propose a Bayesian CQR modeling approach for ordinal latent regression model. In order to overcome the recognizability problem of the considered model and obtain more robust estimation results, we advocate to using the Bayesian relative CQR approach to estimate regression parameters. Additionally, in regression modeling, it is a highly desirable task to obtain a parsimonious model that retains only important covariates. We incorporate the Bayesian penalty into the ordinal latent CQR regression model to simultaneously conduct parameter estimation and variable selection. Finally, the proposed Bayesian relative CQR approach is illustrated by Monte Carlo simulations and a real data application. Simulation results and real data examples show that the suggested Bayesian relative CQR approach has good performance for the ordinal regression models.},
  archive  = {J},
  author   = {Tian Yu-Zhu and Wu Chun-Ho and Tai Ling-Nan and Mian Zhi-Bao and Tian Mao-Zai},
  doi      = {10.1002/sam.11683},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11683},
  title    = {Bayesian relative composite quantile regression approach of ordinal latent regression model with l1/2 regularization},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian shrinkage models for integration and analysis of
multiplatform high-dimensional genomics data. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>17</em>(2),
e11682. (<a href="https://doi.org/10.1002/sam.11682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the increasing availability of biomedical data from multiple platforms of the same patients in clinical research, such as epigenomics, gene expression, and clinical features, there is a growing need for statistical methods that can jointly analyze data from different platforms to provide complementary information for clinical studies. In this paper, we propose a two-stage hierarchical Bayesian model that integrates high-dimensional biomedical data from diverse platforms to select biomarkers associated with clinical outcomes of interest. In the first stage, we use Expectation Maximization-based approach to learn the regulating mechanism between epigenomics (e.g., gene methylation) and gene expression while considering functional gene annotations. In the second stage, we group genes based on the regulating mechanism learned in the first stage. Then, we apply a group-wise penalty to select genes significantly associated with clinical outcomes while incorporating clinical features. Simulation studies suggest that our model-based data integration method shows lower false positives in selecting predictive variables compared with existing method. Moreover, real data analysis based on a glioblastoma (GBM) dataset reveals our method&#39;s potential to detect genes associated with GBM survival with higher accuracy than the existing method. Moreover, most of the selected biomarkers are crucial in GBM prognosis as confirmed by existing literature.},
  archive  = {J},
  author   = {Hao Xue and Sounak Chakraborty and Tanujit Dey},
  doi      = {10.1002/sam.11682},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11682},
  title    = {Bayesian shrinkage models for integration and analysis of multiplatform high-dimensional genomics data},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomized multiarm bandits: An improved adaptive data
collection method. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(2), e11681. (<a
href="https://doi.org/10.1002/sam.11681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In many scientific experiments, multiarmed bandits are used as an adaptive data collection method. However, this adaptive process can lead to a dependence that renders many commonly used statistical inference methods invalid. An example of this is the sample mean, which is a natural estimator of the mean parameter but can be biased. This can cause test statistics based on this estimator to have an inflated type I error rate, and the resulting confidence intervals may have significantly lower coverage probabilities than their nominal values. To address this issue, we propose an alternative approach called randomized multiarm bandits (rMAB). This combines a randomization step with a chosen MAB algorithm, and by selecting the randomization probability appropriately, optimal regret can be achieved asymptotically. Numerical evidence shows that the bias of the sample mean based on the rMAB is much smaller than that of other methods. The test statistic and confidence interval produced by this method also perform much better than its competitors.},
  archive  = {J},
  author   = {Zhigen Zhao and Tong Wang and Bo Ji},
  doi      = {10.1002/sam.11681},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11681},
  title    = {Randomized multiarm bandits: An improved adaptive data collection method},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning under the cox model with interval-censored
data. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(2), e11680. (<a
href="https://doi.org/10.1002/sam.11680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transfer learning, focusing on information borrowing to address limited sample size issues, has gained increasing attention in recent years. Our method aims to utilize data from other population groups as a complement to enhance risk factor discernment and failure time prediction among underrepresented subgroups. However, a literature gap exists in effective knowledge transfer from the source to the target for risk assessment with interval-censored data while accommodating population incomparability and privacy constraints. Our objective is to bridge this gap by developing a transfer learning approach under the Cox proportional hazards model. We introduce the tuning-free Trans-Cox-MIC algorithm, enabling adaptable information sharing in regression coefficients and baseline hazards, while ensuring computational efficiency. Our approach accommodates covariate distribution shifts, coefficient variations, and baseline hazard discrepancies. Extensive simulations showcase the method&#39;s accuracy, robustness, and efficiency. Application to the prostate cancer screening data demonstrates enhanced risk estimation precision and predictive performance in the African American population.},
  archive  = {J},
  author   = {Mengqi Xie and Tao Hu and Jie Zhou},
  doi      = {10.1002/sam.11680},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11680},
  title    = {Transfer learning under the cox model with interval-censored data},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven stochastic model for quantifying the interplay
between amyloid-beta and calcium levels in alzheimer’s disease.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(2), e11679. (<a
href="https://doi.org/10.1002/sam.11679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The abnormal aggregation of extracellular amyloid- β ( A β ) $$ \left( A\beta \right) $$ in senile plaques resulting in calcium Ca + 2 $$ \left({Ca}^{+2}\right) $$ dyshomeostasis is one of the primary symptoms of Alzheimer&#39;s disease (AD). Significant research efforts have been devoted in the past to better understand the underlying molecular mechanisms driving A β $$ A\beta $$ deposition and Ca + 2 $$ {Ca}^{+2} $$ dysregulation. Importantly, synaptic impairments, neuronal loss, and cognitive failure in AD patients are all related to the buildup of intraneuronal A β $$ A\beta $$ accumulation. Moreover, increasing evidence show a feed-forward loop between A β $$ A\beta $$ and Ca + 2 $$ {Ca}^{+2} $$ levels, that is, A β $$ A\beta $$ disrupts neuronal Ca + 2 $$ {Ca}^{+2} $$ levels, which in turn affects the formation of A β $$ A\beta $$ . To better understand this interaction, we report a novel stochastic model where we analyze the positive feedback loop between A β $$ A\beta $$ and Ca + 2 $$ {Ca}^{+2} $$ using ADNI data. A good therapeutic treatment plan for AD requires precise predictions. Stochastic models offer an appropriate framework for modeling AD since AD studies are observational in nature and involve regular patient visits. The etiology of AD may be described as a multi-state disease process using the approximate Bayesian computation method. So, utilizing ADNI data from 2 $$ 2 $$ -year visits for AD patients, we employ this method to investigate the interplay between A β $$ A\beta $$ and Ca + 2 $$ {Ca}^{+2} $$ levels at various disease development phases. Incorporating the ADNI data in our physics-based Bayesian model, we discovered that a sufficiently large disruption in either A β $$ A\beta $$ metabolism or intracellular Ca + 2 $$ {Ca}^{+2} $$ homeostasis causes the relative growth rate in both Ca + 2 $$ {Ca}^{+2} $$ and A β $$ A\beta $$ , which corresponds to the development of AD. The imbalance of Ca + 2 $$ {Ca}^{+2} $$ ions causes A β $$ A\beta $$ disorders by directly or indirectly affecting a variety of cellular and subcellular processes, and the altered homeostasis may worsen the abnormalities of Ca + 2 $$ {Ca}^{+2} $$ ion transportation and deposition. This suggests that altering the Ca + 2 $$ {Ca}^{+2} $$ balance or the balance between A β $$ A\beta $$ and Ca + 2 $$ {Ca}^{+2} $$ by chelating them may be able to reduce disorders associated with AD and open up new research possibilities for AD therapy.},
  archive  = {J},
  author   = {Hina Shaheen and Roderick Melnik and Sundeep Singh},
  doi      = {10.1002/sam.11679},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11679},
  title    = {Data-driven stochastic model for quantifying the interplay between amyloid-beta and calcium levels in alzheimer&#39;s disease},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A treeless absolutely random forest with closed-form
estimators of expected proximities. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>17</em>(2), e11678. (<a
href="https://doi.org/10.1002/sam.11678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce a simple variant of a purely random forest, called an absolute random forest (ARF) used for clustering. At every node , splits of units are determined by a randomly chosen feature and a random threshold drawn from a uniform distribution whose support, the range of the selected feature in the root node , does not change. This enables closed-form estimators of parameters, such as pairwise proximities, to be obtained without having to grow a forest . The probabilistic structure corresponding to an ARF is called a treeless absolute random forest (TARF). With high probability, the algorithm will split units whose feature vectors are far apart and keep together units whose feature vectors are similar. Thus, the underlying structure of the data drives the growth of the tree. The expected value of pairwise proximities is obtained for three pathway functions. One, a completely common pathway function, is an indicator of whether a pair of units follow the same path from the root to the leaf node. The properties of TARF-based proximity estimators for clustering and classification are compared to other methods in eight real-world datasets and in simulations. Results show substantial performance and computing efficiencies of particular value for large datasets.},
  archive  = {J},
  author   = {Eugene Laska and Ziqiang Lin and Carole Siegel and Charles Marmar},
  doi      = {10.1002/sam.11678},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11678},
  title    = {A treeless absolutely random forest with closed-form estimators of expected proximities},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expert-in-the-loop design of integral nuclear data
experiments. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>17</em>(2), e11677. (<a
href="https://doi.org/10.1002/sam.11677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nuclear data are fundamental inputs to radiation transport codes used for reactor design and criticality safety. The design of experiments to reduce nuclear data uncertainty has been a challenge for many years, but advances in the sensitivity calculations of radiation transport codes within the last two decades have made optimal experimental design possible. The design of integral nuclear experiments poses numerous challenges not emphasized in classical optimal design, in particular, constrained design spaces (in both a statistical and engineering sense), severely under-determined systems, and optimality uncertainty. We present a design pipeline to optimize critical experiments that uses constrained Bayesian optimization within an iterative expert-in-the-loop framework. We show a successfully completed experiment campaign designed with this framework that involved two critical configurations and multiple measurements that targeted compensating errors in 239 Pu nuclear data.},
  archive  = {J},
  author   = {Isaac Michaud and Michael Grosskopf and Jesson Hutchinson and Scott Vander Wiel},
  doi      = {10.1002/sam.11677},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11677},
  title    = {Expert-in-the-loop design of integral nuclear data experiments},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hub-aware random walk graph embedding methods for
classification. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>17</em>(2), e11676. (<a
href="https://doi.org/10.1002/sam.11676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the last two decades, we are witnessing a huge increase of valuable big data structured in the form of graphs or networks. To apply traditional machine learning and data analytic techniques to such data it is necessary to transform graphs into vector-based representations that preserve the most essential structural properties of graphs. For this purpose, a large number of graph embedding methods have been proposed in the literature. Most of them produce general-purpose embeddings suitable for a variety of applications such as node clustering, node classification, graph visualization and link prediction. In this article, we propose two novel graph embedding algorithms based on random walks that are specifically designed for the node classification problem. Random walk sampling strategies of the proposed algorithms have been designed to pay special attention to hubs–high-degree nodes that have the most critical role for the overall connectedness in large-scale graphs. The proposed methods are experimentally evaluated by analyzing the classification performance of three classification algorithms trained on embeddings of real-world networks. The obtained results indicate that our methods considerably improve the predictive power of examined classifiers compared with currently the most popular random walk method for generating general-purpose graph embeddings (node2vec).},
  archive  = {J},
  author   = {Aleksandar Tomčić and Miloš Savić and Miloš Radovanović},
  doi      = {10.1002/sam.11676},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11676},
  title    = {Hub-aware random walk graph embedding methods for classification},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-uniform active learning for gaussian process models with
applications to trajectory informed aerodynamic databases.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(2), e11675. (<a
href="https://doi.org/10.1002/sam.11675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The ability to non-uniformly weight the input space is desirable for many applications, and has been explored for space-filling approaches. Increased interests in linking models, such as in a digital twinning framework, increases the need for sampling emulators where they are most likely to be evaluated. In particular, we apply non-uniform sampling methods for the construction of aerodynamic databases. This paper combines non-uniform weighting with active learning for Gaussian Processes (GPs) to develop a closed-form solution to a non-uniform active learning criterion. We accomplish this by utilizing a kernel density estimator as the weight function. We demonstrate the need and efficacy of this approach with an atmospheric entry example that accounts for both model uncertainty as well as the practical state space of the vehicle, as determined by forward modeling within the active learning loop.},
  archive  = {J},
  author   = {Kevin R. Quinlan and Jagadeesh Movva and Brad Perfect},
  doi      = {10.1002/sam.11675},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11675},
  title    = {Non-uniform active learning for gaussian process models with applications to trajectory informed aerodynamic databases},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compositional variable selection in quantile regression for
microbiome data with false discovery rate control. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(2), e11674. (<a
href="https://doi.org/10.1002/sam.11674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Advancement in high-throughput sequencing technologies has stimulated intensive research interests to identify specific microbial taxa that are associated with disease conditions. Such knowledge is invaluable both from the perspective of understanding biology and from the biomedical perspective of therapeutic development, as the microbiome is inherently modifiable. Despite availability of massive data, analysis of microbiome compositional data remains difficult. The nature that relative abundances of all components of a microbial community sum to one poses challenges for statistical analysis, especially in high-dimensional settings, where a common research theme is to select a small fraction of signals from amid many noisy features. Motivated by studies examining the role of microbiome in host transcriptomics, we propose a novel approach to identify microbial taxa that are associated with host gene expressions. Besides accommodating compositional nature of microbiome data, our method both achieves FDR-controlled variable selection, and captures heterogeneity due to either heteroscedastic variance or non-location-scale covariate effects displayed in the motivating dataset. We demonstrate the superior performance of our method using extensive numerical simulation studies and then apply it to real-world microbiome data analysis to gain novel biological insights that are missed by traditional mean-based linear regression analysis.},
  archive  = {J},
  author   = {Runze Li and Jin Mu and Songshan Yang and Cong Ye and Xiang Zhan},
  doi      = {10.1002/sam.11674},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11674},
  title    = {Compositional variable selection in quantile regression for microbiome data with false discovery rate control},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of nonparametric quantifiers for online
handwritten signature verification: A statistical learning approach.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(2), e11673. (<a
href="https://doi.org/10.1002/sam.11673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work explores the use of nonparametric quantifiers in the signature verification problem of handwritten signatures. We used the MCYT-100 (MCYT Fingerprint subcorpus) database, widely used in signature verification problems. The discrete-time sequence positions in the x -axis and y -axis provided in the database are preprocessed, and time causal information based on nonparametric quantifiers such as entropy, complexity, Fisher information, and trend are employed. The study also proposes to evaluate these quantifiers with the time series obtained, applying the first and second derivatives of each sequence position to evaluate the dynamic behavior by looking at their velocity and acceleration regimes, respectively. The signatures in the MCYT-100 database are classified via Logistic Regression, Support Vector Machines (SVM), Random Forest, and Extreme Gradient Boosting (XGBoost). The quantifiers were used as input features to train the classifiers. To assess the ability and impact of nonparametric quantifiers to distinguish forgery and genuine signatures, we used variable selection criteria, such as: information gain, analysis of variance, and variance inflation factor. The performance of classifiers was evaluated by measures of classification error such as specificity and area under the curve. The results show that the SVM and XGBoost classifiers present the best performance.},
  archive  = {J},
  author   = {Raydonal Ospina and Ranah Duarte Costa and Leandro Chaves Rêgo and Fernando Marmolejo-Ramos},
  doi      = {10.1002/sam.11673},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11673},
  title    = {Application of nonparametric quantifiers for online handwritten signature verification: A statistical learning approach},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smart data augmentation: One equation is all you need.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(2), e11672. (<a
href="https://doi.org/10.1002/sam.11672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Class imbalance is a common and critical challenge in machine learning classification problems, resulting in low prediction accuracy. While numerous methods, especially data augmentation methods, have been proposed to address this issue, a method that works well on one dataset may perform poorly on another. To the best of our knowledge, there is still no one single best approach for handling class imbalance that can be uniformly applied. In this paper, we propose an approach named smart data augmentation (SDA), which aims to augment imbalanced data in an optimal way to maximize downstream classification accuracy. The key novelty of SDA is an equation that can bring about an augmentation method that provides a unified representation of existing sampling methods for handling multi-level class imbalance and allows easy fine-tuning. This framework allows SDA to be seen as a generalization of traditional methods, which in turn can be viewed as specific cases of SDA. Empirical results on a wide range of datasets demonstrate that SDA could significantly improve the performance of the most popular classifiers such as random forest, multi-layer perceptron, and histogram-based gradient boosting.},
  archive  = {J},
  author   = {Yuhao Zhang and Lu Tang and Yuxiao Huang and Yan Ma},
  doi      = {10.1002/sam.11672},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11672},
  title    = {Smart data augmentation: One equation is all you need},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The finite mixture model for the tails of distribution:
Monte carlo experiment and empirical applications. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(2), e11671. (<a
href="https://doi.org/10.1002/sam.11671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The finite mixture model estimates regression coefficients distinct in each of the different groups of the dataset endogenously determined by this estimator. In what follows the analysis is extended beyond the mean, estimating the model in the tails of the conditional distribution of the dependent variable within each group. While the clustering reduces the overall heterogeneity, since the model is estimated for groups of similar observations, the analysis in the tails uncovers within groups heterogeneity and/or skewness. By integrating the endogenously determined clustering with the quantile regression analysis within each group, enhances the finite mixture models and focuses on the tail behavior of the conditional distribution of the dependent variable. A Monte Carlo experiment and two empirical applications conclude the analysis. In the well-known birthweight dataset, the finite mixture model identifies and computes the regression coefficients of different groups, each one with its own characteristics, both at the mean and in the tails. In the family expenditure data, the analysis of within and between groups heterogeneity provides interesting economic insights on price elasticities. The analysis in classes proves to be more efficient than the model estimated without clustering. By extending the finite mixture approach to the tails provides a more accurate investigation of the data, introducing a robust tool to unveil sources of within groups heterogeneity and asymmetry otherwise left undetected. It improves efficiency and explanatory power with respect to the standard OLS-based FMM.},
  archive  = {J},
  author   = {Marilena Furno and Francesco Caracciolo},
  doi      = {10.1002/sam.11671},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11671},
  title    = {The finite mixture model for the tails of distribution: Monte carlo experiment and empirical applications},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ERPCA: Robust principal component analysis for exponential
family distributions. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(2), e11670. (<a
href="https://doi.org/10.1002/sam.11670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robust principal component analysis (RPCA) is a widely used method for recovering low-rank structure from data matrices corrupted by significant and sparse outliers. These corruptions may arise from occlusions, malicious tampering, or other causes for anomalies, and the joint identification of such corruptions with low-rank background is critical for process monitoring and diagnosis. However, existing RPCA methods and their extensions largely do not account for the underlying probabilistic distribution for the data matrices, which in many applications are known and can be highly non-Gaussian. We thus propose a new method called RPCA for exponential family distributions ( e RPCA $$ {e}^{\mathrm{RPCA}} $$ ), which can perform the desired decomposition into low-rank and sparse matrices when such a distribution falls within the exponential family. We present a novel alternating direction method of multiplier optimization algorithm for efficient e RPCA $$ {e}^{\mathrm{RPCA}} $$ decomposition, under either its natural or canonical parametrization. The effectiveness of e RPCA $$ {e}^{\mathrm{RPCA}} $$ is then demonstrated in two applications: the first for steel sheet defect detection and the second for crime activity monitoring in the Atlanta metropolitan area.},
  archive  = {J},
  author   = {Xiaojun Zheng and Simon Mak and Liyan Xie and Yao Xie},
  doi      = {10.1002/sam.11670},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11670},
  title    = {ERPCA: Robust principal component analysis for exponential family distributions},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online learning for streaming data classification in
nonstationary environments. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>17</em>(2), e11669. (<a
href="https://doi.org/10.1002/sam.11669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we implement the classification of nonstationary streaming data. Due to the inability to obtain full data in the context of streaming data, we adopt a strategy based on clustering structure for data classification. Specifically, this strategy involves dynamically maintaining clustering structures to update the model, thereby updating the objective function for classification. Simultaneously, incoming samples are monitored in real-time to identify the emergence of new classes or the presence of outliers. Moreover, this strategy can also deal with the concept drift problem, where the distribution of data changes with the inflow of data. Regarding the handling of novel instances, we introduce a buffer analysis mechanism to delay their processing, which in turn improves the prediction performance of the model. In the process of model updating, we also introduce a novel renewable strategy for the covariance matrix. Numerical simulations and experiments on datasets show that our method has significant advantages.},
  archive  = {J},
  author   = {Yujie Gai and Kang Meng and Xiaodi Wang},
  doi      = {10.1002/sam.11669},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11669},
  title    = {Online learning for streaming data classification in nonstationary environments},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Marginal clustered multistate models for longitudinal
progressive processes with informative cluster size. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(2), e11668. (<a
href="https://doi.org/10.1002/sam.11668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Informative cluster size (ICS) is a phenomenon where cluster size is related to the outcome. While multistate models can be applied to characterize the unit-level transition process for clustered interval-censored data, there is a research gap addressing ICS within this framework. We propose two extensions of multistate model that account for ICS to make marginal inference: one by incorporating within-cluster resampling and another by constructing cluster-weighted score functions. We evaluate the performances of the proposed methods through simulation studies and apply them to the Veterans Affairs Dental Longitudinal Study (VADLS) to understand the effect of risk factors on periodontal disease progression. ICS occurs frequently in dental data, particularly in the study of periodontal disease, as people with fewer teeth due to the disease are more susceptible to disease progression. According to the simulation results, the mean estimates of the parameters obtained from the proposed methods are close to the true values, but methods that ignore ICS can lead to substantial bias. Our proposed methods for clustered multistate model are able to appropriately take ICS into account when making marginal inference of a typical unit from a randomly sampled cluster.},
  archive  = {J},
  author   = {Sean Xinyang Feng and Aya A. Mitani},
  doi      = {10.1002/sam.11668},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11668},
  title    = {Marginal clustered multistate models for longitudinal progressive processes with informative cluster size},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Error-controlled feature selection for ultrahigh-dimensional
and highly correlated feature space using deep learning. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(2), e11664. (<a
href="https://doi.org/10.1002/sam.11664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep learning has been at the center of analytics in recent years due to its impressive empirical success in analyzing complex data objects. Despite this success, most existing tools behave like black-box machines, thus the increasing interest in interpretable, reliable, and robust deep learning models applicable to a broad class of applications. Feature-selected deep learning has emerged as a promising tool in this realm. However, the recent developments do not accommodate ultrahigh-dimensional and highly correlated features or high noise levels. In this article, we propose a novel screening and cleaning method with the aid of deep learning for a data-adaptive multi-resolutional discovery of highly correlated predictors with a controlled error rate. Extensive empirical evaluations over a wide range of simulated scenarios and several real datasets demonstrate the effectiveness of the proposed method in achieving high power while keeping the false discovery rate at a minimum.},
  archive  = {J},
  author   = {Arkaprabha Ganguli and Tapabrata Maiti and David Todem},
  doi      = {10.1002/sam.11664},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {e11664},
  title    = {Error-controlled feature selection for ultrahigh-dimensional and highly correlated feature space using deep learning},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian inference for nonprobability samples with
nonignorable missingness. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>17</em>(1), e11667. (<a
href="https://doi.org/10.1002/sam.11667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nonprobability samples, especially web survey data, have been available in many different fields. However, nonprobability samples suffer from selection bias, which will yield biased estimates. Moreover, missingness, especially nonignorable missingness, may also be encountered in nonprobability samples. Thus, it is a challenging task to make inference from nonprobability samples with nonignorable missingness. In this article, we propose a Bayesian approach to infer the population based on nonprobability samples with nonignorable missingness. In our method, different Logistic regression models are employed to estimate the selection probabilities and the response probabilities; the superpopulation model is used to explain the relationship between the study variable and covariates. Further, Bayesian and approximate Bayesian methods are proposed to estimate the response model parameters and the superpopulation model parameters, respectively. Specifically, the estimating functions for the response model parameters and superpopulation model parameters are utilized to derive the approximate posterior distribution in superpopulation model estimation. Simulation studies are conducted to investigate the finite sample performance of the proposed method. The data from the Pew Research Center and the Behavioral Risk Factor Surveillance System are used to show better performance of our proposed method over the other approaches.},
  archive  = {J},
  author   = {Zhan Liu and Xuesong Chen and Ruohan Li and Lanbao Hou},
  doi      = {10.1002/sam.11667},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11667},
  title    = {Bayesian inference for nonprobability samples with nonignorable missingness},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling matrix variate time series via hidden markov models
with skewed emissions. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(1), e11666. (<a
href="https://doi.org/10.1002/sam.11666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data collected today have increasingly become more complex and cannot be analyzed using regular statistical methods. Matrix variate time series data is one such example where the observations in the time series are matrices. Herein, we introduce a set of three hidden Markov models using skewed matrix variate emission distributions for modeling matrix variate time series data. Compared to the hidden Markov model with matrix variate normal emissions, the proposed models present greater flexibility and are capable of modeling skewness in time series data. Parameter estimation is performed using an expectation maximization algorithm. We then look at both simulated data and salary data for public Texas universities.},
  archive  = {J},
  author   = {Michael P. B. Gallaugher and Xuwen Zhu},
  doi      = {10.1002/sam.11666},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11666},
  title    = {Modeling matrix variate time series via hidden markov models with skewed emissions},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel two-step extrapolation-insertion risk model based on
the expectile under the pareto-type distribution. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(1), e11665. (<a
href="https://doi.org/10.1002/sam.11665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The catastrophe loss model developed is a challenging problem in the insurance industry. In the context of Pareto-type distribution, measuring risk at the extreme right tail has become a major focus for academic research. The quantile and Expectile of distribution are found to be useful descriptors of its tail, in the same way as the median and mean are related to its central behavior. In this article, a novel two-step extrapolation-insertion method is introduced and proved its advantages of less bias and variance theoretically through asymptotic normality by modifying the existing far-right tail numerical model using the risk measures of Expectile and Expected Shortfall (ES). In addition, another solution to obtain the ES is proposed based on the fitted extreme distribution, which is demonstrated to have superior unbiased statistical properties. Uniting these two methods provides the numerical interval upper and lower bounds for capturing the real quantile-based ES commonly used in insurance. The numerical simulation and the empirical analysis results of Danish reinsurance claim data indicate that these methods offer high prediction accuracy in the applications of catastrophe risk management.},
  archive  = {J},
  author   = {Ziwen Geng},
  doi      = {10.1002/sam.11665},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11665},
  title    = {A novel two-step extrapolation-insertion risk model based on the expectile under the pareto-type distribution},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse bayesian variable selection in high-dimensional
logistic regression models with correlated priors. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(1), e11663. (<a
href="https://doi.org/10.1002/sam.11663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose a sparse Bayesian procedure with global and local (GL) shrinkage priors for the problems of variable selection and classification in high-dimensional logistic regression models. In particular, we consider two types of GL shrinkage priors for the regression coefficients, the horseshoe (HS) prior and the normal-gamma (NG) prior, and then specify a correlated prior for the binary vector to distinguish models with the same size. The GL priors are then combined with mixture representations of logistic distribution to construct a hierarchical Bayes model that allows efficient implementation of a Markov chain Monte Carlo (MCMC) to generate samples from posterior distribution. We carry out simulations to compare the finite sample performances of the proposed Bayesian method with the existing Bayesian methods in terms of the accuracy of variable selection and prediction. Finally, two real-data applications are provided for illustrative purposes.},
  archive  = {J},
  author   = {Zhuanzhuan Ma and Zifei Han and Souparno Ghosh and Liucang Wu and Min Wang},
  doi      = {10.1002/sam.11663},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11663},
  title    = {Sparse bayesian variable selection in high-dimensional logistic regression models with correlated priors},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rarity updated ensemble with oversampling: An ensemble
approach to classification of imbalanced data streams. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(1), e11662. (<a
href="https://doi.org/10.1002/sam.11662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Today&#39;s ever-increasing generation of streaming data demands novel data mining approaches tailored to mining dynamic data streams. Data streams are non-static in nature, continuously generated, and endless. They often suffer from class imbalance and undergo temporal drift. To address the classification of consecutive data instances within imbalanced data streams, this research introduces a new ensemble classification algorithm called Rarity Updated Ensemble with Oversampling (RUEO). The RUEO approach is specifically designed to exhibit robustness against class imbalance by incorporating an imbalance-specific criterion to assess the efficacy of the base classifiers and employing an oversampling technique to reduce the imbalance in the training data. The RUEO algorithm was evaluated on a set of 20 data streams and compared against 14 baseline algorithms. On average, the proposed RUEO algorithm achieves an average-accuracy of 0.69 on the real-world data streams, while the chunk-based algorithms AWE, AUE, and KUE achieve average-accuracies of 0.48, 0.65, and 0.66, respectively. The statistical analysis, conducted using the Wilcoxon test, reveals a statistically significant improvement in average-accuracy for the proposed RUEO algorithm when compared to 12 out of the 14 baseline algorithms. The source code and experimental results of this research work will be publicly available at https://github.com/vkiani/RUEO .},
  archive  = {J},
  author   = {Zahra Nouri and Vahid Kiani and Hamid Fadishei},
  doi      = {10.1002/sam.11662},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11662},
  title    = {Rarity updated ensemble with oversampling: An ensemble approach to classification of imbalanced data streams},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subsampling under distributional constraints.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(1), e11661. (<a
href="https://doi.org/10.1002/sam.11661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Some complex models are frequently employed to describe physical and mechanical phenomena. In this setting, we have an input X $$ X $$ in a general space, and an output Y = f ⁡ ( X ) $$ Y=f(X) $$ where f $$ f $$ is a very complicated function, whose computational cost for every new input is very high, and may be also very expensive. We are given two sets of observations of X $$ X $$ , S 1 $$ {S}_1 $$ and S 2 $$ {S}_2 $$ of different sizes such that only f ⁡ ( S 1 ) $$ f\left({S}_1\right) $$ is available. We tackle the problem of selecting a subset S 3 ⊂ S 2 $$ {S}_3\subset {S}_2 $$ of smaller size on which to run the complex model f $$ f $$ , and such that the empirical distribution of f ⁡ ( S 3 ) $$ f\left({S}_3\right) $$ is close to that of f ⁡ ( S 1 ) $$ f\left({S}_1\right) $$ . We suggest three algorithms to solve this problem and show their efficiency using simulated datasets and the Airfoil self-noise data set.},
  archive  = {J},
  author   = {Florian Combes and Ricardo Fraiman and Badih Ghattas},
  doi      = {10.1002/sam.11661},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11661},
  title    = {Subsampling under distributional constraints},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning approach for the comparison of handwritten
documents using latent feature vectors. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>17</em>(1), e11660.
(<a href="https://doi.org/10.1002/sam.11660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Forensic questioned document examiners still largely rely on visual assessments and expert judgment to determine the provenance of a handwritten document. Here, we propose a novel approach to objectively compare two handwritten documents using a deep learning algorithm. First, we implement a bootstrapping technique to segment document data into smaller units, as a means to enhance the efficiency of the deep learning process. Next, we use a transfer learning algorithm to systematically extract document features. The unique characteristics of the document data are then represented as latent vectors. Finally, the similarity between two handwritten documents is quantified via the cosine similarity between the two latent vectors. We illustrate the use of the proposed method by implementing it on a variety of collections of handwritten documents with different attributes, and show that in most cases, we can accurately classify pairs of documents into same or different author categories.},
  archive  = {J},
  author   = {Juhyeon Kim and Soyoung Park and Alicia Carriquiry},
  doi      = {10.1002/sam.11660},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11660},
  title    = {A deep learning approach for the comparison of handwritten documents using latent feature vectors},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated alignment algorithm for identification of the
source of footwear impressions with common class characteristics.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(1), e11659. (<a
href="https://doi.org/10.1002/sam.11659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce an algorithmic approach designed to compare similar shoeprint images, with automated alignment. Our method employs the Iterative Closest Points (ICP) algorithm to attain optimal alignment, further enhancing precision through phase-only correlation. Utilizing diverse metrics to quantify similarity, we train a random forest model to predict the empirical probability that two impressions originate from the same shoe. Experimental evaluations using high-quality two-dimensional shoeprints showcase our proposed algorithm&#39;s robustness in managing dissimilarities between impressions from the same shoe, outperforming existing approaches.},
  archive  = {J},
  author   = {Hana Lee and Alicia Carriquiry and Soyoung Park},
  doi      = {10.1002/sam.11659},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11659},
  title    = {An automated alignment algorithm for identification of the source of footwear impressions with common class characteristics},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imputed quantile vector autoregressive model for
multivariate spatial–temporal data. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>17</em>(1), e11658. (<a
href="https://doi.org/10.1002/sam.11658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Imputing missing values in multivariate spatial–temporal data is important in many fields. Existing low rank tensor learning methods are popular for handling this task but are sensitive to high level of skewness. The aim of this paper is to develop an alternative method with robustness and high imputation accuracy for multivariate spatial–temporal data. In view of the fact that quantile regression is robust to noises and outliers, we propose an imputed quantile vector autoregressive (IQVAR) model. IQVAR can simultaneously impute missing values and estimate parameters of quantile vector autoregressive model. The objective function includes check loss and nuclear norm penalization. We develop an ADMM (Alternating Direction Method of Multipliers) algorithm to solve the resulting optimization problem. Simulation studies and real data analysis are conducted to verify the efficiency of IQVAR. Compared with other approaches, IQVAR is more robust and accurate.},
  archive  = {J},
  author   = {Liang Jinwen and Tian Maozai},
  doi      = {10.1002/sam.11658},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11658},
  title    = {Imputed quantile vector autoregressive model for multivariate spatial–temporal data},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric bayesian functional clustering with
applications to racial disparities in breast cancer. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(1), e11657. (<a
href="https://doi.org/10.1002/sam.11657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As we have easier access to massive data sets, functional analyses have gained more interest. However, such data sets often contain large heterogeneities, noises, and dimensionalities. When generalizing the analyses from vectors to functions, classical methods might not work directly. This paper considers noisy information reduction in functional analyses from two perspectives: functional clustering to group similar observations and thus reduce the sample size and functional variable selection to reduce the dimensionality. The complicated data structures and relations can be easily modeled by a Bayesian hierarchical model due to its flexibility. Hence, this paper proposes a nonparametric Bayesian functional clustering and peak point selection method via weighted Dirichlet process mixture (WDPM) modeling that automatically clusters and provides accurate estimations, together with conditional Laplace prior, which is a conjugate variable selection prior. The proposed method is named WDPM-VS for short, and is able to simultaneously perform the following tasks: (1) Automatic cluster without specifying the number of clusters or cluster centers beforehand; (2) Cluster for heterogeneously behaved functions; (3) Select vibrational peak points; and (4) Reduce noisy information from the two perspectives: sample size and dimensionality. The method will greatly outperform its comparison methods in root mean squared errors. Based on this proposed method, we are able to identify biological factors that can explain the breast cancer racial disparities.},
  archive  = {J},
  author   = {Wenyu Gao and Inyoung Kim and Wonil Nam and Xiang Ren and Wei Zhou and Masoud Agah},
  doi      = {10.1002/sam.11657},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11657},
  title    = {Nonparametric bayesian functional clustering with applications to racial disparities in breast cancer},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Study of a bounded interval perks distribution with quantile
regression analysis. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>17</em>(1), e11656. (<a
href="https://doi.org/10.1002/sam.11656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, a novel bounded interval model called the unit-Perks model is developed by suitably transforming the positive random variable of the Perks distribution. Numerous statistical features of the bounded interval Perks model are being explored based on the expansion of the density function. Eight distinct estimation approaches are being used to estimate the parameters of the unit-Perks model. A throughout simulation analysis is also included to evaluate the precision of the resulting estimators from eight estimating approaches. Two real bounded interval data sets are being utilized to investigate the practical applicability of the unit-Perks model. A comparison is also made to determine which method of estimation works better for the given model. According to a comparison of eight different estimation approaches, the maximum likelihood estimation approach outperformed than the other seven estimating approaches. The unit-perks model is then used to introduce the quantile regression model named as quantile unit-Perks distribution. Application to real data set for the quantile unit-Perks distribution is also performed. The quantile residuals are used for the residual analysis of the fitted regression model. On the basis of mathematical, computational, and pictorial evidences, it is concluded that the presented model exhibited greater modeling capabilities.},
  archive  = {J},
  author   = {Laila A. Al-Essa and Shakaiba Shafiq and Deniz Ozonur and Farrukh Jamal},
  doi      = {10.1002/sam.11656},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11656},
  title    = {Study of a bounded interval perks distribution with quantile regression analysis},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Considerations in bayesian agent-based modeling for the
analysis of COVID-19 data. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>17</em>(1), e11655. (<a
href="https://doi.org/10.1002/sam.11655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Agent-based model (ABM) has been widely used to study infectious disease transmission by simulating behaviors and interactions of autonomous individuals called agents. In the ABM, agent states, for example infected or susceptible, are assigned according to a set of simple rules, and a complex dynamics of disease transmission is described by the collective states of agents over time. Despite the flexibility in real-world modeling, ABMs have received less attention by statisticians because of the intractable likelihood functions which lead to difficulty in estimating parameters and quantifying uncertainty around model outputs. To overcome this limitation, a Bayesian framework that treats the entire ABM as a Hidden Markov Model has been previously proposed. However, existing approach is limited due to computational inefficiency and unidentifiability of parameters. We extend the ABM approach within Bayesian framework to study infectious disease transmission addressing these limitations. We estimate the hidden states, represented by individual agent&#39;s states over time, and the model parameters by applying an improved particle Markov Chain Monte Carlo algorithm, that accounts for computing efficiency. We further evaluate the performance of the approach for parameter recovery and prediction, along with sensitivity to prior assumptions under various simulation conditions. Finally, we apply the proposed approach to the study of COVID-19 outbreak on Diamond Princess cruise ship. We examine the differences in transmission by key demographic characteristics, while considering two different networks and limited COVID-19 testing in the cruise.},
  archive  = {J},
  author   = {Seungha Um and Samrachana Adhikari},
  doi      = {10.1002/sam.11655},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11655},
  title    = {Considerations in bayesian agent-based modeling for the analysis of COVID-19 data},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting diversity in regression ensembles. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(1), e11654. (<a
href="https://doi.org/10.1002/sam.11654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Ensemble methods, such as Bagging, Boosting, or Random Forests, often enhance the prediction performance of single learners on both classification and regression tasks. In the context of regression, we propose a gradient boosting-based algorithm incorporating a diversity term with the aim of constructing different learners that enrich the ensemble while achieving a trade-off of some individual optimality for global enhancement. Verifying the hypotheses of Biau and Cadre&#39;s theorem (2021, Advances in contemporary statistics and econometrics—Festschrift in honour of Christine Thomas-Agnan , Springer), we present a convergence result ensuring that the associated optimization strategy reaches the global optimum. In the experiments, we consider a variety of different base learners with increasing complexity: stumps, regression trees, Purely Random Forests, and Breiman&#39;s Random Forests. Finally, we consider simulated and benchmark datasets and a real-world electricity demand dataset to show, by means of numerical experiments, the suitability of our procedure by examining the behavior not only of the final or the aggregated predictor but also of the whole generated sequence.},
  archive  = {J},
  author   = {Mathias Bourel and Jairo Cugliari and Yannig Goude and Jean-Michel Poggi},
  doi      = {10.1002/sam.11654},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11654},
  title    = {Boosting diversity in regression ensembles},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate contaminated normal mixture regression modeling
of longitudinal data based on joint mean-covariance model.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(1), e11653. (<a
href="https://doi.org/10.1002/sam.11653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Outliers are common in longitudinal data analysis, and the multivariate contaminated normal (MCN) distribution in model-based clustering is often used to detect outliers and provide robust parameter estimates in each subgroup. In this paper, we propose a method, the mixture of MCN (MCNM), based on the joint mean-covariance model, specifically designed to analyze longitudinal data characterized by mild outliers. Our model can automatically detect outliers in longitudinal data and provide robust parameter estimates in each subgroup. We use iteratively expectation-conditional maximization (ECM) algorithm and Aitken acceleration to estimate the model parameters, achieving both algorithm acceleration and stable convergence. Our proposed method simultaneously clusters the population, identifies progression patterns of the mean and covariance structures for different subgroups over time, and detects outliers. To demonstrate the effectiveness of our method, we conduct simulation studies under various cases involving different proportions and degrees of contamination. Additionally, we apply our method to real data on the number of people infected with AIDS in 49 countries or regions from 2001 to 2021. Results show that our proposed method effectively clusters the data based on various mean progression trajectories. In summary, our proposed MCNM based on the joint mean-covariance model and MCD of covariance matrices provides a robust method for clustering longitudinal data with mild outliers. It effectively detects outliers and identifies progression patterns in different groups over time, making it valuable for various applications in longitudinal data analysis.},
  archive  = {J},
  author   = {Niu Xiaoyu and Tian Yuzhu and Tang Manlai and Tian Maozai},
  doi      = {10.1002/sam.11653},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11653},
  title    = {Multivariate contaminated normal mixture regression modeling of longitudinal data based on joint mean-covariance model},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The generalized hyperbolic family and automatic model
selection through the multiple-choice LASSO. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>17</em>(1),
e11652. (<a href="https://doi.org/10.1002/sam.11652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We revisit the generalized hyperbolic (GH) distribution and its nested models. These include widely used parametric choices like the multivariate normal, skew- t $$ t $$ , Laplace, and several others. We also introduce the multiple-choice LASSO, a novel penalized method for choosing among alternative constraints on the same parameter. A hierarchical multiple-choice Least Absolute Shrinkage and Selection Operator (LASSO) penalized likelihood is optimized to perform simultaneous model selection and inference within the GH family. We illustrate our approach through a simulation study and a real data example. The methodology proposed in this paper has been implemented in R functions which are available as supplementary material.},
  archive  = {J},
  author   = {Luca Bagnato and Alessio Farcomeni and Antonio Punzo},
  doi      = {10.1002/sam.11652},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11652},
  title    = {The generalized hyperbolic family and automatic model selection through the multiple-choice LASSO},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A machine learning oracle for parameter estimation.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(1), e11651. (<a
href="https://doi.org/10.1002/sam.11651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Competing procedures, involving data smoothing, weighting, imputation, outlier removal, etc., may be available to prepare data for parametric model estimation. Often, however, little is known about the best choice of preparatory procedure for the planned estimation and the observed data. A machine learning-based decision rule, an “oracle,” can be constructed in such cases to decide the best procedure from a set C $$ \mathcal{C} $$ of available preparatory procedures. The oracle learns the decision regions associated with C $$ \mathcal{C} $$ based on training data synthesized solely from the given data using model parameters with high posterior probability. An estimator in combination with an oracle to guide data preparation is called an oracle estimator. Oracle estimator performance is studied in two estimation problems: slope estimation in simple linear regression (SLR) and changepoint estimation in continuous two-linear-segments regression (CTLSR). In both examples, the regression response is given to be increasing, and the oracle must decide whether to isotonically smooth the response data preparatory to fitting the regression model. A measure of performance called headroom is proposed to assess the oracle&#39;s potential for reducing estimation error. Experiments with SLR and CTLSR find for important ranges of problem configurations that the headroom is high, the oracle&#39;s empirical performance is near the headroom, and the oracle estimator offers clear benefit.},
  archive  = {J},
  author   = {Lucas Koepke and Mary Gregg and Michael Frey},
  doi      = {10.1002/sam.11651},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11651},
  title    = {A machine learning oracle for parameter estimation},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling subpopulations for hierarchically structured data.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(1), e11650. (<a
href="https://doi.org/10.1002/sam.11650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The field of forensic statistics offers a unique hierarchical data structure in which a population is composed of several subpopulations of sources and a sample is collected from each source. This subpopulation structure creates an additional layer of complexity. Hence, the data has a hierarchical structure in addition to the existence of underlying subpopulations. Finite mixtures are known for modeling heterogeneity; however, previous parameter estimation procedures assume that the data is generated through a simple random sampling process. We propose using a semi-supervised mixture modeling approach to model the subpopulation structure which leverages the fact that we know the collection of samples came from the same source, yet an unknown subpopulation. A simulation study and a real data analysis based on famous glass datasets and a keystroke dynamic typing data set show that the proposed approach performs better than other approaches that have been used previously in practice.},
  archive  = {J},
  author   = {Andrew Simpson and Semhar Michael and Dylan Borchert and Christopher Saunders and Larry Tang},
  doi      = {10.1002/sam.11650},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11650},
  title    = {Modeling subpopulations for hierarchically structured data},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatially-correlated time series clustering using
location-dependent dirichlet process mixture model. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>17</em>(1), e11649. (<a
href="https://doi.org/10.1002/sam.11649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Dirichlet process mixture (DPM) model has been widely used as a Bayesian nonparametric model for clustering. However, the exchangeability assumption of the Dirichlet process is not valid for clustering spatially correlated time series as these data are indexed spatially and temporally. While analyzing spatially correlated time series, correlations between observations at proximal times and locations must be appropriately considered. In this study, we propose a location-dependent DPM model by extending the traditional DPM model for clustering spatially correlated time series. We model the temporal pattern as an infinite mixture of Gaussian processes while considering spatial dependency using a location-dependent Dirichlet process prior over mixture components. This encourages the assignment of observations from proximal locations to the same cluster. By contrast, because mixture atoms for modeling temporal patterns are shared across space, observations with similar temporal patterns can be still grouped together even if they are located far apart. The proposed model also allows the number of clusters to be automatically determined in the clustering procedure. We validate the proposed model using simulated examples. Moreover, in a real case study, we cluster adjacent roads based on their traffic speed patterns that have changed as a result of a traffic accident occurred in Seoul, South Korea.},
  archive  = {J},
  author   = {Junsub Jung and Sungil Kim and Heeyoung Kim},
  doi      = {10.1002/sam.11649},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11649},
  title    = {Spatially-correlated time series clustering using location-dependent dirichlet process mixture model},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Input-response space-filling designs incorporating response
uncertainty. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>17</em>(1), e11648. (<a
href="https://doi.org/10.1002/sam.11648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditionally space-filling designs have focused on the characteristics of the design in the input space ensuring uniform spread throughout the region. Input-response space-filling designs considered scenarios when having good spread throughout the range or region of the responses is also of interest. This paper acknowledges that there is typically uncertainty associated with the values of the response(s) and hence proposes a method, Input-Response Space-Filling Designs with Uncertainty (IRSFwU), to incorporate this into the design construction. The Pareto front of designs offers alternatives that balance input and response space filling, while prioritizing input combinations with lower associated response uncertainty. These lower uncertainty choices improve the chances of observing the desired response values. We describe the new approach with an uncertainty-adjusted distance to measure the response space filling, the Pareto aggregate point exchange algorithm to populate the set of promising designs, and illustrate the method with three examples of different input and response relationships and dimensions.},
  archive  = {J},
  author   = {Xiankui Yang and Lu Lu and Christine M. Anderson-Cook},
  doi      = {10.1002/sam.11648},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11648},
  title    = {Input-response space-filling designs incorporating response uncertainty},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Driving mode analysis—how uncertain functional inputs
propagate to an output. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>17</em>(1), e11646. (<a
href="https://doi.org/10.1002/sam.11646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Driving mode analysis elucidates how correlated features of uncertain functional inputs jointly propagate to produce uncertainty in the output of a computation. Uncertain input functions are decomposed into three terms: the mean functions, a zero-mean driving mode, and zero-mean residual. The random driving mode varies along a single direction, having fixed functional shape and random scale. It is uncorrelated with the residual, and under linear error propagation, it produces an output variance equal to that of the full input uncertainty. Finally, the driving mode best represents how input uncertainties propagate to the output because it minimizes expected squared Mahalanobis distance amongst competitors. These characteristics recommend interpretation of the driving mode as the single-degree-of-freedom component of input uncertainty that drives output uncertainty. We derive the functional driving mode, show its superiority to other seemingly sensible definitions, and demonstrate the utility of driving mode analysis in an application. The application is the simulation of neutron transport in criticality experiments. The uncertain input functions are nuclear data that describe how Pu reacts to bombardment by neutrons. Visualization of the driving mode helps scientists understand what aspects of correlated functional uncertainty have effects that either reinforce or cancel one another in propagating to the output of the simulation.},
  archive  = {J},
  author   = {Scott A. Vander Wiel and Michael J. Grosskopf and Isaac J. Michaud and Denise Neudecker},
  doi      = {10.1002/sam.11646},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11646},
  title    = {Driving mode analysis—How uncertain functional inputs propagate to an output},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residuals and diagnostics for multinomial regression models.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>17</em>(1), e11645. (<a
href="https://doi.org/10.1002/sam.11645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we extend the concept of a randomized quantile residual to multinomial regression models. Customary diagnostics for these models are limited because they involve difficult-to-interpret residuals and often focus on the fit of one category versus the rest. Our residuals account for associations between categories by using the squared Mahalanobis distances of the observed log-odds relative to their fitted sampling distributions. Aside from sampling variation, these residuals are exactly normal when the data come from the fitted model. This motivates our use of the residuals to detect model misspecification and overdispersion, in addition to an overall goodness-of-fit Kolmogorov–Smirnov test. We illustrate the use of the residuals and diagnostics in both simulation and real data studies.},
  archive  = {J},
  author   = {Eric A. E. Gerber and Bruce A. Craig},
  doi      = {10.1002/sam.11645},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11645},
  title    = {Residuals and diagnostics for multinomial regression models},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On difference-based gradient estimation in nonparametric
regression. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>17</em>(1), e11644. (<a
href="https://doi.org/10.1002/sam.11644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a framework to directly estimate the gradient in multivariate nonparametric regression models that bypasses fitting the regression function. Specifically, we construct the estimator as a linear combination of adjacent observations with the coefficients from a vector-valued difference sequence, so it is more flexible than existing methods. Under the equidistant designs, closed-form solutions of the optimal sequences are derived by minimizing the estimation variance, with the estimation bias well controlled. We derive the theoretical properties of the estimators and show that they achieve the optimal convergence rate. Further, we propose a data-driven tuning parameter-selection criterion for practical implementation. The effectiveness of our estimators is validated via simulation studies and a real data application.},
  archive  = {J},
  author   = {Maoyu Zhang and Wenlin Dai},
  doi      = {10.1002/sam.11644},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11644},
  title    = {On difference-based gradient estimation in nonparametric regression},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stratified learning: A general-purpose statistical method
for improved learning under covariate shift. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>17</em>(1),
e11643. (<a href="https://doi.org/10.1002/sam.11643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a simple, statistically principled, and theoretically justified method to improve supervised learning when the training set is not representative, a situation known as covariate shift. We build upon a well-established methodology in causal inference and show that the effects of covariate shift can be reduced or eliminated by conditioning on propensity scores. In practice, this is achieved by fitting learners within strata constructed by partitioning the data based on the estimated propensity scores, leading to approximately balanced covariates and much-improved target prediction. We refer to the overall method as Stratified Learning, or StratLearn . We demonstrate the effectiveness of this general-purpose method on two contemporary research questions in cosmology, outperforming state-of-the-art importance weighting methods. We obtain the best-reported AUC (0.958) on the updated “Supernovae photometric classification challenge,” and we improve upon existing conditional density estimation of galaxy redshift from Sloan Digital Sky Survey (SDSS) data.},
  archive  = {J},
  author   = {Maximilian Autenrieth and David A. van Dyk and Roberto Trotta and David C. Stenning},
  doi      = {10.1002/sam.11643},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {e11643},
  title    = {Stratified learning: A general-purpose statistical method for improved learning under covariate shift},
  volume   = {17},
  year     = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
