<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietcv---98">IETCV - 98</h2>
<ul>
<li><details>
<summary>
(2024). High precision light field image depth estimation via
multi-region attention enhanced network. <em>IETCV</em>, <em>18</em>(8),
1390–1406. (<a href="https://doi.org/10.1049/cvi2.12326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) depth estimation is a key task with numerous practical applications. However, achieving high-precision depth estimation in challenging scenarios, such as occlusions and detailed regions (e.g. fine structures and edges), remains a significant challenge. To address this problem, the authors propose a LF depth estimation network based on multi-region selection and guided optimisation. Firstly, we construct a multi-region disparity selection module based on angular patch, which selects specific regions for generating angular patch, achieving representative sub-angular patch by balancing different regions. Secondly, different from traditional guided deformable convolution, the guided optimisation leverages colour prior information to learn the aggregation of sampling points, which enhances the deformable convolution ability by learning deformation parameters and fitting irregular windows. Finally, to achieve high-precision LF depth estimation, the authors have developed a network architecture based on the proposed multi-region disparity selection and guided optimisation module. Experiments demonstrate the effectiveness of network on the HCInew dataset, especially in handling occlusions and detailed regions.},
  archive      = {J_IETCV},
  author       = {Jie Li and Wenxuan Yang and Chuanlun Zhang and Heng Li and Xinjia Li and Lin Wang and Yanling Wang and Xiaoyan Wang},
  doi          = {10.1049/cvi2.12326},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1390-1406},
  shortjournal = {IET Comput. Vis.},
  title        = {High precision light field image depth estimation via multi-region attention enhanced network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DPANet: Position-aware feature encoding and decoding for
accurate large-scale point cloud semantic segmentation. <em>IETCV</em>,
<em>18</em>(8), 1376–1389. (<a
href="https://doi.org/10.1049/cvi2.12325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the scattered, unordered, and unstructured nature of point clouds, it is challenging to extract local features. Existing methods tend to design redundant and less-discriminative spatial feature extraction methods in the encoder, while neglecting the utilisation of uneven distribution in the decoder. In this paper, the authors fully exploit the characteristics of the imbalanced distribution in point clouds and design our Position-aware Encoder (PAE) module and Position-aware Decoder (PAD) module. In the PAE module, the authors extract position relationships utilising both Cartesian coordinate system and polar coordinate system to enhance the distinction of features. In the PAD module, the authors recognise the inherent positional disparities between each point and its corresponding upsampled point, utilising these distinctions to enrich features and mitigate information loss. The authors conduct extensive experiments and compare the proposed DPANet with existing methods on two benchmarks S3DIS and Semantic3D. The experimental results demonstrate that the method outperforms the state-of-the-art approaches.},
  archive      = {J_IETCV},
  author       = {Haoying Zhao and Aimin Zhou},
  doi          = {10.1049/cvi2.12325},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1376-1389},
  shortjournal = {IET Comput. Vis.},
  title        = {DPANet: Position-aware feature encoding and decoding for accurate large-scale point cloud semantic segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGAM: A refined global attention mechanism for medical image
segmentation. <em>IETCV</em>, <em>18</em>(8), 1362–1375. (<a
href="https://doi.org/10.1049/cvi2.12323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanisms are popular techniques in computer vision that mimic the ability of the human visual system to analyse complex scenes, enhancing the performance of convolutional neural networks (CNN). In this paper, the authors propose a refined global attention module (RGAM) to address known shortcomings of existing attention mechanisms: (1) Traditional channel attention mechanisms are not refined enough when concentrating features, which may lead to overlooking important information. (2) The 1-dimensional attention map generated by traditional spatial attention mechanisms make it difficult to accurately summarise the weights of all channels in the original feature map at the same position. The RGAM is composed of two parts: refined channel attention and refined spatial attention. In the channel attention part, the authors used multiple weight-shared dilated convolutions with varying dilation rates to perceive features with different receptive fields at the feature compression stage. The authors also combined dilated convolutions with depth-wise convolution to reduce the number of parameters. In the spatial attention part, the authors grouped the feature maps and calculated the attention for each group independently, allowing for a more accurate assessment of each spatial position’s importance. Specifically, the authors calculated the attention weights separately for the width and height directions, similar to SENet, to obtain more refined attention weights. To validate the effectiveness and generality of the proposed method, the authors conducted extensive experiments on four distinct medical image segmentation datasets. The results demonstrate the effectiveness of RGAM in achieving state-of-the-art performance compared to existing methods.},
  archive      = {J_IETCV},
  author       = {Gangjun Ning and Pingping Liu and Chuangye Dai and Mingsi Sun and Qiuzhan Zhou and Qingliang Li},
  doi          = {10.1049/cvi2.12323},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1362-1375},
  shortjournal = {IET Comput. Vis.},
  title        = {RGAM: A refined global attention mechanism for medical image segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing overfitting in vehicle recognition by decorrelated
sparse representation regularisation. <em>IETCV</em>, <em>18</em>(8),
1351–1361. (<a href="https://doi.org/10.1049/cvi2.12320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most state-of-the-art vehicle recognition methods benefit from the excellent feature extraction capabilities of convolutional neural networks (CNNs), which allow the models to perform well on the intra-dataset. However, they often show poor generalisation when facing cross-datasets due to the overfitting problem. For this issue, numerous studies have shown that models do not generalise well in new scenarios due to the high correlation between the representations in CNNs. Furthermore, over-parameterised CNNs have a large number of redundant representations. Therefore, we propose a novel Decorrelated Sparse Representation (DSR) regularisation. (1) It tries to minimise the correlation between feature maps to obtain decorrelated representations. (2) It forces the convolution kernels to extract meaningful features by allowing the sparse kernels to have additional optimisation. The DSR regularisation encourages diverse representations to reduce overfitting. Meanwhile, DSR can be applied to a wide range of vehicle recognition methods based on CNNs, and it does not require additional computation in the testing phase. In the experiments, DSR performs better than the original model on the intra-dataset and cross-dataset. Through ablation analysis, we find that DSR can drive the model to focus on the essential differences among all kinds of vehicles.},
  archive      = {J_IETCV},
  author       = {Wanyu Wei and Xinsha Fu and Siqi Ma and Yaqiao Zhu and Ning Lu},
  doi          = {10.1049/cvi2.12320},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1351-1361},
  shortjournal = {IET Comput. Vis.},
  title        = {Reducing overfitting in vehicle recognition by decorrelated sparse representation regularisation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient transformer tracking with adaptive attention.
<em>IETCV</em>, <em>18</em>(8), 1338–1350. (<a
href="https://doi.org/10.1049/cvi2.12315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETCV},
  author       = {Dingkun Xiao and Zhenzhong Wei and Guangjun Zhang},
  doi          = {10.1049/cvi2.12315},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1338-1350},
  shortjournal = {IET Comput. Vis.},
  title        = {Efficient transformer tracking with adaptive attention},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Occluded object 6D pose estimation using foreground
probability compensation. <em>IETCV</em>, <em>18</em>(8), 1325–1337. (<a
href="https://doi.org/10.1049/cvi2.12314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETCV},
  author       = {Meihui Ren and Junying Jia and Xin Lu},
  doi          = {10.1049/cvi2.12314},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1325-1337},
  shortjournal = {IET Comput. Vis.},
  title        = {Occluded object 6D pose estimation using foreground probability compensation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time semantic segmentation network for crops and weeds
based on multi-branch structure. <em>IETCV</em>, <em>18</em>(8),
1313–1324. (<a href="https://doi.org/10.1049/cvi2.12311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weed recognition is an inevitable problem in smart agriculture, and to realise efficient weed recognition, complex background, insufficient feature information, varying target sizes and overlapping crops and weeds are the main problems to be solved. To address these problems, the authors propose a real-time semantic segmentation network based on a multi-branch structure for recognising crops and weeds. First, a new backbone network for capturing feature information between crops and weeds of different sizes is constructed. Second, the authors propose a weight refinement fusion (WRF) module to enhance the feature extraction ability of crops and weeds and reduce the interference caused by the complex background. Finally, a Semantic Guided Fusion is devised to enhance the interaction of information between crops and weeds and reduce the interference caused by overlapping goals. The experimental results demonstrate that the proposed network can balance speed and accuracy. Specifically, the 0.713 Mean IoU (MIoU), 0.802 MIoU, 0.746 MIoU and 0.906 MIoU can be achieved on the sugar beet (BoniRob) dataset, synthetic BoniRob dataset, CWFID dataset and self-labelled wheat dataset, respectively.},
  archive      = {J_IETCV},
  author       = {Yufan Liu and Muhua Liu and Xuhui Zhao and Junlong Zhu and Lin Wang and Hao Ma and Mingchuan Zhang},
  doi          = {10.1049/cvi2.12311},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1313-1324},
  shortjournal = {IET Comput. Vis.},
  title        = {Real-time semantic segmentation network for crops and weeds based on multi-branch structure},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPANet: Spatial perceptual activation network for
camouflaged object detection. <em>IETCV</em>, <em>18</em>(8), 1300–1312.
(<a href="https://doi.org/10.1049/cvi2.12310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to segment objects embedded in the environment from the background. Most existing methods are easily affected by background interference in cluttered environments and cannot accurately locate camouflage areas, resulting in over-segmentation or incomplete segmentation structures. To effectively improve the performance of COD, we propose a spatial perceptual activation network (SPANet). SPANet extracts the spatial positional relationship between each object in the scene by activating spatial perception and uses it as global information to guide segmentation. It mainly consists of three modules: perceptual activation module (PAM), feature inference module (FIM), and interaction recovery module (IRM). Specifically, the authors design a PAM to model the positional relationship between the camouflaged object and the surrounding environment to obtain semantic correlation information. Then, a FIM that can effectively combine correlation information to suppress background interference and re-encode to generate multi-scale features is proposed. In addition, to further fuse multi-scale features, an IRM to mine the complementary information and differences between features at different scales is designed. Extensive experimental results on four widely used benchmark datasets (i.e. CAMO, CHAMELEON, COD10K, and NC4K) show that the authors’ method outperforms 13 state-of-the-art methods.},
  archive      = {J_IETCV},
  author       = {Jianhao Zhang and Gang Yang and Xun Dai and Pengyu Yang},
  doi          = {10.1049/cvi2.12310},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1300-1312},
  shortjournal = {IET Comput. Vis.},
  title        = {SPANet: Spatial perceptual activation network for camouflaged object detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging modality-specific and shared features for RGB-t
salient object detection. <em>IETCV</em>, <em>18</em>(8), 1285–1299. (<a
href="https://doi.org/10.1049/cvi2.12307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing RGB-T salient object detection methods are usually based on dual-stream encoding single-stream decoding network architecture. These models always rely on the quality of fusion features, which often focus on modality-shared features and overlook modality-specific features, thus failing to fully utilise the rich information contained in multi-modality data. To this end, a modality separate tri-stream net (MSTNet), which consists of a tri-stream encoding (TSE) structure and a tri-stream decoding (TSD) structure is proposed. The TSE explicitly separates and extracts the modality-shared and modality-specific features to improve the utilisation of multi-modality data. In addition, based on the hybrid-attention and cross-attention mechanism, we design an enhanced complementary fusion module (ECF), which fully considers the complementarity between the features to be fused and realises high-quality feature fusion. Furthermore, in TSD, the quality of uni-modality features is ensured under the constraint of supervision. Finally, to make full use of the rich multi-level and multi-scale decoding features contained in TSD, the authors design the adaptive multi-scale decoding module and the multi-stream feature aggregation module to improve the decoding capability. Extensive experiments on three public datasets show that the MSTNet outperforms 14 state-of-the-art methods, demonstrating that this method can extract and utilise the multi-modality information more adequately and extract more complete and rich features, thus improving the model&#39;s performance. The code will be released at https://github.com/JOOOOKII/MSTNet .},
  archive      = {J_IETCV},
  author       = {Shuo Wang and Gang Yang and Qiqi Xu and Xun Dai},
  doi          = {10.1049/cvi2.12307},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1285-1299},
  shortjournal = {IET Comput. Vis.},
  title        = {Leveraging modality-specific and shared features for RGB-T salient object detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive research on light field imaging: Theory and
application. <em>IETCV</em>, <em>18</em>(8), 1269–1284. (<a
href="https://doi.org/10.1049/cvi2.12321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational photography is a combination of novel optical designs and processing methods to capture high-dimensional visual information. As an emerged promising technique, light field (LF) imaging measures the lighting, reflectance, focus, geometry and viewpoint in the free space, which has been widely explored for depth estimation, view synthesis, refocus, rendering, 3D displays, microscopy and other applications in computer vision in the past decades. In this paper, the authors present a comprehensive research survey on the LF imaging theory, technology and application. Firstly, the LF imaging process based on a MicroLens Array structure is derived, that is MLA-LF. Subsequently, the innovations of LF imaging technology are presented in terms of the imaging prototype, consumer LF camera and LF displays in Virtual Reality (VR) and Augmented Reality (AR). Finally the applications and challenges of LF imaging integrating with deep learning models are analysed, which consist of depth estimation, saliency detection, semantic segmentation, de-occlusion and defocus deblurring in recent years. It is believed that this paper will be a good reference for the future research on LF imaging technology in Artificial Intelligence era.},
  archive      = {J_IETCV},
  author       = {Fei Liu and Yunlong Wang and Qing Yang and Shubo Zhou and Kunbo Zhang},
  doi          = {10.1049/cvi2.12321},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1269-1284},
  shortjournal = {IET Comput. Vis.},
  title        = {A comprehensive research on light field imaging: Theory and application},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on person and vehicle re-identification.
<em>IETCV</em>, <em>18</em>(8), 1235–1268. (<a
href="https://doi.org/10.1049/cvi2.12316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person/vehicle re-identification aims to use technologies such as cross-camera retrieval to associate the same person (same vehicle) in the surveillance videos at different locations, different times, and images captured by different cameras so as to achieve cross-surveillance image matching, person retrieval and trajectory tracking. It plays an extremely important role in the fields of intelligent security, criminal investigation etc. In recent years, the rapid development of deep learning technology has significantly propelled the advancement of re-identification (Re-ID) technology. An increasing number of technical methods have emerged, aiming to enhance Re-ID performance. This paper summarises four popular research areas in the current field of re-identification, focusing on the current research hotspots. These areas include the multi-task learning domain, the generalisation learning domain, the cross-modality domain, and the optimisation learning domain. Specifically, the paper analyses various challenges faced within these domains and elaborates on different deep learning frameworks and networks that address these challenges. A comparative analysis of re-identification tasks from various classification perspectives is provided, introducing mainstream research directions and current achievements. Finally, insights into future development trends are presented.},
  archive      = {J_IETCV},
  author       = {Zhaofa Wang and Liyang Wang and Zhiping Shi and Miaomiao Zhang and Qichuan Geng and Na Jiang},
  doi          = {10.1049/cvi2.12316},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1235-1268},
  shortjournal = {IET Comput. Vis.},
  title        = {A survey on person and vehicle re-identification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale feature extraction for energy-efficient object
detection in remote sensing images. <em>IETCV</em>, <em>18</em>(8),
1223–1234. (<a href="https://doi.org/10.1049/cvi2.12317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in remote sensing images aims to interpret images to obtain information on the category and location of potential targets, which is of great importance in traffic detection, marine supervision, and space reconnaissance. However, the complex backgrounds and large scale variations in remote sensing images present significant challenges. Traditional methods relied mainly on image filtering or feature descriptor methods to extract features, resulting in underperformance. Deep learning methods, especially one-stage detectors, for example, the Real-Time Object Detector (RTMDet) offers advanced solutions with efficient network architectures. Nevertheless, difficulty in feature extraction from complex backgrounds and target localisation in scale variations images limits detection accuracy. In this paper, an improved detector based on RTMDet, called the Multi-Scale Feature Extraction-assist RTMDet (MRTMDet), is proposed which address limitations through enhancement feature extraction and fusion networks. At the core of MRTMDet is a new backbone network MobileViT++ and a feature fusion network SFC-FPN, which enhances the model&#39;s ability to capture global and multi-scale features by carefully designing a hybrid feature processing unit of CNN and a transformer based on vision transformer (ViT) and poly-scale convolution (PSConv), respectively. The experiment in DIOR-R demonstrated that MRTMDet achieves competitive performance of 62.2% mAP, balancing precision with a lightweight design.},
  archive      = {J_IETCV},
  author       = {Di Wu and Hongning Liu and Jiawei Xu and Fei Xie},
  doi          = {10.1049/cvi2.12317},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1223-1234},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-scale feature extraction for energy-efficient object detection in remote sensing images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DEUFormer: High-precision semantic segmentation for urban
remote sensing images. <em>IETCV</em>, <em>18</em>(8), 1209–1222. (<a
href="https://doi.org/10.1049/cvi2.12313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban remote sensing image semantic segmentation has a wide range of applications, such as urban planning, resource exploration, intelligent transportation, and other scenarios. Although UNetFormer performs well by introducing the self-attention mechanism of Transformer, it still faces challenges arising from relatively low segmentation accuracy and significant edge segmentation errors. To this end, this paper proposes DEUFormer by employing a special weighted sum method to fuse the features of the encoder and the decoder, thus capturing both local details and global context information. Moreover, an Enhanced Feature Refinement Head is designed to finely re-weight features on the channel dimension and narrow the semantic gap between shallow and deep features, thereby enhancing multi-scale feature extraction. Additionally, an Edge-Guided Context Module is introduced to enhance edge areas through effective edge detection, which can improve edge information extraction. Experimental results show that DEUFormer achieves an average Mean Intersection over Union (mIoU) of 53.8% on the LoveDA dataset and 69.1% on the UAVid dataset. Notably, the mIoU of buildings in the LoveDA dataset is 5.0% higher than that of UNetFormer. The proposed model outperforms methods such as UNetFormer on multiple datasets, which demonstrates its effectiveness.},
  archive      = {J_IETCV},
  author       = {Xinqi Jia and Xiaoyong Song and Lei Rao and Guangyu Fan and Songlin Cheng and Niansheng Chen},
  doi          = {10.1049/cvi2.12313},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1209-1222},
  shortjournal = {IET Comput. Vis.},
  title        = {DEUFormer: High-precision semantic segmentation for urban remote sensing images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). To crop or not to crop: Comparing whole-image and cropped
classification on a large dataset of camera trap images. <em>IETCV</em>,
<em>18</em>(8), 1193–1208. (<a
href="https://doi.org/10.1049/cvi2.12318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera traps facilitate non-invasive wildlife monitoring, but their widespread adoption has created a data processing bottleneck: a camera trap survey can create millions of images, and the labour required to review those images strains the resources of conservation organisations. AI is a promising approach for accelerating image review, but AI tools for camera trap data are imperfect; in particular, classifying small animals remains difficult, and accuracy falls off outside the ecosystems in which a model was trained. It has been proposed that incorporating an object detector into an image analysis pipeline may help address these challenges, but the benefit of object detection has not been systematically evaluated in the literature. In this work, the authors assess the hypothesis that classifying animals cropped from camera trap images using a species-agnostic detector yields better accuracy than classifying whole images. We find that incorporating an object detection stage into an image classification pipeline yields a macro-average F1 improvement of around 25% on a large, long-tailed dataset; this improvement is reproducible on a large public dataset and a smaller public benchmark dataset. The authors describe a classification architecture that performs well for both whole and detector-cropped images, and demonstrate that this architecture yields state-of-the-art benchmark accuracy.},
  archive      = {J_IETCV},
  author       = {Tomer Gadot and Ștefan Istrate and Hyungwon Kim and Dan Morris and Sara Beery and Tanya Birch and Jorge Ahumada},
  doi          = {10.1049/cvi2.12318},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1193-1208},
  shortjournal = {IET Comput. Vis.},
  title        = {To crop or not to crop: Comparing whole-image and cropped classification on a large dataset of camera trap images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recognition of european mammals and birds in camera trap
images using deep neural networks. <em>IETCV</em>, <em>18</em>(8),
1162–1192. (<a href="https://doi.org/10.1049/cvi2.12294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most machine learning methods for animal recognition in camera trap images are limited to mammal identification and group birds into a single class. Machine learning methods for visually discriminating birds, in turn, cannot discriminate between mammals and are not designed for camera trap images. The authors present deep neural network models to recognise both mammals and bird species in camera trap images. They train neural network models for species classification as well as for predicting the animal taxonomy, that is, genus, family, order, group, and class names. Different neural network architectures, including ResNet, EfficientNetV2, Vision Transformer, Swin Transformer, and ConvNeXt, are compared for these tasks. Furthermore, the authors investigate approaches to overcome various challenges associated with camera trap image analysis. The authors’ best species classification models achieve a mean average precision (mAP) of 97.91% on a validation data set and mAPs of 90.39% and 82.77% on test data sets recorded in forests in Germany and Poland, respectively. Their best taxonomic classification models reach a validation mAP of 97.18% and mAPs of 94.23% and 79.92% on the two test data sets, respectively.},
  archive      = {J_IETCV},
  author       = {Daniel Schneider and Kim Lindner and Markus Vogelbacher and Hicham Bellafkir and Nina Farwig and Bernd Freisleben},
  doi          = {10.1049/cvi2.12294},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1162-1192},
  shortjournal = {IET Comput. Vis.},
  title        = {Recognition of european mammals and birds in camera trap images using deep neural networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). SAM-y: Attention-enhanced hazardous vehicle object
detection algorithm. <em>IETCV</em>, <em>18</em>(8), 1149–1161. (<a
href="https://doi.org/10.1049/cvi2.12293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle transportation of hazardous chemicals is one of the important mobile hazards in modern logistics, and its unsafe factors bring serious threats to people&#39;s lives, property and environmental safety. Although the current object detection algorithm has certain applications in the detection of hazardous chemical vehicles, due to the complexity of the transportation environment, the small size and low resolution of the vehicle target etc., object detection becomes more difficult in the face of a complex background. In order to solve these problems, the authors propose an improved algorithm based on YOLOv5 to enhance the detection accuracy and efficiency of hazardous chemical vehicles. Firstly, in order to better capture the details and semantic information of hazardous chemical vehicles, the algorithm solves the problem of mismatch between the receptive field of the detector and the target object by introducing the receptive field expansion block into the backbone network, so as to improve the ability of the model to capture the detailed information of hazardous chemical vehicles. Secondly, in order to improve the ability of the model to express the characteristics of hazardous chemical vehicles, the authors introduce a separable attention mechanism in the multi-scale target detection stage, and enhances the prediction ability of the model by combining the object detection head and attention mechanism coherently in the feature layer of scale perception, the spatial location of spatial perception and the output channel of task perception. Experimental results show that the improved model significantly surpasses the baseline model in terms of accuracy and achieves more accurate object detection. At the same time, the model also has a certain improvement in inference speed and achieves faster inference ability.},
  archive      = {J_IETCV},
  author       = {Shanshan Wang and Bushi Liu and Pengcheng Zhu and Xianchun Meng and Bolun Chen and Wei Shao and Liqing Chen},
  doi          = {10.1049/cvi2.12293},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1149-1161},
  shortjournal = {IET Comput. Vis.},
  title        = {SAM-Y: Attention-enhanced hazardous vehicle object detection algorithm},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DualAD: Dual adversarial network for image anomaly
detection⋆. <em>IETCV</em>, <em>18</em>(8), 1138–1148. (<a
href="https://doi.org/10.1049/cvi2.12297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly Detection, also known as outlier detection, is critical in domains such as network security, intrusion detection, and fraud detection. One popular approach to anomaly detection is using autoencoders, which are trained to reconstruct input by minimising reconstruction error with the neural network. However, these methods usually suffer from the trade-off between normal reconstruction fidelity and abnormal reconstruction distinguishability, which damages the performance. The authors find that the above trade-off can be better mitigated by imposing constraints on the latent space of images. To this end, the authors propose a new Dual Adversarial Network (DualAD) that consists of a Feature Constraint (FC) module and a reconstruction module. The method incorporates the FC module during the reconstruction training process to impose constraints on the latent space of images, thereby yielding feature representations more conducive to anomaly detection. Additionally, the authors employ dual adversarial learning to model the distribution of normal data. On the one hand, adversarial learning was implemented during the reconstruction process to obtain higher-quality reconstruction samples, thereby preventing the effects of blurred image reconstructions on model performance. On the other hand, the authors utilise adversarial training of the FC module and the reconstruction module to achieve superior feature representation, making anomalies more distinguishable at the feature level. During the inference phase, the authors perform anomaly detection simultaneously in the pixel and latent spaces to identify abnormal patterns more comprehensively. Experiments on three data sets CIFAR10, MNIST, and FashionMNIST demonstrate the validity of the authors’ work. Results show that constraints on the latent space and adversarial learning can improve detection performance.},
  archive      = {J_IETCV},
  author       = {Yonghao Wan and Aimin Feng},
  doi          = {10.1049/cvi2.12297},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1138-1148},
  shortjournal = {IET Comput. Vis.},
  title        = {DualAD: Dual adversarial network for image anomaly detection⋆},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing semi-supervised contrastive learning through
saliency map for diabetic retinopathy grading. <em>IETCV</em>,
<em>18</em>(8), 1127–1137. (<a
href="https://doi.org/10.1049/cvi2.12308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is a severe ophthalmic condition that can lead to blindness if not diagnosed and provided timely treatment. Hence, the development of efficient automated DR grading systems is crucial for early screening and treatment. Although progress has been made in DR detection using deep learning techniques, these methods still face challenges in handling the complexity of DR lesion characteristics and the nuances in grading criteria. Moreover, the performance of these algorithms is hampered by the scarcity of large-scale, high-quality annotated data. An innovative semi-supervised fundus image DR grading framework is proposed, employing a saliency estimation map to bolster the model&#39;s perception of fundus structures, thereby improving the differentiation between lesions and healthy regions. By integrating semi-supervised and contrastive learning, the model&#39;s ability to recognise inter-class and intra-class variations in DR grading is enhanced, allowing for precise discrimination of various lesion features. Experiments conducted on publicly available DR grading datasets, such as EyePACS and Messidor, have validated the effectiveness of our proposed method. Specifically, our approach outperforms the state of the art on the kappa metric by 0.8% on the full EyePACS dataset and by 3.2% on a 10% subset of EyePACS, demonstrating its superiority over previous methodologies. The authors’ code is publicly available at https://github.com/500ZhangJC/SCL-SEM-framework-for-DR-Grading .},
  archive      = {J_IETCV},
  author       = {Jiacheng Zhang and Rong Jin and Wenqiang Liu},
  doi          = {10.1049/cvi2.12308},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1127-1137},
  shortjournal = {IET Comput. Vis.},
  title        = {Enhancing semi-supervised contrastive learning through saliency map for diabetic retinopathy grading},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusing crops representation into snippet via mutual learning
for weakly supervised surveillance anomaly detection. <em>IETCV</em>,
<em>18</em>(8), 1112–1126. (<a
href="https://doi.org/10.1049/cvi2.12289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the challenge of detecting anomalies in real-world surveillance videos using weakly supervised data has emerged. Traditional methods, utilising multi-instance learning (MIL) with video snippets, struggle with background noise and tend to overlook subtle anomalies. To tackle this, the authors propose a novel approach that crops snippets to create multiple instances with less noise, separately evaluates them and then fuses these evaluations for more precise anomaly detection. This method, however, leads to higher computational demands, especially during inference. Addressing this, our solution employs mutual learning to guide snippet feature training using these low-noise crops. The authors integrate multiple instance learning (MIL) for the primary task with snippets as inputs and multiple-multiple instance learning (MMIL) for an auxiliary task with crops during training. The authors’ approach ensures consistent multi-instance results in both tasks and incorporates a temporal activation mutual learning module (TAML) for aligning temporal anomaly activations between snippets and crops, improving the overall quality of snippet representations. Additionally, a snippet feature discrimination enhancement module (SFDE) refines the snippet features further. Tested across various datasets, the authors’ method shows remarkable performance, notably achieving a frame-level AUC of 85.78% on the UCF-Crime dataset, while reducing computational costs.},
  archive      = {J_IETCV},
  author       = {Bohua Zhang and Jianru Xue},
  doi          = {10.1049/cvi2.12289},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1112-1126},
  shortjournal = {IET Comput. Vis.},
  title        = {Fusing crops representation into snippet via mutual learning for weakly supervised surveillance anomaly detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving unsupervised pedestrian re-identification with
enhanced feature representation and robust clustering. <em>IETCV</em>,
<em>18</em>(8), 1097–1111. (<a
href="https://doi.org/10.1049/cvi2.12309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian re-identification (re-ID) is an important research direction in computer vision, with extensive applications in pattern recognition and monitoring systems. Due to uneven data distribution, and the need to solve clustering standards and similarity evaluation problems, the performance of unsupervised methods is limited. To address these issues, an improved unsupervised re-ID method, called Enhanced Feature Representation and Robust Clustering (EFRRC), which combines EFRRC is proposed. First, a relation network that considers the relations between each part of the pedestrian&#39;s body and other parts is introduced, thereby obtaining more discriminative feature representations. The network makes the feature at the single-part level also contain partial information of other body parts, making it more discriminative. A global contrastive pooling (GCP) module is introduced to obtain the global features of the image. Second, a dispersion-based clustering method, which can effectively evaluate the quality of clustering and discover potential patterns in the data is designed. This approach considers a wider context of sample-level pairwise relationships for robust cluster affinity assessment. It effectively addresses challenges posed by imbalanced data distributions in complex situations. The above structures are connected through a clustering contrastive learning framework, which not only improves the discriminative power of features and the accuracy of clustering, but also solves the problem of inconsistent clustering updates. Experimental results on three public datasets demonstrate the superiority of our method over existing unsupervised re-ID methods.},
  archive      = {J_IETCV},
  author       = {Jiang Luo and Lingjun Liu},
  doi          = {10.1049/cvi2.12309},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1097-1111},
  shortjournal = {IET Comput. Vis.},
  title        = {Improving unsupervised pedestrian re-identification with enhanced feature representation and robust clustering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient class-agnostic obstacle detection for UAV-assisted
waterway inspection systems. <em>IETCV</em>, <em>18</em>(8), 1087–1096.
(<a href="https://doi.org/10.1049/cvi2.12319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the safety of water airport runways is essential for the correct operation of seaplane flights. Among other tasks, airport operators must identify and remove various objects that may have drifted into the runway area. In this paper, the authors propose a complete and embedded-friendly waterway obstacle detection pipeline that runs on a camera-equipped drone. This system uses a class-agnostic version of the YOLOv7 detector, which is capable of detecting objects regardless of its class. Additionally, through the usage of the GPS data of the drone and camera parameters, the location of the objects are pinpointed with 0.58 m Distance Root Mean Square. In our own annotated dataset, the system is capable of generating alerts for detected objects with a recall of 0.833 and a precision of 1.},
  archive      = {J_IETCV},
  author       = {Pablo Alonso and Jon Ander Íñiguez de Gordoa and Juan Diego Ortega and Marcos Nieto},
  doi          = {10.1049/cvi2.12319},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1087-1096},
  shortjournal = {IET Comput. Vis.},
  title        = {Efficient class-agnostic obstacle detection for UAV-assisted waterway inspection systems},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated knowledge distillation for enhanced insulator
defect detection in resource-constrained environments. <em>IETCV</em>,
<em>18</em>(8), 1072–1086. (<a
href="https://doi.org/10.1049/cvi2.12290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insulator defect detection is crucial for the stable operation of power systems. It has become a mainstream research direction to realise insulator defect detection based on the combination of line images captured by UAVs and deep learning techniques. However, the existing high-quality insulator defect detection models still face problems such as relying on massive-labelled data and huge model parameters. Especially on resource-constrained devices, it becomes a challenge to strike a balance between model lightweighting and performance. Although the knowledge distillation technique provides a solution for model lightweighting, the loss of information in the distillation process leads to the performance degradation of small models, which in turn creates a paradox between lightweighting and performance. Hence, an insulator defect detection method based on federated knowledge distillation is proposed. The method not only realises the lightweighting of the model, but also effectively improves the model performance by collaboratively training the model through the federated learning approach. Moreover, the asynchronous aggregation approach and model freshness mechanism designed in the method further enhance the training efficiency and collaborative effect. The experimental results show that the detection accuracy and efficiency of this paper&#39;s method on public datasets are significantly better than the benchmark algorithm.},
  archive      = {J_IETCV},
  author       = {Xiaohu Huang and Minghui Jia and Xianghua Tai and Wei Wang and Qi Hu and Dongping Liu and Peiheng Guo and Shengxiang Tian and Dequan Yan and Haishan Han},
  doi          = {10.1049/cvi2.12290},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1072-1086},
  shortjournal = {IET Comput. Vis.},
  title        = {Federated knowledge distillation for enhanced insulator defect detection in resource-constrained environments},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balanced parametric body prior for implicit clothed human
reconstruction from a monocular RGB. <em>IETCV</em>, <em>18</em>(7),
1057–1067. (<a href="https://doi.org/10.1049/cvi2.12306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors study the problem of reconstructing detailed 3D human surfaces in various poses and clothing from images. The parametric human body allows accurate 3D clothed human reconstruction. However, the offset of large and loose clothing from the inferred parametric body mesh confines the generalisation of the existing parametric body-based methods. A distinctive method that simultaneously generalises well to unseen poses and unseen clothing is proposed. The authors first discover the unbalanced nature of existing implicit function-based methods. To address this issue, the authors propose to synthesise the balanced training samples with a new dependency coefficient in training. The dependency coefficient can tell the network whether the prior from the parametric body model is reliable. The authors then design a novel positional embedding-based attenuation strategy to incorporate the dependency coefficient into the implicit function (IF) network. Comprehensive experiments are conducted on the CAPE dataset to study the effectiveness of the authors’ approach. The proposed method significantly surpasses state-of-the-art approaches and generalises well on unseen poses and clothing. As an illustrative example, the proposed method improves the Chamfer Distance Error and Normal Error by 38.2% and 57.6%.},
  archive      = {J_IETCV},
  author       = {Rong Xue and Jiefeng Li and Cewu Lu},
  doi          = {10.1049/cvi2.12306},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {1057-1067},
  shortjournal = {IET Comput. Vis.},
  title        = {Balanced parametric body prior for implicit clothed human reconstruction from a monocular RGB},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HIST: Hierarchical and sequential transformer for image
captioning. <em>IETCV</em>, <em>18</em>(7), 1043–1056. (<a
href="https://doi.org/10.1049/cvi2.12305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning aims to automatically generate a natural language description of a given image, and most state-of-the-art models have adopted an encoder–decoder transformer framework. Such transformer structures, however, show two main limitations in the task of image captioning. Firstly, the traditional transformer obtains high-level fusion features to decode while ignoring other-level features, resulting in losses of image content. Secondly, the transformer is weak in modelling the natural order characteristics of language. To address theseissues, the authors propose a HI erarchical and S equential T ransformer ( HIST ) structure, which forces each layer of the encoder and decoder to focus on features of different granularities, and strengthen the sequentially semantic information. Specifically, to capture the details of different levels of features in the image, the authors combine the visual features of multiple regions and divide them into multiple levels differently. In addition, to enhance the sequential information, the sequential enhancement module in each decoder layer block extracts different levels of features for sequentially semantic extraction and expression. Extensive experiments on the public datasets MS-COCO and Flickr30k have demonstrated the effectiveness of our proposed method, and show that the authors’ method outperforms most of previous state of the arts.},
  archive      = {J_IETCV},
  author       = {Feixiao Lv and Rui Wang and Lihua Jing and Pengwen Dai},
  doi          = {10.1049/cvi2.12305},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {1043-1056},
  shortjournal = {IET Comput. Vis.},
  title        = {HIST: Hierarchical and sequential transformer for image captioning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SRL-ProtoNet: Self-supervised representation learning for
few-shot remote sensing scene classification. <em>IETCV</em>,
<em>18</em>(7), 1034–1042. (<a
href="https://doi.org/10.1049/cvi2.12304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a deep learning method to classify a large amount of labelled remote sensing scene data produces good performance. However, it is challenging for deep learning based methods to generalise to classification tasks with limited data. Few-shot learning allows neural networks to classify unseen categories when confronted with a handful of labelled data. Currently, episodic tasks based on meta-learning can effectively complete few-shot classification, and training an encoder that can conduct representation learning has become an important component of few-shot learning. An end-to-end few-shot remote sensing scene classification model based on ProtoNet and self-supervised learning is proposed. The authors design the Pre-prototype for a more discrete feature space and better integration with self-supervised learning, and also propose the ProtoMixer for higher quality prototypes with a global receptive field. The authors’ method outperforms the existing state-of-the-art self-supervised based methods on three widely used benchmark datasets: UC-Merced, NWPU-RESISC45, and AID. Compare with previous state-of-the-art performance. For the one-shot setting, this method improves by 1.21%, 2.36%, and 0.84% in AID, UC-Merced, and NWPU-RESISC45, respectively. For the five-shot setting, this method surpasses by 0.85%, 2.79%, and 0.74% in the AID, UC-Merced, and NWPU-RESISC45, respectively.},
  archive      = {J_IETCV},
  author       = {Bing Liu and Hongwei Zhao and Jiao Li and Yansheng Gao and Jianrong Zhang},
  doi          = {10.1049/cvi2.12304},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {1034-1042},
  shortjournal = {IET Comput. Vis.},
  title        = {SRL-ProtoNet: Self-supervised representation learning for few-shot remote sensing scene classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal video search by examples—a video quality impact
analysis. <em>IETCV</em>, <em>18</em>(7), 1017–1033. (<a
href="https://doi.org/10.1049/cvi2.12303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the proliferation of video content continues, and many video archives lack suitable metadata, therefore, video retrieval, particularly through example-based search, has become increasingly crucial. Existing metadata often fails to meet the needs of specific types of searches, especially when videos contain elements from different modalities, such as visual and audio. Consequently, developing video retrieval methods that can handle multi-modal content is essential. An innovative Multi-modal Video Search by Examples (MVSE) framework is introduced, employing state-of-the-art techniques in its various components. In designing MVSE, the authors focused on accuracy, efficiency, interactivity, and extensibility, with key components including advanced data processing and a user-friendly interface aimed at enhancing search effectiveness and user experience. Furthermore, the framework was comprehensively evaluated, assessing individual components, data quality issues, and overall retrieval performance using high-quality and low-quality BBC archive videos. The evaluation reveals that: (1) multi-modal search yields better results than single-modal search; (2) the quality of video, both visual and audio, has an impact on the query precision. Compared with image query results, audio quality has a greater impact on the query precision (3) a two-stage search process (i.e. searching by Hamming distance based on hashing, followed by searching by Cosine similarity based on embedding); is effective but increases time overhead; (4) large-scale video retrieval is not only feasible but also expected to emerge shortly.},
  archive      = {J_IETCV},
  author       = {Guanfeng Wu and Abbas Haider and Xing Tian and Erfan Loweimi and Chi Ho Chan and Mengjie Qian and Awan Muhammad and Ivor Spence and Rob Cooper and Wing W. Y. Ng and Josef Kittler and Mark Gales and Hui Wang},
  doi          = {10.1049/cvi2.12303},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {1017-1033},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-modal video search by examples—A video quality impact analysis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Centre-loss—a preferred class verification approach over
sample-to-sample in self-checkout products datasets. <em>IETCV</em>,
<em>18</em>(7), 1004–1016. (<a
href="https://doi.org/10.1049/cvi2.12302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese networks excel at comparing two images, serving as an effective class verification technique for a single-per-class reference image. However, when multiple reference images are present, Siamese verification necessitates multiple comparisons and aggregation, often unpractical at inference. The Centre-Loss approach, proposed in this research, solves a class verification task more efficiently, using a single forward-pass during inference, than sample-to-sample approaches. Optimising a Centre-Loss function learns class centres and minimises intra-class distances in latent space. The authors compared verification accuracy using Centre-Loss against aggregated Siamese when other hyperparameters (such as neural network backbone and distance type) are the same. Experiments were performed to contrast the ubiquitous Euclidean against other distance types to discover the optimum Centre-Loss layer, its size, and Centre-Loss weight. In optimal architecture, the Centre-Loss layer is connected to the penultimate layer, calculates Euclidean distance, and its size depends on distance type. The Centre-Loss method was validated on the Self-Checkout products and Fruits 360 image datasets. Centre-Loss comparable accuracy and lesser complexity make it a preferred approach over sample-to-sample for the class verification task, when the number of reference image per class is high and inference speed is a factor, such as in self-checkouts.},
  archive      = {J_IETCV},
  author       = {Bernardas Ciapas and Povilas Treigys},
  doi          = {10.1049/cvi2.12302},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {1004-1016},
  shortjournal = {IET Comput. Vis.},
  title        = {Centre-loss—A preferred class verification approach over sample-to-sample in self-checkout products datasets},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale skeleton simplification graph convolutional
network for skeleton-based action recognition. <em>IETCV</em>,
<em>18</em>(7), 992–1003. (<a
href="https://doi.org/10.1049/cvi2.12300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition based on graph convolutional networks (GCNs) is one of the hotspots in computer vision. However, previous methods generally rely on handcrafted graph, which limits the effectiveness of the model in characterising the connections between indirectly connected joints. The limitation leads to weakened connections when joints are separated by long distances. To address the above issue, the authors propose a skeleton simplification method which aims to reduce the number of joints and the distance between joints by merging adjacent joints into simplified joints. Group convolutional block is devised to extract the internal features of the simplified joints. Additionally, the authors enhance the method by introducing multi-scale modelling, which maps inputs into sequences across various levels of simplification. Combining with spatial temporal graph convolution, a multi-scale skeleton simplification GCN for skeleton-based action recognition (M3S-GCN) is proposed for fusing multi-scale skeleton sequences and modelling the connections between joints. Finally, M3S-GCN is evaluated on five benchmarks of NTU RGB+D 60 (C-Sub, C-View), NTU RGB+D 120 (X-Sub, X-Set) and NW-UCLA datasets. Experimental results show that the authors’ M3S-GCN achieves state-of-the-art performance with the accuracies of 93.0%, 97.0% and 91.2% on C-Sub, C-View and X-Set benchmarks, which validates the effectiveness of the method.},
  archive      = {J_IETCV},
  author       = {Fan Zhang and Ding Chongyang and Kai Liu and Liu Hongjin},
  doi          = {10.1049/cvi2.12300},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {992-1003},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-scale skeleton simplification graph convolutional network for skeleton-based action recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GR-former: Graph-reinforcement transformer for
skeleton-based driver action recognition. <em>IETCV</em>,
<em>18</em>(7), 982–991. (<a
href="https://doi.org/10.1049/cvi2.12298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In in-vehicle driving scenarios, composite action recognition is crucial for improving safety and understanding the driver&#39;s intention. Due to spatial constraints and occlusion factors, the driver&#39;s range of motion is limited, thus resulting in similar action patterns that are difficult to differentiate. Additionally, collecting skeleton data that characterise the full human posture is difficult, posing additional challenges for action recognition. To address the problems, a novel Graph-Reinforcement Transformer (GR-Former) model is proposed. Using limited skeleton data as inputs, by introducing graph structure information to directionally reinforce the effect of the self-attention mechanism, dynamically learning and aggregating features between joints at multiple levels, the authors’ model constructs a richer feature vector space, enhancing its expressiveness and recognition accuracy. Based on the Drive &amp; Act dataset for composite action recognition, the authors’ work only applies human upper-body skeleton data to achieve state-of-the-art performance compared to existing methods. Using complete human skeleton data also has excellent recognition accuracy on the NTU RGB + D- and NTU RGB + D 120 dataset, demonstrating the great generalisability of the GR-Former. Generally, the authors’ work provides a new and effective solution for driver action recognition in in-vehicle scenarios.},
  archive      = {J_IETCV},
  author       = {Zhuoyan Xu and Jingke Xu},
  doi          = {10.1049/cvi2.12298},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {982-991},
  shortjournal = {IET Comput. Vis.},
  title        = {GR-former: Graph-reinforcement transformer for skeleton-based driver action recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2D human skeleton action recognition with spatial
constraints. <em>IETCV</em>, <em>18</em>(7), 968–981. (<a
href="https://doi.org/10.1049/cvi2.12296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human actions are predominantly presented in 2D format in video surveillance scenarios, which hinders the accurate determination of action details not apparent in 2D data. Depth estimation can aid human action recognition tasks, enhancing accuracy with neural networks. However, reliance on images for depth estimation requires extensive computational resources and cannot utilise the connectivity between human body structures. Besides, the depth information may not accurately reflect actual depth ranges, necessitating improved reliability. Therefore, a 2D human skeleton action recognition method with spatial constraints (2D-SCHAR) is introduced. 2D-SCHAR employs graph convolution networks to process graph-structured human action skeleton data comprising three parts: depth estimation, spatial transformation, and action recognition. The initial two components, which infer 3D information from 2D human skeleton actions and generate spatial transformation parameters to correct abnormal deviations in action data, support the latter in the model to enhance the accuracy of action recognition. The model is designed in an end-to-end, multitasking manner, allowing parameter sharing among these three components to boost performance. The experimental results validate the model&#39;s effectiveness and superiority in human skeleton action recognition.},
  archive      = {J_IETCV},
  author       = {Lei Wang and Jianwei Zhang and Wenbing Yang and Song Gu and Shanmin Yang},
  doi          = {10.1049/cvi2.12296},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {968-981},
  shortjournal = {IET Comput. Vis.},
  title        = {2D human skeleton action recognition with spatial constraints},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FastFaceCLIP: A lightweight text-driven high-quality face
image manipulation. <em>IETCV</em>, <em>18</em>(7), 950–967. (<a
href="https://doi.org/10.1049/cvi2.12295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many new methods have emerged in text-driven images, the large computational power required for model training causes these methods to have a slow training process. Additionally, these methods consume a considerable amount of video random access memory (VRAM) resources during training. When generating high-resolution images, the VRAM resources are often insufficient, which results in the inability to generate high-resolution images. Nevertheless, recent Vision Transformers (ViTs) advancements have demonstrated their image classification and recognition capabilities. Unlike the traditional Convolutional Neural Networks based methods, ViTs have a Transformer-based architecture, leverage attention mechanisms to capture comprehensive global information, moreover enabling enhanced global understanding of images through inherent long-range dependencies, thus extracting more robust features and achieving comparable results with reduced computational load. The adaptability of ViTs to text-driven image manipulation was investigated. Specifically, existing image generation methods were refined and the FastFaceCLIP method was proposed by combining the image-text semantic alignment function of the pre-trained CLIP model with the high-resolution image generation function of the proposed FastFace. Additionally, the Multi-Axis Nested Transformer module was incorporated for advanced feature extraction from the latent space, generating higher-resolution images that are further enhanced using the Real-ESRGAN algorithm. Eventually, extensive face manipulation-related tests on the CelebA-HQ dataset challenge the proposed method and other related schemes, demonstrating that FastFaceCLIP effectively generates semantically accurate, visually realistic, and clear images using fewer parameters and less time.},
  archive      = {J_IETCV},
  author       = {Jiaqi Ren and Junping Qin and Qianli Ma and Yin Cao},
  doi          = {10.1049/cvi2.12295},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {950-967},
  shortjournal = {IET Comput. Vis.},
  title        = {FastFaceCLIP: A lightweight text-driven high-quality face image manipulation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated finger vein presentation attack detection for
various clients. <em>IETCV</em>, <em>18</em>(7), 935–949. (<a
href="https://doi.org/10.1049/cvi2.12292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the application of finger vein recognition has become popular. Studies have shown finger vein presentation attacks increasingly threaten these recognition devices. As a result, research on finger vein presentation attack detection (fvPAD) methods has received much attention. However, the current fvPAD methods have two limitations. (1) Most terminal devices cannot train fvPAD models independently due to a lack of data. (2) Several research institutes can train fvPAD models; however, these models perform poorly when applied to terminal devices due to inadequate generalisation. Consequently, it is difficult for threatened terminal devices to obtain an effective fvPAD model. To address this problem, the method of federated finger vein presentation attack detection for various clients is proposed, which is the first study that introduces federated learning (FL) to fvPAD. In the proposed method, the differences in data volume and computing power between clients are considered. Traditional FL clients are expanded into two categories: institutional and terminal clients. For institutional clients, an improved triplet training mode with FL is designed to enhance model generalisation. For terminal clients, their inability is solved to obtain effective fvPAD models. Finally, extensive experiments are conducted on three datasets, which demonstrate the superiority of our method.},
  archive      = {J_IETCV},
  author       = {Hengyu Mu and Jian Guo and Xingli Liu and Chong Han and Lijuan Sun},
  doi          = {10.1049/cvi2.12292},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {935-949},
  shortjournal = {IET Comput. Vis.},
  title        = {Federated finger vein presentation attack detection for various clients},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PSANet: Automatic colourisation using position-spatial
attention for natural images. <em>IETCV</em>, <em>18</em>(7), 922–934.
(<a href="https://doi.org/10.1049/cvi2.12291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the richness of natural image semantics, natural image colourisation is a challenging problem. Existing methods often suffer from semantic confusion due to insufficient semantic understanding, resulting in unreasonable colour assignments, especially at the edges of objects. This phenomenon is referred to as colour bleeding. The authors have found that using the self-attention mechanism benefits the model&#39;s understanding and recognition of object semantics. However, this leads to another problem in colourisation, namely dull colour. With this in mind, a Position-Spatial Attention Network(PSANet) is proposed to address the colour bleeding and the dull colour. Firstly, a novel new attention module called position-spatial attention module (PSAM) is introduced. Through the proposed PSAM module, the model enhances the semantic understanding of images while solving the dull colour problem caused by self-attention. Then, in order to further prevent colour bleeding on object boundaries, a gradient-aware loss is proposed. Lastly, the colour bleeding phenomenon is further improved by the combined effect of gradient-aware loss and edge-aware loss. Experimental results show that this method can reduce colour bleeding largely while maintaining good perceptual quality.},
  archive      = {J_IETCV},
  author       = {Peng-Jie Zhu and Yuan-Yuan Pu and Qiuxia Yang and Siqi Li and Zheng-Peng Zhao and Hao Wu and Dan Xu},
  doi          = {10.1049/cvi2.12291},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {922-934},
  shortjournal = {IET Comput. Vis.},
  title        = {PSANet: Automatic colourisation using position-spatial attention for natural images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Social-ATPGNN: Prediction of multi-modal pedestrian
trajectory of non-homogeneous social interaction. <em>IETCV</em>,
<em>18</em>(7), 907–921. (<a
href="https://doi.org/10.1049/cvi2.12286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of automatic driving and path planning technology, predicting the moving trajectory of pedestrians in dynamic scenes has become one of key and urgent technical problems. However, most of the existing techniques regard all pedestrians in the scene as equally important influence on the predicted pedestrian&#39;s trajectory, and the existing methods which use sequence-based time-series generative models to obtain the predicted trajectories, do not allow for parallel computation, it will introduce a significant computational overhead. A new social trajectory prediction network, Social-ATPGNN which integrates both temporal information and spatial one based on ATPGNN is proposed. In space domain, the pedestrians in the predicted scene are formed into an undirected and non fully connected graph, which solves the problem of homogenisation of pedestrian relationships, then, the spatial interaction between pedestrians is encoded to improve the accuracy of modelling pedestrian social consciousness. After acquiring high-level spatial data, the method uses Temporal Convolutional Network which could perform parallel calculations to capture the correlation of time series of pedestrian trajectories. Through a large number of experiments, the proposed model shows the superiority over the latest models on various pedestrian trajectory datasets.},
  archive      = {J_IETCV},
  author       = {Kehao Wang and Han Zou},
  doi          = {10.1049/cvi2.12286},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {907-921},
  shortjournal = {IET Comput. Vis.},
  title        = {Social-ATPGNN: Prediction of multi-modal pedestrian trajectory of non-homogeneous social interaction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SkatingVerse: A large-scale benchmark for comprehensive
evaluation on human action understanding. <em>IETCV</em>,
<em>18</em>(7), 888–906. (<a
href="https://doi.org/10.1049/cvi2.12287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action understanding (HAU) is a broad topic that involves specific tasks, such as action localisation, recognition, and assessment. However, most popular HAU datasets are bound to one task based on particular actions. Combining different but relevant HAU tasks to establish a unified action understanding system is challenging due to the disparate actions across datasets. A large-scale and comprehensive benchmark, namely SkatingVerse is constructed for action recognition, segmentation, proposal, and assessment. SkatingVerse focus on fine-grained sport action, hence figure skating is chosen as the task object, which eliminates the biases of the object, scene, and space that exist in most previous datasets. In addition, skating actions have inherent complexity and similarity, which is an enormous challenge for current algorithms. A total of 1687 official figure skating competition videos was collected with a total of 184.4 h, exceeding four times over other datasets with a similar topic. SkatingVerse enables to formulate a unified task to output fine-grained human action classification and assessment results from a raw figure skating competition video. In addition, SkatingVerse can facilitate the study of HAU foundation model due to its large scale and abundant categories. Moreover, image modality is incorporated for human pose estimation task into SkatingVerse . Extensive experimental results show that (1) SkatingVerse significantly helps the training and evaluation of HAU methods, (2) the performance of existing HAU methods has much room to improve, and SkatingVerse helps to reduce such gaps, and (3) unifying relevant tasks in HAU through a uniform dataset can facilitate more practical applications. SkatingVerse will be publicly available to facilitate further studies on relevant problems.},
  archive      = {J_IETCV},
  author       = {Ziliang Gan and Lei Jin and Yi Cheng and Yu Cheng and Yinglei Teng and Zun Li and Yawen Li and Wenhan Yang and Zheng Zhu and Junliang Xing and Jian Zhao},
  doi          = {10.1049/cvi2.12287},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {888-906},
  shortjournal = {IET Comput. Vis.},
  title        = {SkatingVerse: A large-scale benchmark for comprehensive evaluation on human action understanding},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge distillation of face recognition via attention
cosine similarity review. <em>IETCV</em>, <em>18</em>(7), 875–887. (<a
href="https://doi.org/10.1049/cvi2.12288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based face recognition models have demonstrated remarkable performance in benchmark tests, and knowledge distillation technology has been frequently accustomed to obtain high-precision real-time face recognition models specifically designed for mobile and embedded devices. However, in recent years, the knowledge distillation methods for face recognition, which mainly focus on feature or logit knowledge distillation techniques, neglect the attention mechanism that play an important role in the domain of neural networks. An innovation cross-stage connection review path of the attention cosine similarity knowledge distillation method that unites the attention mechanism with review knowledge distillation method is proposed. This method transfers the attention map obtained from the teacher network to the student through a cross-stage connection path. The efficacy and excellence of the proposed algorithm are demonstrated in popular benchmark tests.},
  archive      = {J_IETCV},
  author       = {Zhuo Wang and SuWen Zhao and WanYi Guo},
  doi          = {10.1049/cvi2.12288},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {875-887},
  shortjournal = {IET Comput. Vis.},
  title        = {Knowledge distillation of face recognition via attention cosine similarity review},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLaSP: Cross-view 6-DoF localisation assisted by synthetic
panorama. <em>IETCV</em>, <em>18</em>(7), 859–874. (<a
href="https://doi.org/10.1049/cvi2.12285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive progress in visual localisation, 6-DoF cross-view localisation is still a challenging task in the computer vision community due to the huge appearance changes. To address this issue, the authors propose the CLaSP, a coarse-to-fine framework, which leverages a synthetic panorama to facilitate cross-view 6-DoF localisation in a large-scale scene. The authors first leverage a segmentation map to correct the prior pose, followed by a synthetic panorama on the ground to enable coarse pose estimation combined with a template matching method. The authors finally formulate the refine localisation process as feature matching and pose refinement to obtain the final result. The authors evaluate the performance of the CLaSP and several state-of-the-art baselines on the Airloc dataset, which demonstrates the effectiveness of our proposed framework.},
  archive      = {J_IETCV},
  author       = {Juelin Zhu and Shen Yan and Xiaoya Cheng and Rouwan Wu and Yuxiang Liu and Maojun Zhang},
  doi          = {10.1049/cvi2.12285},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {859-874},
  shortjournal = {IET Comput. Vis.},
  title        = {CLaSP: Cross-view 6-DoF localisation assisted by synthetic panorama},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eigenspectrum regularisation reverse neighbourhood
discriminative learning. <em>IETCV</em>, <em>18</em>(6), 842–858. (<a
href="https://doi.org/10.1049/cvi2.12284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis is a classical method for solving problems of dimensional reduction and pattern classification. Although it has been extensively developed, however, it still suffers from various common problems, such as the Small Sample Size (SSS) and the multimodal problem. Neighbourhood linear discriminant analysis (nLDA) was recently proposed to solve the problem of multimodal class caused by the contravention of independently and identically distributed samples. However, due to the existence of many small-scale practical applications, nLDA still has to face the SSS problem, which leads to instability and poor generalisation caused by the singularity of the within-neighbourhood scatter matrix. The authors exploit the eigenspectrum regularisation techniques to circumvent the singularity of the within-neighbourhood scatter matrix of nLDA, which is called Eigenspectrum Regularisation Reverse Neighbourhood Discriminative Learning (ERRNDL). The algorithm of nLDA is reformulated as a framework by searching two projection matrices. Three eigenspectrum regularisation models are introduced to our framework to evaluate the performance. Experiments are conducted on the University of California, Irvine machine learning repository and six image classification datasets. The proposed ERRNDL-based methods achieve considerable performance.},
  archive      = {J_IETCV},
  author       = {Ming Xie and Hengliang Tan and Jiao Du and Shuo Yang and Guofeng Yan and Wangwang Li and Jianwei Feng},
  doi          = {10.1049/cvi2.12284},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {842-858},
  shortjournal = {IET Comput. Vis.},
  title        = {Eigenspectrum regularisation reverse neighbourhood discriminative learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Person re-identification via deep compound eye network and
pose repair module. <em>IETCV</em>, <em>18</em>(6), 826–841. (<a
href="https://doi.org/10.1049/cvi2.12282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification is aimed at searching for specific target pedestrians from non-intersecting cameras. However, in real complex scenes, pedestrians are easily obscured, which makes the target pedestrian search task time-consuming and challenging. To address the problem of pedestrians&#39; susceptibility to occlusion, a person re-identification via deep compound eye network (CEN) and pose repair module is proposed, which includes (1) A deep CEN based on multi-camera logical topology is proposed, which adopts graph convolution and a Gated Recurrent Unit to capture the temporal and spatial information of pedestrian walking and finally carries out pedestrian global matching through the Siamese network; (2) An integrated spatial-temporal information aggregation network is designed to facilitate pose repair. The target pedestrian features under the multi-level logic topology camera are utilised as auxiliary information to repair the occluded target pedestrian image, so as to reduce the impact of pedestrian mismatch due to pose changes; (3) A joint optimisation mechanism of CEN and pose repair network is introduced, where multi-camera logical topology inference provides auxiliary information and retrieval order for the pose repair network. The authors conducted experiments on multiple datasets, including Occluded-DukeMTMC, CUHK-SYSU, PRW, SLP, and UJS-reID. The results indicate that the authors’ method achieved significant performance across these datasets. Specifically, on the CUHK-SYSU dataset, the authors’ model achieved a top-1 accuracy of 89.1% and a mean Average Precision accuracy of 83.1% in the recognition of occluded individuals.},
  archive      = {J_IETCV},
  author       = {Hongjian Gu and Wenxuan Zou and Keyang Cheng and Bin Wu and Humaira Abdul Ghafoor and Yongzhao Zhan},
  doi          = {10.1049/cvi2.12282},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {826-841},
  shortjournal = {IET Comput. Vis.},
  title        = {Person re-identification via deep compound eye network and pose repair module},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal channel reconfiguration multi-graph convolution
network for skeleton-based action recognition. <em>IETCV</em>,
<em>18</em>(6), 813–825. (<a
href="https://doi.org/10.1049/cvi2.12279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition has received much attention and achieved remarkable achievements in the field of human action recognition. In time series action prediction for different scales, existing methods mainly focus on attention mechanisms to enhance modelling capabilities in spatial dimensions. However, this approach strongly depends on the local information of a single input feature and fails to facilitate the flow of information between channels. To address these issues, the authors propose a novel Temporal Channel Reconfiguration Multi-Graph Convolution Network (TRMGCN). In the temporal convolution part, the authors designed a module called Temporal Channel Fusion with Guidance (TCFG) to capture important temporal information within channels at different scales and avoid ignoring cross-spatio-temporal dependencies among joints. In the graph convolution part, the authors propose Top-Down Attention Multi-graph Independent Convolution (TD-MIG), which uses multi-graph independent convolution to learn the topological graph feature for different length time series. Top-down attention is introduced for spatial and channel modulation to facilitate information flow in channels that do not establish topological relationships. Experimental results on the large-scale datasets NTU-RGB + D60 and 120, as well as UAV-Human, demonstrate that TRMGCN exhibits advanced performance and capabilities. Furthermore, experiments on the smaller dataset NW-UCLA have indicated that the authors’ model possesses strong generalisation abilities.},
  archive      = {J_IETCV},
  author       = {Siyue Lei and Bin Tang and Yanhua Chen and Mingfu Zhao and Yifei Xu and Zourong Long},
  doi          = {10.1049/cvi2.12279},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {813-825},
  shortjournal = {IET Comput. Vis.},
  title        = {Temporal channel reconfiguration multi-graph convolution network for skeleton-based action recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tag-inferring and tag-guided transformer for image
captioning. <em>IETCV</em>, <em>18</em>(6), 801–812. (<a
href="https://doi.org/10.1049/cvi2.12280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is an important task for understanding images. Recently, many studies have used tags to build alignments between image information and language information. However, existing methods ignore the problem that simple semantic tags have difficulty expressing the detailed semantics for different image contents. Therefore, the authors propose a tag-inferring and tag-guided Transformer for image captioning to generate fine-grained captions. First, a tag-inferring encoder is proposed, which uses the tags extracted by the scene graph model to infer tags with deeper semantic information. Then, with the obtained deep tag information, a tag-guided decoder that includes short-term attention to improve the features of words in the sentence and gated cross-modal attention to combine image features, tag features and language features to produce informative semantic features is proposed. Finally, the word probability distribution of all positions in the sequence is calculated to generate descriptions for the image. The experiments demonstrate that the authors’ method can combine tags to obtain precise captions and that it achieves competitive performance with a 40.6% BLEU-4 score and 135.3% CIDEr score on the MSCOCO data set.},
  archive      = {J_IETCV},
  author       = {Yaohua Yi and Yinkai Liang and Dezhu Kong and Ziwei Tang and Jibing Peng},
  doi          = {10.1049/cvi2.12280},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {801-812},
  shortjournal = {IET Comput. Vis.},
  title        = {Tag-inferring and tag-guided transformer for image captioning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous-dilated temporal and inter-frame motion
excitation feature learning for gait recognition. <em>IETCV</em>,
<em>18</em>(6), 788–800. (<a
href="https://doi.org/10.1049/cvi2.12278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors present global-interval and local-continuous feature extraction networks for gait recognition. Unlike conventional gait recognition methods focussing on the full gait cycle, the authors introduce a novel global- continuous-dilated temporal feature extraction ( TFE ) to extract continuous and interval motion features from the silhouette frames globally. Simultaneously, an inter-frame motion excitation ( IME ) module is proposed to enhance the unique motion expression of an individual, which remains unchanged regardless of clothing variations. The spatio-temporal features extracted from the TFE and IME modules are then weighted and concatenated by an adaptive aggregator network for recognition. Through the experiments over CASIA-B and mini-OUMVLP datasets, the proposed method has shown the comparable performance (as 98%, 95%, and 84.9% in the normal walking, carrying a bag or packbag, and wearing coats or jackets categories in CASIA-B, and 89% in mini-OUMVLP) to the other state-of-the-art approaches. Extensive experiments conducted on the CASIA-B and mini-OUMVLP datasets have demonstrated the comparable performance of our proposed method compared to other state-of-the-art approaches.},
  archive      = {J_IETCV},
  author       = {Chunsheng Hua and Hao Zhang and Jia Li and Yingjie Pan},
  doi          = {10.1049/cvi2.12278},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {788-800},
  shortjournal = {IET Comput. Vis.},
  title        = {Continuous-dilated temporal and inter-frame motion excitation feature learning for gait recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompt guidance query with cascaded constraint decoders for
human–object interaction detection. <em>IETCV</em>, <em>18</em>(6),
772–787. (<a href="https://doi.org/10.1049/cvi2.12276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–object interaction (HOI) detection, which localises and recognises interactions between human and object, requires high-level image and scene understanding. Recent methods for HOI detection typically utilise transformer-based architecture to build unified future representation. However, these methods use random initial queries to predict interactive human–object pairs, leading to a lack of prior knowledge. Furthermore, most methods provide unified features to forecast interactions using conventional decoder structures, but they lack the ability to build efficient multi-task representations. To address these problems, we propose a novel two-stage HOI detector called PGCD, mainly consisting of prompt guidance query and cascaded constraint decoders. Firstly, the authors propose a novel prompt guidance query generation module (PGQ) to introduce the guidance-semantic features. In PGQ, the authors build visual-semantic transfer to obtain fuller semantic representations. In addition, a cascaded constraint decoder architecture (CD) with random masks is designed to build fine-grained interaction features and improve the model&#39;s generalisation performance. Experimental results demonstrate that the authors’ proposed approach obtains significant performance on the two widely used benchmarks, that is, HICO-DET and V-COCO.},
  archive      = {J_IETCV},
  author       = {Sheng Liu and Bingnan Guo and Feng Zhang and Junhao Chen and Ruixiang Chen},
  doi          = {10.1049/cvi2.12276},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {772-787},
  shortjournal = {IET Comput. Vis.},
  title        = {Prompt guidance query with cascaded constraint decoders for human–object interaction detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint image restoration for object detection in snowy
weather. <em>IETCV</em>, <em>18</em>(6), 759–771. (<a
href="https://doi.org/10.1049/cvi2.12274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although existing object detectors achieve encouraging performance of object detection and localisation under real ideal conditions, the detection performance in adverse weather conditions (snowy) is very poor and not enough to cope with the detection task in adverse weather conditions. Existing methods do not deal well with the effect of snow on the identity of object features or usually ignore or even discard potential information that can help improve the detection performance. To this end, the authors propose a novel and improved end-to-end object detection network joint image restoration. Specifically, in order to address the problem of identity degradation of object detection due to snow, an ingenious restoration-detection dual branch network structure combined with a Multi-Integrated Attention module is proposed, which can well mitigate the effect of snow on the identity of object features, thus improving the detection performance of the detector. In order to make more effective use of the features that are beneficial to the detection task, a Self-Adaptive Feature Fusion module is introduced, which can help the network better learn the potential features that are beneficial to the detection and eliminate the effect of heavy or large local snow in the object area on detection by a special feature fusion, thus improving the network&#39;s detection capability in snowy. In addition, the authors construct a large-scale, multi-size snowy dataset called Synthetic and Real Snowy Dataset (SRSD), and it is a good and necessary complement and improvement to the existing snowy-related tasks. Extensive experiments on a public snowy dataset (Snowy-weather Datasets) and SRSD indicate that our method outperforms the existing state-of-the-art object detectors.},
  archive      = {J_IETCV},
  author       = {Jing Wang and Meimei Xu and Huazhu Xue and Zhanqiang Huo and Fen Luo},
  doi          = {10.1049/cvi2.12274},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {759-771},
  shortjournal = {IET Comput. Vis.},
  title        = {Joint image restoration for object detection in snowy weather},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pruning-guided feature distillation for an efficient
transformer-based pose estimation model. <em>IETCV</em>, <em>18</em>(6),
745–758. (<a href="https://doi.org/10.1049/cvi2.12277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors propose a compression strategy for a 3D human pose estimation model based on a transformer which yields high accuracy but increases the model size. This approach involves a pruning-guided determination of the search range to achieve lightweight pose estimation under limited training time and to identify the optimal model size. In addition, the authors propose a transformer-based feature distillation (TFD) method, which efficiently exploits the pose estimation model in terms of both model size and accuracy by leveraging transformer architecture characteristics. Pruning-guided TFD is the first approach for 3D human pose estimation that employs transformer architecture. The proposed approach was tested on various extensive data sets, and the results show that it can reduce the model size by 30% compared to the state-of-the-art while ensuring high accuracy.},
  archive      = {J_IETCV},
  author       = {Dong-hwi Kim and Dong-hun Lee and Aro Kim and Jinwoo Jeong and Jong Taek Lee and Sungjei Kim and Sang-hyo Park},
  doi          = {10.1049/cvi2.12277},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {745-758},
  shortjournal = {IET Comput. Vis.},
  title        = {Pruning-guided feature distillation for an efficient transformer-based pose estimation model},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instance segmentation by blend u-net and VOLO network.
<em>IETCV</em>, <em>18</em>(6), 735–744. (<a
href="https://doi.org/10.1049/cvi2.12275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation is still challengeable to correctly distinguish different instances on overlapping, dense and large number of target objects. To address this, the authors simplify the instance segmentation problem to an instance classification problem and propose a novel end-to-end trained instance segmentation algorithm CotuNet. Firstly, the algorithm combines convolutional neural networks (CNN), Outlooker and Transformer to design a new hybrid Encoder (COT) to further feature extraction. It consists of extracting low-level features of the image using CNN, which is passed through the Outlooker to extract more refined local data representations. Then global contextual information is generated by aggregating the data representations in local space using Transformer. Finally, the combination of cascaded upsampling and skip connection modules is used as Decoders (C-UP) to enable the blend of multiple different scales of high-resolution information to generate accurate masks. By validating on the CVPPP 2017 dataset and comparing with previous state-of-the-art methods, CotuNet shows superior competitiveness and segmentation performance.},
  archive      = {J_IETCV},
  author       = {Hongfei Deng and Bin Wen and Rui Wang and Zuwei Feng},
  doi          = {10.1049/cvi2.12275},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {735-744},
  shortjournal = {IET Comput. Vis.},
  title        = {Instance segmentation by blend U-net and VOLO network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised multi-view clustering in computer vision: A
survey. <em>IETCV</em>, <em>18</em>(6), 709–734. (<a
href="https://doi.org/10.1049/cvi2.12299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-view clustering (MVC) has had significant implications in the fields of cross-modal representation learning and data-driven decision-making. Its main objective is to cluster samples into distinct groups by leveraging consistency and complementary information among multiple views. However, the field of computer vision has witnessed the evolution of contrastive learning, and self-supervised learning has made substantial research progress. Consequently, self-supervised learning is progressively becoming dominant in MVC methods. It involves designing proxy tasks to extract supervisory information from image and video data, thereby guiding the clustering process. Despite the rapid development of self-supervised MVC, there is currently no comprehensive survey analysing and summarising the current state of research progress. Hence, the authors aim to explore the emergence of self-supervised MVC by discussing the reasons and advantages behind it. Additionally, the internal connections and classifications of common datasets, data issues, representation learning methods, and self-supervised learning methods are investigated. The authors not only introduce the mechanisms for each category of methods, but also provide illustrative examples of their applications. Finally, some open problems are identified for further investigation and development.},
  archive      = {J_IETCV},
  author       = {Jiatai Wang and Zhiwei Xu and Xuewen Yang and Hailong Li and Bo Li and Xuying Meng},
  doi          = {10.1049/cvi2.12299},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {709-734},
  shortjournal = {IET Comput. Vis.},
  title        = {Self-supervised multi-view clustering in computer vision: A survey},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attentional bias for hands: Cascade dual-decoder transformer
for sign language production. <em>IETCV</em>, <em>18</em>(5), 696–708.
(<a href="https://doi.org/10.1049/cvi2.12273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign Language Production (SLP) refers to the task of translating textural forms of spoken language into corresponding sign language expressions. Sign languages convey meaning by means of multiple asynchronous articulators, including manual and non-manual information channels. Recent deep learning-based SLP models directly generate the full-articulatory sign sequence from the text input in an end-to-end manner. However, these models largely down weight the importance of subtle differences in the manual articulation due to the effect of regression to the mean. To explore these neglected aspects, an efficient cascade dual-decoder Transformer (CasDual-Transformer) for SLP is proposed to learn, successively, two mappings SLP hand : Text → Hand pose and SLP sign : Text → Sign pose , utilising an attention-based alignment module that fuses the hand and sign features from previous time steps to predict more expressive sign pose at the current time step. In addition, to provide more efficacious guidance, a novel spatio-temporal loss to penalise shape dissimilarity and temporal distortions of produced sequences is introduced. Experimental studies are performed on two benchmark sign language datasets from distinct cultures to verify the performance of the proposed model. Both quantitative and qualitative results show that the authors’ model demonstrates competitive performance compared to state-of-the-art models, and in some cases, achieves considerable improvements over them.},
  archive      = {J_IETCV},
  author       = {Xiaohan Ma and Rize Jin and Jianming Wang and Tae-Sun Chung},
  doi          = {10.1049/cvi2.12273},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {696-708},
  shortjournal = {IET Comput. Vis.},
  title        = {Attentional bias for hands: Cascade dual-decoder transformer for sign language production},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SIANet: 3D object detection with structural information
augment network. <em>IETCV</em>, <em>18</em>(5), 682–695. (<a
href="https://doi.org/10.1049/cvi2.12272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection technology from point clouds has been widely applied in the field of automatic driving in recent years. In practical applications, the shape point clouds of some objects are incomplete due to occlusion or far distance, which means they suffer from insufficient structural information. This greatly affects the detection performance. To address this challenge, the authors design a Structural Information Augment (SIA) Network for 3D object detection, named SIANet. Specifically, the authors design a SIA module to reconstruct the complete shapes of objects within proposals for enhancing their geometric features, which are further fused into the spatial feature of the object for box refinement to predict accurate detection boxes. Besides, the authors construct a novel Unet-liked Context-enhanced Transformer backbone network, which stacks Context-enhanced Transformer modules and an upsampling branch to capture contextual information efficiently and generate high-quality proposals for the SIA module. Extensive experiments show that the authors’ well-designed SIANet can effectively improve detection performance, especially surpassing the baseline network by 1.04% mean Average Precision (mAP) gain in the KITTI dataset and 0.75% LEVEL_2 mAP gain in the Waymo dataset.},
  archive      = {J_IETCV},
  author       = {Jing Zhou and Tengxing Lin and Zixin Gong and Xinhan Huang},
  doi          = {10.1049/cvi2.12272},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {682-695},
  shortjournal = {IET Comput. Vis.},
  title        = {SIANet: 3D object detection with structural information augment network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASDNet: A robust involution-based architecture for diagnosis
of autism spectrum disorder utilising eye-tracking technology.
<em>IETCV</em>, <em>18</em>(5), 666–681. (<a
href="https://doi.org/10.1049/cvi2.12271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism Spectrum Disorder (ASD) is a chronic condition characterised by impairments in social interaction and communication. Early detection of ASD is desired, and there exists a demand for the development of diagnostic aids to facilitate this. A lightweight Involutional Neural Network (INN) architecture has been developed to diagnose ASD. The model follows a simpler architectural design and has less number of parameters than the state-of-the-art (SOTA) image classification models, requiring lower computational resources. The proposed model is trained to detect ASD from eye-tracking scanpath (SP), heatmap (HM), and fixation map (FM) images. Monte Carlo Dropout has been applied to the model to perform an uncertainty analysis and ensure the effectiveness of the output provided by the proposed INN model. The model has been trained and evaluated using two publicly accessible datasets. From the experiment, it is seen that the model has achieved 98.12% accuracy, 96.83% accuracy, and 97.61% accuracy on SP, FM, and HM, respectively, which outperforms the current SOTA image classification models and other existing works conducted on this topic.},
  archive      = {J_IETCV},
  author       = {Nasirul Mumenin and Mohammad Abu Yousuf and Md Asif Nashiry and A. K. M. Azad and Salem A. Alyami and Pietro Lio&#39; and Mohammad Ali Moni},
  doi          = {10.1049/cvi2.12271},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {666-681},
  shortjournal = {IET Comput. Vis.},
  title        = {ASDNet: A robust involution-based architecture for diagnosis of autism spectrum disorder utilising eye-tracking technology},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware relation enhancement and similarity reasoning
for image-text retrieval. <em>IETCV</em>, <em>18</em>(5), 652–665. (<a
href="https://doi.org/10.1049/cvi2.12270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-text retrieval is a fundamental yet challenging task, which aims to bridge a semantic gap between heterogeneous data to achieve precise measurements of semantic similarity. The technique of fine-grained alignment between cross-modal features plays a key role in various successful methods that have been proposed. Nevertheless, existing methods cannot effectively utilise intra-modal information to enhance feature representation and lack powerful similarity reasoning to get a precise similarity score. Intending to tackle these issues, a context-aware Relation Enhancement and Similarity Reasoning model, called RESR, is proposed, which conducts both intra-modal relation enhancement and inter-modal similarity reasoning while considering the global-context information. For intra-modal relation enhancement, a novel context-aware graph convolutional network is introduced to enhance local feature representations by utilising relation and global-context information. For inter-modal similarity reasoning, local and global similarity features are exploited by the bidirectional alignment of image and text, and the similarity reasoning is implemented among multi-granularity similarity features. Finally, refined local and global similarity features are adaptively fused to get a precise similarity score. The experimental results show that our effective model outperforms some state-of-the-art approaches, achieving average improvements of 2.5% and 6.3% in R@sum on the Flickr30K and MS-COCO dataset.},
  archive      = {J_IETCV},
  author       = {Zheng Cui and Yongli Hu and Yanfeng Sun and Baocai Yin},
  doi          = {10.1049/cvi2.12270},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {652-665},
  shortjournal = {IET Comput. Vis.},
  title        = {Context-aware relation enhancement and similarity reasoning for image-text retrieval},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel multi-model 3D object detection framework with
adaptive voxel-image feature fusion. <em>IETCV</em>, <em>18</em>(5),
640–651. (<a href="https://doi.org/10.1049/cvi2.12269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multifaceted nature of sensor data has long been a hurdle for those seeking to harness its full potential in the field of 3D object detection. Although the utilisation of point clouds as input has yielded exceptional results, the challenge of effectively combining the complementary properties of multi-sensor data looms large. This work presents a new approach to multi-model 3D object detection, called adaptive voxel-image feature fusion (AVIFF). Adaptive voxel-image feature fusion is an end-to-end single-shot framework that can dynamically and adaptively fuse point cloud and image features, resulting in a more comprehensive and integrated analysis of the camera sensor and the LiDar sensor data. With the aid of the adaptive feature fusion module, spatialised image features can be adroitly fused with voxel-based point cloud features, while the Dense Fusion module ensures the preservation of the distinctive characteristics of 3D point cloud data through the use of a heterogeneous architecture. Notably, the authors’ framework features a novel generalised intersection over union loss function that enhances the perceptibility of object localsation and rotation in 3D space. Comprehensive experimentation has validated the efficacy of the authors’ proposed modules, firmly establishing AVIFF as a novel framework in the field of 3D object detection.},
  archive      = {J_IETCV},
  author       = {Zhao Liu and Zhongliang Fu and Gang Li and Shengyuan Zhang},
  doi          = {10.1049/cvi2.12269},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {640-651},
  shortjournal = {IET Comput. Vis.},
  title        = {A novel multi-model 3D object detection framework with adaptive voxel-image feature fusion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OmDet: Large-scale vision-language multi-dataset
pre-training with multimodal detection network. <em>IETCV</em>,
<em>18</em>(5), 626–639. (<a
href="https://doi.org/10.1049/cvi2.12268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of object detection (OD) in open-vocabulary and open-world scenarios is a critical challenge in computer vision. OmDet, a novel language-aware object detection architecture and an innovative training mechanism that harnesses continual learning and multi-dataset vision-language pre-training is introduced. Leveraging natural language as a universal knowledge representation, OmDet accumulates “visual vocabularies” from diverse datasets, unifying the task as a language-conditioned detection framework. The multimodal detection network (MDN) overcomes the challenges of multi-dataset joint training and generalizes to numerous training datasets without manual label taxonomy merging. The authors demonstrate superior performance of OmDet over strong baselines in object detection in the wild, open-vocabulary detection, and phrase grounding, achieving state-of-the-art results. Ablation studies reveal the impact of scaling the pre-training visual vocabulary, indicating a promising direction for further expansion to larger datasets. The effectiveness of our deep fusion approach is underscored by its ability to learn jointly from multiple datasets, enhancing performance through knowledge sharing.},
  archive      = {J_IETCV},
  author       = {Tiancheng Zhao and Peng Liu and Kyusong Lee},
  doi          = {10.1049/cvi2.12268},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {626-639},
  shortjournal = {IET Comput. Vis.},
  title        = {OmDet: Large-scale vision-language multi-dataset pre-training with multimodal detection network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale feature attention-DEtection TRansformer:
Multi-scale feature attention for security check object detection.
<em>IETCV</em>, <em>18</em>(5), 613–625. (<a
href="https://doi.org/10.1049/cvi2.12267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray security checks aim to detect contraband in luggage; however, the detection accuracy is hindered by the overlapping and significant size differences of objects in X-ray images. To address these challenges, the authors introduce a novel network model named Multi-Scale Feature Attention (MSFA)-DEtection TRansformer (DETR). Firstly, the pyramid feature extraction structure is embedded into the self-attention module, referred to as the MSFA. Leveraging the MSFA module, MSFA-DETR extracts multi-scale feature information and amalgamates them into high-level semantic features. Subsequently, these features are synergised through attention mechanisms to capture correlations between global information and multi-scale features. MSFA significantly bolsters the model&#39;s robustness across different sizes, thereby enhancing detection accuracy. Simultaneously, A new initialisation method for object queries is proposed. The authors’ foreground sequence extraction (FSE) module extracts key feature sequences from feature maps, serving as prior knowledge for object queries. FSE expedites the convergence of the DETR model and elevates detection accuracy. Extensive experimentation validates that this proposed model surpasses state-of-the-art methods on the CLCXray and PIDray datasets.},
  archive      = {J_IETCV},
  author       = {Haifeng Sima and Bailiang Chen and Chaosheng Tang and Yudong Zhang and Junding Sun},
  doi          = {10.1049/cvi2.12267},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {613-625},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-scale feature attention-DEtection TRansformer: Multi-scale feature attention for security check object detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clean, performance-robust, and performance-sensitive
historical information based adversarial self-distillation.
<em>IETCV</em>, <em>18</em>(5), 591–612. (<a
href="https://doi.org/10.1049/cvi2.12265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training suffers from poor effectiveness due to the challenging optimisation of loss with hard labels. To address this issue, adversarial distillation has emerged as a potential solution, encouraging target models to mimic the output of the teachers. However, reliance on pre-training teachers leads to additional training costs and raises concerns about the reliability of their knowledge. Furthermore, existing methods fail to consider the significant differences in unconfident samples between early and late stages, potentially resulting in robust overfitting. An adversarial defence method named Clean, Performance-robust, and Performance-sensitive Historical Information based Adversarial Self-Distillation (CPr &amp; PsHI-ASD) is presented. Firstly, an adversarial self-distillation replacement method based on clean, performance-robust, and performance-sensitive historical information is developed to eliminate pre-training costs and enhance guidance reliability for the target model. Secondly, adversarial self-distillation algorithms that leverage knowledge distilled from the previous iteration are introduced to facilitate the self-distillation of adversarial knowledge and mitigate the problem of robust overfitting. Experiments are conducted to evaluate the performance of the proposed method on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. The results demonstrate that the CPr&amp;PsHI-ASD method is more effective than existing adversarial distillation methods in enhancing adversarial robustness and mitigating robust overfitting issues against various adversarial attacks.},
  archive      = {J_IETCV},
  author       = {Shuyi Li and Hongchao Hu and Shumin Huo and Hao Liang},
  doi          = {10.1049/cvi2.12265},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {591-612},
  shortjournal = {IET Comput. Vis.},
  title        = {Clean, performance-robust, and performance-sensitive historical information based adversarial self-distillation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning framework for multi-object tracking in team
sports videos. <em>IETCV</em>, <em>18</em>(5), 574–590. (<a
href="https://doi.org/10.1049/cvi2.12266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the challenges of Multi-Object Tracking (MOT) in sports scenes, such as severe occlusions, similar appearances, drastic pose changes, and complex motion patterns, a deep-learning framework CTGMOT (CNN-Transformer-GNN-based MOT) specifically for multiple athlete tracking in sports videos that performs joint modelling of detection, appearance and motion features is proposed. Firstly, a detection network that combines Convolutional Neural Networks (CNN) and Transformers is constructed to extract both local and global features from images. The fusion of appearance and motion features is achieved through a design of parallel dual-branch decoders. Secondly, graph models are built using Graph Neural Networks (GNN) to accurately capture the spatio-temporal correlations between object and trajectory features from inter-frame and intra-frame associations. Experimental results on the public sports tracking dataset SportsMOT show that the proposed framework outperforms other state-of-the-art methods for MOT in complex sport scenes. In addition, the proposed framework shows excellent generality on benchmark datasets MOT17 and MOT20.},
  archive      = {J_IETCV},
  author       = {Wei Cao and Xiaoyong Wang and Xianxiang Liu and Yishuai Xu},
  doi          = {10.1049/cvi2.12266},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {574-590},
  shortjournal = {IET Comput. Vis.},
  title        = {A deep learning framework for multi-object tracking in team sports videos},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial catoptric light: An effective, stealthy and
robust physical-world attack to DNNs. <em>IETCV</em>, <em>18</em>(5),
557–573. (<a href="https://doi.org/10.1049/cvi2.12264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated that finely tuned deep neural networks (DNNs) are susceptible to adversarial attacks. Conventional physical attacks employ stickers as perturbations, achieving robust adversarial effects but compromising stealthiness. Recent innovations utilise light beams, such as lasers and projectors, for perturbation generation, allowing for stealthy physical attacks at the expense of robustness. In pursuit of implementing both stealthy and robust physical attacks, the authors present an adversarial catoptric light (AdvCL). This method leverages the natural phenomenon of catoptric light to generate perturbations that are both natural and stealthy. AdvCL first formalises the physical parameters of catoptric light and then optimises these parameters using a genetic algorithm to derive the most adversarial perturbation. Finally, the perturbations are deployed in the physical scene to execute stealthy and robust attacks. The proposed method is evaluated across three dimensions: effectiveness, stealthiness, and robustness. Quantitative results obtained in simulated environments demonstrate the efficacy of the proposed method, achieving an attack success rate of 83.5%, surpassing the baseline. The authors utilise common catoptric light as a perturbation to enhance the method&#39;s stealthiness, rendering physical samples more natural in appearance. Robustness is affirmed by successfully attacking advanced DNNs with a success rate exceeding 80% in all cases. Additionally, the authors discuss defence strategies against AdvCL and introduce some light-based physical attacks.},
  archive      = {J_IETCV},
  author       = {Chengyin Hu and Weiwen Shi and Ling Tian and Wen Li},
  doi          = {10.1049/cvi2.12264},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {557-573},
  shortjournal = {IET Comput. Vis.},
  title        = {Adversarial catoptric light: An effective, stealthy and robust physical-world attack to DNNs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial feature embedding for robust visual object tracking.
<em>IETCV</em>, <em>18</em>(4), 540–556. (<a
href="https://doi.org/10.1049/cvi2.12263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the offline-trained Siamese pipeline has drawn wide attention due to its outstanding tracking performance. However, the existing Siamese trackers utilise offline training to extract ‘universal’ features, which is insufficient to effectively distinguish between the target and fluctuating interference in embedding the information of the two branches, leading to inaccurate classification and localisation. In addition, the Siamese trackers employ a pre-defined scale for cropping the search candidate region based on the previous frame&#39;s result, which might easily introduce redundant background noise (clutter, similar objects etc.), affecting the tracker&#39;s robustness. To solve these problems, the authors propose two novel sub-network spatial employed to spatial feature embedding for robust object tracking. Specifically, the proposed spatial remapping (SRM) network enhances the feature discrepancy between target and distractor categories by online remapping, and improves the discriminant ability of the tracker on the embedding space. The MAML is used to optimise the SRM network to ensure its adaptability to complex tracking scenarios. Moreover, a temporal information proposal-guided (TPG) network that utilises a GRU model to dynamically predict the search scale based on temporal motion states to reduce potential background interference is introduced. The proposed two network is integrated into two popular trackers, namely SiamFC++ and TransT, which achieve superior performance on six challenging benchmarks, including OTB100, VOT2019, UAV123, GOT10K, TrackingNet and LaSOT, TrackingNet and LaSOT denoting them as SiamSRMC and SiamSRMT, respectively. Moreover, the proposed trackers obtain competitive tracking performance compared with the state-of-the-art trackers in the attribute of background clutter and similar object, validating the effectiveness of our method.},
  archive      = {J_IETCV},
  author       = {Kang Liu and Long Liu and Shangqi Yang and Zhihao Fu},
  doi          = {10.1049/cvi2.12263},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {540-556},
  shortjournal = {IET Comput. Vis.},
  title        = {Spatial feature embedding for robust visual object tracking},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language guided 3D object detection in point clouds for MEP
scenes. <em>IETCV</em>, <em>18</em>(4), 526–539. (<a
href="https://doi.org/10.1049/cvi2.12261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, contrastive language-image pre-training (CLIP) has gained popularity for processing 2D data. However, the application of cross-modal transferable learning to 3D data remains a relatively unexplored area. In addition, high-quality, labelled point cloud data for Mechanical, Electrical, and Plumbing (MEP) scenarios are in short supply. To address this issue, the authors introduce a novel object detection system that employs 3D point clouds and 2D camera images, as well as text descriptions as input, using image-text matching knowledge to guide dense detection models for 3D point clouds in MEP environments. Specifically, the authors put forth the proposition of a language-guided point cloud modelling (PCM) module, which leverages the shared image weights inherent in the CLIP backbone. This is done with the aim of generating pertinent category information for the target, thereby augmenting the efficacy of 3D point cloud target detection. After sufficient experiments, the proposed point cloud detection system with the PCM module is proven to have a comparable performance with current state-of-the-art networks. The approach has 5.64% and 2.9% improvement in KITTI and SUN-RGBD, respectively. In addition, the same good detection results are obtained in their proposed MEP scene dataset.},
  archive      = {J_IETCV},
  author       = {Junjie Li and Shengli Du and Jianfeng Liu and Weibiao Chen and Manfu Tang and Lei Zheng and Lianfa Wang and Chunle Ji and Xiao Yu and Wanli Yu},
  doi          = {10.1049/cvi2.12261},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {526-539},
  shortjournal = {IET Comput. Vis.},
  title        = {Language guided 3D object detection in point clouds for MEP scenes},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep network with double reuses and convolutional shortcuts.
<em>IETCV</em>, <em>18</em>(4), 512–525. (<a
href="https://doi.org/10.1049/cvi2.12260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors design a novel convolutional network architecture, that is, deep network with double reuses and convolutional shortcuts, in which new compressed reuse units are presented. Compressed reuse units combine the reused features from the first 3 × 3 convolutional layer and the features from the last 3 × 3 convolutional layer to produce new feature maps in the current compressed reuse unit, simultaneously reuse the feature maps from all previous compressed reuse units to generate a shortcut by an 1 × 1 convolution, and then concatenate these new maps and this shortcut as the input to next compressed reuse unit. Deep network with double reuses and convolutional shortcuts uses the feature reuse concatenation from all compressed reuse units as the final features for classification. In deep network with double reuses and convolutional shortcuts, the inner- and outer-unit feature reuses and the convolutional shortcut compressed from the previous outer-unit feature reuses can alleviate the vanishing-gradient problem by strengthening the forward feature propagation inside and outside the units, improve the effectiveness of features and reduce calculation cost. Experimental results on CIFAR-10, CIFAR-100, ImageNet ILSVRC 2012, Pascal VOC2007 and MS COCO benchmark databases demonstrate the effectiveness of authors’ architecture for object recognition and detection, as compared with the state-of-the-art.},
  archive      = {J_IETCV},
  author       = {Qian Liu and Cunbao Wang},
  doi          = {10.1049/cvi2.12260},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {512-525},
  shortjournal = {IET Comput. Vis.},
  title        = {Deep network with double reuses and convolutional shortcuts},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learnable fusion mechanisms for multimodal object detection
in autonomous vehicles. <em>IETCV</em>, <em>18</em>(4), 499–511. (<a
href="https://doi.org/10.1049/cvi2.12259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perception systems in autonomous vehicles need to accurately detect and classify objects within their surrounding environments. Numerous types of sensors are deployed on these vehicles, and the combination of such multimodal data streams can significantly boost performance. The authors introduce a novel sensor fusion framework using deep convolutional neural networks. The framework employs both camera and LiDAR sensors in a multimodal, multiview configuration. The authors leverage both data types by introducing two new innovative fusion mechanisms: element-wise multiplication and multimodal factorised bilinear pooling. The methods improve the bird&#39;s eye view moderate average precision score by +4.97% and +8.35% on the KITTI dataset when compared to traditional fusion operators like element-wise addition and feature map concatenation. An in-depth analysis of key design choices impacting performance, such as data augmentation, multi-task learning, and convolutional architecture design is offered. The study aims to pave the way for the development of more robust multimodal machine vision systems. The authors conclude the paper with qualitative results, discussing both successful and problematic cases, along with potential ways to mitigate the latter.},
  archive      = {J_IETCV},
  author       = {Yahya Massoud and Robert Laganiere},
  doi          = {10.1049/cvi2.12259},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {499-511},
  shortjournal = {IET Comput. Vis.},
  title        = {Learnable fusion mechanisms for multimodal object detection in autonomous vehicles},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised image blind super resolution via real
degradation feature learning. <em>IETCV</em>, <em>18</em>(4), 485–498.
(<a href="https://doi.org/10.1049/cvi2.12262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many methods for image super-resolution (SR) have relied on pairs of low-resolution (LR) and high-resolution (HR) images for training, where the degradation process is predefined by bicubic downsampling. While such approaches perform well in standard benchmark tests, they often fail to accurately replicate the complexity of real-world image degradation. To address this challenge, researchers have proposed the use of unpaired image training to implicitly model the degradation process. However, there is a significant domain gap between the real-world LR and the synthetic LR images from HR, which severely degrades the SR performance. A novel unsupervised image-blind super-resolution method that exploits degradation feature-based learning for real-image super-resolution reconstruction (RDFL) is proposed. Their approach learns the degradation process from HR to LR using a generative adversarial network (GAN) and constrains the data distribution of the synthetic LR with real degraded images. The authors then encode the degraded features into a Transformer-based SR network for image super-resolution reconstruction through degradation representation learning. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness and superiority of the RDFL method, which achieves visually pleasing reconstruction results.},
  archive      = {J_IETCV},
  author       = {Cheng Yang and Guanming Lu},
  doi          = {10.1049/cvi2.12262},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {485-498},
  shortjournal = {IET Comput. Vis.},
  title        = {Unsupervised image blind super resolution via real degradation feature learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A decoder structure guided CNN-transformer network for face
super-resolution. <em>IETCV</em>, <em>18</em>(4), 473–484. (<a
href="https://doi.org/10.1049/cvi2.12251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep convolutional neural networks have shown improved performance in face super-resolution through joint training with other tasks such as face analysis and landmark prediction. However, these methods have certain limitations. One major limitation is the requirement for manual marking information on the dataset for multi-task joint learning. This additional marking process increases the computational cost of the network model. Additionally, since prior information is often estimated from low-quality faces, the obtained guidance information tends to be inaccurate. To address these challenges, a novel Decoder Structure Guided CNN-Transformer Network (DCTNet) is introduced, which utilises the newly proposed Global-Local Feature Extraction Unit (GLFEU) for effective embedding. Specifically, the proposed GLFEU is composed of an attention branch and a Transformer branch, to simultaneously restore global facial structure and local texture details. Additionally, a Multi-Stage Feature Fusion Module is incorporated to fuse features from different network stages, further improving the quality of the restored face images. Compared with previous methods, DCTNet improves Peak Signal-to-Noise Ratio by 0.23 and 0.19 dB on the CelebA and Helen datasets, respectively. Experimental results demonstrate that the designed DCTNet offers a simple yet powerful solution to recover detailed facial structures from low-quality images.},
  archive      = {J_IETCV},
  author       = {Rui Dou and Jiawen Li and Xujie Wan and Heyou Chang and Hao Zheng and Guangwei Gao},
  doi          = {10.1049/cvi2.12251},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {473-484},
  shortjournal = {IET Comput. Vis.},
  title        = {A decoder structure guided CNN-transformer network for face super-resolution},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video frame interpolation via spatial multi-scale modelling.
<em>IETCV</em>, <em>18</em>(4), 458–472. (<a
href="https://doi.org/10.1049/cvi2.12281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame interpolation (VFI) is a technique that synthesises intermediate frames between adjacent original video frames to enhance the temporal super-resolution of the video. However, existing methods usually rely on heavy model architectures with a large number of parameters. The authors introduce an efficient VFI network based on multiple lightweight convolutional units and a Local three-scale encoding (LTSE) structure. In particular, the authors introduce a LTSE structure with two-level attention cascades. This design is tailored to enhance the efficient capture of details and contextual information across diverse scales in images. Secondly, the authors introduce recurrent convolutional layers (RCL) and residual operations, designing the recurrent residual convolutional unit to optimise the LTSE structure. Additionally, a lightweight convolutional unit named separable recurrent residual convolutional unit is introduced to reduce the model parameters. Finally, the authors obtain the three-scale decoding features from the decoder and warp them for a set of three-scale pre-warped maps. The authors fuse them into the synthesis network to generate high-quality interpolated frames. The experimental results indicate that the proposed approach achieves superior performance with fewer model parameters.},
  archive      = {J_IETCV},
  author       = {Zhe Qu and Weijing Liu and Lizhen Cui and Xiaohui Yang},
  doi          = {10.1049/cvi2.12281},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {458-472},
  shortjournal = {IET Comput. Vis.},
  title        = {Video frame interpolation via spatial multi-scale modelling},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A temporal shift reconstruction network for compressive
video sensing. <em>IETCV</em>, <em>18</em>(4), 448–457. (<a
href="https://doi.org/10.1049/cvi2.12234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive sensing provides a promising sampling paradigm for video acquisition for resource-limited sensor applications. However, the reconstruction of original video signals from sub-sampled measurements is still a great challenge. To exploit the temporal redundancies within videos during the recovery, previous works tend to perform alignment on initial reconstructions, which are too coarse to provide accurate motion estimations. To solve this problem, the authors propose a novel reconstruction network, named TSRN, for compressive video sensing. Specifically, the authors utilise a number of stacked temporal shift reconstruction blocks (TSRBs) to enhance the initial reconstruction progressively. Each TSRB could learn the temporal structures by exchanging information with last and next time step, and no additional computations is imposed on the network compared to regular 2D convolutions due to the high efficiency of temporal shift operations. After the enhancement, a bidirectional alignment module to build accurate temporal dependencies directly with the help of optical flows is employed. Different from previous methods that only extract supplementary information from the key frames, the proposed alignment module can receive temporal information from the whole video sequence via bidirectional propagations, thus yielding better performance. Experimental results verify the superiority of the proposed method over other state-of-the-art approaches quantitatively and qualitatively.},
  archive      = {J_IETCV},
  author       = {Zhenfei Gu and Chao Zhou and Guofeng Lin},
  doi          = {10.1049/cvi2.12234},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {448-457},
  shortjournal = {IET Comput. Vis.},
  title        = {A temporal shift reconstruction network for compressive video sensing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A point-image fusion network for event-based frame
interpolation. <em>IETCV</em>, <em>18</em>(4), 439–447. (<a
href="https://doi.org/10.1049/cvi2.12220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal information in event streams plays a critical role in event-based video frame interpolation as it provides temporal context cues complementary to images. Most previous event-based methods first transform the unstructured event data to structured data formats through voxelisation, and then employ advanced CNNs to extract temporal information. However, voxelisation inevitably leads to information loss, and processing the sparse voxels introduces severe computation redundancy. To address these limitations, this study proposes a point-image fusion network (PIFNet). In our PIFNet, rich temporal information from the events can be directly extracted at the point level. Then, a fusion module is designed to fuse complementary cues from both points and images for frame interpolation. Extensive experiments on both synthetic and real datasets demonstrate that our PIFNet achieves state-of-the-art performance with high efficiency.},
  archive      = {J_IETCV},
  author       = {Chushu Zhang and Wei An and Ye Zhang and Miao Li},
  doi          = {10.1049/cvi2.12220},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {439-447},
  shortjournal = {IET Comput. Vis.},
  title        = {A point-image fusion network for event-based frame interpolation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CR-net: Robot grasping detection method integrating
convolutional block attention module and residual module.
<em>IETCV</em>, <em>18</em>(3), 420–433. (<a
href="https://doi.org/10.1049/cvi2.12252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grasping detection, which involves identifying and assessing the grasp ability of objects by robotic systems, has garnered significant attention in recent years due to its pivotal role in the development of robotic systems and automated assembly processes. Despite notable advancements in this field, current methods often grapple with both practical and theoretical challenges that hinder their real-world applicability. These challenges encompass low detection accuracy, the burden of oversized model parameters, and the inherent complexity of real-world scenarios. In response to these multifaceted challenges, a novel lightweight grasping detection model that not only addresses the technical aspects but also delves into the underlying theoretical complexities is introduced. The proposed model incorporates attention mechanisms and residual modules to tackle the theoretical challenges posed by varying object shapes, sizes, materials, and environmental conditions. To enhance its performance in the face of these theoretical complexities, the proposed model employs a Convolutional Block Attention Module (CBAM) to extract features from RGB and depth channels, recognising the multifaceted nature of object properties. Subsequently, a feature fusion module effectively combines these diverse features, providing a solution to the theoretical challenge of information integration. The model then processes the fused features through five residual blocks, followed by another CBAM attention module, culminating in the generation of three distinct images representing capture quality, grasping angle, and grasping width. These images collectively yield the final grasp detection results, addressing the theoretical complexities inherent in this task. The proposed model&#39;s rigorous training and evaluation on the Cornell Grasp dataset demonstrate remarkable detection accuracy rates of 98.44% on the Image-wise split and 96.88% on the Object-wise split. The experimental results strongly corroborate the exceptional performance of the proposed model, underscoring its ability to overcome the theoretical challenges associated with grasping detection. The integration of the residual module ensures rapid training, while the attention module facilitates precise feature extraction, ultimately striking an effective balance between detection time and accuracy.},
  archive      = {J_IETCV},
  author       = {Song Yan and Lei Zhang},
  doi          = {10.1049/cvi2.12252},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {420-433},
  shortjournal = {IET Comput. Vis.},
  title        = {CR-net: Robot grasping detection method integrating convolutional block attention module and residual module},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatio-temporal enhanced graph-transformer AutoEncoder
embedded pose for anomaly detection. <em>IETCV</em>, <em>18</em>(3),
405–419. (<a href="https://doi.org/10.1049/cvi2.12257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the robustness of skeleton data to human scale, illumination changes, dynamic camera views, and complex backgrounds, great progress has been made in skeleton-based video anomaly detection in recent years. The spatio-temporal graph convolutional network has been proven to be effective in modelling the spatio-temporal dependencies of non-Euclidean data such as human skeleton graphs, and the autoencoder based on this basic unit is widely used to model sequence features. However, due to the limitations of the convolution kernel, the model cannot capture the correlation between non-adjacent joints, and it is difficult to deal with long-term sequences, resulting in an insufficient understanding of behaviour. To address this issue, this paper applies the Transformer to the human skeleton and proposes the Spatio-Temporal Enhanced Graph-Transformer AutoEncoder (STEGT-AE) to improve the capability of modelling. In addition, the multi-memory model with skip connections is employed to provide different levels of coding features, thereby enhancing the ability of the model to distinguish similar heterogeneous behaviours. Furthermore, the STEGT-AE has a single encoder-double decoder architecture, which can improve the detection performance by the combining reconstruction and prediction error. The experimental results show that performances of STEGT-AE is significantly better than other advanced algorithms on four baseline datasets.},
  archive      = {J_IETCV},
  author       = {Honglei Zhu and Pengjuan Wei and Zhigang Xu},
  doi          = {10.1049/cvi2.12257},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {405-419},
  shortjournal = {IET Comput. Vis.},
  title        = {A spatio-temporal enhanced graph-transformer AutoEncoder embedded pose for anomaly detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anti-occlusion person re-identification via body topology
information restoration and similarity evaluation. <em>IETCV</em>,
<em>18</em>(3), 393–404. (<a
href="https://doi.org/10.1049/cvi2.12256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, pedestrian images often suffer from occlusion, where certain body features become invisible, making it challenging for existing methods to accurately identify pedestrians with the same ID. Traditional approaches typically focus on matching only the visible body parts, which can lead to misalignment when the occlusion patterns vary. To address this issue and alleviate misalignment in occluded pedestrian images, the authors propose a novel framework called body topology information generation and matching. The framework consists of two main modules: the body topology information generation module and the body topology information matching module. The body topology information generation module employs an adaptive detection mechanism and capsule generative adversarial network to restore a holistic pedestrian image while preserving the body topology information. The body topology information matching module leverages the restored holistic image from body topology information generation to overcome spatial misalignment and utilises cosine distance as the similarity measure for matching. By combining the body topology information generation and body topology information matching modules, the authors achieve consistency in the body topology information features of pedestrian images, ranging from restoration to retrieval. Extensive experiments are conducted on both holistic person re-identification datasets (Market-1501, DukeMTMC-ReID) and occluded person re-identification datasets (Occluded-DukeMTMC, Occluded-ReID). The results demonstrate the superior performance of the authors proposed model, and visualisations of the generation and matching modules are provided to illustrate their effectiveness. Furthermore, an ablation study is conducted to validate the contributions of the proposed framework.},
  archive      = {J_IETCV},
  author       = {Chunyun Meng and Ernest Domanaanmwi Ganaa and Bin Wu and Zhen Tan and Li Luan},
  doi          = {10.1049/cvi2.12256},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {393-404},
  shortjournal = {IET Comput. Vis.},
  title        = {Anti-occlusion person re-identification via body topology information restoration and similarity evaluation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point cloud semantic segmentation based on local feature
fusion and multilayer attention network. <em>IETCV</em>, <em>18</em>(3),
381–392. (<a href="https://doi.org/10.1049/cvi2.12255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation from a three-dimensional point cloud is vital in autonomous driving, computer vision, and augmented reality. However, current semantic segmentation does not effectively use the point cloud&#39;s local geometric features and contextual information, essential for improving segmentation accuracy. A semantic segmentation network that uses local feature fusion and a multilayer attention mechanism is proposed to address these challenges. Specifically, the authors designed a local feature fusion module to encode the geometric and feature information separately, which fully leverages the point cloud&#39;s feature perception and geometric structure representation. Furthermore, the authors designed a multilayer attention pooling module consisting of local attention pooling and cascade attention pooling to extract contextual information. Local attention pooling is used to learn local neighbourhood information, and cascade attention pooling captures contextual information from deeper local neighbourhoods. Finally, an enhanced feature representation of important information is obtained by aggregating the features from the two deep attention pooling methods. Extensive experiments on large-scale point-cloud datasets Stanford 3D large-scale indoor spaces and SemanticKITTI indicate that authors network shows excellent advantages over existing representative methods regarding local geometric feature description and global contextual relationships.},
  archive      = {J_IETCV},
  author       = {Junjie Wen and Jie Ma and Yuehua Zhao and Tong Nie and Mengxuan Sun and Ziming Fan},
  doi          = {10.1049/cvi2.12255},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {381-392},
  shortjournal = {IET Comput. Vis.},
  title        = {Point cloud semantic segmentation based on local feature fusion and multilayer attention network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised domain adaptation via subspace exploration.
<em>IETCV</em>, <em>18</em>(3), 370–380. (<a
href="https://doi.org/10.1049/cvi2.12254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent methods of learning latent representations in Domain Adaptation (DA) often entangle the learning of features and exploration of latent space into a unified process. However, these methods can cause a false alignment problem and do not generalise well to the alignment of distributions with large discrepancy. In this study, the authors propose to explore a robust subspace for Semi-Supervised Domain Adaptation (SSDA) explicitly. To be concrete, for disentangling the intricate relationship between feature learning and subspace exploration, the authors iterate and optimise them in two steps: in the first step, the authors aim to learn well-clustered latent representations by aggregating the target feature around the estimated class-wise prototypes; in the second step, the authors adaptively explore a subspace of an autoencoder for robust SSDA. Specially, a novel denoising strategy via class-agnostic disturbance to improve the discriminative ability of subspace is adopted. Extensive experiments on publicly available datasets verify the promising and competitive performance of our approach against state-of-the-art methods.},
  archive      = {J_IETCV},
  author       = {Zheng Han and Xiaobin Zhu and Chun Yang and Zhiyu Fang and Jingyan Qin and Xucheng Yin},
  doi          = {10.1049/cvi2.12254},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {370-380},
  shortjournal = {IET Comput. Vis.},
  title        = {Semi-supervised domain adaptation via subspace exploration},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MBMF: Constructing memory banks of multi-scale features for
anomaly detection. <em>IETCV</em>, <em>18</em>(3), 355–369. (<a
href="https://doi.org/10.1049/cvi2.12258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial manufacturing, how to accurately classify defective products and locate the location of defects has always been a concern. Previous studies mainly measured similarity based on extracting single-scale features of samples. However, only using the features of a single scale is hard to represent different sizes and types of anomalies. Therefore, the authors propose a set of memory banks of multi-scale features (MBMF) to enrich feature representation and detect and locate various anomalies. To extract features of different scales, different aggregation functions are designed to produce the feature maps at different granularity. Based on the multi-scale features of normal samples, the MBMF are constructed. Meanwhile, to better adapt to the feature distribution of the training samples, the authors proposed a new iterative updating method for the memory banks. Testing on the widely used and challenging dataset of MVTec AD, the proposed MBMF achieves competitive image-level anomaly detection performance (Image-level Area Under the Receiver Operator Curve (AUROC)) and pixel-level anomaly segmentation performance (Pixel-level AUROC). To further evaluate the generalisation of the proposed method, we also implement anomaly detection on the BeanTech AD dataset, a commonly used dataset in the field of anomaly detection, and the Fashion-MNIST dataset, a widely used dataset in the field of image classification. The experimental results also verify the effectiveness of the proposed method.},
  archive      = {J_IETCV},
  author       = {Yanfeng Sun and Haitao Wang and Yongli Hu and Huajie Jiang and Baocai Yin},
  doi          = {10.1049/cvi2.12258},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {355-369},
  shortjournal = {IET Comput. Vis.},
  title        = {MBMF: Constructing memory banks of multi-scale features for anomaly detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene context-aware graph convolutional network for
skeleton-based action recognition. <em>IETCV</em>, <em>18</em>(3),
343–354. (<a href="https://doi.org/10.1049/cvi2.12253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition methods commonly employ graph neural networks to learn different aspects of skeleton topology information However, these methods often struggle to capture contextual information beyond the skeleton topology. To address this issue, a Scene Context-aware Graph Convolutional Network (SCA-GCN) that leverages potential contextual information in the scene is proposed. Specifically, SCA-GCN learns the co-occurrence probabilities of actions in specific scenarios from a common knowledge base and fuses these probabilities into the original skeleton topology decoder, producing more robust results. To demonstrate the effectiveness of SCA-GCN, extensive experiments on four widely used datasets, that is, SBU, N-UCLA, NTU RGB + D, and NTU RGB + D 120 are conducted. The experimental results show that SCA-GCN surpasses existing methods, and its core idea can be extended to other methods with only some concatenation operations that consume less computational complexity.},
  archive      = {J_IETCV},
  author       = {Wenxian Zhang},
  doi          = {10.1049/cvi2.12253},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {343-354},
  shortjournal = {IET Comput. Vis.},
  title        = {Scene context-aware graph convolutional network for skeleton-based action recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A survey on weakly supervised 3D point cloud semantic
segmentation. <em>IETCV</em>, <em>18</em>(3), 329–342. (<a
href="https://doi.org/10.1049/cvi2.12250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity and advancement of 3D point cloud data acquisition technologies and sensors, research into 3D point clouds has made considerable strides based on deep learning. The semantic segmentation of point clouds, a crucial step in comprehending 3D scenes, has drawn much attention. The accuracy and effectiveness of fully supervised semantic segmentation tasks have greatly improved with the increase in the number of accessible datasets. However, these achievements rely on time-consuming and expensive full labelling. In solve of these existential issues, research on weakly supervised learning has recently exploded. These methods train neural networks to tackle 3D semantic segmentation tasks with fewer point labels. In addition to providing a thorough overview of the history and current state of the art in weakly supervised semantic segmentation of 3D point clouds, a detailed description of the most widely used data acquisition sensors, a list of publicly accessible benchmark datasets, and a look ahead to potential future development directions is provided.},
  archive      = {J_IETCV},
  author       = {Jingyi Wang and Yu Liu and Hanlin Tan and Maojun Zhang},
  doi          = {10.1049/cvi2.12250},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {329-342},
  shortjournal = {IET Comput. Vis.},
  title        = {A survey on weakly supervised 3D point cloud semantic segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StableNet: Distinguishing the hard samples to overcome
language priors in visual question answering. <em>IETCV</em>,
<em>18</em>(2), 315–327. (<a
href="https://doi.org/10.1049/cvi2.12249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the booming fields of computer vision and natural language processing, cross-modal intersections such as visual question answering (VQA) have become very popular. However, several studies have shown that many VQA models suffer from severe language prior problems. After a series of experiments, the authors found that previous VQA models are in an unstable state, that is, when training is repeated several times on the same dataset, there are significant differences between the distributions of the predicted answers given by the models each time, and these models also perform unsatisfactorily in terms of accuracy. The reason for model instability is that some of the difficult samples bring serious interference to model training, so we design a method to measure model stability quantitatively and further propose a method that can alleviate both model imbalance and instability phenomena. Precisely, the question types are classified into simple and difficult ones different weighting measures are applied. By imposing constraints on the training process for both types of questions, the stability and accuracy of the model improve. Experimental results demonstrate the effectiveness of our method, which achieves 63.11% on VQA-CP v2 and 75.49% with the addition of the pre-trained model.},
  archive      = {J_IETCV},
  author       = {Zhengtao Yu and Jia Zhao and Chenliang Guo and Ying Yang},
  doi          = {10.1049/cvi2.12249},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {315-327},
  shortjournal = {IET Comput. Vis.},
  title        = {StableNet: Distinguishing the hard samples to overcome language priors in visual question answering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving neural ordinary differential equations via
knowledge distillation. <em>IETCV</em>, <em>18</em>(2), 304–314. (<a
href="https://doi.org/10.1049/cvi2.12248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural ordinary differential equations (ODEs) (Neural ODEs) construct the continuous dynamics of hidden units using ODEs specified by a neural network, demonstrating promising results on many tasks. However, Neural ODEs still do not perform well on image recognition tasks. The possible reason is that the one-hot encoding vector commonly used in Neural ODEs can not provide enough supervised information. A new training based on knowledge distillation is proposed to construct more powerful and robust Neural ODEs fitting image recognition tasks. Specially, the training of Neural ODEs is modelled into a teacher-student learning process, in which ResNets are proposed as the teacher model to provide richer supervised information. The experimental results show that the new training manner can improve the classification accuracy of Neural ODEs by 5.17%, 24.75%, 7.20%, and 8.99%, on Street View House Numbers, CIFAR10, CIFAR100, and Food-101, respectively. In addition, the effect of knowledge distillation is also evaluated in Neural ODEs on robustness against adversarial examples. The authors discover that incorporating knowledge distillation, coupled with the increase of the time horizon, can significantly enhance the robustness of Neural ODEs. The performance improvement is analysed from the perspective of the underlying dynamical system.},
  archive      = {J_IETCV},
  author       = {Haoyu Chu and Shikui Wei and Qiming Lu and Yao Zhao},
  doi          = {10.1049/cvi2.12248},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {304-314},
  shortjournal = {IET Comput. Vis.},
  title        = {Improving neural ordinary differential equations via knowledge distillation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representation constraint-based dual-channel network for
face antispoofing. <em>IETCV</em>, <em>18</em>(2), 289–303. (<a
href="https://doi.org/10.1049/cvi2.12245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although multimodal face data have obvious advantages in describing live and spoofed features, single-modality face antispoofing technologies are still widely used when it is difficult to obtain multimodal face images or inconvenient to integrate and deploy multimodal sensors. Since the live/spoofed representations in visible light facial images include considerable face identity information interference, existing deep learning-based face antispoofing models achieve poor performance when only the visible light modality is used. To address the above problems, the authors design a dual-channel network structure and a constrained representation learning method for face antispoofing. First, they design a dual-channel attention mechanism-based grouped convolutional neural network (CNN) to learn important deceptive cues in live and spoofed faces. Second, they design inner contrastive estimation-based representation constraints for both live and spoofed samples to minimise the sample similarity loss to prevent the CNN from learning more facial appearance information. This increases the distance between live and spoofed faces and enhances the network&#39;s ability to identify deceptive cues. The evaluation results indicate that the framework we designed achieves an average classification error rate (ACER) of 2.37% on the visible light modality subset of the CASIA-SURF dataset and an ACER of 2.4% on the CASIA-SURF CeFA dataset, outperforming existing methods. The proposed method achieves low ACER scores in cross-dataset testing, demonstrating its advantage in domain generalisation.},
  archive      = {J_IETCV},
  author       = {Zuhe Li and Yuhao Cui and Fengqin Wang and Weihua Liu and Yongshuang Yang and Zeqi Yu and Bin Jiang and Hui Chen},
  doi          = {10.1049/cvi2.12245},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {289-303},
  shortjournal = {IET Comput. Vis.},
  title        = {Representation constraint-based dual-channel network for face antispoofing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGB depth salient object detection via cross-modal attention
and boundary feature guidance. <em>IETCV</em>, <em>18</em>(2), 273–288.
(<a href="https://doi.org/10.1049/cvi2.12244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB depth (RGB-D) salient object detection (SOD) is a meaningful and challenging task, which has achieved good detection performance in dealing with simple scenes using convolutional neural networks, however, it cannot effectively handle scenes with complex contours of salient objects or similarly coloured salient objects and background. A novel end-to-end framework is proposed for RGB-D SOD, which comprises of four main components: the cross-modal attention feature enhancement (CMAFE) module, the multi-level contextual feature interaction (MLCFI) module, the boundary feature extraction (BFE) module, and the multi-level boundary attention guidance (MLBAG) module. The CMAFE module retains the more effective salient features by employing a dual-attention mechanism to filter noise from two modalities. In the MLCFI module, a shuffle operation is used for high-level and low-level channels to promote cross-channel information communication, and rich semantic information is extracted. The BFE module converts salient features into boundary features to generate boundary maps. The MLBAG module produces saliency maps by aggregating multi-level boundary saliency maps to guide cross-modal features in the decode stage. Extensive experiments are conducted on six public benchmark datasets, with the results demonstrating that the proposed model significantly outperforms 23 state-of-the-art RGB-D SOD models with regards to multiple evaluation metrics.},
  archive      = {J_IETCV},
  author       = {Lingbing Meng and Mengya Yuan and Xuehan Shi and Le Zhang and Qingqing Liu and Dai Ping and Jinhua Wu and Fei Cheng},
  doi          = {10.1049/cvi2.12244},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {273-288},
  shortjournal = {IET Comput. Vis.},
  title        = {RGB depth salient object detection via cross-modal attention and boundary feature guidance},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GFRNet: Rethinking the global contexts extraction in medical
images segmentation through matrix factorization and self-attention.
<em>IETCV</em>, <em>18</em>(2), 260–272. (<a
href="https://doi.org/10.1049/cvi2.12243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the large fluctuations of the boundaries and internal variations of the lesion regions in medical image segmentation, current methods may have difficulty capturing sufficient global contexts effectively to deal with these inherent challenges, which may lead to a problem of segmented discrete masks undermining the performance of segmentation. Although self-attention can be implemented to capture long-distance dependencies between pixels, it has the disadvantage of computational complexity and the global contexts extracted by self-attention are still insufficient. To this end, the authors propose the GFRNet, which resorts to the idea of low-rank matrix factorization by forming global contexts locally to obtain global contexts that are totally different from contexts extracted by self-attention. The authors effectively integrate the different global contexts extract by self-attention and low-rank matrix factorization to extract versatile global contexts. Also, to recover the spatial contexts lost during the matrix factorization process and enhance boundary contexts, the authors propose the Modified Matrix Decomposition module which employ depth-wise separable convolution and spatial augmentation in the low-rank matrix factorization process. Comprehensive experiments are performed on four benchmark datasets showing that GFRNet performs better than the relevant CNN and transformer-based recipes.},
  archive      = {J_IETCV},
  author       = {Lifang Chen and Shanglai Wang and Li Wan and Jianghu Su and Shunfeng Wang},
  doi          = {10.1049/cvi2.12243},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {260-272},
  shortjournal = {IET Comput. Vis.},
  title        = {GFRNet: Rethinking the global contexts extraction in medical images segmentation through matrix factorization and self-attention},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous sign language recognition based on hierarchical
memory sequence network. <em>IETCV</em>, <em>18</em>(2), 247–259. (<a
href="https://doi.org/10.1049/cvi2.12240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the goal of solving the problem of feature extractors lacking strong supervision training and insufficient time information concerning single-sequence model learning, a hierarchical sequence memory network with a multi-level iterative optimisation strategy is proposed for continuous sign language recognition. This method uses the spatial-temporal fusion convolution network (STFC-Net) to extract the spatial-temporal information of RGB and Optical flow video frames to obtain the multi-modal visual features of a sign language video. Then, in order to enhance the temporal relationships of visual feature maps, the hierarchical memory sequence network is used to capture local utterance features and global context dependencies across time dimensions to obtain sequence features. Finally, the decoder decodes the final sentence sequence. In order to enhance the feature extractor, the authors adopted a multi-level iterative optimisation strategy to fine-tune STFC-Net and the utterance feature extractor. The experimental results on the RWTH-Phoenix-Weather multi-signer 2014 dataset and the Chinese sign language dataset show the effectiveness and superiority of this method.},
  archive      = {J_IETCV},
  author       = {Cuihong Xue and Jingli Jia and Ming Yu and Gang Yan and Yingchun Guo and Yuehao Liu},
  doi          = {10.1049/cvi2.12240},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {247-259},
  shortjournal = {IET Comput. Vis.},
  title        = {Continuous sign language recognition based on hierarchical memory sequence network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dense multi-scale context and asymmetric pooling embedding
network for smoke segmentation. <em>IETCV</em>, <em>18</em>(2), 236–246.
(<a href="https://doi.org/10.1049/cvi2.12246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is very challenging to accurately segment smoke images because smoke has some adverse vision characteristics, such as anomalous shapes, blurred edges, and translucency. Existing methods cannot fully focus on the texture details of anomalous shapes and blurred edges simultaneously. To solve these problems, a Dense Multi-scale context and Asymmetric pooling Embedding Network (DMAENet) is proposed to model the smoke edge details and anomalous shapes for smoke segmentation. To capture the feature information from different scales, a Dense Multi-scale Context Module (DMCM) is proposed to further enhance the feature representation capability of our network under the help of asymmetric convolutions. To efficiently extract features for long-shaped objects, the authors use asymmetric pooling to propose an Asymmetric Pooling Enhancement Module (APEM). The vertical and horizontal pooling methods are responsible for enhancing features of irregular objects. Finally, a Feature Fusion Module (FFM) is designed, which accepts three inputs for improving performance. Low and high-level features are fused by pixel-wise summing, and then the summed feature maps are further enhanced in an attention manner. Experimental results on synthetic and real smoke datasets validate that all these modules can improve performance, and the proposed DMAENet obviously outperforms existing state-of-the-art methods.},
  archive      = {J_IETCV},
  author       = {Gang Wen and Fangrong Zhou and Yutang Ma and Hao Pan and Hao Geng and Jun Cao and Kang Li and Feiniu Yuan},
  doi          = {10.1049/cvi2.12246},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {236-246},
  shortjournal = {IET Comput. Vis.},
  title        = {A dense multi-scale context and asymmetric pooling embedding network for smoke segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDBNet: Improved differentiable binarisation network for
natural scene text detection. <em>IETCV</em>, <em>18</em>(2), 224–235.
(<a href="https://doi.org/10.1049/cvi2.12241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The text in the natural scene can express rich semantic information, which helps people understand and analyse daily things. This paper focuses on the problems of discrete text spatial distribution and variable text geometric size in natural scenes with complex backgrounds and proposes an end-to-end natural scene text detection method based on DBNet. The authors first use IResNet as the backbone network, which does not increase network parameters while retaining more text features. Furthermore, a module with Transformer is introduced in the feature extraction stage to strengthen the correlation between high-level feature pixels. Then, the authors add a spatial pyramid pooling structure in the end of feature extraction, which realises the combination of local and global features, enriches the expressive ability of feature maps, and alleviates the detection limitations caused by the geometric size of features. Finally, to better integrate the features of each level, a dual attention module is embedded after multi-scale feature fusion. Extensive experiments on the MSRA-TD500, CTW1500, ICDAR2015, and MLT2017 data set are conducted. The results showed that IDBNet can improve the average precision, recall, and F-measure of a text compared with the state of art text detection methods and has higher predictive ability and practicability.},
  archive      = {J_IETCV},
  author       = {Zhijia Zhang and Yiming Shao and Ligang Wang and Haixing Li and Yunpeng Liu},
  doi          = {10.1049/cvi2.12241},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {224-235},
  shortjournal = {IET Comput. Vis.},
  title        = {IDBNet: Improved differentiable binarisation network for natural scene text detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene flow estimation from 3D point clouds based on
dual-branch implicit neural representations. <em>IETCV</em>,
<em>18</em>(2), 210–223. (<a
href="https://doi.org/10.1049/cvi2.12237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, online optimisation-based scene flow estimation has attracted significant attention due to its strong domain adaptivity. Although online optimisation-based methods have made significant advances, the performance is far from satisfactory as only flow priors are considered, neglecting scene priors that are crucial for the representations of dynamic scenes. To address this problem, the authors introduce a dual-branch MLP-based architecture to encode implicit scene representations from a source 3D point cloud, which can additionally synthesise a target 3D point cloud. Thus, the mapping function between the source and synthesised target 3D point clouds is established as an extra implicit regulariser to capture scene priors. Moreover, their model infers both flow and scene priors in a stronger bidirectional manner. It can effectively establish spatiotemporal constraints among the synthesised, source, and target 3D point clouds. Experiments on four challenging datasets, including KITTI scene flow, FlyingThings3D, Argoverse, and nuScenes, show that our method can achieve potential and comparable results, proving its effectiveness and generality.},
  archive      = {J_IETCV},
  author       = {Mingliang Zhai and Kang Ni and Jiucheng Xie and Hao Gao},
  doi          = {10.1049/cvi2.12237},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {210-223},
  shortjournal = {IET Comput. Vis.},
  title        = {Scene flow estimation from 3D point clouds based on dual-branch implicit neural representations},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time vehicle detection using segmentation-based
detection network and trajectory prediction. <em>IETCV</em>,
<em>18</em>(2), 191–209. (<a
href="https://doi.org/10.1049/cvi2.12236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The position of vehicles is determined using an algorithm that includes two stages of detection and prediction. The more the number of frames in which the detection network is used, the more accurate the detector is, and the more the prediction network is used, the algorithm is faster. Therefore, the algorithm is very flexible to achieve the required accuracy and speed. YOLO&#39;s base detection network is designed to be robust against vehicle scale changes. Also, feature maps are produced in the detector network, which contribute greatly to increasing the accuracy of the detector. In these maps, using differential images and a u-net-based module, image segmentation has been done into two classes: vehicle and background. To increase the accuracy of the recursive predictive network, vehicle manoeuvres are classified. For this purpose, the spatial and temporal information of the vehicles are considered simultaneously. This classifier is much more effective than classifiers that consider spatial and temporal information separately. The Highway and UA-DETRAC datasets demonstrate the performance of the proposed algorithm in urban traffic monitoring systems.},
  archive      = {J_IETCV},
  author       = {Nafiseh Zarei and Payman Moallem and Mohammadreza Shams},
  doi          = {10.1049/cvi2.12236},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {191-209},
  shortjournal = {IET Comput. Vis.},
  title        = {Real-time vehicle detection using segmentation-based detection network and trajectory prediction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IoUNet++: Spatial cross-layer interaction-based bounding box
regression for visual tracking. <em>IETCV</em>, <em>18</em>(1), 177–189.
(<a href="https://doi.org/10.1049/cvi2.12235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate target prediction, especially bounding box estimation, is a key problem in visual tracking. Many recently proposed trackers adopt the refinement module called IoU predictor by designing a high-level modulation vector to achieve bounding box estimation. However, due to the lack of spatial information that is important for precise box estimation, this simple one-dimensional modulation vector has limited refinement representation capability. In this study, a novel IoU predictor (IoUNet++) is designed to achieve more accurate bounding box estimation by investigating spatial matching with a spatial cross-layer interaction model. Rather than using a one-dimensional modulation vector to generate representations of the candidate bounding box for overlap prediction, this paper first extracts and fuses multi-level features of the target to generate template kernel with spatial description capability. Then, when aggregating the features of the template and the search region, the depthwise separable convolution correlation is adopted to preserve the spatial matching between the target feature and candidate feature, which makes their IoUNet++ network have better template representation and better fusion than the original network. The proposed IoUNet++ method with a plug-and-play style is applied to a series of strengthened trackers including DiMP++, SuperDiMP++ and SuperDIMP_AR++, which achieve consistent performance gain. Finally, experiments conducted on six popular tracking benchmarks show that their trackers outperformed the state-of-the-art trackers with significantly fewer training epochs.},
  archive      = {J_IETCV},
  author       = {Shilei Wang and Yamin Han and Baozhen Sun and Jifeng Ning},
  doi          = {10.1049/cvi2.12235},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {177-189},
  shortjournal = {IET Comput. Vis.},
  title        = {IoUNet++: Spatial cross-layer interaction-based bounding box regression for visual tracking},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STFT: Spatial and temporal feature fusion for transformer
tracker. <em>IETCV</em>, <em>18</em>(1), 165–176. (<a
href="https://doi.org/10.1049/cvi2.12233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese-based trackers have demonstrated robust performance in object tracking, while Transformers have achieved widespread success in object detection. Currently, many researchers use a hybrid structure of convolutional neural networks and Transformers to design the backbone network of trackers, aiming to improve performance. However, this approach often underutilises the global feature extraction capability of Transformers. The authors propose a novel Transformer-based tracker that fuses spatial and temporal features. The tracker consists of a multilayer spatial feature fusion network (MSFFN), a temporal feature fusion network (TFFN), and a prediction head. The MSFFN includes two phases: feature extraction and feature fusion, and both phases are constructed with a Transformer. Compared with the hybrid structure of “CNNs + Transformer,” the proposed method enhances the continuity of feature extraction and the ability of information interaction between features, enabling comprehensive feature extraction. Moreover, to consider the temporal dimension, the authors propose a TFFN for updating the template image. The network utilises the Transformer to fuse the tracking results of multiple frames with the initial frame, allowing the template image to continuously incorporate more information and maintain the accuracy of target features. Extensive experiments show that the tracker STFT achieves state-of-the-art results on multiple benchmarks (OTB100, VOT2018, LaSOT, GOT-10K, and UAV123). Especially, the tracker STFT achieves remarkable area under the curve score of 0.652 and 0.706 on the LaSOT and OTB100 benchmark respectively.},
  archive      = {J_IETCV},
  author       = {Hao Zhang and Yan Piao and Nan Qi},
  doi          = {10.1049/cvi2.12233},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {165-176},
  shortjournal = {IET Comput. Vis.},
  title        = {STFT: Spatial and temporal feature fusion for transformer tracker},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature fusion over hyperbolic graph convolution networks
for video summarisation. <em>IETCV</em>, <em>18</em>(1), 150–164. (<a
href="https://doi.org/10.1049/cvi2.12232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel video summarisation method called the Hyperbolic Graph Convolutional Network (HVSN) is proposed, which addresses the challenges of summarising edited videos and capturing the semantic consistency of video shots at different time points. Unlike existing methods that use linear video sequences as input, HVSN leverages Hyperbolic Graph Convolutional Networks (HGCNs) and an adaptive graph convolutional adjacency matrix network to learn and aggregate features from video shots. Moreover, a feature fusion mechanism based on the attention mechanism is employed to facilitate cross-module feature fusion. To evaluate the performance of the proposed method, experiments are conducted on two benchmark datasets, TVSum and SumMe. The results demonstrate that HVSN achieves state-of-the-art performance, with F1-scores of 62.04% and 50.26% on TVSum and SumMe, respectively. The use of HGCNs enables the model to better capture the complex spatial structures of video shots, and thus contributes to the improved performance of video summarisation.},
  archive      = {J_IETCV},
  author       = {GuangLi Wu and ShengTao Wang and ShiPeng Xu},
  doi          = {10.1049/cvi2.12232},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {150-164},
  shortjournal = {IET Comput. Vis.},
  title        = {Feature fusion over hyperbolic graph convolution networks for video summarisation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Determining the proper number of proposals for individual
images. <em>IETCV</em>, <em>18</em>(1), 141–149. (<a
href="https://doi.org/10.1049/cvi2.12230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The region proposal network is indispensable to two-stage object detection methods. It generates a fixed number of proposals that are to be classified and regressed by detection heads to produce detection boxes. However, the fixed number of proposals may be too large when an image contains only a few objects but too small when it contains much more objects. Considering this, the authors explored determining a proper number of proposals according to the number of objects in an image to reduce the computational cost while improving the detection accuracy. Since the number of ground truth objects is unknown at the inference stage, the authors designed a simple but effective module to predict the number of foreground regions, which will be substituted for the number of objects for determining the proposal number. Experimental results of various two-stage detection methods on different datasets, including MS-COCO, PASCAL VOC, and CrowdHuman showed that equipping the designed module increased the detection accuracy while decreasing the FLOPs of the detection head. For example, experimental results on the PASCAL VOC dataset showed that applying the designed module to Libra R-CNN and Grid R-CNN increased over 1.5 AP 50 while decreasing the FLOPs of detection heads from 28.6 G to nearly 9.0 G.},
  archive      = {J_IETCV},
  author       = {Zihang He and Yong Li},
  doi          = {10.1049/cvi2.12230},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {141-149},
  shortjournal = {IET Comput. Vis.},
  title        = {Determining the proper number of proposals for individual images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-rank preserving embedding regression for robust image
feature extraction. <em>IETCV</em>, <em>18</em>(1), 124–140. (<a
href="https://doi.org/10.1049/cvi2.12228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although low-rank representation (LRR)-based subspace learning has been widely applied for feature extraction in computer vision, how to enhance the discriminability of the low-dimensional features extracted by LRR based subspace learning methods is still a problem that needs to be further investigated. Therefore, this paper proposes a novel low-rank preserving embedding regression (LRPER) method by integrating LRR, linear regression, and projection learning into a unified framework. In LRPER, LRR can reveal the underlying structure information to strengthen the robustness of projection learning. The robust metric L 2,1 -norm is employed to measure the low-rank reconstruction error and regression loss for moulding the noise and occlusions. An embedding regression is proposed to make full use of the prior information for improving the discriminability of the learned projection. In addition, an alternative iteration algorithm is designed to optimise the proposed model, and the computational complexity of the optimisation algorithm is briefly analysed. The convergence of the optimisation algorithm is theoretically and numerically studied. At last, extensive experiments on four types of image datasets are carried out to demonstrate the effectiveness of LRPER, and the experimental results demonstrate that LRPER performs better than some state-of-the-art feature extraction methods.},
  archive      = {J_IETCV},
  author       = {Tao Zhang and Chen-Feng Long and Yang-Jun Deng and Wei-Ye Wang and Si-Qiao Tan and Heng-Chao Li},
  doi          = {10.1049/cvi2.12228},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {124-140},
  shortjournal = {IET Comput. Vis.},
  title        = {Low-rank preserving embedding regression for robust image feature extraction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual privacy behaviour recognition for social robots based
on an improved generative adversarial network. <em>IETCV</em>,
<em>18</em>(1), 110–123. (<a
href="https://doi.org/10.1049/cvi2.12231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although social robots equipped with visual devices may leak user information, countermeasures for ensuring privacy are not readily available, making visual privacy protection problematic. In this article, a semi-supervised learning algorithm is proposed for visual privacy behaviour recognition based on an improved generative adversarial network for social robots; it is called PBR-GAN. A 9-layer residual generator network enhances the data quality, and a 10-layer discriminator network strengthens the feature extraction. A tailored objective function, loss function, and strategy are proposed to dynamically adjust the learning rate to guarantee high performance. A social robot platform and architecture for visual privacy recognition and protection are implemented. The recognition accuracy of the proposed PBR-GAN is compared with Inception_v3, SS-GAN, and SF-GAN. The average recognition accuracy of the proposed PBR-GAN is 85.91%, which is improved by 3.93%, 9.91%, and 1.73% compared with the performance of Inception_v3, SS-GAN, and SF-GAN respectively. Through a case study, seven situations are considered related to privacy at home, and develop training and test datasets with 8,720 and 1,280 images, respectively, are developed. The proposed PBR-GAN recognises the designed visual privacy information with an average accuracy of 89.91%.},
  archive      = {J_IETCV},
  author       = {Guanci Yang and Jiacheng Lin and Zhidong Su and Yang Li},
  doi          = {10.1049/cvi2.12231},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {110-123},
  shortjournal = {IET Comput. Vis.},
  title        = {Visual privacy behaviour recognition for social robots based on an improved generative adversarial network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving object detection by enhancing the effect of
localisation quality evaluation on detection confidence. <em>IETCV</em>,
<em>18</em>(1), 97–109. (<a
href="https://doi.org/10.1049/cvi2.12227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The one-stage object detector has been widely applied in many computer vision applications due to its high detection efficiency and simple framework. However, one-stage detectors heavily rely on Non-maximum Suppression to remove the duplicated predictions for the same objects, and the detectors produce detection confidence to measure the quality of those predictions. The localisation quality is an important factor to evaluate the predicted bounding boxes, but its role has not been fully utilised in previous works. To alleviate the problem, the Quality Prediction Block (QPB), a lightweight sub-network, is designed by the authors, which strengthens the effect of localisation quality evaluation on detection confidence by leveraging the features of predicted bounding boxes. The QPB is simple in structure and applies to different forms of detection confidence. Extensive experiments are conducted on the public benchmarks, MS COCO, PASCAL VOC and Berkeley DeepDrive. The results demonstrate the effectiveness of our method in the detectors with various forms of detection confidence. The proposed approach also achieves better performance in the stronger one-stage detectors.},
  archive      = {J_IETCV},
  author       = {Zuyi Wang and Wei Zhao and Li Xu},
  doi          = {10.1049/cvi2.12227},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {97-109},
  shortjournal = {IET Comput. Vis.},
  title        = {Improving object detection by enhancing the effect of localisation quality evaluation on detection confidence},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved triplet loss for domain adaptation. <em>IETCV</em>,
<em>18</em>(1), 84–96. (<a
href="https://doi.org/10.1049/cvi2.12226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A technique known as domain adaptation is utilised to address classification challenges in an unlabelled target domain by leveraging labelled source domains. Previous domain adaptation approaches have predominantly focussed on global domain adaptation, neglecting class-level information and resulting in suboptimal transfer performance. In recent years, a considerable number of researchers have explored class-level domain adaptation, aiming to precisely align the distribution of diverse domains. Nevertheless, existing research on class-level alignment tends to align domain features either on or in proximity to classification boundaries, which introduces ambiguous samples that can impact classification accuracy. In this study, the authors propose a novel strategy called class guided constraints (CGC) to tackle this issue. Specifically, CGC is employed to preserve the compactness within classes and separability between classes of domain features prior to class-level alignment. Furthermore, the authors incorporate CGC in conjunction with similarity guided constraint. Comprehensive evaluations conducted on four public datasets demonstrate that our approach outperforms numerous state-of-the-art domain adaptation methods significantly and achieves greater improvements compared to the baseline approach.},
  archive      = {J_IETCV},
  author       = {Xiaoshun Wang and Yunhan Li and Xiangliang Zhang},
  doi          = {10.1049/cvi2.12226},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {84-96},
  shortjournal = {IET Comput. Vis.},
  title        = {Improved triplet loss for domain adaptation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lite-weight semantic segmentation with AG self-attention.
<em>IETCV</em>, <em>18</em>(1), 72–83. (<a
href="https://doi.org/10.1049/cvi2.12225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the large computational and GPUs memory cost of semantic segmentation, some works focus on designing a lite weight model to achieve a good trade-off between computational cost and accuracy. A common method is to combined CNN and vision transformer. However, these methods ignore the contextual information of multi receptive fields. And existing methods often fail to inject detailed information losses in the downsampling of multi-scale feature. To fix these issues, we propose AG Self-Attention, which is Enhanced Atrous Self-Attention (EASA), and Gate Attention. AG Self-Attention adds the contextual information of multi receptive fields into the global semantic feature. Specifically, the Enhanced Atrous Self-Attention uses weight shared atrous convolution with different atrous rates to get the contextual information under the specific different receptive fields. Gate Attention introduces gating mechanism to inject detailed information into the global semantic feature and filter detailed information by producing “fusion” gate and “update” gate. In order to prove our insight. We conduct numerous experiments in common semantic segmentation datasets, consisting of ADE20 K, COCO-stuff, PASCAL Context, Cityscapes, to show that our method achieves state-of-the-art performance and achieve a good trade-off between computational cost and accuracy.},
  archive      = {J_IETCV},
  author       = {Bing Liu and Yansheng Gao and Hai Li and Zhaohao Zhong and Hongwei Zhao},
  doi          = {10.1049/cvi2.12225},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {72-83},
  shortjournal = {IET Comput. Vis.},
  title        = {Lite-weight semantic segmentation with AG self-attention},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing human parsing with region-level learning.
<em>IETCV</em>, <em>18</em>(1), 60–71. (<a
href="https://doi.org/10.1049/cvi2.12222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human parsing is very important in a diverse range of industrial applications. Despite the considerable progress that has been achieved, the performance of existing methods is still less than satisfactory, since these methods learn the shared features of various parsing labels at the image level. This limits the representativeness of the learnt features, especially when the distribution of parsing labels is imbalanced or the scale of different labels is substantially different. To address this limitation, a Region-level Parsing Refiner (RPR) is proposed to enhance parsing performance by the introduction of region-level parsing learning. Region-level parsing focuses specifically on small regions of the body, for example, the head. The proposed RPR is an adaptive module that can be integrated with different existing human parsing models to improve their performance. Extensive experiments are conducted on two benchmark datasets, and the results demonstrated the effectiveness of our RPR model in terms of improving the overall parsing performance as well as parsing rare labels. This method was successfully applied to a commercial application for the extraction of human body measurements and has been used in various online shopping platforms for clothing size recommendations. The code and dataset are released at this link https://github.com/applezhouyp/PRP .},
  archive      = {J_IETCV},
  author       = {Yanghong Zhou and P. Y. Mok},
  doi          = {10.1049/cvi2.12222},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {60-71},
  shortjournal = {IET Comput. Vis.},
  title        = {Enhancing human parsing with region-level learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust object tracking via ensembling semantic-aware network
and redetection. <em>IETCV</em>, <em>18</em>(1), 46–59. (<a
href="https://doi.org/10.1049/cvi2.12219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most Siamese-based trackers use classification and regression to determine the target bounding box, which can be formulated as a linear matching process of the template and search region. However, this only takes into account the similarity of features while ignoring the semantic object information, resulting in some cases in which the regression box with the highest classification score is not accurate. To address the lack of semantic information, an object tracking approach based on an ensemble semantic-aware network and redetection (ESART) is proposed. Furthermore, a DarkNet53 network with transfer learning is used as our semantic-aware model to adapt the detection task for extracting semantic information. In addition, a semantic tag redetection method to re-evaluate the bounding box and overcome inaccurate scaling issues is proposed. Extensive experiments based on OTB2015, UAV123, UAV20L, and GOT-10k show that our tracker is superior to other state-of-the-art trackers. It is noteworthy that our semantic-aware ensemble method can be embedded into any tracker for classification and regression task.},
  archive      = {J_IETCV},
  author       = {Peiqiang Liu and Qifeng Liang and Zhiyong An and Jingyi Fu and Yanyan Mao},
  doi          = {10.1049/cvi2.12219},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {46-59},
  shortjournal = {IET Comput. Vis.},
  title        = {Robust object tracking via ensembling semantic-aware network and redetection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic facial expression recognition with pseudo-label
guided multi-modal pre-training. <em>IETCV</em>, <em>18</em>(1), 33–45.
(<a href="https://doi.org/10.1049/cvi2.12217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the huge cost of manual annotations, the labelled data may not be sufficient to train a dynamic facial expression (DFR) recogniser with good performance. To address this, the authors propose a multi-modal pre-training method with a pseudo-label guidance mechanism to make full use of unlabelled video data for learning informative representations of facial expressions. First, the authors build a pre-training dataset of videos with aligned vision and audio modals. Second, the vision and audio feature encoders are trained through an instance discrimination strategy and a cross-modal alignment strategy on the pre-training data. Third, the vision feature encoder is extended as a dynamic expression recogniser and is fine-tuned on the labelled training data. Fourth, the fine-tuned expression recogniser is adopted to predict pseudo-labels for the pre-training data, and then start a new pre-training phase with the guidance of pseudo-labels to alleviate the long-tail distribution problem and the instance-class confliction. Fifth, since the representations learnt with the guidance of pseudo-labels are more informative, a new fine-tuning phase is added to further boost the generalisation performance on the DFR recognition task. Experimental results on the Dynamic Facial Expression in the Wild dataset demonstrate the superiority of the proposed method.},
  archive      = {J_IETCV},
  author       = {Bing Yin and Shi Yin and Cong Liu and Yanyong Zhang and Changfeng Xi and Baocai Yin and Zhenhua Ling},
  doi          = {10.1049/cvi2.12217},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {33-45},
  shortjournal = {IET Comput. Vis.},
  title        = {Dynamic facial expression recognition with pseudo-label guided multi-modal pre-training},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mirror complementary transformer network for RGB-thermal
salient object detection. <em>IETCV</em>, <em>18</em>(1), 15–32. (<a
href="https://doi.org/10.1049/cvi2.12221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional RGB-T salient object detection treats RGB and thermal modalities equally to locate the common salient regions. However, the authors observed that the rich colour and texture information of the RGB modality makes the objects more prominent compared to the background; and the thermal modality records the temperature difference of the scene, so the objects usually contain clear and continuous edge information. In this work, a novel mirror-complementary Transformer network (MCNet) is proposed for RGB-T SOD, which supervise the two modalities separately with a complementary set of saliency labels under a symmetrical structure. Moreover, the attention-based feature interaction and serial multiscale dilated convolution (SDC)-based feature fusion modules are introduced to make the two modalities complement and adjust each other flexibly. When one modality fails, the proposed model can still accurately segment the salient regions. To demonstrate the robustness of the proposed model under challenging scenes in real world, the authors build a novel RGB-T SOD dataset VT723 based on a large public semantic segmentation RGB-T dataset used in the autonomous driving domain. Extensive experiments on benchmark and VT723 datasets show that the proposed method outperforms state-of-the-art approaches, including CNN-based and Transformer-based methods. The code and dataset can be found at https://github.com/jxr326/SwinMCNet .},
  archive      = {J_IETCV},
  author       = {Xiurong Jiang and Yifan Hou and Hui Tian and Lin Zhu},
  doi          = {10.1049/cvi2.12221},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {15-32},
  shortjournal = {IET Comput. Vis.},
  title        = {Mirror complementary transformer network for RGB-thermal salient object detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAGAN: Classifier-augmented generative adversarial networks
for weakly-supervised COVID-19 lung lesion localisation. <em>IETCV</em>,
<em>18</em>(1), 1–14. (<a
href="https://doi.org/10.1049/cvi2.12216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Coronavirus Disease 2019 (COVID-19) epidemic has constituted a Public Health Emergency of International Concern. Chest computed tomography (CT) can help early reveal abnormalities indicative of lung disease. Thus, accurate and automatic localisation of lung lesions is particularly important to assist physicians in rapid diagnosis of COVID-19 patients. The authors propose a classifier-augmented generative adversarial network framework for weakly supervised COVID-19 lung lesion localisation. It consists of an abnormality map generator, discriminator and classifier. The generator aims to produce the abnormality feature map M to locate lesion regions and then constructs images of the pseudo-healthy subjects by adding M to the input patient images. Besides constraining the generated images of healthy subjects with real distribution by the discriminator, a pre-trained classifier is introduced to enhance the generated images of healthy subjects to possess similar feature representations with real healthy people in terms of high-level semantic features. Moreover, an attention gate is employed in the generator to reduce the noise effect in the irrelevant regions of M . Experimental results on the COVID-19 CT dataset show that the method is effective in capturing more lesion areas and generating less noise in unrelated areas, and it has significant advantages in terms of quantitative and qualitative results over existing methods.},
  archive      = {J_IETCV},
  author       = {Xiaojie Li and Xin Fei and Zhe Yan and Hongping Ren and Canghong Shi and Xian Zhang and Imran Mumtaz and Yong Luo and Xi Wu},
  doi          = {10.1049/cvi2.12216},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {IET Comput. Vis.},
  title        = {CAGAN: Classifier-augmented generative adversarial networks for weakly-supervised COVID-19 lung lesion localisation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
