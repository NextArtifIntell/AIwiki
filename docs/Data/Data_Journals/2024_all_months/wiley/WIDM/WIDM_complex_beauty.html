<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>WIDM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="widm---47">WIDM - 47</h2>
<ul>
<li><details>
<summary>
(2024). Reflecting on a decade of evolution: MapReduce-based
advances in partitioning-based, hierarchical-based, and density-based
clustering (2013–2023). <em>WIDM</em>, <em>14</em>(6), e1566. (<a
href="https://doi.org/10.1002/widm.1566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional clustering algorithms are not appropriate for large real-world datasets or big data, which is attributable to computational expensiveness and scalability issues. As a solution, the last decade&#39;s research headed towards distributed clustering using the MapReduce framework. This study conducts a bibliometric review to assess, establish, and measure the patterns and trends of the MapReduce-based partitioning, hierarchical, and density clustering algorithms over the past decade (2013–2023). A digital text-mining-based comprehensive search technique with multiple field-specific keywords, inclusion measures, and exclusion criteria is employed to obtain the research landscape from the Scopus database. The Scopus-obtained data is analyzed using the VOSViewer software tool and coded using the R statistical analysis tool. The analysis identifies the numbers of scholarly articles, diversities of article sources, their impact and growth patterns, details of most influential authors and co-authors, most cited articles, most contributing affiliations and countries, and their collaborations, use of different keywords and their impact, and so forth. The study further explores the articles and reports the methodologies employed for designing MapReduce-based counterparts of traditional partitioning, hierarchical, and density clustering algorithms and their optimizations and hybridizations. Finally, the study lists the main research challenges encountered in the past decade for MapReduce-based partitioning, hierarchical, and density clustering. It suggests possible areas for future research to contribute further in this field.},
  archive      = {J_WIDM},
  author       = {Tanvir Habib Sardar},
  doi          = {10.1002/widm.1566},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1566},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Reflecting on a decade of evolution: MapReduce-based advances in partitioning-based, hierarchical-based, and density-based clustering (2013–2023)},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A conceptual framework for human-centric and semantics-based
explainable event detection. <em>WIDM</em>, <em>14</em>(6), e1565. (<a
href="https://doi.org/10.1002/widm.1565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainability in the field of event detection is a new emerging research area. For practitioners and users alike, explainability is essential to ensuring that models are widely adopted and trusted. Several research efforts have focused on the efficacy and efficiency of event detection. However, a human-centric explanation approach to existing event detection solutions is still lacking. This paper presents an overview of a conceptual framework for human-centric semantic-based explainable event detection with the acronym HUSEED. The framework considered the affordances of XAI and semantics technologies for human-comprehensible explanations of events to facilitate 5W1H explanations (Who did what, when, where, why, and how). Providing this kind of explanation will lead to trustworthy, unambiguous, and transparent event detection models with a higher possibility of uptake by users in various domains of application. We illustrated the applicability of the proposed framework by using two use cases involving first story detection and fake news detection.},
  archive      = {J_WIDM},
  author       = {Taiwo Kolajo and Olawande Daramola},
  doi          = {10.1002/widm.1565},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1565},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A conceptual framework for human-centric and semantics-based explainable event detection},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal emotion recognition: A comprehensive review,
trends, and challenges. <em>WIDM</em>, <em>14</em>(6), e1563. (<a
href="https://doi.org/10.1002/widm.1563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic emotion recognition is a burgeoning field of research and has its roots in psychology and cognitive science. This article comprehensively reviews multimodal emotion recognition, covering various aspects such as emotion theories, discrete and dimensional models, emotional response systems, datasets, and current trends. This article reviewed 179 multimodal emotion recognition literature papers from 2017 to 2023 to reflect on the current trends in multimodal affective computing. This article covers various modalities used in emotion recognition based on the emotional response system under four categories: subjective experience comprising text and self-report; peripheral physiology comprising electrodermal, cardiovascular, facial muscle, and respiration activity; central physiology comprising EEG, neuroimaging, and EOG; behavior comprising facial, vocal, whole-body behavior, and observer ratings. This review summarizes the measures and behavior of each modality under various emotional states. This article provides an extensive list of multimodal datasets and their unique characteristics. The recent advances in multimodal emotion recognition are grouped based on the research focus areas such as emotion elicitation strategy, data collection and handling, the impact of culture and modality on multimodal emotion recognition systems, feature extraction, feature selection, alignment of signals across the modalities, and fusion strategies. The recent multimodal fusion strategies are detailed in this article, as extracting shared representations of different modalities, removing redundant features from different modalities, and learning critical features from each modality are crucial for multimodal emotion recognition. This article summarizes the strengths and weaknesses of multimodal emotion recognition based on the review outcome, along with challenges and future work in multimodal emotion recognition. This article aims to serve as a lucid introduction, covering all aspects of multimodal emotion recognition for novices. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Manju Priya Arthanarisamy Ramaswamy and Suja Palaniswamy},
  doi          = {10.1002/widm.1563},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1563},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Multimodal emotion recognition: A comprehensive review, trends, and challenges},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An overview of current developments and methods for
identifying diabetic foot ulcers: A survey. <em>WIDM</em>,
<em>14</em>(6), e1562. (<a
href="https://doi.org/10.1002/widm.1562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic foot ulcers (DFUs) present a substantial health risk across diverse age groups, creating challenges for healthcare professionals in the accurate classification and grading. DFU plays a crucial role in automated health monitoring and diagnosis systems, where the integration of medical imaging, computer vision, statistical analysis, and gait information is essential for comprehensive understanding and effective management. Diagnosing DFU is imperative, as it plays a major role in the processes of diagnosis, treatment planning, and neuropathy research within automated health monitoring and diagnosis systems. To address this, various machine learning and deep learning-based methodologies have emerged in the literature to support healthcare practitioners in achieving improved diagnostic analyses for DFU. This survey paper investigates various diagnostic methodologies for DFU, spanning traditional statistical approaches to cutting-edge deep learning techniques. It systematically reviews key stages involved in diabetic foot ulcer classification (DFUC) methods, including preprocessing, feature extraction, and classification, explaining their benefits and drawbacks. The investigation extends to exploring state-of-the-art convolutional neural network models tailored for DFUC, involving extensive experiments with data augmentation and transfer learning methods. The overview also outlines datasets commonly employed for evaluating DFUC methodologies. Recognizing that neuropathy and reduced blood flow in the lower limbs might be caused by atherosclerotic blood vessels, this paper provides recommendations to researchers and practitioners involved in routine medical therapy to prevent substantial complications. Apart from reviewing prior literature, this survey aims to influence the future of DFU diagnostics by outlining prospective research directions, particularly in the domains of personalized and intelligent healthcare. Finally, this overview is to contribute to the continual evolution of DFU diagnosis in order to provide more effective and customized medical care. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {L. Jani Anbarasi and Malathy Jawahar and R. Beulah Jayakumari and Modigari Narendra and Vinayakumar Ravi and R. Neeraja},
  doi          = {10.1002/widm.1562},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1562},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {An overview of current developments and methods for identifying diabetic foot ulcers: A survey},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lead–lag effect of research between conference papers and
journal papers in data mining. <em>WIDM</em>, <em>14</em>(6), e1561. (<a
href="https://doi.org/10.1002/widm.1561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The examination of the lead–lag effect between different publication types, incorporating a temporal dimension, is very significant for assessing research. In this article, we introduce a novel framework to quantify the lead–lag effect between the research topics of conference papers and journal papers. We first identify research topics via the text-embedding-based topic modeling technique BERTopic, then extract the research topics of each time slice, construct and visualize the similarity matrix of topics to reveal the time-lag direction and finally quantify the lead–lag effect by four proposed indicators, as well as by average influence topic similarity comparison maps. We conduct a detailed analysis of 19,166 bibliographic data for top conference papers and journal papers from 2015 to 2019 in the data mining field, calculate the similarity of topics obtained by BERTopic between each time slice divided by quarters. The results show that journal paper topics lag behind conference paper topics in the data mining field. The most significant lead–lag effect is 2.5 years, with approximately 33.45% of topics affected by this lag. The methodology presented here holds potential for broader application in the analysis of lead–lag effects across diverse research areas, offering valuable insights into the state of research development and informing policy decisions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Yue Huang and Runyu Tian},
  doi          = {10.1002/widm.1561},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1561},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Lead–lag effect of research between conference papers and journal papers in data mining},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence in assessing cardiovascular diseases
and risk factors via retinal fundus images: A review of the last decade.
<em>WIDM</em>, <em>14</em>(6), e1560. (<a
href="https://doi.org/10.1002/widm.1560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (CVDs) are the leading cause of death globally. The use of artificial intelligence (AI) methods—in particular, deep learning (DL)—has been on the rise lately for the analysis of different CVD-related topics. The use of fundus images and optical coherence tomography angiography (OCTA) in the diagnosis of retinal diseases has also been extensively studied. To better understand heart function and anticipate changes based on microvascular characteristics and function, researchers are currently exploring the integration of AI with noninvasive retinal scanning. There is great potential to reduce the number of cardiovascular events and the financial strain on healthcare systems by utilizing AI-assisted early detection and prediction of cardiovascular diseases on a large scale. A comprehensive search was conducted across various databases, including PubMed, Medline, Google Scholar, Scopus, Web of Sciences, IEEE Xplore, and ACM Digital Library, using specific keywords related to cardiovascular diseases and AI. The study included 87 English-language publications selected for relevance, and additional references were considered. This article provides an overview of the recent developments and difficulties in using AI and retinal imaging to diagnose cardiovascular diseases. It provides insights for further exploration in this field. Researchers are trying to develop precise disease prognosis patterns in response to the aging population and the growing global burden of CVD. AI and DL are revolutionizing healthcare by potentially diagnosing multiple CVDs from a single retinal image. However, swifter adoption of these technologies in healthcare systems is required. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mirsaeed Abdollahi and Ali Jafarizadeh and Amirhosein Ghafouri-Asbagh and Navid Sobhi and Keysan Pourmoghtader and Siamak Pedrammehr and Houshyar Asadi and Ru-San Tan and Roohallah Alizadehsani and U. Rajendra Acharya},
  doi          = {10.1002/widm.1560},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1560},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Artificial intelligence in assessing cardiovascular diseases and risk factors via retinal fundus images: A review of the last decade},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital twins in healthcare: Applications, technologies,
simulations, and future trends. <em>WIDM</em>, <em>14</em>(6), e1559.
(<a href="https://doi.org/10.1002/widm.1559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The healthcare industry has witnessed significant interest in applying DTs (DTs), due to technological advancements. DTs are virtual replicas of physical entities that adapt to real-time data, enabling predictions of their physical counterparts. DT technology enhances understanding of disease occurrence, enabling more accurate diagnoses and treatments. Integrating emerging technologies like big data, cloud computing, Virtual Reality (VR), and internet-of-things (IoT) provides a solid foundation for DT implementation in healthcare. However, defining DTs within the healthcare context still has become increasingly challenging. Therefore, exploring the potential of DTs in healthcare contributes to research, emphasizing their transformative impact on personalized medicine and precision healthcare. In this study, we present diverse healthcare applications of DTs, including healthcare 4.0, cardiac analysis, monitoring and management, data privacy, socio-ethical, and surgical. Moreover, this paper discusses the software and simulations of DTs that can be used in these applications of healthcare, as well as, the future trends of DTs in healthcare. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mohamed Abd Elaziz and Mohammed A. A. Al-qaness and Abdelghani Dahou and Mohammed Azmi Al-Betar and Mona Mostafa Mohamed and Mohamed El-Shinawi and Amjad Ali and Ahmed A. Ewees},
  doi          = {10.1002/widm.1559},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1559},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Digital twins in healthcare: Applications, technologies, simulations, and future trends},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual learning and its industrial applications: A
selective review. <em>WIDM</em>, <em>14</em>(6), e1558. (<a
href="https://doi.org/10.1002/widm.1558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many industrial applications, datasets are often obtained in a sequence associated with a series of similar but different tasks. To model these datasets, a machine-learning algorithm, which performed well on the previous task, may not have as strong a performance on the current task. When the architecture of the algorithm is trained to adapt to new tasks, often the whole architecture needs to be revised and the old knowledge of modeling can be forgotten. Efforts to make the algorithm work for all the relevant tasks can cost large computational resources and data storage. Continual learning, also called lifelong learning or continual lifelong learning, refers to the concept that these algorithms have the ability to continually learn without forgetting the information obtained from previous task. In this work, we provide a broad view of continual learning techniques and their industrial applications. Our focus will be on reviewing the current methodologies and existing applications, and identifying a gap between the current methodology and the modern industrial needs. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {J. Lian and K. Choi and B. Veeramani and A. Hu and S. Murli and L. Freeman and E. Bowen and X. Deng},
  doi          = {10.1002/widm.1558},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1558},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Continual learning and its industrial applications: A selective review},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The evolution of frailty assessment using inertial
measurement sensor-based gait parameter measurements: A detailed
analysis. <em>WIDM</em>, <em>14</em>(6), e1557. (<a
href="https://doi.org/10.1002/widm.1557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frailty is a significant issue in geriatric health, may cause adverse effects such as falls, delirium, weight loss, or physical decline. Over time, various methods have been developed for measuring frailty, including clinical judgment, the frailty index, the clinical frailty scale, and the comprehensive geriatric assessment. These traditional frailty assessment approaches rely on healthcare professionals, which can lead to inaccuracy and require frequent clinic visits, making it burdensome for elderly patients. This review paper explores the latest trends in frailty assessment by measuring gait parameters using wearable sensors, specifically the inertial measurement unit (IMU). The aim of this study is to provide a comprehensive overview of objective methods for evaluating and quantifying frailty. We focus on the application of machine learning (ML) and deep learning (DL) techniques to IMU gait data, highlighting key aspects of recent publications such as algorithms, sensor types, sample sizes, and performance evaluations. By examining the strengths and challenges of each technique, this review aims to guide future studies on utilizing cost-effective and portable devices integrated with clinical data. This integration can help to propose optimized IMU gait parameters or ML models to detect early-stage frailty. This advances the emerging trend of intelligent, individualized, and efficient healthcare systems for older adults. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Arslan Amjad and Shahzad Qaiser and Monika Błaszczyszyn and Agnieszka Szczęsna},
  doi          = {10.1002/widm.1557},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1557},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The evolution of frailty assessment using inertial measurement sensor-based gait parameter measurements: A detailed analysis},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the convergence of metaverse, blockchain, and AI:
A comprehensive survey of enabling technologies, applications,
challenges, and future directions. <em>WIDM</em>, <em>14</em>(6), e1556.
(<a href="https://doi.org/10.1002/widm.1556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Metaverse, distinguished by its capacity to integrate the physical and digital realms seamlessly, presents a dynamic virtual environment offering diverse opportunities for engagement across innovation, entertainment, socialization, and commercial endeavors. However, the Metaverse is poised for a transformative evolution through the convergence of contemporary technological advancements, including artificial intelligence (AI), Blockchain, Robotics, augmented reality, virtual reality, and mixed reality. This convergence is anticipated to revolutionize the global digital landscape, introducing novel social, economic, and operational paradigms for organizations and communities. To comprehensively elucidate the future potential of this technological fusion and its implications for digital innovation, this research endeavors to undertake a thorough analysis of scholarly discourse and research pertaining to the Metaverse, AI, Blockchain, and associated technologies. This survey delves into various critical facets of the Metaverse ecosystem, encompassing component analysis, exploration of digital currencies, assessment of AI utilization in virtual environments, and examination of Blockchain&#39;s role in enhancing digital content and data security. Leveraging articles retrieved from esteemed digital repositories including ScienceDirect, IEEE Xplore, Springer Nature, Google Scholar, and ACM, published between 2017 and 2023, this study adopts an analytical approach to engage with these materials. Through rigorous examination and discourse, this research aims to provide insights into the emerging trends, challenges, and future directions in the convergence of the Metaverse, Blockchain, and AI. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mueen Uddin and Muath Obaidat and Selvakumar Manickam and Shams Ul Arfeen Laghari and Abdulhalim Dandoush and Hidayat Ullah and Syed Sajid Ullah},
  doi          = {10.1002/widm.1556},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1556},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Exploring the convergence of metaverse, blockchain, and AI: A comprehensive survey of enabling technologies, applications, challenges, and future directions},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A taxonomy of automatic differentiation pitfalls.
<em>WIDM</em>, <em>14</em>(6), e1555. (<a
href="https://doi.org/10.1002/widm.1555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic differentiation is a popular technique for computing derivatives of computer programs. While automatic differentiation has been successfully used in countless engineering, science, and machine learning applications, it can sometimes nevertheless produce surprising results. In this paper, we categorize problematic usages of automatic differentiation, and illustrate each category with examples such as chaos, time-averages, discretizations, fixed-point loops, lookup tables, linear solvers, and probabilistic programs, in the hope that readers may more easily avoid or detect such pitfalls. We also review debugging techniques and their effectiveness in these situations. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Jan Hückelheim and Harshitha Menon and William Moses and Bruce Christianson and Paul Hovland and Laurent Hascoët},
  doi          = {10.1002/widm.1555},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1555},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A taxonomy of automatic differentiation pitfalls},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From 3D point-cloud data to explainable geometric deep
learning: State-of-the-art and future challenges. <em>WIDM</em>,
<em>14</em>(6), e1554. (<a
href="https://doi.org/10.1002/widm.1554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an exciting journey from 3D point-cloud data (PCD) to the state of the art in graph neural networks (GNNs) and their evolution with explainable artificial intelligence (XAI), and 3D geometric priors with the human-in-the-loop. We follow a simple definition of a “digital twin,” as a high-precision, three-dimensional digital representation of a physical object or environment, captured, for example, by Light Detection and Ranging (LiDAR) technology. After a digression into transforming PCD into images, graphs, combinatorial complexes and hypergraphs, we explore recent developments in geometric deep learning (GDL) and provide insight into the application of these network architectures for analyzing and learning from graph-structured data. We emphasize the importance of the explainability of these models and recognize that the ability to interpret and validate the results of complex models is a crucial aspect of their wider adoption. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Anna Saranti and Bastian Pfeifer and Christoph Gollob and Karl Stampfer and Andreas Holzinger},
  doi          = {10.1002/widm.1554},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1554},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {From 3D point-cloud data to explainable geometric deep learning: State-of-the-art and future challenges},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A brief review on quantum computing based drug design.
<em>WIDM</em>, <em>14</em>(6), e1553. (<a
href="https://doi.org/10.1002/widm.1553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design and development of new drug molecules are essential for the survival of human society. New drugs are designed for therapeutic purposes to combat new diseases. Besides treating new diseases, new drug development is also needed to treat pre-existing diseases more effectively and reduce the existing drugs&#39; side effects. The design of drugs involves several steps, from the discovery of the drug molecule to its commercialization in the market. One of the most critical steps in drug design is to find the molecular interactions between the target (infected) molecule and the drug molecule. Several complex chemical equations need to be solved to determine the molecular interactions. In the late 20th Century, the advancement of computational technologies has made the solution of chemical equations relatively easier and faster. Moreover, the design of drug molecules involves multi-criteria optimization. Classical computational methodologies have been used for drug design since the end of the 20th Century. However, nowadays, more advanced computational methodologies are inevitable in designing drugs for new diseases and drugs with fewer side effects. In this context, the quantum computing paradigm has proved beneficial in drug design due to its advanced computational capabilities. This paper presents a state-of-the-art comprehensive review of the quantum computing-based methodologies involved in drug design. A comparative study is made about the different quantum-aided drug design methods, stating each methodology&#39;s merits and demerits. The review work presented in this manuscript will help new researchers assess the present state-of-the-art concept of quantum-based drug design. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Poulami Das and Avishek Ray and Siddhartha Bhattacharyya and Jan Platos and Vaclav Snasel and Leo Mrsic and Tingwen Huang and Ivan Zelinka},
  doi          = {10.1002/widm.1553},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1553},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A brief review on quantum computing based drug design},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medical intelligence for anxiety research: Insights from
genetics, hormones, implant science, and smart devices with future
strategies. <em>WIDM</em>, <em>14</em>(6), e1552. (<a
href="https://doi.org/10.1002/widm.1552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This comprehensive review article embarks on an extensive exploration of anxiety research, navigating a multifaceted landscape that incorporates various disciplines, such as molecular genetics, hormonal influences, implant science, regenerative engineering, and real-time cardiac signal analysis, all while harnessing the transformative potential of medical intelligence [medical + artificial intelligence (AI)]. By addressing fundamental research questions, this study investigated the molecular and hormonal foundations underlying anxiety disorders, shedding light on the intricate interplay of genetic and hormonal factors contributing to the etiology and progression of anxiety. Furthermore, this review delves into the emerging implications of biomaterials, defibrillators, and state-of-the-art devices for anxiety research, elucidating their potential roles in diagnosis, treatment, and patient management. A pivotal contribution of this review is the development and exploration of an AI-driven model for real-time cardiac signal analysis. This innovative approach offers a promising avenue for enhancing the precision and timeliness of anxiety diagnosis and monitoring. Leveraging machine learning and AI techniques enables the accurate classification of persons with anxiety based on real-time cardiac data, thereby ushering in a new era of personalized and data-driven mental health care. Identifying emerging themes and knowledge gaps lays the foundation for future research directions and offers a roadmap for scholars and practitioners to navigate this intricate field. In conclusion, this comprehensive review serves as a vital resource, consolidating diverse perspectives and fostering a deeper understanding of anxiety disorders from biological, engineering, and technological standpoints, ultimately contributing to advancing mental health research and clinical practice. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Faijan Akhtar and Md Belal Bin Heyat and Arshiya Sultana and Saba Parveen and Hafiz Muhammad Zeeshan and Stalin Fathima Merlin and Bairong Shen and Dustin Pomary and Jian Ping Li and Mohamad Sawan},
  doi          = {10.1002/widm.1552},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1552},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Medical intelligence for anxiety research: Insights from genetics, hormones, implant science, and smart devices with future strategies},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review and research recommendations on
artificial intelligence for automated cervical cancer detection.
<em>WIDM</em>, <em>14</em>(6), e1550. (<a
href="https://doi.org/10.1002/widm.1550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis of abnormal cervical cells enhances the chance of prompt treatment for cervical cancer (CrC). Artificial intelligence (AI)-assisted decision support systems for detecting abnormal cervical cells are developed because manual identification needs trained healthcare professionals, and can be difficult, time-consuming, and error-prone. The purpose of this study is to present a comprehensive review of AI technologies used for detecting cervical pre-cancerous lesions and cancer. The review study includes studies where AI was applied to Pap Smear test (cytological test), colposcopy, sociodemographic data and other risk factors, histopathological analyses, magnetic resonance imaging-, computed tomography-, and positron emission tomography-scan-based imaging modalities. We performed searches on Web of Science, Medline, Scopus, and Inspec. The preferred reporting items for systematic reviews and meta-analysis guidelines were used to search, screen, and analyze the articles. The primary search resulted in identifying 9745 articles. We followed strict inclusion and exclusion criteria, which include search windows of the last decade, journal articles, and machine/deep learning-based methods. A total of 58 studies have been included in the review for further analysis after identification, screening, and eligibility evaluation. Our review analysis shows that deep learning models are preferred for imaging techniques, whereas machine learning-based models are preferred for sociodemographic data. The analysis shows that convolutional neural network-based features yielded representative characteristics for detecting pre-cancerous lesions and CrC. The review analysis also highlights the need for generating new and easily accessible diverse datasets to develop versatile models for CrC detection. Our review study shows the need for model explainability and uncertainty quantification to increase the trust of clinicians and stakeholders in the decision-making of automated CrC detection models. Our review suggests that data privacy concerns and adaptability are crucial for deployment hence, federated learning and meta-learning should also be explored. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Smith K. Khare and Victoria Blanes-Vidal and Berit Bargum Booth and Lone Kjeld Petersen and Esmaeil S. Nadimi},
  doi          = {10.1002/widm.1550},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1550},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A systematic review and research recommendations on artificial intelligence for automated cervical cancer detection},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancements in q-learning meta-heuristic optimization
algorithms: A survey. <em>WIDM</em>, <em>14</em>(6), e1548. (<a
href="https://doi.org/10.1002/widm.1548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reviews the integration of Q-learning with meta-heuristic algorithms (QLMA) over the last 20 years, highlighting its success in solving complex optimization problems. We focus on key aspects of QLMA, including parameter adaptation, operator selection, and balancing global exploration with local exploitation. QLMA has become a leading solution in industries like energy, power systems, and engineering, addressing a range of mathematical challenges. Looking forward, we suggest further exploration of meta-heuristic integration, transfer learning strategies, and techniques to reduce state space. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Yang Yang and Yuchao Gao and Zhe Ding and Jinran Wu and Shaotong Zhang and Feifei Han and Xuelan Qiu and Shangce Gao and You-Gan Wang},
  doi          = {10.1002/widm.1548},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1548},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Advancements in Q-learning meta-heuristic optimization algorithms: A survey},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning for pest detection and infestation
prediction: A comprehensive review. <em>WIDM</em>, <em>14</em>(5),
e1551. (<a href="https://doi.org/10.1002/widm.1551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pests pose a major danger to a variety of industries, including agriculture, public health, and ecosystems. Fast and precise pest detection, as well as the ability to predict infestations, are required for effective pest management tactics. This paper provides a comprehensive literature review on this subject to provide an overview of the state of research on pest detection and infestation prediction. The paper investigates and presents background information on the necessity of pest control as well as the difficulty in recognizing pests and forecasting. Several strategies, including approaches to data collection, modeling, and assessment of models, are reviewed in the research described. The authors examine various pest detection methods involving the utilization of convolutional neural networks and several object detection architectures categorized broadly into one-stage and two-stage object detection algorithms. Methods for predicting pest infestations that involve regression, classification, and time series forecasting are also thoroughly investigated. The challenges of recognizing pests and predicting infestations are underlined, as are issues with data quality, feature selection, and model interpretability. The report also indicates the limitations to pest detection and infestation prediction as well as intriguing topics for further research on the same. The findings of the literature research demonstrate how Artificial Intelligence, Computer Vision, and the Internet of Things have been applied for Pest Detection and Infestation Prediction. The research serves as a base for surveying and summarizing the approaches utilized for the task of pest detection (an object detection problem) and pest infestation prediction (a forecasting problem) and its findings and recommendations serve as a platform for future study and the development of effective pest management solutions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mamta Mittal and Vedika Gupta and Mohammad Aamash and Tejas Upadhyay},
  doi          = {10.1002/widm.1551},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1551},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Machine learning for pest detection and infestation prediction: A comprehensive review},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning applied to tourism: A systematic review.
<em>WIDM</em>, <em>14</em>(5), e1549. (<a
href="https://doi.org/10.1002/widm.1549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of machine learning techniques in the field of tourism is experiencing a remarkable growth, as they allow to propose efficient solutions to problems present in this sector, by means of an intelligent analysis of data in their specific context. The increase of work in this field requires an exhaustive analysis through a quantitative approach of research activity, contributing to a deeper understanding of the progress of this field. Thus, different approaches in the field of tourism will be analyzed, such as planning, forecasting, recommendation, prevention, and security, among others. As a result of this analysis, among other findings, the greater impact of supervised learning in the field of tourism, and more specifically those techniques based on neural networks, has been confirmed. The results of this study would allow researchers not only to have the most up-to-date and accurate overview of the application of machine learning in tourism, but also to identify the most appropriate techniques to apply to their domain of interest, as well as other similar approaches with which to compare their own solutions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {José Carlos Sancho Núñez and Juan A. Gómez-Pulido and Rafael Robina Ramírez},
  doi          = {10.1002/widm.1549},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1549},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Machine learning applied to tourism: A systematic review},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Onset of a conceptual outline map to get a hold on the
jungle of cluster analysis. <em>WIDM</em>, <em>14</em>(5), e1547. (<a
href="https://doi.org/10.1002/widm.1547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The domain of cluster analysis is a meeting point for a very rich multidisciplinary encounter, with cluster-analytic methods being studied and developed in discrete mathematics, numerical analysis, statistics, data analysis, data science, and computer science (including machine learning, data mining, and knowledge discovery), to name but a few. The other side of the coin, however, is that the domain suffers from a major accessibility problem as well as from the fact that it is rife with division across many pretty isolated islands. As a way out, the present paper offers a thorough and in-depth review of the clustering domain as a whole under the form of an outline map based on an overarching conceptual framework and a common language. With this framework we wish to contribute to structuring the clustering domain, to characterizing methods that have often been developed and studied in quite different contexts, to identifying links between methods, and to introducing a frame of reference for optimally setting up cluster analyses in data-analytic practice. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Iven Van Mechelen and Christian Hennig and Henk A. L. Kiers},
  doi          = {10.1002/widm.1547},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1547},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Onset of a conceptual outline map to get a hold on the jungle of cluster analysis},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoding cognitive health using machine learning: A
comprehensive evaluation for diagnosis of significant memory concern.
<em>WIDM</em>, <em>14</em>(5), e1546. (<a
href="https://doi.org/10.1002/widm.1546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The timely identification of significant memory concern (SMC) is crucial for proactive cognitive health management, especially in an aging population. Detecting SMC early enables timely intervention and personalized care, potentially slowing cognitive disorder progression. This study presents a state-of-the-art review followed by a comprehensive evaluation of machine learning models within the randomized neural networks (RNNs) and hyperplane-based classifiers (HbCs) family to investigate SMC diagnosis thoroughly. Utilizing the Alzheimer&#39;s Disease Neuroimaging Initiative 2 (ADNI2) dataset, 111 individuals with SMC and 111 healthy older adults are analyzed based on T1W magnetic resonance imaging (MRI) scans, extracting rich features. This analysis is based on baseline structural MRI (sMRI) scans, extracting rich features from gray matter (GM), white matter (WM), Jacobian determinant (JD), and cortical thickness (CT) measurements. In RNNs, deep random vector functional link (dRVFL) and ensemble dRVFL (edRVFL) emerge as the best classifiers in terms of performance metrics in the identification of SMC. In HbCs, Kernelized pinball general twin support vector machine (Pin-GTSVM-K) excels in CT and WM features, whereas Linear Pin-GTSVM (Pin-GTSVM-L) and Linear intuitionistic fuzzy TSVM (IFTSVM-L) performs well in the JD and GM features sets, respectively. This comprehensive evaluation emphasizes the critical role of feature selection, feature  based-interpretability and model choice in attaining an effective classifier for SMC diagnosis. The inclusion of statistical analyses further reinforces the credibility of the results, affirming the rigor of this analysis. The performance measures exhibit the suitability of this framework in aiding researchers with the automated and accurate assessment of SMC. The source codes of the algorithms and datasets used in this study are available at https://github.com/mtanveer1/SMC . This article is categorized under:},
  archive      = {J_WIDM},
  author       = {M. Sajid and R. Sharma and I. Beheshti and M. Tanveer},
  doi          = {10.1002/widm.1546},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1546},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Decoding cognitive health using machine learning: A comprehensive evaluation for diagnosis of significant memory concern},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of multidimensional relevance estimation
in information retrieval. <em>WIDM</em>, <em>14</em>(5), e1541. (<a
href="https://doi.org/10.1002/widm.1541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In information retrieval, relevance is perceived as a multidimensional and dynamic concept influenced by user, task, and domain factors. Relying on this perspective, researchers have introduced multidimensional relevance models addressing diverse search tasks across numerous knowledge domains. Through our systematic review of 72 studies, we categorize research based on domain specificity and the distinct relevance aspects employed for estimating multidimensional relevance. Moreover, we highlight the approaches used to aggregate scores related to these factors and rank information items. Our insights underline the importance of concise definitions and unified methods for estimating relevance factors within and across domains. Finally, we identify benchmark collections for evaluations based on multiple relevance aspects while underscoring the necessity for new ones. Our findings suggest that large language models hold considerable promise for shaping future research in this field, mainly due to their relevance labeling abilities. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Georgios Peikos and Gabriella Pasi},
  doi          = {10.1002/widm.1541},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1541},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A systematic review of multidimensional relevance estimation in information retrieval},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A topic modeling-based bibliometric exploration of automatic
summarization research. <em>WIDM</em>, <em>14</em>(5), e1540. (<a
href="https://doi.org/10.1002/widm.1540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in text data has driven extensive research into developing diverse automatic summarization approaches to effectively handle vast textual information. There are several reviews on this topic, yet no large-scale analysis based on quantitative approaches has been conducted. To provide a comprehensive overview of the field, this study conducted a bibliometric analysis of 3108 papers published from 2010 to 2022, focusing on automatic summarization research regarding topics and trends, top sources, countries/regions, institutions, researchers, and scientific collaborations. We have identified the following trends. First, the number of papers has experienced 65% growth, with the majority being published in computer science conferences. Second, Asian countries and institutions, notably China and India, actively engage in this field and demonstrate a strong inclination toward inter-regional international collaboration, contributing to more than 24% and 20% of the output, respectively. Third, researchers show a high level of interest in multihead and attention mechanisms, graph-based semantic analysis, and topic modeling and clustering techniques, with each topic having a prevalence of over 10%. Finally, scholars have been increasingly interested in self-supervised and zero/few-shot learning, multihead and attention mechanisms, and temporal analysis and event detection. This study is valuable when it comes to enhancing scholars&#39; and practitioners&#39; understanding of the current hotspots and future directions in automatic summarization. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Xieling Chen and Haoran Xie and Xiaohui Tao and Lingling Xu and Jingjing Wang and Hong-Ning Dai and Fu Lee Wang},
  doi          = {10.1002/widm.1540},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1540},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A topic modeling-based bibliometric exploration of automatic summarization research},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predictive machine learning in optimizing the performance of
electric vehicle batteries: Techniques, challenges, and solutions.
<em>WIDM</em>, <em>14</em>(5), e1539. (<a
href="https://doi.org/10.1002/widm.1539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research paper explores the importance of optimizing the performance of electric vehicle (EV) batteries to align with the rapid growth in EV usage. It uses predictive machine learning (ML) techniques to achieve this optimization. The paper covers various ML methods like supervised, unsupervised, and deep learning (DL) and ways to measure their effectiveness. Significant battery performance factors, such as state of charge (SoC), state of health (SoH), state of function (SoF), and remaining useful life (RUL), are discussed, along with methods to collect and prepare data for accurate predictions. The paper introduces an operation research model for optimizing the performance of EV Batteries. It also looks at challenges unique to battery systems and ways to overcome them. The study showcases ML models&#39; ability to predict battery behavior for real-time monitoring, efficient energy use, and proactive maintenance. The paper categorizes different applications and case studies, providing valuable insights and forward-looking perspectives for researchers, practitioners, and policymakers involved in improving EV battery performance through predictive ML. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Vankamamidi S. Naresh and Guduru V. N. S. R. Ratnakara Rao and D. V. N. Prabhakar},
  doi          = {10.1002/widm.1539},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1539},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Predictive machine learning in optimizing the performance of electric vehicle batteries: Techniques, challenges, and solutions},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Navigating the metaverse: A technical review of emerging
virtual worlds. <em>WIDM</em>, <em>14</em>(4), e1538. (<a
href="https://doi.org/10.1002/widm.1538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metaverse, a burgeoning virtual reality realm, has garnered substantial attention owing to its multifaceted applications. Rapid advancements and widespread acceptance of metaverse technologies have birthed a dynamic and intricate digital landscape. As various platforms, virtual worlds, and social networks within the metaverse increase, there is a growing imperative for a comprehensive analysis of its implications across societal, technological, and business dimensions. Notably, existing review studies have, for the past decade, primarily overlooked a metaverse-based multidomain approach. A meticulous examination encompassing 207 research studies delves into the technological innovation of the metaverse, elucidating its future trajectory and ethical imperatives. Additionally, the article introduces the term “ MetaWarria ” to conceptualize potential conflicts arising from metaverse dynamics. The study discerns that healthcare (45%) and education (22%) are pivotal sectors steering metaverse developments, while the entertainment sector (9%) reshapes the corporate landscape. Artificial intelligence (AI) plays a 9% role in enhancing the metaverse&#39;s marketing and user experience. Security, privacy, and policy concerns (11%) are addressed due to escalating threats, yielding practical solutions. The analysis underscores the metaverse&#39;s profound influence (57%) on the digital realm, a phenomenon accelerated by the COVID-19 pandemic. The article culminates in contemplating the metaverse&#39;s role in future warfare and national security, introducing “ MetaWarria ” as a conceptual framework for such discussions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {H. M. K. K. M. B. Herath and Mamta Mittal and Aman Kataria},
  doi          = {10.1002/widm.1538},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1538},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Navigating the metaverse: A technical review of emerging virtual worlds},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of reasoning characteristics of RDF-based semantic
web systems. <em>WIDM</em>, <em>14</em>(4), e1537. (<a
href="https://doi.org/10.1002/widm.1537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presented as a research challenge in 2001, the Semantic Web (SW) is now a mature technology, used in several cross-domain applications. One of its key benefits is a formal semantics of its RDF data format, which enables a system to validate data, infer implicit knowledge by automated reasoning, and explain it to a user; yet the analysis presented here of 71 RDF-based SW systems (out of which 17 reasoners) reveals that the exploitation of such semantics varies a lot among all SW applications. Since the simple enumeration of systems, each one with its characteristics, might result in a clueless listing, we borrow from Software Engineering the idea of maturity model, and organize our classification around it. Our model has three orthogonal dimensions: treatment of blank nodes, degree of deductive capabilities, and explanation of results. For each dimension, we define 3–4 levels of increasing exploitation of semantics, corresponding to an increasingly sophisticated output in that dimension. Each system is then classified in each dimension, based on its documentation and published articles. The distribution of systems along each dimension is depicted in the graphical abstract. We deliberately exclude resources consumption (time and space) since it is a dimension not peculiar to SW. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Simona Colucci and Francesco M. Donini and Eugenio Di Sciascio},
  doi          = {10.1002/widm.1537},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1537},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review of reasoning characteristics of RDF-based semantic web systems},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review on detection and adaptation of concept
drift in streaming data using machine learning techniques.
<em>WIDM</em>, <em>14</em>(4), e1536. (<a
href="https://doi.org/10.1002/widm.1536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Last decade demonstrate the massive growth in organizational data which keeps on increasing multi-fold as millions of records get updated every second. Handling such vast and continuous data is challenging which further opens up many research areas. The continuously flowing data from various sources and in real-time is termed as streaming data. While deriving valuable statistics from data streams, the variation that occurs in data distribution is called concept drift. These drifts play a significant role in a variety of disciplines, including data mining, machine learning, ubiquitous knowledge discovery, quantitative decision theory, and so forth. As a result, a substantial amount of research is carried out for studying methodologies and approaches for dealing with drifts. However, the available material is scattered and lacks guidelines for selecting an effective technique for a particular application. The primary novel objective of this survey is to present an understanding of concept drift challenges and allied studies. Further, it assists researchers from diverse domains to accommodate detection and adaptation algorithms for concept drifts in their applications. Overall, this study aims to contribute to deeper insights into the classification of various types of drifts and methods for detection and adaptation along with their key features and limitations. Furthermore, this study also highlights performance metrics used to evaluate the concept drift detection methods for streaming data. This paper presents the future research scope by highlighting gaps in the existing literature for the development of techniques to handle concept drifts. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Shruti Arora and Rinkle Rani and Nitin Saxena},
  doi          = {10.1002/widm.1536},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1536},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A systematic review on detection and adaptation of concept drift in streaming data using machine learning techniques},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Does a language model “understand” high school math? A
survey of deep learning based word problem solvers. <em>WIDM</em>,
<em>14</em>(4), e1534. (<a
href="https://doi.org/10.1002/widm.1534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the latter half of the last decade, there has been a growing interest in developing algorithms for automatically solving mathematical word problems (MWP). It is a challenging and unique task that demands blending surface level text pattern recognition with mathematical reasoning. In spite of extensive research, we still have a lot to explore for building robust representations of elementary math word problems and effective solutions for the general task. In this paper, we critically examine the various models that have been developed for solving word problems, their pros and cons and the challenges ahead. In the last 2 years, a lot of deep learning models have recorded competing results on benchmark datasets, making a critical and conceptual analysis of literature highly useful at this juncture. We take a step back and analyze why, in spite of this abundance in scholarly interest, the predominantly used experiment and dataset designs continue to be a stumbling block. From the vantage point of having analyzed the literature closely, we also endeavor to provide a road-map for future math word problem research. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sowmya S. Sundaram and Sairam Gurajada and Deepak Padmanabhan and Savitha Sam Abraham and Marco Fisichella},
  doi          = {10.1002/widm.1534},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1534},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Does a language model “understand” high school math? a survey of deep learning based word problem solvers},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causality and causal inference for engineers: Beyond
correlation, regression, prediction and artificial intelligence.
<em>WIDM</em>, <em>14</em>(4), e1533. (<a
href="https://doi.org/10.1002/widm.1533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to engineer new materials, structures, systems, and processes that address persistent challenges, engineers seek to tie causes to effects and understand the effects of causes. Such a pursuit requires a causal investigation to uncover the underlying structure of the data generating process (DGP) governing phenomena. A causal approach derives causal models that engineers can adopt to infer the effects of interventions (and explore possible counterfactuals). Yet, and for the most part, we continue to design experiments in the hope of empirically observing engineered intervention(s). Such experiments are idealized, complex, and costly and hence are narrow in scope. On the contrary, a causal investigation will allow us to peek into the how and why of a DGP and provide us with the essential means to articulate a causal model that accurately describes the phenomenon on hand and better predicts the outcome of possible interventions. Adopting a causal approach in engineering is perhaps more warranted than ever—especially with the rise of big data and the adoption of artificial intelligence (AI); wherein AI models are naivety presumed to describe causal ties. To bridge such knowledge gap, this primer presents fundamental principles behind causal discovery, causal inference, and counterfactuals from an engineering perspective and contrasts that to those pertaining to correlation, regression, and AI. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {M. Z. Naser},
  doi          = {10.1002/widm.1533},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1533},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Causality and causal inference for engineers: Beyond correlation, regression, prediction and artificial intelligence},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing privacy concerns with wearable health monitoring
technology. <em>WIDM</em>, <em>14</em>(3), e1535. (<a
href="https://doi.org/10.1002/widm.1535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing popularity of wearable health devices like fitness trackers and smartwatches enables continuous personal health monitoring but also raises significant privacy concerns due to the real-time collection of sensitive data. Many users are unaware of vulnerabilities that could lead to unauthorized access or discrimination if health information is revealed without consent. However, even informed users may willingly share data despite understanding privacy risks. The recent implementation of the General Data Protection Regulation (GDPR) in the EU and states taking initiatives to regulate privacy shows growing regulatory efforts to address these threats. This paper evaluates the key privacy threats posed specifically by consumer wearable devices. It provides a focused analysis of how health data could be exploited or shared without users&#39; knowledge and the security flaws that enable such risks. Potential solutions including improving protections, empowering user control, enhancing transparency, and strengthening regulations are examined. However, it is argued that effective change requires balancing privacy risks with health benefits while also considering human decision-making behaviors. The paper concludes by proposing a multifaceted approach to enable informed choices about wearable health data. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {C. L. V. Sivakumar and Varda Mone and Rakhmanov Abdumukhtor},
  doi          = {10.1002/widm.1535},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1535},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Addressing privacy concerns with wearable health monitoring technology},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to “expression of concern: Wang, c., zhang, q.,
liu, w., liu, y. &amp; miao, l. Facial feature discovery for ethnicity
recognition. WIREs data mining knowl. Discov. 9, e1278 (2019).
Https://doi.org/10.1002/widm.1278.” <em>WIDM</em>, <em>14</em>(3),
e1532. (<a href="https://doi.org/10.1002/widm.1532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_WIDM},
  doi          = {10.1002/widm.1532},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1532},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Correction to “Expression of concern: wang, c., zhang, q., liu, w., liu, y. &amp; miao, l. facial feature discovery for ethnicity recognition. WIREs data mining knowl. discov. 9, e1278 (2019). https://doi.org/10.1002/widm.1278”},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing programming languages for data analytics: Accuracy
of estimation in python and r. <em>WIDM</em>, <em>14</em>(3), e1531. (<a
href="https://doi.org/10.1002/widm.1531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several open-source programming languages, particularly R and Python, are utilized in industry and academia for statistical data analysis, data mining, and machine learning. While most commercial software programs and programming languages provide a single way to deliver a statistical procedure, open-source programming languages have multiple libraries and packages offering many ways to complete the same analysis, often with varying results. Applying the same statistical method across these different libraries and packages can lead to entirely different solutions due to the differences in their implementations. Therefore, reliability and accuracy should be essential considerations when making library and package usage decisions while conducting statistical analysis using open source programming languages. Instead, most users take this for granted, assuming that their chosen libraries and packages produce accurate results for their statistical analysis. To this extent, this study assesses the estimation accuracy and reliability of Python and R&#39;s various libraries and packages by evaluating the univariate summary statistics, analysis of variance (ANOVA), and linear regression procedures using benchmarking data from the National Institutes of Standards and Technology (NIST). Further, experimental results are presented comparing machine learning methods for classification and regression. The libraries and packages assessed in this study include the stats package in R and Pandas, Statistics, NumPy, statsmodels, SciPy, statsmodels, scikit-learn, and pingouin in Python. The results show that the stats package in R and statsmodels library in Python are reliable for univariate summary statistics. In contrast, Python&#39;s scikit-learn library produces the most accurate results and is recommended for ANOVA. Among the libraries and packages assessed for linear regression, the results demonstrated that the stats package in R is more reliable, accurate, and flexible; thus, it is recommended for linear regression analysis. Further, we present results and recommendations for machine learning using R and Python. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Chelsey Hill and Lanqing Du and Marina Johnson and B. D. McCullough},
  doi          = {10.1002/widm.1531},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1531},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Comparing programming languages for data analytics: Accuracy of estimation in python and r},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence for atrial fibrillation detection,
prediction, and treatment: A systematic review of the last decade
(2013–2023). <em>WIDM</em>, <em>14</em>(3), e1530. (<a
href="https://doi.org/10.1002/widm.1530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atrial fibrillation (AF) affects more than 30 million individuals worldwide, making it the most prevalent cardiac arrhythmia on a global scale. This systematic review summarizes recent advancements in applying artificial intelligence (AI) techniques for AF detection, prediction, and guiding treatment selection and risk stratification. In adherence with the PRISMA guidelines (Preferred Reporting Items for Systematic Reviews and Meta-Analyses), a total of 171 pertinent studies conducted between 2013 and 2023 were examined. Studies applying machine learning (ML) and deep learning (DL) to electrocardiogram (ECG), photoplethysmography (PPG), wearable data, and other sources were evaluated. For AF detection, most works employed DL (48 studies) and ML (28 studies) on ECG data. DL methods directly analyzed ECG waveforms and outperformed approaches relying on hand-crafted features. For prediction and risk stratification, 22 studies used ML while 7 leveraged DL on ECG. An emerging trend showed the growing potential of PPG for AF screening. Overall, AI demonstrated promising capabilities across various AF-related tasks. However, real-world implementation faces challenges including a lack of interpretability, the need for multimodal data integration, prospective performance validation, and regulatory compliance. Future research directions involve quantifying model uncertainty, enhancing transparency, and conducting population-based clinical trials to facilitate translation into practice. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Massimo Salvi and Madhav R. Acharya and Silvia Seoni and Oliver Faust and Ru-San Tan and Prabal Datta Barua and Salvador García and Filippo Molinari and U. Rajendra Acharya},
  doi          = {10.1002/widm.1530},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1530},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Artificial intelligence for atrial fibrillation detection, prediction, and treatment: A systematic review of the last decade (2013–2023)},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graph-driven data processing for business
intelligence. <em>WIDM</em>, <em>14</em>(3), e1529. (<a
href="https://doi.org/10.1002/widm.1529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With proliferation of Big Data, organizational decision making has also become more complex. Business Intelligence (BI) is no longer restricted to querying about marketing and sales data only. It is more about linking data from disparate applications and also churning through large volumes of unstructured data like emails, call logs, social media, News, and so on in an attempt to derive insights that can also provide actionable intelligence and better inputs for future strategy making. Semantic technologies like knowledge graphs have proved to be useful tools that help in linking disparate data sources intelligently and also enable reasoning through complex networks that are created as a result of this linking. Over the last decade the process of creation, storage, and maintenance of knowledge graphs have sufficiently matured, and they are now making inroads into business decision making also. Very recently, these graphs are also seen as a potential way to reduce hallucinations of large language models, by including these during pre-training as well as generation of output. There are a number of challenges also. These include building and maintaining the graphs, reasoning with missing links, and so on. While these remain as open research problems, we present in this article a survey of how knowledge graphs are currently used for deriving business intelligence with use-cases from various domains. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Lipika Dey},
  doi          = {10.1002/widm.1529},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1529},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Knowledge graph-driven data processing for business intelligence},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A literature review on satellite image time series
forecasting: Methods and applications for remote sensing. <em>WIDM</em>,
<em>14</em>(3), e1528. (<a
href="https://doi.org/10.1002/widm.1528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite image time-series are time series produced from remote sensing images; they generally correspond to features or indicators extracted from those images. With the increasing availability of remote sensing images and new methodologies to process such data, image time-series methods have been used extensively for assessing temporal pattern detection, monitoring, classification, object detection, and feature estimation. Since the study of time series is broad, this article focuses on analyzing articles related to forecasting the value of one or more attributes of the image time-series. The image time series forecasting (ITSF) problem appears in different disciplines; most focus on improving the quality of life by harnessing natural resources for sustainable development and minimizing the lethality of dangerous natural phenomena. Scientists tackle these problems using different tools or methods depending on the application. This review analyzes the field&#39;s leading, most recent contributions, grouping them by application area and solution methods. Our findings indicate that artificial neural networks, regression trees, support vector regression, and cellular automata are the most common methods for ITSF. Application areas address this problem as renewable energy, agriculture, and land-use change. This study retrieved and analyzed relevant information about the recent activity of image time series forecasting, generating a reproducible list of the most pertinent articles in the field published from 2009 to 2021. To the author&#39;s best knowledge, this is the first review presenting and analyzing a reproducible list of the most relevant state-of-the-art articles focusing on the applications, techniques, and research trends for ITSF. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Carlos Lara-Alvarez and Juan J. Flores and Hector Rodriguez-Rangel and Rodrigo Lopez-Farias},
  doi          = {10.1002/widm.1528},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1528},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A literature review on satellite image time series forecasting: Methods and applications for remote sensing},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of autonomous monitoring systems in mental health.
<em>WIDM</em>, <em>14</em>(3), e1527. (<a
href="https://doi.org/10.1002/widm.1527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smartphones and personal sensing technologies have made collecting data continuously and in real time feasible. The promise of pervasive sensing technologies in the realm of mental health has recently garnered increased attention. Using Artificial Intelligence methods, it is possible to forecast a person&#39;s emotional state based on contextual information such as their current location, movement patterns, and so on. As a result, conditions like anxiety, stress, depression, and others might be tracked automatically and in real-time. The objective of this research was to survey the state-of-the-art autonomous psychological health monitoring (APHM) approaches, including those that make use of sensor data, virtual chatbot communication, and artificial intelligence methods like Machine learning and deep learning algorithms. We discussed the main processing phases of APHM from the sensing layer to the application layer and an observation taxonomy deals with various observation devices, observation duration, and phenomena related to APHM. Our goal in this study includes research works pertaining to working of APHM to predict the various mental disorders and difficulties encountered by researchers working in this sector and potential application for future clinical use highlighted. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Abinaya Gopalakrishnan and Raj Gururajan and Xujuan Zhou and Revathi Venkataraman and Ka Ching Chan and Niall Higgins},
  doi          = {10.1002/widm.1527},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1527},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey of autonomous monitoring systems in mental health},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The role of lifelong machine learning in bridging the gap
between human and machine learning: A scientometric analysis.
<em>WIDM</em>, <em>14</em>(2), e1526. (<a
href="https://doi.org/10.1002/widm.1526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to advancements in data collection, storage, and processing techniques, machine learning has become a thriving and dominant paradigm. However, one of its main shortcomings is that the classical machine learning paradigm acts in isolation without utilizing the knowledge gained through learning from related tasks in the past. To circumvent this, the concept of Lifelong Machine Learning (LML) has been proposed, with the goal of mimicking how humans learn and acquire cognition. Human learning research has revealed that the brain connects previously learned information while learning new information from a single or small number of examples. Similarly, an LML system continually learns by storing and applying acquired information. Starting with an analysis of how the human brain learns, this paper shows that the LML framework shares a functional structure with the brain when it comes to solving new problems using previously learned information. It also provides a description of the LML framework, emphasizing its similarities to human brain learning. It also provides citation graph generation and scientometric analysis algorithms for the LML literatures, including information about the datasets and evaluation metrics that have been used in the empirical evaluation of LML systems. Finally, it presents outstanding issues and possible future research directions in the field of LML. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Muhammad Abulaish and Nesar Ahmad Wasi and Shachi Sharma},
  doi          = {10.1002/widm.1526},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1526},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The role of lifelong machine learning in bridging the gap between human and machine learning: A scientometric analysis},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Establishment and implementation of a clinical application
and research database for surgical robots in china. <em>WIDM</em>,
<em>14</em>(2), e1525. (<a
href="https://doi.org/10.1002/widm.1525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots are widely used in surgeries worldwide. To improve robotic surgical treatments, we established a database of clinical applications and research on surgical robots. Robotics-related literature included in the China national knowledge infrastructure (CNKI), Wanfang, Vipshop, Chinese science citation database (CSCD), Web of Science, EMBASE, PubMed, and Cochrane databases up to September 2022 was searched and entered into the database. Information on all patients who had undergone robotic surgery at our hospital since 2016 was also included. Literature and case information was classified and evaluated according to standard guidelines and statements. The Gansu Provincial Hospital was the first to use evidence-based medical research methods to successfully establish a database of clinical applications and research on surgical robots. This database comprised literature search, upload, quality evaluation, risk of bias assessment, case information entry, and access functions. Based on the database, we conducted related studies in general surgery, gynecology, urology, thoracic surgery, and cardiovascular surgery, and published 16 meta-analyses and 15 clinical studies. Establishing this database removes language, retrieval, and evaluation barriers in clinicians&#39; use of clinical evidence and lays the foundation for the efficient and accurate use of clinical data for future research. Further, it facilitates evidence-based evaluation of the effectiveness of robotic therapy, which may guide future medical practice. This typical case of interdisciplinary research aims to build a platform to disseminate knowledge and technology in robotic surgery. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Shixun Ma and Li Li and Dengfeng Lv and Xiaojun Yang and He Su and Leisheng Zhang and Min Zhang and Yuntao Ma and Tiankang Guo and Hui Cai},
  doi          = {10.1002/widm.1525},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1525},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Establishment and implementation of a clinical application and research database for surgical robots in china},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of episode mining. <em>WIDM</em>, <em>14</em>(2),
e1524. (<a href="https://doi.org/10.1002/widm.1524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Episode mining is a research area in data mining, where the aim is to discover interesting episodes, that is, subsequences of events, in an event sequence. The most popular episode-mining task is frequent episode mining (FEM), which consists of identifying episodes that appear frequently in an event sequence, but this task has also been extended in various ways. It was shown that episode mining can reveal insightful patterns for numerous applications such as web stream analysis, network fault management, and cybersecurity, and that episodes can be useful for prediction. Episode mining is an active research area, and there have been numerous advances in the field over the last 25 years. However, due to the rapid evolution of the pattern mining field, there is no prior study that summarizes and gives a detailed overview of this field. The contribution of this article is to fill this gap by presenting an up-to-date survey that provides an introduction to episode mining and an overview of recent developments and research opportunities. This advanced review first gives an introduction to the field of episode mining and the first algorithms. Then, the main concepts used in these algorithms are explained. After that, several recent studies are reviewed that have addressed some limitations of these algorithms and proposed novel solutions to overcome them. Finally, the paper lists some possible extensions of the existing frameworks to mine more meaningful patterns and presents some possible orientations for future work that may contribute to the evolution of the episode mining field. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Oualid Ouarem and Farid Nouioua and Philippe Fournier-Viger},
  doi          = {10.1002/widm.1524},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1524},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey of episode mining},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The use of gene expression datasets in feature selection
research: 20 years of inherent bias? <em>WIDM</em>, <em>14</em>(2),
e1523. (<a href="https://doi.org/10.1002/widm.1523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection algorithms are frequently employed in preprocessing machine learning pipelines applied to biological data to identify relevant features. The use of feature selection in gene expression studies began at the end of the 1990s with the analysis of human cancer microarray datasets. Since then, gene expression technology has been perfected, the Human Genome Project has been completed, new microarray platforms have been created and discontinued, and RNA-seq has gradually replaced microarrays. However, most feature selection methods in the last two decades were designed, evaluated, and validated on the same datasets from the microarray technology&#39;s infancy. In this review of over 1200 publications regarding feature selection and gene expression, published between 2010 and 2020, we found that 57% of the publications used at least one outdated dataset, 23% used only outdated data, and 32% did not cite data sources. Other issues include referencing databases that are no longer available, the slow adoption of RNA-seq datasets, and bias toward human cancer data, even for methods designed for a broader scope. In the most popular datasets, some being 23 years old, mislabeled samples, experimental biases, distribution shifts, and the absence of classification challenges are common. These problems are more predominant in publications with computer science backgrounds compared to publications from biology and can lead to inaccurate and misleading biological results. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Bruno I. Grisci and Bruno César Feltes and Joice de Faria Poloni and Pedro H. Narloch and Márcio Dorn},
  doi          = {10.1002/widm.1523},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1523},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The use of gene expression datasets in feature selection research: 20 years of inherent bias?},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multispectral data mining: A focus on remote sensing
satellite images. <em>WIDM</em>, <em>14</em>(2), e1522. (<a
href="https://doi.org/10.1002/widm.1522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article gives a brief overview of various aspects of data mining of multispectral image data. We focus on specifically the remote sensing satellite images acquired using multispectral imaging (MSI), given the technology used across multiple knowledge domains, such as chemistry, medical imaging, remote sensing, and so on with a sufficient amount of variation. In this article, the different data mining processes are reviewed along with state-of-the-art methods and applications. To study data mining, it is important to know how the data are acquired and preprocessed. Hence, those topics are briefly covered in the article. The article concludes with applications demonstrating the knowledge discovery from data mining, modern challenges, and promising future directions for MSI data mining research. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sin Liang Lim and Jaya Sreevalsan-Nair and B. S. Daya Sagar},
  doi          = {10.1002/widm.1522},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1522},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Multispectral data mining: A focus on remote sensing satellite images},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deepfake detection using deep learning methods: A systematic
and comprehensive review. <em>WIDM</em>, <em>14</em>(2), e1520. (<a
href="https://doi.org/10.1002/widm.1520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning (DL) has been effectively utilized in various complicated challenges in healthcare, industry, and academia for various purposes, including thyroid diagnosis, lung nodule recognition, computer vision, large data analytics, and human-level control. Nevertheless, developments in digital technology have been used to produce software that poses a threat to democracy, national security, and confidentiality. Deepfake is one of those DL-powered apps that has lately surfaced. So, deepfake systems can create fake images primarily by replacement of scenes or images, movies, and sounds that humans cannot tell apart from real ones. Various technologies have brought the capacity to change a synthetic speech, image, or video to our fingers. Furthermore, video and image frauds are now so convincing that it is hard to distinguish between false and authentic content with the naked eye. It might result in various issues and ranging from deceiving public opinion to using doctored evidence in a court. For such considerations, it is critical to have technologies that can assist us in discerning reality. This study gives a complete assessment of the literature on deepfake detection strategies using DL-based algorithms. We categorize deepfake detection methods in this work based on their applications, which include video detection, image detection, audio detection, and hybrid multimedia detection. The objective of this paper is to give the reader a better knowledge of (1) how deepfakes are generated and identified, (2) the latest developments and breakthroughs in this realm, (3) weaknesses of existing security methods, and (4) areas requiring more investigation and consideration. The results suggest that the Conventional Neural Networks (CNN) methodology is the most often employed DL method in publications. According to research, the majority of the articles are on the subject of video deepfake detection. The majority of the articles focused on enhancing only one parameter, with the accuracy parameter receiving the most attention. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Arash Heidari and Nima Jafari Navimipour and Hasan Dag and Mehmet Unal},
  doi          = {10.1002/widm.1520},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1520},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Deepfake detection using deep learning methods: A systematic and comprehensive review},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolution toward intelligent communications: Impact of deep
learning applications on the future of 6G technology. <em>WIDM</em>,
<em>14</em>(1), e1521. (<a
href="https://doi.org/10.1002/widm.1521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sixth generation (6G) represents the next evolution in wireless communication technology and is currently under research and development. It is expected to deliver faster speeds, reduced latency, and greater capacity compared to the current 5G wireless technology. 6G is envisioned as a technology capable of establishing a fully data-driven network, proficient in analyzing and optimizing end-to-end behavior and handling massive volumes of real-time data at rates of up to terabits per second (Tb/s). Moreover, 6G is designed to accommodate an average of 1000+ substantial connections per person over the course of a decade. The concept of a data-driven network introduces a new service paradigm, which offers fresh opportunities for applications within 6G wireless communication and network design in the future. This paper aims to provide a survey of existing applications of 6G that are based on deep learning techniques. It also explores the potential, essential technologies, scenarios, challenges, and related topics associated with 6G. These aspects are crucial for meeting the requirements for the development of future intelligent networks. Furthermore, this work delves into various research gaps between deep learning and 6G that remain unexplored. Different potential deep learning applications for 6G networks, including privacy, security, environmentally friendly communication, sustainability, and various wireless applications, are discussed. Additionally, we shed light on the challenges and future trends in this field. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mohamed Abd Elaziz and Mohammed A. A. Al-qaness and Abdelghani Dahou and Saeed Hamood Alsamhi and Laith Abualigah and Rehab Ali Ibrahim and Ahmed A. Ewees},
  doi          = {10.1002/widm.1521},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1521},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Evolution toward intelligent communications: Impact of deep learning applications on the future of 6G technology},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning models for price forecasting of financial time
series: A review of recent advancements: 2020–2022. <em>WIDM</em>,
<em>14</em>(1), e1519. (<a
href="https://doi.org/10.1002/widm.1519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting the prices of financial time series is essential and challenging for the financial sector. Owing to recent advancements in deep learning techniques, deep learning models are gradually replacing traditional statistical and machine learning models as the first choice for price forecasting tasks. This shift in model selection has led to a notable rise in research related to applying deep learning models to price forecasting, resulting in a rapid accumulation of new knowledge. Therefore, we conducted a literature review of relevant studies over the past 3 years with a view to aiding researchers and practitioners in the field. This review delves deeply into deep learning-based forecasting models, presenting information on model architectures, practical applications, and their respective advantages and disadvantages. In particular, detailed information is provided on advanced models for price forecasting, such as Transformers, generative adversarial networks (GANs), graph neural networks (GNNs), and deep quantum neural networks (DQNNs). The present contribution also includes potential directions for future research, such as examining the effectiveness of deep learning models with complex structures for price forecasting, extending from point prediction to interval prediction using deep learning models, scrutinizing the reliability and validity of decomposition ensembles, and exploring the influence of data volume on model performance. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Cheng Zhang and Nilam Nur Amir Sjarif and Roslina Ibrahim},
  doi          = {10.1002/widm.1519},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1519},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Deep learning models for price forecasting of financial time series: a review of recent advancements: 2020–2022},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-trained language models: What do they know?
<em>WIDM</em>, <em>14</em>(1), e1518. (<a
href="https://doi.org/10.1002/widm.1518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have substantially pushed artificial intelligence (AI) research and applications in the last few years. They are currently able to achieve high effectiveness in different natural language processing (NLP) tasks, such as machine translation, named entity recognition, text classification, question answering, or text summarization. Recently, significant attention has been drawn to OpenAI&#39;s GPT models&#39; capabilities and extremely accessible interface. LLMs are nowadays routinely used and studied for downstream tasks and specific applications with great success, pushing forward the state of the art in almost all of them. However, they also exhibit impressive inference capabilities when used off the shelf without further training. In this paper, we aim to study the behavior of pre-trained language models (PLMs) in some inference tasks they were not initially trained for. Therefore, we focus our attention on very recent research works related to the inference capabilities of PLMs in some selected tasks such as factual probing and common-sense reasoning. We highlight relevant achievements made by these models, as well as some of their current limitations that open opportunities for further research. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Nuno Guimarães and Ricardo Campos and Alípio Jorge},
  doi          = {10.1002/widm.1518},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1518},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Pre-trained language models: What do they know?},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The state-of-art review of ultra-precision machining using
text mining: Identification of main themes and recommendations for the
future direction. <em>WIDM</em>, <em>14</em>(1), e1517. (<a
href="https://doi.org/10.1002/widm.1517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-precision machining (UPM), one of the most advanced machining techniques that can produce exact components, significantly impacts the technological community. The significance of UPM attracts the attention of academic and industrial partners. As a result of the rapid development of UPM caused by technological advancement, it is necessary to revisit the current stages and evolution of UPM to sustain and advance this technology. The state of the art in UPM is first investigated systematically in this study by identifying the current four major UPM themes. The UPM thematic network is then built, along with a structural analysis of the network, to determine the interactions between each theme and the primary roles of theme members responsible for the interactions. Furthermore, the “bridge” role is assigned to the specific UPM theme content. On the other hand, Sentiment analysis is conducted to determine how the academic community at UPM feels about the themes for UPM research to focus on those themes with a need for more confidence. Considering the above findings, the future perspective of UPM and suggestions for its advancement are discussed and provided. This study provides a comprehensive understanding and the current state-of-the-art review of UPM technology by a text mining technique to critically analyze its research content, as well as suggestions to enhance UPM development by focusing on its current challenges, thereby assisting academia and institutions in leveraging this technology to benefit society. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Wai Sze YIP and Hengzhou Edward Yan and Baolong Zhang and Suet To},
  doi          = {10.1002/widm.1517},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1517},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The state-of-art review of ultra-precision machining using text mining: Identification of main themes and recommendations for the future direction},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smart city maturity models: A multidimensional synthesized
approach. <em>WIDM</em>, <em>14</em>(1), e1516. (<a
href="https://doi.org/10.1002/widm.1516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart cities are one of the consequences of digital transformation, and there have been many attempts to assess the smartness of cities with various frameworks. Among these frameworks, smart city maturity models (SCMMs) evaluate the existing conditions of cities and provide guidelines for progressing through the subsequent stages of maturity. However, most maturity models follow the instructions of the first model, published by the International Data Corporation, and there are many similarities across the models. These maturity models have advantages and disadvantages, while previous studies have not addressed the differences. Therefore, this article fills this knowledge gap by systematically reviewing the existing SCMMs. The findings suggest that some trending topics, such as resiliency concerning global pandemics and cultural aspects are neglected in SCMMs. Moreover, the validation techniques of the models are not rational. Finally, given the theoretical nature of most models, they cannot be applied to multiple regions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sepehr Ghazinoory and Jinus Roshandel and Fatemeh Parvin and Shohreh Nasri and Mehdi Fatemi},
  doi          = {10.1002/widm.1516},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1516},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Smart city maturity models: A multidimensional synthesized approach},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning and blockchain technologies for
cybersecurity in connected vehicles. <em>WIDM</em>, <em>14</em>(1),
e1515. (<a href="https://doi.org/10.1002/widm.1515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future connected and autonomous vehicles (CAVs) must be secured against cyberattacks for their everyday functions on the road so that safety of passengers and vehicles can be ensured. This article presents a holistic review of cybersecurity attacks on sensors and threats regarding multi-modal sensor fusion. A comprehensive review of cyberattacks on intra-vehicle and inter-vehicle communications is presented afterward. Besides the analysis of conventional cybersecurity threats and countermeasures for CAV systems, a detailed review of modern machine learning, federated learning, and blockchain approach is also conducted to safeguard CAVs. Machine learning and data mining-aided intrusion detection systems and other countermeasures dealing with these challenges are elaborated at the end of the related section. In the last section, research challenges and future directions are identified. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Jameel Ahmad and Muhammad Umer Zia and Ijaz Haider Naqvi and Jawwad Nasar Chattha and Faran Awais Butt and Tao Huang and Wei Xiang},
  doi          = {10.1002/widm.1515},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1515},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Machine learning and blockchain technologies for cybersecurity in connected vehicles},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
