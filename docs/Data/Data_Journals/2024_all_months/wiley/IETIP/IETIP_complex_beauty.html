<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietip---346">IETIP - 346</h2>
<ul>
<li><details>
<summary>
(2024). Atten-SEVNETR for volumetric segmentation of glioblastoma
and interactive refinement to limit over-segmentation. <em>IETIP</em>,
<em>18</em>(14), 4928–4943. (<a
href="https://doi.org/10.1049/ipr2.13218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise localization and volumetric segmentation of glioblastoma before and after surgery are crucial for various clinical purposes, including post-surgery treatment planning, monitoring tumour recurrence, and creating radiotherapy maps. Manual delineation is time-consuming and prone to errors, hence the adoption of automated 3D quantification methods using deep learning algorithms from MRI scans in recent times. However, automated segmentation often leads to over-segmentation or under-segmentation of tumour regions. Introducing an interactive deep-learning tool would empower radiologists to rectify these inaccuracies by adjusting the over-segmented and under-segmented voxels as needed. This paper proposes a network named Atten-SEVNETR, that has a combined architecture of vision transformers and convolutional neural networks (CNN). This hybrid architecture helps to learn the input volume representation in sequences and focuses on the global multi-scale information. An interactive graphical user interface is also developed where the initial 3D segmentation of glioblastoma can be interactively corrected to remove falsely detected spurious tumour regions. Atten-SEVNETR is trained on BraTS training dataset and tested on BraTS validation dataset and on Uppsala University post-operative glioblastoma dataset. The methodology outperformed state-of-the-art networks like nnFormer, SwinUNet, and SwinUNETR. The mean dice score achieved is 0.7302, and the mean Hausdorff distance-95 got is 7.78 mm for the Uppsala University dataset.},
  archive      = {J_IETIP},
  author       = {Swagata Kundu and Dimitrios Toumpanakis and Johan Wikstrom and Robin Strand and Ashis Kumar Dhara},
  doi          = {10.1049/ipr2.13218},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4928-4943},
  shortjournal = {IET Image Process.},
  title        = {Atten-SEVNETR for volumetric segmentation of glioblastoma and interactive refinement to limit over-segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved multi-scale YOLOv8 for apple leaf dense lesion
detection and recognition. <em>IETIP</em>, <em>18</em>(14), 4913–4927.
(<a href="https://doi.org/10.1049/ipr2.13223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apple leaf lesions present a challenge for their detection and recognition because of their wide variety of species, morphologies, uneven sizes, and complex backgrounds. This paper proposes an improved multi-scale YOLOv8 for apple leaf dense lesion detection and recognition. In the proposed YOLOv8, an improved C2f-RFEM module is constructed in the backbone network to improve the feature extraction of disease object. A new neck network is designed by using C2f-DCN and C2f-DCN-EMA module, which are established with deformable convolutions and efficient multi-scale attention module with cross-spatial learning attention mechanism. Moreover, a large-scale detection head is introduced for increasing the resolution of the small lesion targets, so as to further improve the detection ability for multi-scale diseases. Finally, the improved YOLOv8 is tested on the common objects in context (COCO) database with 80 kinds of objectives and an apple leaf disease database with 8 kinds of diseases. Compared to the baseline YOLOv8 model, the proposed improved YOLOv8 increases the mAP0.5 by 3%, and decreases the floating-point operations per second (FLOPs) by 0.3G on the COCO database. For the apple leaf disease database, the improved YOLOv8 outperforms in terms of mAP and FLOPs compared to other models, for parameters and model size, it is ranked second and third, respectively. Experimental results show that the improved YOLOv8 has better adaptability to multi-scale dense distribution of apple leaf disease spots with complex scenarios.},
  archive      = {J_IETIP},
  author       = {Shixin Huo and Na Duan and Zhizheng Xu},
  doi          = {10.1049/ipr2.13223},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4913-4927},
  shortjournal = {IET Image Process.},
  title        = {An improved multi-scale YOLOv8 for apple leaf dense lesion detection and recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on adaptive object detection via improved
HSA-YOLOv5 for raspberry maturity detection. <em>IETIP</em>,
<em>18</em>(14), 4898–4912. (<a
href="https://doi.org/10.1049/ipr2.13149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of machine vision, target detection models have experienced rapid development and have been practically applied in various domains. In agriculture, target detection models are commonly used to identify various types of fruits. However, when it comes to recognizing berries, such as raspberries, the fruits nearing ripeness exhibit highly similar colours, posing a challenge for existing target detection models to accurately identify raspberries in this stage. Addressing this issue, a raspberry detection method called HSA-YOLOv5 (HSV self-adaption YOLOv5) is proposed. This method detects immature, nearly ripe, and ripe raspberries. The approach involves transforming the RGB colour space of the original dataset images into an improved HSV colour space. By adjusting corresponding parameters and enhancing the contrast of similar colours while retaining the maximum features of the original image, the method strengthens data features. Adaptive selection of HSV parameters is performed based on data captured under different weather conditions, applying homogeneous preprocessing to the dataset. The improved model is compared with the original YOLOv5 model using a self-constructed dataset. Experimental results demonstrate that the improved model achieves a mean average precision (mAP) of 0.97, a 6.42 percentage point increase compared to the baseline YOLOv5 model. In terms of immature, nearly ripe, and ripe raspberries, there are improvements of 6, 4, and 7 percentage points, respectively, validating the effectiveness of the proposed model.},
  archive      = {J_IETIP},
  author       = {Chen Ling and Qunying Zhang and Mei Zhang and Chihan Gao},
  doi          = {10.1049/ipr2.13149},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4898-4912},
  shortjournal = {IET Image Process.},
  title        = {Research on adaptive object detection via improved HSA-YOLOv5 for raspberry maturity detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced classification performance through GauGAN-based
data augmentation for tomato leaf images. <em>IETIP</em>,
<em>18</em>(14), 4887–4897. (<a
href="https://doi.org/10.1049/ipr2.13069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigated a data augmentation method for plant disease classification and early diagnosis based on a generative adversarial neural network (GAN). In the development of classification models using deep learning, data imbalance is a primary factor that reduces classification performance. To address this issue, tomato disease images from the public dataset PlantVillage were used to evaluate the performance of the GauGAN algorithm. The images generated by the proposed GauGAN model were used to train a MobileNet-based classification model and compared with methods trained with conventional data augmentation techniques and cut-mix and mix-up algorithms. The experimental results demonstrate that based on F1-scores, GauGAN-based data augmentation outperformed conventional methods by more than 10%. In addition, after the model was retrained on data collected in the field, it efficiently generated various disease images. The evaluation results from those images also revealed a data augmentation effect of about 10% compared with traditional augmentation techniques.},
  archive      = {J_IETIP},
  author       = {Seung-Beom Cho and Yu Cheng and Sanghun Sul},
  doi          = {10.1049/ipr2.13069},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4887-4897},
  shortjournal = {IET Image Process.},
  title        = {Enhanced classification performance through GauGAN-based data augmentation for tomato leaf images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMC-YOLO: Improved YOLOv8-based defect detection for
cigarette packs. <em>IETIP</em>, <em>18</em>(14), 4873–4886. (<a
href="https://doi.org/10.1049/ipr2.13272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection in cigarette packaging is a crucial process for ensuring product quality meets industry standards within the tobacco manufacturing sector. Defect detection methods based on deep learning have significantly enhanced efficiency. However, challenges remain in addressing issues such as blurred boundary textures of cigarette pack defects and the complex differentiability and similarity among defects. Consequently, this study presents the design of a defect detector for cigarette packs that is sensitive to detail features, named AMC-YOLO. Initially, an Adaptive Spatial Weight Perception (ASWP) module is designed to emphasize local information from different regions during the downsampling process and integrate effective features. Additionally, a Multidimensional Aggregation Radiative Feature Pyramid Networks (MARFPN) is proposed to aggregate multi-scale semantic information across dimensions and relay it back to various levels within the network to facilitate the learning of more refined feature. Lastly, a Cross-Layer Collaborative Detection Head (CLCDH) is introduced to further weight and fuse the contextual information between local and global aspects. Experimental results demonstrate that AMC-YOLO outperforms state-of-the-art methods on the Cigarette Pack Defect and GC10-DET datasets, exhibiting the highest detection precision and excellent generalization. These findings highlight the significant potential for the application of AMC-YOLO in cigarette pack defect detection.},
  archive      = {J_IETIP},
  author       = {Peng Dong and Yongsheng Wang and Qianyuan Yu and Weihua Feng and Guohao Zong},
  doi          = {10.1049/ipr2.13272},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4873-4886},
  shortjournal = {IET Image Process.},
  title        = {AMC-YOLO: Improved YOLOv8-based defect detection for cigarette packs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized zero-watermark algorithm based on non-uniform
weighted reconstruction and WGAN. <em>IETIP</em>, <em>18</em>(14),
4862–4872. (<a href="https://doi.org/10.1049/ipr2.13291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image protection of non-fungible tokens based on zero-watermark methods has received widespread attention. However, on the one hand, existing zero-watermark methods are often limited to complex texture changes in the host image, and the features for constructing the zero-watermark are vulnerable to geometric attacks. On the other hand, a single watermark image cannot adapt to the diverse usage scenarios of non-fungible tokens. This paper proposes a robust personalized zero-watermark scheme to address the challenges above. Firstly, the image regions suitable for constructing zero-watermarks are highlighted by the non-uniform weighted reconstruction, and the U-Net is introduced for feature extraction against the geometric attacks. At the same time, the watermark images generated by generative models that are suitable for the usage scenario have achieved zero-watermark addition and protection for non-fungible token image content. The proposed method has been experimentally verified to have a certain degree of robustness in geometric and non-geometric attacks.},
  archive      = {J_IETIP},
  author       = {Yiran Peng and Chenheng Deng and Jiaqi Li and KinTak U},
  doi          = {10.1049/ipr2.13291},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4862-4872},
  shortjournal = {IET Image Process.},
  title        = {Personalized zero-watermark algorithm based on non-uniform weighted reconstruction and WGAN},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fine-grained image classification method based on
information interaction. <em>IETIP</em>, <em>18</em>(14), 4852–4861. (<a
href="https://doi.org/10.1049/ipr2.13295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the accuracy of fine-grained image classification and address challenges such as excessive interference factors within the dataset, inadequate extraction of local key features, and insufficient channel semantic association, a dual-branch information interaction model that integrates convolutional neural networks (CNN) with Vision Transformers is proposed. This model leverages the Vision Transformer branch to extract global features, which are subsequently combined with the CNN branch to further augment the model&#39;s capability for local information extraction. In order to enhance the ability of the CNN branch to extract global information and reduce the loss of feature information, a feature enhancement module is added to the CNN branch. Since the Vision Transformer branch directly convolves with the convolution kernel will result in the inability to learn the underlying features of the image, a shallow feature extraction module is proposed, and the CNN and Vision Transformer branches interact with the information of the dual branches through the down-sampling Down module and the up-sampling UP module. The accuracy of the improved method on CUB-200-2011, Stanford Cars and FGVC-Aircraft fine-grained image classification datasets are 95.2%, 97.1% and 96.9%, respectively. The experimental results show that the method has good generalization on different datasets.},
  archive      = {J_IETIP},
  author       = {Shuo Zhu and Xukang Zhang and Yu Wang and Zongyang Wang and Jiahao Sun},
  doi          = {10.1049/ipr2.13295},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4852-4861},
  shortjournal = {IET Image Process.},
  title        = {A fine-grained image classification method based on information interaction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage video anomaly detection based on dual-stream
networks and multi-instance learning. <em>IETIP</em>, <em>18</em>(14),
4843–4851. (<a href="https://doi.org/10.1049/ipr2.13286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To promptly detect abnormal events in surveillance videos, this article designs a video anomaly detection method based on multiple instance learning. Generally, abnormal events occur less frequently compared to normal events. Traditional video surveillance relies on manual operation to monitor scenes and detect abnormal events by watching surveillance videos. However, watching surveillance footage is a labor-intensive task, and prolonged observation can lead to visual fatigue and lack of concentration, which in turn results in missed detections and false positives [1]. Therefore, it is crucial to develop intelligent algorithms for video anomaly detection. The method can detect whether segments of a video contain abnormal events. First, the I3D network is used as a feature extractor to capture spatiotemporal features from the input video. Then, the spatiotemporal information is processed and input into a segment-level anomaly detector based on multiple instance learning for detection. The authors treat abnormal videos as positive bags and normal videos as negative bags, and automatically learn a deep anomaly ranking model that can predict abnormal segments. Finally, the results of the training were tested and analyzed, demonstrating that the model is capable of detecting abnormal traffic segments.},
  archive      = {J_IETIP},
  author       = {Dejun Zhang and Wenbo Fang and Yuhang Liu and Zirong Lyu and Chen Xiong and Zhan Wang},
  doi          = {10.1049/ipr2.13286},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4843-4851},
  shortjournal = {IET Image Process.},
  title        = {Two-stage video anomaly detection based on dual-stream networks and multi-instance learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced lightweight infrared object detection algorithm for
assistive navigation in visually impaired individuals. <em>IETIP</em>,
<em>18</em>(14), 4824–4842. (<a
href="https://doi.org/10.1049/ipr2.13233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an advanced infrared scene detection algorithm, enhancing the YOLOv8 model for aiding visually impaired individuals in navigation. The focus is on the neck network, integrating attention scale sequences to boost multi-level perception, particularly for small object detection. This is achieved by adding upsampling and downsampling in the P2 module. Additionally, the CIoU loss function is refined with Inner-SIoU, elevating bounding box detection precision. A distinctive feature of the approach is its monocular distance and velocity measurement integration, which operates independently of external devices, providing direct navigation support for visually impaired people. Further, the enhanced YOLOv8 is adapted for mobile use, employing pruning and lightweight methods, which substantially enhance its practicality. The experimental results on the FLIR and WOTR datasets demonstrate that, compared to the original YOLOv8n, the improved algorithm has achieved a 2.1% and 3.2% increase in , respectively. Furthermore, the has seen a 2.2% and 3.8% improvement. Concurrently, the model size has been reduced by 55% and 60%, and the number of parameters has decreased by 60% and 67%. Compared to other assistive travel methods for visually impaired individuals, our work demonstrates superior practicality.},
  archive      = {J_IETIP},
  author       = {Zhimin Bai and Yang Yang and Jian Wang and Zhengyang Li and Jiajun Wang and Chunxin Liu},
  doi          = {10.1049/ipr2.13233},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4824-4842},
  shortjournal = {IET Image Process.},
  title        = {Enhanced lightweight infrared object detection algorithm for assistive navigation in visually impaired individuals},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous single image super-resolution and blind
gaussian denoising via slim ghost full-frequency residual blocks.
<em>IETIP</em>, <em>18</em>(14), 4799–4823. (<a
href="https://doi.org/10.1049/ipr2.13230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given that super-resolution (SR) aims to recover lost information, and low-resolution (LR) images in real-world conditions might be corrupted with multiple degradations, considering basic bicubic down-sampling as the sole degradation significantly limits the performance of most existing SR models. This paper presents a model for simultaneous super-resolution and blind additive white Gaussian noise (AWGN) denoising with two components (netdeg and netSR) that is based on a generative adversarial network (GAN) to achieve detailed results. netdeg, featuring residual and innovative cost-effective ghost residual blocks with a frequency separation module for obtaining long-range information, blindly restores a clean version of the LR image. netSR leverages slim ghost full-frequency residual blocks to process low-frequency (LF) and high-frequency (HF) information via static large convolutions and pixel-wise highlighted input-adaptive dynamic convolutions, respectively. To address the susceptibility of dynamic layers to noise and preserve feature diversity while reducing model’s costs, static and dynamic layer features are combined and highlighted. Diverse and non-redundant features are then processed using ghost-style blocks. The proposed model achieves comparable SR results in bicubic down-sampling scenarios, outperform existing SR methods in the complex task of concurrent SR and AWGN denoising, and demonstrate robustness in handling images corrupted with varying levels of AWGN.},
  archive      = {J_IETIP},
  author       = {Saghar Farhangfar and Aryaz Baradarani and Mohammad Asadpour and Mohammad Ali Balafar and Roman Gr. Maev},
  doi          = {10.1049/ipr2.13230},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4799-4823},
  shortjournal = {IET Image Process.},
  title        = {Simultaneous single image super-resolution and blind gaussian denoising via slim ghost full-frequency residual blocks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive construction of deep neural network-based
encoder–decoder framework for automatic image captioning systems.
<em>IETIP</em>, <em>18</em>(14), 4778–4798. (<a
href="https://doi.org/10.1049/ipr2.13287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel encoder–decoder framework based on deep neural networks and provides a thorough investigation into the field of automatic picture captioning systems. The suggested model uses a “long short-term memory” decoder for word prediction and sentence construction, and a “convolutional neural network” as an encoder that is skilled at object recognition and spatial information retention. The long short-term memory network functions as a sequence processor, generating a fixed-length output vector for final predictions, while the VGG-19 model is utilized as an image feature extractor. For both training and testing, the study uses a variety of photos from open-access datasets, such as Flickr8k, Flickr30k, and MS COCO. The Python platform is used for implementation, with Keras and TensorFlow as backends. The experimental findings, which were assessed using the “bilingual evaluation understudy” metric, demonstrate the effectiveness of the suggested methodology in automatically captioning images. By addressing spatial relationships in images and producing logical, contextually relevant captions, the paper advances image captioning technology. Insightful ideas for future study directions are generated by the discussion of the difficulties faced during the experimentation phase. By establishing a strong neural network architecture for automatic picture captioning, this study creates opportunities for future advancement and improvement in the area.},
  archive      = {J_IETIP},
  author       = {Md Mijanur Rahman and Ashik Uzzaman and Sadia Islam Sami and Fatema Khatun and Md Al-Amin Bhuiyan},
  doi          = {10.1049/ipr2.13287},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4778-4798},
  shortjournal = {IET Image Process.},
  title        = {A comprehensive construction of deep neural network-based encoder–decoder framework for automatic image captioning systems},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). K-wise multi-graph matching. <em>IETIP</em>,
<em>18</em>(14), 4760–4777. (<a
href="https://doi.org/10.1049/ipr2.13285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-graph matching (MGM), which aims to find correspondences among multiple graphs, is an extension of conventional two-graph matching. Existing MGM methods fall into two categories: pairwise based and tensor based. Pairwise-based methods consider similarities between every two features; while tensor-based methods consider the overall similarity among all features, offering much more flexibility similarity measurements and less information loss, but at the cost of exorbitant computational demands. Here, a fresh perspective on MGM task is delivered, that is, matching based on any k features. It enables the consideration of more complex affinity relationship beyond pairwise while keeping computational demands within a manageable threshold. Furthermore, a factorization technique for the k -wise global affinity matrix is proposed, significantly reducing space complexity. This approach unifies existing MGM methods and inspires future research focusing on k -wise affinity relationship, showcasing both theoretical and practical advancements in the field. Experiments on synthetic and real-world datasets demonstrate the superiority of our method.},
  archive      = {J_IETIP},
  author       = {Xinwen Zhu and Liangliang Zhu and Xiurui Geng},
  doi          = {10.1049/ipr2.13285},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4760-4777},
  shortjournal = {IET Image Process.},
  title        = {K-wise multi-graph matching},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object detection in smart indoor shopping using an enhanced
YOLOv8n algorithm. <em>IETIP</em>, <em>18</em>(14), 4745–4759. (<a
href="https://doi.org/10.1049/ipr2.13284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an enhanced object detection algorithm tailored for indoor shopping applications, a critical component of smart cities and smart shopping ecosystems. The proposed method builds on the YOLOv8n algorithm by integrating a ParNetAttention module into the backbone&#39;s C2f module, creating the novel C2f-ParNet structure. This innovation enhances feature extraction, crucial for detecting intricate details in complex indoor environments. Additionally, the channel-wise attention-recurrent feature extraction (CARAFE) module is incorporated into the neck network, improving target feature fusion and focus on objects of interest, thereby boosting detection accuracy. To optimize training efficiency, the model employs the Wise Intersection over Union (WIoUv3) as its regression loss function, accelerating data convergence and improving performance. Experimental results demonstrate the enhanced YOLOv8n achieves a mean average precision (mAP) at 50% threshold (mAP@50) of 61.2%, a 1.2 percentage point improvement over the baseline. The fully optimized algorithm achieves an mAP@50 of 65.9% and an F1 score of 63.5%, outperforming both the original YOLOv8n and existing algorithms. Furthermore, with a frame rate of 106.5 FPS and computational complexity of just 12.9 GFLOPs (Giga Floating-Point Operations per Second), this approach balances high performance with lightweight efficiency, making it ideal for real-time applications in smart retail environments.},
  archive      = {J_IETIP},
  author       = {Yawen Zhao and Defu Yang and Sheng Cao and Bingyu Cai and Maryamah Maryamah and Mahmud Iwan Solihin},
  doi          = {10.1049/ipr2.13284},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4745-4759},
  shortjournal = {IET Image Process.},
  title        = {Object detection in smart indoor shopping using an enhanced YOLOv8n algorithm},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Swin-YOLOX for autonomous and accurate drone visual landing.
<em>IETIP</em>, <em>18</em>(14), 4731–4744. (<a
href="https://doi.org/10.1049/ipr2.13282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As UAVs are more and more widely used in military and civilian fields, their intelligent applications have also been developed rapidly. However, high-precision autonomous landing is still an industry challenge. GPS-based methods will not work in places where GPS signals are not available; multi-sensor combination navigation is difficult to be widely used because of the high equipment requirements; traditional vision-based methods are sensitive to scale transformation, background complexity and occlusion, which affect the detection performance. In this paper, we address these problems and apply deep learning methods to target detection in the UAV landing phase. Firstly, we optimize the backbone network of YOLOX and propose the Swin Transformer based YOLOX (Swin-YOLOX) UAV landing visual positioning algorithm. Secondly, based on the UAV-VPD database, a batch of actual acquisition data is added to build the UAV-VPDV2 database by AI annotation method. And finally, the RBN data batch normalization method is used to improve the performance of the model in extracting effective features from the data. Extensive experiments have shown that the AP50 of the proposed method can reach 98.7%, which is superior to other detection models, with a detection speed of 38.4 frames/second, and can meet the requirements of real-time detection.},
  archive      = {J_IETIP},
  author       = {Rongbin Chen and Ying Xu and Mohamad Sabri bin Sinal and Dongsheng Zhong and Xinru Li and Bo Li and Yadong Guo and Qingjia Luo},
  doi          = {10.1049/ipr2.13282},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4731-4744},
  shortjournal = {IET Image Process.},
  title        = {Swin-YOLOX for autonomous and accurate drone visual landing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CASSC: Context-aware method for depth guided semantic scene
completion. <em>IETIP</em>, <em>18</em>(14), 4716–4730. (<a
href="https://doi.org/10.1049/ipr2.13280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic scene completion is a crucial end-to-end 3D perception task, and the 3D information perception subjects is vital for autonomous driving. This paper presents CASSC, a novel adaptive context-aware method based on Transformer networks, aimed at realizing camera-based semantic scene completion algorithms. The key idea is to leverage rich context information from images to obtain pixel-level label proposals, followed by designing a multiscale fusion mechanism to merge this information and match it with voxel space. A weakly supervised training strategy is proposed to obtain semantic label distribution features from images and introduce an adaptive multiscale fusion module to fuse and adaptively match these features with voxel space. Here, CASSC achieves state-of-the-art performance on the SemanticKITTI dataset and demonstrates excellent performance on the SSC-Bench dataset. Ablation experiments validate the rationality and effectiveness of our design, and the model and code of CASSC will be open-sourced on https://github.com/dogooooo/CASSC .},
  archive      = {J_IETIP},
  author       = {Jinghao Cao and Ming Li and Sheng Liu and Yang Li and Sidan Du},
  doi          = {10.1049/ipr2.13280},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4716-4730},
  shortjournal = {IET Image Process.},
  title        = {CASSC: Context-aware method for depth guided semantic scene completion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An innovative traffic flow detection model based on temporal
video frame analysis and grayscale aggregation quantification.
<em>IETIP</em>, <em>18</em>(14), 4704–4715. (<a
href="https://doi.org/10.1049/ipr2.13279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current traffic status detection methods heavily rely on historical traffic flow data and vehicle counts. However, these methods often fail to meet the stringent real-time requirements of state detection, especially on edge devices with limited computing resources.To address these challenges, this study develops a traffic alert model using temporal video frame analysis and grayscale aggregation quantization techniques. Initially, the model uses distance mapping between pixel features and frames of road traffic videos to construct a comprehensive road environment and vehicle segmentation model. The model also establishes a mapping between pixel equidistant lines and actual distances, enabling precise congestion detection. This approach significantly reduces costs associated with traditional traffic detection methods as it does not rely on historical data. Performance evaluation using fixed-point road monitoring data indicates that the proposed model outperforms traditional traffic state detection models, with a performance improvement of approximately 4.7% to 9.5%. Additionally, the model improves computing resource efficiency by approximately 72.5% and demonstrates substantial real-time detection capabilities.},
  archive      = {J_IETIP},
  author       = {Xin Liu and Qiao Meng and Xin Li and Zhijie Wang and Siyuan Kong and Bingyu Li},
  doi          = {10.1049/ipr2.13279},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4704-4715},
  shortjournal = {IET Image Process.},
  title        = {An innovative traffic flow detection model based on temporal video frame analysis and grayscale aggregation quantification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automatic detection method for cervical liquid-based
cells based on improved yolov5s network. <em>IETIP</em>,
<em>18</em>(14), 4695–4703. (<a
href="https://doi.org/10.1049/ipr2.13278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the significant challenges of high false positive and false negative rates in existing algorithms for detecting cervical fluid-based cells, an enhanced Yolov5s network is introduced. This paper details a novel approach that dynamically adjusts the weights of channels and the spatial attention in modules, substantially improving feature extraction from small objects and boosting the detection capabilities of the network. Furthermore, Mixup data augmentation technology is incorporated to counter the issue of imbalanced data categories in the custom dataset. The Complete Intersection over Union loss function is also employed to refine coordinate localization accuracy during training. Tested on the proprietary cervical cytology dataset, the modified Yolov5s achieves a mean Average Precision of 92.1%, surpassing the previous state-of-the-art by 5.6%. This enhancement substantiates the efficacy of the proposed model. Code and models are accessible at https://github.com/youyi888/yolov5_CPCA .},
  archive      = {J_IETIP},
  author       = {Shen Xudong and Wu Zhihua and Tao Yebo and Wu Xianglian and Chen Linfei},
  doi          = {10.1049/ipr2.13278},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4695-4703},
  shortjournal = {IET Image Process.},
  title        = {An automatic detection method for cervical liquid-based cells based on improved yolov5s network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised person re-identification based on adaptive
information supplementation and foreground enhancement. <em>IETIP</em>,
<em>18</em>(14), 4680–4694. (<a
href="https://doi.org/10.1049/ipr2.13277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification has attracted vital interest because of its ability to protect privacy, significantly lower the expense of manual annotation, and eliminate the need for data labels. General unsupervised methods train the network only through global features, which causes the fine-grained information contained in local features to be ignored in the recognition process, resulting in large amounts of label noise and affecting the recognition accuracy. Moreover, more robust pedestrian features can also improve the accuracy of clustering and enable unsupervised person re-identification to obtain better results. To address these issues, first, a dual-branch structure was proposed, which separately obtains the global features of the pedestrian and the local features by dividing the global features into a few equal sections. Then, an adaptive information supplementation (AIS) method based on the k-nearest neighbor algorithm is designed to ascertain each local feature&#39;s relevance to the global features, calculating adaptive weight scores for information supplementation. Finally, these weight scores are used to reallocate the weights of the global features in each part, acquiring features that contain more pedestrian information during the representation learning process. These better features are used to reduce label noise to obtain more accurate pseudo-labels. Second, an adaptive foreground enhancement module (AFEM) was proposed and inserted before clustering to increase the robustness of pedestrian features, which increases the precision of the pseudo-labels that are produced after clustering. Experiments on Market-1501, DukeMTMC-reID, and MSMT17 demonstrate that the proposed method achieves better results than state-of-the-art methods in fully unsupervised person re-identification tasks.},
  archive      = {J_IETIP},
  author       = {Qiang Wang and Zhihong Huang and Huijie Fan and Shengpeng Fu and Yandong Tang},
  doi          = {10.1049/ipr2.13277},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4680-4694},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised person re-identification based on adaptive information supplementation and foreground enhancement},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning spatial-frequency interaction for generalizable
deepfake detection. <em>IETIP</em>, <em>18</em>(14), 4666–4679. (<a
href="https://doi.org/10.1049/ipr2.13276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, face forgery detection has gained significant attention, resulting in considerable advancements. However, most existing methods rely on CNNs to extract artefacts from the spatial domain, overlooking the pervasive frequency-domain artefacts present in deepfake content, which poses challenges in achieving robust and generalized detection. To address these issues, we propose the dual-stream frequency—spatial fusion network is proposed for deepfake detection. The dual-stream frequency-spatial fusion network consists of three components: the spatial forgery feature extraction module, the frequency forgery feature extraction module, and the spatial–frequency feature fusion module. The spatial forgery feature extraction module employs spatial-channel attention to extract spatial domain features, targeting artefacts in the spatial domain. The frequency forgery feature extraction module leverages the focused linear attention to detect frequency domain anomalies in internal regions, enabling the identification of generated content. The spatial–frequency feature fusion module then fuses forgery features extracted from both the spatial and frequency domains, facilitating accurate detection of splicing artefacts and internally generated forgeries. This approach enhances the model&#39;s ability to more accurately capture forgery characteristics. Extensive experiments on several widely-used benchmarks demonstrate that our carefully designed network exhibits superior generalization and robustness, significantly improving deepfake detection performance.},
  archive      = {J_IETIP},
  author       = {Tianbo Zhai and Kaiyin Lu and Jiajun Li and Yukai Wang and Wenjie Zhang and Peipeng Yu and Zhihua Xia},
  doi          = {10.1049/ipr2.13276},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4666-4679},
  shortjournal = {IET Image Process.},
  title        = {Learning spatial-frequency interaction for generalizable deepfake detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PFFNet: A pyramid feature fusion network for microaneurysm
segmentation in fundus images. <em>IETIP</em>, <em>18</em>(14),
4653–4665. (<a href="https://doi.org/10.1049/ipr2.13275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal microaneurysm (MA) is a definite earliest clinical sigh of diabetic retinopathy (DR). Its automatic segmentation is key to realizing intelligent screening for early DR, which can significantly reduce the risk of visual impairment in patients. However, the minute scale and subtle contrast of MAs against the background pose challenges for segmentation. This paper focuses on automatic MA segmentation in fundus images. A novel pyramid feature fusion network (PFFNet) that progressively develops and fuses rich contextual information by integrating two pyramid modules is proposed. Multiple global pyramid scene parsing (GPSP) modules are introduced between the encoder and decoder to provide diverse global contextual information for the decoder through reconstructing skip connections. Additionally, a spatial scale-aware pyramid (SSAP) module is introduced to dynamically fuse multi-scale contextual information. This rich contextual information will help to identify MAs from low-contrast background. Furthermore, to mitigate issue related to category imbalance, a combo loss function is introduced. Finally, to validate the effectiveness of the proposed method, experiments are conducted on two publicly available datasets, IDRiD and DDR, and PFFNet is compared with several state-of-the-art models. The experimental results demonstrate the superiority of our PFFNet in the MA segmentation task.},
  archive      = {J_IETIP},
  author       = {Jiaxin Lu and Beiji Zou and Xiaoxia Xiao and Qinghua Peng and Junfeng Yan and Wensheng Zhang and Kejuan Yue},
  doi          = {10.1049/ipr2.13275},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4653-4665},
  shortjournal = {IET Image Process.},
  title        = {PFFNet: A pyramid feature fusion network for microaneurysm segmentation in fundus images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A small-target traffic sign detection algorithm based on
partial conv and atrous spatial pyramid. <em>IETIP</em>,
<em>18</em>(14), 4639–4652. (<a
href="https://doi.org/10.1049/ipr2.13274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign detection is essential to an intelligent driving assistance system. The deep learning-based algorithm proposed in this paper aims to address the issue of low detection accuracy caused by the small size and high density of traffic signs in real-world traffic scenarios. First, to improve the feature extraction module of the backbone network and to increase the model&#39;s ability to capture contextual information, partial convolution (PConv) is introduced. Second, to prevent information loss during the downsampling process, a cross-stage atrous spatial pyramid (ASPPFCSPC) is constructed using atrous convolution. This method combines feature map information from various scales and expands the receptive field. Lastly, the small-target detection precision is improved by incorporating an additional small-target detection head, which uses high-resolution feature maps for shallow features. The detection head is decoupled to extract the location and class information of the predicted target separately, thereby enhancing the generalization ability of the proposed model. Experiments have demonstrated the superiority of the proposed algorithm, as testing on the TT100K dataset resulted in a mAP@0.5 of 91.2% and a mAP@0.5:0.95 of 71.8% using the proposed algorithm.},
  archive      = {J_IETIP},
  author       = {Yuqi Li and Zijian Wang and Han Zhang and Xinpeng Yao and Zhou Zhou and Xin Cheng},
  doi          = {10.1049/ipr2.13274},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4639-4652},
  shortjournal = {IET Image Process.},
  title        = {A small-target traffic sign detection algorithm based on partial conv and atrous spatial pyramid},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metric-based pill recognition with the help of textual and
visual cues. <em>IETIP</em>, <em>18</em>(14), 4623–4638. (<a
href="https://doi.org/10.1049/ipr2.13273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pill image recognition by machine vision can reduce the risk of taking the wrong medications, a severe healthcare problem. Automated dispensing machines or home applications both need reliable image processing techniques to compete with the problem of changing viewing conditions, large number of classes, and the similarity in pill appearance. The problem is attacked with a multi-stream, two-phase metric embedding neural model. To enhance the metric learning procedure, dynamic margin setting is introduced into the loss function. Moreover, it is shown that besides the visual features of drug samples, even free text of drug leaflets (processed with a natural language model) can be used to set the value of the margin in the triplet loss and thus increase the recognition accuracy of testing. Thus, besides using the conventional metric learning approach, the given discriminating features can be explicitly injected into the metric model using the NLP of the free text of pill leaflets or descriptors of images of selected pills. The performance on two datasets is analysed and a 1.6% (two-sided) and 2.89% (one-sided) increase in Top-1 accuracy on the CURE dataset is reported compared to existing best results. The inference time on CPU and GPU makes the proposed model suitable for different kinds of applications in medical pill verification; moreover, the approach applies to other areas of object recognition where few-shot problems arise. The proposed high-level feature injection method (into a low-level metric learning model) can also be exploited in other cases, where class features can be well described with textual or visual cues.},
  archive      = {J_IETIP},
  author       = {Richárd Rádli and Zsolt Vörösházi and László Czúni},
  doi          = {10.1049/ipr2.13273},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4623-4638},
  shortjournal = {IET Image Process.},
  title        = {Metric-based pill recognition with the help of textual and visual cues},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unmanned aerial vehicle image stitching based on
multi-region segmentation. <em>IETIP</em>, <em>18</em>(14), 4607–4622.
(<a href="https://doi.org/10.1049/ipr2.13271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) image stitching is a key technology for aerial remote sensing applications. Most existing image stitching methods based on optimal seamline searching algorithms can eliminate defects such as ghosting and distortion in stitched images, which unfortunately suffer from the problem that the seamline may cross those regions with significant geometric misalignment between different images. Therefore, a novel image stitching method based on multi-region image segmentation is proposed. The algorithm starts by performing a multi-scale morphological reconstruction in the overlapping regions between UAV images to obtain superpixel images with precise contours. Then, the fast density peaks clustering based on K-nearest neighbours is applied to automatically determine the clustering centres and the number of clusters. By constructing a cost function, an energy map is generated in the overlapping regions between UAV images. Finally, the optimal seamline can be determined with a graph-cut method. Compared to several popular image stitching algorithms in real experiments, the proposed method can essentially prevent the seamline from crossing significant ground objects to ensure the integrity of structural objects while achieving satisfactory accuracy and efficiency during the UAV image stitching process.},
  archive      = {J_IETIP},
  author       = {Weidong Pan and Anhu Li and Xingsheng Liu and Zhaojun Deng},
  doi          = {10.1049/ipr2.13271},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4607-4622},
  shortjournal = {IET Image Process.},
  title        = {Unmanned aerial vehicle image stitching based on multi-region segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compressive sensing and DNA coding operation: Revolutionary
approach to colour medical image compression-encryption algorithm.
<em>IETIP</em>, <em>18</em>(14), 4589–4606. (<a
href="https://doi.org/10.1049/ipr2.13270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in medical imaging technology, colour medical images make lesion diagnosis more intuitive. However, when transmitting these high-capacity images, doctors and researchers must not only address the challenges of storage and transmission efficiency but also guard against unauthorized access and data security risks. To address these issues, a revolutionary approach for colour medical image compression encryption based on compressive sensing and deoxyribonucleic acid (DNA) coding operation is introduced in this study. Randomness and sparse optimizations are performed on three floating-point matrices obtained through discrete wavelet and sparse transforms of plain colour medical images by employing position scrambling and reduced-stiffness operations. Subsequently, the floating-point matrices are measured and quantized to generate three 8-bit integer matrices. Further, pixel-by-pixel DNA encoding, DNA-base scrambling, DNA XOR, and DNA decoding operations are performed to achieve DNA base-position scrambling and value diffusion. Finally, the regrouped bit planes help yield the compressed encrypted images. A comprehensive analysis of the proposed algorithm&#39;s encryption and decryption effectiveness, compression performance, and security was conducted. The results show that, with a compression ratio of 0.5, average PSNR = 42.7153 dB and average MSSIM = 0.9779, key space is , average entropy = 7.9986 bits, average histogram variance = 509.53, and the correlation coefficients are close to 0. Moreover, the algorithm shows some immunity to common cryptographic attacks, such as differential, known-plaintext, noise, and occlusion attacks. Thus, the proposed algorithm addresses the challenges posed by the sensitive nature of patient information and limited storage space.},
  archive      = {J_IETIP},
  author       = {Xianglian Xue and Haiyan Jin and Changjun Zhou},
  doi          = {10.1049/ipr2.13270},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4589-4606},
  shortjournal = {IET Image Process.},
  title        = {Compressive sensing and DNA coding operation: Revolutionary approach to colour medical image compression-encryption algorithm},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical multi-modal video summarization with dynamic
sampling. <em>IETIP</em>, <em>18</em>(14), 4577–4588. (<a
href="https://doi.org/10.1049/ipr2.13269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous video summarization methods often neglected inter-frame variations during the preprocessing stage. Sampling repeated frames can lead to information redundancy, while missing key frames can result in deviations in semantic comprehension and inaccuracies in the generated summaries. This work proposes a dynamic sampling module that leverages frame-level motion information to alleviate these issues. The module conducts high-frequency sampling during intervals with significant changes, allowing for a finer capture of details. Combined with a hierarchical multi-modal structure, it integrates shot-level visual and textual information to enhance the semantic understanding of video clips and improve the accuracy of the summarized content. Extensive experiments on benchmark datasets SumMe and TVSum demonstrate the effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Lingjian Yu and Xing Zhao and Liang Xie and Haoran Liang and Ronghua Liang},
  doi          = {10.1049/ipr2.13269},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4577-4588},
  shortjournal = {IET Image Process.},
  title        = {Hierarchical multi-modal video summarization with dynamic sampling},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task image restoration network based on spatial
aggregation attention and multi-feature fusion. <em>IETIP</em>,
<em>18</em>(14), 4563–4576. (<a
href="https://doi.org/10.1049/ipr2.13268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main purpose of image restoration is to recover high-quality image content from degraded versions. However, current mainstream models tend to focus solely on spatial details or contextual semantics, resulting in poor repair effects. To address this issue, a multi-task image repair network based on spatial aggregation attention and multi-feature fusion (SAAM) is proposed. It utilizes the global semantic information from the low-resolution subnetwork to guide the local feature extraction of the high-resolution subnetwork, thereby preserving the overall image structure while enhancing local details. Additionally, to enhance the model&#39;s understanding and representation capabilities of images, the feature fusion mechanism (FFM) is designed to merge feature information from different levels. Finally, the spatial aggregation attention mechanism SAAM enhances the accuracy and quality of image restoration by weighting the importance of different regions in the image at multiple scales. The experimental results demonstrate that the proposed SAAM method outperforms similar approaches in image denoising, deraining and decracking tasks in peak signal-to-noise ratio, structural similarity and learned perceptual image patch similarity metrics. The model also exhibits promising performance in restoring real old photos and murals which demonstrates its generalizability.},
  archive      = {J_IETIP},
  author       = {Chunyan Peng and Xueya Zhao and Yangbo Chen and Wanqing Zhang and Yuhui Zheng},
  doi          = {10.1049/ipr2.13268},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4563-4576},
  shortjournal = {IET Image Process.},
  title        = {Multi-task image restoration network based on spatial aggregation attention and multi-feature fusion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multiple-attention refinement network for RGB-d salient
object detection. <em>IETIP</em>, <em>18</em>(14), 4551–4562. (<a
href="https://doi.org/10.1049/ipr2.13267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing RGB-D salient object detection (SOD) algorithms, when using depth information for cross-modal fusion, result in significant information redundancy due to issues with low-quality depth information, ambiguity, and difficulty in discriminating complex scenes, ultimately leading to poor-quality saliency maps. This article proposes a Multiple-Attention Refinement Network (MARNet) to address the issues of insufficient cross-modal fusion and poor quality of depth images in RGB-D salient object detection. MARNet adopts an end-to-end structure and enables the fusion of cross-modal features through multiple-attention refinement and cross-attention fusion with each other. This article, in particular, designs an Attention Interaction Module (AIM), which uses multiple-attention and cross-attention to refine and fuse the two modalities, reducing the information redundancy generated during cross-modal interactions and background noise interference. This article designs a Multi-Scale Compensation Module (MSCM) to guide the multi-scale feature fusion step-by-step, enabling the fusion of local and global contexts of multi-scale features. Extensive experimental results demonstrate that the MARNet in this article has significant advantages over 16 state-of-the-art RGB-D methods on five publicly available datasets. The codes can be found at https://github.com/wzxxmj/MARNet .},
  archive      = {J_IETIP},
  author       = {Zijian Jiang and Ling Yu and Junru Li and Fanglin Niu},
  doi          = {10.1049/ipr2.13267},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4551-4562},
  shortjournal = {IET Image Process.},
  title        = {A multiple-attention refinement network for RGB-D salient object detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MRS-net: Brain tumour segmentation network based on feature
fusion and attention mechanism. <em>IETIP</em>, <em>18</em>(14),
4542–4550. (<a href="https://doi.org/10.1049/ipr2.13266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of brain tumor magnetic resonance imaging (MRI) is crucial for treatment planning. Addressing the challenges of complex tumor structures and inadequate cross-channel information utilization in Unet-based segmentation, this paper proposes the multi-scale residual brain tumor MRI segmentation network (MRS-Net) incorporating an attention mechanism to enhance segmentation accuracy. First, the double residual feature fusion module is utilized to enhance the fusion of feature information between different levels. Second, the Atrous Spatial Pyramid Pooling is introduced as a bridging module of the network to capture the features at different scales of the image, so as to enhance the extraction capability of the network for detailed features. Finally, the inverted residual coordinate attention module replaces the direct splicing in Unet to fuse the large feature information at each level and scale, thus enhancing the model&#39;s ability to recognize the spatial location information of brain tumors. The Dice coefficients, positive predictive values (PPVs), sensitivities (Sensitivity) and Hausdorff distance (HD), which are the four evaluation indexes, reach 84.54%, 87.43%, 88.37% and 2.248, respectively, which are improved by 1.85%, 2.11%, 2.88% and 6.0%, respectively, compared with Unet. The experimental results show that MRS-Net achieves better brain tumor image segmentation.},
  archive      = {J_IETIP},
  author       = {Xiaoyan Shen and Ju Wang and Yuhua Zhao and Rui Zhou and Han Gao and Jiakai Zhang and Hongming Shen},
  doi          = {10.1049/ipr2.13266},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4542-4550},
  shortjournal = {IET Image Process.},
  title        = {MRS-net: Brain tumour segmentation network based on feature fusion and attention mechanism},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DMA-net: A dual branch encoder and multi-scale cross
attention fusion network for skin lesion segmentation. <em>IETIP</em>,
<em>18</em>(14), 4531–4541. (<a
href="https://doi.org/10.1049/ipr2.13265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of skin lesion is an important step in computer-aided diagnosis. However, due to the significant variations in the size and shape of the lesion areas, as well as the low contrast with normal skin tissue, the boundaries are not clearly distinguishable, leading to a high possibility of incorrect segmentation. Therefore, this task is highly challenging. To overcome these difficulties, this paper proposes a medical image segmentation architecture named dual branch encoder and multi-scale cross attention fusion network, which includes a dual-branch encoder based on convolutional neural network and an improved channel-enhanced Mamba to comprehensively extract local and global information from dermoscopy images. Additionally, to enhance the feature interaction and fusion of local and global information, a multi-scale cross attention fusion module is adopted to cross-merge features in different directions and at different scales, maximizing the advantages of the dual-branch encoder and achieving precise segmentation of skin lesions. Extensive experiments are conducted on three public skin lesion datasets: ISIC-2018, ISIC-2017, and ISIC-2016, to verify the effectiveness and superiority of the proposed method. The dice similarity coefficient scores on the three datasets reached 81.77%, 81.68% and 85.60%, respectively, surpassing most state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Guangyao Zhai and Guanglei Wang and Qinghua Shang and Yan Li and Hongrui Wang},
  doi          = {10.1049/ipr2.13265},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4531-4541},
  shortjournal = {IET Image Process.},
  title        = {DMA-net: A dual branch encoder and multi-scale cross attention fusion network for skin lesion segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-path residual network for image denoising based on
edge prior information. <em>IETIP</em>, <em>18</em>(14), 4514–4530. (<a
href="https://doi.org/10.1049/ipr2.13264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-preserving denoising is important in image analysis. However, most existing methods suffer from smoothing and loss of high frequency detail at the edges. Aiming at this issue, a multi-path residual network using edge prior information is proposed. Specifically, the edge is first recovered by a newly designed parallel edge extraction module. Then it is used as a priori to generate affine transform parameters to guide the weight distribution between noise and edge, so that the shallow feature extraction pays more attention to preserving edges. In the deep feature extraction stage, a multi-scale attention unit is designed to provide the spatial neighbourhood information of the feature points to filter and activate the deep features, which further enhances the ability of the network to extract fine-grained noisy features. Finally, by taking the difference between the noisy image and the noise feature obtained with residual learning, the denoised image is produced. Extensive experiments show that the PSNR is improved by 0.00–0.78 dB on grey image denoising and 0.00–2.69 dB on colour image denoising compared with others. The denoised image has a better ability to preserve image edges, high frequency details and visual perception.},
  archive      = {J_IETIP},
  author       = {Xuefei Bai and Yongsong Wan and Weiming Wang},
  doi          = {10.1049/ipr2.13264},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4514-4530},
  shortjournal = {IET Image Process.},
  title        = {A multi-path residual network for image denoising based on edge prior information},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MW-SAM: Mangrove wetland remote sensing image segmentation
network based on segment anything model. <em>IETIP</em>,
<em>18</em>(14), 4503–4513. (<a
href="https://doi.org/10.1049/ipr2.13263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mangrove wetlands are important ecosystems in tropical and subtropical coastal areas, providing wind and wave attenuation and embankment protection functions. However, mangrove wetlands worldwide are facing severe loss and degradation. Accurate identification of mangrove wetland extent is crucial for their protection, but traditional methods struggle to meet the requirements of large-scale, high-precision identification. Furthermore, field data collection and annotation in wetlands in complex intertidal zones pose challenges, and the lack of high-quality training samples limits the application of deep learning methods in this field. This paper proposes a novel semantic segmentation framework for mangrove wetlands called mangrove wetland remote sensing image segmentation network based on segment anything model (MW-SAM) to address these issues. MW-SAM is based on the pre-trained SAM and achieves cross-domain adaptation through parameter-efficient fine-tuning techniques. It introduces wetland-specific prompts, auxiliary branches, semi-supervised training, and iterative optimization strategies to improve accuracy and tackle the problem of sample scarcity. Moreover, on the mangrove wetland dataset constructed in this paper, MW-SAM significantly outperforms SAM and other traditional methods. MW-SAM provides a new technology for monitoring and protecting mangrove wetlands, and the constructed dataset will be made publicly available, which is expected to promote the development of mangrove wetland conservation efforts.},
  archive      = {J_IETIP},
  author       = {Yu Zhang and Xin Wang and Jingye Cai and Qun Yang},
  doi          = {10.1049/ipr2.13263},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4503-4513},
  shortjournal = {IET Image Process.},
  title        = {MW-SAM: Mangrove wetland remote sensing image segmentation network based on segment anything model},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Map construction algorithm based on dense point cloud.
<em>IETIP</em>, <em>18</em>(14), 4492–4502. (<a
href="https://doi.org/10.1049/ipr2.13260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to address the issue of the difficulty in constructing maps from sensor-acquired point clouds, a map construction algorithm is proposed for the effective conversion from sparse point clouds to dense point clouds. First, it constructs a dense point cloud map in real-time using colour and depth information of keyframes. Then, it generates an accurate point cloud map by voxel filtering and coordinate conversion. Finally, the point cloud map is converted into an octree map using OctoMap. This article uses the TUM dataset for simulation analysis to verify the effectiveness of the algorithm. The results demonstrate that the algorithm can construct dense point cloud maps with high positioning accuracy in sparse point cloud scenarios. The algorithm improves the positioning accuracy by more than 15% under the fr1_xyz, fr1_room, fr1_desk2, fr1_desk1, fr2_desk and fr3_str_tex_near sequences, compared to ORB-SLAM2, ORB-SLAM3 and GX ORB-SLAM2 algorithm},
  archive      = {J_IETIP},
  author       = {Chengjie Liu and Xuemei Li and Shangsong Lv and Bin Liu and Min Li},
  doi          = {10.1049/ipr2.13260},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4492-4502},
  shortjournal = {IET Image Process.},
  title        = {Map construction algorithm based on dense point cloud},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid variational level set model based on double
gaussian distribution fitting energy for image segmentation and bias
correction. <em>IETIP</em>, <em>18</em>(14), 4461–4491. (<a
href="https://doi.org/10.1049/ipr2.13248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The variational level set model has been widely used in image segmentation. However, its performance is significantly hindered by bias fields and noise within the images. To address these limitations, we introduce a novel hybrid variational level set model based on dual Gaussian distribution fitting (DGDF) energy in this paper. The DGDF energy integrates both local and global Gaussian distributions. The local energy is derived from the original image, while the global energy employs the corrected image, enabling effective segmentation of images with intensity inhomogeneity. Furthermore, the model demonstrates low sensitivity to weighting parameters and robust performance for noisy images. We develop an alternating iteration algorithm that combines variational methods with gradient descent to efficiently solve the proposed model. Experimental results validate the effectiveness of the model and the algorithm. In addition, the proposed model shows competitiveness on test images and three datasets compared to several state-of-the-art models, including other variational level set models and deep learning-based techniques.},
  archive      = {J_IETIP},
  author       = {Liming Tang and Honglu Zhang and Yaya Xu and Yanjun Ren and Chunyan Li},
  doi          = {10.1049/ipr2.13248},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4461-4491},
  shortjournal = {IET Image Process.},
  title        = {A hybrid variational level set model based on double gaussian distribution fitting energy for image segmentation and bias correction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on deep learning based reenactment methods for
deepfake applications. <em>IETIP</em>, <em>18</em>(14), 4433–4460. (<a
href="https://doi.org/10.1049/ipr2.13201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the sectors that deep learning has transformed, deepfake, a novel method of manipulating multimedia, deserves particular attention. The long-term objective of many researchers is to seamlessly mimic human facial movement or whole-body activity, referred to as reenactment. Deepfake progress has made this goal much more feasible in recent years. Yet, achieving more realistic facial and body reenactment remains a challenging task. The primary focus of this study is to explore the current capability of the reenactment techniques and expand them further to attain greater results. The analysis offers a thorough overview of the various techniques involved, the challenges addressed, the datasets utilized, and the metrics employed by the underlying methods of reenactment technologies. The study also addresses the potential risks and their mitigating strategies to ensure responsible reenactment techniques. To the best of the authors&#39; knowledge, this is the first survey paper that delves deeper into the topic of deepfake reenactment.},
  archive      = {J_IETIP},
  author       = {Ramamurthy Dhanyalakshmi and Claudiu-Ionut Popirlan and Duraisamy Jude Hemanth},
  doi          = {10.1049/ipr2.13201},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4433-4460},
  shortjournal = {IET Image Process.},
  title        = {A survey on deep learning based reenactment methods for deepfake applications},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Front cover: Advances in medical image analysis: A
comprehensive survey of lung infection detection. <em>IETIP</em>,
<em>18</em>(13), i. (<a
href="https://doi.org/10.1049/ipr2.13281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  author       = {Shirin Kordnoori and Maliheh Sabeti and Hamidreza Mostafaei and Saeed Seyed Agha Banihashemi},
  doi          = {10.1049/ipr2.13281},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {i},
  shortjournal = {IET Image Process.},
  title        = {Front cover: advances in medical image analysis: a comprehensive survey of lung infection detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving YOLOv8 with parallel frequency channel attention
for taxi passengers. <em>IETIP</em>, <em>18</em>(13), 4422–4431. (<a
href="https://doi.org/10.1049/ipr2.13208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting taxi passengers is crucial for assessing taxi driver behavior, which plays a significant role in regulating the taxi industry. Despite the advancements in deep learning, object detection algorithms have not been extensively applied to this domain. In this article, an innovative taxi passenger detection algorithm is introduced based on YOLOv8, a lightweight and highly accurate method designed to automatically monitor driver behavior and regulate the taxi industry. To address the challenge of deploying complex object detection models on mobile devices, the ghost module is incorporated in place of standard convolutions within the C2f module, thereby making the model more lightweight. Furthermore, the model&#39;s performance is enhanced by integrating an improved version of Frequency Channel Attention (FCA), termed Parallel Frequency Channel Attention (PFCA), which boosts detection accuracy with minimal additional parameters and computational overhead. Experimental results on a specific taxi passenger dataset demonstrate that the proposed method significantly outperforms the baseline YOLOv8n model. Specifically, the model reduces the number of parameters and floating point operations by 12.96% and 8.18%, respectively, while achieving increases in mAP50 and mAP50-95 by 0.27 and 0.73 percentage points, respectively.},
  archive      = {J_IETIP},
  author       = {Qi Gao and Di He and Guilin Xu},
  doi          = {10.1049/ipr2.13208},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4422-4431},
  shortjournal = {IET Image Process.},
  title        = {Improving YOLOv8 with parallel frequency channel attention for taxi passengers},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A semi-supervised segmentation network fusing pseudo-label
with multi-level feature consistency correction for hard exudates.
<em>IETIP</em>, <em>18</em>(13), 4411–4421. (<a
href="https://doi.org/10.1049/ipr2.13262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely detection of hard exudates in fundus images can effectively avoid the severity of the disease, but the labelling of small and numerous lesion areas requires a lot of labour costs. This paper proposes a semi-supervised segmentation network, which integrates pseudo-labels and multi-level features consistency correction. It achieves accurate segmentation of hard exudates by making full use of a small amount of labelled data and a large amount of unlabelled data. The network effectively extracts features from the unlabelled data through knowledge transfer of the teacher-student model, and incorporates a Transformer network for auxiliary training to promote the quality of transfer. In addition, three unsupervised losses are introduced to improve the performance: the perturbation loss improves the robustness of the model to noise by adding different noises to the same input; the multi-level feature consistency correction loss ensures the consistency of features of the student model at different scales; and the pseudo-labelling cross-supervision loss utilizes the generated pseudo-labels for supervision between CNN and Transformer. By comparing the segmentation results with different proportion of the labelled data, it has better segmentation performance compared to other methods. The proposed methods can totally increase dice by 16.56% and mean intersection over union (MIoU) by 25.11%.},
  archive      = {J_IETIP},
  author       = {Xinfeng Zhang and Jiaming Zhang and Jie Shao and Hui Li and Xiaomin Liu and Maoshen Jia},
  doi          = {10.1049/ipr2.13262},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4411-4421},
  shortjournal = {IET Image Process.},
  title        = {A semi-supervised segmentation network fusing pseudo-label with multi-level feature consistency correction for hard exudates},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SFDiff: Diffusion model with sufficient spatial-fourier
frequency information interaction for low-light image enhancement.
<em>IETIP</em>, <em>18</em>(13), 4394–4410. (<a
href="https://doi.org/10.1049/ipr2.13259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models are increasingly applied in low-light image enhancement tasks due to their exceptional capability to model data distributions, but most current methods focus only on the original pixel space and neglect the potential of Fourier frequency information. In this article, SFDiff is proposed, a novel low-light image enhancement method that integrates Fourier frequency information into the diffusion process. Specifically, Fourier transforms are applied at both the image and feature levels to separately enhance the amplitude and phase components, which restores global illumination degradation and positional information. Then a Spatial-Frequency Fusion (SFF) block is used to fully integrate and interact with the information across spatial and frequency domains. Since illumination degradation is primarily manifested in the amplitude component, a loss function based on maximum likelihood learning is employed to constrain the amplitude component at each step of the sampling process, ensuring that the reverse process maintains an optimal trajectory. Owing to the streamlined network design and the fact that the Fourier transform requires no extra parameters, SFDiff achieves a reduction in parameters of over compared to several state-of-the-art (SOTA) diffusion models and delivers high-quality enhancement results on multiple real-world datasets. The code is available at https://github.com/MrWan001/SFDiff .},
  archive      = {J_IETIP},
  author       = {Fei Wan and Bingxin Xu and Jingli Yao and Lu Zeng and Weiguo Pan and Hongzhe Liu},
  doi          = {10.1049/ipr2.13259},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4394-4410},
  shortjournal = {IET Image Process.},
  title        = {SFDiff: Diffusion model with sufficient spatial-fourier frequency information interaction for low-light image enhancement},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Breaking the rain barrier: A novel approach in image
processing with AMGR-net. <em>IETIP</em>, <em>18</em>(13), 4381–4393.
(<a href="https://doi.org/10.1049/ipr2.13258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality is significantly impacted by rain, posing challenges in fields like surveillance, autonomous driving, and outdoor robotics. The field of image deraining, particularly for single image, has attracted considerable attention to improve image clarity in inclement weather. To overcome the inherent complexity of the single-image rain removal task, we proposed a novel architecture of the attention mechanism and gated recurrent network (AMGR-Net) that combines spatial and channel attention mechanisms with gated recurrent units. AMGR-Net contains multiple modules, each of which uses convolution kernels and attention mechanisms to enhance feature extraction. AMGR-Net demonstrates superior performance over state-of-the-art methods in both synthetic and real-world image datasets, as evidenced by higher peak signal to noise ratio and structural similarity index measurement scores. The integration of spatial attention significantly enhances feature expression, enabling more effective rain streak removal and detail preservation. Furthermore, this method also shows promising results in the application of stripe noise removal from meteorological satellite cloud images.},
  archive      = {J_IETIP},
  author       = {Hongxu Li and Wenpeng Zhang and Xueting Li and Chen Li},
  doi          = {10.1049/ipr2.13258},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4381-4393},
  shortjournal = {IET Image Process.},
  title        = {Breaking the rain barrier: A novel approach in image processing with AMGR-net},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Palette-based colour normalization for histopathology
images. <em>IETIP</em>, <em>18</em>(13), 4368–4380. (<a
href="https://doi.org/10.1049/ipr2.13257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of computer-aided diagnostic (CAD) systems, the speed of histopathological image analysis and the accuracy of cancer detection have considerably improved. However, the appearance of histopathological slides can vary depending on the consistency of tissue thickness, stain concentrations, and equipment, thus affecting the judgment of CAD systems. This study proposes a palette-based colour normalization method for histopathology images is proposed to solve the problem of colour differences between histopathology images. The method builds a graphical user interface based on an improved palette generation algorithm and colour transfer definitions, through which the user can complete the corresponding colour modification to achieve colour normalization. The evaluation of the metrics on histopathological images shows that our method is able to achieve higher image quality and better preservation of structural information of the source image compared with four other colour normalization algorithms. The peak signal-to-noise ratio values obtained by the proposed method on two publicly available datasets were 24.1914 and 21.3666, and the structural similarity index matrix values were 0.9871 and 0.9760. The proposed method provides new ideas for the development and design of CAD systems.},
  archive      = {J_IETIP},
  author       = {Shengzhe Shi and Yanyan Wang and Kaikai Zhang and Qingqing Wang and Sheng Liu and Chun Wang},
  doi          = {10.1049/ipr2.13257},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4368-4380},
  shortjournal = {IET Image Process.},
  title        = {Palette-based colour normalization for histopathology images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contraband detection algorithm for x-ray security inspection
images based on global semantic enhancement. <em>IETIP</em>,
<em>18</em>(13), 4356–4367. (<a
href="https://doi.org/10.1049/ipr2.13256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of low detection accuracy caused by overlapping occlusion and noise disturbance, a contraband detection algorithm for X-ray security inspection images based on global semantic enhancement is proposed to achieve accurate contraband target detection by enhancing global semantic information. First, the disturbance suppression module is used to weaken the noise disturbance at different positions by local suppression and aggregate finer detail information. Then, the parallel cascade search module is used to capture long-range dependencies and strengthen the representation of global semantic information, which helps the model identify contraband under overlapping occlusion. Finally, the contribution of different features is adaptively adjusted through the feature-weighted fusion module, which promotes the effective fusion of multi-scale features and improves the accuracy of model detection. The method in this article has been extensively evaluated and experimented on three mainstream benchmark datasets: SIXray, OPIXray, and PIDray. The mAPs of three datasets reach 93.5%, 91.9%, and 85.9%, respectively. The experimental results fully demonstrate that the method in this article has better performance compared with the latest method, which can meet the practical application requirements of real-time target detection.},
  archive      = {J_IETIP},
  author       = {Xiaofang Pei and Changsong Ma and Jin Zhou and Jihai Yang and Yongheng Xu},
  doi          = {10.1049/ipr2.13256},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4356-4367},
  shortjournal = {IET Image Process.},
  title        = {Contraband detection algorithm for X-ray security inspection images based on global semantic enhancement},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stroke-seg: A deep learning-based framework for chinese
stroke segmentation. <em>IETIP</em>, <em>18</em>(13), 4341–4355. (<a
href="https://doi.org/10.1049/ipr2.13255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese stroke segmentation is a crucial and challenging task for various downstream applications such as font generation, aesthetic evaluation etc. Conventional semantic segmentation techniques typically face difficulties in accurately segmenting strokes, as intersection regions in Chinese characters can belong to multiple strokes simultaneously, and these approaches often lack a holistic understanding of character composition. This paper proposes a character stroke segmentation framework named Stroke-Seg that integrates with various semantic segmentation architectures, demonstrating adaptability to different backbone networks to tackle the above tasks. A multi-label output strategy is proposed to effectively classify strokes in intersection areas, overcoming the limitations of traditional semantic segmentation approaches. Additionally, a prior knowledge vector is incorporated into the input layer to provide character-specific information on stroke composition, enhancing the ability of the framework to precisely identify and segment strokes. The effectiveness of the proposed framework is demonstrated through evaluations of a comprehensive dataset (brush calligraphy stroke segmentation dataset). Evaluations show that the proposed framework significantly improves the capability of semantic segmentation networks, achieving a remarkable improvement of up to 19.9% in the true stroke rate, compared to traditional stroke segmentation techniques. The Stroke-Seg framework integrated with TransUNet demonstrates its high performance with an impressive 98.2% true stroke rate. Furthermore, the proposed framework combined with FCN still achieves good performance while consuming the least amount of computational resources and memory, demonstrating its potential for lightweight design.},
  archive      = {J_IETIP},
  author       = {Xinyu Gong and Zeyang Bai and Haitao Nie and Bin Xie},
  doi          = {10.1049/ipr2.13255},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4341-4355},
  shortjournal = {IET Image Process.},
  title        = {Stroke-seg: A deep learning-based framework for chinese stroke segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MultiScale spectral–spatial convolutional transformer for
hyperspectral image classification. <em>IETIP</em>, <em>18</em>(13),
4328–4340. (<a href="https://doi.org/10.1049/ipr2.13254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the powerful ability in capturing the global information, transformer has become an alternative architecture of CNNs for hyperspectral image classification. However, general transformer mainly considers the global spectral information while ignores the multiscale spatial information of the hyperspectral image. In this paper, we propose a multiscale spectral–spatial convolutional transformer (MultiFormer) for hyperspectral image classification. First, the developed method utilizes multiscale spatial patches as tokens to formulate the spatial transformer and generates multiscale spatial representation of each band in each pixel. Second, the spatial representation of all the bands in a given pixel are utilized as tokens to formulate the spectral transformer and generate the multiscale spectral–spatial representation of each pixel. Besides, a modified spectral–spatial CAF module is constructed in the MultiFormer to fuse cross-layer spectral and spatial information. Therefore, the proposed MultiFormer can capture the multiscale spectral–spatial information and provide better performance than most of other architectures for hyperspectral image classification. Experiments are conducted over commonly used real-world datasets and the comparison results show the superiority of the proposed method.},
  archive      = {J_IETIP},
  author       = {Zhiqiang Gong and Xian Zhou and Wen Yao},
  doi          = {10.1049/ipr2.13254},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4328-4340},
  shortjournal = {IET Image Process.},
  title        = {MultiScale spectral–spatial convolutional transformer for hyperspectral image classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning informed diffusion equation model for image
denoising. <em>IETIP</em>, <em>18</em>(13), 4310–4327. (<a
href="https://doi.org/10.1049/ipr2.13253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is one of the fundamental problems in image processing. Convolutional neural network (CNN) based denoising approaches have achieved better performance than traditional methods, such as STROLLR and BM3D. However, CNNs can easily bring unexplainable artifacts to denoised images. In this article, a Deep Learning-Informed Diffusion Equation (DLI-DE) framework utilizing the image prior or the image gradient prior for image denoising is proposed. The image priors and gradient priors are learned from CNN models and used as coefficients in diffusion equations. The solution of DLI-DE is infinitely smooth from the uniqueness of existence theorem, which guarantees that the denoised image is free of artifacts. Good properties of DLI-DE also ensure high-quality of denoising. The experimental analysis confirms that the denoising performance of DLI-DE is comparable to that of contemporary CNN-based denoising methods such as TNRD and DnCNN, while effectively preventing artifacts.},
  archive      = {J_IETIP},
  author       = {Yao Li and Li Cheng and Zhichang Guo and Yuming Xing},
  doi          = {10.1049/ipr2.13253},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4310-4327},
  shortjournal = {IET Image Process.},
  title        = {Deep learning informed diffusion equation model for image denoising},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Super-resolution reconstruction algorithm for medical images
by fusion of wavelet transform and multi-scale adaptive feature
selection. <em>IETIP</em>, <em>18</em>(13), 4297–4309. (<a
href="https://doi.org/10.1049/ipr2.13252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional computed tomography (CT) images often suffer from blurred edges and unclear details. Image super-resolution methods can significantly enhance CT image quality, thereby improving diagnostic accuracy. To better extract detailed features and enhance the cascading effects of different feature levels, we propose a novel medical image super-resolution algorithm that integrates discrete wavelet transform and multi-scale adaptive feature selection. Our approach uses both the low-resolution image and its high-frequency component from the frequency domain as network inputs, with the high-frequency component providing learning supervision, which enhances detail fidelity in reconstruction. Additionally, we introduce a multi-scale adaptive feature selection module to learn from different layers of CT images and their inter-layer correlations. Finally, the pixel information is efficiently integrated by a coordinate attention mechanism incorporating the concept of squeeze excitation. Experimental results show that our method outperforms state-of-the-art methods, achieving superior reconstruction at scale factors of 2, 4, and 8, especially at scale factor 8, where it surpasses others by 1.12 in PSNR, 0.0145 in SSIM, and 0.0038 in LPIPS. Visually, our method also delivers more accurate details and better perceptual quality.},
  archive      = {J_IETIP},
  author       = {QiaoSu Wang and Qiaomei Ma},
  doi          = {10.1049/ipr2.13252},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4297-4309},
  shortjournal = {IET Image Process.},
  title        = {Super-resolution reconstruction algorithm for medical images by fusion of wavelet transform and multi-scale adaptive feature selection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fused score computation approach to reflect the overlap
between the predicted box and the ground truth in pedestrian detection.
<em>IETIP</em>, <em>18</em>(13), 4287–4296. (<a
href="https://doi.org/10.1049/ipr2.13251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pedestrian detection task, numerous predicted boxes and their corresponding scores are generated and these scores are used to filter these predicted boxes by non-maximum suppression. This paper analysed the training process of the popular anchor-based pedestrian detection models (e.g. YOLO and Faster RCNN), and found that the score of the predicted box reflects the overlap between the corresponding anchor and the ground truth, rather than the predicted box itself. Due to the many-to-one strategy adopted by anchor-based methods, multiple predicted boxes could be generated around one predicted box. This study refers to the number of other predicted boxes around the target predicted box as its local density. When a predicted box has a higher local density, it should have a greater overlap with the ground truth. Therefore, this study proposed the fused score by introducing local density into the score. The experiments showed that replacing the score with the fused score can effectively improve the model&#39;s detection accuracy. The code and experiments will soon be open-sourced at https://github.com/zefeichen/FusedScore .},
  archive      = {J_IETIP},
  author       = {Zefei Chen and Yongjie Lin and Jianmin Xu and Kai Lu and Zihao Huang},
  doi          = {10.1049/ipr2.13251},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4287-4296},
  shortjournal = {IET Image Process.},
  title        = {A fused score computation approach to reflect the overlap between the predicted box and the ground truth in pedestrian detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Surface defect detection method of aluminium alloy castings
based on data enhancement and CRT-DETR. <em>IETIP</em>, <em>18</em>(13),
4275–4286. (<a href="https://doi.org/10.1049/ipr2.13250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface quality of aluminium alloy castings is crucial to quality control. Aiming to address the challenges of limited samples and extensive computation in deep learning-based surface defect detection for aluminium alloy castings, this paper proposes a surface defect detection method based on data enhancement and the Casting Real-Time DEtection TRansformer. First, to tackle the issue of small sample sizes and uneven distribution in surface defect data sets of aluminium alloy castings, ECA-MetaAconC Deep Convolution Generative Adversarial Networks is proposed for generating defects with fewer samples and employ the image augmentation (IMGAUG) library for sample enhancement. Second, building upon the Real-Time DEtection TRansformer (RT-DETR), a lightweight partial-rep convolution is designed to decrease the network&#39;s parameter count. Simultaneously, the Deformable attention module and the DRBC3 module are introduced to enhance the neck network, thereby improving the model&#39;s capability to capture information and enhancing its detection performance. Compared to RT-DETR, this method reduces the number of model parameters by 38.7%, increases mAP by 1.5%, and achieves a frame rate that is 1.58 times higher than the original model. The experimental results demonstrate that this method can effectively and accurately detect surface defects in aluminium alloy castings, satisfying industrial requirements.},
  archive      = {J_IETIP},
  author       = {Long Li and Mingyang Gao and Xing Tian and Chengjun Wang and Jiangli Yu},
  doi          = {10.1049/ipr2.13250},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4275-4286},
  shortjournal = {IET Image Process.},
  title        = {Surface defect detection method of aluminium alloy castings based on data enhancement and CRT-DETR},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLO-RSFM: An efficient road small object detection method.
<em>IETIP</em>, <em>18</em>(13), 4263–4274. (<a
href="https://doi.org/10.1049/ipr2.13247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To tackle challenges in road multi-object detection, such as object occlusion, small object detection, and multi-scale object detection difficulties, a new YOLOv8n-RSFM structure is proposed. The key improvement of this structure lies in the introduction of the transformer decoder head, which optimizes the matching between the ground truth and predicted boxes, thereby effectively addressing issues of object overlap and multi-scale detection. Additionally, a small object detection layer is incorporated to retain crucial information beneficial for detecting small objects, significantly improving the detection accuracy for small targets. To enhance learning capacity and reduce redundant computations, the FasterNet backbone is employed to replace CSPDarknet53, thus accelerating the training process. Finally, the INNER-MPDIoU loss function is introduced to replace the original algorithm&#39;s complete IoU to accelerate the convergence and obtain more accurate regression results. A series of experiments were conducted on different datasets. The experimental results show that the proposed model YOLOv8N-RSFM outperforms the original model YOLOv8n in small target detection. On the VisDrone, TinyPerson, and VSCrowd datasets, the mean accuracy percentage improved by 7.9%, 12.3%, and 4.5%, respectively.},
  archive      = {J_IETIP},
  author       = {Pei Tang and Zhenyu Ding and Mao Lv and Minnan Jiang and Weikai Xu},
  doi          = {10.1049/ipr2.13247},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4263-4274},
  shortjournal = {IET Image Process.},
  title        = {YOLO-RSFM: An efficient road small object detection method},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Action recognition algorithm based on skeleton graph with
multiple features and improved adjacency matrix. <em>IETIP</em>,
<em>18</em>(13), 4250–4262. (<a
href="https://doi.org/10.1049/ipr2.13245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although graph convolutional networks have achieved good performances in skeleton-graph-based action recognition, there are still some problems which include the incomplete utilization of skeleton graph features and the lacking of logical adjacency information between nodes in adjacency matrix. In this article, a human action recognition algorithm is proposed based on multiple features from the skeleton graph to solve these problems. More specifically, an improved adjacency matrix is constructed to make full use of the multiple skeleton graph features. These features include local differential features, multi-scale edge features, features of the original skeleton graph, nodal features, and nodal motion features. Extensive results are conducted on four standard datasets (NTU RGB-D 60, NTU RGB-D 120, Kinetics, and Northwestern-UCLA). The experimental results show that the proposed algorithm outperforms the SOTA action recognition algorithms.},
  archive      = {J_IETIP},
  author       = {Shanqing Zhang and Shuheng Jiao and Yujie Chen and Jiayi Xu},
  doi          = {10.1049/ipr2.13245},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4250-4262},
  shortjournal = {IET Image Process.},
  title        = {Action recognition algorithm based on skeleton graph with multiple features and improved adjacency matrix},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information hiding using approximate POSIT representation.
<em>IETIP</em>, <em>18</em>(13), 4221–4249. (<a
href="https://doi.org/10.1049/ipr2.13244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {POSIT is a numerical system consists of sign, regime, exponential, and fraction bits to overcome some limitations of the IEEE-754 floating point (FP) representation. This work proposes a technique to hide critical information in the least significant bits (LSBs) of the POSIT FP representation by exploiting approximate computing (AC). The proposed technique, called information hiding using POSIT and LSB (IHUPL), explores the opportunities offered by both the POSIT representation and the AC to hide information with a minimum loss in accuracy and other performance metrics. IHUPL offers two options for hiding information: either by embedding one digit of each character into each pixel or by embedding all the digits of the character at the same time into each pixel of the alpha-red, green, blue, or black/white image. Experimental results are evaluated for benchmark images and showed that IHUPL enhanced the accuracy of embedding data into LSB of POSIT FP by an average of 5%, the image quality improvement rates of IHUPL are 19% and 16% for options 1 and 2, respectively. Besides the encoding method using IHUPL, the paper outlines an extraction decoding technique that saves the original replacement bits in a key-image to recover the hidden security message.},
  archive      = {J_IETIP},
  author       = {Sa&#39;ed Abed and Ghadeer Aldamkhi and Imtiaz Ahmad},
  doi          = {10.1049/ipr2.13244},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4221-4249},
  shortjournal = {IET Image Process.},
  title        = {Information hiding using approximate POSIT representation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep handwritten diagram segmentation. <em>IETIP</em>,
<em>18</em>(13), 4207–4220. (<a
href="https://doi.org/10.1049/ipr2.13243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwriting is a natural way to communicate and exchange ideas, but converting handwritten diagrams to application-specific digital formats requires skill and time. Automatic handwritten document conversion can save time, but diagrams and text require different recognition engines. Since accurate segmentation of handwritten diagrams can improve the accuracy of later diagram recognition steps, the authors propose to solve the problem of segmentation of text and non-text elements of handwritten diagrams using deep semantic segmentation. The model, DeepDP is a flexible U-net style architecture that can be tuned in complexity to a level appropriate for a particular dataset and diagram type. Experiments on a public hand-drawn flowchart dataset and a business process diagram dataset show excellent performance, with a per pixel accuracy of 98.6% on the public flowchart datasets and improvement over the 99.3% text stroke accuracy and 96.6% non-text stroke accuracy obtained by state of the art methods that use online stroke information. On the smaller offline business process diagram dataset, the method obtains a per-pixel accuracy of 96.9%.},
  archive      = {J_IETIP},
  author       = {Buntita Pravalpruk and Matthew N. Dailey},
  doi          = {10.1049/ipr2.13243},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4207-4220},
  shortjournal = {IET Image Process.},
  title        = {Deep handwritten diagram segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stegocrypt: A robust tri-stage spatial steganography
algorithm using TLM encryption and DNA coding for securing digital
images. <em>IETIP</em>, <em>18</em>(13), 4189–4206. (<a
href="https://doi.org/10.1049/ipr2.13242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research work presents a novel secured spatial steganography algorithm consisting of three stages. In the first stage, a secret message is divided into three parts, each is encrypted using a tan logistic map encryption key with a unique seed value. In the second stage, the encrypted parts are transformed into quick response codes, serving as a layer of channel coding. Subsequently, the quick response codes are decoded back into bit-streams. To enhance security, a uniquely-seeded Mersenne Twister key is generated and employed to apply DNA coding onto each bit-stream. The resulting bit-streams are then embedded in the least significant bits of the RGB channels of a cover image. Finally, the RGB channels are merged to form a single stego image. A comprehensive set of experimental analyses is conducted to evaluate the performance of the proposed secure steganography algorithm. The experimental results demonstrate the algorithm&#39;s robustness against various attacks and its ability to achieve high embedding capacity while maintaining imperceptibility. The proposed algorithm offers a promising solution for secure information hiding in the spatial domain, with potential applications in areas such as data transmission, digital forensics, and covert communication.},
  archive      = {J_IETIP},
  author       = {Wassim Alexan and Eyad Mamdouh and Amr Aboshousha and Youssef S. Alsahafi and Mohamed Gabr and Khalid M. Hosny},
  doi          = {10.1049/ipr2.13242},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4189-4206},
  shortjournal = {IET Image Process.},
  title        = {Stegocrypt: A robust tri-stage spatial steganography algorithm using TLM encryption and DNA coding for securing digital images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel target detection method with dual-domain
multi-frequency feature in side-scan sonar images. <em>IETIP</em>,
<em>18</em>(13), 4168–4188. (<a
href="https://doi.org/10.1049/ipr2.13241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Side-scan sonar (SSS) detection is a key method in underwater environmental security and subsea resource development. However, many detection approaches primarily concentrate on tracking the evolution path of optical image object detection tasks when using acoustic images, resulting in complex structures and limited versatility. To tackle this issue, we introduce a pioneering dual-domain multi-frequency network (D 2 MFNet) meticulously crafted to harness the distinct characteristics of SSS image detection. In D 2 MFNet, a novel method for optimizing and improving the detection sensitivity in different frequency ranges called multi-frequency combined attention mechanism (MFCAM) is proposed. This mechanism amplifies the relevance of dual-domain features across different channels and spaces. Moreover, we introduce a dual-domain feature pyramid network (D 2 FPN) significantly augments the depth and breadth of feature information in underwater small datasets. The methods offer plug-and-play functionality with substantial performance enhancements. Extensive experiments are conducted to validate the efficacy of the proposed techniques, and the results showcase their state-of-the-art performance. MFCAM improves the mAP by 16.9% in the KLSG dataset and 15.5% in the SCTD dataset. The mAP of D 2 FPN was improved by 8.4% in the KLSG dataset and by 9.8% in the SCTD dataset. The code and models will be publicly available at https://dagshub.com/estrellaww00/D2MFNet .},
  archive      = {J_IETIP},
  author       = {Wen Wang and Yifan Zhang and Houpu Li and Yixin Kang and Lei Liu and Cheng Chen and Guojun Zhai},
  doi          = {10.1049/ipr2.13241},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4168-4188},
  shortjournal = {IET Image Process.},
  title        = {A novel target detection method with dual-domain multi-frequency feature in side-scan sonar images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An algorithm for cattle counting in rangeland based on
multi-scale perception and image association. <em>IETIP</em>,
<em>18</em>(13), 4151–4167. (<a
href="https://doi.org/10.1049/ipr2.13240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively address common issues such as cattle being obscured by fences and images prone to colour shifts and high brightness in a ranch setting, this paper proposes an algorithm for counting cows based on multi-scale perception and image correlation. The algorithm first adjusts the model output scale to enhance cattle detection under current conditions. It incor-porates efficient Partial Convolution (PConv) to replace 3 × 3 convolutions in the Neck segment of the YOLOv7 network, boost-ing computational speed and reducing complexity. To streamline feature fusion, Dynamic Head (DyHead) unifies multiple at-tentional operations in the Neck segment, enhancing efficiency. Additionally, it introduces a novel bounding box similarity metric Minimum Point DioU (MPDIoU) based on minimum point distance, encompassing factors from existing loss functions, while simplifying computations. Experimental results demonstrate the algorithm significantly improves detection, achieving 98.8% accuracy, 99.0% recall, and a 92.1% mAP value. Compared with mainstream SOTA models, Precision increases by 0.4%, Recall by 2.0%, and mAP value by 2.2%. Model size decreases by 23.9%, parameter count by 23.0%, and computational load by 6.1%. the algorithm shows improvements across all indices, meeting the challenge of real-time cattle counting in ranches under complex conditions.},
  archive      = {J_IETIP},
  author       = {Bingxuan Li and Jiandong Fang and Yudong Zhao},
  doi          = {10.1049/ipr2.13240},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4151-4167},
  shortjournal = {IET Image Process.},
  title        = {An algorithm for cattle counting in rangeland based on multi-scale perception and image association},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bilateral fusion low-light image enhancement with implicit
information constraints. <em>IETIP</em>, <em>18</em>(13), 4141–4150. (<a
href="https://doi.org/10.1049/ipr2.13239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on low-light image enhancement focuses on improving image quality in dim conditions. Recently, deep learning has driven significant advancements, with many studies using neural networks to enhance low-light images. However, most focus on complex network designs to increase nonlinearity, often neglecting the implicit information from local image transformations. This paper introduces an improved U-net-based method for low-light enhancement, retaining the original encoding network and adding branch links in the decoding network. The method uses attention feature fusion to handle image noise and gradients separately, adjusting brightness through a gradient-adaptive transform. This approach optimizes performance using loss functions like peak signal-to-noise ratio and colour consistency. Unlike previous methods, the approach emphasizes extracting implicit information from image gradients, achieving enhancement that aligns with the original brightness distribution. The result is enhanced images with high detail similarity to the original, achieved through end-to-end inference in experiments.},
  archive      = {J_IETIP},
  author       = {Jiahui Zhu and Shengbo Sang and Aoqun Jian and Le Yang and Luxiao Sang and Yang Ge and Rihui Kang and LiuWei Yang and Lei Tao and RunFang Hao},
  doi          = {10.1049/ipr2.13239},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4141-4150},
  shortjournal = {IET Image Process.},
  title        = {Bilateral fusion low-light image enhancement with implicit information constraints},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modality separation approach for facial sketch synthesis.
<em>IETIP</em>, <em>18</em>(13), 4127–4140. (<a
href="https://doi.org/10.1049/ipr2.13238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technology for face-to-sketch synthesis transforms optical face images into a sketch-style format. However, traditional style losses are insufficient to discern the modal differences between optical and sketch domain images, leading to unclear images. At the same time, generated images lack clarity due to traditional approaches&#39; disregard for high-frequency texture. To address these issues, a modality separation approach for facial sketch synthesis is proposed. First, a modality separation structure is proposed, using a quicksort algorithm to merge features of optical and sketch images as target modality (positive samples), ensuring the generated images&#39; feature distribution matches real sketches. By controlling the Euclidean distance between generated images (anchors) and both target and filtered modality (positive and negative samples), irrelevant information is effectively filtered out. Next, an edge-promoting module feeds processed blurry sketch images into the discriminator to enhance robustness. Lastly, a detail optimization module uses Laplacian filtering to extract high-frequency texture from optical face images for local enhancement. Experimental validation on CUHK, AR, and XM2VTS datasets shows that this method outperforms mainstream sketch face synthesis methods in terms of Fréchet inception distance and learned perceptual image patch similarity, producing more realistic and natural images with richer texture details.},
  archive      = {J_IETIP},
  author       = {Kangning Du and Chaoyi Wang and Lin Cao and Yanan Guo and Wenwen Sun},
  doi          = {10.1049/ipr2.13238},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4127-4140},
  shortjournal = {IET Image Process.},
  title        = {A modality separation approach for facial sketch synthesis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Against linkage: A novel generative face anonymization
framework with style diversification. <em>IETIP</em>, <em>18</em>(13),
4114–4126. (<a href="https://doi.org/10.1049/ipr2.13237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the digital era, millions of facial images are shared online daily, posing severe privacy threats. Generative face anonymization (GFA) methods generate virtual faces to conceal original identities, protecting sensitive information while preserving utility. However, deep learning based user identity linkage (UIL) methods can link similar faces to the same identity and leverage the linked profiles for malicious purposes, including localization and behaviour prediction. These UIL methods pose a significant challenge to the diversity of virtual faces, a challenge that existing GFA methods have not adequately addressed. To address this research gap, we propose Style Diversification-based Generative Face Anonymization (SD-GFA), a framework that generates virtual faces with diverse identities and high visual quality. SD-GFA features an equalized control module to balance input faces and user-specified keys, a face generation module with a re-connection strategy for high-quality synthesis, and a maximum probability simulation module to enhance diversity. Our experiments demonstrate that SD-GFA effectively mitigates linkage risk by improving the diversity of virtual faces, while also enhancing their utility and visual quality. This study provides a robust solution to enhance the security of anonymized faces shared on the internet.},
  archive      = {J_IETIP},
  author       = {Mingcheng Zhu and Peisong He and Yuhao Zhang and Jinghan Li and Yupeng Qiu},
  doi          = {10.1049/ipr2.13237},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4114-4126},
  shortjournal = {IET Image Process.},
  title        = {Against linkage: A novel generative face anonymization framework with style diversification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-domain pseudo-reference quality evaluation for
infrared and visible image fusion. <em>IETIP</em>, <em>18</em>(13),
4095–4113. (<a href="https://doi.org/10.1049/ipr2.13236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion involves merging the advantages of infrared and visible images to generate a composite image that encompasses thermal radiation as well as intricate texture details. Infrared and visible image fusion has garnered increasing attention, with numerous fusion methods proposed. However, how to fairly perceive the performance of fused image remains a contentious topic. This paper is dedicated to solving this problem from two perspectives (e.g., subjective and objective aspects). Firstly, an infrared and visible fusion image quality assessment dataset was constructed, including 60 pairs of infrared and visible images captured in various scenes, along with 540 fusion images with different types and degrees of distortions. Additionally, a subjective evaluation dataset of 16,200 subjective scores by 30 participants was further provided for the fused image. Secondly, to overcome the challenging assessment for infrared and visible fusion images without a real reference image, an interesting multi-domain pseudo-reference image quality assessment model (MPIQAM) is proposed, by comprehensively considering the thermal radiation information distortion, texture information distortion, and overall naturalness of the fused image. The proposed MPIQAM was compared with 18 mainstream objective metrics, and the experimental findings showcased a commendable level of competitiveness.},
  archive      = {J_IETIP},
  author       = {Xiangchao Meng and Chaoqi Chen and Qiang Liu and Feng Shao},
  doi          = {10.1049/ipr2.13236},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4095-4113},
  shortjournal = {IET Image Process.},
  title        = {Multi-domain pseudo-reference quality evaluation for infrared and visible image fusion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dunhuang mural inpainting based on reference guidance and
multi-scale fusion. <em>IETIP</em>, <em>18</em>(13), 4081–4094. (<a
href="https://doi.org/10.1049/ipr2.13235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the inadequate utilization of prior information in current mural inpainting processes, leading to issues such as semantically unreliable inpaintings and the presence of artifacts in the inpainting area, a Dunhuang mural inpainting method based on reference guidance and multi-scale feature fusion is proposed. First, the simulated broken mural, the mask image, and the reference mural are input into the model to complete the multi-level embedding of patches and align the multi-scale fine-grained features of damaged murals and reference murals. Following the patch embedding module, a hybrid residual module is added based on hybrid attention to fully extract mural features. In addition, by continuing the residual concatenation of outputs of the hierarchical embedding module improves the ability of the model to represent deeper features, and improves the robustness and generalisation of the model. Second, the encoded features are fed into the decoder to generate decoded features. Finally, the convolutional tail is employed to propagate them and complete the mural painting. Experimental validation on the Dunhuang mural dataset demonstrates that, compared to other algorithms, this model exhibits higher evaluation metrics in the inpainting of extensively damaged murals and demonstrates overall robustness. In terms of visual effects, the results of this model in the inpainting process exhibit finer textures, richer semantic information, more coherent edge structures, and a closer resemblance to authentic murals.},
  archive      = {J_IETIP},
  author       = {Zhongmin Liu and Yaolong Li},
  doi          = {10.1049/ipr2.13235},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4081-4094},
  shortjournal = {IET Image Process.},
  title        = {Dunhuang mural inpainting based on reference guidance and multi-scale fusion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recognition of vehicle license plates in highway scenes with
deep fusion network and connectionist temporal classification.
<em>IETIP</em>, <em>18</em>(13), 4066–4080. (<a
href="https://doi.org/10.1049/ipr2.13234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {License plate recognition is crucial in Intelligent Transportation Systems (ITS) for vehicle management, traffic monitoring, and security inspection. In highway scenarios, this task faces challenges such as diversity, blurriness, occlusion, and illumination variation of license plates. This article explores Recurrent Neural Networks based on Connectionist Temporal Classification (RNN-CTC) for license plate recognition in challenging highway conditions. Four neural network models: ResNet50, ResNeXt, InceptionV3, and SENet, all combined with RNN-CTC are comparatively evaluated. Furthermore, a novel architecture named ResNet50 Deep Fusion Network using Connectionist Temporal Classification (ResNet50-DFN-CTC) is proposed. Comparative and ablation experiments are conducted using the Highway License Plate Dataset of Southeast University (HLPD-SU). Results demonstrate the superior performance of ResNet50-DFN-CTC in challenging highway conditions, achieving 93.158% accuracy with a processing time of 7.91 ms, outperforming other tested models. This research contributes to advancing license plate recognition technology for real-world highway applications under adverse conditions.},
  archive      = {J_IETIP},
  author       = {Liru Hua and Xinyi Ma and Chihang Zhao and Bailing Zhang and Zijun Su and Yuhang Wu},
  doi          = {10.1049/ipr2.13234},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4066-4080},
  shortjournal = {IET Image Process.},
  title        = {Recognition of vehicle license plates in highway scenes with deep fusion network and connectionist temporal classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing precision in medical image segmentation: A
performance analysis of loss functions for COVID-19 lung infection
segmentation in computed tomography images. <em>IETIP</em>,
<em>18</em>(13), 4047–4065. (<a
href="https://doi.org/10.1049/ipr2.13232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study evaluates the effectiveness of three loss functions Asymmetric Unified Focal Loss (AUFL), Dice Similarity Coefficient Loss (DSCL), and Cross-Entropy (CE) for segmenting COVID-19 lung infections in computed tomography images. Detailed analyses using the intersection over union metric assessed each function&#39;s accuracy. AUFL achieved an average Dice Similarity Coefficient (DSC) of 85.18% ± 8.86%, outperforming DSCL 85.18% ± 8.86%, which had the same average DSC but less precise segmentation, and CE, which had an average DSC of 78.31% ± 11.93%. Segmentations using AUFL demonstrated more defined contours and better alignment with actual anatomical structures than those obtained with DSCL and CE. Observations revealed that AUFL-generated segmentations had more precise boundaries and were more consistent with the expected anatomical regions of lung infections. This study is the first to quantitatively and qualitatively compare the effectiveness of AUFL, DSCL, and CE in segmenting COVID-19 lung infections, providing concrete evidence of AUFL&#39;s superiority in segmentation performance and reliability for clinical applications. The findings underscore the importance of selecting appropriate loss functions to enhance segmentation in medical imaging, highlighting their crucial role in improving image-based diagnostics and treatment. The study emphasizes the need for ongoing research to optimize these segmentation techniques further.},
  archive      = {J_IETIP},
  author       = {Emilio Delgado and Roberto Rodriguez-Echeverria and Antonio Jesús Fernández-García and Juan D. Gutiérrez and Miguel Ángel Suero-Rodrigo},
  doi          = {10.1049/ipr2.13232},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4047-4065},
  shortjournal = {IET Image Process.},
  title        = {Advancing precision in medical image segmentation: A performance analysis of loss functions for COVID-19 lung infection segmentation in computed tomography images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Underwater organisms detection algorithm based on
multi-scale perception and representation enhancement. <em>IETIP</em>,
<em>18</em>(13), 4032–4046. (<a
href="https://doi.org/10.1049/ipr2.13231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address issues such as object-background confusion and difficulties in multi-scale object feature extraction in underwater scenarios, this article proposes an underwater organisms detection algorithm based on multi-scale perception and representation enhancement. The key innovation of the proposed algorithm is the perception improvement of the deep learning model for underwater multi-scale objects. First, for underwater large-scale objects, omni-dimensional dynamic convolution is embedded as an attention mechanism (AM) into the deep network to improve the network&#39;s sensitivity to large-scale underwater objects. For underwater small-scale objects, an information retention downsampling module is designed to reduce the effects of serious information loss. Then, a contextual transformer as an AM is introduced into shallow networks to strengthen the network&#39;s ability to extract features from small objects. The second innovation of the proposed algorithm is an underwater spatial pooling pyramid module which enhances the representation ability of the model. Furthermore, a lightweight decoupled head is designed to eliminate the conflict between classification and localization. The ablation experiment on the URPC dataset shows that the proposed models are effective for underwater object detection. The comparative experiments on the URPC and DUT-USEG datasets demonstrate that the proposed algorithm achieves an advantage in detection performance compared with the mainstream detection algorithms and underwater detection algorithms.},
  archive      = {J_IETIP},
  author       = {Jiawei Xu and Fen Chen and Lian Huang and Tingna Liu and Zongju Peng},
  doi          = {10.1049/ipr2.13231},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4032-4046},
  shortjournal = {IET Image Process.},
  title        = {Underwater organisms detection algorithm based on multi-scale perception and representation enhancement},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FM-YOLOv8: Lightweight gesture recognition algorithm.
<em>IETIP</em>, <em>18</em>(13), 4023–4031. (<a
href="https://doi.org/10.1049/ipr2.13229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical production applications, the efficiency and success rate of gesture recognition directly affect the user experience and work efficiency. However, the existing gesture recognition models have the problem of a large number of model parameters and high computational complexity, which makes them unable to meet the needs of end-to-end industrial deployment. To solve these problems, this article proposes a gesture recognition model based on YOLOv8. First, FasterNet is adopted as the backbone of YOLOv8, which significantlydecrease the number of parameters and made the model more lightweight. By reducing the number of parameters, the computational complexity of the model can be reduced while maintaining the performance of the model, and the operation efficiency of the model can be improved. Second, recombination convolution ScConv is introduced to replace common convolution operations to further improve the model&#39;s efficiency. Recombination convolution can reduce the computation and make up for the loss of precision to some extent. Finally, the MDPIoU loss function is used to optimize target location and prediction, to improve the accuracy of the model. The MDPIoU loss function can better deal with the problem of target boundary frame positioning and prediction so that the model can locate and predict gestures more accurately in gesture recognition tasks. Experiments on a data set containing 10 types of gestures show that the number of parameters and floating point calculations of the improved network model are reduced by 45% and 42.7%, respectively, while the accuracy is unchanged. The improved model can be deployed on edge terminals, providing efficient and accurate gesture recognition.},
  archive      = {J_IETIP},
  author       = {Fanghai Li and Xitai Na and Jinshuo Shi and Qingbin Sun},
  doi          = {10.1049/ipr2.13229},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4023-4031},
  shortjournal = {IET Image Process.},
  title        = {FM-YOLOv8: Lightweight gesture recognition algorithm},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight object detection approach based on edge
computing for mining industry. <em>IETIP</em>, <em>18</em>(13),
4005–4022. (<a href="https://doi.org/10.1049/ipr2.13228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coal Mining enterprises deploy numerous monitoring devices to ensure safe and efficient production using target detection technologies. However, deploying deep detection models on edge devices poses challenges due to high computational loads, impacting detection speed and accuracy. A mining target detection dataset has been created to address these issues, featuring key targets in coal mining scenes such as miners, safety helmets, and coal gangue. A model is proposed to improve real-time performance for edge mining detection tasks. Detection performance is enhanced by incorporating a Pixel-wise Normalization Spatial Attention Module (PN-SAM) into the MobileNet-v3 bneck structure and replacing the h-swish activation function with Mish, providing more prosperous gradient information transfer. The proposed model, YOLO-v4-LSAM, shows a 3.2% mAP improvement on the VOC2012 dataset and a 2.4% improvement on the mining target dataset compared to YOLO-v4-Tiny, demonstrating its effectiveness in mining environments. These enhancements enable more accurate and efficient detection in resource-constrained edge environments, contributing to safer and more reliable monitoring in coal mining operations.},
  archive      = {J_IETIP},
  author       = {Muhammad Wahab Hanif and Zhanli Li and Zhenhua Yu and Rehmat Bashir},
  doi          = {10.1049/ipr2.13228},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {4005-4022},
  shortjournal = {IET Image Process.},
  title        = {A lightweight object detection approach based on edge computing for mining industry},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised contextual cognitive augmentation-based
cross-teaching network for multiclass medical image segmentation.
<em>IETIP</em>, <em>18</em>(13), 3989–4004. (<a
href="https://doi.org/10.1049/ipr2.13227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of medical image segmentation technology enables accurate localization of human tissues, providing doctors with a reliable foundation for diagnosis. While deep learning methods have proven effective in this task, most current approaches rely on a single prediction framework, which overlooks Edge semantic features and results in flawed texture features. Moreover, existing supervised methods face challenges due to limited availability of high-quality annotations in the field of medical imaging. In this article, a Semi-supervised Contextual Cognitive Augmentation-based Cross-teaching Network is proposed. A Contextual Cognitive Enhancement Module is introduced consisting of two components: data augmentation and information extraction. The data augmentation component provides multi-level data distribution by incorporating diverse perturbation strategies such as Discrete Cosine Transform and Gaussian noise. The information extraction component employs the Comprehensive Information Extraction module, which consists of Global Perception Information Extraction module and Multi-channel Information Extraction module to extract perceptual information from images and enhance interaction between image channels, respectively. Additionally, a cross-teaching strategy is adopted and a hybrid loss function is utilized to encourage knowledge sharing among the networks, leveraging the advantages of dual networks for improved performance. Experimental results demonstrate significant enhancements in multiclass medical image segmentation compared to several state-of-the-art single-framework networks.},
  archive      = {J_IETIP},
  author       = {Di Gai and Yuxuan Wu and Yusong Xiao and Yuhan Geng and Lei Cao and Xin Xiong and An-qi Zhong},
  doi          = {10.1049/ipr2.13227},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3989-4004},
  shortjournal = {IET Image Process.},
  title        = {Semi-supervised contextual cognitive augmentation-based cross-teaching network for multiclass medical image segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Enhancing low-light images with lightweight fused
fixed-directional filters network. <em>IETIP</em>, <em>18</em>(13),
3976–3988. (<a href="https://doi.org/10.1049/ipr2.13226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has made significant progress in the field of low-light image enhancement. However, challenges remain, such as the substantial parameter consumption required for effective image enhancement. Inspired by multi-scale geometric transformations in image detail enhancement, a novel model called the fixed-directional filters network is proposed. Fixed-directional filters network takes the original image as input and employs multiple branches for parallel processing. One branch uses conventional convolutional layers to extract features from the original image, while the other branches apply non-linear mapping layers based on wavelet transforms. These wavelet transform branches capture the multi-scale information of the image by combining different directions and convolutional kernels and utilize a trainable custom gamma mapping layer for non-linear modulation to enhance specific regions of the image. The feature maps processed by each branch are merged through concatenation operations and then passed through convolutional layers to output the enhanced image. Using trainable mapping functions alone to enhance details significantly reduces the reliance on convolutional layers, effectively lowering the model&#39;s parameter count to only 13k parameters. Additionally, experiments demonstrate that fixed-directional filters network significantly improves image quality, particularly in capturing image details and enhancing image contrast.},
  archive      = {J_IETIP},
  author       = {Yang Li},
  doi          = {10.1049/ipr2.13226},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3976-3988},
  shortjournal = {IET Image Process.},
  title        = {Enhancing low-light images with lightweight fused fixed-directional filters network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hiding image with inception transformer. <em>IETIP</em>,
<em>18</em>(13), 3961–3975. (<a
href="https://doi.org/10.1049/ipr2.13225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganography aims to hide secret data in the cover media for covert communication. Though many deep-learning-based image steganography methods have been presented, these approaches suffer from the inefficiency of building long-distance connections between the cover and secret images, leading to noticeable modification traces and poor steganalysis resistance. To improve the visual imperceptibility of generated stego images, it is essential to establish a global correlation between the cover and secret images. In this way, the secret image can be dispersed throughout the cover image globally. To bridge this gap, a novel image steganography framework called HiiT is proposed, which takes advantage of CNN and Transformer to learn both the local and global pixel correlation in image hiding. Specifically, a new Transformer structure called Inception Transformer is proposed, which incorporates the Inception Net in the attention-based Transformer architecture. The Inception Net can learn different scaled image features using multiple convolution kernels, while the attention mechanism can learn the global pixel correlation. By this, the proposed Inception Transformer learns the long-distance pixel dependency between the cover and secret images. Furthermore, we propose a ‘Skip Connection’ mechanism in the proposed Inception Transformer, which merges the low-level visual features and high-level semantic features and achieves better model performance. In detail, The HiiT generates higher-quality stego images with 45.46 PSNR and 0.9915 SSIM. Besides, accurately restored secret images achieve 47.27 PSNR and 0.9952 SSIM. Extensive experimental results show the proposed HiiT significantly improves the image-hiding performance compared with state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Yunyun Dong and Ping Wei and Ruxin Wang and Bingbing Song and Tingchu Wei and Wei Zhou},
  doi          = {10.1049/ipr2.13225},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3961-3975},
  shortjournal = {IET Image Process.},
  title        = {Hiding image with inception transformer},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SEY-net: Semantic edge y-shaped network for pancreas
segmentation. <em>IETIP</em>, <em>18</em>(13), 3950–3960. (<a
href="https://doi.org/10.1049/ipr2.13222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pancreas segmentation has great significance in computer-aided diagnosis of pancreatic diseases. The small size of the pancreas, high variability in shape, and blurred edges make the task of pancreas segmentation challenging. A new model called SEY-Net is proposed to solve the above problems, which is a one-stage model with multi-inputs. SEY-Net is composed of three main components. Firstly, the edge information extraction (EIE) module is designed to improve the segmentation accuracy of the pancreas boundary. Then, the SE_ResNet50 is selected as the encoder&#39;s backbone to fit the size of the pancreas. Finally, the dual cross-attention is integrated into the skip connection to better focus on the variable shape of the pancreas. The experimental results shows that the proposed method has better performance and outperforms the other existing state-of-the-art pancreas segmentation methods.},
  archive      = {J_IETIP},
  author       = {Bangyuan Zhou and Guojiang Xin and Hao Liang and Changsong Ding},
  doi          = {10.1049/ipr2.13222},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3950-3960},
  shortjournal = {IET Image Process.},
  title        = {SEY-net: Semantic edge Y-shaped network for pancreas segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual knowledge-guided two-stage model for precise small
organ segmentation in abdominal CT images. <em>IETIP</em>,
<em>18</em>(13), 3935–3949. (<a
href="https://doi.org/10.1049/ipr2.13221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-organ segmentation from abdominal CT scans is crucial for various medical examinations and diagnoses. Despite the remarkable achievements of existing deep-learning-based methods, accurately segmenting small organs remains challenging due to their small size and low contrast. This article introduces a novel knowledge-guided cascaded framework that utilizes two types of knowledge—image intrinsic (anatomy) and clinical expertise (radiology)—to improve the segmentation accuracy of small abdominal organs. Specifically, based on the anatomical similarities in abdominal CT scans, the approach employs entropy-based registration techniques to map high-quality segmentation results onto inaccurate results from the first stage, thereby guiding precise localization of small organs. Additionally, inspired by the practice of annotating images from multiple perspectives by radiologists, novel Multi-View Fusion Convolution (MVFC) operator is developed, which can extract and adaptively fuse features from various directions of CT images to refine segmentation of small organs effectively. Simultaneously, the MVFC operator offers a seamless alternative to conventional convolutions within diverse model architectures. Extensive experiments on the Abdominal Multi-Organ Segmentation (AMOS) dataset demonstrate the superiority of the method, setting a new benchmark in the segmentation of small organs.},
  archive      = {J_IETIP},
  author       = {Tao Liu and Xukun Zhang and Zhongwei Yang and Minghao Han and Haopeng Kuang and Shuwei Ma and Le Wang and Xiaoying Wang and Lihua Zhang},
  doi          = {10.1049/ipr2.13221},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3935-3949},
  shortjournal = {IET Image Process.},
  title        = {Dual knowledge-guided two-stage model for precise small organ segmentation in abdominal CT images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anisotropic sparse transformation for spectral CT image
reconstruction. <em>IETIP</em>, <em>18</em>(13), 3916–3934. (<a
href="https://doi.org/10.1049/ipr2.13217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photon-counting detector-based computed tomography (PCD-CT) is an advanced realization of spectral CT, the multi-energy projection data is captured from the same object, hence, CT images can provide additional spectral resolution, making it possible to perform material decomposition. However, multiple projections may have a low signal-to-noise ratio (SNR), such that CT images suffer from noise. To handle this problem, a spectral CT image reconstruction method based on anisotropic sparse transformation (AST) is proposed. To increase the quality of reconstruction, AST through an anisotropic guided filter (AGF) and quasi norm is proposed. Then, as a new regularization, AST is introduced into an iterative reconstruction process, generating an AST-based method. Moreover, to utilize the correlation among projection data, the average image serves as the guidance image of AGF, it varies with the iterative index, resulting in a technique of dynamic average image (DAI). The AST-based model involves quasi norm minimization, hence an effective strategy is employed to solve the corresponding problem. A series of experiments were performed. The experiment showed that, compared to other listed methods, the result of the AST-based method can achieve better visualization and higher quantitative indexes, hence it has application potential in the medical imaging field.},
  archive      = {J_IETIP},
  author       = {ZhaoJun Yang and Li Zeng and Wei Yu and Qiong Xu and Zhe Wang and YuanWei He and Wei Chen},
  doi          = {10.1049/ipr2.13217},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3916-3934},
  shortjournal = {IET Image Process.},
  title        = {Anisotropic sparse transformation for spectral CT image reconstruction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Swin-fisheye: Object detection for fisheye images.
<em>IETIP</em>, <em>18</em>(13), 3904–3915. (<a
href="https://doi.org/10.1049/ipr2.13216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisheye cameras have been widely used in autonomous navigation, visual surveillance, and automatic driving. Due to severe geometric distortion, fisheye images cannot be processed effectively by conventional methods. The existing object detection algorithms cannot better detect the small targets or the objects with large distortion in the fisheye images. The size and scene of available fisheye datasets (such as WoodScape and VOC-360) cannot satisfy the training of robust network models. Herein, the authors propose Swin-Fisheye, an end-to-end object detection algorithm based on Swin Transformer. A feature pyramid module based on deformable convolution (DFPM) is designed to obtain richer contextual information from the multi-scale feature maps. In addition, a projection transformation algorithm (PTA) is proposed, which can convert rectilinear images into fisheye images more accurately, and then create a fisheye image dataset (COCO-Fish). The results of extensive experiments conducted on VOC-360, WoodScape, and COCO-Fish demonstrate that the proposed algorithm can achieve satisfactory results compared with state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Dawei Zhang and Tingting Yang and Bokai Zhao},
  doi          = {10.1049/ipr2.13216},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3904-3915},
  shortjournal = {IET Image Process.},
  title        = {Swin-fisheye: Object detection for fisheye images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Explicit–implicit symmetric diffeomorphic deformable image
registration with convolutional neural network. <em>IETIP</em>,
<em>18</em>(13), 3892–3903. (<a
href="https://doi.org/10.1049/ipr2.13215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image registration is essential and a key step in many advanced medical image tasks. In recent years, medical image registration has been applied to many clinical diagnoses, but large deformation registration is still a challenge. Deep learning-based methods typically have higher accuracy but do not involve spatial transformation, which ignores some desirable properties, including topology preservation and the invertibility of transformation, for medical imaging studies. On the other hand, diffeomorphic registration methods achieve a differentiable spatial transformation, which guarantees topology preservation and invertibility of transformation, but registration accuracy is low. Therefore, a diffeomorphic deformation registration with CNN is proposed, based on a symmetric architecture, simultaneously estimating forward and inverse deformation fields. CNN with Efficient Channel Attention is used to better capture the spatial relationship. Deformation fields are optimized explicitly and implicitly to enhance the invertibility of transformations. An extensive experimental evaluation is performed using two 3D datasets. The proposed method is compared with different state-of-the-art methods. The experimental results show excellent registration accuracy while better guaranteeing the diffeomorphic transformation.},
  archive      = {J_IETIP},
  author       = {Longhao Li and Li Li and Yunfeng Zhang and Fangxun Bao and Xunxiang Yao and Zewen Zhang and Weilin Chen},
  doi          = {10.1049/ipr2.13215},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3892-3903},
  shortjournal = {IET Image Process.},
  title        = {Explicit–implicit symmetric diffeomorphic deformable image registration with convolutional neural network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-image encryption and authentication using
computational ghost imaging and singular value decomposition.
<em>IETIP</em>, <em>18</em>(13), 3878–3891. (<a
href="https://doi.org/10.1049/ipr2.13214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-local imaging characteristics of computational ghost imaging (CGI) make it widely used in the field of information security. However, the information processing method based on traditional CGI has some problems in the process of multi-image encryption, such as low efficiency and unsatisfactory reconstruction quality. Therefore, this article proposes to use wavelet transform and singular value decomposition technology to help computational ghost imaging to efficiently complete the coding and encryption of multiple images, and use alternating direction multiplier method (ADMM) in the process of image restoration to carry out high-quality reconstruction of the original encrypted image. The results show that the scheme has good reconstruction effect, strong security and robustness even at a low sampling rate. It provides some reference for digital watermarking technology and cryptography.},
  archive      = {J_IETIP},
  author       = {Hong Huang and ZhiGuang Han},
  doi          = {10.1049/ipr2.13214},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3878-3891},
  shortjournal = {IET Image Process.},
  title        = {Multi-image encryption and authentication using computational ghost imaging and singular value decomposition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Target localization and defect detection of distribution
insulators based on ECA-SqueezeNet and CVAE-GAN. <em>IETIP</em>,
<em>18</em>(13), 3864–3877. (<a
href="https://doi.org/10.1049/ipr2.13213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insulators, as typical equipment for distribution networks, provide good electrical insulation between live conductors and earth. Timely and accurate detection is essential for insulator detection issues. However, as the complexity of neural networks increases, the detection efficiency is often lower. Therefore, this paper proposes a fast insulator positioning and defect detection method. Firstly, for insulator target localization, the SqueezeNet network is improved using ECA attention mechanism. In addition, to address the issue of low defect detection accuracy, a joint algorithm has been proposed. The integration of convolutional variational autoencoder (CVAE) and generative adversarial network (GAN) solve their own shortcomings due to different image focus angles. The target localization accuracy reaches 94.30%, and the defect detection accuracy reaches 89.60%. It solves the problems of difficulty in locating small targets in a large field of view and inaccurate detection due to a small number of abnormal samples. This method has been tried and tested in practical distribution network systems.},
  archive      = {J_IETIP},
  author       = {Chao Zhang and Yu Liu and Honggang Liu},
  doi          = {10.1049/ipr2.13213},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3864-3877},
  shortjournal = {IET Image Process.},
  title        = {Target localization and defect detection of distribution insulators based on ECA-SqueezeNet and CVAE-GAN},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved protection for colour images via invertible
colour-to-grey, image distortion, and visible-imperceptible
watermarking. <em>IETIP</em>, <em>18</em>(13), 3854–3863. (<a
href="https://doi.org/10.1049/ipr2.13212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invertible colour-to-grey algorithms protect the colour of an image against illegal usage and distribution. These schemes hide the colour information into a grayscale version of the image, making colour recovery without proper authorization challenging. A previous study proposed the incorporation of content security by distorting the protected image to hinder illegal structure restoration. This work addresses the limitations of this method, significantly improving both processing time and the quality of the recovered colour image. Specifically, processing times are reduced from 45 h to 9 s, and the mean PSNR value is increased by up to 2.23 dB. Furthermore, this proposal achieves competitive results compared to existing approaches that do not implement content security, obtaining mean peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) values above 42.32 dB and 0.992, respectively. Finally, quantitative results are supported by visual comparisons.},
  archive      = {J_IETIP},
  author       = {Eduardo Fragoso-Navarro and Francisco Garcia-Ugalde and Manuel Cedillo-Hernandez},
  doi          = {10.1049/ipr2.13212},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3854-3863},
  shortjournal = {IET Image Process.},
  title        = {Improved protection for colour images via invertible colour-to-grey, image distortion, and visible-imperceptible watermarking},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Glaucoma detection with explainable AI using convolutional
neural networks based feature extraction and machine learning
classifiers. <em>IETIP</em>, <em>18</em>(13), 3827–3853. (<a
href="https://doi.org/10.1049/ipr2.13211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma is an eye disease that damages the optic nerve as a result of vision loss, it is the leading cause of blindness worldwide. Due to the time-consuming, inaccurate, and manual nature of traditional methods, automation in glaucoma detection is important. This paper proposes an explainable artificial intelligence (XAI) based model for automatic glaucoma detection using pre-trained convolutional neural networks (PCNNs) and machine learning classifiers (MLCs). PCNNs are used as feature extractors to obtain deep features that can capture the important visual patterns and characteristics from fundus images. Using extracted features MLCs then classify glaucoma and healthy images. An empirical selection of the CNN and MLC parameters has been made in the performance evaluation. In this work, a total of 1,865 healthy and 1,590 glaucoma images from different fundus datasets were used. The results on the ACRIMA dataset show an accuracy, precision, and recall of 98.03%, 97.61%, and 99%, respectively. Explainable artificial intelligence aims to create a model to increase the user&#39;s trust in the model&#39;s decision-making process in a transparent and interpretable manner. An assessment of image misclassification has been carried out to facilitate future investigations.},
  archive      = {J_IETIP},
  author       = {Vijaya Kumar Velpula and Diksha Sharma and Lakhan Dev Sharma and Amarjit Roy and Manas Kamal Bhuyan and Sultan Alfarhood and Mejdl Safran},
  doi          = {10.1049/ipr2.13211},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3827-3853},
  shortjournal = {IET Image Process.},
  title        = {Glaucoma detection with explainable AI using convolutional neural networks based feature extraction and machine learning classifiers},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RDMS: Reverse distillation with multiple students of
different scales for anomaly detection. <em>IETIP</em>, <em>18</em>(13),
3815–3826. (<a href="https://doi.org/10.1049/ipr2.13210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection, often approached as a one-class classification problem, is a critical task in computer vision. Knowledge distillation has emerged as a promising technique for enhancing anomaly detection accuracy, especially with the advent of reverse distillation networks that employ encoder–decoder architectures. This study introduces a novel reverse knowledge distillation framework known as RDMS, which incorporates a pretrained teacher encoding module, a multi-level feature fusion connection module, and a student decoding module consisting of three independent decoders. RDMS is designed to distill distinct features from the teacher encoder, mitigating overfitting issues associated with similar or identical teacher–student structures. The model achieves an average of 99.3 image-level AUROC and 98.34 pixel-level AUROC on the MVTec-AD dataset and demonstrates state-of-the-art performance on the more challenging BTAD dataset. The RDMS model&#39;s high accuracy in anomaly detection and localization underscores the potential of multi-student reverse distillation to advance unsupervised anomaly detection capabilities. The source code is available at https://github.com/zihengchen777/RDMS},
  archive      = {J_IETIP},
  author       = {Ziheng Chen and Chenzhi Lyu and Lei Zhang and ShaoKang Li and Bin Xia},
  doi          = {10.1049/ipr2.13210},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3815-3826},
  shortjournal = {IET Image Process.},
  title        = {RDMS: Reverse distillation with multiple students of different scales for anomaly detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RIHINNet: A robust image hiding method against JPEG
compression based on invertible neural network. <em>IETIP</em>,
<em>18</em>(13), 3801–3814. (<a
href="https://doi.org/10.1049/ipr2.13209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hiding is a task that embeds secret images in digital images without being detected. The performance of image hiding has been greatly improved by using the invertible neural network. However, current image hiding methods are less robust in the face of Joint Photographic Experts Group (JPEG) compression. The secret image cannot be extracted from the stego image after JPEG compression of the stego image. Some methods show good robustness for some certain JPEG compression quality factors but poor robustness for other common JPEG compression quality factors. An image-hiding network (RIHINNet) that is robust to all common JPEG compression quality factors is proposed. First of all, the loss function is redesigned; thus, the secret image is hidden as much as possible in the area that is less likely to be changed after JPEG compression. Second, the classifier is designed, which can help the model to select the extractor according to the range of JPEG compression degree. Finally, the interval robustness of the secret image extraction is improved through the design of a denoising module. Experimental results show that this RIHINNet outperforms other state-of-the-art image-hiding methods in the face of JPEG compressed noise with random compression quality factors, with more than 10 dB peak signal-to-noise ratio improvement in secret image recovery on ImageNet, COCO and DIV2K datasets.},
  archive      = {J_IETIP},
  author       = {Xin Jin and Chengyi Pan and Zien Cheng and Yunyun Dong and Qian Jiang},
  doi          = {10.1049/ipr2.13209},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3801-3814},
  shortjournal = {IET Image Process.},
  title        = {RIHINNet: A robust image hiding method against JPEG compression based on invertible neural network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Advances in medical image analysis: A comprehensive survey
of lung infection detection. <em>IETIP</em>, <em>18</em>(13), 3750–3800.
(<a href="https://doi.org/10.1049/ipr2.13246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigates advanced approaches in medical image analysis, specifically focusing on segmentation and classification techniques, as well as their integration into multi-task architectures for lung infections. This research begins by explaining key architectural models used in segmentation and classification tasks. The study extends to the enhancement of these architectures through attention modules and conditional random fields. Relevant datasets and evaluation metrics, incorporating discussions on loss functions are also reviewed. This review encompasses recent advancements in single-task and multi-task models, highlighting innovations in semi-supervised, self-supervised, few-shot, and zero-shot learning techniques. Empirical analysis is conducted on both single-task and multi-task architectures, predominantly utilizing the U-Net framework, and is applied across multiple datasets for segmentation and classification tasks. Results demonstrate the effectiveness of these models and provide insights into the strengths and limitations of different approaches. This research contributes to improved detection and diagnosis of lung infections by offering a comprehensive overview of current methodologies and their practical applications.},
  archive      = {J_IETIP},
  author       = {Shirin Kordnoori and Maliheh Sabeti and Hamidreza Mostafaei and Saeed Seyed Agha Banihashemi},
  doi          = {10.1049/ipr2.13246},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3750-3800},
  shortjournal = {IET Image Process.},
  title        = {Advances in medical image analysis: A comprehensive survey of lung infection detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skin cancer identification utilizing deep learning: A
survey. <em>IETIP</em>, <em>18</em>(13), 3731–3749. (<a
href="https://doi.org/10.1049/ipr2.13219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Melanoma, a highly prevalent and lethal form of skin cancer, has a significant impact globally. The chances of recovery for melanoma patients substantially improve with early detection. Currently, deep learning (DL) methods are gaining popularity in assisting with the early identification of melanoma. Despite their high performance, relying solely on an image classifier undermines the credibility of the application and makes it difficult to understand the rationale behind the model&#39;s predictions highlighting the need for Explainable AI (XAI). This study provides a survey on skin cancer identification using DL techniques utilized in studies from 2017 to 2024. Compared to existing survey studies, the authors address the latest related studies covering several public skin cancer image datasets and focusing on segmentation, classification based on convolutional neural networks and vision transformers, and explainability. The analysis and the comparisons of the existing studies will be beneficial for the researchers and developers in this area, to identify the suitable techniques to be used for automated skin cancer image classification. Thereby, the survey findings can be used to implement support applications advancing the skin cancer diagnosis process.},
  archive      = {J_IETIP},
  author       = {Dulani Meedeniya and Senuri De Silva and Lahiru Gamage and Uditha Isuranga},
  doi          = {10.1049/ipr2.13219},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3731-3749},
  shortjournal = {IET Image Process.},
  title        = {Skin cancer identification utilizing deep learning: A survey},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to “attention-generative adversarial networks for
simulating rain field.” <em>IETIP</em>, <em>18</em>(12), 3729. (<a
href="https://doi.org/10.1049/ipr2.13137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  doi          = {10.1049/ipr2.13137},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3729},
  shortjournal = {IET Image Process.},
  title        = {Correction to “Attention-generative adversarial networks for simulating rain field”},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time fire and smoke detection with transfer learning
based on cloud-edge collaborative architecture. <em>IETIP</em>,
<em>18</em>(12), 3716–3728. (<a
href="https://doi.org/10.1049/ipr2.13187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen increased interest in object detection-based applications for fire detection in digital images and videos from edge devices. The environment&#39;s complexity and variability often lead to interference from factors such as fire and smoke characteristics, background noise, and camera settings like angle, sharpness, and exposure, which hampers the effectiveness of fire detection applications. Limited picture data for fire and smoke scenes further challenges model accuracy and robustness, resulting in high false detection and leakage rates. To address the need for efficient detection and adaptability to various environments, this paper focuses on (1) proposing a cloud-edge collaborative architecture for real-time fire and smoke detection, incorporating an iterative transfer learning strategy based on user feedback to enhance adaptability; (2) improving the detection capabilities of the base model YOLOv8 by enhancing the data augmentation method and introducing the coordinate attention mechanism to improve global feature extraction. The improved algorithm shows a 2-point accuracy increase. After three iterations of transfer learning in the production environment, accuracy improves from 93.3% to 96.4%, and mAP0.5:0.95 increases by nearly 5 points. This program effectively addresses false detection issues in fire and smoke detection systems, demonstrating practical applicability.},
  archive      = {J_IETIP},
  author       = {Ming Yang and Songrong Qian and Xiaoqin Wu},
  doi          = {10.1049/ipr2.13187},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3716-3728},
  shortjournal = {IET Image Process.},
  title        = {Real-time fire and smoke detection with transfer learning based on cloud-edge collaborative architecture},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLODCC: Improved YOLOv8 combined with dynamic confidence
compensation for lightweight moving object detection. <em>IETIP</em>,
<em>18</em>(12), 3699–3715. (<a
href="https://doi.org/10.1049/ipr2.13207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most multiple object tracking algorithms depend on the output of the detector. Aiming at the problem that the higher detection quality model is restricted by the computing power, and the robustness of the lightweight detection model is easily affected by motion blur, this paper proposes a lightweight moving object detector based on improved YOLOv8 combined with dynamic confidence compensation algorithm. The algorithm combines various technical means such as network structure optimization, lightweight design, self-knowledge distillation, loss function improvement and dynamic confidence compensation. ByteTrack is used as a tracker to conduct experiments on PASCAL VOC07+12 data set and UA-DETRAC test sequence. Compared with the baseline YOLOv8n+ByteTrack, the proposed algorithm improves the HOTA by 1.3% when the single frame tracking delay is reduced by 1.1%. Mostly tracked target is improved by 79.7%, mostly lost target is reduced by 10.9%, and the detection effect is better than the original detector and other popular object detectors. The YOLODCC model achieves a balance between lightweight and multi-object motion blur.},
  archive      = {J_IETIP},
  author       = {Dongting Zhang and Hongbin Ma},
  doi          = {10.1049/ipr2.13207},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3699-3715},
  shortjournal = {IET Image Process.},
  title        = {YOLODCC: Improved YOLOv8 combined with dynamic confidence compensation for lightweight moving object detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual information perception system of coal mine
comprehensive excavation working face for edge computing terminal.
<em>IETIP</em>, <em>18</em>(12), 3681–3698. (<a
href="https://doi.org/10.1049/ipr2.13206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of low detection accuracy, high computational complexity and long-time consumption of visual perception model in a complex mining environment, this research designs a visual information perception system of coal mine comprehensive excavation working face for an edge computing terminal. Firstly, the C3-Fast feature extraction module, spatial pyramid pooling with cross-stage partial connection (SPPCSPC) pooling module, bi-directional feature pyramid network and lightweight decoupled detection head are used to optimize the YOLOv5s model, so as to construct the FSBD-YOLOv5s multi-object detection model. Secondly, the pruning and distillation algorithm is used to lighten the FSBD-YOLOv5s model, and the model complexity is greatly reduced while maintaining the model detection accuracy. Further, the lightweight FSBD-YOLOv5s model is migrated and deployed to the edge computing terminal platform and the TensorRT engine is used to accelerate model inference. Finally, experiments are carried out based on the data set of the coal mine comprehensive excavation working face. The experimental results show that on the edge computing terminal platform, the parameters and computational volume of the lightweight FSBD-YOLOv5s model are reduced by 50.8% and 34.0%, while its detection accuracy and speed reach 94.0% and 43.7 fps, which can fully satisfy the requirements of the accuracy and real-time for the coal mine engineering applications.},
  archive      = {J_IETIP},
  author       = {Dongyang Zhao and Guoyong Su and Pengyu Wang},
  doi          = {10.1049/ipr2.13206},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3681-3698},
  shortjournal = {IET Image Process.},
  title        = {Visual information perception system of coal mine comprehensive excavation working face for edge computing terminal},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FlexibleCP: A data augmentation strategy for traffic sign
detection. <em>IETIP</em>, <em>18</em>(12), 3667–3680. (<a
href="https://doi.org/10.1049/ipr2.13204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of traffic sign detection, effective data augmentation can improve the model&#39;s detection capacity, enabling the model to distinguish and locate traffic signs more precisely and enhancing driving safety. However, due to the small size and low representation of traffic signs in the dataset, standard common data augmentation techniques are not suitable for traffic sign detection. To address this issue, a novel data augmentation strategy called flexible cut and paste (FlexibleCP) is proposed. The overall enhancement approach is shifted from multi-image fusion to target cropping and pasting. By introducing parameters to control the target pasting ratio and scaling ratio, the diversity of small target data and their size variations are enriched. Additionally, target size and type filters are added to enable targeted enhancement for different sizes and types of targets. This study, evaluates the proposed strategy using two representative traffic sign detection datasets, namely CTSD and GTSDB. The experimental results demonstrate a significant improvement in both detection and recognition performance of the model: on the CTSD dataset, the models trained with FlexibleCP data enhancement achieve 88.9% and 64.5% mAP0.5 and mAP0.5:0.95, respectively, which are 3.5% and 2.5% better than those trained with mosaic data enhancement; on the GTSDB dataset mAP0.5 and mAP0.5:0.95 reached 89.2% and 56.0%, respectively, an improvement of 4.0% and 3.9% over mosaic.},
  archive      = {J_IETIP},
  author       = {Jingyi Shi and Huanle Rao and Qinyang Jing and Ziqiang Wen and Gangyong Jia},
  doi          = {10.1049/ipr2.13204},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3667-3680},
  shortjournal = {IET Image Process.},
  title        = {FlexibleCP: A data augmentation strategy for traffic sign detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global information aware network with global interaction
graph attention for infrared small target detection. <em>IETIP</em>,
<em>18</em>(12), 3650–3666. (<a
href="https://doi.org/10.1049/ipr2.13203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting small targets in infrared images is crucial for ground surveillance and air traffic control. However, distinguishing small infrared targets from similar backgrounds is challenging due to their lack of structural and textural characteristics. To address these challenges, this study proposes a novel global information-aware network with global interaction graph attention (GIGA) for infrared small target detection. The GIGA consists of a global interaction layer (GILayer), graph attention weights (GAW), and a global relational learning (GRL) module. Specifically, the GILayer dynamically learns global inter-pixel relationships of small target images by enhancing the dependencies between feature dimensions. The GAW component calculates pixel-by-pixel similarity across the entire feature map using graph attention mechanisms, while the GRL module retains critical similarity features in the feature extraction network, thereby facilitating small target detection. Additionally, the multi-scale context fusion module utilises self-attention and dilation convolution to complement richer feature details at different scales. Experimental results on both natural and synthetic datasets demonstrate the proposed method&#39;s superiority over other state-of-the-art conventional and deep learning approaches in infrared small target detection.},
  archive      = {J_IETIP},
  author       = {Ruimin Yang and Yidan Zhang and Guangshuai Gao and Liang Liao and Chunlei Li},
  doi          = {10.1049/ipr2.13203},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3650-3666},
  shortjournal = {IET Image Process.},
  title        = {Global information aware network with global interaction graph attention for infrared small target detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FishFocusNet: An improved method based on YOLOv8 for
underwater tropical fish identification. <em>IETIP</em>,
<em>18</em>(12), 3634–3649. (<a
href="https://doi.org/10.1049/ipr2.13202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately identifying tropical fish serves as a crucial indicator, offering an insight into the state of marine biodiversity and the condition of coral reef ecosystems. However, the current detection networks are prone to omission and misidentification due to occlusion between fish and the complex underwater environment. This paper proposes a modified approach named FishFocusNet, in which alterable kernel convolution modules, asymptotic feature pyramid network (AFPN), and Shape-IoU are integrated into YOLOv8. To extract a more comprehensive set of fish features, AKConv modules with arbitrary kernel sizes are proposed to take the place of the conventional fixed-shaped kernels in the backbone for downsampling. AFPN is adopted as the feature integration structure in the neck, which enhances feature fusion and adaptive spatial fusion between non-adjacent layers. In the detector head, Shape-IoU is employed to achieve precise localization of fish targets. The superiorities of these modifications are proved by ablation experiments and comparative experiments. The experimental results show that the optimized approach obtained an mAP of 81.8% accompanied by 2.4 MB parameters and 3.6 GB FLOPS. Meanwhile, compared with more complicated models of similar scale, the proposed method can enhance recognition accuracy to 84.2% and significantly reduce computational costs.},
  archive      = {J_IETIP},
  author       = {Zhaoxuan Lu and Xiaolong Zhu and Haitao Guo and Xingang Xie and Xiangzi Chen and Xiangqian Quan},
  doi          = {10.1049/ipr2.13202},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3634-3649},
  shortjournal = {IET Image Process.},
  title        = {FishFocusNet: An improved method based on YOLOv8 for underwater tropical fish identification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M-SSD based on anchor proposal and ResNet101 backbone for
placental haemorrhage MRI detection. <em>IETIP</em>, <em>18</em>(12),
3617–3633. (<a href="https://doi.org/10.1049/ipr2.13198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MRI (magnetic resonance imaging) images can effectively show the placental haemorrhage area. In view of the special properties and real-time detection requirements of placental haemorrhage MRI images, this paper has systematically improved the single-shot multi-box detector (SSD) target detection algorithm (M-SSD). First, taking advantage of the particularity of the MRI image, the maximum stable extremum region (MSER) algorithm was used as the anchor proposal network which integrated the proposal information into the feature layer of SSD to avoid the hungry traversal of the original algorithm. Second, after the scale statistics of the placental haemorrhage area in MRI images, the bounding box matching the size of the placental haemorrhage area was redefined, in this way, the scale of the bounding box will have application pertinence, which can effectively improve the detection accuracy of the algorithm. Third, due to the small target property of the placental haemorrhage area in the MRI image, the VGG16 basic network in the original SSD was replaced by ResNet101, this made the algorithm have higher performance in small target detection. Finally, the Placental Haemorrhage MRI Detection Database (PHMD) has been built which is not only a base for this paper, but also for further research in this area.},
  archive      = {J_IETIP},
  author       = {Heng Zhang and Dong Wang and Faming Shao and Juying Dai and Tao Zhang},
  doi          = {10.1049/ipr2.13198},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3617-3633},
  shortjournal = {IET Image Process.},
  title        = {M-SSD based on anchor proposal and ResNet101 backbone for placental haemorrhage MRI detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Insulator detection based on FA-YOLO network with improved
feature extraction ability. <em>IETIP</em>, <em>18</em>(12), 3600–3616.
(<a href="https://doi.org/10.1049/ipr2.13197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle insulator detection that aims to recognize defective insulators from transmission lines has made significant progress in recent years. However, it still faces challenges, such as the complex background of aerial images and the small memory of unmanned aerial vehicles. This paper proposes a refined insulator detection algorithm that integrates the attention mechanism in YOLOv8 to improve the feature extraction ability. Specifically, this paper introduces a fast vision transformers structure in the you only look once (YOLO) v8 backbone section to enhance feature extraction by capturing local and global features. Additionally, the global attention mechanism is incorporated in the neck for additional feature extraction by merging comprehensive spatial and channel information into the output. Furthermore, we amalgamate depth-wise convolution, graph convolution, and residual operation in the global attention mechanism module. This design can mitigate the issues of gradient vanishing or exploding and meanwhile enhance the distinction between spatial attention and channel attention. The proposed model is then applied to a public dataset and a set of real images from a specific power station, and the detection results show that it outperforms many competitors in terms of accuracy, efficiency, and memory size.},
  archive      = {J_IETIP},
  author       = {Yixiao Jing and Tao Huang and Linfeng Gao and Jiangli Deng},
  doi          = {10.1049/ipr2.13197},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3600-3616},
  shortjournal = {IET Image Process.},
  title        = {Insulator detection based on FA-YOLO network with improved feature extraction ability},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CF-net: Cross fusion network for semantic segmentation.
<em>IETIP</em>, <em>18</em>(12), 3587–3599. (<a
href="https://doi.org/10.1049/ipr2.13196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a fundamental computer vision task, and deep learning methods have been successfully applied to this field. However, target morphology continues to exhibit the incomplete prediction problem, which is attributable to the low feature utilisation and the insufficiency of spatial location information. This paper proposes a novel cross fusion network with unit attention mechanism (CF-Net) for semantic segmentation. The two hallmarks of the framework are the usage of a multi-scale fusion module and the unit attention mechanism. Multi-scale fusion module can integrate multi-branch outputs with different receptive fields, which obtain fine-grained target details and visual contextual information. The cross fusion network is optimised with a unit attention mechanism to fuse intermediate features, which enables the acquisition of more accurate and effective spatial location information while maintaining consistency in feature space. The experimental results demonstrate that the proposed CF-Net outperforms favourably comparable with other existing methods on the CamVid, Cityscapes, and PASCAL VOC 2012 databases, which also verifies the Effectiveness and reliability of our method.},
  archive      = {J_IETIP},
  author       = {Baoyu Wang and Aihong Shen and Xu Dong and Pingping Cao},
  doi          = {10.1049/ipr2.13196},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3587-3599},
  shortjournal = {IET Image Process.},
  title        = {CF-net: Cross fusion network for semantic segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Face super resolution with a high frequency highway.
<em>IETIP</em>, <em>18</em>(12), 3570–3586. (<a
href="https://doi.org/10.1049/ipr2.13195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face shape priors such as landmarks, heatmaps, and parsing maps are widely used to improve face super resolution (SR). It is observed that face priors provide locations of high-frequency details in key facial areas such as the eyes and mouth. However, existing methods fail to effectively exploit the high-frequency information by using the priors as either constraints or inputs. This paper proposes a novel high frequency highway ( H 2 F ${\rm H}_2{\rm F}$ ) framework to better utilize prior information for face SR, which dynamically decomposes the final SR face into a coarse SR face and a high frequency (HF) face. The coarse SR face is reconstructed from a low-resolution face via a texture branch, using only pixel-wise reconstruction loss. Meanwhile, the HF face is directly generated from face priors via an HF branch that employs the proposed inception–hourglass model. As a result, H 2 F ${\rm H}_2{\rm F}$ allows the face priors to have a direct impact on the SR face by adding the outputs of both branches as the final result and provides an extra face editing function. Extensive experiments show that H 2 F ${\rm H}_2{\rm F}$ significantly outperforms state-of-the-art face SR methods, is general for different texture branch models and face priors, and is robust to dataset mismatch and pose variations.},
  archive      = {J_IETIP},
  author       = {Dan Zeng and Wen Jiang and Xiao Yan and Weibao Fu and Qiaomu Shen and Raymond Veldhuis and Bo Tang},
  doi          = {10.1049/ipr2.13195},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3570-3586},
  shortjournal = {IET Image Process.},
  title        = {Face super resolution with a high frequency highway},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cervical-YOSA: Utilizing prompt engineering and pre-trained
large-scale models for automated segmentation of multi-sequence MRI
images in cervical cancer. <em>IETIP</em>, <em>18</em>(12), 3556–3569.
(<a href="https://doi.org/10.1049/ipr2.13194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer is a major health concern, particularly in developing countries with limited medical resources. This study introduces two models aimed at improving cervical tumor segmentation: a semi-automatic model that fine-tunes the Segment Anything Model (SAM) and a fully automated model designed for efficiency. Evaluations were conducted using a dataset of 8586 magnetic resonance imaging (MRI) slices, where the semi-automatic model achieved a Dice Similarity Coefficient (DSC) of 0.9097, demonstrating high accuracy. The fully automated model also performed robustly with a DSC of 0.8526, outperforming existing methods. These models offer significant potential to enhance cervical cancer diagnosis and treatment, especially in resource-limited settings.},
  archive      = {J_IETIP},
  author       = {Yanwei Xia and Zhengjie Ou and Lihua Tan and Qiang Liu and Yanfen Cui and Da Teng and Dan Zhao},
  doi          = {10.1049/ipr2.13194},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3556-3569},
  shortjournal = {IET Image Process.},
  title        = {Cervical-YOSA: Utilizing prompt engineering and pre-trained large-scale models for automated segmentation of multi-sequence MRI images in cervical cancer},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LPAM: A lightweight medical segmentation network based on
mamba improved by prompt attention. <em>IETIP</em>, <em>18</em>(12),
3545–3555. (<a href="https://doi.org/10.1049/ipr2.13193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presently, State Space Models (SSMs), including frameworks like Mamba, have been incorporated into the realm of computer vision. These models not only sustain remote interactions and encapsulate global semantic information effectively, but also preserve linear computational complexity, offering a balance between performance and computational efficiency. Given that Mamba inherently adheres to the principle of selectivity when constructing sequence models, the goal is to further unleash the potential of Mamba through this innovative combination of convolution and self-attention, improve accuracy and minimize the number of parameters while achieving linear complexity. Mamba is employed as an encoder to distill semantic information from the image, and it is supplemented with convolutional blocks, thereby conserving the details of the image. Concurrently, embedding prompts at a deeper level enhances its adaptability to cater to diverse requirements. Lastly, a bidirectional attention mechanism is incorporated for inference, striving to retain both global connections and local details to the maximum extent. This culminates in a novel, lightweight medical segmentation model. Exhaustive experiments were executed on six public datasets. The empirical results show that the proposed model exhibits competitive performance in medical image segmentation tasks.},
  archive      = {J_IETIP},
  author       = {Kaiqi Hu and Chudong Xu},
  doi          = {10.1049/ipr2.13193},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3545-3555},
  shortjournal = {IET Image Process.},
  title        = {LPAM: A lightweight medical segmentation network based on mamba improved by prompt attention},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JOA-GAN: An improved single-image super-resolution network
for remote sensing based on GAN. <em>IETIP</em>, <em>18</em>(12),
3530–3544. (<a href="https://doi.org/10.1049/ipr2.13192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) has been widely applied in remote sensing to generate high-resolution (HR) images without increasing hardware costs. However, SR is a severe ill-posed problem. As deep learning advances, existing methods have solved this problem to a certain extent. However, the complex spatial distribution of remote sensing images still poses a challenge in effectively extracting abundant high-frequency details from the images. Here, a single-image super-resolution (SISR) network based on the generative adversarial network (GAN) for remote sensing is presented, called JOA-GAN. Firstly, a joint-attention module (JOA) is proposed to focus the network on high-frequency regions in remote sensing images to enhance the quality of image reconstruction. In the generator network, a multi-scale densely connected feature extraction block (ERRDB) is proposed, which acquires features at different scales using MSconv blocks containing multi-scale convolutions and automatically adjusts the features by JOA. In the discriminator network, the relative discriminator is used to compute the relative probability instead of the absolute probability, which helps the network learn clearer and more realistic texture details. JOA-GAN is compared with other advanced methods, and the results demonstrate that JOA-GAN has improved objective evaluation metrics and achieved superior visual effects.},
  archive      = {J_IETIP},
  author       = {Zijun Gao and Lei Shen and Zhankui Song and Hua Yan},
  doi          = {10.1049/ipr2.13192},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3530-3544},
  shortjournal = {IET Image Process.},
  title        = {JOA-GAN: An improved single-image super-resolution network for remote sensing based on GAN},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight transmission line defect identification method
based on OFN network and distillation method. <em>IETIP</em>,
<em>18</em>(12), 3518–3529. (<a
href="https://doi.org/10.1049/ipr2.13191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge devices are increasingly utilized for defect detection in power line inspection, necessitating algorithms that optimize model size and accuracy. This study introduces a lightweight detection method using the Optimized Feature Network with MobileOne-FPN-NASHead (OFN network) OFN network and distillation technique. The OFN incorporates a lightweight backbone, neural network search, re-parametrization, and a feature pyramid to create a compact yet effective detection network. To enhance feature learning, a heterogeneous distillation approach is applied, leveraging a modified YOLOv8 as a teacher network. This modification includes an explicit visual centre for improved global and local information extraction, crucial for dense target detection in power line inspections. Additionally, the Minimize the points distance IoU (MPDloU) loss function is used to improve localization accuracy over the the Complete-Intersection Over Union (CIoU) loss. Experimental results show a 1.1% mean Average Precision (mAP) increase for the enhanced YOLOv8 and a 70.2% mAP for the OFN network with 18.95 GFLOPs and 343 FPS, achieving a commendable balance between model efficiency and detection performance. The research underscores the viability of the OFN for edge computing in power line defect detection, highlighting the potential of innovative algorithmic structures in this application.},
  archive      = {J_IETIP},
  author       = {Shaotong Pei and Hangyuan Zhang and Yuxin Zhu and Chenlong Hu},
  doi          = {10.1049/ipr2.13191},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3518-3529},
  shortjournal = {IET Image Process.},
  title        = {Lightweight transmission line defect identification method based on OFN network and distillation method},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point’n move: Interactive scene object manipulation on
gaussian splatting radiance fields. <em>IETIP</em>, <em>18</em>(12),
3507–3517. (<a href="https://doi.org/10.1049/ipr2.13190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors propose Point&#39;n Move, a method that achieves interactive scene object manipulation with exposed region inpainting. Interactivity here further comes from intuitive object selection and real-time editing. To achieve this, Gaussian Splatting Radiance Field is adopted as the scene representation and its explicit nature and speed advantage are fully leveraged. Its explicit representation formulation allows to devise a 2D prompt points to 3D masks dual-stage self-prompting segmentation algorithm, perform mask refinement and merging, minimize changes, and provide good initialization for scene inpainting and perform editing in real-time without per-editing training; all lead to superior quality and performance. The method was tested by editing both forward-facing and 360 scenes. The method is also compared against existing methods, showing superior quality despite being more capable and having a speed advantage.},
  archive      = {J_IETIP},
  author       = {Jiajun Huang and Hongchuan Yu and Jianjun Zhang and Hammadi Nait-Charif},
  doi          = {10.1049/ipr2.13190},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3507-3517},
  shortjournal = {IET Image Process.},
  title        = {Point&#39;n move: Interactive scene object manipulation on gaussian splatting radiance fields},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel density-based representation for point cloud and its
ability to facilitate classification. <em>IETIP</em>, <em>18</em>(12),
3496–3506. (<a href="https://doi.org/10.1049/ipr2.13189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, in the field of processing 3D point cloud data, two primary representation methods have emerged: point-based methods and voxel-based methods. However, the former suffer from significant computational costs and lack the ease of handling exhibited by voxel-based methods. Conversely, the later often encounter challenges related to information loss resulting from downsampling operations, thereby impeding subsequent tasks. To address these limitations, this article introduces a novel density-based representation method for voxel partitioning. Additionally, a corresponding network structure is devised to extract features from this specific density representation, thereby facilitating the successful completion of classification tasks. The experiments are implemented on ModelNet40 and MNIST demonstrate that the proposed 3D convolution can achieve the-state-of-the-art performance based on the voxels.},
  archive      = {J_IETIP},
  author       = {Xianlin Xie and Xue-song Tang},
  doi          = {10.1049/ipr2.13189},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3496-3506},
  shortjournal = {IET Image Process.},
  title        = {A novel density-based representation for point cloud and its ability to facilitate classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Intracranial hematoma segmentation on head CT based on
multiscale convolutional neural network and transformer. <em>IETIP</em>,
<em>18</em>(12), 3480–3495. (<a
href="https://doi.org/10.1049/ipr2.13188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intracranial hematoma, a severe brain injury caused by trauma or cerebrovascular disease, can result in blood accumulation and compression of brain tissue. Untreated cases can cause headaches, impaired consciousness, and even brain tissue damage or death. Therefore, early and accurate diagnosis is crucial. Traditional segmentation methods require physicians with extensive clinical experience and expertise to manually mark out the hematoma region, but for hematoma cases with irregular shapes and uneven grey levels, this process is cumbersome, and the segmentation results are not good. Existing deep learning-based methods are more likely to perform binary segmentation, considering all hematomas as a class and segmenting them, but this segmentation cannot capture more detailed information and lacks the analysis of different types of hematomas. To address these problems, an ICH segmentation network combining CNN and Transformer Encoder is proposed for accurate segmentation of different types of hematomas. The network incorporated edge information and long-range context into the segmentation process. Experimental results using the CQ500 dataset demonstrate comparable performance to existing methods, with mIoU (0.8705), TPR (0.9273), mAP (0.9300), and DSC (0.9286) as the best metrics achieved by this paper&#39;s method.},
  archive      = {J_IETIP},
  author       = {Guangyu Li and Kai Gao and Changlong Liu and Shanze Li},
  doi          = {10.1049/ipr2.13188},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3480-3495},
  shortjournal = {IET Image Process.},
  title        = {Intracranial hematoma segmentation on head CT based on multiscale convolutional neural network and transformer},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual attentional skip connection based swin-UNet for
real-time cloud segmentation. <em>IETIP</em>, <em>18</em>(12),
3460–3479. (<a href="https://doi.org/10.1049/ipr2.13186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing real-time cloud segmentation technology is urgent for many remote sensing based applications such as weather forecasting. Existing deep learning based cloud segmentation methods involve two shortcomings. (a): They tend to produce discontinuous boundaries and fail to capture less salient feature, which corresponds to thin cloud pixels; (b): they are unrobust towards different scenarios. Those issues are circumvented by integrating U-Net and the swin transformer together, with an efficiently designed dual attention mechanism based skip connection. Typically, a swin transformer based encoder-decoder network, by incorporating a dual attentional skip connection with Swin-UNet (DASUNet) is proposed. DASUNet captures the global relationship of image patches based on its window attention mechanism, which fits the real-time requirement. Moreover, DASUNet characterizes the less salient features by equipping with token dual attention modules among the skip connection, which compensates the ignorance of less salient features incurred from traditional attention mechanism during the stacking of transformer layers. Experiments on ground-based images ( SWINySeg ) and remote sensing images ( HRC-WHU , 38-Cloud ) show that, DASUNet achieves the state-of-the-art or competitive results for cloud segmentation (six top-1 positions of six metrics among 11 methods on SWINySeg , two top-1 positions of five metrics among 10 methods on HRC-WHU , two top-1 positions of four metrics among 12 methods with ParaNum ≤ 30 M $\le 30{\rm M}$ on 38-Cloud ), with 100FPS implementation speed averagely for each 224 × 224 $224\times 224$ image.},
  archive      = {J_IETIP},
  author       = {Fuhao Wei and Shaofan Wang and Yanfeng Sun and Baocai Yin},
  doi          = {10.1049/ipr2.13186},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3460-3479},
  shortjournal = {IET Image Process.},
  title        = {A dual attentional skip connection based swin-UNet for real-time cloud segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A study on a target detection model for autonomous driving
tasks. <em>IETIP</em>, <em>18</em>(12), 3447–3459. (<a
href="https://doi.org/10.1049/ipr2.13185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target detection in autonomous driving tasks presents a complex and critical challenge due to the diversity of targets and the intricacy of the environment. To address this issue, this paper proposes an enhanced YOLOv8 model. Firstly, the original large target detection head is removed and replaced with a detection head tailored for small targets and high-level semantic details. Secondly, an adaptive feature fusion method is proposed, where input feature maps are processed using dilated convolutions with different dilation rates, followed by adaptive feature fusion to generate adaptive weights. Finally, an improved attention mechanism is incorporated to enhance the model&#39;s focus on target regions. Additionally, the impact of Group Shuffle Convolution (GSConv) on the model&#39;s detection speed is investigated. Validated on two public datasets, the model achieves a mean Average Precision (mAP) of 53.7% and 53.5%. Although introducing GSConv results in a slight decrease in mAP, it significantly improves frames per second. These findings underscore the effectiveness of the proposed model in autonomous driving tasks.},
  archive      = {J_IETIP},
  author       = {Hao Chen and Byung-Won Min and Haifei Zhang},
  doi          = {10.1049/ipr2.13185},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3447-3459},
  shortjournal = {IET Image Process.},
  title        = {A study on a target detection model for autonomous driving tasks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust low-rank tensor completion model with sparse noise
for higher-order data recovery. <em>IETIP</em>, <em>18</em>(12),
3430–3446. (<a href="https://doi.org/10.1049/ipr2.13184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tensor singular value decomposition-based model has garnered increasing attention in addressing tensor recovery challenges. However, existing tensor recovery methods exhibit certain inherent limitations. Some ignore the simultaneous effects of noise and missing values, while most can&#39;t handle higher-order tensors, which are not reflective of real-world scenarios. The information redundancy within tensor data often leads to a prevailing low-rank structure, making low-rankness a vital prior in the tensor recovery process. To tackle this pressing issue, a robust low-rank tensor recovery framework is proposed to rehabilitate higher-order tensors corrupted by sparse noise and missing entries. In the model, the tensor nuclear norm derived for order- d tensors ( d ≥ $\ge$ 4) are employed as a representation of the low-rank prior, while utilizing the L 1 $\mathtt {L}_1$ -norm to model the sparse noise. To solve the proposed model, an efficient Alternating direction method of multipliers algorithm is developed. A series of experiments are performed on synthetic and real-world datasets. The results show that the superior performance of the method compared with other algorithms dedicated to addressing order- d tensor recovery challenges. Notably, in scenarios where the data is severely compromised (noise ratio 40%, sample ratio 70%), the algorithm consistently outperforms its competitors, achieving significantly improved results.},
  archive      = {J_IETIP},
  author       = {Min Wang and Zhuying Chen and Shuyi Zhang},
  doi          = {10.1049/ipr2.13184},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3430-3446},
  shortjournal = {IET Image Process.},
  title        = {A robust low-rank tensor completion model with sparse noise for higher-order data recovery},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel underwater object detection enhanced algorithm based
on YOLOv5-MH. <em>IETIP</em>, <em>18</em>(12), 3415–3429. (<a
href="https://doi.org/10.1049/ipr2.13183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection is an important application of underwater vehicles. However, traditional underwater object detection algorithms have several shortcomings in underwater settings. These include imprecise feature extraction, slow detection speeds, and a lack of robustness. To address these shortcomings, a novel approach based on YOLOv5-MH (Multi-Head) is proposed in this paper. Firstly, an image enhancement technique is utilised. This technique uses adaptive linear mapping to adjust contrast and improve the quality of underwater images. Secondly, the C2f module for feature extraction is employed to enable more effective capture of object characteristics. Subsequently, the multi-head self-attention and coordinate attention are integrated into the network&#39;s backbone. This integration increases the attention given to input data, thereby enhancing the network&#39;s performance in handling complex tasks. Furthermore, a bidirectional feature pyramid is implemented to adeptly handle objects of varying scales and sizes, and elevate model performance. Finally, through comprehensive testing on the 2018 URPC dataset and deep plastic dataset, this method demonstrates superior performance. This performance is compared to the original YOLOv5 and other similar networks. It holds immense promise for practical applications across a wide spectrum of underwater tasks.},
  archive      = {J_IETIP},
  author       = {Ruishen Xu and Daqi Zhu and Mingzhi Chen},
  doi          = {10.1049/ipr2.13183},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3415-3429},
  shortjournal = {IET Image Process.},
  title        = {A novel underwater object detection enhanced algorithm based on YOLOv5-MH},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ExposureNet: Mobile camera exposure parameters autonomous
control for blur effect prevention. <em>IETIP</em>, <em>18</em>(12),
3403–3414. (<a href="https://doi.org/10.1049/ipr2.13182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of images we perceive visually is heavily impacted by the settings used for camera exposure. When these settings are imbalanced, it can result in an undesired prominent phenomenon known as blur effects. To address this problem, an ExposureNet project has been undertaken, which aims to develop an autonomous camera exposure settings control system for blur effects prevention. The proposed ExposureNet model is a CNN/Transformer hybrid neural structure, created and trained in a comprehensive manner to effectively predict the ideal exposure settings based on the semantic features of the scene being captured. This system is designed to learn the necessary steps for processing, such as identifying relevant scene features, using only two camera exposure parameters (shutter speed (SHS) and ISO) as training signals. As a result, this system can associate the semantic features of a scene with the appropriate exposure parameter adjustments, customized to the scene&#39;s dynamics and lighting conditions. By simultaneously optimizing all processing steps and bypassing traditional post-processing stages, the proposed system is designed to achieve faster performance, reduced computational cost, and lower power consumption. Experimental results demonstrate that the proposed system significantly outperforms existing methods and achieves cutting-edge performance.},
  archive      = {J_IETIP},
  author       = {Abdelwahed Nahli and Dan Li and Rahim Uddin and Muhammad Irfan and Mohamed Oubibi and Qiyong Lu and Jian Qiu Zhang},
  doi          = {10.1049/ipr2.13182},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3403-3414},
  shortjournal = {IET Image Process.},
  title        = {ExposureNet: Mobile camera exposure parameters autonomous control for blur effect prevention},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on image saliency detection based on deep neural
network. <em>IETIP</em>, <em>18</em>(12), 3393–3402. (<a
href="https://doi.org/10.1049/ipr2.13181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a hot research field at present, computer vision is devoted to the rapid acquisition and application of target information from images or videos by simulating human visual mechanism. In order to improve the accuracy and efficiency of image detection, image saliency region detection technology has received more and more attention in the field of computer vision research; an important research content in the field, the core part of which lies in the research on algorithms related to feature extraction and saliency calculation of targets. This paper analyzes the multi-feature fusion saliency detection model and visual saliency calculation process, and based on the existing algorithm, by improving the VGG16 network, a fully convolutional network saliency detection algorithm is proposed. The qualitative and quantitative experimental results show that compared with the four mainstream methods of BL, GS, SF, and RFCN, our algorithm not only improves the accuracy of salient object detection, but also effectively solves the problem of target edge blur. Therefore, this study has improved the accuracy and efficiency of saliency detection, which can not only promote the development of computer vision technology, but also provide support for research in the field of image processing.},
  archive      = {J_IETIP},
  author       = {Linrun Qiu and Dongbo Zhang and Yingkun Hu},
  doi          = {10.1049/ipr2.13181},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3393-3402},
  shortjournal = {IET Image Process.},
  title        = {Research on image saliency detection based on deep neural network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The retinex enhancement algorithm for low-light intensity
image based on improved illumination map. <em>IETIP</em>,
<em>18</em>(12), 3381–3392. (<a
href="https://doi.org/10.1049/ipr2.13180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taken in low-light intensity conditions, image with low brightness affects processing precision. In this article, the Gamma Function based on the brightness average and weighted fusion method according to gray entropy is proposed, which is combined with the improved Retinex algorithm. First, the maximum values of R, G, and B channels in original image are extracted to generate the primary illumination map. Second, the illumination map is optimized and adjusted via the Gamma correction function based on the average brightness value. Finally, the illumination map and detail layer are fused by a weighted fusion algorithm of gray entropy to obtain the reflection map. Reflection maps are used as enhancement. The algorithm proposed in this article can improve the brightness and maintain light distribution in the original image with higher precision and less color distortion.},
  archive      = {J_IETIP},
  author       = {Ruidi Weng and Ya Zhang and Hanyang Wu and Weiyong Wang and Dongyun Wang},
  doi          = {10.1049/ipr2.13180},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3381-3392},
  shortjournal = {IET Image Process.},
  title        = {The retinex enhancement algorithm for low-light intensity image based on improved illumination map},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial guided image captioning: Guiding attention with
object’s spatial interaction. <em>IETIP</em>, <em>18</em>(12),
3368–3380. (<a href="https://doi.org/10.1049/ipr2.13124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays relational position embedding is widely used in many large multi-modal models. It begins with relational captioning (a branch of image captioning) and contains two procedures: geometric modelling and prior attention. However, there are some problems that remain unsolved in the conventional procedures. This paper reviews the shortcomings of geometric modelling and prior attention. Then, a new framework called relational guided transformer (RGT) is proposed to verify the authors&#39; conclusion from the origin of relational position embedding—relational captioning. Specifically, RGT has two simple but effective improvements in geometric modelling and prior attention: (1) A machine-learned geometric modelling strategy called multi-task geometric modelling (MTG) is used under multi-task learning, replacing the original hand-made geometric feature. (2) The effectiveness of multiple kinds of prior attention is discussed and preserved in a better form, which is called spatial guided attention (SGA) to integrate the geometric prior knowledge. Extensive experiments on MSCOCO and Flickr30k have been performed to investigate the effectiveness of each module and prove our argument. The superiority of the model comparing to the authors&#39; baseline has also been proven on the offline evaluation with the “Karpathy” test split of both datasets.},
  archive      = {J_IETIP},
  author       = {Runyan Du and Wenkai Zhang and Shuoke Li and Jialiang Chen and Zhi Guo},
  doi          = {10.1049/ipr2.13124},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3368-3380},
  shortjournal = {IET Image Process.},
  title        = {Spatial guided image captioning: Guiding attention with object&#39;s spatial interaction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video object detection via space–time feature aggregation
and result reuse. <em>IETIP</em>, <em>18</em>(12), 3356–3367. (<a
href="https://doi.org/10.1049/ipr2.13179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When detecting the objects in videos, motion always leads to object deterioration, like blurring and occlusion, as well as the strange state of the object&#39;s shape and posture. Consequently, the detection of video frames will lead to a decline in accuracy by using the image object detection model. This paper proposes an online video object detection method based on the one-stage detector YOLOx. First, the module for space–time feature aggregation is given, which uses the space–time information of past frames to enhance the feature quality of the current frame. Then, the module for result reuse is given, which incorporates the detection results of past frames to improve the detection stability of the current frame. By these two modules, the trade-off between accuracy and speed of video object detection could be achieved. Experimental results on the ImageNet VID show the improvement of speed and accuracy of the proposed method.},
  archive      = {J_IETIP},
  author       = {Liang Duan and Rongfei Yang and Kun Yue and Zhengbao Sun and Guowu Yuan},
  doi          = {10.1049/ipr2.13179},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3356-3367},
  shortjournal = {IET Image Process.},
  title        = {Video object detection via space–time feature aggregation and result reuse},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-stage image inpainting using improved partial
convolutions. <em>IETIP</em>, <em>18</em>(12), 3343–3355. (<a
href="https://doi.org/10.1049/ipr2.13178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning models have dramatically influenced image inpainting. However, many existing studies still suffer from over-smoothed or blurred textures when missing regions are large or contain rich visual details. To restore textures at a fine-grained level, a multi-stage inpainting approach is proposed, which applies a series of partial inpainting modules as well as a progressive inpainting module to inpaint missing areas from their boundaries to the centre successively. Some improvements are made on the partial convolutions to reduce artifacts like blurriness, which require a convolution kernel to contain known pixels more than a certain proportion. Towards photorealistic inpainting results, the intermediate outputs from each stage are used to compute the loss. Finally, to facilitate the training process, a multi-step training is designed that progressively adds inpainting modules to optimize the model. Experiments show that this method outperforms the current excellent techniques on the publicly available datasets: CelebA, Places2 and Paris StreetView.},
  archive      = {J_IETIP},
  author       = {Cheng Li and Dan Xu and Hao Zhang},
  doi          = {10.1049/ipr2.13178},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3343-3355},
  shortjournal = {IET Image Process.},
  title        = {Multi-stage image inpainting using improved partial convolutions},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VidaGAN: Adaptive GAN for image steganography.
<em>IETIP</em>, <em>18</em>(12), 3329–3342. (<a
href="https://doi.org/10.1049/ipr2.13177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent approach to image steganography is to use deep learning. Mainly, convolutional neural networks can extract complex features and use them as patterns to combine hidden messages and images. Also, by using generative adversarial networks, it is possible to generate realistic and high-quality stego images without any noticeable artifacts. Previous methods suffered from challenges such as simple architecture, low network accuracy, imbalance between capacity and transparency, vanishing gradients, and low capacity. This study introduces a steganography framework named VidaGAN that utilizes deep learning techniques. The network being proposed is made up of three components: an encoder, a decoder, and a critic, and introduces a novel architecture and several innovations to address some of the unresolved challenges mentioned above. This study introduces a novel method for embedding any type of binary data into images using generative adversarial networks, enabling us to enhance the visual appeal of images generated by the specified model. This neural network called VarIable aDAptive GAN (VidaGAN) achieved state-of-the-art status by reaching a hiding capacity of 3.9 bits per pixel in the DIV2K dataset. Furthermore, examination by the StegExpose steganalysis tool shows an AUC of 0.6, a suitable threshold for transparency.},
  archive      = {J_IETIP},
  author       = {Vida Yousefi Ramandi and Mansoor Fateh and Mohsen Rezvani},
  doi          = {10.1049/ipr2.13177},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3329-3342},
  shortjournal = {IET Image Process.},
  title        = {VidaGAN: Adaptive GAN for image steganography},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WNet: A dual-encoded multi-human parsing network.
<em>IETIP</em>, <em>18</em>(12), 3316–3328. (<a
href="https://doi.org/10.1049/ipr2.13176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-human parsing has become a focal point in research, yet prevailing methods often rely on intermediate stages and lacking pixel-level analysis. Moreover, their high computational demands limit real-world efficiency. To address these challenges and enable real-time performance, low-latency end-to-end network is proposed. This approach leverages vision transformer and convolutional neural network in a dual-encoded network, featuring a lightweight Transformer-based vision encoder) and a convolution encoder based on Darknet. This combination adeptly captures long-range dependencies and spatial relationships. Incorporating a fuse block enables the seamless merging of features from the encoders. Residual connections in the decoder design amplify information flow. Experimental validation on crowd instance-level human parsing and look into person datasets showcases the WNet&#39;s effectiveness, achieving high-speed multi-human parsing at 26.7 frames per second. Ablation studies further underscore WNet&#39;s capabilities, emphasizing its efficiency and accuracy in complex multi-human parsing tasks.},
  archive      = {J_IETIP},
  author       = {Md Imran Hosen and Tarkan Aydin and Md Baharul Islam},
  doi          = {10.1049/ipr2.13176},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3316-3328},
  shortjournal = {IET Image Process.},
  title        = {WNet: A dual-encoded multi-human parsing network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UCSwin-UNet model for medical image segmentation based on
cardiac haemangioma. <em>IETIP</em>, <em>18</em>(12), 3302–3315. (<a
href="https://doi.org/10.1049/ipr2.13175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiac hemangioma is a rare benign tumour that presents diagnostic challenges due to its variable clinical symptoms, imaging features, and locations. This study proposes a novel segmentation method based on a Convolutional Neural Network (CNN) and Transformer integration, with Swin-UNet as the core model. We incorporated a U-shaped convolutional neural network block into the original jump connection of Swin-UNet. The Binary Cross Entropy Loss (BCE Loss) algorithm was added, and the learning rate decay algorithm was modified to select the appropriate one by comparing loss values. This paper utilizes the publicly available cardiac angioma dataset in AI Studio, consisting of 215 images for training and testing. To evaluate the effectiveness of the proposed model, this paper demonstrates its optimality through ablation experiments and comparisons with other mainstream models. The comparison experiments show that this model improves Dice by approximately 12%, HD95 by approximately 4.7 mm, Accuracy by approximately 6.1%, and F1 score by 0.11 compared to models such as UNet, UNet++, and Deeplabv3+, etc. For the recently proposed SOTO models, such as TransUNet, Swin-UNet, and MultiResUnet, the Dice score improved by about 1.2%, HD95 reduced by about 1mm, Accuracy improved by about 0.3%, and F1 score improved by 0.015.},
  archive      = {J_IETIP},
  author       = {Jian-Ting Shi and Gui-Xu Qu and Zhi-Jun Li},
  doi          = {10.1049/ipr2.13175},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3302-3315},
  shortjournal = {IET Image Process.},
  title        = {UCSwin-UNet model for medical image segmentation based on cardiac haemangioma},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotion recognition in user-generated videos with long-range
correlation-aware network. <em>IETIP</em>, <em>18</em>(12), 3288–3301.
(<a href="https://doi.org/10.1049/ipr2.13174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in user-generated videos plays an essential role in affective computing. In general, visual information directly affects human emotions, so the visual modality is significant for emotion recognition. Most classic approaches mainly focus on local temporal information of videos, which potentially restricts their capacity to encode the correlation of long-range context. To address this issue, a novel network is proposed to recognize emotions in videos. To be specific, a spatio-temporal correlation-aware block is designed to depict the long-range correlations between input tokens, where the convolutional layers are used to learn the local correlations and the inter-image cross-attention is designed to learn the long-range and spatio-temporal correlations between input tokens. To generate diverse and challenging samples, a dual-augmentation fusion layer is devised, which fuses each frame with its corresponding frame in the temporal domain. To produce rich video clips, a long-range sampling layer is designed, which generates clips in a wide range of spatial and temporal domains. Extensive experiments are conducted on two challenging video emotion datasets, namely VideoEmotion-8 and Ekman-6. The experimental results demonstrate that the proposed method obtains better performance than baseline methods. Moreover, the proposed method achieves state-of-the-art results on the two datasets. The source code of the proposed network is available at: https://github.com/JinChow/LRCANet .},
  archive      = {J_IETIP},
  author       = {Yun Yi and Jin Zhou and Hanli Wang and Pengjie Tang and Min Wang},
  doi          = {10.1049/ipr2.13174},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3288-3301},
  shortjournal = {IET Image Process.},
  title        = {Emotion recognition in user-generated videos with long-range correlation-aware network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new network model for multiple object detection for
autonomous vehicle detection in mining environment. <em>IETIP</em>,
<em>18</em>(12), 3277–3287. (<a
href="https://doi.org/10.1049/ipr2.13173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the challenges of low multi-object detection accuracy and difficulty in identifying small targets caused by challenging environmental conditions including irregular lighting patterns and ambient noise levels in the mining environment with autonomous electric locomotives. A new network model based on SOD−YOLOv5s−4L has been proposed to detect multi-objects for autonomous electric locomotives in underground coal mines. Improvements have been applied in YOLOv5s to construct the SOD−YOLOv5s−4L model, by introducing the SIoU loss function to address the mismatch between real and predicted bounding box directions, facilitating the model to learn target position information more efficiently. This research introduces a decoupled head to enhance feature fusion and improve the positioning precision of the network model, enabling rapid capture of multi-scale target features. Furthermore, the detection capability of the model has been increased by introducing the small target detection layer which is developed by increasing the number of detection layers from three to four. The experimental results on multiple object detection dataset show that the proposed model achieves significant improvement in mean average precision (mAP) of almost 98% for various types of targets and an average precision (AP) of nearly 99% for small targets on the other hand it achieves 5.19% (mAP) and 9.79% (AP) compared to the YOLOv5s model. Furthermore, comparative analysis with other models like YOLOv7 and YOLOv8 shows that the proposed model has superior performance in terms of object detection.},
  archive      = {J_IETIP},
  author       = {Muhammad Wahab Hanif and Zhenhua Yu and Rehmat Bashir and Zhanli Li and Sardar Annes Farooq and Muhammad Usman Sana},
  doi          = {10.1049/ipr2.13173},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3277-3287},
  shortjournal = {IET Image Process.},
  title        = {A new network model for multiple object detection for autonomous vehicle detection in mining environment},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New method of colour image encryption using triple chaotic
maps. <em>IETIP</em>, <em>18</em>(12), 3262–3276. (<a
href="https://doi.org/10.1049/ipr2.13171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new image encryption algorithm based on the triple chaotic maps is proposed to deal with the issues of inadequate security and low encryption efficiency. Coloured images consist of three linked channels used in the scheme. This method uses different keys to break the correlations between adjacent pixels in each channel. The triple chaotic maps are Lorenz, 2D-Logistic, and Henon. First, the plain image is split into RGB channels to encrypt each channel separately. Second, the triple chaotic maps generate two groups of keys. The first group of keys performs a pixel permutation, resulting in scrambled channels used as input for the following step. Finally, the second group of keys is used to diffuse the scrambled channels independently, resulting in diffused channels, which are then merged to obtain a cipher image. The triple chaotic maps of different orders generate the cipher image with great unpredictability and security. The security is evaluated using various measures. The results demonstrated a high level of security attained by successfully encrypting coloured images. Recent encryption algorithms are compared in terms of entropy, correlation coefficients, and attack robustness. The proposed method provided outstanding security and outperformed existing image encryption algorithms.},
  archive      = {J_IETIP},
  author       = {Khalid M. Hosny and Yasmin M. Elnabawy and Ahmed M. Elshewey and Sarah M. Alhammad and Doaa Sami Khafaga and Rania Salama},
  doi          = {10.1049/ipr2.13171},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3262-3276},
  shortjournal = {IET Image Process.},
  title        = {New method of colour image encryption using triple chaotic maps},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noise2Variance: Dual networks with variance constraint for
self-supervised real-world image denoising. <em>IETIP</em>,
<em>18</em>(12), 3251–3261. (<a
href="https://doi.org/10.1049/ipr2.13170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising aims to restore a clean image from a noisy image. Traditional methods utilizing convolutional neural networks (CNN) for denoising are trained using pairs of noisy and clean images to comprehend the transformation from a noisy image to a clean one. However, the acquisition of such image pairs in real-world scenarios presents a challenge. Hence, numerous self-supervised denoising techniques have been developed that do not require clean images for training. This study demonstrates that a straightforward loss design, concentrating on variance, can effectively train a standard CNN denoiser in a self-supervised fashion. A novel theoretical framework is introduced for training a basic CNN denoising model using three constraints: mean, variance, and augmentation. The variance constraint is crucial as it prevents the trained model from converging to trivial solutions such as identity or zero mapping. This theory provides valuable insights for the development of new self-supervised denoising methods. Furthermore, a method that applies this theory to proposed dual networks is developed, which consist of two standard CNN models predicting both the clean image and the noise. This approach enhances model capacity during training while minimizing computational costs during inference. This method exemplifies the implementation of the variance constraint and introduces a data constraint for dual networks. Notably, the proposed method only assumes the presence of additive white noise, irrespective of the noise distribution. This minimal assumption enhances the model&#39;s robustness against noise with complex or unknown distributions in real-world distorted images. Experimental results indicate that the proposed Noise2Variance method exhibits commendable performance on peak signal noise ratio and structural similarity metrics compared to existing self-supervised denoising techniques. Visual comparison of results further substantiates the efficacy of the proposed method. A comparison of model complexity reveals that the method is efficient among the compared CNN-based techniques.},
  archive      = {J_IETIP},
  author       = {Hanlin Tan and Yu Liu and Maojun Zhang},
  doi          = {10.1049/ipr2.13170},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3251-3261},
  shortjournal = {IET Image Process.},
  title        = {Noise2Variance: Dual networks with variance constraint for self-supervised real-world image denoising},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning neural implicit surfaces with local probability
standard variance. <em>IETIP</em>, <em>18</em>(12), 3241–3250. (<a
href="https://doi.org/10.1049/ipr2.13169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing geometric shapes from sparse multiview has always been a challenging task. With the development of neural implicit surfaces, geometry-based volume rendering surface reconstruction methods have been proven to be able to reconstruct high-quality surfaces. However, existing geometry-based reconstruction methods completely associate volume density with signed distance function or unsigned distance function, resulting in the same volume density peak that can only be reconstructed near the object surface. When there are transparent surfaces in the scene, existing methods prioritize the reconstruction of opaque surfaces, neglecting the reconstruction of transparent surfaces, which is disadvantageous when reconstructing real scenes. To solve this problem, we introduce local probability standard variance, which calculates volume density together with signed distance function. In this way, it can reconstruct the volume density that matches the transparency characteristics of the object surface. The method can reconstruct the surface of transparent objects, and experiments on two transparent surface datasets show that the method performs better.},
  archive      = {J_IETIP},
  author       = {Hai Nan and Kai Zhao and Xuefei Han and Dongjie Zhao},
  doi          = {10.1049/ipr2.13169},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3241-3250},
  shortjournal = {IET Image Process.},
  title        = {Learning neural implicit surfaces with local probability standard variance},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A siamese network optimization using genetic algorithm for
brain diseases. <em>IETIP</em>, <em>18</em>(12), 3231–3240. (<a
href="https://doi.org/10.1049/ipr2.13168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex nature of human brain tissues is important in ensuring accurate diagnosis to save human lives. Research on early detection of brain diseases has gained significant prominence within medical intelligence using highly complex model architectures with only a single label that cannot be verified. This paper introduces an innovative approach to Siamese Network Genetic Algorithm (SN-GA) leveraging Siamese contrastive learning for classifying brain images across diverse diseases. Our core architecture is a Bi-Convolutional Neural Network (Bi-CNN) optimized by a genetic algorithm to enhance brain image classification. Specifically, five widely recognized transfer learning-based architectures, namely AlexNet, Efficient-B0, VGG-16, ResNet-50, and Inception-v3, have been incorporated to evaluate the effectiveness of the proposed SN-GA system. The performance of these models has been rigorously analyzed and compared using two distinct datasets: Brain tumors and Alzheimer&#39;s datasets. The experimental results robustly affirm the efficacy of the proposed Siamese model, yielding exceptional levels of accuracy, precision, and recall, all peaking at 97%. These findings underscore the potential and resilience of the optimized Siamese network in the context of brain disease classification, emphasizing its significance in advancing the field of medical imaging and diagnosis, with implications for early intervention and patient care.},
  archive      = {J_IETIP},
  author       = {Amal Saif and Rawan Ghnemat and Qasem Abu Al-Haija},
  doi          = {10.1049/ipr2.13168},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3231-3240},
  shortjournal = {IET Image Process.},
  title        = {A siamese network optimization using genetic algorithm for brain diseases},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Absolute velocity estimation of UAVs based on phase
correlation and monocular vision in unknown GNSS-denied environments.
<em>IETIP</em>, <em>18</em>(12), 3218–3230. (<a
href="https://doi.org/10.1049/ipr2.13167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel approach for absolute velocity estimation of unmanned aerial vehicles in unknown and unmapped GNSS-denied environments. The proposed method leverages the advantages of Fourier-based image phase correlation and utilizes off-the-shelf onboard sensors, including a downward-facing monocular camera, an inertial sensor, and a sonar. The non-matching tracking approach is particularly appealing, offering accurate estimation while remaining robust against frequency-dependent noise, significant intensity variations, and time-varying illumination disturbances. In the proposed method, the first step involves computing global pixel motion from consecutive images using phase correlation, which utilizes the shift property of the Fourier transform. This pixel motion estimation serves as the basis for creating a closed-loop solution for absolute velocity estimation. To further enhance accuracy, a Kalman filter is implemented to fuse all available data and provide a reliable velocity estimate. Validation of the proposed visual-inertial technique is conducted through simulation experiments using AirSim and real-world flight tests. The results demonstrate the practicality and effectiveness of the approach across a range of challenging scenarios.},
  archive      = {J_IETIP},
  author       = {Heng Deng and Duhao Li and Boyang Shen and Zhiyao Zhao and Usman Arif},
  doi          = {10.1049/ipr2.13167},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3218-3230},
  shortjournal = {IET Image Process.},
  title        = {Absolute velocity estimation of UAVs based on phase correlation and monocular vision in unknown GNSS-denied environments},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pyramid gaussian pooling based CNN and transformer hybrid
network for smoke segmentation. <em>IETIP</em>, <em>18</em>(12),
3206–3217. (<a href="https://doi.org/10.1049/ipr2.13166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual smoke semantic segmentation is a challenging task due to semi-transparency, variable shapes, and complex textures of smoke. To improve segmentation performance, a convolutional neural network and transformer hybrid network are proposed based on pyramid Gaussian pooling (PGP) for smoke segmentation. In order to utilize low-pass filtering to suppress noise, a PGP method is designed. Then, the output of PGP is reshaped to construct a set of visual tokens for transformers, thus a PGP-transformer module is presented to make full use of the self-attention mechanism. Finally, the PGP-transformer module is inserted into the U-shaped architecture with skip connections. A large number of experiments have proved that the method is significantly superior to existing state-of-the-art algorithms on virtual and real smoke datasets, and ablation experiments have also verified the effectiveness of the proposed modules.},
  archive      = {J_IETIP},
  author       = {Guiqian Wang and Feiniu Yuan and Hongdi Li and Zhijun Fang},
  doi          = {10.1049/ipr2.13166},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3206-3217},
  shortjournal = {IET Image Process.},
  title        = {A pyramid gaussian pooling based CNN and transformer hybrid network for smoke segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A flare removal network for night vision perception:
Resistant to the interference of complex light. <em>IETIP</em>,
<em>18</em>(12), 3192–3205. (<a
href="https://doi.org/10.1049/ipr2.13165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-precision visual perception results are easily affected by the lens flare issue when the image sensor is facing to strong light. The existing flare removal methods have poor robustness when confronted with flare interference caused by complex nighttime lighting, which has to preserve natural light source information. A simulated dataset for the removal of night flares is created to solve the problem of collecting complete paired training data, and night flare removal network (NFR-Net) is proposed to remove the interference caused by various light disturbances at night. The light source extraction module is introduced to retain light source information realistically and effectively in night vision scenes. Extensive experimental results demonstrate that the proposed method is superior to the existing related methods in the various complex night vision scenes. The proposed NFR-Net can enhance visual perception of nighttime images significantly and improve the performance of night vision tasks.},
  archive      = {J_IETIP},
  author       = {Yan Liu and Guan Huang and Wenting Qi and Yujie Li},
  doi          = {10.1049/ipr2.13165},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3192-3205},
  shortjournal = {IET Image Process.},
  title        = {A flare removal network for night vision perception: Resistant to the interference of complex light},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fruit fast tracking and recognition of apple picking robot
based on improved YOLOv5. <em>IETIP</em>, <em>18</em>(12), 3179–3191.
(<a href="https://doi.org/10.1049/ipr2.13164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article proposes a real-time apple picking method based on an improved YOLOv5. This method accurately recognizes different apple targets on fruit trees for robots and helps them adjust their position to avoid obstructions during fruit picking. Firstly, the original BottleneckCSP module in the YOLOv5 backbone network is enhanced to extract deeper features from images while maintaining lightweight. Secondly, the ECA module is embedded into the improved backbone network to better extract features of different apple targets. Lastly, the initial anchor frame size of the network is adjusted to avoid recognizing apples in distant planting rows. The results demonstrate that this improved model achieves high accuracy rates and recall rates for recognizing various types of apple picking methods with an average recognition time of 0.025s per image. Compared with other models tested on six types of apple picking methods, including the original YOLOv5 model as well as YOLOv3 and EfficientDet-D0 algorithms, our improved model shows significant improvements in mAP by 1.95%, 17.6%, and 12.7% respectively. This method provides technical support for robots&#39; picking hands to actively avoid obstructions caused by branches during fruit harvesting, effectively reducing apple loss.},
  archive      = {J_IETIP},
  author       = {Yao Xu and Liu Zuodong},
  doi          = {10.1049/ipr2.13164},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3179-3191},
  shortjournal = {IET Image Process.},
  title        = {Fruit fast tracking and recognition of apple picking robot based on improved YOLOv5},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D position and pose measurement based on coded light field.
<em>IETIP</em>, <em>18</em>(12), 3168–3178. (<a
href="https://doi.org/10.1049/ipr2.13163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-precision relative position and attitude measurement technology has a wide range of applications in aerospace and industrial production. Currently, the commonly used method for measurement is based on visual cooperative signs. However, its accuracy significantly decreases as the distance increases. Therefore, a relative positioning system is designed based on light field spatial coding and visual recognition. The projector emits spatially encoded structured light within the coverage of the light field, while the receiving end captures, identifies, measures the code, and calculates its pose in the light field coordinate system. Compared with the traditional measurement method, the measurement accuracy of this system does not decrease greatly with the increase in distance, the measurement distance can be adjusted in real-time and does not depend on an external light source. By changing the projection pattern with different resolutions without changing hardware systems, it can adjust effective measurement distance accordingly. Theoretical and experimental results show that the proposed method can maintain measurement accuracy with the increase in distance.},
  archive      = {J_IETIP},
  author       = {Wei Liu and Ding Chang and Jiajun Shao and Yanxi Yang},
  doi          = {10.1049/ipr2.13163},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3168-3178},
  shortjournal = {IET Image Process.},
  title        = {3D position and pose measurement based on coded light field},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VPDet: Refined region CNN for hair follicle recognition in
arbitrary angle. <em>IETIP</em>, <em>18</em>(12), 3156–3167. (<a
href="https://doi.org/10.1049/ipr2.13162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the growing demand for hair-loss treatments, this study introduces the vector proposal detector (VPDet), a groundbreaking solution in hair transplant robotics. VPDet, distinct from traditional approaches, addresses the complex challenges of hair follicle detection, notably the variability in hair growth orientations and the intricacies of hair clustering. The method innovatively leverages the linear nature of hair, spanning a full 360-degree orientation spectrum. The VPDet framework, a novel two-stage object detection system, incorporates the vector proposal network and vector align blocks. These elements are crucial in transforming conventional anchor boxes into anchor vectors, thereby generating reference vectors across various scales and angles. The vector align block, a key innovation, uniquely combines vector and adjacent feature data to align features through shared maps. The extensive experiments, conducted on the FDU_HairFollicleDataset and an extended dataset, exhibit a remarkable enhancement in model performance, with a 51.3% increase in precision and a 20.8% boost in F 1 score. The results not only demonstrate VPDet&#39;s superior capability in hair follicle recognition but also its potential in posture recognition for vector-characteristic objects. This approach represents a significant advancement in both the field of hair transplant robotics and vector-based object detection.},
  archive      = {J_IETIP},
  author       = {Xinyu Gu and Xiaoxu Zhang},
  doi          = {10.1049/ipr2.13162},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3156-3167},
  shortjournal = {IET Image Process.},
  title        = {VPDet: Refined region CNN for hair follicle recognition in arbitrary angle},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A lightweight underwater fish image semantic segmentation
model based on u-net. <em>IETIP</em>, <em>18</em>(12), 3143–3155. (<a
href="https://doi.org/10.1049/ipr2.13161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of underwater fish images is vital for monitoring fish stocks, assessing marine resources, and sustaining fisheries. To tackle challenges such as low segmentation accuracy, inadequate real-time performance, and imprecise location segmentation in current methods, a novel lightweight U-Net model is proposed. The proposed model acquires more segmentation details by applying a multiple-input approach at the first four encoder levels. To achieve both lightweight and high accuracy, a multi-scale residual structure (MRS) module is proposed to reduce parameters and compensate for the accuracy loss caused by the reduction of channels. To improve segmentation accuracy, a multi-scale skip connection (MSC) structure is further proposed, and the convolution block attention mechanism (CBAM) is introduced at the end of each decoder level for weight adjustment. Experimental results demonstrate a notable reduction in model volume, parameters, and floating-point operations by 94.20%, 94.39%, and 51.52% respectively, compared to the original model. The proposed model achieves a high mean intersection over union (mIOU) of 94.44%, mean pixel accuracy (mPA) of 97.03%, and a frame rate of 43.62 frames per second (FPS). With its high precision and minimal parameters, the model strikes a balance between accuracy and speed, making it particularly suitable for underwater image segmentation.},
  archive      = {J_IETIP},
  author       = {Zhenkai Zhang and Wanghua Li and Boon-Chong Seet},
  doi          = {10.1049/ipr2.13161},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3143-3155},
  shortjournal = {IET Image Process.},
  title        = {A lightweight underwater fish image semantic segmentation model based on U-net},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MA-ResUNet: Multi-attention optic cup and optic disc
segmentation based on improved u-net. <em>IETIP</em>, <em>18</em>(12),
3128–3142. (<a href="https://doi.org/10.1049/ipr2.13160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma poses a significant threat to vision, capable of causing irreversible damage and, in severe instances, leading to permanent blindness. Accurate optic cup (OC) and optic disc (OD) segmentation are essential in glaucoma screening. In this study, a novel OC and OD segmentation approach is proposed. Based on U-Net, it is optimized by introducing cardinality dimensions. Moreover, attention gates are implemented to reinforce salient features while suppressing irrelevant information. Additionally, a convolutional block attention module (CBAM) is integrated into the decoder segment. This fusion hones in on effective information in both channel and spatial dimensions. Meanwhile, an image processing procedure is proposed for image normalization and enhancement. All of these increase the accuracy of the model. This model is evaluated on the ORIGA and REFUGE datasets, demonstrating the model&#39;s superiority in OC and OD segmentation compared to the state-of-the-art methods. Additionally, after the proposed image processing, cup-to-disc ratio (CDR) prediction on a batch of 155 in-house fundus images yields an absolute CDR error of 0.099, which is reduced by 0.04 compared to the case where only conventional processing was performed.},
  archive      = {J_IETIP},
  author       = {Xiaoqian Zhang and Ying Lin and Linxuan Li and Jingyu Zeng and Xianmei Lan and Xinyi Zhang and Yongjian Jia and Ye Tao and Lin Wang and Yu Wang and Yu Li and Yang Zong and Xin Jin and Panhong Liu and Xinyu Cheng and Huanhuan Zhu},
  doi          = {10.1049/ipr2.13160},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3128-3142},
  shortjournal = {IET Image Process.},
  title        = {MA-ResUNet: Multi-attention optic cup and optic disc segmentation based on improved U-net},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iris image retrieval using partial matching of image blocks.
<em>IETIP</em>, <em>18</em>(12), 3109–3127. (<a
href="https://doi.org/10.1049/ipr2.13159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of human identification through recognition of patterns in iris images captured in unconstrained environments results in image artefacts such as image occlusion and specular reflection, in which iris tissue is observable to extremely low content. To overcome this problem, this paper presents a novel method for iris image retrieval and recognition based on partial pattern matching. The main contribution of the proposed method relies on an image partitioning schema in which the iris is divided into non-overlapping blocks with varying dimensions, facilitating the identification and removal of image regions impaired by eyelids, eyelashes, and specular reflections. In fact, the blocks that contain artefacts are completely ignored and those blocks are preserved that include useful patterns for identification. In addition, a multi-feature similarity followed by a score fusion technique is employed for ranking the retrieval results. The remarkable results of the classification stage include an accuracy of 100%, 98.75%, and 99.94% on three benchmark databases, including UPOL, UBIRIS.V2, and CASIA-Iris-Interval.v3, respectively. Additionally, in the retrieval stage, the proposed method achieves a precision of 100% on all three benchmark databases, a recall of 83.33%, and a F1-measure of 90.91 on the CASIA-Iris-Interval.v3 dataset.},
  archive      = {J_IETIP},
  author       = {Fahimeh Afkhamnia and Farsad Zamani Boroujeni and Mohammad Reza Soltanaghaei},
  doi          = {10.1049/ipr2.13159},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3109-3127},
  shortjournal = {IET Image Process.},
  title        = {Iris image retrieval using partial matching of image blocks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maritime vessel classification based on a dual network
combining EfficientNet with a hybrid network MPANet. <em>IETIP</em>,
<em>18</em>(11), 3093–3107. (<a
href="https://doi.org/10.1049/ipr2.13158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship classification is an important technique for enhancing maritime management and security. Visible and infrared sensors are generally employed to deal with the challenging problem and improve classification performance. Herein, a two-branch feature fusion neural network structure is proposed to classify the visible and infrared maritime vessel images simultaneously. Specifically, in this two-branch neural network, one branch is based on a deep convolutional neural network that is used to extract the visible image features, while the other is a hybrid network structure that is a multi-scale patch embedding network called MPANet. The sub-network MPANet can extract fine- and coarse-grained features, in which the pooling operation instead of the multi-head attention mechanism is utilized to reduce memory consumption. When there are infrared images, it is used to extract the infrared image features, otherwise, this branch is also utilized to extract visible image features. Therefore, this dual network is suitable with or without infrared images. The experimental results on the visible and infrared spectrums (VAIS) dataset demonstrate that the introduced network achieves state-of-the-art ship classification performance on visible images and paired visible and infrared ship images.},
  archive      = {J_IETIP},
  author       = {Wenhui Liu and Yulong Qiao and Yue Zhao and Zhengyi Xing and Hengxiang He},
  doi          = {10.1049/ipr2.13158},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3093-3107},
  shortjournal = {IET Image Process.},
  title        = {Maritime vessel classification based on a dual network combining EfficientNet with a hybrid network MPANet},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature ensemble network for medical image segmentation with
multi-scale atrous transformer. <em>IETIP</em>, <em>18</em>(11),
3082–3092. (<a href="https://doi.org/10.1049/ipr2.13157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed notable advancements in medical image segmentation through deep convolutional neural networks. However, a notable limitation lies in the local operation of convolution, which hinders the ability to fully exploit global semantic information. To overcome the challenges prevalent in medical image segmentation, the feature ensemble network with multi-scale atrous transformer is proposed. At the core of the approach lies the multi-scale contextual integration module, which is based on the multi-scale atrous transformer and facilitates contextual integration of multi-level features. To extract discriminative fine-grained features of the target region, a hybrid attention mechanism that synergistically combines spatial and channel attention, thereby sharpening the model&#39;s focus on crucial target information within high-level features, is incorporated. Additionally, the channel-aware feature reconstruction module is introduced as an innovative component engineered to tackle feature similarity issues across different categories. This module performs feature reconstruction based on channel perception, effectively widening the feature gap between categories and enhancing the segmentation capability. It is worth mentioning that our approach surpasses the state-of-the-art method using three benchmark datasets in medical image segmentation.},
  archive      = {J_IETIP},
  author       = {Di Gai and Yuhan Geng and Xia Huang and Zheng Huang and Xin Xiong and Ruihua Zhou and Qi Wang},
  doi          = {10.1049/ipr2.13157},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3082-3092},
  shortjournal = {IET Image Process.},
  title        = {Feature ensemble network for medical image segmentation with multi-scale atrous transformer},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on a multi-scale degradation fusion network in
all-in-one image restoration. <em>IETIP</em>, <em>18</em>(11),
3070–3081. (<a href="https://doi.org/10.1049/ipr2.13156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration aims to recover high-quality clean images from degraded low-quality ones. Deep learning-based approaches have been a focal point in the field of image restoration. However, most methods focus solely on a single type of degradation and may not extend well to real-world scenarios with unknown degradation. For this purpose, the present study introduces an all-in-one image restoration approach by designing a multi-scale feature fusion UNet structure (MdfUNet). In summary, the proposed method exhibits two significant advantages. For starters, it implicitly fuses degradation information across multiple scales, enabling the network to extract rich hierarchical features and enhancing its generalization ability towards unknown degradations. Secondly, MdfUnet possesses strong image reconstruction capabilities. It utilizes a simple non-linear feature optimizer to enhance skip connections, providing rich feature representations for the image reconstruction process, and ultimately generating high-quality restored images. Extensive experimental results show the proposed method outperforms multiple baselines on deraining, dehazing, and denoising datasets.},
  archive      = {J_IETIP},
  author       = {Bohang Shi and Bingqing Xiong and Yuanhui Yu},
  doi          = {10.1049/ipr2.13156},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3070-3081},
  shortjournal = {IET Image Process.},
  title        = {Research on a multi-scale degradation fusion network in all-in-one image restoration},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chinese image captioning with fusion encoder and visual
keyword search. <em>IETIP</em>, <em>18</em>(11), 3055–3069. (<a
href="https://doi.org/10.1049/ipr2.13155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic generation of image captions is essentially a cross-modal conversion from image to text. Owing to the differences in linguistic characteristics between Chinese and English, quite a few Chinese image captioning methods have recently been proposed. Nevertheless, the existing Chinese image captioning models usually lack attention to local details of images or tend to produce general descriptions. To address these challenges, a Chinese image captioning method is proposed that incorporates fusion encoder, visual keyword search, and reinforcement learning. The fusion encoder can simultaneously extract local and global features of the input image to enrich the semantic information in the decoding stage, visual keyword search can pursue potential visual words associated with the image content, and the reinforcement learning mechanism can optimize the evaluation metric CIDEr at sentence level to promote the lexical diversity of image description. The results of extensive experiments demonstrate that the proposed model outperforms the state-of-the-art models and delivers expressive and informative Chinese image captions.},
  archive      = {J_IETIP},
  author       = {Yang Zou and Shiyu Liao and Qifei Wang},
  doi          = {10.1049/ipr2.13155},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3055-3069},
  shortjournal = {IET Image Process.},
  title        = {Chinese image captioning with fusion encoder and visual keyword search},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI-generated video steganography based on semantic
segmentation. <em>IETIP</em>, <em>18</em>(11), 3042–3054. (<a
href="https://doi.org/10.1049/ipr2.13154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional video steganography methods primarily rely on modifying concealed spaces for embedding, thereby exhibiting a certain degree of security and embedding capacity. Nevertheless, these methods do not fully capitalize on the rich semantic information inherent in videos, limiting their overall effectiveness. In this paper, an AI-generated video steganography scheme based on semantic segmentation is proposed. The mapping relationship between secret and semantic information is established by using a semantic segmentation model. The secret information can be converted into semantic labels by semantic histograms or pixels means, and semantic labels containing secret information are obtained and input into the video-to-video model to drive the generation of stego videos. After receiving the stego video, the receiver extracts the secret information using a pre-defined specific embedding mode, including the methods of sub-block partitioning and embedding capacity per frame. The experimental results show that the stego video has good visual quality, security, and robustness against various noise attacks.},
  archive      = {J_IETIP},
  author       = {Yangping Lin and Peng Luo and Zhuo Zhang and Jia Liu and Xiaoyuan Yang},
  doi          = {10.1049/ipr2.13154},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3042-3054},
  shortjournal = {IET Image Process.},
  title        = {AI-generated video steganography based on semantic segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retinex decomposition based low-light image enhancement by
integrating swin transformer and u-net-like architecture.
<em>IETIP</em>, <em>18</em>(11), 3028–3041. (<a
href="https://doi.org/10.1049/ipr2.13153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light images are captured in environments with minimal lighting, such as nighttime or underwater conditions. These images often suffer from issues like low brightness, poor contrast, lack of detail, and overall darkness, significantly impairing human visual perception and subsequent high-level visual tasks. Enhancing low-light images holds great practical significance. Among the various existing methods for Low-Light Image Enhancement (LLIE), those based on the Retinex theory have gained significant attention. However, despite considerable efforts in prior research, the challenge of Retinex decomposition remains unresolved. In this study, an LLIE network based on the Retinex theory is proposed, which addresses these challenges by integrating attention mechanisms and a U-Net-like architecture. The proposed model comprises three modules: the Decomposition module (DECM), the Reflectance Recovery module (REFM), and the Illumination Enhancement module (ILEM). Its objective is to decompose low-light images based on the Retinex theory and enhance the decomposed reflectance and illumination maps using attention mechanisms and a U-Net-like architecture. We conducted extensive experiments on several widely used public datasets. The qualitative results demonstrate that the approach produces enhanced images with superior visual quality compared to the existing methods on all test datasets, especially for some extremely dark images. Furthermore, the quantitative evaluation results based on metrics PSNR, SSIM, LPIPS, BRISQUE, and MUSIQ show the proposed model achieves superior performance, with PSNR and BRISQUE significantly outperforming the baseline approaches, where (PSNR, mean BRISQUE) values of the proposed method and the second best results are (17.14, 17.72) and (16.44, 19.65). Additionally, further experimental results such as ablation studies indicate the effectiveness of the proposed model.},
  archive      = {J_IETIP},
  author       = {Zexin Wang and Letu Qingge and Qingyi Pan and Pei Yang},
  doi          = {10.1049/ipr2.13153},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3028-3041},
  shortjournal = {IET Image Process.},
  title        = {Retinex decomposition based low-light image enhancement by integrating swin transformer and U-net-like architecture},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BSMH: Cross-dataset object detection based on box-separated
multiple-head. <em>IETIP</em>, <em>18</em>(11), 3013–3027. (<a
href="https://doi.org/10.1049/ipr2.13152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-dataset object detection methods can adapt to the needs of rapid category expansion in object detection tasks. However, these methods are prone to generate dataset-aware errors with false alarm objects. This study is aimed to address these issues. A box-separated multiple-head module and box-separated loss function based on the YOLOv8 network are devised to achieve cross-dataset object detection. Additionally, a sameclass-aware fusion module to avoid gradient conflicts due to cross-category conflicts is developed. A multiple-head fusion module is devised to reduce the number of false alarm objects caused by dataset-aware errors. A global class-aware sampler is also designed to adapt to the impact of the imbalanced number of categories and training samples across datasets. The effectiveness of the box-separated multiple-head module is verified using cross-datasets built using the COCO, WiderFace, WiderPerson, and OpenImages V4 datasets. Extensive experiments demonstrate the efficiency and precision of the proposed method.},
  archive      = {J_IETIP},
  author       = {Feng Lu and You Chun Xu and Yao Qi and De Sheng Xie and Yogn Le Li},
  doi          = {10.1049/ipr2.13152},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3013-3027},
  shortjournal = {IET Image Process.},
  title        = {BSMH: Cross-dataset object detection based on box-separated multiple-head},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible thin parts multi-target positioning method of
multi-level feature fusion. <em>IETIP</em>, <em>18</em>(11), 2996–3012.
(<a href="https://doi.org/10.1049/ipr2.13151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In new energy battery manufacturing, machine vision is widely used in automated assembly scenarios for key parts. To improve the accuracy and real-time multi-target positioning recognition of flexible thin parts, this paper proposes a multi-level feature fusion template matching algorithm based on the Gaussian pyramid. Firstly, the algorithm constructs a Gaussian pyramid by multi-scale image construction. Secondly, considering the image features of each layer of the pyramid, this paper uses the grey-based Fast Normalized Matching algorithm to obtain coarse positioning coordinates on the upper layer, and the improved Linemod-2D algorithm is applied to the bottom layer image to get accurate positioning coordinates. Finally, the positioning coordinates returned from each layer are fused to obtain the final positioning coordinate. The experimental results show that the proposed algorithm achieves excellent performance in nickel plate positioning and recognition. It exhibits satisfactory performance in nickel sheet localization and recognition. In terms of angular error, repeat accuracy, and matching speed, it competes favourably with Halcon, VisionMaster, and SCISmart. Its positioning error closely approximates that of Halcon, effectively meeting the practical production demands for high-speed feeding and high-precision positioning.},
  archive      = {J_IETIP},
  author       = {Yaohua Deng and Xiali Liu and Kenan Yang and Zehang Li},
  doi          = {10.1049/ipr2.13151},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2996-3012},
  shortjournal = {IET Image Process.},
  title        = {Flexible thin parts multi-target positioning method of multi-level feature fusion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). B-splines image approximation using resampled chordal
parameterization. <em>IETIP</em>, <em>18</em>(11), 2984–2995. (<a
href="https://doi.org/10.1049/ipr2.13150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing often requires filtering, which can be effectively performed by using B-spline surface approximation. This article presents a fast method of approximating data in rectangular (image) form using such surfaces. The method considers dynamic changes of data by representing rows and columns in chordal parameterized form. To improve performance, after parametrization, lines are uniformly resampled using linear interpolation. It allows using the same basis functions with uniform parameterization for approximation of all processed lines. After approximation, reconstruction of original parameterization is required, but its computational complexity is also linear.},
  archive      = {J_IETIP},
  author       = {Robert Świta and Zbigniew Suszyński},
  doi          = {10.1049/ipr2.13150},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2984-2995},
  shortjournal = {IET Image Process.},
  title        = {B-splines image approximation using resampled chordal parameterization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on image scaling algorithm for granular image
detection. <em>IETIP</em>, <em>18</em>(11), 2974–2983. (<a
href="https://doi.org/10.1049/ipr2.13148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the accuracy of granular image detection during image scaling, an image scaling algorithm combining the interpolation algorithm of protective features and Kalman filtering of neurons is proposed. Firstly, the interpolation algorithm of protective features is de-designed according to the phenomenon of sudden change of edge grey values, and the difference and trend of grey values in horizontal and vertical directions are used to obtain the pre-scaled image with maximum contrast. Then the grey value data input of the pre-scaled image is introduced into the Kalman filter of neurons, and the filtering process is optimized by using the black-box thinking of neurons, and the pre-scaled image is smoothed. Finally, the scale transformation is implemented to obtain the final grey value to complete the final scaling, so that the image has both visual experience and features. Comparison with mainstream image scaling algorithms shows that the proposed algorithm can effectively overcome the degradation of granularity image detection accuracy due to image scaling, with point acutance value (PAV) improved by 10%–41%, and the granularity detection error caused by image scaling algorithms reduced from about 0.7% to about 0.1%, with a relative improvement of 85%.},
  archive      = {J_IETIP},
  author       = {Lirong Yang and Cong Sun},
  doi          = {10.1049/ipr2.13148},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2974-2983},
  shortjournal = {IET Image Process.},
  title        = {Research on image scaling algorithm for granular image detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFE-MVSNet: Multi-scale feature enhancement multi-view
stereo with bi-directional connections. <em>IETIP</em>, <em>18</em>(11),
2962–2973. (<a href="https://doi.org/10.1049/ipr2.13147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning have significantly improved performance in the multi-view stereo (MVS) domain, yet achieving a balance between reconstruction efficiency and quality remains challenging for learning-based MVS methods. To address this, we introduce MFE-MVSNet, designed for more effective and precise depth estimation. Our model incorporates a pyramid feature extraction network, featuring efficient multi-scale attention and multi-scale feature enhancement modules. These components capture pixel-level pairwise relationships and semantic features with long-range contextual information, enhancing feature representation. Additionally, we propose a lightweight 3D UNet regularization network based on depthwise separable convolutions to reduce computational costs. This network employs bi-directional skip connections, establishing a fluid relationship between encoders and decoders and enabling cyclic reuse of building blocks without adding learnable parameters. By integrating these methods, MFE-MVSNet effectively balances reconstruction quality and efficiency. Extensive qualitative and quantitative experiments on the DTU dataset validate our model&#39;s competitiveness, demonstrating approximately 33% and 12% relative improvements in overall score compared to MVSNet and CasMVSNet, respectively. Compared to other MVS networks, our approach more effectively balances reconstruction quality with efficiency.},
  archive      = {J_IETIP},
  author       = {HongWei Lai and ChunLong Ye and Zhenglin Li and Peng Yan and Yang Zhou},
  doi          = {10.1049/ipr2.13147},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2962-2973},
  shortjournal = {IET Image Process.},
  title        = {MFE-MVSNet: Multi-scale feature enhancement multi-view stereo with bi-directional connections},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale feature aggregation network for single-image
dehazing. <em>IETIP</em>, <em>18</em>(11), 2943–2961. (<a
href="https://doi.org/10.1049/ipr2.13146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer possesses a broader perceptual scope, while the Convolutional Neural Network (CNN) excels at capturing local information. In this paper, the authors propose the Multi-Sclale Feature Aggregation Network (MSFA-Net) for single-image dehazing which is fused with the advantages of Transformer and CNN. Our MSFA-Net is based on the encoder–decoder structure, and there are four main innovations. Firstly, the authors make some improvements to the original Swin Transformer to make it more effective for dehazing tasks, and the authors name it Spatial Information Aggregation Transformer (SIAT). The authors place the SIAT in both encoder and decoder of MSFA-Net for feature extraction. The authors propose an upsampling module called Efficient Spatial Resolution Recovery (ESRR) which is placed in the decoder part. Compared to commonly used transposed convolutions, the authors’ ESRR module has fewer computational cost. Considering that the haze distribution is always uneven and the information from each channel is different, the authors introduce the Dynamic Multi-Attention (DMA) module to provide pixel-wise weights and channel-wise weights for input features. The authors place the DMA module between the encoder and decoder parts. As the network depth increases, the spatial structural information from the high-resolution layer tends to degrade. To deal with the problem, the authors propose the Multi-Scale Feature Fusion (MSFF) module to recover missing spatial structural information. The authors place the MSFF module in both the encoder and decoder parts. Extensive experimental results show that the authors’ proposed dehazing network achieves state-of-the-art dehazing performance with relatively low computational cost.},
  archive      = {J_IETIP},
  author       = {Donghui Zhao and Bo Mo and Xiang Zhu},
  doi          = {10.1049/ipr2.13146},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2943-2961},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale feature aggregation network for single-image dehazing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MDTrans: Multi-scale and dual-branch feature fusion network
based on swin transformer for building extraction in remote sensing
images. <em>IETIP</em>, <em>18</em>(11), 2930–2942. (<a
href="https://doi.org/10.1049/ipr2.13145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective extraction of building from remote sensing images requires both global and local information. Despite convolutional neural networks (CNNs) excelling at capturing local details, their intrinsic focus on local operations poses challenge in effectively extracting global features, especially in the context of large-scale buildings. In contrast, transformers excel at capturing global information, but compared to CNNs, they tend to overly rely on large-scale datasets and pre-trained parameters. To tackle the challenge, this paper presents the multi-scale and dual-branch feature fusion network (MDTrans). Specifically, the CNN and transformer branches are integrated in a dual-branch parallel manner during both encoding and decoding stages, local information for small-scale buildings is extracted by utilizing Dense Connection Blocks in the CNN branch, while crucial global information for large-scale buildings is effectively captured through Swin Transformer Block in the transformer branch. Additionally, Dual Branch Information Fusion Block is designed to fuse local and global features from the two branches. Furthermore, Multi-Convolutional Block is designed to further enhance the feature extraction capability of buildings with different sizes. Through extensive experiments on the WHU, Massachusetts, and Inria building datasets, MDTrans achieves intersection over union (IoU) scores of 91.36%, 64.69%, and 79.25%, respectively, outperforming other state-of-the-art models.},
  archive      = {J_IETIP},
  author       = {Kuo Diao and Jinlong Zhu and Guangjie Liu and Meng Li},
  doi          = {10.1049/ipr2.13145},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2930-2942},
  shortjournal = {IET Image Process.},
  title        = {MDTrans: Multi-scale and dual-branch feature fusion network based on swin transformer for building extraction in remote sensing images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized dehazing algorithm based on dark channel prior
with gabor filter and multiscale minimum filter. <em>IETIP</em>,
<em>18</em>(11), 2918–2929. (<a
href="https://doi.org/10.1049/ipr2.13143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional dehazing algorithms have long been limited in their ability to remove fog effectively, especially in preserving details. This study proposes an improved dark channel prior dehazing algorithm designed to overcome the limitations of traditional algorithms by refining the transmission map and estimating atmospheric light intensity. The algorithm fully exploits the anti-haze characteristics of the Gabor filter to extract multi-directional texture features. By fusing these features to form a guidance map for guided filtering, it effectively reduces the blurring caused by guided filtering on the image edges, thereby producing a more accurate transmission map. Simultaneously, an enhanced atmospheric light successfully reduces interference from white objects in the image. The experiment phase utilized the publicly available RESIDE dataset for validation. The algorithm achieved a Peak Signal-to-Noise Ratio (PSNR) of 22.4 and a Structural Similarity Index (SSIM) of 0.88. These metrics indicate the algorithm&#39;s superior dehazing capabilities.},
  archive      = {J_IETIP},
  author       = {Feiran Fu and Zelong Zhang and Xuena Geng and Jing Xu and Ming Fang},
  doi          = {10.1049/ipr2.13143},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2918-2929},
  shortjournal = {IET Image Process.},
  title        = {Optimized dehazing algorithm based on dark channel prior with gabor filter and multiscale minimum filter},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Head pose estimation with particle swarm optimization-based
contrastive learning and multimodal entangled GCN. <em>IETIP</em>,
<em>18</em>(11), 2899–2917. (<a
href="https://doi.org/10.1049/ipr2.13142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation is an especially challenging task due to the complexity nonlinear mapping from 2D feature space to 3D pose space. To address the above issue, this paper presents a novel and efficient head pose estimation framework based on particle swarm optimized contrastive learning and multimodal entangled graph convolution network. Firstly, a new network, the region and difference-aware feature pyramid network (RD-FPN), is proposed for 2D keypoints detection to alleviate the background interference and enhance the feature expressiveness. Then, particle swarm optimized contrastive learning is constructed to alternatively match 2D and 3D keypoints, which takes the multimodal keypoints matching accuracy as the optimization objective, while considering the similarity of cross-modal positive and negative sample pairs from contrastive learning as a local contrastive constraint. Finally, multimodal entangled graph convolution network is designed to enhance the ability of establishing geometric relationships between keypoints and head pose angles based on second-order bilinear attention, in which point-edge attention is introduced to improve the representation of geometric features between multimodal keypoints. Compared with other methods, the average error of our method is reduced by 8.23%, indicating the accuracy, generalization, and efficiency of our method on the 300W-LP, AFLW2000, BIWI datasets.},
  archive      = {J_IETIP},
  author       = {Yuanfeng Lian and Yinliang Shi and Zhaonian Liu and Bin Jiang and Xingtao Li},
  doi          = {10.1049/ipr2.13142},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2899-2917},
  shortjournal = {IET Image Process.},
  title        = {Head pose estimation with particle swarm optimization-based contrastive learning and multimodal entangled GCN},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSD-YOLO: Small traffic sign detection based on improved
YOLO v8. <em>IETIP</em>, <em>18</em>(11), 2884–2898. (<a
href="https://doi.org/10.1049/ipr2.13141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign detection is critical for autonomous driving technology. However, accurately detecting traffic signs in complex traffic environments remains challenge despite the widespread use of one-stage detection algorithms known for their real-time processing capabilities. In this paper, the authors propose a traffic sign detection method based on YOLO v8. Specifically, this study introduces the Space-to-Depth (SPD) module to address missed detections caused by multi-scale variations of traffic signs in traffic scenes. The SPD module compresses spatial information into depth channels, expanding the receptive field and enhancing the detection capabilities for objects of varying sizes. Furthermore, to address missed detections caused by complex backgrounds such as trees, this paper employs the Select Kernel attention mechanism. This mechanism enables the model to dynamically adjust its focus and more effectively concentrate on key features. Additionally, considering the uneven distribution of training data, the authors adopted the WIoUv3 loss function, which optimizes loss calculation through a weighted approach, thereby improving the model&#39;s detection performance across various sizes and frequencies of instances. The proposed methods were validated on the CCTSDB and TT100K datasets. Experimental results demonstrate that the authors’ method achieves substantial improvements of 3.2% and 5.1% on the mAP50 metric compared to YOLOv8s, while maintaining high detection speed, significantly enhancing the overall performance of the detection system. The code for this paper is located at https://github.com/dusongjie/TSD-YOLO-Small-Traffic-Sign-Detection-Based-on-Improved-YOLO-v8},
  archive      = {J_IETIP},
  author       = {Songjie Du and Weiguo Pan and Nuoya Li and Songyin Dai and Bingxin Xu and Hongzhe Liu and Cheng Xu and Xuewei Li},
  doi          = {10.1049/ipr2.13141},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2884-2898},
  shortjournal = {IET Image Process.},
  title        = {TSD-YOLO: Small traffic sign detection based on improved YOLO v8},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GDB-YOLOv5s: Improved YOLO-based model for ship detection in
SAR images. <em>IETIP</em>, <em>18</em>(11), 2869–2883. (<a
href="https://doi.org/10.1049/ipr2.13140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning methods were good solutions for object detection in synthetic aperture radar (SAR) images. However, the problems of complex scenarios, large object scale differences and imperfect fine-grained classification in ship detection were intractable. In response, an improved model GDB-YOLOv5s (Improved YOLOv5s model incorporating global attention mechanism (GAM), DCN-v2 and BiFusion) is designed. This model introduces deformable convolution networks (DCN-v2) into the Backbone to enhance the adaptability of the receptive field. It replaces the original Neck&#39;s PANet structure with a BiFusion structure to better fuse the extracted multiscale features. Additionally, it integrates GAM into the network to reduce information loss and improve global feature interaction. Experiments were conducted on single-class dataset SSDD and multi-class dataset SRSSD-V1.0. The results show that the GDB-YOLOv5s model improves mean average precision scores (mAP) significantly, outperforming the original YOLOv5s model and other traditional methods. GDB-YOLOv5s also improves Precision-score (P) and Recall-score (R) for fine-grained classification to some extent, thereby reducing false alarms and missed detections. It has been proved that the improved model is relatively effective.},
  archive      = {J_IETIP},
  author       = {Dongdong Chen and Rusheng Ju and Chuangye Tu and Guangwei Long and Xiaoyang Liu and Jiyuan Liu},
  doi          = {10.1049/ipr2.13140},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2869-2883},
  shortjournal = {IET Image Process.},
  title        = {GDB-YOLOv5s: Improved YOLO-based model for ship detection in SAR images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MDSK-net: Multi-scale dynamic segmentation kernel network
for renal tumour endoscopic image segmentation. <em>IETIP</em>,
<em>18</em>(11), 2855–2868. (<a
href="https://doi.org/10.1049/ipr2.13139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of renal tumours during renal cell carcinoma surgery can help doctors accurately locate the tumour region, protect the tissues and organs around the kidneys, enhance surgical efficiency, and reduce the possibility of leakage and misdiagnosis. However, since general polyp endoscopic image segmentation models have many problems when facing the task of renal tumour segmentation, there needs to be more research on the segmentation of endoscopic images of renal tumours. This paper proposes a multi-scale dynamic segmentation kernel network for endoscopic image segmentation of kidney tumours. First, a spatial receptive field module is proposed to augment the feature information and improve the performance of the whole network. Second, an enhanced cross-attention module is offered to attenuate the effect of a high-similarity segmentation background. Finally, a multi-scale dynamic segmentation kernel module is introduced to gradually refine the segmentation results from small to large sizes to obtain more accurate tumour boundaries. Extensive experiments on the established kidney tumour endoscopic dataset and publicly available endoscopic datasets show that this method exhibits enhanced performance and generalization capabilities compared to existing techniques. On this renal tumour dataset, MDSK-Net achieved excellent results of 94.1% and 90.1% on mDice and mIoU.},
  archive      = {J_IETIP},
  author       = {Minpeng Jiang and LeiLei Li and Chao Xu and Zhengping Li and Chao Nie and Tianyu Zheng and Longyu Li},
  doi          = {10.1049/ipr2.13139},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2855-2868},
  shortjournal = {IET Image Process.},
  title        = {MDSK-net: Multi-scale dynamic segmentation kernel network for renal tumour endoscopic image segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A metaheuristic image cryptosystem using improved parallel
model and many-objective optimization. <em>IETIP</em>, <em>18</em>(11),
2838–2854. (<a href="https://doi.org/10.1049/ipr2.13126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic is one of the techniques to improve the security of image encryption. However, existing metaheuristic image cryptosystems based on metaheuristic may have convergence difficulties during the optimization process, which cause insecurity and slow convergence. Besides, the time cost of the parallel execution model applied to metaheuristic image cryptosystems is not low enough. Therefore, a parallel many-objective optimized key generation framework is proposed. Firstly, if four or more security indicators of cryptosystem, which are the results of security test, need to be optimized, the many-objective optimization algorithm is employed to the proposed framework. With method adjusts the chaotic system parameters as the optimization key, which effectively avoid the convergence difficulty of the encryption key. Secondly, a master-slave parallel model is improved to metaheuristic encryption. The model allocates the most time-consuming fitness calculation work to slave nodes, which makes the modified model more reasonable and thus reduces the encryption time. To evaluate the performance of the proposed framework, a specific encryption scheme is constructed, that utilizes a 2D quadric map and many-objective optimization algorithm based on dominance and decomposition (MOEA/DD) to optimize five security indicators. Experimental results reveal that this scheme has good security performances and less parallel encryption time.},
  archive      = {J_IETIP},
  author       = {Zihao Xin and Sheng Liu and Xiaoyi Zhou and Wenbao Han and Jianqiang Ma},
  doi          = {10.1049/ipr2.13126},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2838-2854},
  shortjournal = {IET Image Process.},
  title        = {A metaheuristic image cryptosystem using improved parallel model and many-objective optimization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Light field imaging technology for virtual reality content
creation: A review. <em>IETIP</em>, <em>18</em>(11), 2817–2837. (<a
href="https://doi.org/10.1049/ipr2.13144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The light field (LF) imaging technique can capture 3D scene information in 4D by recording both 2D intensity and 2D direction of incoming light rays. Due to this capability, LF has shown a great interest in virtual reality (VR) and augmented reality (AR) for enhanced immersion, improved depth perception and reconstruction of realistic 3D environments. This paper presents a comprehensive review of LF imaging technology and other approaches used for VR content creation. The applications of LF technology beyond VR and AR are also discussed. The challenges and limitations of other approaches for VR content creation are examined. State-of-the-art research has focused on how VR experiences benefit from LF technology and identified the challenges to creating comfortable, immersive and realistic VR content such as (1) image size and resolution, (2) processing speed, (3) precise calibration and (4) depth reconstruction. Recommendations that can be considered for creating immersive VR content are provided to enhance user experience. These recommendations aim to contribute to developing more comfortable and realistic VR content, extending the potential applications of LF imaging technology in diverse fields.},
  archive      = {J_IETIP},
  author       = {Ali Khan and Md. Moinul Hossain and Alexandra Covaci and Konstantinos Sirlantzis and Chuanlong Xu},
  doi          = {10.1049/ipr2.13144},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2817-2837},
  shortjournal = {IET Image Process.},
  title        = {Light field imaging technology for virtual reality content creation: A review},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SIHNet: A safe image hiding method with less information
leaking. <em>IETIP</em>, <em>18</em>(10), 2800–2815. (<a
href="https://doi.org/10.1049/ipr2.13138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hiding is a task that hides secret images into cover images. The purposes of image hiding are to ensure the secret images are invisible to the human and the secret images can be recovered. The current state-of-the-art steganography methods run the risk of secret information leakage. A safe image hiding network (SIHNet) is presented to reduce the leakage of secret information. Based on some phenomena of image hiding methods which use invertible neural network, a reversible secret image processing (SIP) module is proposed to make the secret images suitable for hiding and make the stego images leak less secret information. Besides, a reversible lost information hiding (LIH) module is used to hide the lost information into the cover images, thus the method can recover the secret images better than the method that uses random noise to replace the lost information. Experimental results show that SIHNet outperforms other state-of-the-art methods on the PSNR and SSIM values of the recovered secret images and the stego images. Besides, residual images of other state-of-the-art methods all contain information about secret images while residual images of SIHNet leak almost no secret information. Thus the method can prevent the listener of transmission channel from obtaining the information of the secret image through the residual image, which means SIHNet performs better in security than other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Zien Cheng and Xin Jin and Qian Jiang and Liwen Wu and Yunyun Dong and Wei Zhou},
  doi          = {10.1049/ipr2.13138},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2800-2815},
  shortjournal = {IET Image Process.},
  title        = {SIHNet: A safe image hiding method with less information leaking},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-scale resolution consistent network for salient object
detection. <em>IETIP</em>, <em>18</em>(10), 2788–2799. (<a
href="https://doi.org/10.1049/ipr2.13136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The salient object detection task tries to simulate the human visual system for most eye-catching objects or regions detection. However, due to the complexity of the visual mechanisms, current methods will suffer from severe performance degradation, leading to inconsistent prediction results for the same regions, when directly adopting a model trained on a fixed resolution to evaluate at other different resolutions. Considering that consistency in predictions is essential for salient object detection, a cross-scale resolution consistent salient object detection method, called RCNet, is proposed. Specifically, to enhance the model&#39;s capacity for generalization across images of varying resolutions and make the model implicitly learn the scale invariance, a multi-resolution data enhancement module is constructed to generate images with arbitrary resolutions for the same scene. Moreover, to accomplish better multi-level feature fusion, a cross-scale fusion module is developed to fuse high-level semantic features and low-level detail features. Additionally, to explicitly learn the scale invariance of the salient scores, a hybrid salient consistency loss is formulated on salient object detection with different resolutions. Comprehensive evaluations on five benchmark datasets show that RCNet achieves a highly competitive result.},
  archive      = {J_IETIP},
  author       = {Xiaoyu Huang and Wei Liu and Minghui Li and Hangyu Nie},
  doi          = {10.1049/ipr2.13136},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2788-2799},
  shortjournal = {IET Image Process.},
  title        = {Cross-scale resolution consistent network for salient object detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ADF-net: Attention-guided deep feature decomposition network
for infrared and visible image fusion. <em>IETIP</em>, <em>18</em>(10),
2774–2787. (<a href="https://doi.org/10.1049/ipr2.13134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively enhance the ability to acquire information by making full use of the complementary features of infrared and visible images, the widely used image fusion algorithm is faced with challenges such as information loss and image blurring. In response to this issue, the authors propose a dual-branch deep hierarchical fusion network (ADF-Net) guided by an attention mechanism. Initially, the attention convolution module extracts the shallow features of the image. Subsequently, a dual-branch deep decomposition feature extractor is introduced, where in the transformer encoder block (TEB) employs remote attention to process low-frequency global features, while the CNN encoder block (CEB) extracts high-frequency local information. Ultimately, the global fusion layer based on TEB and the local fusion layer based on CEB produce the fused image through the encoder. Multiple experiments demonstrate that ADF-Net excels in various aspects by utilizing two-stage training and an appropriate loss function for training and testing.},
  archive      = {J_IETIP},
  author       = {Sen Shen and Taotao Zhang and Haidi Dong and ShengZhi Yuan and Min Li and RenKai Xiao and Xiaohui Zhang},
  doi          = {10.1049/ipr2.13134},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2774-2787},
  shortjournal = {IET Image Process.},
  title        = {ADF-net: Attention-guided deep feature decomposition network for infrared and visible image fusion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Change detection in SAR image based on weighted difference
image generation and optimized random forest. <em>IETIP</em>,
<em>18</em>(10), 2754–2773. (<a
href="https://doi.org/10.1049/ipr2.13133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar (SAR) image change detection suffers from poor quality of the difference image and low detection accuracy. Hence, this paper proposes a SAR image change detection method based on a fused difference image and an optimized random forest scheme, termed LRN-SSARF. Specifically, a fusion operator difference image LRN is proposed, which is generated using a weighted fusion of log-ratio (LR), ratio (R), and normalized ratio (NoR). This difference image generation method reduces noise&#39;s influence. Then, the Otsu algorithm is applied to segment the difference image and select the training samples. The training samples are input into the random forest (RF) model optimised by the sparrow search algorithm (SSA) for training and classification. Finally, the region link is uesd to refine the detection results and generate the final result. The change detection results of six real SAR image scenes highlight that the proposed algorithm has a high detection accuracy, and affords appealing integrity and detailed information about the change regions. Specially, the detection accuracy advantage of the Bangladesh dataset is larger, with the accuracy and Kappa coefficient reaching 98.04% and 92.00%, much higher than the competitor methods.},
  archive      = {J_IETIP},
  author       = {Mengting Yuan and Zhihui Xin and Guisheng Liao and Penghui Huang and Yongxin Li},
  doi          = {10.1049/ipr2.13133},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2754-2773},
  shortjournal = {IET Image Process.},
  title        = {Change detection in SAR image based on weighted difference image generation and optimized random forest},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topology-aware anatomical segmentation of the circle of
willis: HUNet unveils the vascular network. <em>IETIP</em>,
<em>18</em>(10), 2745–2753. (<a
href="https://doi.org/10.1049/ipr2.13132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigates the Circle of Willis, a critical vascular structure vital for cerebral blood supply. A modified novel dual-pathway multi-scale hierarchical upsampling network (HUNet) is presented, tailored explicitly for accurate segmentation of Circle of Willis anatomical components from medical imaging data. Evaluating both the multi-label magnetic resonance angiography region of interest and the multi-label magnetic resonance angiography whole brain-case datasets, HUNet consistently outperforms the convolutional U-net model, demonstrating superior capabilities and achieving higher accuracy across various classes. Additionally, the HUNet model achieves an exceptional dice similarity coefficient of 98.61 and 97.95, along with intersection over union scores of 73.32 and 85.76 in both the multi-label magnetic resonance angiography region of interest and the multi-label magnetic resonance angiography whole brain-case datasets, respectively. These metrics highlight HUNet&#39;s exceptional performance in achieving precise and accurate segmentation of anatomical structures within the Circle of Willis, underscoring its robustness in medical image segmentation tasks. Visual representations substantiate HUNet&#39;s efficacy in delineating Circle of Willis structures, offering comprehensive insights into its superior performance.},
  archive      = {J_IETIP},
  author       = {Md. Shakib Shahariar Junayed and Kazi Shahriar Sanjid and Md. Tanzim Hossain and M. Monir Uddin and Sheikh Anisul Haque},
  doi          = {10.1049/ipr2.13132},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2745-2753},
  shortjournal = {IET Image Process.},
  title        = {Topology-aware anatomical segmentation of the circle of willis: HUNet unveils the vascular network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RRER: A refined registration method based on contrast
minimum for event and RGB cameras. <em>IETIP</em>, <em>18</em>(10),
2732–2744. (<a href="https://doi.org/10.1049/ipr2.13131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precise perception of the surrounding environment in traffic scenes is an important part of an intelligent transportation system. The event camera could provide complementary information to traditional frame-based cameras, such as high dynamic range, and high time resolution, in the perception of traffic targets. To improve the precision and reliability of perception as well as facilitate lots of RGB camera-based studies introduced to event cameras directly, a refined registration method for event-based cameras and RGB cameras on the basis of pixel-level region segmentation is proposed, to provide a fusion method at pixel level. A total of eight sequences and a dataset containing 260 typical traffic scenes are contained in the experiment dataset, both selected from DSEC, a traffic event-based dataset. The registered event image shows a better spatial consistency with RGB images visually. Compared to the baseline, the evaluation indicators, such as the performance of the contrast, the proportion of overlapping pixels, and average registration accuracy have been improved. In the traffic object segmentation task, the average boundary displacement error of our method has decreased and the max decline value has reached 79.665%, compared to the boundary displacement error between ground truth and baseline. These results indicate prospective applications in the perception of intelligent transportation systems combined with event and RGB cameras. The traffic dataset with pixel-level semantic annotations will be provided soon.},
  archive      = {J_IETIP},
  author       = {Shijie Zhang and Tao Tang and Fan Sang and Xuan Pei and Taogang Hou},
  doi          = {10.1049/ipr2.13131},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2732-2744},
  shortjournal = {IET Image Process.},
  title        = {RRER: A refined registration method based on contrast minimum for event and RGB cameras},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RFLSE: Joint radiomics feature-enhanced level-set
segmentation for low-contrast SPECT/CT tumour images. <em>IETIP</em>,
<em>18</em>(10), 2715–2731. (<a
href="https://doi.org/10.1049/ipr2.13130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doctors typically use non-contrast-enhanced computed tomography (NCECT) in the treatment of kidney cancer to map kidney and tumour structural information to functional imaging single-photon emission computed tomography, which is then used to assess patient kidney function and predict postoperative recovery. However, the assessment of kidney function and formulation of surgical plans is constrained by the low contrast of tumours in NCECT, which hinders the acquisition of accurate tumour boundaries. Therefore, this study designed a radiomics feature-enhanced level-set evolution (RFLSE) to precisely segment small-sample low-contrast kidney tumours. Integration of high-dimensional radiomics features into the level-set energy function enhances the edge detection capability of low-contrast kidney tumours. The use of sensitive radiomics features to control the regional term parameters achieves adaptive adjustment of the curve evolution amplitude, improving the level-set segmentation process. The experimental data used low-contrast, limited-sample tumours provided by hospitals, as well as the public datasets BUSI18 and KiTS19. Comparative results with advanced energy functionals and deep learning models demonstrate the precision and robustness of RFLSE segmentation. Additionally, the application value of RFLSE in assisting doctors with accurately marking tumours and generating high-quality pseudo-labels for deep learning datasets is demonstrated.},
  archive      = {J_IETIP},
  author       = {Zhaotong Guo and Pinle Qin and Jianchao Zeng and Rui Chai and Zhifang Wu and Jinjing Zhang and Jia Qin and Zanxia Jin and Pengcheng Zhao and Yixiong Wang},
  doi          = {10.1049/ipr2.13130},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2715-2731},
  shortjournal = {IET Image Process.},
  title        = {RFLSE: Joint radiomics feature-enhanced level-set segmentation for low-contrast SPECT/CT tumour images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TanrsColour: Transformer-based medical image colourization
with content and structure preservation. <em>IETIP</em>,
<em>18</em>(10), 2702–2714. (<a
href="https://doi.org/10.1049/ipr2.13129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image colouring techniques enable to colourize grey-scale medical images for assisting doctors in diagnosis. Benefiting from the non-linear fitting ability of deep neural network, deep medical image colouring techniques have achieved remarkable results. However, existing methods are still facing content and structure feature leakage, unrealistic colouring and poor scale invariability. Thus, this paper, proposes a Transformer-based medical image colouring algorithm with long-term dependency to avoid feature leakage of coloured images. To be specific, this method employs two different Transformer encoders to generate and encode feature sequences for grey-scale medical images and real human colour slice images, respectively. Then, a novel multi-layer Transformer decoder is used to stylize grey-scale map image features based on the real physical colour feature sequences. For colouring images at different scales, we implement content- aware positional encoding with scale invariance and propose style-aware positional encoding strategy to take realistic and physical colour prior into account. Extensive experimental results indicate our method has achieved better colourization effects than recent state-of-the-art medical image colourization methods.},
  archive      = {J_IETIP},
  author       = {Qinghai Liu and Dengping Zhao and Lun Tang and Limin Xu},
  doi          = {10.1049/ipr2.13129},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2702-2714},
  shortjournal = {IET Image Process.},
  title        = {TanrsColour: Transformer-based medical image colourization with content and structure preservation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image super-resolution reconstruction based on implicit
image functions. <em>IETIP</em>, <em>18</em>(10), 2690–2701. (<a
href="https://doi.org/10.1049/ipr2.13128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) reconstruction is a key technique for improving image quality and details. Conventional methods are frequently limited by interpolation, filtering, or statistical approaches; thus, they are unable to reconstruct high-quality continuously enlarged images with detailed information. This study proposes an image SR reconstruction network model, called LALNet, based on implicit image functions and residual multilayered perceptron (RAMLP) with an attention mechanism. Through the implicit image function and RAMLP + attention, high-quality SR reconstruction with continuous scale factors is achieved, and LALNets can run on embedded edge computing platforms. This method exhibits the following advantages: lightweight network structure reduces computing requirements, introduction of implicit image functions and RAMLP improves reconstruction quality, and attention mechanism suppresses artefacts and distortions. Experimental results show that LALNet outperforms traditional and other deep learning methods in terms of reconstruction performance and computational efficiency. This research provides new ideas and methods for the further development of the field of image SR reconstruction.},
  archive      = {J_IETIP},
  author       = {Hai Lin and JunJie Yang},
  doi          = {10.1049/ipr2.13128},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2690-2701},
  shortjournal = {IET Image Process.},
  title        = {Image super-resolution reconstruction based on implicit image functions},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hiding image into image with hybrid attention mechanism
based on GANs. <em>IETIP</em>, <em>18</em>(10), 2679–2689. (<a
href="https://doi.org/10.1049/ipr2.13127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganography is the art of concealing secret information within images to prevent detection. In deep-learning-based image steganography, a common practice is to fuse the secret image with the cover image to directly generate the stego image. However, not all features are equally critical for data hiding, and some insignificant ones may lead to the appearance of residual artifacts in the stego image. In this article, a novel network architecture for image steganography with hybrid attention mechanism based on generative adversarial network is introduced. This model consists of three subnetworks: a generator for generate stego images, an extractor for extracting the secret images, and a discriminator to simulate the detection process, which aids the generator in producing more realistic stego images. A specific hybrid attention mechanism (HAM) module is designed that effectively fuses information across channel and spatial domains, facilitating adaptive feature refinement within deep image representations. The experimental results suggest that the HAM module not only enhances the image quality during both the steganography and extraction processes but also improves the model&#39;s undetectability. Stego images are mixed with varying levels of noise in the training process, which can further improve robustness. Finally, it is verified that the model outperforms current steganography approaches on three datasets and exhibits good undetectability.},
  archive      = {J_IETIP},
  author       = {Yuling Zhu and Yunyun Dong and Bingbing Song and Shaowen Yao},
  doi          = {10.1049/ipr2.13127},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2679-2689},
  shortjournal = {IET Image Process.},
  title        = {Hiding image into image with hybrid attention mechanism based on GANs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight multiscale smoke segmentation algorithm based
on improved DeepLabV3+. <em>IETIP</em>, <em>18</em>(10), 2665–2678. (<a
href="https://doi.org/10.1049/ipr2.13125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fires not only cause devastating consequences for human life and property, but also lead to soil erosion in forests. Therefore, it is necessary to design a novel algorithm that can quickly monitor smoke from fires. Most existing smoke segmentation methods do not consider the segmentation accuracy of algorithms under limited computational resources. To address this research gap, this paper proposes a lightweight smoke segmentation algorithm based on DeepLabV3+ that achieves fast inference speed and high accuracy for different sizes smoke. To reduce the number of parameters, the feature extraction network of the DeeplabV3+ algorithm is replaced by MobileNetV2, which enhances the extraction ability of the algorithm in segment smoke images. Then, the Convolutional Block Attention Module (CBAM) is added to the encoder part to enhance the perception of the algorithm for small smoke and effectively alleviates smoke mis-segmentation. Furthermore, a newly designed loss function is used in the network. The experimental results show that the proposed method has improved by 1.27% in Smoke IoU and 1.21% in mPA compared with other methods. The weight size has been reduced to 10.76% of the original DeepLabV3+, and the inference time is only 33.71ms. Therefore, it is a more suitable early fire detection algorithm for resource-constrained environments.},
  archive      = {J_IETIP},
  author       = {Xin Chen and Qingshan Hou and Yan Fu and Yaolin Zhu},
  doi          = {10.1049/ipr2.13125},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2665-2678},
  shortjournal = {IET Image Process.},
  title        = {A lightweight multiscale smoke segmentation algorithm based on improved DeepLabV3+},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cantonese sentence dataset for lip-reading. <em>IETIP</em>,
<em>18</em>(10), 2645–2664. (<a
href="https://doi.org/10.1049/ipr2.13123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lip-reading deciphers speech by observing lip movements without relying on audio data. The rapid advancements in deep learning have significantly improved lip-reading for both English and Chinese; however, research on dialects such as Cantonese remains scarce. Consequently, most Chinese lip-reading datasets focus on Mandarin, with only a few addressing Cantonese. To bridge this gap, a sentence-level Cantonese lip-reading dataset, designated as Cantonese lip-reading sentences are introduced, comprising over 500 unique speakers and more than 30,000 samples. To ensure alignment with real-world scenarios, no restrictions are imposed on factors such as gender, age, posture, lighting conditions, or speech rate. A comprehensive description of the pipeline employed is provided for collecting and constructing the dataset and introduce an innovative visual frontend, 3D-visual attention net. This frontend combines the advantages of convolution and self-attention mechanisms to extract fine-grained lip region features. These features are subsequently input into the conformer backend for temporal sequence modelling, achieving comparable performance on Chinese Mandarin lip reading dataset, lip reading sentences 2, lip reading sentences 3, and Cantonese lip-reading sentences datasets. Benchmark tests on Cantonese lip-reading sentences demonstrate the challenges it poses, providing a novel research foundation for dialect lip-reading and fostering the advancement of Cantonese lip-reading tasks.},
  archive      = {J_IETIP},
  author       = {Yewei Xiao and Xuanming Liu and Lianwei Teng and Aosu Zhu and Picheng Tian and Jian Huang},
  doi          = {10.1049/ipr2.13123},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2645-2664},
  shortjournal = {IET Image Process.},
  title        = {Cantonese sentence dataset for lip-reading},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the generalization of image denoising via
structure-preserved MLP-based denoiser and generative diffusion prior.
<em>IETIP</em>, <em>18</em>(10), 2625–2644. (<a
href="https://doi.org/10.1049/ipr2.13122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising aims to remove noise from images and improve the quality of images. However, most image denoising methods heavily rely on pairwise training strategies and strict prior knowledge about image structure or noise distribution. While these methods exhibit significant results when handling known types of noise, their generalization performance diminishes when confronted with images containing unknown noise distributions. To address this issue, a two-stage approach is introduced for enhancing the generalizability of image denoising. The proposed method does not rely on a large amount of paired data or prior knowledge of the noise type and level. Instead, it constructs a denoising pipeline with improved generalizability through an MLP-based denoiser and generative diffusion prior. Specifically, in the first stage, an initial denoised image is predicted with a structure resembling that of the underlying clean image by introducing an MLP-based U-shaped denoising network aided by an implicit structural prior. In the second stage, the generalizability and quality of the denoiser are further enhanced by conditioning the result obtained from the previous stage on the pretrained denoising diffusion null-space model. Extensive experimentation on multiple datasets demonstrates that this method exhibits better denoising performance and generalizability than other image denoising methods.},
  archive      = {J_IETIP},
  author       = {Jing Wu and Ruilin Xie and Hao Wu and Guowu Yuan},
  doi          = {10.1049/ipr2.13122},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2625-2644},
  shortjournal = {IET Image Process.},
  title        = {Improving the generalization of image denoising via structure-preserved MLP-based denoiser and generative diffusion prior},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maritime distress target detection algorithm based on
YOLOv5s-EFOE network. <em>IETIP</em>, <em>18</em>(10), 2614–2624. (<a
href="https://doi.org/10.1049/ipr2.13120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional maritime search and rescue methods mainly rely on manual operation, which takes a long time to identify and results in poor search and rescue results. This paper applies computer vision technology to the field of maritime distress object detection and proposes an improved object detection algorithm YOLOv5s-EFOE based on the YOLOv5s algorithm. Firstly, the authors change the detection head of the YOLOv5s algorithm to use a mixed channel strategy to build a more efficient decoupling head, reducing the number of 3 × 3 convolutional layers in the middle of the decoupling head to only one. The width of the head is scaled by the width multipliers of Backbone and Neck. Secondly, in conjunction with the SimOTA matching algorithm, the positive samples of the target to be tested are dynamically allocated to improve the recognition ability of different targets. Finally, considering the low pixel value of the maritime distress target from the perspective of unmanned aerial vehicles (UAV), the loss function CIoU in the original YOLOv5s is changed to EIoU. EIoU not only considers the distance and aspect ratio of the centre point, but also considers the true difference between the width and height of the prediction box and the real box, which improves the prediction accuracy of the anchor box. Experiments are conducted on a subset of the public dataset SeaDronesSee. The of the YOLOv5s-EFOE model proposed in this paper reached 75.9%, reached 79.9%, and reached 44.5%. These indicators are superior to the original YOLOv5s model, YOLOv7 series models, and YOLOv8 series models. Compared with the original model, the YOLOv5s-EFOE model has increased the by 5.4%, by 5.6%, and by 4.6%, improving the difficult to detect and missed detection situations. This model can be deployed on UAVs and can effectively identify maritime distress target, achieving the purpose of search and rescue.},
  archive      = {J_IETIP},
  author       = {Kun Liu and Hongru Ma and Guofeng Xu and Jianglong Li},
  doi          = {10.1049/ipr2.13120},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2614-2624},
  shortjournal = {IET Image Process.},
  title        = {Maritime distress target detection algorithm based on YOLOv5s-EFOE network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Microalgae detection based on improved YOLOv5.
<em>IETIP</em>, <em>18</em>(10), 2602–2613. (<a
href="https://doi.org/10.1049/ipr2.13119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of algae in microscopic image plays a crucial role in water quality monitoring. However, the existing object detection methods still face challenges in accurately detecting different categories of algae in microscopic image. In order to improve the accuracy, an improved YOLOv5s model is proposed for microalgae detection, by combining a Receptive Field Enhancement (RFE) module, the Wise-IoU v3 dynamic non-monotonic focal loss function, and a Dynamic head (Dyhead), which is termed receptive field enhancement wise-IoU dyhead (RWD)-you only look once (YOLO). Firstly, to detect microalgae of various scales, the Bottleneck in the C3 module of YOLOv5s is replaced with a more reasonable RFE module. Secondly, Wise-IoU v3 is applied to enhance detection accuracy by assigning varying weights between high-quality and low-quality images. Finally, Dyhead is introduced to enhance the representation capacity of the detection head by integrating three attention mechanisms: scale awareness, spatial awareness, and task awareness. The proposed RWD-YOLO model significantly enhances the accuracy of algae detection in microscopic image. Specifically, the experimental results on the microalgae dataset show that the RWD-YOLO achieves an mAP@0.5 of 93.2% and an mAP@0.5:0.95 of 65.1%. Compared to the original YOLOv5s, mAP@0.5 and mAP@0.5:0.95 are improved by 3.7% and 5.7%, respectively.},
  archive      = {J_IETIP},
  author       = {Ziqiang Duan and Ting Xie and Lucai Wang and Yang Chen and Jie Wu},
  doi          = {10.1049/ipr2.13119},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2602-2613},
  shortjournal = {IET Image Process.},
  title        = {Microalgae detection based on improved YOLOv5},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A 4D spontaneous micro-expression database: Establishment
and evaluation. <em>IETIP</em>, <em>18</em>(10), 2584–2601. (<a
href="https://doi.org/10.1049/ipr2.13118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions are spontaneous and unconscious facial movements that reveal individuals’ genuine inner emotions. They hold significant potential in various psychological testing fields. As the face is a 3D deformation object, the emergence of facial expression leads to spatial deformation of the face. However, existing databases primarily offer 2D video sequences, limiting descriptions of 3D spatial information related to micro-expressions. Here, a new micro-expression database is proposed, which contains 2D image sequences and corresponding 3D point cloud sequences. These samples were classified using both an objective method based on the facial action coding system and a non-objective emotion classification method that considers video contents and participants’ self-reports. A variety of feature extraction techniques are applied to 2D data, including traditional algorithms and deep learning methods. Additionally, a novel local curvature-based algorithm is developed to extract 3D spatio-temporal deformation features from the 3D data. The authors evaluated the classification accuracies of these two features individually and their fusion results under leave-one-subject-out (LOSO) and tenfold cross-validation. The results demonstrate that fusing 3D features with 2D features results in improved recognition performance compared to using 2D features alone.},
  archive      = {J_IETIP},
  author       = {Fengping Wang and Jie Li and Chun Qi and Lin Wang and Pan Wang},
  doi          = {10.1049/ipr2.13118},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2584-2601},
  shortjournal = {IET Image Process.},
  title        = {A 4D spontaneous micro-expression database: Establishment and evaluation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive prediction: Video anomaly detection via
multi-grained prediction. <em>IETIP</em>, <em>18</em>(10), 2568–2583.
(<a href="https://doi.org/10.1049/ipr2.13117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Anomaly Detection (VAD) has been an active research field for several decades. However, most existing approaches merely extract a single type of feature from videos and define a single paradigm to indicate the extent of abnormalities. A coarse-to-fine three-level prediction is built by integrating different levels of spatio-temporal representations, better highlighting the difference between normal and abnormal behaviors. First, an object-level trajectory prediction is proposed to model human historical position using a graph transformer network. Subsequently, skeleton-level prediction is achieved by incorporating the positional information from the trajectory prediction. More importantly, based on the predicted skeleton, a skeleton-guided pixel-level region prediction is performed. A novel Skeleton Conditioned Generative Adversarial Network (SCGAN) is designed to explore the correlation between skeleton-level and pixel-level motion prediction. Benefiting from SCGAN, the prediction of human regions is contributed by both coarse-grained and fine-grained motion features. This three-level prediction, namely Progressive Prediction Video Anomaly Detection (P 3 VAD), enlarges the prediction error on irregular motion patterns. Besides, a pixel-level analysis method is proposed to achieve Background-bias Elimination (BE) and denoise the predicted region. Experimental results validate the effectiveness of P 3 VAD on the four benchmark datasets (ShanghaiTech, CUHK Avenue, IITB-Corridor, and ADOC).},
  archive      = {J_IETIP},
  author       = {Xianlin Zeng and Yalong Jiang and Yufeng Wang and Qiang Fu and Wenrui Ding},
  doi          = {10.1049/ipr2.13117},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2568-2583},
  shortjournal = {IET Image Process.},
  title        = {Progressive prediction: Video anomaly detection via multi-grained prediction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factor annealing decoupling compositional training method
for imbalanced hyperspectral image classification. <em>IETIP</em>,
<em>18</em>(10), 2553–2567. (<a
href="https://doi.org/10.1049/ipr2.13116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to differences in the quantity and size of observed targets, hyperspectral images are characterized by class imbalance. The standard deep learning classification model training scheme optimizes the overall classification error, which may lead to performance imbalance between classes in hyperspectral image classification frameworks. Therefore, a novel factor annealing decoupling compositional training method is proposed in this paper. Without requiring resampling or reweighting, it implicitly modulates the training process, so standard models can sufficiently learn the representation of the minority classes and further be trained as robust classifiers. Specifically, the label-distribution-aware margin loss is combined with the error-rate-based cross-entropy loss via combination factor, which considers both imbalanced data representation learning and classifier overall performance. Then, a factor annealing optimization training scheme is designed to adjust the combination factor, which solves the stage division problem of two-stage decoupling learning. Experimental results on two hyperspectral image datasets demonstrate that, as compared with other competing approaches, the proposed method can continuously and stably optimize the model parameters, achieving improvements in class average metrics and difficult classes without affecting overall classification performance.},
  archive      = {J_IETIP},
  author       = {Xiaojun Li and Yi Su and Junping Yao and Yi Guo and Shuai Fan},
  doi          = {10.1049/ipr2.13116},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2553-2567},
  shortjournal = {IET Image Process.},
  title        = {Factor annealing decoupling compositional training method for imbalanced hyperspectral image classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stereo matching from monocular images using feature
consistency. <em>IETIP</em>, <em>18</em>(10), 2540–2552. (<a
href="https://doi.org/10.1049/ipr2.13114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic images facilitate stereo matching. However, synthetic images may suffer from image distortion, domain bias, and stereo mismatch, which would significantly restrict the widespread use of stereo matching models in the real world. The first goal in this paper is to synthesize real-looking images for minimizing the domain bias between the synthesized and real images. For this purpose, sharpened disparity maps are produced from a mono real image. Then, stereo image pairs are synthesized using these imperfect disparity maps and the single real image in the proposed pipeline. Although the synthesized images are as realistic as possible, the domain styles of the synthesized images are always very different from the real images. Thus, the second goal is to enhance the domain generalization ability of the stereo matching network. For that, the feature extraction layer is replaced with a teacher–student model. Then, a constraint of binocular contrast features is imposed on the output of the model. When tested on the KITTI, ETH3D, and Middlebury datasets, the accuracy of the method outperforms traditional methods by at least 30%. Experiments demonstrate that the approaches are general and can be conveniently embedded into existing stereo networks.},
  archive      = {J_IETIP},
  author       = {Zhongjian Lu and An Chen and Hongxia Gao and Langwen Zhang and Congyu Zhang and Yang Yang},
  doi          = {10.1049/ipr2.13114},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2540-2552},
  shortjournal = {IET Image Process.},
  title        = {Stereo matching from monocular images using feature consistency},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of optic disc and optic cup segmentation based on
fundus images. <em>IETIP</em>, <em>18</em>(10), 2521–2539. (<a
href="https://doi.org/10.1049/ipr2.13115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optic disc (OD) and optic cup (OC) segmentation is an important task in ophthalmic medicine and is crucial for aiding glaucoma screening. With the development of smart healthcare and the increase of large datasets, there is an increasing number of research efforts targeting OD and OC segmentation, making it particularly important to provide a systematic review of the latest advances in the field. This paper presents a systematic review of commonly used datasets, evaluation metrics, and related research results in the field of OD and OC segmentation. The advantages and disadvantages of segmentation techniques based on traditional and deep learning methods are comparatively analysed. In addition, this study emphasizes the importance of OD and OC segmentation efforts in smart healthcare. Despite the technological advances, the lack of generalization capability is still a major obstacle limiting its clinical application. To address this issue, this study explores unsupervised domain adaptation methods to enhance the generalization performance of segmentation techniques and provide new strategies for clinical diagnosis. Finally, this paper discusses the challenges and future research directions faced by OD and OC segmentation when applied in the medical field to help readers comprehensively grasp the research dynamics in this area.},
  archive      = {J_IETIP},
  author       = {Xiaoyue Ma and Guiqun Cao and Yuanyuan Chen},
  doi          = {10.1049/ipr2.13115},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2521-2539},
  shortjournal = {IET Image Process.},
  title        = {A review of optic disc and optic cup segmentation based on fundus images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ADIR: Advanced domain-invariant representation via
decoupling learning and information bottleneck. <em>IETIP</em>,
<em>18</em>(9), 2506–2520. (<a
href="https://doi.org/10.1049/ipr2.13113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrepancy in data distribution between training and testing scenarios, as well as the inductive bias of convolutional neural networks towards image styles, reduces the model&#39;s generalization ability. Many unsupervised domain generalization methods based on feature decoupling suffer from an initial neglect of explicit decoupling of content and style features, resulting in content features that still contain considerable redundant information, thereby restricting improvements in generalization capability. To tackle this problem, this paper optimizes the learning process of domain-invariant (content) features into an information compression issue, minimizing redundancy in content features. Furthermore, to enhance decoupled learning, this paper introduces innovative cross-domain loss functions and image reconstruction modules that explicitly decouple and merge content and style across different domains. Extensive experiments demonstrate the method&#39;s significant enhancements over recent cutting-edge approaches.},
  archive      = {J_IETIP},
  author       = {Yangyang Zhong and Yunfeng Yan and Pengxin Luo and Yuhao Zhou and Donglian Qi},
  doi          = {10.1049/ipr2.13113},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2506-2520},
  shortjournal = {IET Image Process.},
  title        = {ADIR: Advanced domain-invariant representation via decoupling learning and information bottleneck},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLO-UOD: An underwater small object detector via improved
efficient layer aggregation network. <em>IETIP</em>, <em>18</em>(9),
2490–2505. (<a href="https://doi.org/10.1049/ipr2.13112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of underwater objects is a key indicator technology to effectively enhance the field of marine development and application, and is of great importance to various fields including marine military defense and seafood aquaculture. Efficient and rapid detection of underwater targets is a crucial technological challenge in this field. To meet the challenges posed by these issues, this study applies the convolutional omni-efficient layer aggregation network (CO-ELAN) module to the detector backbone to improve the ability of the network structure to acquire underwater objects from image information. The module improves the feature representation of gradient branching through a multi-dimensional dynamic convolution and attention mechanism. In terms of loss calculation, the optimized normalized Wasserstein distance approach is used to predict the box distribution probabilistic modelling method to determine comparable distances to the ground box and obtain better samples of small target labels. Here, an underwater image enhancement algorithm based on white balance and underwater blur fusion is used to obtain clear images that enable improved detector performance. After the verification experiment on the URPC2018 dataset, it is found that the detector has better underwater detection ability compared with other detectors in the complex underwater environment. The proposed method achieves a 2.4% improvement over the YOLOv7 baseline model, while reducing computation costs by 5%.},
  archive      = {J_IETIP},
  author       = {Weiwen Chen and Tingting Zhuang and Yuanfang Zhang and Teng Mei and Xiaoyu Tang},
  doi          = {10.1049/ipr2.13112},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2490-2505},
  shortjournal = {IET Image Process.},
  title        = {YOLO-UOD: An underwater small object detector via improved efficient layer aggregation network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). SEHRNet: A lightweight, high-resolution network for
aircraft keypoint detection. <em>IETIP</em>, <em>18</em>(9), 2476–2489.
(<a href="https://doi.org/10.1049/ipr2.13111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current research on apron conflict detection is often limited to the interaction between the aircraft as a whole and other objects, making it difficult to accomplish targeted identification of vulnerable and high-cost aircraft components. However, the implementation of detailed aircraft identification is of great significance to enhance the safety of airport surface operations. Based on the excellent performance of High-Resolution Network (HRNet) in keypoint detection, a lightweight end-to-end keypoint detection network, namely Squeeze and Excitation High-Resolution Network (SEHRNet), is proposed in this paper to solve the problems of HRNet&#39;s slower computation and more redundancy. First, the errors arising from coordinate transformations in the coding and decoding process are solved by an improved feature map coding and decoding process. Second, replace the BasicBlock in HRNet with the Depthwise separable convolutions based on the Squeeze-and-Excitation Networks, which drastically cuts the computational cost of the network. Third, the improved Bottleneck module is used to further enhance the capability of feature extraction. Experimental results prove that, based on the aircraft keypoint detection dataset, the SEHRNet proposed in this paper shows stronger applicability compared to the current mainstream networks. Compared with the original HRNet, the improved network has higher accuracy, faster speed, and lighter model for aircraft keypoint detection.},
  archive      = {J_IETIP},
  author       = {Zhiqiang Zhang and Tianxiong Zhang and Xinping Zhu and Jiajun Li},
  doi          = {10.1049/ipr2.13111},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2476-2489},
  shortjournal = {IET Image Process.},
  title        = {SEHRNet: A lightweight, high-resolution network for aircraft keypoint detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DAF-retinex: Preserve the image detailed features and
restore the reflected image. <em>IETIP</em>, <em>18</em>(9), 2461–2475.
(<a href="https://doi.org/10.1049/ipr2.13110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, deep learning methods for low-light image enhancement tasks mainly focus on the illumination of images, while neglecting the problems of image noise and feature loss. To address this issue, this paper proposes a novel low-light image enhancement network called DAF-Retinex, based on the Retinex-Net. To address the issue of image noise, different from traditional image denoise methods, this paper utilizes a fully convolutional neural network to denoise the reflection component, additionally, a denoising loss function is introduced to suppress noise. For preserving image details and extracting features, this paper creatively introduces self-calibrated convolutions into low-light image enhancement tasks, furthermore, a feature augmented attention block consisting of feature-guided attention (FGA) is designed for feature learning to effectively enhance image illumination and extract image detail features. Experimental results demonstrate that the proposed algorithm in this paper effectively removes image noise and extracts detailed features, resulting in visually improved outcomes. On public datasets, the average improvement in objective evaluation metrics of image quality such as PSNR, SSIM, and NIQE are 1.13%, 4.12%, and 1.28%, respectively.},
  archive      = {J_IETIP},
  author       = {Shiyu Huang and Zijun Gao and Jue Wang and Bo Li},
  doi          = {10.1049/ipr2.13110},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2461-2475},
  shortjournal = {IET Image Process.},
  title        = {DAF-retinex: Preserve the image detailed features and restore the reflected image},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling appearance variations in expressive and neutral
face image for automatic facial expression recognition. <em>IETIP</em>,
<em>18</em>(9), 2449–2460. (<a
href="https://doi.org/10.1049/ipr2.13109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In automatic facial expression recognition (AFER) systems, modelling the spatio-temporal feature information in a specific manner, coalescing, and its effective utilization is challenging. The state-of-the-art studies have examined integrating multiple features to enhance the recognition rate of AFER systems. However, the feature variations between expressive and neutral face images are not fully explored to identify the expression class. The proposed research presents an innovative approach to AFER by modelling appearance variations in both expressive and neutral face images. The prominent contributions of the work are developing a novel and hybrid feature space by integrating the discriminative feature distribution derived from expressive and neutral face images; preserving the highly discriminative latent feature distribution using autoencoders. Local binary pattern (LBP) and histogram of oriented gradients (HOG) are the feature descriptors employed to derive the discriminative texture and shape information, respectively. The component-based approach is employed, wherein the features are derived from the salient facial regions instead of the whole face. The three-stage stacked deep convolutional autoencoder (SDCA) and multi-class support vector machine (MSVM) are employed to address dimensionality reduction and classification, respectively. The efficacy of the proposed model is substantiated by empirical findings, which establish its superiority in terms of accuracy in AFER tasks on widely recognized benchmark datasets.},
  archive      = {J_IETIP},
  author       = {Naveen Kumar H N and Guru Prasad M S and Mohd Asif Shah and Mahadevaswamy and Jagadeesh B and Sudheesh K},
  doi          = {10.1049/ipr2.13109},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2449-2460},
  shortjournal = {IET Image Process.},
  title        = {Modelling appearance variations in expressive and neutral face image for automatic facial expression recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MACN: A cascade defect detection for complex background
based on mixture attention mechanism. <em>IETIP</em>, <em>18</em>(9),
2434–2448. (<a href="https://doi.org/10.1049/ipr2.13108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection in complex background is a critical issue. To address this issue, this paper proposes the mixture attention mechanism cascade network, in which the new channel attention network is linked with the spatial attention network to create an effective mixed attention network that takes advantage of their respective advantages, adaptively suppresses background noise, and highlights defect features. To ensure the efficiency and effectiveness of effective mixed attention network, the new channel attention network splices the output features of the global average pooling layer and the global maximum pooling layer and then sends the spliced features into a shared network, which is a one-dimensional convolutional network, and uses cross-channel interaction for fusion. Furthermore, in order to provide more discriminative feature representation, the authors extract the intermediate features of the region proposal network and input them into effective mixed attention network. Finally, the cascade head is used to refine the predicted bounding box to achieve high-quality defect location. To demonstrate the superiority and usefulness of this method, it is compared to the latest method using widely used PCB and NEU data sets. A large number of trials demonstrate that this strategy outperforms other methods for detecting defects in complicated backgrounds.},
  archive      = {J_IETIP},
  author       = {Langyue Zhao and Yiquan Wu and Yubin Yuan and Kang Tong},
  doi          = {10.1049/ipr2.13108},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2434-2448},
  shortjournal = {IET Image Process.},
  title        = {MACN: A cascade defect detection for complex background based on mixture attention mechanism},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LPNet: A remote sensing scene classification method based on
large kernel convolution and parameter fusion. <em>IETIP</em>,
<em>18</em>(9), 2417–2433. (<a
href="https://doi.org/10.1049/ipr2.13107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing scene images contain numerous feature targets with unrelated semantic information, so how to extract to the local key information and semantic features of the image becomes the key to achieving accurate classification. Existing Convolutional Neural Networks (CNNs) mostly concentrate on the global representation of an image and lose the shallow features. To overcome these issues, this paper proposes LPNet for remote sensing scene image classification. First, LPNet employs LKConv to extract the semantic features in the image, while using standard convolution to extract local key information in the image. Additionally, the LPNet applies a shortcut residual concatenation branch to reuse features. Then, parameter fusion combines parameters from previous branches, improving the capacity of the model to obtain a more comprehensive and rich feature representation of the image. Finally, considering the relationship between the classification ability of the model and the depth of feature extraction, the Feature Mixture (FM) Block is used to deepen the model for feature extraction. Comparative experiments on four publicly available datasets show that LPNet provides comparable results to other state-of-the-art methods. The effectiveness of LPNet is further demonstrated by visualizing the effective receptive fields (ERFs).},
  archive      = {J_IETIP},
  author       = {Guowei Wang and Furong Shi and Xinyu Wang and Haixia Xu and Liming Yuan and Xianbin Wen},
  doi          = {10.1049/ipr2.13107},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2417-2433},
  shortjournal = {IET Image Process.},
  title        = {LPNet: A remote sensing scene classification method based on large kernel convolution and parameter fusion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective screening of COVID-19 pneumonia by employing
chest x-ray segmentation and attention-based ensembled classification.
<em>IETIP</em>, <em>18</em>(9), 2400–2416. (<a
href="https://doi.org/10.1049/ipr2.13106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quick and accurate diagnosis of COVID-19 is crucial in preventing its transmission. Chest X-ray (CXR) imaging is often used for diagnosis, however, even experienced radiologists may misinterpret the results, necessitating computer-aided diagnosis. Deep learning has yielded favourable results previously, but overfitting, excessive variance, and generalization errors may occur due to noise and limited datasets. Ensemble learning can improve predictions by using robust techniques. Therefore, this study, proposes two-fold strategy that combines advanced and robust algorithms, including DenseNet201, EfficientNetB7, and Xception, to achieve faster and more accurate COVID-19 detection. Segmented lung images were generated from CXR images using the residual U-Net model, and two attention-based ensemble neural networks were used for classification. The COVID-19 radiography dataset was used to evaluate the proposed approach, which achieved an accuracy of 98.21%, 93.4%, and 89.06% for two, three, and four classes respectively which outperformed previous studies by a significant margin considering COVID, viral pneumonia, and lung opacity simultaneously. Despite the similarity in CXR images of COVID, pneumonia, and lung opacity, the proposed approach achieved 89.06% accuracy, demonstrating its ability to recognize distinguishable features. The developed algorithm is expected to have applications in clinics for diagnosing different diseases using X-ray images.},
  archive      = {J_IETIP},
  author       = {Abu Sayeed and Nasif Osman Khansur and Azmain Yakin Srizon and Md. Farukuzzaman Faruk and Salem A. Alyami and AKM Azad and Mohammad Ali Moni},
  doi          = {10.1049/ipr2.13106},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2400-2416},
  shortjournal = {IET Image Process.},
  title        = {An effective screening of COVID-19 pneumonia by employing chest X-ray segmentation and attention-based ensembled classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised hyperspectral images classification using
hypergraph convolutional extreme learning machines. <em>IETIP</em>,
<em>18</em>(9), 2389–2399. (<a
href="https://doi.org/10.1049/ipr2.13105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem that traditional methods are difficult to fully utilize the rich spectral information in hyperspectral images (HSI) and fail to capture the complex higher-order relations in hyperspectral data, which leads to limited classification performance extreme learning machine and fails to further improve the classification accuracy of HSIs, the authors propose the hypergraph convolutional extreme learning machine (HGCELM) method. The method not only inherits all the advantages of extreme learning machine (ELM), but also embeds hypergraph convolution for feature selection, which is capable of handling higher-order relations. This enables HGCELM to capture more complex relationships between nodes and provide richer representation capabilities. At the same time, the training speed advantage of ELM is retained, thus speeding up the model training process. Experimental results show that the proposed algorithm achieves better accuracy compared to other clustering algorithms.},
  archive      = {J_IETIP},
  author       = {Hongrui Zhang and Hongfei Lv and Mengke Wang and Luyao Wang and Jinhuan Xu and Fenggui Wang and Xiangdong Li},
  doi          = {10.1049/ipr2.13105},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2389-2399},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised hyperspectral images classification using hypergraph convolutional extreme learning machines},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal attention modules in
orientation-magnitude-response guided multi-stream CNNs for human action
recognition. <em>IETIP</em>, <em>18</em>(9), 2372–2388. (<a
href="https://doi.org/10.1049/ipr2.13104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new descriptor called orientation-magnitude response maps as a single 2D image to effectively explore motion patterns. Moreover, boosted multi-stream CNN-based model with various attention modules is designed for human action recognition. The model incorporates a convolutional self-attention autoencoder to represent compressed and high-level motion features. Sequential convolutional self-attention modules are used to exploit the implicit relationships within motion patterns. Furthermore, 2D discrete wavelet transform is employed to decompose RGB frames into discriminative coefficients, providing supplementary spatial information related to the actors actions. A spatial attention block, implemented through the weighted inception module in a CNN-based structure, is designed to weigh the multi-scale neighbours of various image patches. Moreover, local and global body pose features are combined by extracting informative joints based on geometry features and joint trajectories in 3D space. To provide the importance of specific channels in pose descriptors, a multi-scale channel attention module is proposed. For each data modality, a boosted CNN-based model is designed, and the action predictions from different streams are seamlessly integrated. The effectiveness of the proposed model is evaluated across multiple datasets, including HMDB51, UTD-MHAD, and MSR-daily activity, showcasing its potential in the field of action recognition.},
  archive      = {J_IETIP},
  author       = {Fatemeh Khezerlou and Aryaz Baradarani and Mohammad Ali Balafar and Roman Gr. Maev},
  doi          = {10.1049/ipr2.13104},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2372-2388},
  shortjournal = {IET Image Process.},
  title        = {Spatio-temporal attention modules in orientation-magnitude-response guided multi-stream CNNs for human action recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HGSNet: A hypergraph network for subtle lesions segmentation
in medical imaging. <em>IETIP</em>, <em>18</em>(9), 2357–2371. (<a
href="https://doi.org/10.1049/ipr2.13103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lesion segmentation is a fundamental task in medical image processing, often facing the challenge of subtle lesions. It is important to detect these lesions, even though they can be difficult to identify. Convolutional neural networks, an effective method in medical image processing, often ignore the relationship between lesions, leading to topological errors during training. To tackle topological errors, move is made from pixel-level to hypergraph representations. Hypergraphs can model lesions as vertices connected by hyperedges, capturing the topology between lesions. This paper introduces a novel dynamic hypergraph learning strategy called DHLS. DHLS allows for the dynamic construction of hypergraphs contingent upon input vertex variations. A hypergraph global-aware segmentation network, termed HGSNet, is further proposed. HGSNet can capture the key high-order structure information, which is able to enhance global topology expression. Additionally, a composite loss function is introduced. The function emphasizes the global aspect and the boundary of segmentation regions. The experimental setup compared HGSNet with other advanced models on medical image datasets from various organs. The results demonstrate that HGSNet outperforms other models and achieves state-of-the-art performance on three public datasets.},
  archive      = {J_IETIP},
  author       = {Junze Wang and Wenjun Zhang and Dandan Li and Chao Li and Weipeng Jing},
  doi          = {10.1049/ipr2.13103},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2357-2371},
  shortjournal = {IET Image Process.},
  title        = {HGSNet: A hypergraph network for subtle lesions segmentation in medical imaging},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic segmentation of remote sensing images based on
dual-channel attention mechanism. <em>IETIP</em>, <em>18</em>(9),
2346–2356. (<a href="https://doi.org/10.1049/ipr2.13101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inadequate utilization of data correlation and complementarity in the feature extraction process of multimodal remote sensing images, the paper proposes a deep learning semantic segmentation algorithm based on the Dual Channel Attention Mechanism (DCAM). This algorithm uses U-Net as the backbone, combining the RGB remote sensing image as one input channel with the Convolutional Block Attention Module to extract colour space features. Simultaneously, it utilizes near-infrared (NIR) as another input channel with the Self-Attention Module (SAM) to extract shape space features. Finally, by concatenating the multi-scale attention features of the RGB remote sensing image channel and the NIR remote sensing image channel, it achieves the correlation and complementarity of contextual features between the two modal remote sensing images. Experimental results on the GID-15 dataset demonstrate that the DCAM algorithm significantly improves the segmentation accuracy, edge segmentation quality, and object segmentation integrity for various types of targets compared to current mainstream segmentation methods.},
  archive      = {J_IETIP},
  author       = {Jionghui Jiang and Xi&#39;an Feng and Hui Huang},
  doi          = {10.1049/ipr2.13101},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2346-2356},
  shortjournal = {IET Image Process.},
  title        = {Semantic segmentation of remote sensing images based on dual-channel attention mechanism},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A transformer-based lightweight method for multiple-object
tracking. <em>IETIP</em>, <em>18</em>(9), 2329–2345. (<a
href="https://doi.org/10.1049/ipr2.13099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the multi-object tracking method based on transformer generally uses its powerful self-attention mechanism and global modelling ability to improve the accuracy of object tracking. However, most existing methods excessively rely on hardware devices, leading to an inconsistency between accuracy and speed in practical applications. Therefore, a lightweight transformer joint position awareness algorithm is proposed to solve the above problems. Firstly, a joint attention module to enhance the ShuffleNet V2 network is proposed. This module comprises the spatio-temporal pyramid module and the convolutional block attention module. The spatio-temporal pyramid module fuses multi-scale features to capture information on different spatial and temporal scales. The convolutional block attention module aggregates channel and spatial dimension information to enhance the representation ability of the model. Then, a position encoding generator module and a dynamic template update strategy are proposed to solve the occlusion. Group convolution is adopted in the input sequence through position encoding generator module, with each convolution group responsible for handling the relative positional relationships of a specific range. In order to improve the reliability of the template, dynamic template update strategy is used to update the template at the appropriate time. The effectiveness of the approach is validated on the MOT16, MOT17, and MOT20 datasets.},
  archive      = {J_IETIP},
  author       = {Qin Wan and Zhu Ge and Yang Yang and Xuejun Shen and Hang Zhong and Hui Zhang and Yaonan Wang and Di Wu},
  doi          = {10.1049/ipr2.13099},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2329-2345},
  shortjournal = {IET Image Process.},
  title        = {A transformer-based lightweight method for multiple-object tracking},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight fruit detection algorithms for low-power
computing devices. <em>IETIP</em>, <em>18</em>(9), 2318–2328. (<a
href="https://doi.org/10.1049/ipr2.13098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A lightweight fruit detection algorithm is important to ensure real-time detection on low-power computing devices while maintaining detection accuracy. In addition, the fruit detection algorithm is also faced with some environmental factors. To solve these challenges, lightweight detection algorithms termed YOLO-Lite, YOLO-Liter and YOLO-Litest were developed based on the YOLOv5 framework. The compared mean average precision (mAP) detection revealed that YOLO-Lite at 0.86 is 2%, 4%, 5%, 7%, and 16% more than YOLO-Liter and YOLOv5n at 0.84 each, YOLOv4-tiny at 0.82, YOLO-Liter at 0.81, YOLO-MobileNet at 0.79, and YOLO-ShuffleNet at 0.70, respectively, but not for YOLOv8n at 0.87. On the Computer platform, except for YOLOv4-tiny at 178.6 frames per second (FPS), the speed of YOLO-Litest at 158.7 FPS is faster than YOLO-Liter at 129.9 FPS, YOLO-Lite at 120.5 FPS, YOLO-ShuffleNet at 119.0 FPS, YOLOv8n at 116 FPS, YOLOv5n at 111.1 FPS, and YOLO-MobileNet at 89.3 FPS. Using Jetson Nano, the 32.3 FPS of YOLO-Litest is faster than other algorithms, but not YOLOv4-tiny&#39;s 34.1 FPS. On the Raspberry Pi 4B, YOLO-Litest with 4.69 FPS, outperformed other algorithms. The choices for an accurate and faster detection algorithm are YOLO-Lite and YOLO-Litest respectively, while YOLO-Liter maintains a balance between them.},
  archive      = {J_IETIP},
  author       = {Olarewaju Mubashiru Lawal and Huamin Zhao and Shengyan Zhu and Liu Chuanli and Kui Cheng},
  doi          = {10.1049/ipr2.13098},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2318-2328},
  shortjournal = {IET Image Process.},
  title        = {Lightweight fruit detection algorithms for low-power computing devices},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Local feature-based video captioning with multiple
classifier and CARU-attention. <em>IETIP</em>, <em>18</em>(9),
2304–2317. (<a href="https://doi.org/10.1049/ipr2.13096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning aims to identify multiple objects and their behaviours in a video event and generate captions for the current scene. This task aims to generate a detailed description of the current video in real-time using natural language, which requires deep learning to analyze and determine the relationships between interesting objects in the frame sequence. In practice, existing methods typically involve detecting objects in the frame sequence and then generating captions based on features extracted through object coverage locations. Therefore, the results of caption generation are highly dependent on the performance of object detection and identification. This work proposes an advanced video captioning approach that works in adaptively and effectively addresses the interdependence between event proposals and captions. Additionally, an attention-based multimodel framework is introduced to capture the main context from the frame and sound in the video scene. Also, an intermediate model is presented to collect the hidden states captured from the input sequence, which performs to extract the main features and implicitly produce multiple event proposals. For caption prediction, the proposed method employs the CARU layer with attention consideration as the primary RNN layer for decoding. Experimental results showed that the proposed work achieves improvements compared to the baseline method and also better performance compared to other state-of-the-art models on the ActivityNet dataset, presenting competitive results in the tasks of video captioning.},
  archive      = {J_IETIP},
  author       = {Sio-Kei Im and Ka-Hou Chan},
  doi          = {10.1049/ipr2.13096},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2304-2317},
  shortjournal = {IET Image Process.},
  title        = {Local feature-based video captioning with multiple classifier and CARU-attention},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight tomato leaf disease identification method
based on shared-twin neural networks. <em>IETIP</em>, <em>18</em>(9),
2291–2303. (<a href="https://doi.org/10.1049/ipr2.13094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of tomato leaf spot disease is essential for control and loss reduction. Traditional algorithms face challenges such as large amount of data, multiple training and heavy computation. In this study, a lightweight shared Siamese neural network method was proposed for tomato leaf disease identification, which is suitable for resource-limited environments. Experiments on Plant-Village, Taiwan and Taiwan ++ datasets show that the accuracy fluctuates very little even when trained with only 60% of the data, which confirms the effectiveness of the proposed method in the small data environment. In addition, compared with the mainstream algorithms, it improves the accuracy by up to 35.3%on Plant-Village and two Taiwan datasets respectively. The experimental results also show that the proposed method still performs well when the data is imbalanced and the sample size is small.},
  archive      = {J_IETIP},
  author       = {Wang Linfeng and Liu Jiayao and Liu Yong and Wang Yunsheng and Xu Shipu},
  doi          = {10.1049/ipr2.13094},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2291-2303},
  shortjournal = {IET Image Process.},
  title        = {A lightweight tomato leaf disease identification method based on shared-twin neural networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-target detection and tracking of shallow marine
organisms based on improved YOLO v5 and DeepSORT. <em>IETIP</em>,
<em>18</em>(9), 2273–2290. (<a
href="https://doi.org/10.1049/ipr2.13090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the related problems of detection and tracking of shallow marine organisms, this paper designs a YOLO v5 multi-target detection and tracking algorithm with attention mechanism. When working underwater, the authors usually encounter many difficulties. Different luminosity and complex coral background usually affect the detection of marine organisms. At the same time, the unrestricted movement of marine organisms, the ability to hide behind rocks and algae, and their mutual occlusion while swimming pose additional challenges to this task. Considering the characteristics of shallow marine organisms activity environment, the attention mechanism is added to the feature extraction network of YOLO v5 to reduce redundant information and improve the detection accuracy of shallow marine organism targets in complex environment. The improved algorithm improves the average detection accuracy of marine organisms target detection by 3.2%. Aiming at the problem of shallow marine organisms target tracking, a shallow marine organisms multi-target tracking algorithm based on improved Deep Simple Online And Realtime Tracking (SORT) is designed. The improved YOLO v5 algorithm is used to replace Faster R-CNN (Faster Region-Convolutional Neural Networks) as the detector of DeepSORT tracking algorithm, and the cascade matching strategy is adopted to solve the problem that the target cannot be tracked continuously when it is occluded for a long time. The experimental results show that the shallow marine organisms multi-target tracking algorithm based on improved DeepSORT reduces the number of id transformation of marine biological target tracking in shallow sea environment, and improves the accuracy of shallow marine organisms multi-target tracking.},
  archive      = {J_IETIP},
  author       = {Yang Liu and Bailin An and Shaohua Chen and Dongmei Zhao},
  doi          = {10.1049/ipr2.13090},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2273-2290},
  shortjournal = {IET Image Process.},
  title        = {Multi-target detection and tracking of shallow marine organisms based on improved YOLO v5 and DeepSORT},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video object segmentation via couple streams and feature
memory. <em>IETIP</em>, <em>18</em>(9), 2257–2272. (<a
href="https://doi.org/10.1049/ipr2.13051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, most video segmentation methods use deep CNN to process the input image, but they did not fully mine the rich intermediate predictions in spatio-temporal space. And, the segmentation challenges such as occlusion, severe deformation and illumination have not been well solved so far. To alleviate these problems, this paper focuses on constructing multi module network structures that represent multi semantics and proposes a video object segmentation network via coupled-stream architecture with feature memory mechanism. This network first extracts high-level semantic features, edge features, long-term and short-term stable depth features of the target, and then decode them into the segmentation mask of target. In addition, negative skeleton inhibition and frame interpolation are used to prevent the interference of similar objects and motion blur, respectively. The method has a low GPU memory usage, regardless of the number of object in video. And performs 86.5%and 62.4% in J&amp;F measure on DAVIS 2016 and DAVIS 2017 validation set, without fine-tuning and online training.},
  archive      = {J_IETIP},
  author       = {Yun Liang and Xinjie Xiao and Shaojian Qiu and Yuqing Zhang and Zhuo Su},
  doi          = {10.1049/ipr2.13051},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2257-2272},
  shortjournal = {IET Image Process.},
  title        = {Video object segmentation via couple streams and feature memory},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Color subspace exploring for natural image matting.
<em>IETIP</em>, <em>18</em>(9), 2244–2256. (<a
href="https://doi.org/10.1049/ipr2.13012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have seen a surge of successful methods in natural image matting. However, the overlap of foreground and background color distributions in an image is still troubling in matting. It is observed that the three color channels contain different contrast information of an image: some color channels may provide clearer contrast information for separating the foreground from the image, while the foreground and background color distributions in other channels may heavily overlap, resulting in blurred foreground-background boundaries. Motivated by this observation, the Color Subspace Exploring Network (CSEMat) is proposed to extract the foreground object from an image by exploring high-contrast appearance information in individual color spaces. Specifically, a 4-branch encoder is constructed, with one branch for the RGB image and three branches for subdividing the color space. Each color channel is individually processed by a sub-encoder. Additionally, the trimap-based color information aggregation module (CIA) is introduced to integrate the feature maps from the independent sub-encoders, facilitating the transfer of optimized features to the decoder. Extensive experiments demonstrate that the proposed CSEMat achieves favorable performance on publicly available matting datasets.},
  archive      = {J_IETIP},
  author       = {Yating Kong and Jide Li and Liangpeng Hu and Xiaoqiang Li},
  doi          = {10.1049/ipr2.13012},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2244-2256},
  shortjournal = {IET Image Process.},
  title        = {Color subspace exploring for natural image matting},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network architecture for single image super-resolution: A
comprehensive review and comparison. <em>IETIP</em>, <em>18</em>(9),
2215–2243. (<a href="https://doi.org/10.1049/ipr2.13100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR) is a promising research direction in computer vision and image processing for improving the visual perception of low-quality images. In recent years, deep learning algorithms have driven tremendous development in SR, and SR methods based on various network architectures have significantly improved the quality of reconstructed images. Although there has been a large amount of reviews focusing on SISR, few studies have focused specifically on network architectures for SISR. This paper aims to provide a systematic overview of the design ideas of SISR using multiple architectures, including Convolutional Neural Networks (CNN), Generative Adversarial Networks (GAN), Transformer, and Diffusion model. In addition, an experimental analysis and comparison of state-of-the-art SR algorithms have been performed on publicly available quantitative and qualitative datasets. Finally, some future directions are discussed that may help other community researchers.},
  archive      = {J_IETIP},
  author       = {Zhicun Zhang and Yu Han and Linlin Zhu and Xiaoqi Xi and Lei Li and Mengnan Liu and Siyu Tan and Bin Yan},
  doi          = {10.1049/ipr2.13100},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2215-2243},
  shortjournal = {IET Image Process.},
  title        = {Network architecture for single image super-resolution: A comprehensive review and comparison},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VGA-net: Vessel graph based attentional u-net for retinal
vessel segmentation. <em>IETIP</em>, <em>18</em>(8), 2191–2213. (<a
href="https://doi.org/10.1049/ipr2.13102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation is crucial in diagnosing retinal diseases by accurately identifiying retinal vessels. This paper addresses the complexity of segmenting retinal vessels, highlighting the need for precise analysis of blood vessel structures. Despite the progress made by convolutional neural networsks (CNNs) in image segmentation, their limitations in capturing the global structure of retinal vsessels and maintaining segmentation continuity present challenges. To tackle these issues, our proposed network integrates graph convolutional networks (GCNs) and attention mechansims. This allows the model to consider pixel relationships and learn vessel graphical structures, significantly improving segmentation accuracy. Additionally, the attentional feature fusion module, including pixel-wise and channel-wise attention mechansims within the U-Net architecture, refines the model&#39;s focus on relevant features. This paper emphasizes the importance of continuty preservation, ensuring an accurate representation of pixel-level information and structural details during sefmentation. Therefore, our method performs as an effective solution to overcome challenges in retinal vessel segmentation. The proposed method outperformed the state-of-the-art approaches on DRIVE (Digital Retinal Images for Vessel Extraction) and STARE (Structed Analysis of the Retina) datasets with accuracies of 0.12% and 0.14%, respecttively. Importantly, our proposed approach excelled in delineating slender and diminutive blood vessels, crucial for diagnosing vascular-related diseases. Implementation is accessible on https://github.com/CVLab-SHUT/VGA-Net .},
  archive      = {J_IETIP},
  author       = {Yeganeh Jalali and Mansoor Fateh and Mohsen Rezvani},
  doi          = {10.1049/ipr2.13102},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2191-2213},
  shortjournal = {IET Image Process.},
  title        = {VGA-net: Vessel graph based attentional U-net for retinal vessel segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature comparison residuals for foreign fibre
classification model. <em>IETIP</em>, <em>18</em>(8), 2179–2190. (<a
href="https://doi.org/10.1049/ipr2.13097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various types of foreign fibres may be mixed in the planting, transportation, and production processes of cotton, which not only cause equipment to be out of control, but also leads to a decrease in the quality of cotton textile products and economic losses. The machine vision based detection method for cotton foreign fibres is widely used. Based on existing related research, we construct a classification dataset for cotton foreign fibres in practical scenarios, named the CF2113-10 dataset. The authors design a basic foreign fibre classification network called CottonNet that balances performance and efficiency. The classification accuracy on the validation set reached 94.2%. In order to enhance the high-level feature extraction ability, this paper improves the feature fusion method of residual networks and proposes CottonNet-Res, which improves the classification accuracy to 95.1%. Finally, a classification model based on feature difference fitting, CottonNet-Fusion, is proposed to address the classification problem of foreign fibre images sampled in complex environments. The classification accuracy of foreign fibre images sampled in ordinary scenes has improved to 97.4%, while the images sampled in complex environments maintain an accuracy of 90.3%.},
  archive      = {J_IETIP},
  author       = {Wei Wei and Xue Zhou and Zhen Huang and Zhiwei Su},
  doi          = {10.1049/ipr2.13097},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2179-2190},
  shortjournal = {IET Image Process.},
  title        = {Feature comparison residuals for foreign fibre classification model},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point cloud registration method for indoor depth sensor
acquisition system based on dual graph computation with irregular shape
factors. <em>IETIP</em>, <em>18</em>(8), 2161–2178. (<a
href="https://doi.org/10.1049/ipr2.13095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The registration performance determines the widespread indoor application of 3D models acquired by depth sensors. Many advanced registration methods lack comprehensive feature aggregation and poor generalization capabilities, which improves the mismatching ratio. Here, a dual graph network is proposed by incorporating irregular shape factors to make point cloud features more expressive. At first, we transform point cloud sets into the stellar graph within the local neighbourhood of each point. The deep feature and shape factor of each point are combined in the directional-connected irregular projection space. Subsequently, the combined features are modelled as the second graph. By the attention mechanism computation, feature information is continuously aggregated with intra-graph and inter-graph. Finally, a loss function is utilized to confirm point correspondence and perform the registration through singular value decomposition. Extensive experiments validate that the proposed point cloud registration method achieves state-of-the-art performance.},
  archive      = {J_IETIP},
  author       = {Munan Yuan and Xiru Li},
  doi          = {10.1049/ipr2.13095},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2161-2178},
  shortjournal = {IET Image Process.},
  title        = {Point cloud registration method for indoor depth sensor acquisition system based on dual graph computation with irregular shape factors},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UM-GAN: Underground mine GAN for underground mine low-light
image enhancement. <em>IETIP</em>, <em>18</em>(8), 2154–2160. (<a
href="https://doi.org/10.1049/ipr2.13092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, low-light image enhancement has become increasingly active. However, in underground mine environments, acquiring high-quality images is still challenging due to low light, low contrast, and occlusion. To address this problem, this study proposes a low-light image enhancement method for underground mine based on generative adversarial networks (UM-GAN), which aims to take full advantage of the ability of GAN to achieve the restoration of details, the reduction of noise, and the improvement of overall image quality. The model proposed in this paper is divided into three main parts. Initially, a generator network adopting an encoder–decoder structure is developed. Subsequently, a novel strategy is introduced to merge information by utilizing inverted greyscale images and low-light images as inputs. Further image quality enhancement is achieved by incorporating a noise reduction module employing the diffusion model. To ascertain the efficacy of the UM-GAN method, evaluations are conducted on diverse real-world and synthetic datasets, juxtaposing the approach against superior methods. Through qualitative and quantitative comparative experiments, the method showcases noteworthy advancements through qualitative and quantitative comparative experiments, substantiating its effectiveness. This research provides new ideas and methods for overcoming image quality problems in underground mine environments and contributes to the development of underground mine image processing.},
  archive      = {J_IETIP},
  author       = {Wenwu Han and Yigai Xiao and Yu Yin},
  doi          = {10.1049/ipr2.13092},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2154-2160},
  shortjournal = {IET Image Process.},
  title        = {UM-GAN: Underground mine GAN for underground mine low-light image enhancement},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-stage substation equipment classification method based
on dual-scale attention. <em>IETIP</em>, <em>18</em>(8), 2144–2153. (<a
href="https://doi.org/10.1049/ipr2.13091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of substation equipment images remains challenging due to various factors such as unexpected illumination, viewing angles, scale variations, shadows, surface contaminants, and different elements sharing similar appearances. This paper presents a novel two-stage substation equipment classification method based on dual-scale attention. Leveraging the region proposal technique from Faster-regions with CNN features (RCNN), the input images are initially decomposed into multiple scales to capture latent features. A dual-scale attention module is introduced to enhance the precision of feature extraction. Furthermore, a two-stage network is proposed to address the challenge of classifying closely similar substation equipment. A multi-layer perceptron performs a coarse classification to categorize the equipment into broad categories. Then, a lightweight classifier is employed for fine-grained subclassification, further distinguishing equipment within the same broad category. To mitigate the issue of limited training data, a specialized dataset is collected and annotated for the substation equipment classification. Experimental results demonstrate that the proposed method achieves remarkable accuracy, recall, and F1-score surpassing 0.91, outperforming mainstream approaches in terms of recall and F1 scores. Ablation experiments further validate the significant contributions of both the dual-scale attention and the two-stage classification module in improving the overall performance of the classification network.},
  archive      = {J_IETIP},
  author       = {Yiyang Yao and Xue Wang and Guoqing Zhou and Qing Wang},
  doi          = {10.1049/ipr2.13091},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2144-2153},
  shortjournal = {IET Image Process.},
  title        = {A two-stage substation equipment classification method based on dual-scale attention},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TTMRI: Multislice texture transformer network for
undersampled MRI reconstruction. <em>IETIP</em>, <em>18</em>(8),
2126–2143. (<a href="https://doi.org/10.1049/ipr2.13089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) is a non-interposition imaging technique that provides rich anatomical and physiological information. Yet it is limited by the long imaging time. Recently, deep neural networks have shown potential to significantly accelerate MRI. However, most of these approaches ignore the correlation between adjacent slices in MRI image sequences. In addition, the existing deep learning-based methods for MRI are mainly based on convolutional neural networks (CNNs). They fail to capture long-distance dependencies due to the small receptive field. Inspired by the feature similarity in adjacent slices and impressive performance of Transformer for exploiting the long-distance dependencies, a novel multislice texture transformer network is presented for undersampled MRI reconstruction (TTMRI). Specifically, the proposed TTMRI is consisted of four modules, namely the texture extraction, correlation calculation, texture transfer and texture synthesis. It takes three adjacent slices as inputs, in which the middle one is the target image to be reconstructed, and the other two are auxiliary images. The multiscale features are extracted by the texture extraction module and their inter-dependencies are calculated by the correlation calculation module, respectively. Then the relevant features are transferred by the texture transfer module and fused by the texture synthesis module. By considering inter-slice correlations and leveraging the Transformer architecture, the joint feature learning across target and adjacent slices are encouraged. Moreover, TTMRI can be stacked with multiple layers to recover more texture information at different levels. Extensive experiments demonstrate that the proposed TTMRI outperforms other state-of-the-art methods in both quantitative and qualitative evaluationsions.},
  archive      = {J_IETIP},
  author       = {Xiaozhi Zhang and Liu Zhou and Yaping Wan and Bingo Wing-Kuen Ling and Dongping Xiong},
  doi          = {10.1049/ipr2.13089},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2126-2143},
  shortjournal = {IET Image Process.},
  title        = {TTMRI: Multislice texture transformer network for undersampled MRI reconstruction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An infrared and visible image fusion network based on
multi-scale feature cascades and non-local attention. <em>IETIP</em>,
<em>18</em>(8), 2114–2125. (<a
href="https://doi.org/10.1049/ipr2.13088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, research on infrared and visible image fusion has mainly focused on deep learning-based approaches, particularly deep neural networks with auto-encoder architectures. However, these approaches suffer from problems such as insufficient feature extraction capability and inefficient fusion strategies. Therefore, this paper introduces a novel image fusion network to address the limitations of infrared and visible image fusion networks with auto-encoder architectures. In the designed network, the encoder employs a multi-branch cascade structure, and these convolution branches with different kernel sizes provide the encoder with an adaptive receptive field to extract multi-scale features. In addition, the fusion layer incorporates a non-local attention module that is inspired by the self-attention mechanism. With its global receptive field, this module is used to build a non-local attention fusion network, which works together with the -norm spatial fusion strategy to extract, split, filter, and fuse global and local features. Comparative experiments on the TNO and MSRS datasets demonstrate that the proposed method outperforms other state-of-the-art fusion approaches.},
  archive      = {J_IETIP},
  author       = {Jing Xu and Zhenjin Liu and Ming Fang},
  doi          = {10.1049/ipr2.13088},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2114-2125},
  shortjournal = {IET Image Process.},
  title        = {An infrared and visible image fusion network based on multi-scale feature cascades and non-local attention},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on surface defect detection model of steel strip
based on MFFA-YOLOv5. <em>IETIP</em>, <em>18</em>(8), 2105–2113. (<a
href="https://doi.org/10.1049/ipr2.13086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface quality of steel strip is a critical indicator of the quality of hot-rolled strip, so accurate inspection of its surface is essential. However, the complex texture of steel strip surface defects makes the detection challenging. Here, a multi-scale feature fusion and attention based YOLOv5 (MFFA-YOLOv5) model is proposed. Specifically, the bottom layer features are up-sampled and fused not only with the middle layer features, but also with the top layer features, so that the model better captures the surface texture information of the steel strip. Secondly, an improved attention mechanism module is introduced to deal with the global and local information of the steel strip surface by introducing down-sampling and up-sampling paths based on Convolutional Block Attention Module (CBAM). Meanwhile, a self-attention mechanism path is added to improve the capability of feature representation. Experimental results on the NEU-DET dataset show that the MFFA-YOLOv5 model significantly outperforms other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Hao Chen and Jianlin Qiu and Depeng Gao and Lanmei Qian and Xiujing Li},
  doi          = {10.1049/ipr2.13086},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2105-2113},
  shortjournal = {IET Image Process.},
  title        = {Research on surface defect detection model of steel strip based on MFFA-YOLOv5},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AFCN: An attention-directed feature-fusion ConvNeXt network
for low-voltage apparatus assembly quality inspection. <em>IETIP</em>,
<em>18</em>(8), 2093–2104. (<a
href="https://doi.org/10.1049/ipr2.13085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the production of low-voltage apparatus, assembly quality inspection is of great relevance for ensuring the final quality of the entire product. With the continuous improvement of production efficiency and people&#39;s requirements for production quality, traditional manual inspection methods can no longer meet the quality inspection requirements. In this paper, an Attention-guided Feature-fusion ConvNeXt Network (AFCN) for the automated visual inspection is proposed. By embedding the attention mechanism of the Coordinate Attention block into the residual channel of the ConvNeXt block, the position-aware information and features of the low-voltage apparatus images can be effectively captured to locate the quality problems. Then, an improved attention feature fusion module is adopted to merge the output features at different stages, which introduces a 3D non-parameter attention SimAM block and adapts output accordingly. Therefore, this model can capture the key information of the feature map in a coordinated way in terms of channel and position, fully integrating multiscale features and obtaining contour texture information and semantic information of the low-voltage apparatus. Experiments show the proposed approach can effectively classify defective and normal products.},
  archive      = {J_IETIP},
  author       = {Haorui Guo and Yicheng Bao and Songyu Hu and Congcong Luan and Jianzhong Fu and Li Li and Yinglin Zhang and Yongle Sun and Zongjun Nie},
  doi          = {10.1049/ipr2.13085},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2093-2104},
  shortjournal = {IET Image Process.},
  title        = {AFCN: An attention-directed feature-fusion ConvNeXt network for low-voltage apparatus assembly quality inspection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Global to multi-scale local architecture with hardwired CNN
for 1-ms tomato defect detection. <em>IETIP</em>, <em>18</em>(8),
2078–2092. (<a href="https://doi.org/10.1049/ipr2.13084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 1 millisecond (1-ms) vision system that guarantees high efficiency and timely response for tomato defect detection is essential for factory automation. Because of various defect appearances, recently many existing researches focus on CNN based defect detection, but few of them attempt to reach high processing speed to adapt to the factorial assembly line. This paper proposes a global to multi-scale local based parallel architecture with hardwired CNN for tomato defect detection. This architecture breaks down image-wise detection into pixel-wise localization and block-wise classification. The pixel-wise localization utilizes tomato-aware information as constraints for localization performance. The block-wise classification uses a fully pipelined network structure to obtain the classification result for each block as the pixel stream moves through the network. The classification network has a six-layer lightweight network structure with quantization for hardwired type implementation on FPGA. The experiment results show that the proposed architecture processes 1000 FPS images with 0.9476 ms/frame delay. And for detection performance, this architecture keeps at 80.18%, only 1.31% lower than ResNet50 based detection system.},
  archive      = {J_IETIP},
  author       = {Yuan Li and Tingting Hu and Ryuji Fuchikami and Takeshi Ikenaga},
  doi          = {10.1049/ipr2.13084},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2078-2092},
  shortjournal = {IET Image Process.},
  title        = {Global to multi-scale local architecture with hardwired CNN for 1-ms tomato defect detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feature-enhanced hybrid attention network for traffic sign
recognition in real scenes. <em>IETIP</em>, <em>18</em>(8), 2064–2077.
(<a href="https://doi.org/10.1049/ipr2.13083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, traffic sign recognition techniques have been brought into the assistive driving of automobiles. However, small traffic sign recognition in real scenes is still a challenging task due to the class imbalance issue and the size limit of the traffic signs. To address the above issues, a feature-enhanced hybrid attention network is proposed based on YOLOv5s for a small, fast, and accurate traffic sign detector. First, a series of online data augmentation strategies are designed in the preprocessing module for the model training. Second, the hybrid channel and spatial attention module CSAM are integrated into the backbone for a better feature extraction ability. Third, the channel attention module CAM is used in the detection head for a more efficient feature fusion ability. To validate the approach, extensive experiments are conducted based on the Tsinghua-Tencent 100K dataset. It is found that the novel method achieves state-of-the-art performance with only negligible increases in the model parameter and computational overhead. Specifically, the , parameters, and FLOPs are 85.8%, 7.13 M, and 16.1 G, respectively.},
  archive      = {J_IETIP},
  author       = {Lewei He and Fucai Lan and Chuanzhe Zhou and Yaoguang Ye and Wencong Zhang and Bingzhi Chen and Jiahui Pan},
  doi          = {10.1049/ipr2.13083},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2064-2077},
  shortjournal = {IET Image Process.},
  title        = {A feature-enhanced hybrid attention network for traffic sign recognition in real scenes},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly-background separation and particle swarm
optimization based band selection for hyperspectral anomaly detection.
<em>IETIP</em>, <em>18</em>(8), 2053–2063. (<a
href="https://doi.org/10.1049/ipr2.13082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the dimensionality reduction techniques of hyperspectral image (HSI), band selection (BS) does not change the spectral characteristics and physical meaning of HSIs, which is beneficial to the identification and analysis of surface objects. Recently, many BS methods for target detection have achieved promising results by making full use of the priori spectral features of the target to be detected. Conversely, anomaly detection separates the anomaly based solely on the statistical distribution difference between anomaly and background without any prior information. Therefore, the development of BS for anomaly detection has lagged far behind that of BS for target detection. To this end, this paper proposes a novel BS algorithm dedicated to anomaly detection tasks, named anomaly-background separation and particle swarm optimization (PSO)-based BS. Specifically, an anomaly-background separation framework (ABSF) is established to predetermine a priori knowledge of anomaly distribution. Then, three band prioritization criteria are constructed with the anomaly-background constraints generated by ABSF. Finally, PSO is used to find the optimal subset of bands in the solution space. The experiments on two real datasets demonstrate that the proposed method yields better detection results and greater stability compared to other BS methods discussed in this paper.},
  archive      = {J_IETIP},
  author       = {Xiaodi Shang and Yiqi Duan and Xiaopeng Wang and Baijia Fu and Xudong Sun},
  doi          = {10.1049/ipr2.13082},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2053-2063},
  shortjournal = {IET Image Process.},
  title        = {Anomaly-background separation and particle swarm optimization based band selection for hyperspectral anomaly detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic measurement of slug flow processes from in-line
videos. <em>IETIP</em>, <em>18</em>(8), 2038–2052. (<a
href="https://doi.org/10.1049/ipr2.13081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Slug flow processes draw much attention in chemical and pharmaceutical manufacturing thanks to their ability to eliminate downtime costs and batch-to-batch variation. Using in-line video can monitor the current process status and improve the stability of the slug flow. However, there are no effective image processing methods aiming at such videos from chemical experiments due to limited training sample size and the variety of experimental settings. The paper proposes a training-free method to automatically detect and measure the slugs from in-line videos. Multiple image features are fused to identify the slug shapes under various lighting conditions, a six-point model is fitted to achieve better volume estimation, and a consistency score is estimated to quantify the detection uncertainty. The proposed algorithm achieves similar results to manual labeling for various types of slug flow processes, improving the accuracy of slug column estimation by a large margin compared to the existing automatic methods. Finally, using it to monitor the slug flow process&#39;s stability and the change of the slug volume in an injection point are demonstrated. The method not only shows that handcrafted image features have the capability for detection and segmentation from chemical experiment images but also paves the road for a training-based algorithm for the task.},
  archive      = {J_IETIP},
  author       = {Yanjun Qian and Joel Hulsizer and Mingyao Mou and Consuelo Vega Zambrano and Ema Smith and Mo Jiang},
  doi          = {10.1049/ipr2.13081},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2038-2052},
  shortjournal = {IET Image Process.},
  title        = {Automatic measurement of slug flow processes from in-line videos},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Salient object detection in egocentric videos.
<em>IETIP</em>, <em>18</em>(8), 2028–2037. (<a
href="https://doi.org/10.1049/ipr2.13080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of video salient object detection (VSOD), the majority of research has traditionally been centered on third-person perspective videos. However, this focus overlooks the unique requirements of certain first-person tasks, such as autonomous driving or robot vision. To bridge this gap, a novel dataset and a camera-based VSOD model, CaMSD , specifically designed for egocentric videos, is introduced. First, the SalEgo dataset, comprising 17,400 fully annotated frames for video salient object detection, is presented. Second, a computational model that incorporates a camera movement module is proposed, designed to emulate the patterns observed when humans view videos. Additionally, to achieve precise segmentation of a single salient object during switches between salient objects, as opposed to simultaneously segmenting two objects, a saliency enhancement module based on the Squeeze and Excitation Block is incorporated. Experimental results show that the approach outperforms other state-of-the-art methods in egocentric video salient object detection tasks. Dataset and codes can be found at https://github.com/hzhang1999/SalEgo .},
  archive      = {J_IETIP},
  author       = {Hao Zhang and Haoran Liang and Xing Zhao and Jian Liu and Ronghua Liang},
  doi          = {10.1049/ipr2.13080},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2028-2037},
  shortjournal = {IET Image Process.},
  title        = {Salient object detection in egocentric videos},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic 3D motion model for object tracking in aerial
applications. <em>IETIP</em>, <em>18</em>(8), 2011–2027. (<a
href="https://doi.org/10.1049/ipr2.13079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking, crucial in aerial applications such as surveillance, cinematography, and chasing, faces challenges despite AI advancements. Current solutions lack full reliability, leading to common tracking failures in the presence of fast motions or long-term occlusions of the subject. To tackle this issue, a 3D motion model is proposed that employs camera/vehicle states to locate a subject in the inertial coordinates. Next, a probability distribution is generated over future trajectories and they are sampled using a Monte Carlo technique to provide search regions that are fed into an online appearance learning process. This 3D motion model incorporates machine-learning approaches for direct range estimation from monocular images. The model adapts computationally by adjusting search areas based on tracking confidence. It is integrated into DiMP , an online and deep learning-based appearance model. The resulting tracker is evaluated on the VIOT dataset with sequences of both images and camera states, achieving a 68.9% tracking precision compared to DiMP&#39;s 49.7%. This approach demonstrates increased tracking duration, improved recovery after occlusions, and faster motions. Additionally, this strategy outperforms random searches by about 3.0%.},
  archive      = {J_IETIP},
  author       = {Seyed Hojat Mirtajadini and MohammadAli Amiri Atashgah and Mohammad Shahbazi},
  doi          = {10.1049/ipr2.13079},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2011-2027},
  shortjournal = {IET Image Process.},
  title        = {Probabilistic 3D motion model for object tracking in aerial applications},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3sG: Three-stage guidance for indoor human action
recognition. <em>IETIP</em>, <em>18</em>(8), 2000–2010. (<a
href="https://doi.org/10.1049/ipr2.13078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference using skeleton to steer RGB videos is applicable to fine-grained activities in indoor human action recognition (IHAR). However, existing methods that explore only spatial alignment are prone to bias, resulting in limited performance. The authors propose a Three-stage Guidance (3sG) framework, leveraging skeleton knowledge to promote RGB in three stages. First, a soft shading image is proposed for alleviating background noise in videos, allowing the network to directly focus more on the motion region. Second, the authors propose to extract RGB frames of interest to reduce the computational effort. Furthermore, to explore more fully the complementary information between skeletons and RGB, the skeleton is coupled to the frame representation in a different spatial–temporal sharing pattern. Third, the global skeleton and skeleton-guided RGB features are fed into the shared classifiers, which approximate the logit distributions of the two to enhance the performance in RGB unimodal. Finally, a fusion strategy that utilizes two learnable parameters to adaptively integrate the skeleton with the RGB is proposed. 3sG outperforms the state-of-the-art results on the Toyota Smarthome dataset while it is more efficient than similar methods on the NTU RGB+D dataset.},
  archive      = {J_IETIP},
  author       = {Hai Nan and Qilang Ye and Zitong Yu and Kang An},
  doi          = {10.1049/ipr2.13078},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2000-2010},
  shortjournal = {IET Image Process.},
  title        = {3sG: Three-stage guidance for indoor human action recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inception-YOLO: Computational cost and accuracy improvement
of the YOLOv5 model based on employing modified CSP, SPPF, and inception
modules. <em>IETIP</em>, <em>18</em>(8), 1985–1999. (<a
href="https://doi.org/10.1049/ipr2.13077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for less complex and more accurate architectures has always been a priority since the broad usage of computer vision in everyday life, like auto-drive cars, portable applications, augmented reality systems, medical image analysis etc. There are a lot of methods that have been developed to improve the accuracy and complexity of object detection, like the generations of R-CNNs and YOLOs. However, these methods are not the most efficient architectures, and there is always room to improve. In this study, the 5th version of YOLO is employed and the improved architecture, Inception-YOLO, is presented. The model significantly outperforms the SOTA YOLO family. Specifically, the improvements can be summarised as follows: impressive improvement of floating point operations (FLOPs) and number of parameters, as well as improvement in accuracy compared to the models with fewer FLOPs. All our presented approaches, like the optimized inception module, proposed structures for CSP and SPPF, and the improved loss function used in this research, work together to incrementally improve detection results, accuracy, demanded memory, and FLOPs simultaneously. For a glimpse of performance, the Inception-YOLO-S model hits 38.7% AP with 5.9M parameters and 11.5 BFLOPs and outperforms YOLOv5-S with 37.4% AP, 7.2M parameters, and 16.5 BFLOPs.},
  archive      = {J_IETIP},
  author       = {Hadi Khodaei Jooshin and Mahdi Nangir and Hadi Seyedarabi},
  doi          = {10.1049/ipr2.13077},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1985-1999},
  shortjournal = {IET Image Process.},
  title        = {Inception-YOLO: Computational cost and accuracy improvement of the YOLOv5 model based on employing modified CSP, SPPF, and inception modules},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-trained low-light image enhancement transformer.
<em>IETIP</em>, <em>18</em>(8), 1967–1984. (<a
href="https://doi.org/10.1049/ipr2.13076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement is a longstanding challenge in low-level vision, as images captured in low-light conditions often suffer from significant aesthetic quality flaws. Recent methods based on deep neural networks have made impressive progress in this area. In contrast to mainstream convolutional neural network (CNN)-based methods, an effective solution inspired by the transformer, which has shown impressive performance in various tasks, is proposed. This solution is centred around two key components. The first is an image synthesis pipeline, and the second is a powerful transformer-based pre-trained model, known as the low-light image enhancement transformer (LIET). The image synthesis pipeline includes illumination simulation and realistic noise simulation, enabling the generation of more life-like low-light images to overcome the issue of data scarcity. LIET combines streamlined CNN-based encoder-decoders with a transformer body, efficiently extracting global and local contextual features at a relatively low computational cost. The extensive experiments show that this approach is highly competitive with current state-of-the-art methods. The codes have been released and are available at LIET .},
  archive      = {J_IETIP},
  author       = {Jingyao Zhang and Shijie Hao and Yuan Rao},
  doi          = {10.1049/ipr2.13076},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1967-1984},
  shortjournal = {IET Image Process.},
  title        = {Pre-trained low-light image enhancement transformer},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). SWPanGAN: A hybrid generative adversarial network for
pansharpening. <em>IETIP</em>, <em>18</em>(8), 1950–1966. (<a
href="https://doi.org/10.1049/ipr2.13075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pansharpening is a vital technique in remote sensing that combines a low-resolution multi-spectral image with its corresponding panchromatic image to obtain a high-resolution multi-spectral image. Despite its potential benefits, the challenge lies in extracting features from the source images and eliminating artefacts in the fused images. In response to the challenge, a hybrid generative adversarial network-based model, termed SWPanGAN, is proposed. For better feature extraction, the conventional convolution neural network is replaced with a Swin transformer in the generator, which provides the generator with the ability to model long-range dependencies. Additionally, to suppress artefacts, a wavelet-based discriminator is proposed for effectively distinguishing the frequency discrepancy. With these modifications, both the generator and discriminator networks of SWPanGAN are enhanced. Extensive experiments illustrate that our SWPanGAN can generate high-quality pansharpening images and surpass other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Bo Huang and Xiongfei Li and Xiaoli Zhang},
  doi          = {10.1049/ipr2.13075},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1950-1966},
  shortjournal = {IET Image Process.},
  title        = {SWPanGAN: A hybrid generative adversarial network for pansharpening},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An image-based runway detection method for fixed-wing
aircraft based on deep neural network. <em>IETIP</em>, <em>18</em>(8),
1939–1949. (<a href="https://doi.org/10.1049/ipr2.13087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual information is important in final approach and landing phases for an approaching aircraft, it presents supplementary source for navigation system, and provides backup guidance when radio navigation fails, or even supports a complete vision-based landing. Relative position and attitude can be solved from the runway features in the image. Traditional runway detection methods have high latency and low accuracy, which is unable to satisfy the requirements for a safe landing. This paper proposes a real-time runway detection model, efficient runway feature extractor (ERFE), based on deep convolutional neural network, generating semantic segmentation and feature lines output. In order to evaluate the model&#39;s effectiveness, a benchmark is proposed to calculate the actual error between predicted feature line and ground truth one. A novel runway dataset which is based on pictures from Microsoft Flight Simulator 2020 (FS2020), is also proposed in this paper to train and test the model. The dataset will be released at https://www.kaggle.com/datasets/relufrank/fs2020-runway-dataset . ERFE shows excellent performance in FS2020 dataset, it gives satisfactory results even for real runway images excluded from our dataset.},
  archive      = {J_IETIP},
  author       = {Mingqiang Chen and Yuzhou Hu},
  doi          = {10.1049/ipr2.13087},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1939-1949},
  shortjournal = {IET Image Process.},
  title        = {An image-based runway detection method for fixed-wing aircraft based on deep neural network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable vision transformer based on prototype parts
for COVID-19 detection. <em>IETIP</em>, <em>18</em>(7), 1927–1937. (<a
href="https://doi.org/10.1049/ipr2.13074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, the COVID-19 virus has had a significant impact on the physical and mental health of people around the world. Therefore, in order to effectively distinguish COVID-19 patients, many deep learning efforts have used chest medical images to detect COVID-19. As with model accuracy, interpretability is also important in the work related to human health. This work introduces an interpretable vision transformer that uses the prototype method for the detection of positive patients with COVID-19. The model can learn the prototype features of each category based on the structural characteristics of ViT. The predictions of the model are obtained by comparing all the features of the prototype in the designed prototype block. The proposed model was applied to two chest X-ray datasets and one chest CT dataset, achieving classification performance of 99.3%, 96.8%, and 98.5% respectively. Moreover, the prototype method can significantly improve the interpretability of the model. The decisions of the model can be interpreted based on prototype parts. In the prototype block, the entire inference process of the model can be shown and the predictions of the model can be demonstrated to be meaningful through the visualization of the prototype features.},
  archive      = {J_IETIP},
  author       = {Yang Xu and Zuqiang Meng},
  doi          = {10.1049/ipr2.13074},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1927-1937},
  shortjournal = {IET Image Process.},
  title        = {Interpretable vision transformer based on prototype parts for COVID-19 detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NHD-YOLO: Improved YOLOv8 using optimized neck and head for
product surface defect detection with data augmentation. <em>IETIP</em>,
<em>18</em>(7), 1915–1926. (<a
href="https://doi.org/10.1049/ipr2.13073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface defect detection is an essential task for ensuring the quality of products. Many excellent object detectors have been employed to detect surface defects in resent years, which has achieved outstanding success. To further improve the detection performance, a defect detector based on state-of-the-art YOLOv8, named improved YOLOv8 by neck, head and data (NHD-YOLO), is proposed. Specifically, YOLOv8 from three crucial aspects including neck, head and data is improved. First, a shortcut feature pyramid network is designed to effectively fuse features from backbone by improving the information transmission. Then, an adaptive decoupled head is proposed to alleviate the feature spatial misalignment between the classification and regression tasks. Finally, to enhance the training on small objects, a data augmentation method named selective small object copy and paste is proposed. Extensive experiments are conducted on three real-world datasets: detection dataset from Northeastern University (NEU-DET), printed circuit boards from Peking University (PKU-Market-PCB) and common objects in context (COCO). According to the results, NHD-YOLO achieves the highest detection accuracy and exhibits outstanding inference speed and generalisation performance.},
  archive      = {J_IETIP},
  author       = {Faquan Chen and Miaolei Deng and Hui Gao and Xiaoya Yang and Dexian Zhang},
  doi          = {10.1049/ipr2.13073},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1915-1926},
  shortjournal = {IET Image Process.},
  title        = {NHD-YOLO: Improved YOLOv8 using optimized neck and head for product surface defect detection with data augmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multidimensional fusion image stereo matching algorithm.
<em>IETIP</em>, <em>18</em>(7), 1903–1914. (<a
href="https://doi.org/10.1049/ipr2.13072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the low matching accuracy of stereo matching algorithms in image regions with specular reflection, this paper proposes a multidimensional fusion stereo matching algorithm named MFANet. The algorithm embeds a multispectral attention module into the residual feature extraction network, utilizing two-dimensional discrete cosine transforms to extract frequency features. In the pyramid pooling module, a coordinated attention mechanism is introduced to capture relevant positional information. In the cost aggregation part, the MFANet algorithm incorporates a three-dimensional attention mechanism, focusing on the more important semantic information in high-level features. By combining detailed information from low-level features, semantic information from high-level features, and contextual information, the algorithm generates features that are more conducive to disparity prediction. The MFANet algorithm is evaluated on three standard datasets (SceneFlow, KITTI2015, and KITTI2012). Experimental results demonstrate its robustness against specular reflection interference, accurate prediction of disparities in specular reflection pathological regions, and promising application prospects.},
  archive      = {J_IETIP},
  author       = {Zhenhua Quan and Liang Luo and Bin Wu},
  doi          = {10.1049/ipr2.13072},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1903-1914},
  shortjournal = {IET Image Process.},
  title        = {A multidimensional fusion image stereo matching algorithm},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-attentive fusion network: An efficient model for online
detection of action start. <em>IETIP</em>, <em>18</em>(7), 1892–1902.
(<a href="https://doi.org/10.1049/ipr2.13071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online detection of action start is a significant and challenging task that requires prompt identification of action start positions and corresponding categories within streaming videos. This task presents challenges due to data imbalance, similarity in boundary content, and real-time detection requirements. Here, a novel Time-Attentive Fusion Network is introduced to address the requirements of improved action detection accuracy and operational efficiency. The time-attentive fusion module is proposed, which consists of long-term memory attention and the fusion feature learning mechanism, to improve spatial-temporal feature learning. The temporal memory attention mechanism captures more effective temporal dependencies by employing weighted linear attention. The fusion feature learning mechanism facilitates the incorporation of current moment action information with historical data, thus enhancing the representation. The proposed method exhibits linear complexity and parallelism, enabling rapid training and inference speed. This method is evaluated on two challenging datasets: THUMOS’14 and ActivityNet v1.3. The experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art methods in terms of both detection accuracy and inference speed.},
  archive      = {J_IETIP},
  author       = {Xuejiao Hu and Shijie Wang and Ming Li and Yang Li and Sidan Du},
  doi          = {10.1049/ipr2.13071},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1892-1902},
  shortjournal = {IET Image Process.},
  title        = {Time-attentive fusion network: An efficient model for online detection of action start},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TIM-net: A multi-label classification network for TCM tongue
images fusing global-local features. <em>IETIP</em>, <em>18</em>(7),
1878–1891. (<a href="https://doi.org/10.1049/ipr2.13070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining the extracted tongue features with other medical indicators can effectively judge the diseases of patients. The previous work usually only analyzes a certain feature of the tongue body and is unable to extract multiple features simultaneously. In this study, a multi-label classification network named TIM-Net is proposed, which integrates global and local features to achieve multi-label intelligent diagnosis of Chinese medicine tongue images. First, a feature extraction network based on ResNet is proposed to capture the features of tongue images more sufficiently. Then, a multi-label classification algorithm fusing global and local features is proposed, and targeted screening operations are carried out on the class-related feature maps based on global confidence. In addition, a logical masking algorithm is proposed to ensure that the local features can only correct the feature labels they represent, and do not interfere with other feature labels. The classification accuracy is further improved by using local feature confidence and correcting the global classification results. Finally, the experimental results indicate that the classification accuracy of the tongue images is gradually improved through optimizing the feature extraction network and fusing local features, and it exceeds other state-of-the-art multi-label classification networks.},
  archive      = {J_IETIP},
  author       = {Xinfeng Zhang and Jie Shao and Haonan Bian and Hui Li and Maoshen Jia and Xiaomin Liu},
  doi          = {10.1049/ipr2.13070},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1878-1891},
  shortjournal = {IET Image Process.},
  title        = {TIM-net: A multi-label classification network for TCM tongue images fusing global-local features},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSFA: Multi-stage feature aggregation network for
multi-label image recognition. <em>IETIP</em>, <em>18</em>(7),
1862–1877. (<a href="https://doi.org/10.1049/ipr2.13068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image recognition (MLR) is a significant branch of image classification that aims to assign multiple categorical labels to each input. Previous research has focused on enhancing the learning of category-related regional features. However, the potential impact of multi-scale distributions in intra- and inter-category targets on MLR tends to be neglected. Besides, semantic consistency for categories is restricted to be considered on single-scale features, resulting in suboptimal feature extraction. To address the limitations of above, a Multi-stage Feature Aggregation (MSFA) network is proposed. In MSFA, a novel local feature extraction method is suggested to progressively extract category-related high-resolution local features in both spatial and channel dimensions. Subsequently, local and global features are fused without additional up- and down-sampling to enrich the scale diversity of the features while incorporating refined class-specific information. Furthermore, a hierarchical prediction scheme for MLR is proposed, which generates classification confidence corresponding to different scales under hierarchical loss supervision. Consequently, the final output of the network comes from the joint prediction by the classifiers on multi-scale features, ensuring a stronger feature extraction capability. The extensive experiments have been carried on VOC and MS-COCO datasets, and the superiority of MSFA over existing mainstream methods has been verified.},
  archive      = {J_IETIP},
  author       = {Jiale Chen and Feng Xu and Tao Zeng and Xin Li and Shangjing Chen and Jie Yu},
  doi          = {10.1049/ipr2.13068},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1862-1877},
  shortjournal = {IET Image Process.},
  title        = {MSFA: Multi-stage feature aggregation network for multi-label image recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised remote sensing image thin cloud removal method
based on contrastive learning. <em>IETIP</em>, <em>18</em>(7),
1844–1861. (<a href="https://doi.org/10.1049/ipr2.13067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud removal algorithm is a crucial step of remote sensing image preprocessing. The current mainstream remote sensing image cloud removal algorithms are implemented based on deep learning, and most of them are supervised. A large number of data pairs are required for training to achieve cloud removal. However, real with/without cloud image pairs datasets are difficult to obtain in the real world, and the models obtained by training on synthetic datasets often need to generalize better to natural scenes. And the existing unsupervised thin cloud removal methods based on Cycle-GAN framework with considerable model complexity and unstable training are not an excellent solution to the problem of lack of paired datasets. Based on this, in this paper, the authors propose an unsupervised remote sensing image thin cloud removal method based on contrastive learning—GAN-UD. It is a network consisting of a frequency-spatial attention generator and a discriminator. In addition, the authors introduce local contrastive loss and global content loss to constrain the content of the generated images to ensure that the generated cloud-free images are consistent with the input cloud images in terms of image content. Experimental results show that the proposed method in this paper can still effectively remove thin clouds from remote sensing images without paired training datasets, outperforms current unsupervised cloud removal methods, and achieves comparable performance to supervised methods.},
  archive      = {J_IETIP},
  author       = {Zhan Cong Tan and Xiao Feng Du and Wang Man and Xiao Zhu Xie and Gui Song Wang and Qin Nie},
  doi          = {10.1049/ipr2.13067},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1844-1861},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised remote sensing image thin cloud removal method based on contrastive learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and implementation in an altera’s cyclone IV
EP4CE6E22C8 FPGA board of a fast and robust cipher using combined 1D
maps. <em>IETIP</em>, <em>18</em>(7), 1823–1843. (<a
href="https://doi.org/10.1049/ipr2.13066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an image encryption algorithm based on combined 1D chaotic maps. First, a permutation technique was applied. It was then reorganized into 1D matrices along the rows and columns respectively, which were then shuffled by computing the substituted position indices to obtain the scrambled image. Subsequently, a method of confusion of the scrambled image was used through another generated data map, combined with random sub-matrices for diffusion, then resulting in an encrypted image. Finally, the proposed cryptosystem was implemented in a single kernel platform developed using the Nios II Software Build Tools processor for Eclipse. A hardware architecture was designed using the Qsys-built tool which is available in the Quartus II 13.0sp1 environment. The developed single-core system was implemented using the Cyclone IV EP4CE6E22C8. Robustness evaluation of the cryptosystem was performed through security analysis tests such as histogram analysis, correlation coefficient, differential analysis, and key space analysis to prove that it is of good quality, efficient, fast, and successfully resisting brute force attacks. The hardware performance analysis was also carried out. Then the cryptosystem is compared with those in the literature both in the hardware and security performance aspects.},
  archive      = {J_IETIP},
  author       = {Alain Fanda Djomo and Alain Tiedeu and Janvier Fotsing},
  doi          = {10.1049/ipr2.13066},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1823-1843},
  shortjournal = {IET Image Process.},
  title        = {Design and implementation in an altera&#39;s cyclone IV EP4CE6E22C8 FPGA board of a fast and robust cipher using combined 1D maps},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HAB-net: Hierarchical asymmetric convolution and boundary
enhancement network for brain tumor segmentation. <em>IETIP</em>,
<em>18</em>(7), 1809–1822. (<a
href="https://doi.org/10.1049/ipr2.13065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumour segmentation (BTS) is crucial for diagnosis and treatment planning by delineating tumour boundaries and subregions in multi-modality bio-imaging data. Several BTS models have been proposed to address specific technical challenges encountered in this field. However, accurately capturing intricate tumour structures and boundaries remains a difficult task. To overcome this challenge, HAB-Net, a model that combines the strengths of convolutional neural networks and transformer architectures, is presented. HAB-Net incorporates a custom-designed hierarchical and pseudo-convolutional module called hierarchical asymmetric convolutions (HAC). In the encoder, a coordinate attention is included to extract feature maps. Additionally, swin transformer, which has a self-attention mechanism, is integrated to effectively capture long-range relationships. Moreover, the decoder is enhanced with a boundary attention module (BAM) to improve boundary information and overall segmentation performance. Extensive evaluations conducted on the BraTS2018 and BraTS2021 datasets demonstrate significant improvements in segmentation accuracy for tumour regions.},
  archive      = {J_IETIP},
  author       = {Yuanjing Hu and Aibin Huang and Rui Xu},
  doi          = {10.1049/ipr2.13065},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1809-1822},
  shortjournal = {IET Image Process.},
  title        = {HAB-net: Hierarchical asymmetric convolution and boundary enhancement network for brain tumor segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight waxberry fruit detection model based on
YOLOv5. <em>IETIP</em>, <em>18</em>(7), 1796–1808. (<a
href="https://doi.org/10.1049/ipr2.13064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the safety and efficiency problems in the picking process of Waxberry, the slow speed and low precision of high-density Waxberry target detection under a complex background were studied. A lightweight Waxberry target detection algorithm based on YOLOv5 is studied. In this study, C3-Faster1 and C3-Faster2 modules are proposed, which are located in the backbone and neck of the network: C3-Faster1 can improve the model speed with a simple structure; C3-Faster2 integrates the context attention mechanism and transform module based on C3-Faster1 to make the network pay attention to the information of Waxberry image context and expand the channel receptive field. A new pyramid module, SPPFCSPC, is proposed to expand the sensing field and improve the accuracy of boundary detection. It also combines the Coordinate Attention (CA) and Dyhead dynamic detection head to suppress useless information and enhance the detection ability of small targets. Compared to YOLOv4, YOLOv7, and YOLOv8, mean accuracy percentage (mAP) improved by 5.7%, 9.4%, 8.3%. Compared to the base YOLOv5 model, mAP has improved from 86.5% to 91.9%, running on 2 GB Jeston nano, and the improved model is 5.03 frames per second (FPS) faster than YOLOv5. Experiments show that the designed module is more effective in Waxberry detection tasks.},
  archive      = {J_IETIP},
  author       = {ChengYu Yang and Jun Liu and JianTing He},
  doi          = {10.1049/ipr2.13064},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1796-1808},
  shortjournal = {IET Image Process.},
  title        = {A lightweight waxberry fruit detection model based on YOLOv5},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AODiMP-TIR: Anti-occlusion thermal infrared targets tracker
based on SuperDiMP. <em>IETIP</em>, <em>18</em>(7), 1780–1795. (<a
href="https://doi.org/10.1049/ipr2.13063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issue of tracking drift and failures in thermal infrared (TIR) tracking tasks caused by target occlusion, this study proposes an anti-occlusion TIR target tracker named AODiMP-TIR. This approach involves an anti-occlusion strategy that relies on target occlusion status determination and trajectory prediction. This enables the prediction of the target&#39;s current position when it is identified as occluded, ensuring swift recapture upon reappearance. A criterion is introduced for occlusion status determination based on the classification response map of SuperDiMP. Additionally, a trajectory mapping module designed to decouple target motion from camera motion is presented, enhancing the precision of trajectory prediction. Comparative experiments with other state-of-the-art trackers are conducted on the large-scale high-diversity thermal infrared object tracking benchmark (LSOTB-TIR), LSOTB-TIR100, and thermal infrared pedestrian tracking benchmark (PTB-TIR) datasets. The results indicate that the AODiMP-TIR performs well across all three datasets, particularly exhibiting outstanding performance in occlusion sequences. Furthermore, ablation study experiments confirm the effectiveness of the anti-occlusion strategy, occlusion determination criterion and trajectory mapping module.},
  archive      = {J_IETIP},
  author       = {Shaoyang Ma and Yao Yang and Gang Chen},
  doi          = {10.1049/ipr2.13063},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1780-1795},
  shortjournal = {IET Image Process.},
  title        = {AODiMP-TIR: Anti-occlusion thermal infrared targets tracker based on SuperDiMP},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video stabilization based on low-rank constraint and
trajectory optimization. <em>IETIP</em>, <em>18</em>(7), 1768–1779. (<a
href="https://doi.org/10.1049/ipr2.13062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video stabilization plays a pivotal role in enhancing video quality by eliminating unwanted jitter in shaky videos. This paper introduces a novel video stabilization algorithm that leverages low-rank constraint and trajectory optimization to effectively eliminate undesirable motion and generate stabilized videos. In the proposed algorithm, a low-rank constraint regularization term is incorporated to enhance the smoothness of motion trajectories. Additionally, a predictive path smoothness term is integrated to ensure the consistency of motion across neighbouring frames. To address the problem of excessive cropping resulting from aggressive smoothing, a flexible local window strategy that emphasizes local motion relationships within the trajectories is introduced. The experimental results show that, compared to other excellent video stabilization algorithms, the proposed algorithm improves the stability metric by approximately 2.3%. Furthermore, in the stabilized videos generated by the algorithm, an approximate improvement of 2.18 dB in average image temporal fidelity and a 5.7% increase in average structural similarity between adjacent frames are achieved. The code that implements the proposed method is publicly accessible at https://github.com/CZS0319/VS_Low-Rank_Constraint_Trajectory_Optimization .},
  archive      = {J_IETIP},
  author       = {Zhenhong Shang and Zhishuang Chu},
  doi          = {10.1049/ipr2.13062},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1768-1779},
  shortjournal = {IET Image Process.},
  title        = {Video stabilization based on low-rank constraint and trajectory optimization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An algorithm for detecting dense small objects in aerial
photography based on coordinate position attention module.
<em>IETIP</em>, <em>18</em>(7), 1759–1767. (<a
href="https://doi.org/10.1049/ipr2.13061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of detecting a large number of objects and a high proportion of small objects in aerial drone imagery, an aerial dense small object detection algorithm called coordinate position attention module you only look once (CPAM-YOLO) is proposed based on the coordinate position attention module (CPAM). In the backbone network of CPAM-YOLO, a CPAM is proposed and embedded that decomposes channel attention into two 1D feature encoding processes, and selectively combines the features of each position through the weighted sum of all position features. Finally, features are aggregated along two spatial directions, increasing the effective information utilization of input feature positions and channels. The backbone network, feature enhancement network, and detection heads have been optimized to improve detection accuracy while ensuring a lightweight detection network. Using lightweight backbone networks to significantly reduce the number of parameters while using high-resolution feature enhancement networks to retain more semantic and detailed features. The algorithm&#39;s performance was evaluated using the publicly available VisDrone2019 dataset. Compared to the baseline network YOLOv5l, CPAM-YOLO achieved a 4.5% improvement in mAP 0.5 and a 3.2% improvement in mAP 0.95 . These experimental results demonstrate the strong practicality of the CPAM-YOLO object detection network for detecting dense small objects in aerial image.},
  archive      = {J_IETIP},
  author       = {Huixin Wu and Yang Zhu and Mengdi Cao},
  doi          = {10.1049/ipr2.13061},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1759-1767},
  shortjournal = {IET Image Process.},
  title        = {An algorithm for detecting dense small objects in aerial photography based on coordinate position attention module},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptively hybrid fractal image coding. <em>IETIP</em>,
<em>18</em>(7), 1745–1758. (<a
href="https://doi.org/10.1049/ipr2.13060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptively hybrid method was proposed to improve the performance of fractal coding methods. First, it is found that the range blocks with large variances (RBLVs) play a crucial role in degrading decoded images, and the effect of the remaining range blocks with small variances (RBSVs) can be ignored. Then, an adaptive method was proposed to divide the range blocks into the above two categories: RBLVs and RBSVs. Second, RBLVs were designed to be encoded in an extended domain block pool (EDBP). Then, better block-matching effect can be obtained, which will result in better decoded image quality. Further, the no-search fractal encoding method is adopted for RBSVs to achieve faster encoding speed and fewer bits per pixel (bpp). Finally, four fractal coding methods were adopted to assess the performance of the proposed method. Experimental results show that compared with the previous methods, the PSNR quality of decoded images in the proposed method can be improved by about 0.15–0.4 dB, about 20%–35% of the total computations in encoding process can be saved, and about 0.2 bpp can be saved. Moreover, under the same decoding time, the proposed method can achieve comparable or smaller deviations regarding the decoded image.},
  archive      = {J_IETIP},
  author       = {Qiang Wang and Guohua Jin and Sheng Bi},
  doi          = {10.1049/ipr2.13060},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1745-1758},
  shortjournal = {IET Image Process.},
  title        = {Adaptively hybrid fractal image coding},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AM-MulFSNet: A fast semantic segmentation network combining
attention mechanism and multi-branch. <em>IETIP</em>, <em>18</em>(7),
1733–1744. (<a href="https://doi.org/10.1049/ipr2.13058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to balance accuracy and real-time performance in semantic segmentation, this paper proposes a real-time semantic segmentation algorithm model based on attention mechanism and multi-branch feature fusion using Fast convolutional neural network model (Fast-SCNN). In this method, the spatial detail feature enhancement branch is introduced to enhance spatial detail features firstly. Then, through rational design of fusion module, the feature information of each branch is optimized to achieve better fusion of deep and shallow features. At the end of the feature fusion module, an adaptive feature enhancement focus module is introduced to capture the interdependence between remote pixels. The experimental results show that the proposed algorithm achieves 71.55% segmentation accuracy on Cityscapes dataset, the reasoning speed FPS is 97.6 frames/s, and the number of parameters is 1.39 M, which verifies the effectiveness of the network model constructed by the algorithm. Code is available at https://github.com/ccchhheeennn/model .},
  archive      = {J_IETIP},
  author       = {Rui Jiang and Runa Chen and Li Zhang and Xiaoming Wang and Youyun Xu},
  doi          = {10.1049/ipr2.13058},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1733-1744},
  shortjournal = {IET Image Process.},
  title        = {AM-MulFSNet: A fast semantic segmentation network combining attention mechanism and multi-branch},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). No-reference image quality assessment via a dual-branch
residual network. <em>IETIP</em>, <em>18</em>(7), 1719–1732. (<a
href="https://doi.org/10.1049/ipr2.13057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The No-Reference Image Quality Assessment (NR-IQA) method can predict quality scores of distorted images without the reference image. However, due to the variability in both color and structure of images, existing NR-IQA methods struggle to accurately predict quality scores of distorted images. Therefore, an NR-IQA method based on a Dual-Branch Residual Network (DBRIQA) for evaluating the quality scores of color-distorted images is proposed. First, guided filtering is applied to the hue images in the HSV space to extract distortion information from the image&#39;s color. Then, due to significant differences in distorted images, enhancements are made to the traditional residual blocks, forming a feature extraction module that captures multi-scale features from the image. In order to capture the global relationships in the distorted image, a Global-Level Attention Block (GLAB) is introduced, facilitating the interaction of information among the extracted features. Experiments were conducted across four publicly available IQA datasets, including LIVE, CSIQ, TID2008, and TID2013. The experimental results demonstrate that the proposed method exhibits strong performance and generalization capabilities in predicting image quality compared to peer methods.},
  archive      = {J_IETIP},
  author       = {Peng Ji and Chang Liu and Hao Chen},
  doi          = {10.1049/ipr2.13057},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1719-1732},
  shortjournal = {IET Image Process.},
  title        = {No-reference image quality assessment via a dual-branch residual network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improvement of ship target detection algorithm for
YOLOv7-tiny. <em>IETIP</em>, <em>18</em>(7), 1710–1718. (<a
href="https://doi.org/10.1049/ipr2.13054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In addressing the challenge of ships being prone to occlusion in multi-target situations during ship target detection, leading to missed and false detections, this paper proposes an enhanced ship detection algorithm for YOLOv7-tiny. The proposed method incorporates several key modifications. Firstly, it introduces the Convolutional Block Attention Module in the Backbone section of the original model, emphasizing position information while attending to channel features to enhance the network&#39;s ability to extract crucial information. Secondly, it replaces standard convolution with GSConv convolution in the Neck section, preserving detailed information and reducing computational load. Subsequently, the lightweight operator Content-Aware ReAssembly of Features is employed to replace the original nearest-neighbour interpolation, mitigating the loss of feature information during the up-sampling process. Finally, the localization loss function, SIOU Loss, is utilized to calculate loss, expedite training convergence, and enhance detection accuracy. The research results indicate that the precision of the improved model is 91.2%, mAP@0.5 is 94.5%, and the F1-score is 90.7%. These values are 3.7%, 5.5%, and 4.2% higher than those of the original YOLOv7-tiny model, respectively. The improved model effectively enhances detection accuracy. Additionally, the improved model achieves an FPS of 145.4, meeting real-time requirements.},
  archive      = {J_IETIP},
  author       = {Huixia Zhang and Haishen Yu and Yadong Tao and Wenliang Zhu and Kaige Zhang},
  doi          = {10.1049/ipr2.13054},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1710-1718},
  shortjournal = {IET Image Process.},
  title        = {Improvement of ship target detection algorithm for YOLOv7-tiny},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusing angular features for skeleton-based action
recognition using multi-stream graph convolution network.
<em>IETIP</em>, <em>18</em>(7), 1694–1709. (<a
href="https://doi.org/10.1049/ipr2.13041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distinguishing similar actions has been a challenging challenge in skeleton-based action recognition. Since the joint coordinates in these actions are similar, it is difficult to accomplish the recognition task using traditional joint features. To address this issue, the use of angle features to capture subtle nuances in various body parts, along with a critical angle enhancement module that assigns weights to different angle feature representations for a given action are proposed, highlighting the critical angle feature representation. The approach is evaluated using a three-stream ensemble method on three large action recognition datasets, NTU-RGB+D, NTU-RGB+D 120, and Kinetics-400. The experimental results demonstrate that incorporating angular information can effectively complement joint and skeletal features, leading to improved recognition of similar actions and enhanced model performance and robustness.},
  archive      = {J_IETIP},
  author       = {Qian Huang and Wenting Liu and Mingzhou Shang and Yiming Wang},
  doi          = {10.1049/ipr2.13041},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1694-1709},
  shortjournal = {IET Image Process.},
  title        = {Fusing angular features for skeleton-based action recognition using multi-stream graph convolution network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A person re-identification method for sports event scenes
incorporating textual information mining. <em>IETIP</em>,
<em>18</em>(7), 1681–1693. (<a
href="https://doi.org/10.1049/ipr2.13038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification represents a pivotal sub-problem in image retrieval, boasting broad application prospects in fields such as intelligent security and video surveillance. However, most existing person re-identification methods predominantly focus solely on visual features pertaining to the person targets, thereby disregarding some supporting information closely related to the scene context. In the context of athlete re-identification during sports event scenes, the athlete bib number is fully considered, an important clue that can provide different athletes&#39; identities, and the traditional visual features of the person and high-level semantic information of the bib number text are fused. A multi-source information mutual gain mechanism is designed to improve the accuracy of the person re-identification task. In the existing only publicly available marathon bib number dataset RBNR, the recognition accuracy of this method is significantly superior to that of the existing person re-identification method. In addition, this paper constructs and publishes an athlete re-identification dataset (HNNU-ReID8000) for mainstream sports events, and the mean average precision (mAP) value of this method reaches 96.1% on this dataset, significantly ahead of existing state-of-the-art person re-identification methods. The code and the HNNU-ReID8000 dataset will be released at https://github.com/yanbin-zhu/zyb_person-reid .},
  archive      = {J_IETIP},
  author       = {Runmin Wang and Yanbin Zhu and Zukun Wan and Hua Chen and Zhenlin Zhu and Weixin Zhou and Chang Han and Yajun Ding},
  doi          = {10.1049/ipr2.13038},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1681-1693},
  shortjournal = {IET Image Process.},
  title        = {A person re-identification method for sports event scenes incorporating textual information mining},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSPAN: Multi-scale pyramid attention network for efficient
skin cancer lesion segmentation. <em>IETIP</em>, <em>18</em>(7),
1667–1680. (<a href="https://doi.org/10.1049/ipr2.12989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is common and deadly, needs to be detected and treated properly. Deep learning algorithms like UNet have shown potential results in medical imaging. Such approaches still struggle to capture fine-grained details and scale differences in skin lesions-based occlusions&#39; appearance, size etc. This research proposes a redesign UNet, the Multi-Scale Pyramid Attention Network (MSPAN), to improve skin cancer lesion segmentation. The input data is processed at numerous scales with varied receptive fields. This enhances the network&#39;s ability to identify lesion locations by capturing local and global context. Attention approaches also help the network to suppress noise by focusing on informative features. We have evaluated MSPAN model on the publicly available ISIC2018 benchmark dataset for skin lesion segmentation. The method surpasses traditional UNet and other current methods in accuracy and effectiveness. The model also has a post-processing to estimate lesion area for fast inference, making it suitable for extensive screening. Redesigned UNet with the Multi-Scale Pyramid Attention Network improves skin cancer lesion segmentation. The model&#39;s ability to collect fine-grained information and handle occlusions allows for more accurate skin cancer diagnosis and treatment. The MSPAN design can improve computer-aided diagnosis systems and help dermatologists make precise clinical decisions.},
  archive      = {J_IETIP},
  author       = {Noor Ahmed and Tan Xin and Ma Lizhuang},
  doi          = {10.1049/ipr2.12989},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1667-1680},
  shortjournal = {IET Image Process.},
  title        = {MSPAN: Multi-scale pyramid attention network for efficient skin cancer lesion segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rotated points for object detection in remote sensing
images. <em>IETIP</em>, <em>18</em>(6), 1655–1665. (<a
href="https://doi.org/10.1049/ipr2.13011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in remote sensing images poses great challenges due to the dense distribution, arbitrary orientation, and aspect ratio variations of objects. Most of the existing methods rely on aligned convolutional features, which fail to capture the geometric information of objects effectively and result in the inconsistency between the classification score and localization accuracy. Moreover, densely packed objects suffer from spatial feature aliasing caused by the intersection of reception fields between objects. To address this issue, a deformable convolution-based method named rotated points is proposed, which consists of two modules: a point set loss module and a high-quality sample assignment module. The point set loss module can extract geometric features of objects in arbitrary directions with fine-grained point sets for feature representation and introduce outlier penalties to penalize outlier points. The high-quality sample assignment module measures the classification and localization ability, orientation quality, and point-wise correlation of point sets comprehensively to enhance the consistency of classification and regression significantly. Experiments on the DOTA and FAIR1M datasets demonstrate that the proposed method achieves significant improvements over the benchmark model.},
  archive      = {J_IETIP},
  author       = {Longbao Wang and Yican Shen and Jin Yang and Hui Zeng and Hongmin Gao},
  doi          = {10.1049/ipr2.13011},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1655-1665},
  shortjournal = {IET Image Process.},
  title        = {Rotated points for object detection in remote sensing images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FSKT-GE: Feature maps similarity knowledge transfer for
low-resolution gaze estimation. <em>IETIP</em>, <em>18</em>(6),
1642–1654. (<a href="https://doi.org/10.1049/ipr2.13056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limited of texture details information in low-resolution facial or eye images presents a challenge for gaze estimation. To address this, FSKT-GE (feature maps similarity knowledge transfer for low-resolution gaze estimation) is proposed, a gaze estimation framework consisting of both a high resolution (HR) network and low resolution (LR) network with the identical structure. Rather than mere feature imitation, this issue is addressed by assessing the cosine similarity of feature layers, emphasizing the distribution similarity between the HR and LR networks. This enables the LR network to acquire richer knowledge. This framework utilizes a combination loss function, incorporating cosine similarity measurement, soft loss based on probability distribution difference and gaze direction output, along with a hard loss from the LR network output layer. This approach on low-resolution datasets derived from Gaze360 and RT-Gene datasets is validated, demonstrating excellent performance in low-resolution gaze estimation. Evaluations on low-resolution images obtained through 2×, 4×, and 8× down-sampling are conducted on two datasets. On the Gaze360 dataset, the lowest mean angular errors of 10.97°, 11.22°, and 13.61° were achieved, while on the RT-Gene dataset, the lowest mean angular errors of 6.73°, 6.83°, and 7.75° were obtained.},
  archive      = {J_IETIP},
  author       = {Chao Yan and Weiguo Pan and Songyin Dai and Bingxin Xu and Cheng Xu and Hongzhe Liu and Xuewei Li},
  doi          = {10.1049/ipr2.13056},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1642-1654},
  shortjournal = {IET Image Process.},
  title        = {FSKT-GE: Feature maps similarity knowledge transfer for low-resolution gaze estimation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TMNIO: Triplet merged network with involution operators for
improved few-shot image classification. <em>IETIP</em>, <em>18</em>(6),
1629–1641. (<a href="https://doi.org/10.1049/ipr2.13055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning enables machines to learn efficiently from limited labelled data. However, existing few-shot learning methods may perform poorly when there is a lack of sufficient samples, and may encounter problems such as domain shift or overfitting when applied to new domains or tasks. To address the issues of poor fitting and insufficient generalization ability in new domains, a new method called triplet merged network with involution operators (TMNIO) is proposed. This method employs dual encoders that extract common and distinctive features from the prototype network, thereby enhancing the model&#39;s feature extraction capability. To further improve this ability, the traditional convolutional kernels are replaced with involution operators, which not only reduce the parameter count but also enlarge the receptive field to better extract local feature information. Additionally, this method employs a two-stage training strategy, where triplet loss is used in the first stage to train the model and enhance its robustness and generalization ability. Extensive experiments on the miniImageNet, Omniglot, and Caltech-UCSD Birds-200 (CUB) datasets have shown that our proposed method achieved significant improvements in both training speed and accuracy, particularly on the miniImageNet dataset, where it achieved an outstanding 10% performance improvement.},
  archive      = {J_IETIP},
  author       = {Qi Lulu and Xu Ranhui and Zhao Shaojie and Zheng Mingming and Yu Weiqin},
  doi          = {10.1049/ipr2.13055},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1629-1641},
  shortjournal = {IET Image Process.},
  title        = {TMNIO: Triplet merged network with involution operators for improved few-shot image classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Infrared multi-target detection and tracking in dense urban
traffic scenes. <em>IETIP</em>, <em>18</em>(6), 1613–1628. (<a
href="https://doi.org/10.1049/ipr2.13053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared object detection and tracking in dense urban traffic remain a challenge due to factors such as low contrast, small intra-class differences, and frequent false positives and negatives. To overcome these, the authors introduce YOLO-IR, an algorithm based on the enhanced YOLOv8s, and YOLO-DeepOC-IR, a comprehensive infrared multi-object tracking method for urban traffic, integrating both detection and tracking. During preprocessing, three infrared image enhancement techniques, local contrast multi-scale enhancement, non-local means, and contrast limited adaptive histogram equalization, are applied for better reliability in dense scenes. To further improve the performance, the original YOLOv8s backbone is replaced with MobileVITv3 to enhance detection accuracy and robustness. This infrared feature extraction module, incorporated into the detector, combines canny edge detection, Gabor filtering, and open operation layers, significantly boosting object detection in infrared imagery. The tracker&#39;s feature processing capabilities are improved using the learned arrangements of three patch codes descriptor and locality-sensitive hashing for feature extraction and matching. Experimental results on FLIR ADAS v2 and InfiRay datasets indicate superior performance of this method, achieving 78.6% mAP and 151.1 FPS in detection, and up to 80.8% moving object tracking accuracy, 78.6% identification F1 score, and 62.1% higher order tracking accuracy in multi-object tracking.},
  archive      = {J_IETIP},
  author       = {Chaoneng Zha and Suyun Luo and Xinhao Xu},
  doi          = {10.1049/ipr2.13053},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1613-1628},
  shortjournal = {IET Image Process.},
  title        = {Infrared multi-target detection and tracking in dense urban traffic scenes},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSA-UNet: Whole brain segmentation by u-net with
squeeze-and-excitation block and self-attention block from the 2.5D
slice image. <em>IETIP</em>, <em>18</em>(6), 1598–1612. (<a
href="https://doi.org/10.1049/ipr2.13052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whole brain segmentation from magnetic resonance images (MRI) is crucial in diagnosing brain diseases and analyzing neuroimaging data. Despite advances through deep learning, challenges such as uneven gray distribution and the presence of artifacts still present hurdles in medical image processing. These limitations are often a result of insufficient spatial contextual information and lack of attention to important regions within existing models. To address these issues, this paper presents SSA-UNet (Squeeze-and-Excitation and Self-Attention UNet), a uniquely designed deep convolutional neural network that integrates spatial constraints by converting three consecutive 2D MRI slices into a single 2.5D image. This facilitates capturing inter-slice dependencies effectively. Additionally, the newly formulated SSA block, which sequentially incorporates channel attention and Self-Attention mechanisms, is placed before the decoders in the conventional U-Net architecture. This enables the network to automatically weight different feature maps and focus more effectively on regions requiring precise segmentation. Rigorous evaluations on LPBA40 and IBSR18 datasets substantiate the remarkable improvements in accuracy and stability achieved by SSA-UNet. Results indicate Dice coefficients of 98.38% and 97.47%, specificity of 99.69% and 99.57%, and sensitivity of 98.5% and 97.98% for the respective datasets. Compared to other existing models, SSA-UNet shows significant improvements on both the LPBA40 and IBSR18 datasets. On the LPBA40 dataset, SSA-UNet&#39;s Dice coefficient improved by 0.33% compared to the sub-optimal model, while on the IBSR18 dataset, the improvement reached 1.78%. These empirical findings demonstrate SSA-UNet&#39;s heightened capability in addressing the long-standing challenges in MRI-based whole-brain segmentation.},
  archive      = {J_IETIP},
  author       = {Shaofeng Jiang and Xingyan Chen and Chen Yi},
  doi          = {10.1049/ipr2.13052},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1598-1612},
  shortjournal = {IET Image Process.},
  title        = {SSA-UNet: Whole brain segmentation by U-net with squeeze-and-excitation block and self-attention block from the 2.5D slice image},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-attention residual network-based spatial
super-resolution synthesis for time-varying volumetric data.
<em>IETIP</em>, <em>18</em>(6), 1579–1597. (<a
href="https://doi.org/10.1049/ipr2.13050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of scientific visualization, the upscaling of time-varying volume is meaningful. It can be used in in situ visualization to help scientists overcome the limitations of I/O speed and storage capacity when analysing and visualizing large-scale, time-varying simulation data. This paper proposes self-attention residual network-based spatial super-resolution (SARN-SSR), a spatial super-resolution model based on self-attention residual networks that can generate time-varying data with temporal coherence. SARN-SSR consists of two components: a generator and a discriminator. The generator takes the low-resolution volume sequences as the input and gives the corresponding high-resolution volume sequences as the output. The discriminator takes both synthesized and real high-resolution volume sequence as the input and gives a matrix to predict the realness as the output. To verify the validity of SARN-SSR, four sets of time-varying volume datasets are applied from scientific simulation. In addition, SARN-SSR is compared on these datasets, both qualitatively and quantitatively, with two deep learning-based techniques and one traditional technique. The experimental results show that by using this method, the closest time-varying data to the ground truth can be obtained.},
  archive      = {J_IETIP},
  author       = {Ji Ma and Yuhao Ye and Jinjin Chen},
  doi          = {10.1049/ipr2.13050},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1579-1597},
  shortjournal = {IET Image Process.},
  title        = {Self-attention residual network-based spatial super-resolution synthesis for time-varying volumetric data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly supervised instance segmentation via peak mining and
filtering. <em>IETIP</em>, <em>18</em>(6), 1565–1578. (<a
href="https://doi.org/10.1049/ipr2.13049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the full extent of pixel-level instance response in a weakly supervised manner remains unsatisfactory. Peak response maps (PRMs) localizes the discriminative object regions but cannot provide complete instance information, suffering from incomplete segmentation and unreliable mask prediction by noisy proposal retrieval. This work tackles this challenging problem by mining diverse class peak responses that include more discriminative and complete object regions and retrieving more reliable proposals from noisy segment proposal galleries. First, the existing method is enhanced with two more classification branches, thus contributing to more diverse and abundant instance regions from peak response maps. The mined class peak responses from two of the branches are then merged to generate more complete peak response maps by a clustering approach in their deep feature space. Then, instance segmentation masks are retrieved from a noisy object segment proposal gallery with class confidence, which is calculated by a normal classifier to obtain cleaner mask prediction. Finally, the pseudo-supervision can be used to train an instance segmentation network in a fully supervised manner. Experiments on the PASCAL VOC 2012 dataset and COCO dataset show that the approach works effectively and outperforms other counterparts by a margin of more than 6 %, 4%, and 3% with the mean average precision (mAP) at IoU threshold of 0.25, 0.5 and 0.75, respectively.},
  archive      = {J_IETIP},
  author       = {Zuxian Huang and Dongsheng Pan and Gangshan Wu},
  doi          = {10.1049/ipr2.13049},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1565-1578},
  shortjournal = {IET Image Process.},
  title        = {Weakly supervised instance segmentation via peak mining and filtering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain tumour segmentation framework with deep nuanced
reasoning and swin-t. <em>IETIP</em>, <em>18</em>(6), 1550–1564. (<a
href="https://doi.org/10.1049/ipr2.13048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tumour medical image segmentation plays a crucial role in clinical imaging diagnosis. Existing research has achieved good results, enabling the segmentation of three tumour regions in MRI brain tumour images. Existing models have limited focus on the brain tumour areas, and the long-term dependency of features is weakened as the network depth increases, resulting in blurred edge segmentation of the targets. Additionally, considering the excellent segmentation performance of the Swin Transformer(Swin-T) network, its network structure and parameters are relatively large. To address these limitations, this paper proposes a brain tumour segmentation framework with deep nuanced reasoning and Swin-T. It is mainly composed of the backbone hybrid network (BHN) and the deep micro texture extraction module (DMTE). The BHN combines the Swin-T stage with a new downsampling transition module called dual path feature reasoning (DPFR). The entire network framework is designed to extract global and local features from multi-modal data, enabling it to capture and analyze deep texture features in multi-modal images. It provides significant optimization over the Swin-T network structure. Experimental results on the BraTS dataset demonstrate that the proposed method outperforms other state-of-the-art models in terms of segmentation performance. The corresponding source codes are available at https://github.com/CurbUni/Brain-Tumor-Segmentation-Framework-with-Deep-Nuanced-Reasoning-and-Swin-T .},
  archive      = {J_IETIP},
  author       = {Yang Xu and Kun Yu and Guanqiu Qi and Yifei Gong and Xiaolong Qu and Li Yin and Pan Yang},
  doi          = {10.1049/ipr2.13048},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1550-1564},
  shortjournal = {IET Image Process.},
  title        = {Brain tumour segmentation framework with deep nuanced reasoning and swin-T},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-generative adversarial networks for simulating
rain field. <em>IETIP</em>, <em>18</em>(6), 1540–1549. (<a
href="https://doi.org/10.1049/ipr2.13047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synthesis of rain fields is essential in multiple research fields and applications, including Single-image Derain. However, there is a lack of research on simulated rain fields, and the existing rain field generation models struggle to capture complex spatial distributions and generate truly random rain fields. To address this, the authors propose a generative adversarial networks-based rain field generation network, which consists of a generator, a discriminator, and a feature extraction block that can produce realistic and complex rain fields. The authors’ experiments demonstrate that this method achieves an average Frechet Inception Distance score of 0.035, and user studies indicate that the generated rain distribution looks naturally.},
  archive      = {J_IETIP},
  author       = {Chen Li and Zheng Yang Zhao and Jia Li and Ye Cai Guo},
  doi          = {10.1049/ipr2.13047},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1540-1549},
  shortjournal = {IET Image Process.},
  title        = {Attention-generative adversarial networks for simulating rain field},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature-enhanced representation with transformers for
multi-view stereo. <em>IETIP</em>, <em>18</em>(6), 1530–1539. (<a
href="https://doi.org/10.1049/ipr2.13046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing multi-view stereo (MVS) methods fail to consider global context information in the stage of feature extraction and cost aggregation. As transformers have shown remarkable performance on various vision tasks due to their ability to perceive global contextual information, this paper proposes a transformer-based feature enhancement network (TF-MVSNet) to facilitate feature representation learning by combining local features (both 2D and 3D) with long-range contextual information. To reduce memory consumption of feature matching, the cross-attention mechanism is leveraged to efficiently construct 3D cost volumes under the epipolar constraint. Additionally, a colour-guided network is designed to refine depth maps at a coarse stage, hence reducing incorrect depth predictions at a fine stage. Extensive experiments were performed on the DTU dataset and Tanks and Temples (T&amp;T) benchmark and results are reported.},
  archive      = {J_IETIP},
  author       = {Lintao Xiang and Hujun Yin},
  doi          = {10.1049/ipr2.13046},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1530-1539},
  shortjournal = {IET Image Process.},
  title        = {Feature-enhanced representation with transformers for multi-view stereo},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel automatic annotation method for whole slide
pathological images combined clustering and edge detection technique.
<em>IETIP</em>, <em>18</em>(6), 1516–1529. (<a
href="https://doi.org/10.1049/ipr2.13045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pixel-level labeling of regions of interest in an image is a key step in building a labeled training dataset for supervised deep learning networks of images. However, traditional manual labeling of cancerous regions in digital pathological images by doctors is time-consuming and inefficient. To address this issue, this paper proposes an automatic labeling method for whole slide images, which combines clustering and edge detection techniques. The proposed method utilizes the multi-level feature fusion model and the Long-Short Term Memory network to discriminate the cancerous nature of the whole slide images, thereby improving the classification accuracy of the whole slide images. Subsequently, the automatic labeling of cancerous regions is achieved by integrating a density-based clustering algorithm and an edge point extraction algorithm, both based on the discriminated results of the cancerous properties of whole slide images. The experimental results demonstrate the effectiveness of the proposed method, which offers an efficient and accurate solution to the challenging task of cancerous region labeling in digital pathological images.},
  archive      = {J_IETIP},
  author       = {Wei-long Ding and Wan-yin Liao and Xiao-jie Zhu and Hong-bo Zhu},
  doi          = {10.1049/ipr2.13045},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1516-1529},
  shortjournal = {IET Image Process.},
  title        = {A novel automatic annotation method for whole slide pathological images combined clustering and edge detection technique},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skeleton extraction of hard-pen regular script based on
stroke characterization and ambiguous zone detection. <em>IETIP</em>,
<em>18</em>(6), 1504–1515. (<a
href="https://doi.org/10.1049/ipr2.13044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Intelligent Evaluation System for Calligraphy Characters (IESCC) is used for teaching calligraphy, and users can learn calligraphy through the modifications given by the system. Chinese character skeleton extraction is an important step in the intelligent evaluation algorithm of calligraphic characters. The skeletons of Chinese characters extracted by traditional refinement algorithms are prone to redundant branches and deformed skeletons, which can lead to skeleton extraction results that do not conform to the topology of the original character. In this study, the focus lies on hard-pen regular script, and skeleton repair and extraction are performed for these characters. According to the writing characteristics of regular script, the redundant burs are removed and the deformation zone of the thinned skeleton is detected, and then the idea of first splitting is used, then restructuring, to propose a skeleton extraction algorithm based on stroke characterization and ambiguous zone detection for hard-pen regular script, referred to as SCAD. First, a thinning algorithm is used to extract the skeleton of Chinese characters and remove redundant pixels. By analyzing the stroke characteristics of regular script, the burrs are classified and different conditions are set to detect and remove the burrs. Then the ambiguous zones are detected according to the different kinds of junction points. Then, curvature, stroke width and direction deviation are used to analyze the continuity of stroke segments, and the decision function is used to classify the stroke segments. Finally, the stroke segments with optimal pairings were compensated by interpolation according to the direction trend. This concludes the skeleton extraction. Skeleton extraction is performed on 1000 sample characters, and the SCAD algorithm can extract the skeleton of Chinese characters with an accuracy of up to 98.37%. It is proved that the SCAD method proposed here is a practical and effective method to extract the skeleton of hard-pen regular script.},
  archive      = {J_IETIP},
  author       = {Zhanyang Xu and Feiyang Qin and Ningyang Xiong and Hongyan Shi and Jiarui Zhang and Wei Lin},
  doi          = {10.1049/ipr2.13044},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1504-1515},
  shortjournal = {IET Image Process.},
  title        = {Skeleton extraction of hard-pen regular script based on stroke characterization and ambiguous zone detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). T-skeleton: Accurate scene text detection via
instance-aware skeleton embedding. <em>IETIP</em>, <em>18</em>(6),
1491–1503. (<a href="https://doi.org/10.1049/ipr2.13043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing segmentation-based methods have made considerable progress in arbitrarily shaped text detection due to the advantage of dealing with shape variation. However, there still exist challenges to detecting accurate text instances with dense layouts, inaccurate annotations, and complex backgrounds. Many recent works have focused on improving arbitrary boundary prediction, but it may be difficult to accurately distinguish each instance of dense layouts because their boundary pixels may be mistakenly classified to produce inaccurate results (i.e., adhesive texts) with inaccurate annotation and complex backgrounds. Considering the local and long-range dependencies, this paper proposes an efficient text detector, namely T-Skeleton, to obtain more reliable segmentation detections. In the spirit of object skeletonization, we introduce the text instance skeleton highlighting the semantically significant structure (similar to the skeleton of a fish) to explicitly capture the long-range dependencies of text instances. The key idea of T-Skeleton is to calibrate the coarse text proposals by embedding text instance skeletons to separate crowd texts accurately and robustly. We further design a channel attention module to enlarge the performance margin between T-Skeleton and the segmentation baseline. Experimental results on four publicly available datasets show the superiority of T-Skeleton in handling long and curved texts.},
  archive      = {J_IETIP},
  author       = {Haiyan Li and Xingfei Hu and Hongtao Lu},
  doi          = {10.1049/ipr2.13043},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1491-1503},
  shortjournal = {IET Image Process.},
  title        = {T-skeleton: Accurate scene text detection via instance-aware skeleton embedding},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Diverse branch feature refinement network for efficient
multi-scale super-resolution. <em>IETIP</em>, <em>18</em>(6), 1475–1490.
(<a href="https://doi.org/10.1049/ipr2.13042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the existence of various super-resolution (SR) methods, most of them focus on designing models for specific upscaling factors rather than fully exploiting inter-scale correlation to improve efficiency. In contrast, multi-scale SR methods can effectively reduce the redundancy of network parameters by aggregating the feature extraction processes corresponding to multiple scales into a unified process. The aim of this study is to enhance the compactness and efficiency of the SR model. Thus, an efficient multi-scale SR method called the diverse branch feature refinement network (DBFRN) is proposed. By decoupling the training process and inference process based on the idea of structural re-parameterization, multi-branch topology is adopted to enrich multi-scale learning and merge branches to achieve efficient inference with equivalent effects. Specifically, two re-parameterization strategies are designed and two corresponding feature refinement blocks for different feature levels in multi-scale SR network. Extensive experiments demonstrate that the proposed multi-scale SR method is effective and efficient, and it can outperform advanced single-scale methods in terms of quantity and quality.},
  archive      = {J_IETIP},
  author       = {Dacheng Zhang and Wei Zhang and Weimin Lei and Xinyi Chen},
  doi          = {10.1049/ipr2.13042},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1475-1490},
  shortjournal = {IET Image Process.},
  title        = {Diverse branch feature refinement network for efficient multi-scale super-resolution},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot object detection based on global context and
implicit knowledge decoupled head. <em>IETIP</em>, <em>18</em>(6),
1460–1474. (<a href="https://doi.org/10.1049/ipr2.13040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The acquisition cycle of remote sensing images is slow, and the labelling process encounters challenges, which have become prominent with the rapid development of remote sensing image object detection research. Therefore, this article provides a way to make the model better capture the diversity and contextual relationships in the data, and solve this problem by more than just data augmentation. Specifically, this method is a few-shot object detection method for remote sensing based on global context combined with implicit knowledge decoupled head (GC-IKDH). This method first uses a segmentation strategy to convert high-resolution images into low-resolution images and expands the sample size through a generative model. Secondly, GC attention is introduced to generate a GC vector by weighting and averaging the information of each position in the input sequence, which helps the model better understand the semantics of the input sequence. Finally, an IKDH is added to improve the model head, which is used to learn specific features in the data so that the model can better handle the diversity in the data. Experimental results show that GC attention and IKDH boosting provide a good performance boost to the baseline model. Compared with other few-shot samples, this method achieves state-of-the-art performance under different shot settings and highly competitive results on two benchmark datasets (NWPU VHR-10 and DIOR).},
  archive      = {J_IETIP},
  author       = {Shiyue Li and Guan Yang and Xiaoming Liu and Kekun Huang and Yang Liu},
  doi          = {10.1049/ipr2.13040},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1460-1474},
  shortjournal = {IET Image Process.},
  title        = {Few-shot object detection based on global context and implicit knowledge decoupled head},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal knowledge learning with scene text for
fine-grained image classification. <em>IETIP</em>, <em>18</em>(6),
1447–1459. (<a href="https://doi.org/10.1049/ipr2.13039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text in natural images carries additional semantic information to aid in image classification. Existing methods lack full consideration of the deep understanding of the text and the visual text relationship, which results in the difficult to judge the semantic accuracy and the relevance of the visual text. This paper proposes image classification based on Cross modal Knowledge Learning of Scene Text (CKLST) method. CKLST consists of three stages: cross-modal scene text recognition, text semantic enhancement, and visual-text feature alignment. In the first stage, multi-attention is used to extract features layer by layer, and a self-mask-based iterative correction strategy is utilized to improve the scene text recognition accuracy. In the second stage, knowledge features are extracted using external knowledge and are fused with text features to enhance text semantic information. In the third stage, CKLST realizes visual-text feature alignment across attention mechanisms with a similarity matrix, thus the correlation between images and text can be captured to improve the accuracy of the image classification tasks. On Con-Text dataset, Crowd Activity dataset, Drink Bottle dataset, and Synth Text dataset, CKLST can perform significantly better than other baselines on fine-grained image classification, with improvements of 3.54%, 5.37%, 3.28%, and 2.81% over the best baseline in mAP, respectively.},
  archive      = {J_IETIP},
  author       = {Li Xiong and Yingchi Mao and Zicheng Wang and Bingbing Nie and Chang Li},
  doi          = {10.1049/ipr2.13039},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1447-1459},
  shortjournal = {IET Image Process.},
  title        = {Cross-modal knowledge learning with scene text for fine-grained image classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Super-resolution reconstruction based on generative
adversarial networks with dual branch half instance normalization.
<em>IETIP</em>, <em>18</em>(6), 1434–1446. (<a
href="https://doi.org/10.1049/ipr2.13036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a super-resolution reconstruction model, SRPGANto improve the visual quality of images based on generative adversarial networks (GANs) by improving the network structures of the generator and the discriminator. In the generator, a dual branch residual block is designed instead of the residual block, including a branch with an attention mechanism and a branch without an attention mechanism, to extract more differentiated features. Normalization methods are explored to avoid unstable training and bath normalization artifacts and use a half instance normalization layer that is more suitable for underlying visual problems compared with traditional batch normalization. In the discriminator, PatchGAN is applied instead of typical GAN to improve the generation of local texture by discriminating each patch rather than the global image. The experimental results on the public datasets demonstrate that the proposed SRPGAN can achieve excellent quantitative evaluation while improving the visual quality of reconstructed images.},
  archive      = {J_IETIP},
  author       = {Xiaoxin Guo and Zhenchuan Tu and Haoran Zhang and Hongliang Dong},
  doi          = {10.1049/ipr2.13036},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1434-1446},
  shortjournal = {IET Image Process.},
  title        = {Super-resolution reconstruction based on generative adversarial networks with dual branch half instance normalization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DMCVS: Decomposed motion compensation-based video
stabilization. <em>IETIP</em>, <em>18</em>(6), 1422–1433. (<a
href="https://doi.org/10.1049/ipr2.13035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of handheld devices, video stabilization is becoming increasingly important. In previous studies, many methods have been proposed to stabilize shaky videos. However, these methods fail to balance between image content integrity and stability. Some methods sacrifice image content for better stability. Other methods ignore the subtle jitters, which leads to poor stability. This work innovatively proposes a video stabilization method based on decomposed motion compensation. First, a grid-based motion statistics method is adopted for motion estimation, which obtains more accurate motion vectors according to matched likelihood estimates. Then, the motion compensation is inherently decomposed into two parts: linear motion compensation and auxiliary motion compensation. Linear motion compensation removes complex jitter by constructing linear path constraints to obtain a more stable camera path. Auxiliary motion compensation uses a moving average filter to remove the high-frequency jitter as a supplement and preserve more image content. The two components are combined with individual weights to derive the final transform matrix and warp the original frames. Experimental results show that our method outperforms the previous methods on NUS and DeepStab datasets qualitatively and quantitatively.},
  archive      = {J_IETIP},
  author       = {Qian Huang and Jiwen Liu and Chuanxu Jiang and Yiming Wang},
  doi          = {10.1049/ipr2.13035},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1422-1433},
  shortjournal = {IET Image Process.},
  title        = {DMCVS: Decomposed motion compensation-based video stabilization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-exposure embeddings for graph learning: Towards high
dynamic range image saliency prediction. <em>IETIP</em>, <em>18</em>(6),
1411–1421. (<a href="https://doi.org/10.1049/ipr2.13033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying saliency in high dynamic range (HDR) images is a fundamentally important issue in HDR imaging, and plays critical roles towards comprehensive scene understanding. Most of existing studies leverage hand-crafted features for HDR image saliency prediction, lacking the capabilities of fully exploiting the characteristics of HDR image (i.e. wider luminance range and richer colour gamut). Here, systematical studies are carried out on HDR image saliency prediction by proposing a new framework to single out the contributions from multi-exposure images. Specifically, inspired by the mechanism of HDR imaging, the method first utilizes graph neural networks to model the relations among multi-exposure images and the tone-mapped image obtained from an HDR image, enabling more discriminative saliency-related feature representations. Subsequently, the saliency features driven by global semantic knowledge are aggregated from the tone-mapped image through enhancing global context-aware semantic information. Finally, a fusion module is designed to integrate saliency-oriented feature representations originated from multi-exposure images and the tone-mapped image, producing the saliency maps of HDR images. Moreover, a new challenging HDR eye fixation database (HDR-EYEFix) is created, expecting to further contribute the research on HDR image saliency prediction. Experiment results show that the method obtains superior performance compared to the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Jun Xing and Qiudan Zhang and Xuelin Shen and Xu Wang},
  doi          = {10.1049/ipr2.13033},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1411-1421},
  shortjournal = {IET Image Process.},
  title        = {Multi-exposure embeddings for graph learning: Towards high dynamic range image saliency prediction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of feature matching methods. <em>IETIP</em>,
<em>18</em>(6), 1385–1410. (<a
href="https://doi.org/10.1049/ipr2.13032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching plays a crucial role in computer vision, with applications in visual localization, simultaneous localization and mapping (SLAM), image stitching, and more. It establishes correspondences between sets of feature points from multiple images, enabling various tasks. Over the years, feature matching has witnessed significant development, with an increasing number of methods being applied. However, different methods exhibit different degrees of applicability in different scenarios and requirements due to their different rationales. To cope with these issues, a comprehensive analysis and comparison of matching methods are essential. Existing reviews often lack coverage of deep learning models and focus more on feature detection and description, neglecting the matching process. This survey investigates feature detection, description, and matching techniques within the feature-based image-matching pipeline. Representative methods, their mechanisms, and application scenarios are also briefly introduced. In addition, comprehensive evaluations of classical and state-of-the-art methods are conducted through extensive experiments on representative datasets. Particularly, matching-based applications are compared to fully demonstrate the advantages of the methods. Lastly, this survey highlights current problems and development directions in matching methods, serving as a reference for researchers in the field.},
  archive      = {J_IETIP},
  author       = {Qian Huang and Xiaotong Guo and Yiming Wang and Huashan Sun and Lijie Yang},
  doi          = {10.1049/ipr2.13032},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1385-1410},
  shortjournal = {IET Image Process.},
  title        = {A survey of feature matching methods},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid u-shaped and transformer network for change
detection in high-resolution remote sensing images. <em>IETIP</em>,
<em>18</em>(5), 1373–1384. (<a
href="https://doi.org/10.1049/ipr2.13037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks based remote sensing change detection has recently shown significant performance improvement. However, small region changes and global-local features in high-resolution remote sensing images are not fully explored. This paper introduces a hybrid U-shaped and transformer network for change detection in high-resolution remote sensing images. Specifically, a UNet++-based backbone to facilitate feature learning across different scales. In addition, we introduce a transformer-based feature fusion module for extracting long-range dependencies, which can enhance the representation ability of the network. Furthermore, the introduced efficient channel attention mechanism can efficiently calibrate the feature representation and concentrate on more important feature information. Thanks to the above designs, the proposed method enjoys a strong ability to extract local and global features for remote sensing change detection. Extensive experimental results on different remote sensing images show that our method can achieve superior performance in comparison with state-of-the-art change detection methods.},
  archive      = {J_IETIP},
  author       = {Huapeng Wu and Mengxue Yuan and Tianming Zhan},
  doi          = {10.1049/ipr2.13037},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1373-1384},
  shortjournal = {IET Image Process.},
  title        = {A hybrid U-shaped and transformer network for change detection in high-resolution remote sensing images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive neuro-fuzzy inference system optimized by
genetic algorithm for brain tumour detection in magnetic resonance
images. <em>IETIP</em>, <em>18</em>(5), 1358–1372. (<a
href="https://doi.org/10.1049/ipr2.13031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptive neuro-fuzzy inference system is presented which is optimized by a genetic algorithm to classify normal and abnormal brain tumours. The classifier is fast and simple, named genetic algorithm-adaptive neuro-fuzzy inference system, and the determined learning rules minimize its error and improve its accuracy. The presented system follows five steps including preprocessing, morphological operation, feature extraction, feature selection, and classification. Morphological operators segment the abnormal regions and calculate the tumour area. The statistical features and the grey-level co-occurrence matrix are employed for feature extraction. Magnetic resonance images are considered and 12 statistical features are extracted, then the genetic algorithm-based selection technique helps to select features and reduce the extracted features and improves the accuracy and decision time. So, the high dimensionality and the computational complexity of the adaptive neuro-fuzzy inference system are reduced, and the classifier decides more efficiently. The input data are the figshare brain tumour dataset with 670 abnormal and 670 normal magnetic resonance images, and the classifier requires 10.788 s for classification. The efficient performance of the genetic algorithm-adaptive neuro-fuzzy inference system is confirmed by the accuracy of 99.85%, sensitivity of 99.7%, specificity of 100%, precision of 100%, and mean square error of 0.0027.},
  archive      = {J_IETIP},
  author       = {Marzieh Ghahramani and Nabiollah Shiri},
  doi          = {10.1049/ipr2.13031},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1358-1372},
  shortjournal = {IET Image Process.},
  title        = {An adaptive neuro-fuzzy inference system optimized by genetic algorithm for brain tumour detection in magnetic resonance images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A joint image super-resolution network for multiple
degradations removal via complementary transformer and convolutional
neural network. <em>IETIP</em>, <em>18</em>(5), 1344–1357. (<a
href="https://doi.org/10.1049/ipr2.13030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While recent years have witnessed the unprecedented success of deep convolutional neural networks (CNNs) and vision transformers in single-image super-resolution (SISR), the degradation assumptions are simple and usually bicubic downsampling. Thus, their performances will drop dramatically when the actual degradation does not match this assumption, and they lack the capability to handle multiple degradations (e.g. Gaussian noise, bicubic downsizing, and salt &amp; pepper noise). To address the issues, in this paper, the authors propose a joint SR model (JIRSR) that can effectively handle multiple degradations in a single model. Specifically, the authors build the parallel Transformer and CNN branches that complement each other through bidirectional feature fusion. Moreover, the authors also adopt a random permutation of different kinds of noise and resizing operations to build the training datasets. Extensive experiments on classical SR, denoising, and multiple degradation removal demonstrate that the authors’ JIRSR achieves state-of-the-art (SOTA) performance on public benchmarks. Concretely, the authors’ JIRSR outperforms the second-best model by 0.23 to 0.74 dB for multiple degradations removal and is 0.20 to 0.36 dB higher than the SOTA methods on the Urban100 dataset under the ×4 SR task.},
  archive      = {J_IETIP},
  author       = {Guoping Li and Zhenting Zhou and Guozhong Wang},
  doi          = {10.1049/ipr2.13030},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1344-1357},
  shortjournal = {IET Image Process.},
  title        = {A joint image super-resolution network for multiple degradations removal via complementary transformer and convolutional neural network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-world image deblurring using data synthesis and feature
complementary network. <em>IETIP</em>, <em>18</em>(5), 1328–1343. (<a
href="https://doi.org/10.1049/ipr2.13029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many learning-based approaches to image deblurring have received increasing attention in recent years. However, the models trained on existing synthetic datasets do not generalize well to real-world blur, resulting in undesirable artifacts and residual blur. This work attempts to address this problem from two aspects: training data synthesis and network architecture. To narrow the domain gap between synthetic and real domains, a realistic blur synthesis pipeline to generate high-quality blurred data is proposed. Since the blur is non-uniform and has different scales and degrees, a parallel feature complementary module to fully exploit the local and non-local information, which improves the feature representation and helps the network to perceive the non-uniform blur, is developed. In addition, a spatial Fourier reconstruction block to facilitate correct detail recovery in the spatial and Fourier domains is introduced. Based on these two designs, an effective encoder–decoder network for deblurring is designed. Extensive experiments demonstrate the validity and superiority of the proposed blur synthesis method and deblurring network. In particular, the proposed deblurring network can achieve superior or comparable performance to Restormer, while saving 70% of network parameters and 53% of floating point operations (FLOPs).},
  archive      = {J_IETIP},
  author       = {Hao Wei and Chenyang Ge and Xin Qiao and Pengchao Deng},
  doi          = {10.1049/ipr2.13029},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1328-1343},
  shortjournal = {IET Image Process.},
  title        = {Real-world image deblurring using data synthesis and feature complementary network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time defect detection method based on YOLO-GSS at the
edge end of a transmission line. <em>IETIP</em>, <em>18</em>(5),
1315–1327. (<a href="https://doi.org/10.1049/ipr2.13028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining edge devices with intelligent inspection for transmission lines can fulfill the demand for real-time defect detection in the field. However, there has been limited research on algorithms suitable for edge devices with low computational power and memory, and the existing research primarily focuses on CPU optimization. To address these issues, this paper proposes a real-time defect detection method for transmission line endpoints based on YOLO-GSS (YOLOv8 with Mosaic-9, G-GhostNet, S-FPN, and Spatial Intersection over Union (SIoU) modifications). First, the authors improve the input of the YOLOv8 network using Mosaic-9 to increase the number of input features in the training phase and enhance algorithm robustness. Next, the authors introduce G-GhostNet and S-FPN to enhance the backbone and neck sections while improving inference speed and accuracy. Finally, the authors modify the Complete Intersection over Union loss function of YOLOv8 using SIoU to further improve the detection accuracy. Experimental results demonstrate that compared to the original YOLOv8, the proposed method achieves a 5x increase in inference speed on Nvidia Jetson NX edge devices and a 7.7% improvement in accuracy, meeting the real-time defect detection requirements for transmission line field inspections.},
  archive      = {J_IETIP},
  author       = {Chao Hou and ZhiLei Li and XueLiang Shen and GuoChao Li},
  doi          = {10.1049/ipr2.13028},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1315-1327},
  shortjournal = {IET Image Process.},
  title        = {Real-time defect detection method based on YOLO-GSS at the edge end of a transmission line},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel efficient wildlife detecting method with lightweight
deployment on UAVs based on YOLOv7. <em>IETIP</em>, <em>18</em>(5),
1296–1314. (<a href="https://doi.org/10.1049/ipr2.13027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient animal detection is essential for biodiversity protection. Unmanned aerial vehicles (UAVs) have been widely used because of their low costs and minimal environmental intrusion. However, using UAVs for practical animal detection poses two challenges: (a) the UAV&#39;s fly highly to avoid disturbing animals, resulting in small object detection problems; (b) the limited processing power of UAVs makes large state-of-the-art (SOTA) methods (e.g., You Only Look Once V7, YOLOv7) difficult to deploy. This work proposes the WILD-YOLO based on YOLOv7 to deal with the two problems. To detect small objects, WILD-YOLO improves upon YOLOv7 by adding a small object detection head in the head part. To enable real-time animal detection in field environments with UAVs, the lighten FasterNet and GhostNet have been used to significantly reduce the model size.  Compared to YOLOv7, WILD-YOLO significantly reduces the number of parameters, making it suitable for lightweight deployment on UAVs. Additionally, comparisons with other lightweight models such as YOLOv7-tiny, YOLOv5-s, YOLOv4-s and MobilenetV2 on the datasets are conducted. The experimental results demonstrate that this proposed WILD-YOLO method outperforms other approaches and has great potential for effective detection of wildlife in complex environments encountered by UAVs.},
  archive      = {J_IETIP},
  author       = {Chao Mou and Chengcheng Zhu and Tengfei Liu and Xiaohui Cui},
  doi          = {10.1049/ipr2.13027},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1296-1314},
  shortjournal = {IET Image Process.},
  title        = {A novel efficient wildlife detecting method with lightweight deployment on UAVs based on YOLOv7},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brake disc positioning and defect detection method based on
improved canny operator. <em>IETIP</em>, <em>18</em>(5), 1283–1295. (<a
href="https://doi.org/10.1049/ipr2.13026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firstly, the Canny operator combined with circle fitting is used to position the working surface. Secondly, the median filter and the Laplacian operator are selected for preprocessing. Due to uneven illumination, surface texture, or environmental dust, the extracted scratches exhibit fragmentation, and traditional morphological operations cannot handle scratches well, so an improved Canny operator combined with probabilistic Hough transform is proposed for scratch connection. A bidirectional connection is introduced to optimize the edge detection process of the Canny operator, and a parallel algorithm is introduced to shorten the detection time. Finally, the standard Hough transform needs to traverse the entire parameter space, and the calculation is complicated, and the probability Hough transform is selected. The experimental results show that compared with the traditional Canny algorithm, the precision, recall, and F1-score of the improved Canny algorithm are increased by 3.2%, 13.2%, and 6.0%, respectively. After adding parallel processing, the average detection time was reduced by 45 ms. Finally, the accuracy of scratches extracted using this algorithm reached 97.96%, the leak rate, and mistake rate of this study are only 3.33% and 2.00%, which provides a more accurate and efficient theoretical support for the research on brake disc scratch detection.},
  archive      = {J_IETIP},
  author       = {Jun Guo and Yang Yang and Xin Xiong and Yue Yang and Mengzhen Shao},
  doi          = {10.1049/ipr2.13026},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1283-1295},
  shortjournal = {IET Image Process.},
  title        = {Brake disc positioning and defect detection method based on improved canny operator},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). You only label once: A self-adaptive clustering-based method
for source-free active domain adaptation. <em>IETIP</em>,
<em>18</em>(5), 1268–1282. (<a
href="https://doi.org/10.1049/ipr2.13025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing significance of data privacy protection, Source-Free Domain Adaptation (SFDA) has gained attention as a research topic that aims to transfer knowledge from a labeled source domain to an unlabeled target domain without accessing source data. However, the absence of source data often leads to model collapse or restricts the performance improvements of SFDA methods, as there is insufficient true-labeled knowledge for each category. To tackle this, Source-Free Active Domain Adaptation (SFADA) has emerged as a new task that aims to improve SFDA by selecting a small set of informative target samples labeled by experts. Nevertheless, existing SFADA methods impose a significant burden on human labelers, requiring them to continuously label a substantial number of samples throughout the training period. In this paper, a novel approach is proposed to alleviate the labeling burden in SFADA by only necessitating the labeling of an extremely small number of samples on a one-time basis . Moreover, considering the inherent sparsity of these selected samples in the target domain, a Self-adaptive Clustering-based Active Learning (SCAL) method is proposed that propagates the labels of selected samples to other datapoints within the same cluster. To further enhance the accuracy of SCAL, a self-adaptive scale search method is devised that automatically determines the optimal clustering scale, using the entropy of the entire target dataset as a guiding criterion. The experimental evaluation presents compelling evidence of our method&#39;s supremacy. Specifically, it outstrips previous SFDA methods, delivering state-of-the-art (SOTA) results on standard benchmarks. Remarkably, it accomplishes this with less than 0.5% annotation cost, in stark contrast to the approximate 5% required by earlier techniques. The approach thus not only sets new performance benchmarks but also offers a markedly more practical and cost-effective solution for SFADA, making it an attractive choice for real-world applications where labeling resources are limited.},
  archive      = {J_IETIP},
  author       = {Zhishu Sun and Luojun Lin and Yuanlong Yu},
  doi          = {10.1049/ipr2.13025},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1268-1282},
  shortjournal = {IET Image Process.},
  title        = {You only label once: A self-adaptive clustering-based method for source-free active domain adaptation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLOv5s maritime distress target detection method based on
swin transformer. <em>IETIP</em>, <em>18</em>(5), 1258–1267. (<a
href="https://doi.org/10.1049/ipr2.13024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the task of maritime emergency rescue has increased, while the cost of time for traditional methods of search and rescue is pretty long with poor effect subject to the constraints of the complex circumstances around the sea, the effective conditions, and the support capability. This paper applies deep learning and proposes a YOLOv5s-SwinDS algorithm for target detection in distress at sea. Firstly, the backbone network of the YOLOv5s algorithm is replaced by swin transformer, and a multi-level feature fusion module is introduced to enhance the feature expression ability for maritime targets. Secondly, deformable convolutional networks v2 (DCNv2) is used instead of traditional convolution to improve the recognition capability for irregular targets when the neck network features are output. Finally, the CIoU loss function is replaced with SIoU to reduce the redundant box effectively while accelerating the convergence and regression of the predicted box. Experimenting on the publicly dataset SeaDronesSee, the , and of YOLOv5s-SwinDS model are 87.9%, 75.8%, 79.1% and 42.9%, respectively, which get higher results than the original YOLOv5s model, the YOLOv7 series of models, and the YOLOv8 series of models. The experiments verifies that the algorithm has good performance in detecting maritime distress targets.},
  archive      = {J_IETIP},
  author       = {Kun Liu and Yueshuang Qi and Guofeng Xu and Jianglong Li},
  doi          = {10.1049/ipr2.13024},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1258-1267},
  shortjournal = {IET Image Process.},
  title        = {YOLOv5s maritime distress target detection method based on swin transformer},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel NAM-based image segmentation using hierarchical
density-based spatial clustering. <em>IETIP</em>, <em>18</em>(5),
1245–1257. (<a href="https://doi.org/10.1049/ipr2.13023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new method for hierarchical image segmentation based on the nonsymetry and anti-packing pattern representation model (NAM) and the hierarchical density-based spatial clustering of application with noise (HDBSCAN). The proposed framework consists of two phases. In the first phase, a super-pixel generation algorithm base on NAM is proposed. In the second phase, instead of defining an affinity matrix to merge similar regions using spatial clustering, the distance matrix defined by different region features is directly fitted into an HDBSCAN clustering module in order to merge similar regions efficiently. Similar adjacent regions can be merged into larger ones progressively and form a segmentation dendrogram for image segmentation with the clustering module. The experiments show that the proposed algorithm has a comparable or even better performance compared to the state-of-the-art hierarchical image segmentation algorithms while having much less time and memory consumption.},
  archive      = {J_IETIP},
  author       = {Yunping Zheng and Dilong Wen and Mudar Sarem},
  doi          = {10.1049/ipr2.13023},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1245-1257},
  shortjournal = {IET Image Process.},
  title        = {A novel NAM-based image segmentation using hierarchical density-based spatial clustering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new flame-based colour space for efficient fire detection.
<em>IETIP</em>, <em>18</em>(5), 1229–1244. (<a
href="https://doi.org/10.1049/ipr2.13022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision-based fire detection algorithms have been extensively researched over the past decades as they offer reliable and efficient solutions to early fire detection, as well as economical solutions in outdoor environments. The proposed systems have different levels of complexity; nevertheless, they rely heavily on colour based fire segmentation as the first processing stage using different colour spaces. This paper presents a new flame-based colour space which accentuates the difference between fire and non-fire pixel values. The hybrid Artificial Bee Colony-Teaching Learning-based Optimisation (ABC-TLBO) and the k-medoids clustering algorithm along with a Feature image constructed manually from a wide dataset of fire and non-fire pixels were used to deduce the optimised colour space conversion matrix. The latter can then be used to transform the RGB pixels from a camera feed into the new Flame-based colour space for a more efficient fire segmentation. The proposed colour space is capable to separate fire and non-fire pixels into two intensity classes, whereby the resulting between-class variance is significantly widened; this maximises the effectiveness of the binary thresholding to separate fire from non-fire pixels. The experimental results demonstrate that the proposed flame-based colour space provides a superior average qualitative and quantitative performance on benchmark datasets compared to state-of-the-art colour models; a 20% average increase in F1 score demonstrates a better balance between fire detection and false alarm rates. Moreover, the computation time of the fire detection system is faster by 15.6% in average. The proposed system provides a better implementation of the colour-based fire segmentation processing stage in vision-based fire detection systems; it vindicates, in this application context, the superiority of flame-tuned colour space over conventional ones.},
  archive      = {J_IETIP},
  author       = {Oluwarotimi Giwa and Abdsamad Benkrid},
  doi          = {10.1049/ipr2.13022},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1229-1244},
  shortjournal = {IET Image Process.},
  title        = {A new flame-based colour space for efficient fire detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on oriented surface defect detection in the
aircraft skin-coating process based on an attention detector.
<em>IETIP</em>, <em>18</em>(5), 1213–1228. (<a
href="https://doi.org/10.1049/ipr2.13020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aircraft coating process has been an important part in manufacturing process of modern aviation products. For coating defect detection, the manual observation with naked eyes is usually utilized, which leads to low production efficiency. In this paper, the authors propose the improved YOLOv5-OBB with the channel-spatial attention block (CSAB), feature pyramid non-local module (FPNM) and structured sparsity slimming criterion (SSSC). The CSAB can pay more attention to effective channel information features from the channel dimension and the target information area from the spatial dimension. The effective non-local module called FPNM is proposed to further improve the detection accuracy. The authors utilize the oriented bounding boxes (OBB) to reduce redundant background information for coating defect detection. In addition, the SSSC is proposed to achieve network slimming and trade-off between the efficiency and accuracy. The experimental results on several datasets demonstrate the effectiveness of the authors’ scheme, which achieves superior performance.},
  archive      = {J_IETIP},
  author       = {Yongde Zhang and Wei Wang and Zhonghua Guo and Yangchun Ji},
  doi          = {10.1049/ipr2.13020},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1213-1228},
  shortjournal = {IET Image Process.},
  title        = {Research on oriented surface defect detection in the aircraft skin-coating process based on an attention detector},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust image watermarking using ant colony optimization and
fast generic radial harmonic fourier moment calculation. <em>IETIP</em>,
<em>18</em>(5), 1200–1212. (<a
href="https://doi.org/10.1049/ipr2.13019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In open networks, the geometric deformation and common image processing are common image manipulation modes, which pose a great challenge in robust watermarking. To improve the robustness of GRHFM-based watermarking, a watermarking algorithm based on the fast GRHFM calculation method and ant colony optimization (ACO) based fractional parameter selection method is proposed. With the similarity between the discrete Fourier transform and GRHFM moment calculation, the algorithm utilizes fast Fourier transform to improve the GRHFM calculation accuracy and speed. To select the optimal GRHFM fractional parameter for watermarking algorithm, ACO is introduced to the fractional parameter adaptive selection method. Here, the fast Fourier transform-based calculation method is combined with the adaptive parameter selection method to maximize the invisibility and robustness of watermarking. The experimental results indicate that the algorithm achieves higher robustness with the same payload and invisibility compared with other existing watermarking methods.},
  archive      = {J_IETIP},
  author       = {Wenbing Wang and Liu Feng},
  doi          = {10.1049/ipr2.13019},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1200-1212},
  shortjournal = {IET Image Process.},
  title        = {Robust image watermarking using ant colony optimization and fast generic radial harmonic fourier moment calculation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual encoding DDS-UNet liver tumour segmentation based on
multi-scale deep and shallow feature fusion. <em>IETIP</em>,
<em>18</em>(5), 1189–1199. (<a
href="https://doi.org/10.1049/ipr2.13018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion and utilization of multi-scale deep and shallow features are of great significance in liver tumour segmentation. This study proposes a dual encoding DDS-UNet liver tumour segmentation method based on multi-scale deep and shallow feature fusion, aiming to fully achieve the fusion and utilization of deep and shallow features and achieve accurate segmentation. The proposed method mainly consists of residual convolution module fusion residual convolution (FRC), dual encoding end fusion module dual encoding end fusion (DEF), and skip connection fusion module jump connection fusion module (JCF). In the residual convolution module, a layer by layer fused residual convolution is used instead of traditional convolution to achieve better training. In the dual encoding end fusion module, multi-scale feature fusion at the end provides more comprehensive contextual information, solves the loss of spatial geometric information. The skip connection fusion module reduces the interference of invalid features by changing the weights of spatial attention and channel attention on important features. The Dice coefficient, average intersection to union ratio, accuracy, recall, and accuracy indicators tested on the LiTS dataset were 90.37%, 90.16%, 93.78%, 94.91%, and 98.84%, respectively, which are superior to many advanced liver tumor segmentation methods.},
  archive      = {J_IETIP},
  author       = {JianFeng Li and YanMin Niu},
  doi          = {10.1049/ipr2.13018},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1189-1199},
  shortjournal = {IET Image Process.},
  title        = {Dual encoding DDS-UNet liver tumour segmentation based on multi-scale deep and shallow feature fusion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LIDA-YOLO: An unsupervised low-illumination object detection
based on domain adaptation. <em>IETIP</em>, <em>18</em>(5), 1178–1188.
(<a href="https://doi.org/10.1049/ipr2.13017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-light environment is integral to everyday activities but poses significant challenges in object detection. Due to the low brightness, noise, and insufficient illumination of the acquired image, the model&#39;s object detection performance is reduced. Opposing recent studies mainly developing using supervised learning models, this paper suggests LIDA-YOLO, an approach for unsupervised adaptation of low-illumination object detectors. The model improves the YOLOv3 by using normal illumination images as the source domain and low-illumination images as the target domain and achieves object detection in low-illumination images through an unsupervised learning strategy. Specifically, a multi-scale local feature alignment and global feature alignment module are proposed to align the overall attributes of the image and feature biases such as background, scene, and target layout are thus reduced. The experimental results of LIDA-YOLO on the ExDark dataset achieved the highest performance mAP score of 56.65% compared to several current state-of-the-art unsupervised domain adaptation object detection methods. Compared to I3Net, the performance improvement is 4.04%, and compared to OSHOT, the performance improvement is 6.5%. LIDA-YOLO achieves a performance improvement of 2.7% compared to the supervised baseline method YOLOv3. Overall, the suggested LIDA-YOLO model requires fewer samples and presents a stronger generalization ability than previous works.},
  archive      = {J_IETIP},
  author       = {Yun Xiao and Hai Liao},
  doi          = {10.1049/ipr2.13017},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1178-1188},
  shortjournal = {IET Image Process.},
  title        = {LIDA-YOLO: An unsupervised low-illumination object detection based on domain adaptation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image-based crop row detection utilizing the hough transform
and DBSCAN clustering analysis. <em>IETIP</em>, <em>18</em>(5),
1161–1177. (<a href="https://doi.org/10.1049/ipr2.13016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More accurate methods for crop row detection benefit intelligent operation of agricultural machinery, especially avoiding mishandling or crushing crops. For achieving such a target, a traditional method combining the ExGR exponents, Otsu algorithm, Canny method, Hough transform and DBSCAN clustering analysis is proposed so that centerlines of crop rows can be detected effectively without manual intervention. Specifically, ExGR exponents are first adopted to gray green plants. The threshold of binarization will be further obtained by the Otsu algorithm. Further adopting the edge detection algorithm (Canny), edges of crops can be determined. Finally, combining the Hough transform and DBSCAN clustering analysis, the crop row detection is effectively available. Utilizing these methods, numerical simulation and their comparisons with existing methods are also achieved. For example, the Canny algorithm is relatively accurate than the Suzuki algorithm as well as their combinations with a geometric center extraction method if the density of weed is high. Compared with the K-means clustering method, the DBSCAN algorithm is more suitable to characterize crop rows optimally in more complex conditions. It is validated from experiments that the combination of Canny algorithm, Hough transform and DBSCAN clustering is better than other mentioned traditional methods.},
  archive      = {J_IETIP},
  author       = {Richeng Zhao and Xianju Yuan and Zhanpeng Yang and Lei Zhang},
  doi          = {10.1049/ipr2.13016},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1161-1177},
  shortjournal = {IET Image Process.},
  title        = {Image-based crop row detection utilizing the hough transform and DBSCAN clustering analysis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive edge detection of rebar thread head image based on
improved canny operator. <em>IETIP</em>, <em>18</em>(5), 1145–1160. (<a
href="https://doi.org/10.1049/ipr2.13015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of the rebar thread head is crucial for the connection between the rebars, thereby affecting the public safety of the building. The edge extraction of the thread image is the most important step in obtaining the accurate size parameters, which helps to complete online inspection of quality. Here, an edge detection method based on improved Canny operator is proposed to solve the problems on sensitivity to noise, existence of false edges, and lack of self-adaptability. Firstly, an improved adaptive median filtering algorithm is used to denoise the image. Then replace Sobel with Scharr to enhance the gap between pixel values, add 45° and 135° directional templates to prevent edge information loss as well. Besides, introduce a scale factor to improve the interpolation method for non-maximum suppression and reduce false edges. Finally, use Ostu operator to achieve adaptive extraction of high and low thresholds to complete adaptive edge detection. The qualitative analysis and quantitative results show that the improved algorithm can not only have a good visual effect, have good robustness and feasibility, but also be faster than other algorithms, which provide a basic guarantee for real-time online quality detection of the thread head.},
  archive      = {J_IETIP},
  author       = {Li Liu and Zijin Liu and Aishan Hou and Xuefei Qian and Hui Wang},
  doi          = {10.1049/ipr2.13015},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1145-1160},
  shortjournal = {IET Image Process.},
  title        = {Adaptive edge detection of rebar thread head image based on improved canny operator},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion vector-domain video steganalysis exploiting skipped
macroblocks. <em>IETIP</em>, <em>18</em>(5), 1132–1144. (<a
href="https://doi.org/10.1049/ipr2.13014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video steganography has the potential to be used to convey illegal information, and video steganalysis is a vital tool to detect the presence of this illicit act. Currently, all the motion vector (MV)-based video steganalysis algorithms extract feature sets directly from the MVs, but ignoring the embedding operation may perturb the statistical distribution of other video encoding elements, such as the skipped macroblocks (no direct MVs). This paper proposes a novel 11-dimensional feature set to detect MV-based video steganography based on the above observation. The proposed feature is extracted based on the skipped macroblocks by recompression calibration. Specifically, the feature consists of two components. The first is the probability distribution of motion vector prediction (MVP) difference, and the second is the probability distribution of partition state transfer. Extensive experiments on different conditions demonstrate that the proposed feature set achieves good detection accuracy, especially in lower embedding capacities. In addition, the loss of detection performance caused by recompression calibration using mismatched quantization parameters (QP) is within the acceptable range, so the proposed method can be used in practical scenarios.},
  archive      = {J_IETIP},
  author       = {Jun Li and Minqing Zhang and Ke Niu and Yingnan Zhang and Xiaoyuan Yang},
  doi          = {10.1049/ipr2.13014},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1132-1144},
  shortjournal = {IET Image Process.},
  title        = {Motion vector-domain video steganalysis exploiting skipped macroblocks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based prohibited item detection in x-ray images
during security checking. <em>IETIP</em>, <em>18</em>(5), 1119–1131. (<a
href="https://doi.org/10.1049/ipr2.13013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the intelligent detection of prohibited items in X-ray images during the security checking process. An intelligent semantic segmentation model of prohibited items in X-ray images is proposed based on the attention-based object localization method. Based on the pre-trained CNN classification framework, the attention mechanism can map the high-layer semantic information of objects into the input space, while generating energy saliency maps to locate the prohibited items. In order to make the obtained attention maps discriminative, the lateral and contrastive inhibition strategies are introduced and combined together which can highlight the responses of activated neurons. Under the guidance of attention responses, two traditional image segmentation algorithms are employed to achieve the semantic segmentation results for the prohibited items detection in X-ray images. The proposed semantic segmentation model relies on weakly supervised learning mechanism, and only depends on the category labels of prohibited items, which greatly avoids the work cost of data semantic annotation. The experimental results based on the public SIXray baseline and the self-built X-ray image database demonstrate the proposed method can achieve about 65% IoU localization precise averagely. In addition, comparison experiments were carried out with the state-of-the-arts and ablation experiments to verify the effectiveness of the proposed model.},
  archive      = {J_IETIP},
  author       = {Haigang Zhang and Zihao Zhao and Jinfeng Yang},
  doi          = {10.1049/ipr2.13013},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1119-1131},
  shortjournal = {IET Image Process.},
  title        = {Attention-based prohibited item detection in X-ray images during security checking},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing chest tuberculosis image classification with
oversampling and transfer learning. <em>IETIP</em>, <em>18</em>(5),
1109–1118. (<a href="https://doi.org/10.1049/ipr2.13010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuberculosis (TB) is an extremely contagious illness caused by Mycobacterium tuberculosis . Chest tuberculosis classification is conducted based on a deep convolutional neural network architecture. In this research, a pre-trained network is utilized to demonstrate the advantage of using the oversampling technique on the classification of TB and compare results with recent research that used the same dataset. Therefore, the dataset consists of 3500 uninfected TB cases and 700 infected with TB. This paper circumvents the imbalance by using the oversampling technique in X-ray TB images to be fed into several pre-trained networks for TB classification. The oversampling technique is crucial in enhancing the performance of TB classification compared with other pre-trained models reported here. Inceptionv3 shows a promising result compared to other pre-trained models; it achieves 99.94% accuracy, 99.88% precision, 100% recall, and 99.94% F1-Score.},
  archive      = {J_IETIP},
  author       = {Ali Alqahtani and Qasem Abu Al-Haija and Abdulaziz A. Alsulami and Badraddin Alturki and Nayef Alqahtani and Raed Alsini},
  doi          = {10.1049/ipr2.13010},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1109-1118},
  shortjournal = {IET Image Process.},
  title        = {Optimizing chest tuberculosis image classification with oversampling and transfer learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DGW-YOLOv8: A small insulator target detection algorithm
based on deformable attention backbone and WIoU loss function.
<em>IETIP</em>, <em>18</em>(4), 1096–1108. (<a
href="https://doi.org/10.1049/ipr2.13009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The YOLO series of algorithms have made substantial contributions to the detection of insulator defects in power transmission line operations. However, existing target detection algorithms for the small target detection and low-quality insulator images encounter difficulties in effectively capturing relevant features, resulting in a higher probability of target loss. To identify and classify defects in the operational state of insulators, an improved YOLOv8 target identification algorithm called DGW-YOLOv8 is proposed in this paper. The deformable attention backbone of the DGW-YOLOv8 target identification algorithm is designed by adding the deformable ConvNets v2 module and the global attention mechanism. This addition reduces the feature loss caused by the network feature processing, enhances the sensitivity of the algorithm to small-scale targets, and reduces the impact caused by the different global positions of the targets. Additionally, to address the problem of low quality of captured images, WIoU v3 is used to replace CIoU in the original YOLOv8 target identification algorithm to optimize the loss function, reduce the degrees of freedom, and improve the network robustness. Experimental results demonstrate that the enhanced YOLOv8 algorithm can achieve an improvement of 2.4% and 5.5% in mAP and mAP50-95, respectively, compared with the original algorithm.},
  archive      = {J_IETIP},
  author       = {Deao Hu and Mei Yu and Xianyong Wu and Jingbo Hu and Yuyang Sheng and Yanjing Jiang and Chongjing Huang and Yuelin Zheng},
  doi          = {10.1049/ipr2.13009},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1096-1108},
  shortjournal = {IET Image Process.},
  title        = {DGW-YOLOv8: A small insulator target detection algorithm based on deformable attention backbone and WIoU loss function},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Cross-modal fusion encoder via graph neural network for
referring image segmentation. <em>IETIP</em>, <em>18</em>(4), 1083–1095.
(<a href="https://doi.org/10.1049/ipr2.13008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring image segmentation identifies the object masks from images with the guidance of input natural language expressions. Nowadays, many remarkable cross-modal decoder are devoted to this task. But there are mainly two key challenges in these models. One is that these models usually lack to extract fine-grained boundary information and gradient information of images. The other is that these models usually lack to explore language associations among image pixels. In this work, a Multi-scale Gradient balanced Central Difference Convolution (MG-CDC) and a Graph convolutional network-based Language and Image Fusion (GLIF) for cross-modal encoder, called Graph-RefSeg, are designed. Specifically, in the shallow layer of the encoder, the MG-CDC captures comprehensive fine-grained image features. It could enhance the perception of target boundaries and provide effective guidance for deeper encoding layers. In each encoder layer, the GLIF is used for cross-modal fusion. It could explore the correlation of every pixel and its corresponding language vectors by a graph neural network. Since the encoder achieves robust cross-modal alignment and context mining, a light-weight decoder could be used for segmentation prediction. Extensive experiments show that the proposed Graph-RefSeg outperforms the state-of-the-art methods on three public datasets. Code and models will be made publicly available at https://github.com/ZYQ111/Graph_refseg .},
  archive      = {J_IETIP},
  author       = {Yuqing Zhang and Yong Zhang and Xinglin Piao and Peng Yuan and Yongli Hu and Baocai Yin},
  doi          = {10.1049/ipr2.13008},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1083-1095},
  shortjournal = {IET Image Process.},
  title        = {Cross-modal fusion encoder via graph neural network for referring image segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective fusion module with dilation convolution for
monocular panoramic depth estimate. <em>IETIP</em>, <em>18</em>(4),
1073–1082. (<a href="https://doi.org/10.1049/ipr2.13007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation from monocular panoramic image is a crucial step in 3D reconstruction, which is a close relationship with virtual reality and metaverse technologies. In recent years, some methods, such as HRDFuse, BiFuse++, and UniFuse, have employed a two-branch neural network leveraging two common projections: equirectangular and cubemap projections (CMPs). The equirectangular projection (ERP) provides a complete field of view but introduces distortion, while the CMP avoids distortion but introduces discontinuity at the boundary of the cube. In order to address the issue of distortion and discontinuity, the authors propose an efficient depth estimation fusion module to balance the feature mapping of the two projections. Moreover, for the ERP, the authors propose a novel inflated network architecture to extend the receptive field and effectively harness visual information. Extensive experiments show that the authors’ method predicts more clear boundaries and accurate depth results while outperforming mainstream panoramic depth estimation algorithms.},
  archive      = {J_IETIP},
  author       = {Cheng Han and Yongqing Cai and Xinpeng Pan and Ziyun Wang},
  doi          = {10.1049/ipr2.13007},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1073-1082},
  shortjournal = {IET Image Process.},
  title        = {Effective fusion module with dilation convolution for monocular panoramic depth estimate},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LWE-based verifiable essential secret image sharing scheme
((t,s,k,n) - VESIS). <em>IETIP</em>, <em>18</em>(4), 1053–1072. (<a
href="https://doi.org/10.1049/ipr2.13006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional secret image sharing schemes, shareholders have similar situations, and to recover the secret image, they must be present as many as the threshold. However, in scenarios where the shareholders occupy different positions, essential and non-essential, the use of essential secret image sharing schemes becomes necessary. This article presents a novel verifiable essential secret image sharing scheme based on LWEs that incorporates Bloom filters and hash functions for verification purposes. The proposed scheme for a secret image does not require any pre-processing. The shadow images of both essential and non-essential shareholders are of the same size, eliminating the need for concatenating sub-shadows. Furthermore, shadow images for essential and non-essential shareholders are produced simultaneously in a single step. The scheme is also quantum safe thanks to the use of quantum-resistant primitives. The experimental results confirm the security and efficiency of the proposed scheme.},
  archive      = {J_IETIP},
  author       = {Massoud Hadian Dehkordi and Seyed Taghi Farahi and Samaneh Mashhadi},
  doi          = {10.1049/ipr2.13006},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1053-1072},
  shortjournal = {IET Image Process.},
  title        = {LWE-based verifiable essential secret image sharing scheme ((t,s,k,n) - VESIS)},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An image inpainting method based on generative adversarial
networks inversion and autoencoder. <em>IETIP</em>, <em>18</em>(4),
1042–1052. (<a href="https://doi.org/10.1049/ipr2.13005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting aims to repair the damaged region according to the known content in the damaged image. Recently, image inpainting methods have poor effects on high-resolution damaged images, and the research on the inpainting of large-area damaged images is limited. Therefore, this paper proposes an image inpainting method based on Generative Adversarial Networks (GAN) inversion and autoencoder. This work consists of two phases: first, the authors design an autoencoder-based GAN, which learns the mapping from noise to low-dimensional feature maps by training a generator, and then converts the generated feature maps into high-resolution images. Thus, the difficulty of learning the mapping relationship is reduced. Second, the authors adopt the learning-based GAN inversion to infer the closest latent code. The trained GAN is then used to reconstruct the complete image. Finally, the authors compare their method with other classical methods on the CelebAMask-HQ, Flickr-Faces-HQ, and ImageNet datasets. According to the quantitative comparison, when the mask range is large, in other words, when the image has a large area of damage, the authors’ method is superior to the comparison methods. According to the qualitative comparison, the structure of the high-resolution image inpainted by the authors’ method is more reasonable and the texture details are more realistic.},
  archive      = {J_IETIP},
  author       = {Yechen Wang and Bin Song and Zhiyong Zhang},
  doi          = {10.1049/ipr2.13005},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1042-1052},
  shortjournal = {IET Image Process.},
  title        = {An image inpainting method based on generative adversarial networks inversion and autoencoder},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Forest nighttime image enhancement based on improved SCI
light estimation and reflection optimization. <em>IETIP</em>,
<em>18</em>(4), 1028–1041. (<a
href="https://doi.org/10.1049/ipr2.13004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The images captured by forest nighttime imaging devices are prone to degradation and noise issues due to environmental constraints. This paper proposes a novel framework based on the self-calibrating illumination framework to improve image quality. The framework consists of four specific steps: (1) addressing non-uniform degradation by estimating illuminance components through non-uniform MaxRGB, (2) extracting rich information using multi-scale inflationary convolution, (3) analyzing the relationship between different input levels and the initial level and progressively adjusting the luminance components using a channel self-calibration module to achieve continuous improvement in luminance, and (4) designing a reflectance optimization module that enhances illuminance while suppressing noise through a regularization model. To evaluate the proposed method, a dataset comprising 1000 forest night images is constructed using an image simulation strategy. The effectiveness of this method was validated on both synthetic and real datasets, demonstrating its ability to significantly enhance nighttime forest images. When compared to six other algorithms, this method outperformed them in eight evaluation metrics, including a peak signal-to-noise ratio metric of 22.839 and a structural similarity metric of 0.909. The experimental results clearly indicate that this method excels in enhancing image quality and fidelity.},
  archive      = {J_IETIP},
  author       = {Xian Zhang and Yanfeng Li and Hanyue Zhang},
  doi          = {10.1049/ipr2.13004},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1028-1041},
  shortjournal = {IET Image Process.},
  title        = {Forest nighttime image enhancement based on improved SCI light estimation and reflection optimization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single image dehazing via decomposition and enhancement.
<em>IETIP</em>, <em>18</em>(4), 1014–1027. (<a
href="https://doi.org/10.1049/ipr2.13003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazy images suffer from two problems. The low contrast can be enhanced by estimating a transmission layer, and the colour cast can be restored by estimating an airlight. These two variables, together with the albedo layer, are the constitutive elements of a hazy image. The resulting quality of dehazed images is inextricably linked to the accurate estimation of these components. However, it is ill-posed to decompose these variables from a single image. As such, this paper presents an innovative algorithm intended to facilitate the optimal decomposition of a hazy image. Using the Markov random field model, an optimal framework is established that allows the simultaneous estimation of the three components across the three-colour channels. To improve the visual quality, three improvements are proposed in the variational solution for the optimal components. The dehazed result is recomposed from the components with the transmission enhanced to circumvent any potential artefacts or information loss. Extensive experiments on natural images corroborate that the proposed algorithm outperforms state-of-the-art dehazing methods, both qualitatively and quantitatively.},
  archive      = {J_IETIP},
  author       = {Bo Gu and Haohan Yao and Yanjun Sun and Zhonghang Duan},
  doi          = {10.1049/ipr2.13003},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1014-1027},
  shortjournal = {IET Image Process.},
  title        = {Single image dehazing via decomposition and enhancement},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparative study on deep feature selection methods for
skin lesion classification. <em>IETIP</em>, <em>18</em>(4), 996–1013.
(<a href="https://doi.org/10.1049/ipr2.13002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Melanoma, a widespread and hazardous form of cancer, has prompted researchers to prioritize dermoscopic image-based algorithms for classifying skin lesions. Recently, there has been a growing trend in using pre-trained convolutional neural networks for detecting skin lesions. However, the features extracted from these classifiers may include irrelevant elements, emphasizing the importance of implementing effective feature selection methods. Nevertheless, there has not been a comprehensive study on feature selection methods to enhance the performance of skin lesion detection to date. To identify the most efficient methods, a diverse set of feature selection techniques, including filter, wrapper, embedded, and dimensionality reduction, were applied to images from two well-known datasets, namely ISIC 2017 and ISIC 2018. According to the results, models trained with features chosen by wrapper techniques outperformed those trained with features chosen by filter and embedded methods. Achieving an accuracy of 0.8333 and an F1-Score of 0.8291 for the ISIC 2017 dataset, and an F1-Score of 0.9324 and an accuracy of 0.9350 for the ISIC 2018 dataset, the classification using features obtained via the GWO feature selection technique performed the best.},
  archive      = {J_IETIP},
  author       = {Farzad Golnoori and Farsad Zamani Boroujeni and Seyed Amirhassan Monadjemi},
  doi          = {10.1049/ipr2.13002},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {996-1013},
  shortjournal = {IET Image Process.},
  title        = {A comparative study on deep feature selection methods for skin lesion classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning adaptive motion search for fast versatile video
coding in visual surveillance systems. <em>IETIP</em>, <em>18</em>(4),
981–995. (<a href="https://doi.org/10.1049/ipr2.13001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual surveillance systems have been playing an important role in monitoring and managing at public areas. However, the computational complexity of video compression in these applications is still a great challenge. To meet practical requirements, the authors propose in this paper a low-complexity surveillance video coding solution in which the most recent Versatile Video Coding (VVC) standard is improved with a novel learning adaptive motion search algorithm. The proposed algorithm is designed based on the temporal motion and spatial texture characteristics of surveillance videos. First, the authors study and define a list of spatial and temporal features which indicates the motion and texture characteristics of surveillance video. These features are used together with a machine learning algorithm to appropriately assign a search range for the VVC motion search. Second, to reduce search points, the authors propose an adaptive Test Zone (TZ) search in which TZ steps are early terminated following the variation of spatial–temporal features. Performance evaluation conducted for a rich set of surveillance videos and relevant benchmarks have shown the superiority of the proposed method, notably with around 33% of encoding time saving when compared with the state-of-the art VVC solution and relevant benchmarks while asking for negligible compression loss.},
  archive      = {J_IETIP},
  author       = {Huong Bui Thanh and Sang Nguyen Quang and Tien Vu Huu and Xiem HoangVan},
  doi          = {10.1049/ipr2.13001},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {981-995},
  shortjournal = {IET Image Process.},
  title        = {Learning adaptive motion search for fast versatile video coding in visual surveillance systems},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-granularity feature fusion in visible-infrared person
re-identification. <em>IETIP</em>, <em>18</em>(4), 972–980. (<a
href="https://doi.org/10.1049/ipr2.12999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) aims to recognize images of the same person captured in different modalities. Existing methods mainly focus on learning single-granularity representations, which have limited discriminability and weak robustness. This paper proposes a novel dual-granularity feature fusion network for VI-ReID. Specifically, a dual-branch module that extracts global and local features and then fuses them to enhance the representative ability is adopted. Furthermore, an identity-aware modal discrepancy loss that promotes modality alignment by reducing the gap between features from visible and infrared modalities is proposed. Finally, considering the influence of non-discriminative information in the modal-shared features of RGB-IR, a greyscale conversion is introduced to extract modality-irrelevant discriminative features better. Extensive experiments on the SYSU-MM01 and RegDB datasets demonstrate the effectiveness of the framework and superiority over state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Shuang Cai and Shanmin Yang and Jing Hu and Xi Wu},
  doi          = {10.1049/ipr2.12999},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {972-980},
  shortjournal = {IET Image Process.},
  title        = {Dual-granularity feature fusion in visible-infrared person re-identification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An inverse halftoning method based on supervised deep
convolutional neural network. <em>IETIP</em>, <em>18</em>(4), 961–971.
(<a href="https://doi.org/10.1049/ipr2.12998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse halftoning is a technology that converts a binary image into a continuous tone image. Due to the wide application of inverse halftoning, many scholars have proposed several deep convolutional neural networks (DCNN) to optimize their performance. According to the observation, there is still room for improvement in content generation and detail recovery of the inverse halftone images generated by using the existing methods. Therefore, an inverse halftoning method based on supervised DCNN is proposed in this paper. The method consists of two parts: the multi-level feature extraction model uses the down-sampling to extract the features from the halftone image and remove the halftone noise dots on flat areas, which is implemented by four convolutional layers; the image reconstruction model uses up-sampling to reconstruct image information, which is realized by four convolutional layers and two dense residual blocks. At the same time, in order to further recover the details, the down-sampling feature maps and up-sampling feature maps of the same size are concatenated by addition layers. Experimental results show that compared with other methods, the inverse halftone images obtained by the proposed network have better results in both subjective and objective evaluations.},
  archive      = {J_IETIP},
  author       = {Mei Li and Qi Liu},
  doi          = {10.1049/ipr2.12998},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {961-971},
  shortjournal = {IET Image Process.},
  title        = {An inverse halftoning method based on supervised deep convolutional neural network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning filter selection policies for interpretable image
denoising in parametrised action space. <em>IETIP</em>, <em>18</em>(4),
951–960. (<a href="https://doi.org/10.1049/ipr2.12997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The denoising of images is an important research direction in computer vision. We consider the image denoising task as an estimation problem of the filtering policy related to image features, which is different from end-to-end image mapping. Commonly used simple filters such as gaussian filtering and bilateral filtering have fixed global denoising policies. However, the denoising policies of different filters can only adapt to limited image features. To solve this problem, we propose a method that applies different filters to different spatial ranges and adjusts the parameters of these filters simultaneously. Since not all filters can be easily transformed into differentiable forms and it is difficult to obtain paired datasets of filter action areas, we use reinforcement learning (RL) methods to estimate the spatial domain action range and adjustable parameters of filters, respectively. Furthermore, for removing higher intensity noise, simple filters can iteratively approximate higher-order denoising policies and obtain more accurate and stable denoising results with the increase of iteration steps. Experimental results show that our proposed method can not only generate intuitive and interpretable denoising policies but also achieve comparable or better visual effects and computational efficiency than baseline methods.},
  archive      = {J_IETIP},
  author       = {Runtao Xi and Jiahao Lyu and Tian Ma and Kang Sun and Yu Zhang and XiaoLin Chen},
  doi          = {10.1049/ipr2.12997},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {951-960},
  shortjournal = {IET Image Process.},
  title        = {Learning filter selection policies for interpretable image denoising in parametrised action space},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Segmentation of nucleus based on dynamic convolution and
deep features of stain distribution. <em>IETIP</em>, <em>18</em>(4),
939–950. (<a href="https://doi.org/10.1049/ipr2.12996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of the pathology image is important to diagnose cancer of lung, breast and stomach. Segmenting the nucleus is a key step for quantitative analysis, and has significance to the pathology researches and computer aided diagnosis systems. The inconsistency of colour, fuzzy boundary of nucleus and overlapping of cells are the universally acknowledged challenges. To solve these problems, the difference between the inside and outside of nucleus is enhanced by obtaining the distribution of the haematoxylin based on Lambert–Beer&#39;s law and the optical characteristics of stains. An inferior encoder, which is supervised by the inferior decoder, is proposed to extract the deep features of the distribution of stains. And these features are fed into the primary encoder to improve the accuracy of segmentation. To relieve the problem that some nuclei are segmented as background because the deep feature is inapparent, dynamic convolution is introduced into the encoders. The experiments show that the proposed model can segment the nucleus in the pathological images more precisely than the compared models. The Dice similarity coefficient (DSC) and panoptic quality (PQ) are 0.810 and 0.512, respectively.},
  archive      = {J_IETIP},
  author       = {Haoyang Zhou and Bao Feng and Hongbo Chen},
  doi          = {10.1049/ipr2.12996},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {939-950},
  shortjournal = {IET Image Process.},
  title        = {Segmentation of nucleus based on dynamic convolution and deep features of stain distribution},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-route synthetic-to-real adaption for single image
dehazing. <em>IETIP</em>, <em>18</em>(4), 926–938. (<a
href="https://doi.org/10.1049/ipr2.12995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image dehazing in real scenarios is still a particularly challenging task due to the large domain discrepancy between synthetic and real hazy images. To address this problem, the dual-route domain-aware adaptation framework with the bridging domain is proposed. First, inspired by the effective multistage strategy in low-level tasks, the bridging domain is constructed with the color-preserved adaptive histogram equalization to facilitate the adaptation and improve the performance of real hazy images. Second, the totally shared structure is relaxed and the residual dual-path domain-aware modules (RDDM) for synthetic and the bridging domains are proposed, which facilitates extracting the domain-specific haze information with the different parameters. Third, a half-cyclic constraint is proposed for the unsupervised hazy images to avoid structure distortion during the unsupervised adversarial training process. Finally, for convenience in the inference stage, feature enhancement modules (FEM) are proposed for the original real hazy images to learn the pre-process operation in the training stage. Extensive qualitative and quantitative experiments demonstrate that the proposed method significantly improves the dehazing performance on synthetic and real hazy images.},
  archive      = {J_IETIP},
  author       = {Yingxu Qiao and Zhanqiang Huo and Sensen Meng},
  doi          = {10.1049/ipr2.12995},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {926-938},
  shortjournal = {IET Image Process.},
  title        = {Dual-route synthetic-to-real adaption for single image dehazing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improvement of continuous emotion recognition of temporal
convolutional networks with incomplete labels. <em>IETIP</em>,
<em>18</em>(4), 914–925. (<a
href="https://doi.org/10.1049/ipr2.12994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based emotion recognition has been a long-standing research topic for computer scientists and psychiatrists. In contrast to traditional discrete emotional models, emotion recognition based on continuous emotional models can better describe the progression of emotions. Quantitative analysis of emotions will have crucial impacts on promoting the development of intelligent products. The current solutions to continuous emotion recognition still have many issues. The original continuous emotion dataset contains incomplete data annotations, and the existing methods often ignore temporal information between frames. The following measures are taken in response to the above problems. Initially, aiming at the problem of incomplete video labels, the correlation between discrete and continuous video emotion labels is used to complete the dataset labels. This correlation is used to propose a mathematical model to fill the missing labels of the original dataset without adding data. Moreover, this paper proposes a continuous emotion recognition network based on an optimized temporal convolutional network, which adds a feature extraction submodule and a residual module to retain shallow features while improving the feature extraction ability. Finally, validation experiments on the Aff-wild2 dataset achieved accuracies of 0.5159 and 0.65611 on the valence and arousal dimensions, respectively, by adopting the above measures.},
  archive      = {J_IETIP},
  author       = {Zheyu Wang and Jieying Zheng and Feng Liu},
  doi          = {10.1049/ipr2.12994},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {914-925},
  shortjournal = {IET Image Process.},
  title        = {Improvement of continuous emotion recognition of temporal convolutional networks with incomplete labels},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feasibility analysis of image approximation with image
quality assessments. <em>IETIP</em>, <em>18</em>(4), 897–913. (<a
href="https://doi.org/10.1049/ipr2.12993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With technological developments, the resolution of the display systems is increasing, causing image data to increase. Several technologies are aiding in optimizing these data during transmission and storage while maintaining quality. Still, the internal data transmission for accessing the memory sub-system during reading and writing cycles consumes significant power while requiring large memory for storage, which is inefficient, especially for portable displays. Hence, this work investigates the feasibility of reducing data through approximation for images in different formats and resolutions for human and computer vision based applications. Different image quality assessment metrics are utilized for performance evaluations with optimum image quality assessment selection according to application requirements. In addition, the feasibility analysis is conducted for three different applications as examples of computer vision. The experimental analysis highlights theoretical and industrial importance by showing that image data can be reduced from 25.0% to 62.5%, while satisfying requirements for HVS-based and computer vision-based applications.},
  archive      = {J_IETIP},
  author       = {Md. Abdur Rahman and Aminul Huq and Salma Sultana Tunny and Maruf Hossain Anik and Mst Tasnim Pervin and Md. Rashidul Islam},
  doi          = {10.1049/ipr2.12993},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {897-913},
  shortjournal = {IET Image Process.},
  title        = {A feasibility analysis of image approximation with image quality assessments},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain tumour segmentation of MR images based on custom
attention mechanism with transfer-learning. <em>IETIP</em>,
<em>18</em>(4), 886–896. (<a
href="https://doi.org/10.1049/ipr2.12992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic segmentation of brain tumours is a critical task in patient disease management. It can help specialists easily identify the location, size, and type of tumour to make the best decisions regarding the patients&#39; treatment process. Recently, deep learning methods with attention mechanism helped increase the performance of segmentation models. The proposed method consists of two main parts: the first part leverages a deep neural network architecture for biggest tumour detection (BTD) and in the second part, ResNet152V2 makes it possible to segment the image with the attention block and the extraction of local and global features. The custom attention block is used to consider the most important parts in the slices, emphasizing on related information for segmentation. The results show that the proposed method achieves average Dice scores of 0.81, 0.87 and 0.91 for enhancing core, tumour core and whole tumour on BraTS2020 dataset, respectively. Compared with other segmentation approaches, this method achieves better performance on tumour core and whole tumour. Further comparisons on BraTS2018 and BraTS2017 validation datasets show that this method outperforms other models based on Dice score and Hausdorff criterion.},
  archive      = {J_IETIP},
  author       = {Marjan Vatanpour and Javad Haddadnia},
  doi          = {10.1049/ipr2.12992},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {886-896},
  shortjournal = {IET Image Process.},
  title        = {Brain tumour segmentation of MR images based on custom attention mechanism with transfer-learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSDet: A new method for traffic sign detection based on
YOLOv5-SwinT. <em>IETIP</em>, <em>18</em>(4), 875–885. (<a
href="https://doi.org/10.1049/ipr2.12991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real scenarios, accurate and real-time detection of traffic signs is of great significance to the automatic driving system. To meet the requirements of detection accuracy and speed, a new traffic sign detection method based on YOLOv5 and Swin-Transformer is proposed in this paper. First, based on the traditional Focus structure, a lightweight shallow information enhancement module is designed. Second, to enhance the channel weights of useful information, an adjustable channel attention mechanism is proposed. Additionally, a Cross Stage Partial module based on Swin-Transformer is designed to capture contextual information around traffic signs, thereby improving the detection accuracy of small-scale traffic signs. Finally, to better fuse deep semantic features and shallow detail features, an adaptive feature fusion method is proposed. To verify the superiority of the proposed method, experimental verification was carried out on TT100K and DFG traffic sign detection datasets, and their mAP, AP 50 and FPS were (75.3, 94.8, 82) and (79.7, 85.9, 118), respectively. The experimental results show that the proposed traffic sign detection method has high accuracy and real-time performance, and can meet the needs of traffic sign detection in the actual scene.},
  archive      = {J_IETIP},
  author       = {Yue Jing Qian and Bo Wang},
  doi          = {10.1049/ipr2.12991},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {875-885},
  shortjournal = {IET Image Process.},
  title        = {TSDet: A new method for traffic sign detection based on YOLOv5-SwinT},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MEHW-SVM multi-kernel approach for improved brain tumour
classification. <em>IETIP</em>, <em>18</em>(4), 856–874. (<a
href="https://doi.org/10.1049/ipr2.12990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain, the primary constituent of the nervous system, exhibits distinctive complexities that present considerable difficulties for healthcare practitioners, specifically in categorizing brain tumours. Magnetic resonance imaging is a widely favoured imaging modality for detecting brain tumours due to its extensive range of image characteristics and utilization of non-ionizing radiation. The primary objective of the current investigation is to differentiate between three distinct classifications of brain tumours by introducing a novel methodology. The utilization of a combined feature extraction technique that integrates novel global grey level co-occurrence matrix and local binary patterns is employed, thereby offering a comprehensive representation of the structural and textural information contained within the images. Principal component analysis is used to improve the model&#39;s efficiency for effective feature selection and dimensionality reduction. This study presents a novel framework incorporating four separate kernel functions, Minkowski–Gaussian, exponential support vector machine (SVM), histogram intersection SVM, and wavelet kernel, into a SVM classifier. The ensemble kernel employed in this study is specifically designed to classify glioma, meningioma, and pituitary tumours. Its implementation enhances the model&#39;s robustness and adaptability, surpassing the performance of conventional single-kernel SVM approaches. This study substantially contributes to medical image classification by utilizing innovative kernel functions and advanced machine-learning techniques. The findings demonstrate the potential for enhanced diagnostic accuracy in brain tumour cases. The presented approach shows promise in effectively addressing the intricate challenges associated with classifying brain tumours.},
  archive      = {J_IETIP},
  author       = {G. Dheepak and J. Anita Christaline and D. Vaishali},
  doi          = {10.1049/ipr2.12990},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {856-874},
  shortjournal = {IET Image Process.},
  title        = {MEHW-SVM multi-kernel approach for improved brain tumour classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSCAReg-net: Multi-scale complexity-aware convolutional
neural network for deformable image registration. <em>IETIP</em>,
<em>18</em>(4), 839–855. (<a
href="https://doi.org/10.1049/ipr2.12988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based image registration (DLIR) has been widely developed, but it remains challenging in perceiving small and large deformations. Besides, the effectiveness of the DLIR methods was also rarely validated on the downstream tasks. In the study, a multi-scale complexity-aware registration network (MSCAReg-Net) was proposed by devising a complexity-aware technique to facilitate DLIR under a single-resolution framework. Specifically, the complexity-aware technique devised a multi-scale complexity-aware module (MSCA-Module) to perceive deformations with distinct complexities, and employed a feature calibration module (FC-Module) and a feature aggregation module (FA-Module) to facilitate the MSCA-Module by generating more distinguishable deformation features. Experimental results demonstrated the superiority of the proposed MSCAReg-Net over the existing methods in terms of registration accuracy. Besides, other than the indices of Dice similarity coefficient (DSC) and percentage of voxels with non-positive Jacobian determinant ( ), a comprehensive evaluation of the registration performance was performed by applying this method on a downstream task of multi-atlas hippocampus segmentation (MAHS). Experimental results demonstrated that this method contributed to a better hippocampus segmentation over other DLIR methods, and a comparable segmentation performance with the leading SyN method. The comprehensive assessment including DSC, and the downstream application on MAHS demonstrated the advances of this method.},
  archive      = {J_IETIP},
  author       = {Hu Yu and Qiang Zheng and Fang Hu and Chaoqing Ma and Shuo Wang and Shuai Wang},
  doi          = {10.1049/ipr2.12988},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {839-855},
  shortjournal = {IET Image Process.},
  title        = {MSCAReg-net: Multi-scale complexity-aware convolutional neural network for deformable image registration},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive image compression algorithm based on joint
clustering algorithm and deep learning. <em>IETIP</em>, <em>18</em>(3),
829–837. (<a href="https://doi.org/10.1049/ipr2.13021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep artificial neural networks have attracted much attention and have been applied in various fields because they surpass the parameter fitting effect of traditional methods under the condition of data convergence. On the other hand, limited transmission bandwidth and storage capacity make image compression necessary in communication. Here, a compression algorithm that combines the K-means clustering algorithm with the neural network algorithm is proposed. First, the pixel points of the image are clustered by K-means algorithm in order to reduce the amount of data input to the neural network algorithm. Secondly, neural network is used to extract image features which realizes further compression. The experiment results show that the peak signal-to-noise ratio (PSNR) is 33.48 dB at most with compression ratio at 32:1. The ablation experiment shows that the run time speeds up 9.5% compared to the algorithm without K-means clustering. Comprehensive comparison experiment shows that the average PSNR is 30.09 dB, which is larger than other baseline approaches. The proposed algorithm is an efficient solution for image compression.},
  archive      = {J_IETIP},
  author       = {Yanxia Liang and Meng Zhao and Xin Liu and Jing Jiang and Guangyue Lu and Tong Jia},
  doi          = {10.1049/ipr2.13021},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {829-837},
  shortjournal = {IET Image Process.},
  title        = {An adaptive image compression algorithm based on joint clustering algorithm and deep learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image preprocessing-based ensemble deep learning
classification of diabetic retinopathy. <em>IETIP</em>, <em>18</em>(3),
807–828. (<a href="https://doi.org/10.1049/ipr2.12987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) can cause irreversible eye damage, even blindness. The prognosis improves with early diagnosis. According to the International Classification of Diabetic Retinopathy Severity Scale (ICDRSS), DR has five stages. Modern, cost-effective techniques for automatic DR screening and staging of fundus images are based on deep learning (DL). To obtain higher classification accuracy, the combination of several diverse individual DL models into one ensemble could be used. A new approach to model diversity in an ensemble is proposed by manipulating the training input data involving original and four variants of preprocessed image datasets. There are publicly available datasets with labels for all five stages, but some contain poor-quality images. In contrast, this algorithm was trained on images from a six-class DDR dataset, including the class of poor-quality ungradable images, to enhance the classification performance. The solution was evaluated on the APTOS dataset, containing only ICDRSS classes. Classification results of the ensemble model were presented on two different ensemble convolutional neural network (CNN) models, based on Xception and EfficientNetB4 architectures using two fusion approaches. Our proposed ensemble models outperformed all other single deep learning architectures regarding overall accuracy and Cohen&#39;s Kappa, with the best results using the EfficientNetB4 architecture.},
  archive      = {J_IETIP},
  author       = {Peter Macsik and Jarmila Pavlovicova and Slavomir Kajan and Jozef Goga and Veronika Kurilova},
  doi          = {10.1049/ipr2.12987},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {807-828},
  shortjournal = {IET Image Process.},
  title        = {Image preprocessing-based ensemble deep learning classification of diabetic retinopathy},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A DTI registration algorithm based on spectral
representation. <em>IETIP</em>, <em>18</em>(3), 796–806. (<a
href="https://doi.org/10.1049/ipr2.12986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion Tensor Imaging (DTI) is a new technique to characterize the brain structure. It is a special modality for Magnetic Resonance (MRI) which provides the information of fiber orientation. By quantitatively comparing the difference of the fiber information of different image pairs, one can effectively track the status of the nerve diseases. For this purpose, it is necessary to deform the DTI image of the patient (floating image) to the same shape of the template image (target image). This is so called DTI registration, which is usually formulated by a 3D variational problem. However, the numerical implementation for 3D variational problem costs too much CPU consumption. To reduce CPU consumption, it is meaningful to give some research on the structure of the solution for DTI registration. In this paper, the structure of the solution for DTI registration is mainly focused on. For this purpose, an explicit representation of the solution for a DTI registration model is presented. Based on this representation, a new DTI registration algorithm is induced. Several numerical experiments are also performed to show the efficiency of the proposed algorithm in DTI registration.},
  archive      = {J_IETIP},
  author       = {Ning Xu and Zimo Zhou and Hao Cai},
  doi          = {10.1049/ipr2.12986},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {796-806},
  shortjournal = {IET Image Process.},
  title        = {A DTI registration algorithm based on spectral representation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic alignment and illumination-aware convolution for
shadow removal. <em>IETIP</em>, <em>18</em>(3), 785–795. (<a
href="https://doi.org/10.1049/ipr2.12985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow removal is a challenging task because the variety of shadows is influenced by surface texture and lighting. This paper proposes a dynamic alignment and illumination-aware convolution (DAIC), which consists of a Feature Alignment Module (FAM) and a Dynamic Weight Module (DWM). FAM aligns the downsampled deep features with the original features and helps to extract the optimal local information to ensure that the object texture features are not corrupted. DWM generates weights according to different lighting variations for a better shadow removal result. The shadow removal approach is based on an image decomposition algorithm using a multi-exposure image fusion model. Here, the shadow removal network and refinement network use U-Net framework, and the transposed convolution operations are replaced with DAIC in the decoder part of U-Net to improve the performance of the network. The experiments are conducted on two large shadow removal datasets, ISTD+ and SRD. Compared to state-of-the-art methods, this model achieves optimal performance in terms of Root Mean Square Error (RMSE) for the non-shadow region. It also achieves performance comparable to the state-of-the-art method in terms of RMSE for the shadow region and structural similarity index measurement for the entire image.},
  archive      = {J_IETIP},
  author       = {Xingqi Wang and Jialai Dai and Bin Chen and Dan Wei and Yanli Shao},
  doi          = {10.1049/ipr2.12985},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {785-795},
  shortjournal = {IET Image Process.},
  title        = {A dynamic alignment and illumination-aware convolution for shadow removal},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A weakly supervised learning pipeline for profiled fibre
inspection. <em>IETIP</em>, <em>18</em>(3), 772–784. (<a
href="https://doi.org/10.1049/ipr2.12984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic profiled fibre recognition and analysis can accelerate quality inspection and contributes to the upgrade of the textile industry. However, these tasks often require significant manual effort to generate instance-level annotations for fully supervised training. In this paper, the authors propose a weakly supervised pipeline for profiled fibre inspection using electron-microscopic (EM) images with only image-level annotations. It automatically identifies fibre instances and estimates shape factors to facilitate fibre quality inspection. As the core of the pipeline, the weakly supervised network (WesNet) is designed to localize hundreds of crowded fibre samples by raw patch generation and fibre sample sifting. Particularly, the composite similarity measurement integrates different patch-wise similarities, enabling the network to distinguish fibre from background robustly. For quality inspection, the pipeline further analyzes the fibre instances, utilizing several efficient techniques to estimate the shape factors. Experiments on the real fibre electron-microscopic images demonstrate the efficacy and efficiency of the pipeline. Results show that WesNet outperforms several supervised and weakly supervised methods, including two state-of-the-art weakly supervised networks.},
  archive      = {J_IETIP},
  author       = {Zhao Chen and Yahui Xiu and Yuxin Zheng and Xinxin Wang and Qian Wang and Danqi Guo and Yan Wan},
  doi          = {10.1049/ipr2.12984},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {772-784},
  shortjournal = {IET Image Process.},
  title        = {A weakly supervised learning pipeline for profiled fibre inspection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ACD-YOLO: Improved YOLOv5-based method for steel surface
defects detection. <em>IETIP</em>, <em>18</em>(3), 761–771. (<a
href="https://doi.org/10.1049/ipr2.12983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the quality of steel is of paramount importance in modern production, the defects detection of steel surface is significantly crucial. In this field, two-stage detection algorithms have encountered issues about low detection speed, while one-stage detection algorithms have room for improvement in detection accuracy. How to trade-off between accuracy and speed of detection to better meet the demands of industrial production remains a challenge. To address this problem, this paper proposes a You Only Look Once version 5 (YOLOv5)-based improved method ACD-YOLO. ACD-YOLO model incorporates anchors optimization, context augmentation module, and efficient convolution operators. In anchor optimization, the boundaries of anchors are optimized using an improved genetic algorithm. Moreover, to improve detection accuracy, a context augmentation module is incorporated into both the head and the backbone end of the network. Additionally, efficient convolution operators are adopted to address the increase of computation complexity caused by the context augmentation module. Experimental results show that ACD-YOLO achieves mean average precision of 79.3%, with frames per second of 72. Compared to reference methods, ACD-YOLO achieves the best balance between accuracy and speed of detection, and is more suitable for practice industrial production.},
  archive      = {J_IETIP},
  author       = {Jiacheng Fan and Min Wang and Baolei Li and Mingxue Liu and Dingcai shen},
  doi          = {10.1049/ipr2.12983},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {761-771},
  shortjournal = {IET Image Process.},
  title        = {ACD-YOLO: Improved YOLOv5-based method for steel surface defects detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image restoration with group sparse representation and
low-rank group residual learning. <em>IETIP</em>, <em>18</em>(3),
741–760. (<a href="https://doi.org/10.1049/ipr2.12982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration, as a fundamental research topic of image processing, is to reconstruct the original image from degraded signal using the prior knowledge of image. Group sparse representation (GSR) is powerful for image restoration; it however often leads to undesirable sparse solutions in practice. In order to improve the quality of image restoration based on GSR, the sparsity residual model expects the representation learned from degraded images to be as close as possible to the true representation. In this article, a group residual learning based on low-rank self-representation is proposed to automatically estimate the true group sparse representation. It makes full use of the relation among patches and explores the subgroup structures within the same group, which makes the sparse residual model have better interpretation furthermore, results in high-quality restored images. Extensive experimental results on two typical image restoration tasks (image denoising and deblocking) demonstrate that the proposed algorithm outperforms many other popular or state-of-the-art image restoration methods.},
  archive      = {J_IETIP},
  author       = {Zhaoyuan Cai and Xianghua Xie and Jingjing Deng and Zengfa Dou and Bo Tong and Xiaoke Ma},
  doi          = {10.1049/ipr2.12982},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {741-760},
  shortjournal = {IET Image Process.},
  title        = {Image restoration with group sparse representation and low-rank group residual learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Additive margin networks with adaptive feature recalibration
and its applications in brain stroke CT image classification.
<em>IETIP</em>, <em>18</em>(3), 731–740. (<a
href="https://doi.org/10.1049/ipr2.12981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an additive margin network with adaptive feature recalibration, AMNets, for handling a wide range of visual tasks, is proposed. The new AMNets consists of three improvements on convolutional neural networks: (1) squeeze-and-excitation mechanism was embedded into the the residual block of single layer to improve the performance of the classification, (2) softmax loss was replaced by the additive margin softmax loss to minimize intra-class variance and maximize inter-class variance, and (3) Adam optimizer was exploited to update iteratively the network parameters. Experiments on the Brain Stroke CT Image Dataset show that our additive margin network is quite effective to improve state-of-the-art algorithms.},
  archive      = {J_IETIP},
  author       = {Qinling Gao and Hao Fu and Xuejing Zhao and Zhaoming Ge},
  doi          = {10.1049/ipr2.12981},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {731-740},
  shortjournal = {IET Image Process.},
  title        = {Additive margin networks with adaptive feature recalibration and its applications in brain stroke CT image classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Dynamic estimator selection for double-bit-range estimation
in VVC CABAC entropy coding. <em>IETIP</em>, <em>18</em>(3), 722–730.
(<a href="https://doi.org/10.1049/ipr2.12980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CABAC is the only entropy coding used in Versatile Video Coding (VVC). This is achieved through multiple estimators approach that provide more accurate predictions by considering different estimated probability results, but CABAC coding requires higher complexity and bit-range accuracy than other approaches. Therefore, there is more potential to refine the performance from the perspective of bit allocation and architecture design. In this paper, a selection method is proposed to determine which estimator is recommended to dynamically perform the current entropy coding. Taking advantage of the double-bit-range architecture, the bits contained in the different estimators are also rearranged based on Most Probable Symbol ( ) determination and Least Probable Symbol ( ) considerations. Experimental reports in the work report that the coding time can be reduced using the proposed method and there is a slight gain in Peak Signal-to-Noise Ratio (PSNR) while saving some Rate-distortion (RD) performance in bitrate.},
  archive      = {J_IETIP},
  author       = {Sio-Kei Im and Ka-Hou Chan},
  doi          = {10.1049/ipr2.12980},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {722-730},
  shortjournal = {IET Image Process.},
  title        = {Dynamic estimator selection for double-bit-range estimation in VVC CABAC entropy coding},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Foggy image restoration using deep sub-pixel reconstruction
network. <em>IETIP</em>, <em>18</em>(3), 707–721. (<a
href="https://doi.org/10.1049/ipr2.12979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light undergoes attenuation due to scattering and refraction when propagating through aerosols. In foggy conditions, Aerosol particles in the troposphere exhibit high mobility introducing intricate non-linear noise into images. Foggy image restoration represents an ill-posed problem, where traditional physical models and image enhancement techniques often prove inadequate in delivering effective solutions. This paper introduces a novel deep sub-pixel reconstruction algorithm for foggy image restoration, pioneering the application of sub-pixel reconstruction modules to this domain. This model employs convolutional layers to extract low-level features and dense-connected layers for high-level feature extraction. Furthermore, a specialized sub-pixel reconstruction module tailored for the task of foggy image restoration is designed, with the purpose of reconstructing dehazed images from latent vectors. During training, a generative adversarial training framework is adopted, incorporating a purpose-designed discriminator. Additionally, a fusion loss is implemented to facilitate model refinement. Quantitative and qualitative evaluation experiments conducted on synthetic and real-world image datasets demonstrate the effectiveness of the proposed method in preserving finer details. The Structural Similarity Index (SSIM) is observed to improve by 2.5%, attesting to enhanced perceptual quality for grayscale foggy images.},
  archive      = {J_IETIP},
  author       = {Linge Li and Xiaoqin Liu and Feiyu Shi and Yihua Cai and Ying Zhang and Ping Fang and Chao Mu and Ningquan Weng},
  doi          = {10.1049/ipr2.12979},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {707-721},
  shortjournal = {IET Image Process.},
  title        = {Foggy image restoration using deep sub-pixel reconstruction network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Natureness-preserving tone mapping operator based on
improved guided filter and adaptive gamma curve. <em>IETIP</em>,
<em>18</em>(3), 694–706. (<a
href="https://doi.org/10.1049/ipr2.12978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping high dynamic range (HDR) images to low dynamic range (LDR) display devices while performing enhancement processing is a challenging task. To address the issues of chromatic drift, the imbalance between preserving naturalness and enhancing details during the mapping process, this paper proposed a tone mapping operator (TMO) based on an improved guided filter with multi-layered processing. Firstly, the operator tackled the problem of insufficient extraction of detail layer information in complex scene images by redefining the formula of the guided filter and introducing adaptive regularization parameters. This enhancement enabled more effective extraction of details compared to traditional fixed-parameter guided filters. Secondly, the algorithm employed a centre-surround approach to subdivide and processed the detail layers individually, effectively suppressing halos and artefacts. Thirdly, image entropy was utilized to guide the fusion and enhancement processing of the detail layers, preventing excessive or insufficient enhancement. Finally, this paper proposed an adaptive gamma correction algorithm and a chromatic consistency algorithm to improve overall contrast and hue saturation. The results demonstrate convincing performance in terms of subjective visual evaluation and image quality assessment.},
  archive      = {J_IETIP},
  author       = {Chaoxuan Hu and Peipei Liu and Weifeng Qin and Shuo Wang},
  doi          = {10.1049/ipr2.12978},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {694-706},
  shortjournal = {IET Image Process.},
  title        = {Natureness-preserving tone mapping operator based on improved guided filter and adaptive gamma curve},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intelligent remote sensing image quality inspection
system. <em>IETIP</em>, <em>18</em>(3), 678–693. (<a
href="https://doi.org/10.1049/ipr2.12977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inevitable presence of quality problems, quality inspection of remote sensing images is indeed an indispensable step between the acquisition and the application of them. However, traditional manual inspection suffers from low efficiency. Hence, we propose a novel deep learning-based two-step intelligent system consisting of multiple advanced computer vision models, which first performs image classification by SwinV2 and then accordingly adopts the most appropriate method, such as semantic segmentation by Segformer, to localize the quality problems. Results demonstrate that the proposed method exhibits excellent performance and efficiency, surpassing traditional methods. Furthermore, an initial exploration of applying multimodal models to remote sensing image quality inspection is conducted.},
  archive      = {J_IETIP},
  author       = {Yijiong Yu and Tao Wang and Kang Ran and Chang Li and Hao Wu},
  doi          = {10.1049/ipr2.12977},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {678-693},
  shortjournal = {IET Image Process.},
  title        = {An intelligent remote sensing image quality inspection system},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridge bottom crack detection and modeling based on faster
r-CNN and BIM. <em>IETIP</em>, <em>18</em>(3), 664–677. (<a
href="https://doi.org/10.1049/ipr2.12976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bridge bottom crack detection provides important state information for bridge disease control and safety assessment. This paper proposes a detection method based on deep learning Faster R-CNN and BIM (Building Information Modeling). The UAV (Unmanned Aerial Vehicle) was used for close aerial photography to obtain high-resolution crack images of the concrete surface at the bottom of a bridge. Through deep learning algorithms, a Faster R-CNN model is trained and established for crack identifications. The crack identification accuracy rate and recall rate reach 92.03% and 96.54%, respectively. Crack images are mapped to a BIM model developed for the chosen bridge, and the box girder family with cracks and the three crack families of transverse cracks, longitudinal cracks and turtle cracks are established. The cracks are located and the visualization of the beam bridge with cracks was completed. In order to better assess the health condition of the bridge. The results show that the combination of UAV bridge crack detection and modelling solves the remote, visual and automated detection of cracks on the surface of bridge structures, which are difficult to reach manually, and has important scientific research and engineering application value.},
  archive      = {J_IETIP},
  author       = {Linfeng Gan and Hu Liu and Yue Yan and Aoran Chen},
  doi          = {10.1049/ipr2.12976},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {664-677},
  shortjournal = {IET Image Process.},
  title        = {Bridge bottom crack detection and modeling based on faster R-CNN and BIM},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Single-image deraining algorithm based on multi-stage
recurrent network. <em>IETIP</em>, <em>18</em>(3), 650–663. (<a
href="https://doi.org/10.1049/ipr2.12975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The damage caused by the rain streak attached to the rainy image to the background seriously affects the analysis of image information and subsequent research. Aiming at the problem that the current single image rain removal methods cannot extract the deep rain streak features, a single image deraining algorithm based on multi-stage recurrent network is proposed, which is committed to separating the rain streak layer and the background layer. The network consists of three stages, each stage is mainly composed of upper and lower scale dilation blocks, the upper scale dilation block includes multi-scale dilated convolution and scale fusion, the lower scale dilation block is set the same, and the stages are connected using improved gated recurrent units to facilitate parameter sharing between them. The experimental results show that the proposed algorithm outperforms the comparative algorithm in both quantitative indicators and qualitative analysis on synthetic and real datasets, and the restored image retains more detailed information.},
  archive      = {J_IETIP},
  author       = {Chen Li and Xueting Li and Yecai Guo and Jia Li},
  doi          = {10.1049/ipr2.12975},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {650-663},
  shortjournal = {IET Image Process.},
  title        = {Single-image deraining algorithm based on multi-stage recurrent network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Image encryption algorithm based on hyperchaos and DNA
coding. <em>IETIP</em>, <em>18</em>(3), 627–649. (<a
href="https://doi.org/10.1049/ipr2.12974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel image encryption algorithm based on hyperchaos and DNA encoding is proposed in this paper. Unlike the traditional DNA-based encoding method, a more fine-grained encryption approach is adopted, which enhances the security of the system. To address the inherent binary arithmetic limitation of traditional DNA computing, the authors develop a novel symmetric and reversible DNA subtraction operation that fully utilizes the encryption advantage of DNA computing. To enhance security, the authors introduce a DNA crossover mechanism based on pixel, which can generate different ciphertexts even for the same pixel values. In the algorithm, the authors propose a diffusion strategy that combines left diffusion, right diffusion, and XOR parameters. This strategy not only ensures the avalanche effect, where even a single pixel change in the plaintext image will cause a drastic change in the ciphertext image, but also greatly enhances the security of the algorithm. Experimental results and performance analysis indicate that the authors’ proposed system has good encryption performance and the ability to resist noise attacks and clipping attacks.},
  archive      = {J_IETIP},
  author       = {Lizong Li},
  doi          = {10.1049/ipr2.12974},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {627-649},
  shortjournal = {IET Image Process.},
  title        = {Image encryption algorithm based on hyperchaos and DNA coding},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepFake detection against adversarial examples based on
d-VAEGAN. <em>IETIP</em>, <em>18</em>(3), 615–626. (<a
href="https://doi.org/10.1049/ipr2.12973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, the development of DeepFake has raise a lot of security problems. Therefore, detection of DeepFake is critical. However, the existing DeepFake detection methods are often vulnerable to adversarial attacks, i.e. adding carefully crafted imperceptible perturbations into forged images is possible to evade detection. In this paper, a DeepFake detection method based on image denoising is proposed by combining variational autoencoder (VAE) and generative adversarial network (GAN), namely D-VAEGAN. Firstly, an encoder is designed to extract the features of the image in a low-dimensional latent space. Then, a decoder reconstructs the original clean image using the features in this low-dimensional latent space. Secondly, an auxiliary discriminative network is introduced to further improve the performance of the model, which improves the quality of the reconstructed images. Furthermore, feature similarity loss is added as a penalty term to the reconstruction optimization function to improve the adversarial robustness. Experimental results on the FaceForensics++ dataset show that the proposed approach significantly outperforms the five adversarial training-based defence methods. The approach achieves 96% in accuracy, which is on average about 50% higher than other comparison methods.},
  archive      = {J_IETIP},
  author       = {Ping Chen and Ming Xu and Jianxiang Qi},
  doi          = {10.1049/ipr2.12973},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {615-626},
  shortjournal = {IET Image Process.},
  title        = {DeepFake detection against adversarial examples based on D-VAEGAN},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSFFNet: Lightweight cross-scale feature fusion network for
salient object detection in remote sensing images. <em>IETIP</em>,
<em>18</em>(3), 602–614. (<a
href="https://doi.org/10.1049/ipr2.12972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD), one of the most important applications in the field of computer vision, aims to extract the most visually appealing regions of scenes. However, the improvement of the accuracy of existing salient object detection in optical remote sensing images (ORSI-SOD) is usually accompanied by an increase of network complexity, which affects the application of these models. Motivated by this, a novel lightweight edge-supervised neural network for ORSI-SOD is proposed, named CSFFNet. Specifically, the backbone (ResNet34) is first lightened by feature encoding module (FEM), building a lightweight subnet for feature extraction. Then, in the transformer-based feature pyramid enhancement module (FPEM), the convolutional features obtained in the FEM are enhanced by long-distance dependence to obtain multi-scale features containing rich saliency cues. Based on this, the feature fusion module (FFM) is designed to capture cross-scale long-range dependencies and effectively fuse high-level semantic information with low-level detail information. Thus, the increase in network complexity due to multi-level decoding is avoided. Finally, the segmentation results are optimized by using salient edges as auxiliary information, which effectively improves the contrast and completeness of the results. Experimental results on two public datasets demonstrate that the lightweight CSFFNet achieves competitive or even better performance compared with state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Longbao Wang and Chong Long and Xin Li and Xiaodan Tang and Zhipeng Bai and Hongmin Gao},
  doi          = {10.1049/ipr2.12972},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {602-614},
  shortjournal = {IET Image Process.},
  title        = {CSFFNet: Lightweight cross-scale feature fusion network for salient object detection in remote sensing images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A DenseNet-based feature weighting convolutional network
recognition model and its application in industrial part classification.
<em>IETIP</em>, <em>18</em>(3), 589–601. (<a
href="https://doi.org/10.1049/ipr2.12971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional warehousing typically needs machine learning or manual tagging to classify objects. However, this method is less robust and consumes a lot of labour and material resources. Based on DenseNet, this work proposes a feature weighting convolutional network recognition model and designs a set of software and hardware for data acquisition, which is applied to the efficient classification of industrial parts in warehouse management. Firstly, this work modifies DenseNet by embedding SE-Block, and replaces the cross-entropy loss function with the focus loss function to optimize the model structure. Secondly, a multi-view hardware and software acquisition system is designed to complete the functions of part image acquisition, image preprocessing, model training and part recognition. Finally, an industrial parts sorting experiment was designed. Compared with the original DenseNet model, the proposed weighted convolutional network identification model showed that the accuracy of the modified model was increased by 3.09% and the convergence rate was significantly improved. The modified model proposed in this work aims to improve the recognition accuracy of industrial parts in modern warehouse management, so as to modify the classification efficiency of warehouse parts in production.},
  archive      = {J_IETIP},
  author       = {Kang An and Xiaoqing Sun and Yaqing Song and Yebin Lu and Qianqian Shangguan},
  doi          = {10.1049/ipr2.12971},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {589-601},
  shortjournal = {IET Image Process.},
  title        = {A DenseNet-based feature weighting convolutional network recognition model and its application in industrial part classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2D compressed sensing of encrypted images based on
complex-valued measurement matrix. <em>IETIP</em>, <em>18</em>(3),
572–588. (<a href="https://doi.org/10.1049/ipr2.12970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When using untrusted third parties to compress and transmit images in real-life scenarios, it is vital to encrypt them before compression. In order to better address the issues of low security in the original image and poor reconstruction quality of the encrypted image during compressed sensing, this paper proposes a 2D compressed sensing scheme for encrypted images based on complex-valued measurement matrix (2DCS-CVM). Firstly, the SHA-256 algorithm generates keys for the hyperchaotic Lorenz system, and then the chaotic sequences are used to create encrypted images with increased security through subtractive diffusion and global permutation. Secondly, the complex-valued Vandermonde measurement matrix is utilized for 2D compressed sensing on the encrypted image, and the two-dimensional projected gradient with embedding decryption algorithm is used to generate recovered images with improved reconstruction performance. Finally, the measurement matrix&#39;s computational complexity and transmission bandwidth are reduced through structural sparsification with sparse random matrices. Simulation results demonstrate that this scheme offers an optimal balance between storage, computational complexity, hardware implementation, and reconstruction performance while providing excellent security and robustness.},
  archive      = {J_IETIP},
  author       = {Yuqian Yan and Yue Wang and Linlin Xue and Weiwei Qiu and Zhongpeng Wang},
  doi          = {10.1049/ipr2.12970},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {572-588},
  shortjournal = {IET Image Process.},
  title        = {2D compressed sensing of encrypted images based on complex-valued measurement matrix},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single image denoising based on adaptive fusion dual-domain
network. <em>IETIP</em>, <em>18</em>(3), 561–571. (<a
href="https://doi.org/10.1049/ipr2.12969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based-image denoising methods have recently achieved excellent performance by learning non-linear mapping in the spatial domain. However, these methods fail to address the noise without specific distribution because they only use features of the spatial domain. Meanwhile, existing methods that utilize features in the frequency domain fail to combine the detailed information of both domains properly for effective reconstruction and demonstrate poor generalization. Therefore, a novel adaptive fusion dual-domain network (AFDN) is introduced for single image restoration. Different from deep learning-based methods, which operate on the spatial or dual-domains in a certain order, the proposed AFDN combine the spatial-domain image and corresponding frequency-domain image as the input and use the interlacing dual-domain module with flexible adaptability to learn the relationship between spatial and frequency domains. In experimental results, the AFDN is compared with several state-of-the-art restoration methods. Quantitative results showed that the AFDN achieves enhanced effects and high index values. The code of this paper will be released at https://github.com/jzw0707/AFDN},
  archive      = {J_IETIP},
  author       = {Zhiwei Jiang and Zhizhong Xue and Jue Wang and Yibiao Hu and Qiufu Zheng},
  doi          = {10.1049/ipr2.12969},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {561-571},
  shortjournal = {IET Image Process.},
  title        = {Single image denoising based on adaptive fusion dual-domain network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A CBAM-GAN-based method for super-resolution reconstruction
of remote sensing image. <em>IETIP</em>, <em>18</em>(2), 548–560. (<a
href="https://doi.org/10.1049/ipr2.12968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As satellite imagery technology advances, remote sensing plays an increasingly prominent role in modern society. Nevertheless, the limitations of existing imaging sensors and complex atmospheric conditions constrain the quality of raw remote sensing data, posing challenges for interpretation and noise reduction. Super-resolution technology focuses on enhancing low-quality, low-resolution remote sensing images. In this study, we introduce a method that utilizes a high-order degradation model to generate low-resolution remote sensing images. We employ a Generative Adversarial Network with a Convolutional Block Attention Module (CBAM-GAN) to enhance these images, reducing noise interference and improving texture and feature display. Our approach outperforms other methods on the UCMerced-LandUse, WHU-RS19, and AID datasets. Specifically, it raises SSIM index scores to 0.9443, 0.8928, and 0.8633, respectively, exceeding baselines by 1.31%, 0.19%, and 1.30%. The MOS index also improves to 3.98, 3.96, and 3.83, respectively, representing a 2.31%, 8.20%, and 2.96% gain over the baseline. Our reconstruction produces superior results, demonstrating the effectiveness of our proposed method.},
  archive      = {J_IETIP},
  author       = {Longbao Wang and Qing Yu and Xin Li and Hui Zeng and Hailong Zhang and Hongmin Gao},
  doi          = {10.1049/ipr2.12968},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {548-560},
  shortjournal = {IET Image Process.},
  title        = {A CBAM-GAN-based method for super-resolution reconstruction of remote sensing image},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurately 3D neuron localization using 2D conv-LSTM
super-resolution segmentation network. <em>IETIP</em>, <em>18</em>(2),
535–547. (<a href="https://doi.org/10.1049/ipr2.12967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuron localization is a fundamental step in the neuron morphology reconstruction and quantitative analysis of cell counting and spatial distribution. The recent development of labelling and imaging techniques has resulted in growing demand for automatic neuron localization methods of 3D neuronal microscopy images. However, accurately localizing touching neurons remains a challenge. To address this issue, a novel method utilizing a super-resolution segmentation network based on 2D Conv-LSTM and a region growing method are proposed. This network can map and detect individual neurons in higher resolution space, allowing for the separation of closely touching neurons with reduced resource consumption. Subsequently, a region growing method is employed to localize neurons accurately. This method is evaluated using neuronal images generated by TDI-fMOST. This method achieved neuron localization with an F1 score of 0.91. In comparison, other automatic localization methods achieved F1 scores lower than 0.85. It is also demonstrated that our network has fewer computational requirements compared to 3D neural networks. This method is promising for accurately localizing neurons in large-scale neuron images.},
  archive      = {J_IETIP},
  author       = {Hang Zhou and Yuxin Li and Wu Wen and Hao Yang and Yayu Ma and Min Chen},
  doi          = {10.1049/ipr2.12967},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {535-547},
  shortjournal = {IET Image Process.},
  title        = {Accurately 3D neuron localization using 2D conv-LSTM super-resolution segmentation network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EfficientUNet: An efficient solution for breast tumour
segmentation in ultrasound images. <em>IETIP</em>, <em>18</em>(2),
523–534. (<a href="https://doi.org/10.1049/ipr2.12966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate segmentation of breast tumours is important for the diagnosis and treatment of breast cancer. When using the classic U-Net, Attention-UNet, and UNet++ to segment the tumour, there are problems of oversegmentation, incorrect segmentation, and poor edge continuity. In this paper, an effective tumour segmentation method, EfficientUNet, is proposed. EfficientUNet adopts a step-by-step enhancement method, combining ResNet18, a channel attention mechanism and deep supervision. ResNet18, as the encoder of the whole network, solves the problem of gradient disappearanceand improves the feature extraction ability of the model. The channel attention module makes the model more accurate in tumour edge processing. The deep supervision technology accelerates the model training and provides the convergence direction for the model. In addition, it is found that when adjusting the size of the image, the method of image filling before clipping (or zooming) is more conducive to model learning than the direct interpolation method. And a comparative experiment wasperformed on dataset B. Compared to U-Net, Attention-UNet and UNet++, EfficientUNet has the highest performance. Finally, the ablation experiment also indicated the effectiveness of each module in EfficientUNet.},
  archive      = {J_IETIP},
  author       = {Guizeng You and Xinwu Yang and Xuanbo Lee and Kongqiang Zhu},
  doi          = {10.1049/ipr2.12966},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {523-534},
  shortjournal = {IET Image Process.},
  title        = {EfficientUNet: An efficient solution for breast tumour segmentation in ultrasound images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HQ-I2IT: Redesign the optimization scheme to improve image
quality in CycleGAN-based image translation systems. <em>IETIP</em>,
<em>18</em>(2), 507–522. (<a
href="https://doi.org/10.1049/ipr2.12965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image-to-image translation (I2IT) task aims to transform images from the source domain into the specified target domain. State-of-the-art CycleGAN-based translation algorithms typically use cycle consistency loss and latent regression loss to constrain translation. In this work, it is demonstrated that the model parameters constrained by the cycle consistency loss and the latent regression loss are equivalent to optimizing the medians of the data distribution and the generative distribution. In addition, there is a style bias in the translation. This bias interacts between the generator and the style encoder and visually exhibits translation errors, e.g. the style of the generated image is not equal to the style of the reference image. To address these issues, a new I2IT model termed high-quality-I2IT (HQ-I2IT) is proposed. The optimization scheme is redesigned to prevent the model from optimizing the median of the data distribution. In addition, by separating the optimization of the generator and the latent code estimator, the redesigned model avoids error interactions and gradually corrects errors during training, thereby avoiding learning the median of the generated distribution. The experimental results demonstrate that the visual quality of the images produced by HQ-I2IT is significantly improved without changing the generator structure, especially when guided by the reference images. Specifically, the Fréchet inception distance on the AFHQ and CelebA-HQ datasets are reduced from 19.8 to 10.2 and from 23.8 to 17.0, respectively.},
  archive      = {J_IETIP},
  author       = {Yipeng Zhang and Bingliang Hu and Yingying Huang and Chi Gao and Jianfu Yin and Quang Wang},
  doi          = {10.1049/ipr2.12965},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {507-522},
  shortjournal = {IET Image Process.},
  title        = {HQ-I2IT: Redesign the optimization scheme to improve image quality in CycleGAN-based image translation systems},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image representation and reconstruction by compositing
gaussian ellipses. <em>IETIP</em>, <em>18</em>(2), 493–506. (<a
href="https://doi.org/10.1049/ipr2.12964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a method of stroke-based rendering is proposed for image representation and reconstruction. The proposed method involves compositing a set of ellipses that greatly vary in location, size, rotation angle, color, and opacity. This study proves that alpha compositing is differentiable if a two-dimensional Gaussian function is used to draw a solid ellipse. The gradient method can then be employed for automatically identifying the parameters of each ellipse such that the difference between the input image and the composited image is minimized. Experimental results indicate that the proposed method can represent various types of images including portraits, landscapes, buildings, street scenes, artificial objects, medical images etc. The proposed method can particularly render the most details and significant features of an input image with fewer strokes compared to other stroke-based rendering algorithms. This study also demonstrates that the proposed method can be applied in painting style simulation and sparse-view computed tomography imaging.},
  archive      = {J_IETIP},
  author       = {Chang-Chieh Cheng},
  doi          = {10.1049/ipr2.12964},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {493-506},
  shortjournal = {IET Image Process.},
  title        = {Image representation and reconstruction by compositing gaussian ellipses},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ship detection based on YOLO algorithm for visible images.
<em>IETIP</em>, <em>18</em>(2), 481–492. (<a
href="https://doi.org/10.1049/ipr2.12963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship detection is a crucial task for waterway surveillance and channel optimization, especially in close proximity to the shore. However, detecting ship in visible image-based detection remains a challenge due to the limited nature of visible image datasets. To address this issue, the Inland Ships Data Set (ISDS) is constructed to facilitate research on ship identification. On the other hand, most detection methods struggle to accurately identify ships that are small in size. Therefore, a visible image-based ship detection model is proposed that employs a multi-scale weighted feature fusion structure with the YOLOv4 detection model to improve the efficacy of small ship detection. Specifically, the YOLOv4 model is improved through fusing multi-scale feature, redesigning priori frame, and enhancing loss function. The model, named YOLOv4-MSW (i.e. YOLOv4 based on Multi-Scale Weighted feature fusion), exhibits improved performance on ship detection in experiments conducted on the ISDS dataset, outperforming the original YOLOv4 model by improving the average precision (AP) by 4.87% and the recall rate by 10.03%. Meanwhile, the model achieve better detection accuracy and improve the average precision rate by at least 0.86% compared to existing learned object detection methods. The code related to this work are released at https://github.com/Sunhuashan/YOLOv4-MSW . The whole dataset is available at https://drive.google.com/drive/folders/1fzJ2fcqiko6lFwqIEGghMceoQgv-8jBy .},
  archive      = {J_IETIP},
  author       = {Qian Huang and Huashan Sun and Yiming Wang and Yang Yuan and Xiaotong Guo and Qiang Gao},
  doi          = {10.1049/ipr2.12963},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {481-492},
  shortjournal = {IET Image Process.},
  title        = {Ship detection based on YOLO algorithm for visible images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ADFF: Adaptive de-morphing factor framework for restoring
accomplice’s facial image. <em>IETIP</em>, <em>18</em>(2), 470–480. (<a
href="https://doi.org/10.1049/ipr2.12962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Morphing attacks (MAs) pose a substantial security threat to the Automatic Border Control (ABC) system. While a few morphing attack detection (MAD) methods have been proposed, the face morphing accomplice&#39;s facial restoration has not received sufficient attention. Due to the inability to foresee the morphing factor used for a particular morphed image, selecting the appropriate de-morphing factor becomes a challenging problem in the restoration of the accomplice&#39;s facial image. If the morphing factor cannot be chosen reasonably, achieving the desired restoration effect is difficult. Therefore, this paper presents an adaptive de-morphing factor framework (ADFF) architecture for restoring the accomplice&#39;s facial image. By exploiting the morphed images stored in the electronic passport system and the real-time captured criminal&#39;s images, ADFF can effectively restore the accomplice&#39;s facial image. Experimental results and analysis show that ADFF can significantly reduce the security threats of MAs on ABC.},
  archive      = {J_IETIP},
  author       = {Min Long and Jun Zhou and Le-Bing Zhang and Fei Peng and Dengyong Zhang},
  doi          = {10.1049/ipr2.12962},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {470-480},
  shortjournal = {IET Image Process.},
  title        = {ADFF: Adaptive de-morphing factor framework for restoring accomplice&#39;s facial image},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust watermarking algorithm against JPEG compression
based on multiscale autoencoder. <em>IETIP</em>, <em>18</em>(2),
455–469. (<a href="https://doi.org/10.1049/ipr2.12961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network structure of digital watermarking algorithm based on deep learning is usually encoder-noise layer-decoder. Most of the existing encoders suffer from the problem of insufficient feature extraction, and the introduction of simulated differentiable joint photographic experts group (JPEG) compression in the noise layer cannot ensure the robustness under real JPEG. In this paper, a watermarking algorithm based on multi-scale auto-encoder is proposed, which can effectively extract the image feature information by combining with the channel attention mechanism. At the same time, some parameters of decoder and encoder are shared to reduce redundant feature embedding and improve extraction accuracy. This paper also proposes a robust training scheme against JPEG compression, which can guide the model to store the watermark in the low-frequency region needed for decoding. Experimental results show that the peak signal-to-noise ratio (PSNR) of the proposed algorithm is above 48 and the decoding rate is above 99% under JPEG compression with quality factor Q = 50. Moreover, this scheme can effectively promote the combination of noise layer in training. In addition, the proposed algorithm is also robust to other common network noises.},
  archive      = {J_IETIP},
  author       = {Wei Zhang and Rongrong Chen and Bin Wang},
  doi          = {10.1049/ipr2.12961},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {455-469},
  shortjournal = {IET Image Process.},
  title        = {A robust watermarking algorithm against JPEG compression based on multiscale autoencoder},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiview feature co-factorization based dictionary
selection for video summarization. <em>IETIP</em>, <em>18</em>(2),
439–454. (<a href="https://doi.org/10.1049/ipr2.12960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, video summarization (VS) has emerged as one of the most effective tools for rapidly understanding video big data. Dictionary selection based on self-representation and sparse regularization is consistent with the requirement of VS, which aims to represent the original video with little reconstruction error by a small number of video frames. However, one crucial issue is that the existing methods mainly use a single view feature, which is not sufficient enough for acquiring the full pictorial details and affects the quality of the produced video summary. Although a few methods use more than one features, they only directly concatenate the features, which does not take advantage of the relationship of different features. Considering the complementarity of shallow and deep features, multiview feature co-factorization based dictionary selection for VS is proposed in this paper to use the common information of both view features for VS. Specifically, two view features are used to fully exploit the full pictorial information of video frames, then the common information of two different views is extracted through coupled matrix factorization to conduct the dictionary selection for VS. Experiments have been carried out on two benchmark datasets, and results have demonstrated the effectiveness and superiority of the proposed method.},
  archive      = {J_IETIP},
  author       = {Xiaoning Chen and Mingyang Ma and Runfeng Yang and Yong Peng},
  doi          = {10.1049/ipr2.12960},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {439-454},
  shortjournal = {IET Image Process.},
  title        = {Multiview feature co-factorization based dictionary selection for video summarization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image inpainting network based on multi-level attention
mechanism. <em>IETIP</em>, <em>18</em>(2), 428–438. (<a
href="https://doi.org/10.1049/ipr2.12958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting networks based on deep learning techniques have been widely used in many important fields. However, most inpainting networks fail to generate desirable repaired images. This may be due to their failure to extract effective features and accurately assign high weights to the undamaged regions. To alleviate these problems, an image inpainting network based on gated convolution and multi-level attention mechanism (IIN-GCMAM) is proposed in this paper. This network follows encoder–decoder architecture, consisting of the gated convolution encoder (GC-encoder) and the multi-level attention mechanism decoder (MAM-decoder). The GC-encoder weighs the extracted features with gated convolutions, which reduces the interference caused by the damaged regions. The multi-level attention mechanism employed in the MAM-decoder uses multi-scale feature maps spatially and channel-wise to improve the consistency in global structure and the fineness of repaired results. Extensive experiments are conducted on the common datasets, Paris StreetView and CelebA. Experimental results indicate that the proposed IIN-GCMAM can achieve a good performance on the common evaluation metrics and visual effects. It can achieve 0.0408, 0.720, and 22.27 in MAE, SSIM, and PSNR at the mask ratio of 50%–60%, respectively.},
  archive      = {J_IETIP},
  author       = {Hongyue Xiang and Weidong Min and Zitai Wei and Meng Zhu and Mengxue Liu and Ziyang Deng},
  doi          = {10.1049/ipr2.12958},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {428-438},
  shortjournal = {IET Image Process.},
  title        = {Image inpainting network based on multi-level attention mechanism},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Textile defect detection based on multi-proportion spatial
attention mechanism and channel memory feature fusion network.
<em>IETIP</em>, <em>18</em>(2), 412–427. (<a
href="https://doi.org/10.1049/ipr2.12957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a textile defect detection method that utilizes a multi-proportion spatial attention mechanism and channel memory feature fusion network by addressing the difficulties presented by complicated shapes and large size variations. In particular, a multi-proportion spatial attention mechanism (MPAM) is introduced, which employs multi-proportion convolution to improve the backbone network&#39;s capacity to detect non-uniform structural defects. Additionally, the generality and adaptability of the model are enhanced by a multi-scale spatial pyramid pooling structure (MS-SPP). Second, a channel attention mechanism-based memory feature fusion network is developed, which incorporates channel attention to adaptive weight the feature channels, focusing on crucial information channels to efficiently fuse contextual features and enhance the model&#39;s memory capacity. Finally, a novel efficient Wise-IoU (EWIoU) loss function is proposed, which utilizes a dynamic non-monotonic focusing mechanism to increase the penalty on distance measurement, thus enhancing the model&#39;s detection performance. Experiment findings on the ZJU-Leaper and Tianchi textile datasets reveal that compared to the YOLOv7 baseline, the method in this paper has an increase of 6.5 and 2 percentage points, respectively, and the detection accuracy is better than most existing networks.},
  archive      = {J_IETIP},
  author       = {Yaxin Ji and Lan Di},
  doi          = {10.1049/ipr2.12957},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {412-427},
  shortjournal = {IET Image Process.},
  title        = {Textile defect detection based on multi-proportion spatial attention mechanism and channel memory feature fusion network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight license plate detection algorithm based on
deep learning. <em>IETIP</em>, <em>18</em>(2), 403–411. (<a
href="https://doi.org/10.1049/ipr2.12956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {License plate detection is an important task in Intelligent Transportation Systems (ITS) and has a wide range of applications in vehicle management, traffic control, and public safety. In order to improve the accuracy and speed of mobile recognition, an improved lightweight YOLOv5s model is proposed for license plate detection. First, an improved Stemblock network is used to replace the original Focus layer in the network, which ensures strong feature expression capability and reduces a large number of parameters to lower the computational complexity; then, an improved lightweight network, ShuffleNetv2, is used to replace the backbone network of the YOLOv5s, which makes the model lighter and ensures the detection accuracy at the same time. Then, a feature enhancement module is designed to reduce the information loss caused by the rearrangement of the backbone network channels, which facilitates the information interaction in the feature fusion process; finally, the low-, medium- and high-level features in the Shufflenetv2 network structure are fused to form the final high-level output features. Experimental results on the CCPD dataset show that compared to other methods this paper obtains better performance and faster speed in the license plate detection task, in which the average precision mean value reaches 96.6%, and can achieve a detection speed of 43.86 frame/s, and the parameter volume is reduced to 5.07 M.},
  archive      = {J_IETIP},
  author       = {Shuo Zhu and Yu Wang and Zongyang Wang},
  doi          = {10.1049/ipr2.12956},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {403-411},
  shortjournal = {IET Image Process.},
  title        = {A lightweight license plate detection algorithm based on deep learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric point cloud filter. <em>IETIP</em>,
<em>18</em>(2), 388–402. (<a
href="https://doi.org/10.1049/ipr2.12955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a nonparametric point cloud filter to address the issue that existing point cloud filtering methods cannot retain important point cloud features after filtering and often require complex parameter adjustments. Firstly, a nonparametric clustering method is proposed to cluster various features of the point cloud and filter out isolated outliers. Then, the manifold distance truncation method is adopted to remove the outlier cluster generated by the point cloud clustering to complete point cloud filtering. Additionally, the proposed nonparametric clustering algorithm is compared with four of the latest clustering algorithms, including K-means clustering and OPTICS clustering to verify the rationality of the clustering features. Finally, the filtering results of the nonparametric point cloud filter are compared with those of statistical filtering and two other recently proposed point cloud filters to demonstrate its improved filtering effect and algorithm stability. The experimental results indicate that the proposed nonparametric point cloud filter can achieve better-filtering results and retain more point cloud features without adjusting parameters.},
  archive      = {J_IETIP},
  author       = {Yefa Sun and Jinli Wang},
  doi          = {10.1049/ipr2.12955},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {388-402},
  shortjournal = {IET Image Process.},
  title        = {Nonparametric point cloud filter},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pedestrian trajectory prediction method based on improved
LSTM network. <em>IETIP</em>, <em>18</em>(2), 379–387. (<a
href="https://doi.org/10.1049/ipr2.12954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction based on vision is a popular task in autopilot system. As pedestrian trajectories always cross each other, pedestrian targets will frequently obscure each other, which makes the collected pedestrian trajectories produce errors. Moreover, the interaction between pedestrians will have an impact on their trajectories in crowded areas, which leads to great challenge in trajectory prediction. In order to solve the above two problems, a pedestrian trajectory prediction method is proposed. Based on the YOLOv7 algorithm, it is connected with the StrongSORT algorithm. Then adding a feature recognition (ReID) module to the model, the problem of target switching and jumping in the process of pedestrian tracking can be solved. The dot product attention mechanism is also integrated into the long short term memory (LSTM) algorithm, the improved LSTM correlate target position and distance information for the pedestrian trajectory prediction. Extensive experiments on three most challenging datasets showed that this method improves the performance and largely reduces the model size.},
  archive      = {J_IETIP},
  author       = {Fugang Liu and Songnan Duan and Wang Juan},
  doi          = {10.1049/ipr2.12954},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {379-387},
  shortjournal = {IET Image Process.},
  title        = {A pedestrian trajectory prediction method based on improved LSTM network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Category-related attention domain adaptation for one-stage
cross-domain object detection. <em>IETIP</em>, <em>18</em>(2), 362–378.
(<a href="https://doi.org/10.1049/ipr2.12953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain object detection aims to generalize the distribution of features extracted by an object detector from an annotated domain to an unknown and unlabelled domain. Although one-stage cross-domain object detectors have significant advantages in deployment than two-stage ones, they suffer from two problems. First, neglect of category features and inaccurate alignment between multiple category features would lead to decreased domain adaptation efficiency. Second, one-stage detectors are more sensitive to imbalance of samples and negative samples severely affect the alignment process of domain adaptation. To overcome these two problems, an innovative category-related attention domain adaptive method that refines discrimination for each category&#39;s feature has been proposed in this paper. In the proposed method, a group of domain discriminators is assigned to each category to refine the fine-grained features between categories. The discriminators are trained via an adversarial discriminant framework to align the fine-grained distributions cross different domains. A category attention alignment (CAA) module is proposed to navigate more attention to the foreground regions in instance-level, which effectively alleviates the negative migration problem caused by the positive and negative sample imbalance of the one-stage detector. Specifically, two sub-modules in the CAA module are developed: a local CAA module and a global CAA module. These modules aim to optimize the domain offsets in both the local and global dimensions. In addition, a progressive global alignment module is designed to align image-level features, offering prior knowledge of migration for the CAA module. The progressive global alignment module and CAA module collaboratively engage in benign competition with the backbone network across various levels. Extensive transferring experiments are conducted among cityscapes, foggy cityscapes, SIM10K, and KITTI. Experimental results show that the proposed method has much superior performance than other one-stage cross-domain detectors.},
  archive      = {J_IETIP},
  author       = {Shengxian Guan and Shuai Dong and Yuefang Gao and Kun Zou},
  doi          = {10.1049/ipr2.12953},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {362-378},
  shortjournal = {IET Image Process.},
  title        = {Category-related attention domain adaptation for one-stage cross-domain object detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TRACL: Temporal reconstruction and adaptive consistency loss
for semi-supervised video semantic segmentation. <em>IETIP</em>,
<em>18</em>(2), 348–361. (<a
href="https://doi.org/10.1049/ipr2.12952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While existing supervised semantic segmentation methods have shown significant performance improvements, they heavily rely on large-scale pixel-level annotated data. To reduce this dependence, recent research has proposed semi-supervised learning-based methods that have achieved great success. However, almost all these works are mainly dedicated to image semantic segmentation, while semi-supervised video semantic segmentation (SVSS) has been barely explored. Due to the significant difference between video data and image, simply adapting semi-supervised image semantic segmentation approaches to SVSS may neglect the inherent temporal correlations in video frames. This paper presents a novel method (named TRACL) with temporal reconstruction (TR) and adaptive consistency loss (ACL) for SVSS, aiming to fully utilize the temporal relations of internal frames in video clip. The authors’ TR method implements the reconstruction from the feature and output levels to narrow the distribution gap between internal video frames. Specifically, considering the underlying data distribution, the authors construct Gaussian models for each category, and use probability density function to obtain the similarity between different feature maps for temporal feature reconstruction. The authors’ ACL can adaptively select two pixel-wise consistency loss including Flow Consistency Loss and Reconstruction Consistency Loss, providing stronger supervision signals for unlabelled frames during model training. Additionally, the authors extend their method to unlabelled video for more training data by employing mean-teacher structure. Extensive experiments on three datasets including Cityscapes, Camvid and VSPW demonstrate that the authors’ proposed method outperforms previous state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Zhixue Liang and Wenyong Dong and Bo Zhang},
  doi          = {10.1049/ipr2.12952},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {348-361},
  shortjournal = {IET Image Process.},
  title        = {TRACL: Temporal reconstruction and adaptive consistency loss for semi-supervised video semantic segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blind motion deblurring using improved DeblurGAN.
<em>IETIP</em>, <em>18</em>(2), 327–347. (<a
href="https://doi.org/10.1049/ipr2.12951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To develop a fast and effective image deblurring method, the blind recovery of motion-blurred images based on DeblurGAN(GAN, Generative Adversarial Networks) is researched.Firstly, the number of residual modules in the DeblurGAN network is changed, and an attempt is made to optimize the network structure to achieve better results in the blind recovery of motion-blurred images. Secondly, an image deblurring method based on the improved DeblurGAN network is proposed.This paper makes corresponding adjustments to the generator and discriminator networks to change their input and output size to 512 × 512 while keeping the overall structure of the network unchanged, and the experimental results show that the quality of the restored images has been greatly improved. In addition, the images were divided into four classes of images,to achieve improved restoration of blurred images with specific content. Experimental results on the benchmark GoPro dataset validate that the improved DeblurGAN achieves more than 1.5 dB improvement than DeblurGAN in terms of peak signal-to-noise ratio (PSNR) as trained utilizing the same amount of data, and structural similarity assessment (SSIM) evaluation means and maximum values increased between 0.2 and 0.3. The improved DeblurGAN is more effective in terms of both blur removal and detail recovery.},
  archive      = {J_IETIP},
  author       = {Wentao Ji and Xing Chen and Yihong Li},
  doi          = {10.1049/ipr2.12951},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {327-347},
  shortjournal = {IET Image Process.},
  title        = {Blind motion deblurring using improved DeblurGAN},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Entropy-guided contrastive learning for semi-supervised
medical image segmentation. <em>IETIP</em>, <em>18</em>(2), 312–326. (<a
href="https://doi.org/10.1049/ipr2.12950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately segmenting medical images is a critical step in clinical diagnosis and developing patient-specific treatment plans. While supervised learning algorithms have achieved excellent performance in this area, they require a large amount of annotated data, which is often time-consuming and difficult to obtain. As a result, semi-supervised learning (SSL) has gained attention as it has the potential to alleviate this challenge by using not only limited labelled data but also a large amount of unlabelled data. A common approach in SSL is to filter out high-entropy features and use the low-entropy part to compute unsupervised loss. However, it is believed that the high-entropy part is also beneficial for model training, and discarding it can lead to information loss. To address this issue, a simple yet efficient contrastive learning approach is proposed in this work for semi-supervised medical image segmentation, called Entropy-Guided Contrastive Learning Segmentation Network (EGCL-Net). The proposed method separates the low-entropy and high-entropy features via the average of predictions, using contrastive loss to pull the intra-class entropy representation distance close and push the inter-class entropy representation distance away. Extensive experiments on the automated cardiac diagnosis challenge dataset, COVID-19, and BraTS2019 datasets showed that: (1) EGCL-Net can significantly improve performance by utilizing high-entropy representation, and (2) the authors’ EGCL-Net outperforms recent state-of-the-art semi-supervised methods in both qualitative and quantitative evaluations.},
  archive      = {J_IETIP},
  author       = {Junsong Xie and Qian Wu and Renju Zhu},
  doi          = {10.1049/ipr2.12950},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {312-326},
  shortjournal = {IET Image Process.},
  title        = {Entropy-guided contrastive learning for semi-supervised medical image segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A CRNN-based method for chinese ship license plate
recognition. <em>IETIP</em>, <em>18</em>(2), 298–311. (<a
href="https://doi.org/10.1049/ipr2.12949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning methods cannot achieve satisfactory ship license plate (SLP) recognition due to the harsh marine environment, such as foggy weather, unstable ship state and small targets. Therefore, a convolutional recurrent neural network (CRNN)-based method is proposed for accurate SLP image recognition. Overall, the suggested method improves a CRNN recognition model by SLP image enhancement and data augmentation. The SLP image enhancement employs dark channel prior and Hough transform line detector to address the fog/blurriness and tilt issues existing in SLP images. As separate and joint operations, the two enhancements contribute to data augmentation for CRNN recognition. Preprocessing algorithms, including adaptive histogram equalization and image edge padding, are used to improve and unify the enlarged dataset for augmenting the CRNN model. As a final step, correction of the CRNN recognition results is made according to the character rule of SLPs, using an edit-distance algorithm to match against a pre-established SLP dictionary. A variety of real SLP images were collected to build an SLP image dataset for verification. The experimental results indicate that our method can reach an SLP recognition accuracy of , which is significantly superior to other text-based deep learning methods.},
  archive      = {J_IETIP},
  author       = {Fan Xu and Chuibin Chen and Zhigao Shang and Yuqing Peng and Xinbao Li},
  doi          = {10.1049/ipr2.12949},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {298-311},
  shortjournal = {IET Image Process.},
  title        = {A CRNN-based method for chinese ship license plate recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of intelligent ship marine object detection based
on RGB camera. <em>IETIP</em>, <em>18</em>(2), 281–297. (<a
href="https://doi.org/10.1049/ipr2.12959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents a comprehensive summary of Intelligent Ship Marine Object Detection (ISMOD) based on the RGB Camera. Marine object detection plays a pivotal role in enabling intelligent ships to acquire crucial data and security assurances for autonomous navigation. Among the various detection sensors, the RGB Camera is an informative and cost-effective tool with a wide range of civil applications. In the beginning, the ISMOD metrics based on the RGB camera is analyzed from three significant aspects, namely accuracy, speed, and robustness. Subsequently, the latest research status and comparative overview are presented, encompassing three mainstream detection methods: traditional detection, deep learning detection, and sensor fusion detection. Finally, the existing challenges of ISMOD are discussed and future development trends are recommended. The results demonstrate that forthcoming development will predominantly concentrate on deep learning approaches, complemented by other techniques. It is imperative to advance detection performance in domains such as deep fusion, multi-feature extraction, multi-fusion technology, and lightweight detection architecture.},
  archive      = {J_IETIP},
  author       = {Defu Yang and Mahmud Iwan Solihin and Yawen Zhao and Benchun Yao and Chaoran Chen and Bingyu Cai and Affiani Machmudah},
  doi          = {10.1049/ipr2.12959},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {281-297},
  shortjournal = {IET Image Process.},
  title        = {A review of intelligent ship marine object detection based on RGB camera},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to CMLocate: A cross-modal automatic visual
geo-localization framework for a natural environment without GNSS
information. <em>IETIP</em>, <em>18</em>(1), 280. (<a
href="https://doi.org/10.1049/ipr2.12947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  doi          = {10.1049/ipr2.12947},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {280},
  shortjournal = {IET Image Process.},
  title        = {Correction to CMLocate: A cross-modal automatic visual geo-localization framework for a natural environment without GNSS information},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepRoadNet: A deep residual based segmentation network for
road map detection from remote aerial image. <em>IETIP</em>,
<em>18</em>(1), 265–279. (<a
href="https://doi.org/10.1049/ipr2.12948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extraction of road networks is a critical activity in contemporary transportation networks. Deep neural networks have recently demonstrated excellent performance in the field of road segmentation. However, most of the convolutional neural network (CNN) based architectures could not verify their effectiveness in remote sensing images due to a smaller ratio of the targeted pixels, simple design, and fewer layers. In this study, a practical approach is assessed for road segmentation. The investigation was begun with basic encoder–decoder based segmentation models. Different state-of-the-art segmentation models like U-Net, V-Net, ResUNet and SegNet were used for road network detection experiments in this research. A robust model named DeepRoadNet, a more complicated alternative, is proposed by utilizing a pre-trained EfficientNetB7 architecture in the encoder and residual blocks as the decoder which mostly resembles the U-Net segmentation process. The proposed model has been trained, validated as well as tested using the high-resolution aerial image datasets and yielded good segmentation results with a mean intersection over union (mIoU) of 76%, a mean dice coefficient (mDC) of 73.18%, and an accuracy of 97.64% using Massachusetts road dataset. The proposed DeepRoadNet architecture overcomes the issues of lower mIoU, lower mDC, limited flexibility and interpretability already faced by existing models in the road segmentation field. The code is available at https://github.com/ Imteaz1998/DeepRoadNet.},
  archive      = {J_IETIP},
  author       = {Md. Imteaz Ahmed and Md. Foysal and Manisha Das Chaity and A. B. M. Aowlad Hossain},
  doi          = {10.1049/ipr2.12948},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {265-279},
  shortjournal = {IET Image Process.},
  title        = {DeepRoadNet: A deep residual based segmentation network for road map detection from remote aerial image},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAMNet: Global attention via multi-scale context for depth
estimation algorithm and application. <em>IETIP</em>, <em>18</em>(1),
247–264. (<a href="https://doi.org/10.1049/ipr2.12946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks significantly enhance the accuracy of the stereo-based disparity estimation. Some current methods suffer from inefficient use of the global context information, which will lead to the loss of structural details in ill-posed areas. To this end, a novel stereo network GAMNet is designed, composed of three core components (GDA, MPF, DCA) for estimating the depth prediction in challenging real-world environments. First, a lightweight attention module is presented, integrating the global semantic cues for every feature position across the channel and spatial dimensions. Next, the MPF module is constructed to fuse the diverse semantic and contextual information from different levels of the feature pyramid. Finally, cost volume is aggregated with a stacked encoder-decoder composed of the DCA module and 3D convolutions, filtering the transmission of matching clues and capturing the rich global contexts. Substantial experiments conducted on KITTI 2012, KITTI 2015, SceneFlow, and Middlebury-v3 datasets manifest that GAMNet surpasses preceding methods with contour-preserving disparity predictions. In addition, the first 3D scene reconstruction linear evaluation strategy on spatial grasping points for the end-to-end stereo networks in an unsupervised mode is proposed, and it is deployed on the designed robot vision-guided system. In application experiments, the method can produce densely high-precision 3D reconstructions to implement the grasping task in complex real-world scenes, and achieve excellent robust performance with competitive inference efficiency.},
  archive      = {J_IETIP},
  author       = {Huitong Yang and Liang Lei and Haiwei Sang},
  doi          = {10.1049/ipr2.12946},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {247-264},
  shortjournal = {IET Image Process.},
  title        = {GAMNet: Global attention via multi-scale context for depth estimation algorithm and application},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overcomplete graph convolutional denoising autoencoder for
noisy skeleton action recognition. <em>IETIP</em>, <em>18</em>(1),
233–246. (<a href="https://doi.org/10.1049/ipr2.12944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current skeleton-based action recognition methods usually assume the input skeleton is complete and noise-free. However, it is inevitable that the captured skeletons are incomplete due to occlusions or noisy due to changes in the environment. When dealing with these data, even State Of The Art (SOTA) recognition backbones experience significant degradation in recognition accuracy. Though a few methods have been proposed to address this issue, they still lack flexibility, efficiency and interpretability. In this work, an overcomplete Graph Convolutional Denoising Autoencoder (GCDAE) is proposed which can act as a flexible preprocessing module for pretrained recognition backbones and improve their robustness. Taking advantages of the overcomplete and fully graph convolutional structure, GCDAE is able to rectify noisy joints while keeping information of unspoiled details efficiently. On two large scale skeleton datasets NTU RGB+D 60 and 120, the introducing of GCDAE brings significant robustness improvements to SOTA backbones towards different types of noises.},
  archive      = {J_IETIP},
  author       = {Jiajun Guo and Qingge Ji and Guangwei Shan},
  doi          = {10.1049/ipr2.12944},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {233-246},
  shortjournal = {IET Image Process.},
  title        = {Overcomplete graph convolutional denoising autoencoder for noisy skeleton action recognition},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Triple-loss driven generative adversarial network for
pansharpening. <em>IETIP</em>, <em>18</em>(1), 211–232. (<a
href="https://doi.org/10.1049/ipr2.12943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pansharpening aims at fusing a panchromatic (PAN) image and a low-resolution multispectral (LRMS) image into a high-resolution multispectral (HRMS) image. In recent years, GAN-based pansharpening methods have achieved excellent results, but they suffer from inadequate feature preservation and unstable training. To address these issues, a novel GAN-based model named TriLossGAN is proposed. This method constructs three loss components with the help of the generator and the dual-discriminator, which are calculated in both the original spatial domain and the transform domain to better preserve high-frequency and low-frequency information in the fused image. Additionally, a new training strategy is designed to stabilize the training process. In extensive experiments, the proposed method achieved satisfactory results on three datasets with QNR values of 0.9584 on GaoFen-2, 0.9601 on QuickBird, and 0.9138 on WorldView-3. Qualitative and quantitative comparisons demonstrate that TriLossGAN outperforms other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Bo Huang and Xiongfei Li and Xiaoli Zhang},
  doi          = {10.1049/ipr2.12943},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {211-232},
  shortjournal = {IET Image Process.},
  title        = {Triple-loss driven generative adversarial network for pansharpening},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new hierarchical algorithm based on CapsGAN for imbalanced
image classification. <em>IETIP</em>, <em>18</em>(1), 194–210. (<a
href="https://doi.org/10.1049/ipr2.12942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced image datasets consist of image datasets where there is a significant disparity in the number of samples across different classes. With imbalanced image datasets, learning algorithms often tend to be biased toward the majority class samples. This leads to poor classification of minority class samples as their training is not properly conducted. It becomes more complicated when the number of samples in the minority class is very low. In this paper, a novel hierarchical algorithm is proposed for generating new data using Capsule Generative Adversarial Networks (CapsGAN) to address the class imbalance problem in imbalanced image datasets. Unlike common GAN models, the proposed method incorporates an auxiliary CapsNet to identify high-value images in both minority and majority classes. This identification is based on the ability to detect complex relationships between low-level and high-level features present in capsule networks. Furthermore, the proposed CapsGAN model is conditioned to generate minority class samples based on feature vectors of last capsule layer to achieve a more balanced image dataset. For evaluating the performance of the proposed model, an image dataset called CICS was collected and introduced. Extensive experiments were also conducted using various online image datasets from different domains, with varying numbers of classes and data sizes. The experimental results demonstrated that the proposed model can generate high-quality samples in cases where the image dataset or the number of minority class samples is relatively small. Furthermore, the proposed model has maintained an accuracy of over 80% in an imbalanced ratio of 1:60.},
  archive      = {J_IETIP},
  author       = {Hamed Jabbari and Nooshin Bigdeli},
  doi          = {10.1049/ipr2.12942},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {194-210},
  shortjournal = {IET Image Process.},
  title        = {A new hierarchical algorithm based on CapsGAN for imbalanced image classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage coarse-to-fine method for pathological images in
medical decision-making systems. <em>IETIP</em>, <em>18</em>(1),
175–193. (<a href="https://doi.org/10.1049/ipr2.12941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence decision systems play an important supporting role in the field of medical information. Medical image analysis is an important part of decision systems and an even more important part of medical diagnosis and treatment. The wealth of cellular information in histopathological images makes them a reliable means of diagnosing tumors. However, due to the large size, high resolution, and complex background structure of pathology images, deep learning methods still have various difficulties in the recognition of pathology images. Based on this, this study proposes a two-stage continuous improvement-based approach for pathology image recognition in medical decision systems. For pathology images with complex backgrounds, normalization and enhancement is performed to remove the effects of noise color, and light-dark inconsistencies on the segmentation network. The continuous refinement PSP Net (CRPSPNet) is then designed for accurate recognition of the pathology images. CRPSPNet is divided into two stages: Pyramid Scene Parsing Network segmentation to obtain coarse segmentation results; and continuous refinement model refines the results of the first stage. Experiments using more than 1,000 osteosarcoma pathology images have shown that the method gives more accurate results with fewer computer resources and processing time than traditional optimization models. Its Intersection over Union achieves 0.76.},
  archive      = {J_IETIP},
  author       = {Keke He and Jun Zhu and Limiao Li and Fangfang Gou and Jia Wu},
  doi          = {10.1049/ipr2.12941},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {175-193},
  shortjournal = {IET Image Process.},
  title        = {Two-stage coarse-to-fine method for pathological images in medical decision-making systems},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Road damage detection with bounding box and generative
adversarial networks based augmentation methods. <em>IETIP</em>,
<em>18</em>(1), 154–174. (<a
href="https://doi.org/10.1049/ipr2.12940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, based on the data augmentation techniques of bounding box augmentation and the road damage generative adversarial network based augmentation, a robust road damage detection method has been presented. To this end, first, Iran road damage dataset has been collected by means of a dashboard-installed mobile phone. After processing these images by the blind referenceless image spatial quality evaluator technique, the substandard and inferior data have been automatically eliminated. In the second step, based on the YOLOv5 with several different baseline models, an algorithm has been developed for detecting the road surface damages. In the third step, by using the traditional as well as the bounding box augmentation and road damage generative adversarial network based augmentation techniques, the precision and the robustness of road damage detector under different environmental and field conditions have been improved. Finally, through the ensemble of the best models, the final detector accuracy has been enhanced. The findings of this article indicate that by using the proposed approach, the values of mAP and F1-score are improved by 13.79% and 7.58%, respectively. The dataset and parts of the code are available at: https://github.com/IranRoadDamageDataset/IRRDD .},
  archive      = {J_IETIP},
  author       = {Nima Aghayan-Mashhady and Abdollah Amirkhani},
  doi          = {10.1049/ipr2.12940},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {154-174},
  shortjournal = {IET Image Process.},
  title        = {Road damage detection with bounding box and generative adversarial networks based augmentation methods},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy c-means clustering algorithm based on superpixel
merging and multi-feature adaptive fusion measurement. <em>IETIP</em>,
<em>18</em>(1), 140–153. (<a
href="https://doi.org/10.1049/ipr2.12939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy C-means clustering (FCM) algorithm is widely used in greyscale and colour image segmentation, especially in real colour images. However, in the process of interested regions extraction, it performs barely satisfactory due to the use of single distance metric in traditional FCM. In order to address the issue, a fuzzy C-means clustering algorithm based on superpixel merging and multi-feature adaptive fusion measurement (FCM-SM) are proposed. First of all, since superpixel image adapts well to irregular image boundaries, the FCM algorithm is elevated to the superpixel level. Moreover, the superpixel merging module based on colour similarity is introduced to improve the traditional watershed transformation by incorporating more gradient and intensity information. Finally, the multi-feature adaptive fusion measurement is designed in the clustering process to incorporate multiple sources of spatial information, which comprehensively measures the information from different fields to enhance the segmentation capability further. Experiments performed on Berkeley Segmentation Dataset and Brain Tumor Segmentation Dataset demonstrate that the proposed algorithm provides better segmentation results than benchmarks during the object extraction.},
  archive      = {J_IETIP},
  author       = {Xie Zeyu and Luo Xiao and Zhao Defang and Chen Xinyu},
  doi          = {10.1049/ipr2.12939},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {140-153},
  shortjournal = {IET Image Process.},
  title        = {Fuzzy C-means clustering algorithm based on superpixel merging and multi-feature adaptive fusion measurement},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High performance image steganography integrating IWT and
hamming code within secret sharing. <em>IETIP</em>, <em>18</em>(1),
129–139. (<a href="https://doi.org/10.1049/ipr2.12938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steganography and secret sharing are schemes to increase the security of private information against attackers. Steganography emphasizes secrecy while secret sharing distributes the secret key in shares that are conditionally classified to reconstruct the original secret. This paper introduces a counting-based secret sharing scheme that aims to reduce the computational complexity for longer keys, thereby providing a practical steganographic approach for efficient sharing. The scheme integrates counting-based secret sharing with integer wavelet transform (IWT) and steganography. The subscriptions created using the Hamming code are embedded in the cover image. Using this method, IWT significantly reduces the occurrence of common rounding errors. As a result, secret key extraction becomes very accurate and eliminates the need to access the original images. This design, using secret sharing based on counting, not only has simplicity and efficiency, but also features such as flexibility, scalability, and lack of central authority. In addition, high-quality steganography was obtained for keys with lengths of 64, 256, 512, 1024, and 3072, showing an average PSNR=80.11 in different color images. This outstanding performance makes it a highly efficient alternative to previous designs, representing a groundbreaking contribution with significant public interest in the field.},
  archive      = {J_IETIP},
  author       = {Zahra Saeidi and Ameneh Yazdi and Samaneh Mashhadi and Masoud Hadian and Adnan Gutub},
  doi          = {10.1049/ipr2.12938},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {129-139},
  shortjournal = {IET Image Process.},
  title        = {High performance image steganography integrating IWT and hamming code within secret sharing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust watermarking approach against high-density salt and
pepper noise (RWSPN) to enhance medical image security. <em>IETIP</em>,
<em>18</em>(1), 116–128. (<a
href="https://doi.org/10.1049/ipr2.12937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a robust watermarking method against high-density salt and pepper noise attacks. Automatic region of interest (ROI) detection, embedding encoded data, removing different densities of noise, data extraction by omitting, and labeling the noisy pixels of Region of Non-Interest (RONI), and decoding the extracted data using ROI pixel information as a key, are various steps of the presented scheme. The automatic ROI detection method separates the RONI from ROI with four vertices of the smallest rectangle, for the embedding process. The encoded watermark data is embedded into the least significant bits of RONI in four neighbour pixels. Adaptive Removal of high-density Salt and pepper Noise method can enhance image quality and reduce the effect of salt and pepper noise attacks. The embedded information is preserved from destruction if the host image is impaired through the power of robustness. The best results are obtained through the action of extracting the watermark from RONI pixels, utilizing the same ROI detection method. Omitting and labeling the noisy pixels of the RONI will ensure healthy extracted watermark data, leading to decreased Bit Error Rate (BER) values. Finally, these data are interpreted using the key of ROI pixels, and the watermark data is decoded and retrieved. Due to salt and pepper noise obliterating pixel bits and their corresponding transform coefficients in the transform domain, the spatial domain is employed to enhance robustness against such attacks. The results show the high performance of the presented scheme. The average BER value for five MRI databases in a 97% salt and pepper noise attack is 38.6.},
  archive      = {J_IETIP},
  author       = {Javad Ebrahimnejad and Alireza Naghsh and Hossein Pourghasem},
  doi          = {10.1049/ipr2.12937},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {116-128},
  shortjournal = {IET Image Process.},
  title        = {A robust watermarking approach against high-density salt and pepper noise (RWSPN) to enhance medical image security},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fusion-attention swin transformer for cardiac MRI image
segmentation. <em>IETIP</em>, <em>18</em>(1), 105–115. (<a
href="https://doi.org/10.1049/ipr2.12936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For semantic segmentation of cardiac magnetic resonance image (MRI) with low recognition and high background noise, a fusion-attention Swin Transformer is proposed based on cognitive science and deep learning methods. It has a U-shaped symmetric encoding–decoding structure with an attention-based skip connection. The encoder realizes self-attention for deep feature representation and the decoder up-samples global features to the corresponding input resolution for pixel-level segmentation. By introducing a skip connection between the encoder and decoder based on fusion attention, the remote interaction of global information is realized, and the attention to local features and specific channels is enhanced. A public ACDC cardiac MRI image dataset is used for experiments. The segmentation of the left ventricle, right ventricle, and myocardial layer is realized. The method performs well on a small sample dataset, for example, the pixel accuracy obtained by the proposed model is 93.68%, the Dice coefficient is 92.28%, and HD coefficient is 11.18. Compared with the state-of-the-art models, the segmentation precision has been significantly improved, especially for the low recognition and heavily occluded targets.},
  archive      = {J_IETIP},
  author       = {Ruiping Yang and Kun Liu and Yongquan Liang},
  doi          = {10.1049/ipr2.12936},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {105-115},
  shortjournal = {IET Image Process.},
  title        = {A fusion-attention swin transformer for cardiac MRI image segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FSSDD: Few-shot steel defect detection based on multi-scale
semantic enhancement representation and mask category information
mapping. <em>IETIP</em>, <em>18</em>(1), 88–104. (<a
href="https://doi.org/10.1049/ipr2.12935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steel defect detection is important for industry production as it is tied to the product quality and production efficiency. However, previous steel defect detection methods based on deep convolutional neural networks heavily rely on large-scale data for training and tend to have poor generalization ability for a novel defect category. In this paper, a novel few-shot steel defect detection model based on multi-scale semantic enhancement representation and mask category information mapping is introduced, where only a few annotated samples are acquired for the novel defect category. More concretely, three main components are built: an information-guidance enhanced multi-head detector is proposed to improve the representation of information in meta-feature maps, a mask category representation module is designed to enhance the category feature representation of the mask region in the support set, and a novel multi-scale category edge loss function is designed to assist the generation of category reweighting vector. Extensive experiments on the North-east University few-shot steel defect data set demonstrate that the proposed method significantly outperforms the state-of-the-art methods and verify its effectiveness through ablation studies.},
  archive      = {J_IETIP},
  author       = {Zhoufeng Liu and Zijing Guo and Chunlei Li and Ning Huang and Chengli Gao},
  doi          = {10.1049/ipr2.12935},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {88-104},
  shortjournal = {IET Image Process.},
  title        = {FSSDD: Few-shot steel defect detection based on multi-scale semantic enhancement representation and mask category information mapping},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid attention mechanism of feature fusion for medical
image segmentation. <em>IETIP</em>, <em>18</em>(1), 77–87. (<a
href="https://doi.org/10.1049/ipr2.12934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional convolution neural networks (CNN) have achieved good performance in multi-organ segmentation of medical images. Due to the lack of ability to model long-range dependencies and correlations between image pixels, CNN usually ignores the information of channel dimension. To further improve the performance of multi-organ segmentation, a hybrid attention mechanism model is proposed. First, a CNN was used to extract multi-scale feature maps and fed into the Channel Attention Enhancement Module (CAEM) to selectively pay attention to target organs in medical images, and the Transformer encoded tokenized image patches from CNN feature maps as the input sequence to model long-range dependencies. Second, the decoder upsampled the output from Transformer and fused with the CAEM features in multi-scale through skip connections. Finally, we introduced a Refinement Module (RM) after the decoder to improve feature correlations of the same organ and the feature discriminability between different organs. The model outperformed on dice coefficient (%) and hd95 on both the synapse multi-organ segmentation and cardiac diagnosis challenge datasets. The hybrid attention mechanisms exhibited high efficiency and high segmentation accuracy in medical images.},
  archive      = {J_IETIP},
  author       = {Shanshan Tong and Zhentao Zuo and Zuxiang Liu and Dengdi Sun and Tiangang Zhou},
  doi          = {10.1049/ipr2.12934},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {77-87},
  shortjournal = {IET Image Process.},
  title        = {Hybrid attention mechanism of feature fusion for medical image segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pix2PixSSR: Spatial super-resolution synthesis and
visualization for time-varying volumetric data. <em>IETIP</em>,
<em>18</em>(1), 59–76. (<a
href="https://doi.org/10.1049/ipr2.12933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upscaling of the time-varying volume data is significant, since it can be used in in situ visualization to help scientists fast analyse complex simulations relevant to time-varying volume data. A novel deep learning method called Pix2Pix spatial super-resolution (Pix2PixSSR), which can be used to generate spatial super-resolution of the time-varying volume data is proposed here. It consists of two main components: One is a variant UNet-like generator that takes the low resolution volume sequence as input and generates the high resolution counterparts; one is a PatchGAN discriminator that takes both low and high resolution volume sequences as input and predicts their realness. To validate its advantages, we qualitatively and quantitatively compare it with the state-of-the-art upscaling techniques. More specifically, two experiments are performed. The first experiment uses the same variable of a time-varying volume dataset for training and inference, while the second experiment uses different variables for training and inference. The experimental results show that for most cases, the Pix2PixSSR can generate the most similar super-resolution to the ground truth, compared to the state-of-the-art techniques.},
  archive      = {J_IETIP},
  author       = {Ji Ma and Jinjin Chen},
  doi          = {10.1049/ipr2.12933},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {59-76},
  shortjournal = {IET Image Process.},
  title        = {Pix2PixSSR: Spatial super-resolution synthesis and visualization for time-varying volumetric data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based methods for detecting defects in cast
iron parts and surfaces. <em>IETIP</em>, <em>18</em>(1), 47–58. (<a
href="https://doi.org/10.1049/ipr2.12932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large size tolerance and positional differences of burrs in cast iron blanks make it easy for traditional teaching polishing paths to cause overcutting or undercutting. Rapid and accurate identification of burrs and real-time correction of polishing trajectories are key technical issues for achieving high-precision polishing. Here, a deep learning-based method for defect detection in cast iron parts and surfaces is proposed. Firstly, a self-made dataset of cast iron parts and surface defects is created and annotated, and a variety of data augmentation methods are used to expand the number of samples in the original dataset, alleviating the problem of small sample size. Then, the coordinate attention mechanism is introduced into the backbone network to allocate more attention to the defect target. Finally, the bidirectional weighted feature pyramid network (BiFPN) is used in the feature fusion network to replace the original path aggregation network, improving the model&#39;s ability to fuse features of different sizes. Experimental results show that compared with the original model, the mean average precision (mAP) is increased by 3.1%, and the average precision (AP) in defect classification is increased by 7.6%, with an FPS of 112, achieving accurate and efficient real-time detection of cast iron parts and surface defects.},
  archive      = {J_IETIP},
  author       = {Pengyu Wang and Peng Jing},
  doi          = {10.1049/ipr2.12932},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {47-58},
  shortjournal = {IET Image Process.},
  title        = {Deep learning-based methods for detecting defects in cast iron parts and surfaces},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-source to multi-target domain adaptation method based
on similarity measurement. <em>IETIP</em>, <em>18</em>(1), 34–46. (<a
href="https://doi.org/10.1049/ipr2.12931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing domain adaption methods solve the problem of data distribution similarity in single-source to single-target domain or multi-source to single-target domain adaption, but the more realistic multi-source to multi-target domain transfer scenarios are ignored. At the same time, less attention is paid to the similarity of task distribution among domains and the corresponding relationship between coarse-grained and fine-grained, which leads to the problem of low classification accuracy. In this paper, a multi-source to multi-target domain adaption (MSMTDA) method based on similarity measurement is proposed. Firstly, a cross-domain similarity measurement mechanism based on the idea of data distribution similarity and task distribution similarity is set up to judge and optimize the distribution difference among source domains and target domains. Secondly, global-local-bidirectional alignment is used to solve the problem of poor transfer performance in the existing distributed alignment. Then, the cross-domain mutual learning strategy is proposed, which reduces classifier discrepancy caused by decision boundaries and distribution difference in multiple domains. Finally, the proposed method is verified on three benchmark datasets in image classification. The experimental results show that the proposed method performs better in both classification accuracy and training time when compared with the conventional method.},
  archive      = {J_IETIP},
  author       = {Lan Wu and Han Wang and Yuan Yao},
  doi          = {10.1049/ipr2.12931},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {34-46},
  shortjournal = {IET Image Process.},
  title        = {Multi-source to multi-target domain adaptation method based on similarity measurement},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-supervised causal feature reinforcement learning
method for non-invasive hemoglobin prediction. <em>IETIP</em>,
<em>18</em>(1), 22–33. (<a
href="https://doi.org/10.1049/ipr2.12930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anemia (hemoglobin (Hb) &lt; 12.0 g/dL) is significantly correlated with many diseases. An invasive technique is the peripheral blood Hb detection method, which is used to examine red and white blood cells and platelets in clinical laboratory settings. However, non-invasive methods for measuring Hb mainly include low-precision prediction based on eye images and complex operation prediction based on fundus images. Moreover, these types of anemia testing techniques are time-consuming, tedious, or prone to errors. Thus, developing a convenient and high-precision method is vital for predicting Hb concentration. This study proposes self-supervised causal features using actor-critical reinforcement learning to improve the model prediction performance. Two networks are proposed: Actor Predictor and Hemoglobin Predictor to predict Hb concentration. Moreover, the model performance is evaluated using different techniques, namely, Mean Absolute Error (MAE) and Mean Square Error (MSE), via real eye image data and a smartphone. This model achieved 1.19(1.01,1.38) on the MAE and 2.25(1.59,2.90) on the MSE, which outperformed previous eye images&#39; Hb prediction methods and was nearly similar to the fundus images&#39; Hb prediction methods. The inference time was less than 0.05 s, making it efficient and accurate for predicting Hb. This model can be used for mobile deployment and health self-screening.},
  archive      = {J_IETIP},
  author       = {Linquan Xu and Yuwen Chen and Songmei Lu and Kunhua Zhong and Yujie Li and Bin Yi},
  doi          = {10.1049/ipr2.12930},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {22-33},
  shortjournal = {IET Image Process.},
  title        = {A self-supervised causal feature reinforcement learning method for non-invasive hemoglobin prediction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SE-swin: An improved swin-transfomer network of
self-ensemble feature extraction framework for image retrieval.
<em>IETIP</em>, <em>18</em>(1), 13–21. (<a
href="https://doi.org/10.1049/ipr2.12929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Swin-Transformer is a variant of the Vision Transformer, which constructs a hierarchical Transformer that computes representations with shifted windows and window multi-head self-attention. This method can handle the scale invariance problem and performs well in many computer vision tasks. In image retrieval, high-quality feature descriptors are necessary to improve retrieval accuracy. This paper proposes a self-ensemble Swin-Transformer network structure to fuse the features of different layers of the Swin-Transformer network, eliminating noise points present in a single layer, and improving the retrieval effect. Two experiments were conducted, one on the In-shop Clothes Retrieval dataset and another on the Stanford Online Product dataset. The experiments showed that the proposed method significantly increased the retrieval effect of features extracted using Vision Transformer, surpassing previous state-of-the-art image retrieval methods. In the second experiment, the feature map of the trained model was visualized, revealing that the improved network significantly reduces focus on some noise points and enhances focus on image features compared to the original network.},
  archive      = {J_IETIP},
  author       = {Yixuan Xu and Xianbing Wang and Hua Zhang and Hai Lin},
  doi          = {10.1049/ipr2.12929},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {13-21},
  shortjournal = {IET Image Process.},
  title        = {SE-swin: An improved swin-transfomer network of self-ensemble feature extraction framework for image retrieval},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MCDC-net: Multi-scale forgery image detection network based
on central difference convolution. <em>IETIP</em>, <em>18</em>(1), 1–12.
(<a href="https://doi.org/10.1049/ipr2.12928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) emerged thanks to the development of deep neural networks. Forgery images generated by various variants of GANs are widely spread on the Internet, which may be damage personal credibility and cause huge property losses. Thus, numerous methods are proposed to detect forgery images, but most of them are designed to detect forgery faces. Therefore, a method to detect forgery images of various scenes is proposed. In this work, central difference convolution and vanilla convolution (CDC-Mix) are mixed after considering the depth and width features of neural networks and analyzing the influence of attention on network performance. Based on CDC-Mix, a separable convolution (SeparableCDC-Mix) is proposed. The proposed method consists of three parts: (1) CDC-Mix and SeparableCDC-Mix are used to extract the gradient information and texture features; (2) CDCM is used to extract the multi-scale information of the image; (3) multi-scale fusion module (MS-Fusion) is used to fuse the multi-scale information from different locations of the network. A large number of experiments have been carried out on several datasets generated by GAN, and the experimental results show that the proposed method has a great improvement compared with the existing advanced methods.},
  archive      = {J_IETIP},
  author       = {Defen He and Qian Jiang and Xin Jin and Zien Cheng and Shuai Liu and Shaowen Yao and Wei Zhou},
  doi          = {10.1049/ipr2.12928},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {IET Image Process.},
  title        = {MCDC-net: Multi-scale forgery image detection network based on central difference convolution},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
