<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EXSY_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="exsy---275">EXSY - 275</h2>
<ul>
<li><details>
<summary>
(2024d). RETRACTION: Natural language processing with deep learning
enabled hybrid content retrieval model for digital library management.
<em>EXSY</em>, <em>41</em>(12), e13753. (<a
href="https://doi.org/10.1111/exsy.13753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : M. Ragab , A. Almuhammadi , R. F. Mansour and S. Kadry , “ Natural Language Processing With Deep Learning Enabled Hybrid Content Retrieval Model for Digital Library Management ,” Expert Systems 41 , no. 6 ( 2024 ): e13135, https://doi.org/10.1111/exsy.13135 . The above article, published online on 13 September 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley &amp; Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted on the basis of a compromised peer review process. The editors have therefore decided to retract this article. The authors disagree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13753},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13753},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Natural language processing with deep learning enabled hybrid content retrieval model for digital library management},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). RETRACTION: A predictive typological content retrieval
method for real-time applications using multilingual natural language
processing. <em>EXSY</em>, <em>41</em>(12), e13752. (<a
href="https://doi.org/10.1111/exsy.13752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : S. Baskar , S. Dhote , T. Dhote , G. Jayanandini , D. Akila and S. Doss , “ A Predictive Typological Content Retrieval Method for Real-time Applications Using Multilingual Natural Language Processing ,” Expert Systems 41 , no. 6 ( 2024 ): e13172, https://doi.org/10.1111/exsy.13172 . The above article, published online on 28 October 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley &amp; Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was not reviewed in line with the journal&#39;s peer review standards. The editors have therefore decided to retract this article. The authors disagree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13752},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13752},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: A predictive typological content retrieval method for real-time applications using multilingual natural language processing},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). RETRACTION: Optimization using internet of agent based
stacked sparse autoencoder model for heart disease prediction.
<em>EXSY</em>, <em>41</em>(12), e13751. (<a
href="https://doi.org/10.1111/exsy.13751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION: V. Baviskar , M. Verma , P. Chatterjee , G. Singal and T. R. Gadekallu , “ Optimization Using Internet of Agent Based Stacked Sparse Autoencoder Model for Heart Disease Prediction ,” Expert Systems (Early View): e13359, https://doi.org/10.1111/exsy.13359 . The above article, published online on 10 June 2023 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley &amp; Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that parts of the methods in the article lack sufficient detail such that the research cannot be reproduced. A relevant discussion and discrimination for different cardiovascular diseases is missing. The editors have therefore decided to retract this article. The authors disagree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13751},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13751},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Optimization using internet of agent based stacked sparse autoencoder model for heart disease prediction},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). RETRACTION: Internet of agents system for age and gender
classification using grasshopper optimization with deep convolution
neural networks. <em>EXSY</em>, <em>41</em>(12), e13746. (<a
href="https://doi.org/10.1111/exsy.13746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : A. K. Dutta , B. Qureshi , Y. Albagory , M. Alsanea , D. Gupta , and A. Khanna , “ Internet of Agents System for Age and Gender Classification Using Grasshopper Optimization With Deep Convolution Neural Networks ,” Expert Systems 40 , no. 4 ( 2023 ): e13115. https://doi.org/10.1111/exsy.13115 . The above article, published online on 02 August 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley &amp; Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that parts of the methods in the article lack sufficient detail such that the research cannot be reproduced. Furthermore, images of individuals have been used in figures 1, 3, 4 and 5 without any information provided regarding copyright and consent to use images. The editors have therefore decided to retract this article.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13746},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13746},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Internet of agents system for age and gender classification using grasshopper optimization with deep convolution neural networks},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). RETRACTION: Hybrid multi agent optimization for optimal
battery storage using micro grid. <em>EXSY</em>, <em>41</em>(12),
e13743. (<a href="https://doi.org/10.1111/exsy.13743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : N. Bacanin, “Hybrid Multi Agent Optimization for Optimal Battery Storage Using Micro Grid,” Expert Systems 40, no. 4 (2023): e12995. https://doi.org/10.1111/exsy.12995 . The above article, published online on 14 March 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the author, N. Bacanin; the journal Editor-in-Chief, David Camacho; and John Wiley &amp; Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted on the basis of a compromised peer review process. Furthermore, parts of the methods and figures in the article lack sufficient detail such that the research cannot be reproduced. Therefore the decision to retract this article was taken.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13743},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13743},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Hybrid multi agent optimization for optimal battery storage using micro grid},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ACRES: A framework for (semi)automatic generation of
rule-based expert systems with uncertainty from datasets. <em>EXSY</em>,
<em>41</em>(12), e13723. (<a
href="https://doi.org/10.1111/exsy.13723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, the design of an expert system involves acquiring knowledge, in the form of symbolic rules, directly from the expert(s), which is a complex and time-consuming task. Although expert systems approach is quite old, it is still present, especially where explicit knowledge representation and reasoning, which assure interpretability and explainability, are necessary. Therefore, machine learning methods have been devised to extract rules from data, to facilitate that task. However, those methods are quite inflexible in adapting to the application domain and provide no help in designing the expert system. In this work, we present a framework and corresponding tool, namely ACRES, for semi-automatically generating expert systems from datasets. ACRES allows for data preprocessing, which helps in structuring knowledge in the form of a tree, called rule hierarchy, which represents (possible) dependencies among data variables and is used for rule formation. This improves interpretability and explainability of the produced systems. We have also designed and evaluated alternative methods for rule extraction from data and for calculation and use of certainty factors, to represent uncertainty; CFs can be dynamically updated. Experimental results on seven well-known datasets show that the proposed rule extraction methods are comparable to other popular machine learning approaches like decision trees, CART, JRip, PART, Random Forest, and so on, for the classification task. Finally, we give insights on two applications of ACRES.},
  archive      = {J_EXSY},
  author       = {Konstantinos Kovas and Ioannis Hatzilygeroudis},
  doi          = {10.1111/exsy.13723},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13723},
  shortjournal = {Expert Syst.},
  title        = {ACRES: A framework for (semi)automatic generation of rule-based expert systems with uncertainty from datasets},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-step multiple kernel k-means clustering based on block
diagonal representation. <em>EXSY</em>, <em>41</em>(12), e13720. (<a
href="https://doi.org/10.1111/exsy.13720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel k -means clustering (MKKC) can efficiently incorporate multiple base kernels to generate an optimal kernel. Many existing MKKC methods all need two-step operation: learning clustering indicator matrix and performing clustering on it. However, the optimal clustering results of two steps are not equivalent to those of original problem. To address this issue, in this paper we propose a novel method named one-step multiple kernel k -means clustering based on block diagonal representation (OS-MKKC-BD). By imposing a block diagonal constraint on the product of indicator matrix and its transpose, this method can encourage the indicator matrix to be block diagonal. Then the indicator matrix can produce explicit clustering indicator, so as to implement one-step clustering, which avoids the disadvantage of two-step operation. Furthermore, a simple kernel weighting strategy is used to obtain an optimal kernel, which boosts the quality of optimal kernel. In addition, a three-step iterative algorithm is designed to solve the corresponding optimization problem, where the Riemann conjugate gradient iterative method is used to solve the optimization problem of the indicator matrix. Finally, by extensive experiments on eleven real data sets and comparison of clustering results with 10 MKC methods, it is concluded that OS-MKKC-BD is effective.},
  archive      = {J_EXSY},
  author       = {Cuiling Chen and Zhi Li},
  doi          = {10.1111/exsy.13720},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13720},
  shortjournal = {Expert Syst.},
  title        = {One-step multiple kernel k-means clustering based on block diagonal representation},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlocking the potential: A review of artificial intelligence
applications in wind energy. <em>EXSY</em>, <em>41</em>(12), e13716. (<a
href="https://doi.org/10.1111/exsy.13716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive review of the most recent papers and research trends in the fields of wind energy and artificial intelligence. Our study aims to guide future research by identifying the potential application and research areas of artificial intelligence and machine learning techniques in the wind energy sector and the knowledge gaps in this field. Artificial intelligence techniques offer significant benefits and advantages in many sub-areas, such as increasing the efficiency of wind energy facilities, estimating energy production, optimizing operation and maintenance, providing security and control, data analysis, and management. Our research focuses on studies indexed in the Web of Science library on wind energy between 2000 and 2023 using sub-branches of artificial intelligence techniques such as artificial neural networks, other machine learning methods, data mining, fuzzy logic, meta-heuristics, and statistical methods. In this way, current methods and techniques in the literature are examined to produce more efficient, sustainable, and reliable wind energy, and the findings are discussed for future studies. This comprehensive evaluation is designed to be helpful to academics and specialists interested in acquiring a current and broad perspective on the types of uses of artificial intelligence in wind energy and seeking what research subjects are needed in this field.},
  archive      = {J_EXSY},
  author       = {Safa Dörterler and Seyfullah Arslan and Durmuş Özdemir},
  doi          = {10.1111/exsy.13716},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13716},
  shortjournal = {Expert Syst.},
  title        = {Unlocking the potential: A review of artificial intelligence applications in wind energy},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial electric field algorithm with repulsion
mechanism. <em>EXSY</em>, <em>41</em>(12), e13715. (<a
href="https://doi.org/10.1111/exsy.13715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its outstanding performance in addressing optimization problems, artificial electric field (AEF) algorithm has garnered increasing notice in recent years. Nevertheless, numerous studies indicate that AEF is susceptible to premature convergence when the region influenced by the global optimum constitutes a small fraction of the entire solution space. By conducting micro-level research on the particles during the evolution process of AEF, it is revealed that the primary factors influencing optimization performance are the Coulomb&#39;s electrostatic force mechanism and the fixed attenuation factor. Inspired by this observation, we propose an improved version named artificial electric field algorithm with repulsion mechanism (RMAEF). Specifically, in RMAEF, a repulsion mechanism is incorporated to make particles escape from local optima. Furthermore, an adaptive attenuation factor is employed to update dynamically Coulomb&#39;s constant. RMAEF is compared with AEF and its state-of-art variants under 44 test functions from CEC 2005 and CEC 2014 test suites. From the experiment results, it is obvious that among 14 benchmark functions from CEC 2005 on 30D and 50D optimization, the RMAEF algorithm exhibits superior performance on 8 and 9 functions compared with advanced variants of AEF. For CEC 2014 on 30D and 50D optimization, the RMAEF algorithm produces the best results on 11 and 12 functions, respectively. In addition, three real-world problems are also used to verify the versatility and robustness. The results demonstrate that RMAEF outperforms its competitors in terms of overall performance.},
  archive      = {J_EXSY},
  author       = {Gengfei Zhang and Jiatang Cheng},
  doi          = {10.1111/exsy.13715},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13715},
  shortjournal = {Expert Syst.},
  title        = {Artificial electric field algorithm with repulsion mechanism},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TensorCRO: A TensorFlow-based implementation of a
multi-method ensemble for optimization. <em>EXSY</em>, <em>41</em>(12),
e13713. (<a href="https://doi.org/10.1111/exsy.13713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel implementation of the Coral Reef Optimization with Substrate Layers (CRO-SL) algorithm. Our approach, which we call TensorCRO, takes advantage of the TensorFlow framework to represent CRO-SL as a series of tensor operations, allowing it to run on GPU and search for solutions in a faster and more efficient way. We evaluate the performance of the proposed implementation across a wide range of benchmark functions commonly used in optimization research (such as the Rastrigin, Rosenbrock, Ackley, and Griewank functions), and we show that GPU execution leads to considerable speedups when compared to its CPU counterpart. Then, when comparing TensorCRO to other state-of-the-art optimization algorithms (such as the Genetic Algorithm, Simulated Annealing, and Particle Swarm Optimization), the results show that TensorCRO can achieve better convergence rates and solutions than other algorithms within a fixed execution time, given that the fitness functions are also implemented on TensorFlow. Furthermore, we also evaluate the proposed approach in a real-world problem of optimizing power production in wind farms by selecting the locations of turbines; in every evaluated scenario, TensorCRO outperformed the other meta-heuristics and achieved solutions close to the best known in the literature. Overall, our implementation of the CRO-SL algorithm in TensorFlow GPU provides a new, fast, and efficient approach to solving optimization problems, and we believe that the proposed implementation has significant potential to be applied in various domains, such as engineering, finance, and machine learning, where optimization is often used to solve complex problems. Furthermore, we propose that this implementation can be used to optimize models that cannot propagate an error gradient, which is an excellent choice for non-gradient-based optimizers.},
  archive      = {J_EXSY},
  author       = {A. Palomo-Alonso and V. G. Costa and L. M. Moreno-Saavedra and E. Lorente-Ramos and J. Pérez-Aracil and C. E. Pedreira and S. Salcedo-Sanz},
  doi          = {10.1111/exsy.13713},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13713},
  shortjournal = {Expert Syst.},
  title        = {TensorCRO: A TensorFlow-based implementation of a multi-method ensemble for optimization},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing interpretability of data-driven fuzzy models:
Application in industrial regression problems. <em>EXSY</em>,
<em>41</em>(12), e13710. (<a
href="https://doi.org/10.1111/exsy.13710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has attracted great interest in the modeling of systems using computational learning methods, being utilized in a wide range of advanced fields due to its ability and efficiency to process large amounts of data and to make predictions or decisions with a high degree of accuracy. However, with the increase in the complexity of the models, ML&#39;s methods have presented complex structures that are not always transparent to the users. In this sense, it is important to study how to counteract this trend and explore ways to increase the interpretability of these models, precisely where decision-making plays a central role. This work addresses this challenge by assessing the interpretability and explainability of fuzzy-based models. The structural and semantic factors that impact the interpretability of fuzzy systems are examined. Various metrics have been studied to address this topic, such as the Co-firing Based Comprehensibility Index (COFCI), Nauck Index, Similarity Index, and Membership Function Center Index. These metrics were assessed across different datasets on three fuzzy-based models: (i) a model designed with Fuzzy c-Means and Least Squares Method, (ii) Adaptive-Network-based Fuzzy Inference System (ANFIS), and (iii) Generalized Additive Model Zero-Order Takagi-Sugeno (GAM-ZOTS). The study conducted in this work culminates in a new comprehensive interpretability metric that covers different domains associated with interpretability in fuzzy-based models. When addressing interpretability, one of the challenges lies in balancing high accuracy with interpretability, as these two goals often conflict. In this context, experimental evaluations were performed in many scenarios using 4 datasets varying the model parameters in order to find a compromise between interpretability and accuracy.},
  archive      = {J_EXSY},
  author       = {Jorge S. S. Júnior and Carlos Gaspar and Jérôme Mendes and Cristiano Premebida},
  doi          = {10.1111/exsy.13710},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13710},
  shortjournal = {Expert Syst.},
  title        = {Assessing interpretability of data-driven fuzzy models: Application in industrial regression problems},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi agent collaborative search algorithm with adaptive
weights. <em>EXSY</em>, <em>41</em>(12), e13709. (<a
href="https://doi.org/10.1111/exsy.13709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new version of Multi Agent Collaborative Search (MACS) with Adaptive Weights (named MACS-AW). MACS is a multi-agent memetic scheme for multi-objective optimization originally developed to mix local and population-based search. MACS was proven to perform well on a number of test cases but had three limitations: (i) the amount of computational resources allocated to each agent was not proportional to the difficulty of the sub-problem the agent had to solve; (ii) the population-based search (called social actions in the following) was using only one differential evolution (DE) operator with fixed parameters; (iii) the descent directions were not adapted during convergence, leading to a loss of diversity. In this paper, we propose an improved version of MACS, that implements: (i) a new utility function to better manage computational resources; (ii) new social actions with multiple adaptive DE operators; (iii) an automatic adaptation of the descent directions with an innovative trigger to initiate adaptation. First, MACS-AW is compared against some state-of-art algorithms and its predecessor MACS2.1 on some standard benchmarks. Then, MACS-AW is applied to the solution of two real-life optimization problems and compared against MACS2.1. It will be shown that MACS-AW produces competitive results on most test cases analysed in this paper. On the standard benchmark test set, MACS-AW outperforms all other algorithms in 11 out of 30 cases and comes second in other 8 cases. On the two real engineering test set, MACS-AW and its predecessor obtain same results.},
  archive      = {J_EXSY},
  author       = {Li Cao and Maocai Wang and Massimiliano Vasile and Guangming Dai},
  doi          = {10.1111/exsy.13709},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13709},
  shortjournal = {Expert Syst.},
  title        = {Multi agent collaborative search algorithm with adaptive weights},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CADICA: A new dataset for coronary artery disease detection
by using invasive coronary angiography. <em>EXSY</em>, <em>41</em>(12),
e13708. (<a href="https://doi.org/10.1111/exsy.13708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary artery disease (CAD) remains the leading cause of death globally and invasive coronary angiography (ICA) is considered the gold standard of anatomical imaging evaluation when CAD is suspected. However, risk evaluation based on ICA has several limitations, such as visual assessment of stenosis severity, which has significant interobserver variability. This motivates to development of a lesion classification system that can support specialists in their clinical procedures. Although deep learning classification methods are well-developed in other areas of medical imaging, ICA image classification is still at an early stage. One of the most important reasons is the lack of available and high-quality open-access datasets. In this paper, we reported a new annotated ICA images dataset, CADICA, to provide the research community with a comprehensive and rigorous dataset of coronary angiography consisting of a set of acquired patient videos and associated disease-related metadata. This dataset can be used by clinicians to train their skills in angiographic assessment of CAD severity, by computer scientists to create computer-aided diagnostic systems to help in such assessment, and to validate existing methods for CAD detection. In addition, baseline classification methods are proposed and analysed, validating the functionality of CADICA with deep learning-based methods and giving the scientific community a starting point to improve CAD detection.},
  archive      = {J_EXSY},
  author       = {Ariadna Jiménez-Partinen and Miguel A. Molina-Cabello and Karl Thurnhofer-Hemsi and Esteban J. Palomo and Jorge Rodríguez-Capitán and Ana I. Molina-Ramos and Manuel Jiménez-Navarro},
  doi          = {10.1111/exsy.13708},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13708},
  shortjournal = {Expert Syst.},
  title        = {CADICA: A new dataset for coronary artery disease detection by using invasive coronary angiography},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-pop: Enhancing user engagement with content-based
multimodal popularity prediction in social media. <em>EXSY</em>,
<em>41</em>(12), e13707. (<a
href="https://doi.org/10.1111/exsy.13707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media has entrenched itself as an indispensable marketing tool. We introduce a quantitative approach to predicting the popularity of social media posts within the café and bakery sector. Employing Multi-Pop , a multimodal popularity prediction model that harnesses both images and text from post content, it utilizes the features of posts that significantly influence their popularity on one of the most widely used platforms, Instagram. By focusing solely on post-content features and excluding user information, we analysed 8765 Instagram posts from the cafe and bakery domain, revealing that our model attains a superior accuracy rate of 82.0% compared with existing popularity prediction methods. Furthermore, the study identifies hashtags and post captions as exerting a greater impact on post popularity than images. This research furnishes valuable insights, particularly for small business owners and individual entrepreneurs, by introducing novel computational and empirical methodologies for Instagram marketing strategy and post popularity prediction, thereby enhancing the comprehension of social media marketing dynamics.},
  archive      = {J_EXSY},
  author       = {Jiyoon Kim and Hyeongjin Ahn and Eunil Park},
  doi          = {10.1111/exsy.13707},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13707},
  shortjournal = {Expert Syst.},
  title        = {Multi-pop: Enhancing user engagement with content-based multimodal popularity prediction in social media},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based gesture recognition for surgical
applications: A data augmentation approach. <em>EXSY</em>,
<em>41</em>(12), e13706. (<a
href="https://doi.org/10.1111/exsy.13706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture recognition and classification play a pivotal role in automating Human-Computer Interaction (HCI) and have garnered substantial attention in research. In this study, the focus is placed on the application of gesture recognition in surgical settings to provide valuable feedback during medical training. A tool gesture classification system based on Deep Learning (DL) techniques is proposed, specifically employing a Long Short Term Memory (LSTM)-based model with an attention mechanism. The research is structured in three key stages: data pre-processing to eliminate outliers and smooth trajectories, addressing noise from surgical instrument data acquisition; data augmentation to overcome data scarcity by generating new trajectories through controlled spatial transformations; and the implementation and evaluation of the DL-based classification strategy. The dataset used includes recordings from ten participants with varying surgical experience, covering three types of trajectories and involving both right and left arms. The proposed classifier, combined with the data augmentation strategy, is assessed for its effectiveness in classifying all acquired gestures. The performance of the proposed model is evaluated against other DL-based methodologies commonly employed in surgical gesture classification. The results indicate that the proposed approach outperforms these benchmark methods, achieving higher classification accuracy and robustness in distinguishing diverse surgical gestures.},
  archive      = {J_EXSY},
  author       = {Sofía Sorbet Santiago and Jenny Alexandra Cifuentes},
  doi          = {10.1111/exsy.13706},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13706},
  shortjournal = {Expert Syst.},
  title        = {Deep learning-based gesture recognition for surgical applications: A data augmentation approach},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trust region based chaotic search for solving
multi-objective optimization problems. <em>EXSY</em>, <em>41</em>(12),
e13705. (<a href="https://doi.org/10.1111/exsy.13705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A numerical optimization technique used to address nonlinear programming problems is the trust region (TR) method. TR uses a quadratic model, which may represent the function adequately, to create a neighbourhood around the current best solution as a trust region in each step, rather than searching for the original function&#39;s objective solution. This allows the method to determine the next local optimum. The TR technique has been utilized by numerous researchers to tackle multi-objective optimization problems (MOOPs). But there is not any publication that discusses the issue of applying a chaotic search (CS) with the TR algorithm for solving multi-objective (MO) problems. From this motivation, the main contribution of this study is to introduce trust-region (TR) technique based on chaotic search (CS) for solving MOOPs. First, the reference point interactive approach is used to convert MOOP to a single objective optimization problem (SOOP). The search space is then randomly initialized with a set of initial points. Second, in order to supply locations on the Pareto boundary, the TR method solves the SOOP. Finally, all points on the Pareto frontier are obtained using CS. A range of MO benchmark problems have demonstrated the efficiency of the proposed algorithm (TR based CS) in generating Pareto optimum sets for MOOPs. Furthermore, a demonstration of the suggested algorithm&#39;s ability to resolve real-world applications is provided through a practical implementation of the algorithm to improve an abrasive water-jet machining process (AWJM).},
  archive      = {J_EXSY},
  author       = {M. A. El-Shorbagy},
  doi          = {10.1111/exsy.13705},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13705},
  shortjournal = {Expert Syst.},
  title        = {Trust region based chaotic search for solving multi-objective optimization problems},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class integration of ChatGPT and learning analytics for
higher education. <em>EXSY</em>, <em>41</em>(12), e13703. (<a
href="https://doi.org/10.1111/exsy.13703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Miguel Civit and María José Escalona and Francisco Cuadrado and Salvador Reyes-de-Cozar},
  doi          = {10.1111/exsy.13703},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13703},
  shortjournal = {Expert Syst.},
  title        = {Class integration of ChatGPT and learning analytics for higher education},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A code change-oriented approach to just-in-time defect
prediction with multiple input semantic fusion. <em>EXSY</em>,
<em>41</em>(12), e13702. (<a
href="https://doi.org/10.1111/exsy.13702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research found that fine-tuning pre-trained models is superior to training models from scratch in just-in-time (JIT) defect prediction. However, existing approaches using pre-trained models have their limitations. First, the input length is constrained by the pre-trained models.Secondly, the inputs are change-agnostic.To address these limitations, we propose JIT-Block, a JIT defect prediction method that combines multiple input semantics using changed block as the fundamental unit. We restructure the JIT-Defects4J dataset used in previous research. We then conducted a comprehensive comparison using eleven performance metrics, including both effort-aware and effort-agnostic measures, against six state-of-the-art baseline models. The results demonstrate that on the JIT defect prediction task, our approach outperforms the baseline models in all six metrics, showing improvements ranging from 1.5% to 800% in effort-agnostic metrics and 0.3% to 57% in effort-aware metrics. For the JIT defect code line localization task, our approach outperforms the baseline models in three out of five metrics, showing improvements of 11% to 140%.},
  archive      = {J_EXSY},
  author       = {Teng Huang and Hui-Qun Yu and Gui-Sheng Fan and Zi-Jie Huang and Chen-Yu Wu},
  doi          = {10.1111/exsy.13702},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13702},
  shortjournal = {Expert Syst.},
  title        = {A code change-oriented approach to just-in-time defect prediction with multiple input semantic fusion},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new method based on generative adversarial networks for
multivariate time series prediction. <em>EXSY</em>, <em>41</em>(12),
e13700. (<a href="https://doi.org/10.1111/exsy.13700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series have more complex and high-dimensional characteristics, which makes it difficult to analyze and predict the data accurately. In this paper, a new multivariate time series prediction method is proposed. This method is a generative adversarial networks (GAN) method based on Fourier transform and bi-directional gated recurrent unit (Bi-GRU). First, the Fourier transform is utilized to extend the data features, which helps the GAN to better learn the distributional features of the original data. Second, in order to guide the model to fully learn the distribution of the original time series data, Bi-GRU is introduced as the generator of GAN. To solve the problems of mode collapse and gradient vanishing that exist in GAN, Wasserstein distance is used as the loss function of GAN. Finally, the proposed method is used for the prediction of air quality, stock price and RMB exchange rate. The experimental results show that the model can effectively predict the trend of the time series compared with the other nine baseline models. It significantly improves the accuracy and flexibility of multivariate time series forecasting and provides new ideas and methods for accurate time series forecasting in industrial, financial and environmental fields.},
  archive      = {J_EXSY},
  author       = {Xiwen Qin and Hongyu Shi and Xiaogang Dong and Siqi Zhang},
  doi          = {10.1111/exsy.13700},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13700},
  shortjournal = {Expert Syst.},
  title        = {A new method based on generative adversarial networks for multivariate time series prediction},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RETRACTED: Optimization using internet of agent based
stacked sparse autoencoder model for heart disease prediction.
<em>EXSY</em>, <em>41</em>(12), e13359. (<a
href="https://doi.org/10.1111/exsy.13359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, machine learning methods have been successfully used for the prediction of cardiovascular disease. Early diagnosis and prediction is necessary for giving effective treatment to avoid higher mortality rates. Several classification algorithms have been developed recently which satisfy the need, but show limited accuracy while predicting the heart disease. Hence, the focus of this study is on early prediction of heart disease and to improve the accuracy of prediction using benchmark heart disease datasets such as UCI Cleveland dataset and Heart disease clinical dataset by implementing effective classification and optimization algorithms. Optimization algorithms generally exhibit the benefit of dealing with complex non-linear issues with better adaptability and flexibility. The Emperor penguin optimization algorithm, which can select the best features for classification has been utilized in this study to improve the efficiency, minimize reconstruction errors, and increase the quality of heart disease classification. Further, the newly developed stacked sparse convolutional neural network based auto encoder (SSC-AE) classification algorithm has been employed for significant feature classification with higher robustness and efficacy. Accuracy, Area Under Curve (AUC), and F1 score are some of the measures used to compare the outcomes of several machine learning algorithms to those of the proposed model in this study. The results show that the proposed model, SSC-AE, is superior to other classification models.},
  archive      = {J_EXSY},
  author       = {Vaishali Baviskar and Madhushi Verma and Pradeep Chatterjee and Gaurav Singal and Thippa Reddy Gadekallu},
  doi          = {10.1111/exsy.13359},
  journal      = {Expert Systems},
  month        = {12},
  number       = {12},
  pages        = {e13359},
  shortjournal = {Expert Syst.},
  title        = {RETRACTED: Optimization using internet of agent based stacked sparse autoencoder model for heart disease prediction},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic community query in a large-scale attributed graph
based on an attribute cohesiveness optimization strategy. <em>EXSY</em>,
<em>41</em>(11), e13704. (<a
href="https://doi.org/10.1111/exsy.13704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of a semantic community query is to obtain a subgraph S based on a given query vertex q (or vertex set) and other query parameters in an attributed graph G such that S belongs to G , contains q and satisfies a predefined community cohesiveness model. In most cases, existing community query models based on the network structure for traditional attributed networks usually lack community semantics. However, the features of vertex attributes, especially the attributes of the query vertices, which are closely related to the community semantics, are rarely considered in an attributed graph. Existing community query algorithms based on both structure cohesiveness and attribute cohesiveness usually do not take the attributes of the query vertex as an important factor of the community cohesiveness model, which leads to weak semantics of the communities. This paper proposes a semantic community query method named SCQ in a large-scale attributed graph. First, the k -core structure model is adopted as the structure cohesiveness of our community query model to obtain a subgraph of the original graph. Second, we define attribute cohesiveness based on the average distance between the query vertices and other vertices in terms of attributes in the community to prune the subgraph and obtain the semantic community. In order to improve the community query efficiency in large-scale attributed graphs, SCQ applies two heuristic pruning strategies. The experimental results show that our method outperforms the existing community query methods in multiple evaluation metrics and is ideal for querying semantic communities in large-scale attributed graphs.},
  archive      = {J_EXSY},
  author       = {Jinhuan Ge and Heli Sun and Yezhi Lin and Liang He},
  doi          = {10.1111/exsy.13704},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13704},
  shortjournal = {Expert Syst.},
  title        = {Semantic community query in a large-scale attributed graph based on an attribute cohesiveness optimization strategy},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring transformer models for sentiment classification: A
comparison of BERT, RoBERTa, ALBERT, DistilBERT, and XLNet.
<em>EXSY</em>, <em>41</em>(11), e13701. (<a
href="https://doi.org/10.1111/exsy.13701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning models have proven superior to classical machine learning approaches in various text classification tasks, such as sentiment analysis, question answering, news categorization, and natural language inference. Recently, these models have shown exceptional results in natural language understanding (NLU). Advanced attention-based language models like BERT and XLNet excel at handling complex tasks across diverse contexts. However, they encounter difficulties when applied to specific domains. Platforms like Facebook, characterized by continually evolving casual and sophisticated language, demand meticulous context analysis even from human users. The literature has proposed numerous solutions using statistical and machine learning techniques to predict the sentiment (positive or negative) of online customer reviews, but most of them rely on various business, review, and reviewer features, which leads to generalizability issues. Furthermore, there have been very few studies investigating the effectiveness of state-of-the-art pre-trained language models for sentiment classification in reviews. Therefore, this study aims to assess the effectiveness of BERT, RoBERTa, ALBERT, DistilBERT, and XLNet in sentiment classification using the Yelp reviews dataset. The models were fine-tuned, and the results obtained with the same hyperparameters are as follows: 98.30 for RoBERTa, 98.20 for XLNet, 97.40 for BERT, 97.20 for ALBERT, and 96.00 for DistilBERT.},
  archive      = {J_EXSY},
  author       = {Ali Areshey and Hassan Mathkour},
  doi          = {10.1111/exsy.13701},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13701},
  shortjournal = {Expert Syst.},
  title        = {Exploring transformer models for sentiment classification: A comparison of BERT, RoBERTa, ALBERT, DistilBERT, and XLNet},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparative analysis of paraphrasing performance of ChatGPT,
GPT-3, and t5 language models using a new ChatGPT generated dataset:
ParaGPT. <em>EXSY</em>, <em>41</em>(11), e13699. (<a
href="https://doi.org/10.1111/exsy.13699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paraphrase generation is a fundamental natural language processing (NLP) task that refers to the process of generating a well-formed and coherent output sentence that exhibits both syntactic and/or lexical diversity from the input sentence, while simultaneously ensuring that the semantic similarity between the two sentences is preserved. However, the availability of high-quality paraphrase datasets has been limited, particularly for machine-generated sentences. In this paper, we present ParaGPT, a new paraphrase dataset of 81,000 machine-generated sentence pairs, including 27,000 reference sentences (ChatGPT-generated sentences), and 81,000 paraphrases obtained by using three different large language models (LLMs): ChatGPT, GPT-3, and T5. We used ChatGPT to generate 27,000 sentences that cover a diverse array of topics and sentence structures, thus providing diverse inputs for the models. In addition, we evaluated the quality of the generated paraphrases using various automatic evaluation metrics. Furthermore, we provide insights into the strengths and drawbacks of each LLM in generating paraphrases by conducting a comparative analysis of the paraphrasing performance of the three LLMs. According to our findings, ChatGPT&#39;s performance, as per the evaluation metrics provided, was deemed impressive and commendable, owing to its higher-than-average scores for semantic similarity, which implies a higher degree of similarity between the generated paraphrase and the reference sentence, and its relatively lower scores for syntactic diversity, indicating a greater diversity of syntactic structures in the generated paraphrase. ParaGPT is a valuable resource for researchers working on NLP tasks like paraphrasing, text simplification, and text generation. We make the ParaGPT dataset publicly accessible to researchers, and as far as we are aware, this is the first paraphrase dataset produced based on ChatGPT.},
  archive      = {J_EXSY},
  author       = {Meltem Kurt Pehlivanoğlu and Robera Tadesse Gobosho and Muhammad Abdan Syakura and Vimal Shanmuganathan and Luis de-la-Fuente-Valentín},
  doi          = {10.1111/exsy.13699},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13699},
  shortjournal = {Expert Syst.},
  title        = {Comparative analysis of paraphrasing performance of ChatGPT, GPT-3, and t5 language models using a new ChatGPT generated dataset: ParaGPT},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ResiSC: A system for building resilient smart city
communication networks. <em>EXSY</em>, <em>41</em>(11), e13698. (<a
href="https://doi.org/10.1111/exsy.13698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart city networks are critical for delivering essential services such as healthcare, education, and business operations. However, these networks are highly susceptible to a range of threats, including natural disasters and intentional cyberattacks, which can severely disrupt their functionality. To address these vulnerabilities, we present the resilient smart city (ResiSC) system, designed to enhance the resilience of smart city communication networks through a topological design approach. Our system employs a graph-theoretic algorithm to determine the optimal network topology for a given set of nodes, aiming to maximize connectivity while minimizing link provisioning costs. We introduce two novel connectivity measurements, All Nodes Reachability (ANR) and Sum of All Nodes Reachability (SANR), to evaluate network resilience. We applied our approach to data from two public universities of different sizes, simulating various attack scenarios to assess the robustness of the resulting network topologies. Evaluation results indicate that our solution improves network resilience against targeted attacks by 38% compared to baseline methods such as k-nearest neighbours (k-NN) graphs, while also reducing the number of additional links and their associated costs. Results also indicate that our proposed solution outperforms baseline methods like k-NN in terms of network resilience against targeted attacks by 41%. This work provides a practical framework for developing robust smart city networks capable of withstanding diverse threats.},
  archive      = {J_EXSY},
  author       = {Mohammed J. F. Alenazi},
  doi          = {10.1111/exsy.13698},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13698},
  shortjournal = {Expert Syst.},
  title        = {ResiSC: A system for building resilient smart city communication networks},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prospect of large language models and natural language
processing for lung cancer diagnosis: A systematic review.
<em>EXSY</em>, <em>41</em>(11), e13697. (<a
href="https://doi.org/10.1111/exsy.13697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer, a leading cause of global mortality, demands a combat for its effective prevention, early diagnosis, and advanced treatment methods. Traditional diagnostic methods face limitations in accuracy and efficiency, necessitating innovative solutions. Large Language Models (LLMs) and Natural Language Processing (NLP) offer promising avenues for overcoming these challenges by providing comprehensive insights into medical data and personalizing treatment plans. This systematic review explores the transformative potential of LLMs and NLP in automating lung cancer diagnosis. It evaluates their applications, particularly in medical imaging and the interpretation of complex medical data, and assesses achievements and associated challenges. Emphasizing the critical role of Artificial Intelligence (AI) in medical imaging, the review highlights advancements in lung cancer screening and deep learning approaches. Furthermore, it underscores the importance of on-going advancements in diagnostic methods and encourages further exploration in this field.},
  archive      = {J_EXSY},
  author       = {Arushi Garg and Smridhi Gupta and Soumya Vats and Palak Handa and Nidhi Goel},
  doi          = {10.1111/exsy.13697},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13697},
  shortjournal = {Expert Syst.},
  title        = {Prospect of large language models and natural language processing for lung cancer diagnosis: A systematic review},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Health indicator construction based on normal states through
FFT-graph embedding. <em>EXSY</em>, <em>41</em>(11), e13689. (<a
href="https://doi.org/10.1111/exsy.13689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unexpected faults in rotating machinery can lead to cascading disruptions of the entire work process, emphasizing the importance of early detection of performance degradation and identification of the current state. To accurately assess the health of a machine, this study introduces an FFT-based raw vibration data preprocessing and graph representation technique, which analyses changes in frequency bands to detect early degradation trends in vibration data that may appear normal. The approach proposes a methodology that utilizes a graph convolutional autoencoder trained using only normal data to extract health indicators using the differences in the vectors as degradation progresses. This approach has the advantage of using only normal data to detect subtle performance degradation early and effectively represent health indicators accordingly.},
  archive      = {J_EXSY},
  author       = {GwanPil Kim and Jason J. Jung and David Camacho},
  doi          = {10.1111/exsy.13689},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13689},
  shortjournal = {Expert Syst.},
  title        = {Health indicator construction based on normal states through FFT-graph embedding},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human activity recognition: A comprehensive review.
<em>EXSY</em>, <em>41</em>(11), e13680. (<a
href="https://doi.org/10.1111/exsy.13680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Activity Recognition (HAR) is a highly promising research area meant to automatically identify and interpret human behaviour using data received from sensors in various contexts. The potential uses of HAR are many, among them health care, sports coaching or monitoring the elderly or disabled. Nonetheless, there are numerous hurdles to be circumvented for HAR&#39;s precision and usefulness to be improved. One of the challenges is that there is no uniformity in data collection and annotation making it difficult to compare findings among different studies. Furthermore, more comprehensive datasets are necessary so as to include a wider range of human activities in different contexts while complex activities, which consist of multiple sub-activities, are still a challenge for recognition systems. Researchers have proposed new frontiers such as multi-modal sensor data fusion and deep learning approaches for enhancing HAR accuracy while addressing these issues. Also, we are seeing more non-traditional applications such as robotics and virtual reality/augmented world going forward with their use cases of HAR. This article offers an extensive review on the recent advances in HAR and highlights the major challenges facing this field as well as future opportunities for further researches.},
  archive      = {J_EXSY},
  author       = {Harmandeep Kaur and Veenu Rani and Munish Kumar},
  doi          = {10.1111/exsy.13680},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13680},
  shortjournal = {Expert Syst.},
  title        = {Human activity recognition: A comprehensive review},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Entropy-based hybrid sampling (EHS) method to handle class
overlap in highly imbalanced dataset. <em>EXSY</em>, <em>41</em>(11),
e13679. (<a href="https://doi.org/10.1111/exsy.13679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance and class overlap create difficulties in the training phase of the standard machine learning algorithm. Its performance is not well in minority classes, especially when there is a high class imbalance and significant class overlap. Recently it has been observed by researchers that, the joint effects of class overlap and imbalance are more harmful as compared to their direct impact. To handle these problems, many methods have been proposed by researchers in past years that can be broadly categorized as data-level, algorithm-level, ensemble learning, and hybrid methods. Existing data-level methods often suffer from problems like information loss and overfitting. To overcome these problems, we introduce a novel entropy-based hybrid sampling (EHS) method to handle class overlap in highly imbalanced datasets. The EHS eliminates less informative majority instances from the overlap region during the undersampling phase and regenerates high informative synthetic minority instances in the oversampling phase near the borderline. The proposed EHS achieved significant improvement in F1-score, G-mean, and AUC performance metrics value by DT, NB, and SVM classifiers as compared to well-established state-of-the-art methods. Classifiers performances are tested on 28 datasets with extreme ranges in imbalance and overlap.},
  archive      = {J_EXSY},
  author       = {Anil Kumar and Dinesh Singh and Rama Shankar Yadav},
  doi          = {10.1111/exsy.13679},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13679},
  shortjournal = {Expert Syst.},
  title        = {Entropy-based hybrid sampling (EHS) method to handle class overlap in highly imbalanced dataset},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The study of engagement at work from the artificial
intelligence perspective: A systematic review. <em>EXSY</em>,
<em>41</em>(11), e13673. (<a
href="https://doi.org/10.1111/exsy.13673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engagement has been defined as an attitude toward work, as a positive, satisfying, work-related state of mind characterized by high levels of vigour, dedication, and absorption. Both its definition and its assessment have been controversial; however, new methods for its assessment, including artificial intelligence (AI), have been introduced in recent years. Therefore, this research aims to determine the state of the art of AI in the study of engagement. To this end, we conducted a systematic review in accordance with PRISMA to analyse the publications to date on the use of AI for the analysis of engagement. The search, carried out in six databases, was filtered, and 15 papers were finally analysed. The results show that AI has been used mainly to assess and predict engagement levels, as well as to understand the relationships between engagement and other variables. The most commonly used AI techniques are machine learning (ML) and natural language processing (NLP), and all publications use structured and unstructured data, mainly from self-report instruments, social networks, and datasets. The accuracy of the models varies from 22% to 87%, and its main benefit has been to help both managers and HR staff understand employee engagement, although it has also contributed to research. Most of the articles have been published since 2015, and the geography has been global, with publications predominantly in India and the US. In conclusion, this study highlights the state of the art in AI for the study of engagement and concludes that the number of publications is increasing, indicating that this is possibly a new field or area of research in which important advances can be made in the study of engagement through new and novel techniques.},
  archive      = {J_EXSY},
  author       = {Claudia García-Navarro and Manuel Pulido-Martos and Cristina Pérez-Lozano},
  doi          = {10.1111/exsy.13673},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13673},
  shortjournal = {Expert Syst.},
  title        = {The study of engagement at work from the artificial intelligence perspective: A systematic review},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An EEMD-LSTM, SVR, and BP decomposition ensemble model for
steel future prices forecasting. <em>EXSY</em>, <em>41</em>(11), e13672.
(<a href="https://doi.org/10.1111/exsy.13672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The forecasting of steel futures prices is important for the steel futures market, even for the steel industry. We propose a decomposition ensemble model that incorporates the Ensemble Empirical Mode Decomposition (EEMD), Long Short-Term Memory (LSTM), Support Vector Regression (SVR), and Back Propagation (BP) neural network to forecast steel futures prices. The forecasting procedures are as follows: (1) The price data are initially decomposed into several relatively independent Intrinsic Mode Functions (IMFs) and a residue using EEMD. (2) The IMFs are then reconstructed as components representing short-term, medium-term, and long-term frequencies via fine-to-coarse. (3) LSTM, SVR, and BP neural network are utilized to forecast the short-term, medium-term, and long-term reconstructed components, respectively. (4) The prediction results for each component are simply added to the final prediction results. The accuracy of the proposed model is compared with several benchmark models by experiments and evaluated by some prediction evaluation indexes. The experimental results show that our model outperforms other models in terms of forecast accuracy, confirming its strong predictive capabilities. This study provides some suggestions for investment and decision making by participants in the steel futures market. It may promote the smooth operation of the steel futures market and shed some light on the operation of the steel industry.},
  archive      = {J_EXSY},
  author       = {Sen Wu and Wei Wang and Yanan Song and Shuaiqi Liu},
  doi          = {10.1111/exsy.13672},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13672},
  shortjournal = {Expert Syst.},
  title        = {An EEMD-LSTM, SVR, and BP decomposition ensemble model for steel future prices forecasting},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What distinguishes conspiracy from critical narratives? A
computational analysis of oppositional discourse. <em>EXSY</em>,
<em>41</em>(11), e13671. (<a
href="https://doi.org/10.1111/exsy.13671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current prevalence of conspiracy theories on the internet is a significant issue, tackled by many computational approaches. However, these approaches fail to recognize the relevance of distinguishing between texts which contain a conspiracy theory and texts which are simply critical and oppose mainstream narratives. Furthermore, little attention is usually paid to the role of inter-group conflict in oppositional narratives. We contribute by proposing a novel topic-agnostic annotation scheme that differentiates between conspiracies and critical texts, and that defines span-level categories of inter-group conflict. We also contribute with the multilingual XAI-DisInfodemics corpus (English and Spanish), which contains a high-quality annotation of Telegram messages related to COVID-19 (5000 messages per language). We also demonstrate the feasibility of an NLP-based automatization by performing a range of experiments that yield strong baseline solutions. Finally, we perform an analysis which demonstrates that the promotion of intergroup conflict and the presence of violence and anger are key aspects to distinguish between the two types of oppositional narratives, that is, conspiracy versus critical.},
  archive      = {J_EXSY},
  author       = {Damir Korenčić and Berta Chulvi and Xavier Bonet Casals and Alejandro Toselli and Mariona Taulé and Paolo Rosso},
  doi          = {10.1111/exsy.13671},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13671},
  shortjournal = {Expert Syst.},
  title        = {What distinguishes conspiracy from critical narratives? a computational analysis of oppositional discourse},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal dynamic fusion framework: Multilevel feature
fusion guided by prompts. <em>EXSY</em>, <em>41</em>(11), e13668. (<a
href="https://doi.org/10.1111/exsy.13668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the progressive augmentation of parameters in multimodal models, to optimize computational efficiency, some studies have adopted the approach of fine-tuning the unimodal pre-training model to achieve multimodal fusion tasks. However, these methods tend to rely solely on simplistic or singular fusion strategies, thereby neglecting more flexible fusion approaches. Moreover, existing methods prioritize the integration of modality features containing highly semantic information, often overlooking the influence of fusing low-level features on the outcomes. Therefore, this study introduces an innovative approach named multilevel feature fusion guided by prompts (MFF-GP), a multimodal dynamic fusion framework. It guides the dynamic neural network by prompt vectors to dynamically select the suitable fusion network for each hierarchical feature of the unimodal pre-training model. This method improves the interactions between multiple modalities and promotes a more efficient fusion of features across them. Extensive experiments on the UPMC Food 101, SNLI-VE and MM-IMDB datasets demonstrate that with only a few trainable parameters, MFF-GP achieves significant accuracy improvements compared to a newly designed PMF based on fine-tuning—specifically, an accuracy improvement of 2.15% on the UPMC Food 101 dataset and 0.82% on the SNLI-VE dataset. Further study of the results reveals that increasing the diversity of interactions between distinct modalities is critical and delivers significant performance improvements. Furthermore, for certain multimodal tasks, focusing on the low-level features is beneficial for modality integration. Our implementation is available at: https://github.com/whq2024/MFF-GP .},
  archive      = {J_EXSY},
  author       = {Lei Pan and Huan-Qing Wu},
  doi          = {10.1111/exsy.13668},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13668},
  shortjournal = {Expert Syst.},
  title        = {Multimodal dynamic fusion framework: Multilevel feature fusion guided by prompts},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Portfolio construction using explainable reinforcement
learning. <em>EXSY</em>, <em>41</em>(11), e13667. (<a
href="https://doi.org/10.1111/exsy.13667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While machine learning&#39;s role in financial trading has advanced considerably, algorithmic transparency and explainability challenges still exist. This research enriches prior studies focused on high-frequency financial data prediction by introducing an explainable reinforcement learning model for portfolio management. This model transcends basic asset prediction, formulating concrete, actionable trading strategies. The methodology is applied in a custom trading environment mimicking the CAC-40 index&#39;s financial conditions, allowing the model to adapt dynamically to market changes based on iterative learning from historical data. Empirical findings reveal that the model outperforms an equally weighted portfolio in out-of-sample tests. The study offers a dual contribution: it elevates algorithmic planning while significantly boosting transparency and interpretability in financial machine learning. This approach tackles the enduring ‘black-box’ issue and provides a holistic, transparent framework for managing investment portfolios.},
  archive      = {J_EXSY},
  author       = {Daniel González Cortés and Enrique Onieva and Iker Pastor and Laura Trinchera and Jian Wu},
  doi          = {10.1111/exsy.13667},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13667},
  shortjournal = {Expert Syst.},
  title        = {Portfolio construction using explainable reinforcement learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imbalanced survival prediction for gastric cancer patients
based on improved XGBoost with cost sensitive and focal loss.
<em>EXSY</em>, <em>41</em>(11), e13666. (<a
href="https://doi.org/10.1111/exsy.13666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of gastric cancer survival state is one of great significant tasks for clinical decision-making. Many advanced machine learning classification techniques have been applied to predict the survival status of cancer patients in three or 5 years, however, many of them have a low sensitivity because of class imbalance. This is a non-negligible problem due to the poor prognosis of gastric cancer patients. Furthermore, models in the medical domain require strong interpretability to increase their applicability. Due to the better performance and interpretability of the XGBoost model, we design a loss function taking into account cost sensitive and focal loss from the algorithm level for XGBoost to deal with the imbalance problem. We apply the improved model into the prediction of the survival status of gastric cancer patients and analyse the important related features. We use two types of indicators to evaluate the model, and we also design the confusion matrix of two models&#39; predictive results to compare two models. The results show that the improved model has better performance. Furthermore, we calculate the importance of features related to survival with three different time periods and analyse their evolution, which are consistent with existing clinical research or further expand their research conclusions. These all support for clinically relevant decision-making and has the potential to expand into survival prediction of other cancer patients.},
  archive      = {J_EXSY},
  author       = {Liangchen Xu and Chonghui Guo},
  doi          = {10.1111/exsy.13666},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13666},
  shortjournal = {Expert Syst.},
  title        = {Imbalanced survival prediction for gastric cancer patients based on improved XGBoost with cost sensitive and focal loss},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive weighted feature fusion for multiscale atrous
convolution-based 1DCNN with dilated LSTM-aided fake news detection
using regional language text information. <em>EXSY</em>,
<em>41</em>(11), e13665. (<a
href="https://doi.org/10.1111/exsy.13665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The people in the world rely on social media for gathering news, and it is mainly because of the development of technology. The approaches employed in natural language processing are still deficient in judgement factors, and these techniques frequently rely upon political or social circumstances. Numerous low-level communities in the area are curious after experiencing the negative effects caused by the spread of false information in different sectors. Low-resource languages are still distracted, because these techniques are extensively employed in the English language. This work aims to provide an analysis of regional language fake news and develop a referral system with advanced techniques to identify fake news in Hindi and Tamil. This proposed model includes (a) Regional Language Text Collection; (b) Text preprocessing; (c) Feature Extraction; (d) Weighted Stacked Feature Fusion; and (e) Fake News Detection. The text data is collected from the standard datasets. The collected text data is preprocessed and given into the feature extraction, which is done by using bidirectional encoder representations from transformers (BERT), transformer networks, and seq2seq network for extracting the three sets of language text features. These extracted feature sets are inserted into the weighted stacked feature fusion model, where the three sets of extracted features are integrated with the optimized weights that are acquired through the enhanced osprey optimization algorithm (EOOA). Finally, these resultant features are given to multi-scale atrous convolution-based one-dimensional convolutional neural network with dilated long short-term memory (MACNN-DLSTM) for detecting the fake news. Throughout the result analysis, the experimentation is conducted based on the standard Tamil and Hindi datasets. Moreover, the developed model shows 92% for Hindi datasets and 96% for Tamil datasets which shows effective performance regarding accuracy measures. The experimental analysis is carried out by comparing with the conventional algorithms and detection techniques to showcase the efficiency of the developed regional language-based fake news detection model.},
  archive      = {J_EXSY},
  author       = {V Rathinapriya and J. Kalaivani},
  doi          = {10.1111/exsy.13665},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e13665},
  shortjournal = {Expert Syst.},
  title        = {Adaptive weighted feature fusion for multiscale atrous convolution-based 1DCNN with dilated LSTM-aided fake news detection using regional language text information},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facial emotion recognition: A comprehensive review.
<em>EXSY</em>, <em>41</em>(10), e13670. (<a
href="https://doi.org/10.1111/exsy.13670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial emotion recognition (FER) represents a significant outcome of the rapid advancements in artificial intelligence (AI) technology. In today&#39;s digital era, the ability to decipher emotions from facial expressions has evolved into a fundamental mode of human interaction and communication. As a result, FER has penetrated diverse domains, including but not limited to medical diagnosis, customer feedback analysis, the automation of automobile driver systems, and the evaluation of student comprehension. Furthermore, it has matured into a captivating and dynamic research field, capturing the attention and curiosity of contemporary scholars and scientists. The primary objective of this paper is to provide an exhaustive review of FER systems. Its significance goes beyond offering a comprehensive resource; it also serves as a valuable guide for emerging researchers in the FER domain. Through a meticulous examination of existing FER systems and methodologies, this review equips them with essential insights and guidance for their future research pursuits. Moreover, this comprehensive review contributes to the expansion of their knowledge base, facilitating a profound understanding of this rapidly evolving field. In a world increasingly dependent on technology for communication and interaction, the study of FER holds a pivotal role in human-computer interaction (HCI). It not only provides valuable insights but also unlocks a multitude of possibilities for future innovations and applications. As we continue to integrate AI and facial emotion recognition into our daily lives, the importance of comprehending and enhancing FER systems becomes increasingly evident. This paper serves as a stepping stone for researchers, nurturing their involvement in this exciting and ever-evolving field.},
  archive      = {J_EXSY},
  author       = {Manmeet Kaur and Munish Kumar},
  doi          = {10.1111/exsy.13670},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13670},
  shortjournal = {Expert Syst.},
  title        = {Facial emotion recognition: A comprehensive review},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual resource constrained flexible job shop scheduling with
sequence-dependent setup time. <em>EXSY</em>, <em>41</em>(10), e13669.
(<a href="https://doi.org/10.1111/exsy.13669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the imperative need for efficient solutions in the context of the dual resource constrained flexible job shop scheduling problem with sequence-dependent setup times (DRCFJS-SDSTs). We introduce a pioneering tri-objective mixed-integer linear mathematical model tailored to this complex challenge. Our model is designed to optimize the assignment of operations to candidate multi-skilled machines and operators, with the primary goals of minimizing operators&#39; idleness cost and sequence-dependent setup time-related expenses. Additionally, it aims to mitigate total tardiness and earliness penalties while regulating maximum machine workload. Given the NP-hard nature of the proposed DRCFJS-SDST, we employ the epsilon constraint method to derive exact optimal solutions for small-scale problems. For larger instances, we develop a modified variant of the multi-objective invasive weed optimization (MOIWO) algorithm, enhanced by a fuzzy sorting algorithm for competitive exclusion. In the absence of established benchmarks in the literature, we validate our solutions against those generated by multi-objective particle swarm optimization (MOPSO) and non-dominated sorted genetic algorithm (NSGA-II). Through comparative analysis, we demonstrate the superior performance of MOIWO. Specifically, when compared with NSGA-II, MOIWO achieves success rates of 90.83% and shows similar performance in 4.17% of cases. Moreover, compared with MOPSO, MOIWO achieves success rates of 84.17% and exhibits similar performance in 9.17% of cases. These findings contribute significantly to the advancement of scheduling optimization methodologies.},
  archive      = {J_EXSY},
  author       = {Sasan Barak and Shima Javanmard and Reza Moghdani},
  doi          = {10.1111/exsy.13669},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13669},
  shortjournal = {Expert Syst.},
  title        = {Dual resource constrained flexible job shop scheduling with sequence-dependent setup time},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AES software and hardware system co-design for resisting
side channel attacks. <em>EXSY</em>, <em>41</em>(10), e13664. (<a
href="https://doi.org/10.1111/exsy.13664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The threat of side-channel attacks poses a significant risk to the security of cryptographic algorithms. To counter this threat, we have designed an AES system capable of defending against such attacks, supporting AES-128, AES-192, and AES-256 encryption standards. In our system, the CPU oversees the AES hardware via the AHB bus and employs true random number generation to provide secure random inputs for computations. The hardware implementation of the AES S-box utilizes complex domain inversion techniques, while intermediate data is shielded using full-time masking. Furthermore, the system incorporates double-path error detection mechanisms to thwart fault propagation. Our results demonstrate that the system effectively conceals key power information, providing robust resistance against CPA attacks, and is capable of detecting injected faults, thereby mitigating fault-based attacks.},
  archive      = {J_EXSY},
  author       = {Liguo Dong and Xinliang Ye and Libin Zhuang and Ruidian Zhan and M. Shamim Hossain},
  doi          = {10.1111/exsy.13664},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13664},
  shortjournal = {Expert Syst.},
  title        = {AES software and hardware system co-design for resisting side channel attacks},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic cross- and multi-lingual recognition of dysphonia
by ensemble classification using deep speaker embedding models.
<em>EXSY</em>, <em>41</em>(10), e13660. (<a
href="https://doi.org/10.1111/exsy.13660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) algorithms have demonstrated remarkable performance in dysphonia detection using speech samples. However, their efficacy often diminishes when tested on languages different from the training data, raising questions about their suitability in clinical settings. This study aims to develop a robust method for cross- and multi-lingual dysphonia detection that overcomes the limitation of language dependency in existing ML methods. We propose an innovative approach that leverages speech embeddings from speaker verification models, especially ECAPA and x-vector and employs a majority voting ensemble classifier. We utilize speech features extracted from ECAPA and x-vector embeddings to train three distinct classifiers. The significant advantage of these embedding models lies in their capability to capture speaker characteristics in a language-independent manner, forming fixed-dimensional feature spaces. Additionally, we investigate the impact of generating synthetic data within the embedding feature space using the Synthetic Minority Oversampling Technique (SMOTE). Our experimental results unveil the effectiveness of the proposed method for dysphonia detection. Compared to results obtained from x-vector embeddings, ECAPA consistently demonstrates superior performance in distinguishing between healthy and dysphonic speech, achieving accuracy values of 93.33% and 96.55% in both cross-lingual and multi-lingual scenarios, respectively. This highlights the remarkable capabilities of speaker verification models, especially ECAPA, in capturing language-independent features that enhance overall detection performance. The proposed method effectively addresses the challenges of language dependency in dysphonia detection. ECAPA embeddings, combined with majority voting ensemble classifiers, show significant potential for improving the accuracy and reliability of dysphonia detection in cross- and multi-lingual scenarios.},
  archive      = {J_EXSY},
  author       = {Dosti Aziz and Dávid Sztahó},
  doi          = {10.1111/exsy.13660},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13660},
  shortjournal = {Expert Syst.},
  title        = {Automatic cross- and multi-lingual recognition of dysphonia by ensemble classification using deep speaker embedding models},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convolution-enhanced vision transformer method for lower
limb exoskeleton locomotion mode recognition. <em>EXSY</em>,
<em>41</em>(10), e13659. (<a
href="https://doi.org/10.1111/exsy.13659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing the human body with smooth and natural assistance through lower limb exoskeletons is crucial. However, a significant challenge is identifying various locomotion modes to enable the exoskeleton to offer seamless support. In this study, we propose a method for locomotion mode recognition named Convolution-enhanced Vision Transformer (Conv-ViT). This method maximizes the benefits of convolution for feature extraction and fusion, as well as the self-attention mechanism of the Transformer, to efficiently capture and handle long-term dependencies among different positions within the input sequence. By equipping the exoskeleton with inertial measurement units, we collected motion data from 27 healthy subjects, using it as input to train the Conv-ViT model. To ensure the exoskeleton&#39;s stability and safety during transitions between various locomotion modes, we not only examined the typical five steady modes (involving walking on level ground [WL], stair ascent [SA], stair descent [SD], ramp ascent [RA], and ramp descent [RD]) but also extensively explored eight locomotion transitions (including WL-SA, WL-SD, WL-RA, WL-RD, SA-WL, SD-WL, RA-WL, RD-WL). In tasks involving the recognition of five steady locomotions and eight transitions, the recognition accuracy reached 98.87% and 96.74%, respectively. Compared with three popular algorithms, ViT, convolutional neural networks, and support vector machine, the results show that the proposed method has the best recognition performance, and there are highly significant differences in accuracy and F1 score compared to other methods. Finally, we also demonstrated the excellent performance of Conv-ViT in terms of generalization performance.},
  archive      = {J_EXSY},
  author       = {Jianbin Zheng and Chaojie Wang and Liping Huang and Yifan Gao and Ruoxi Yan and Chunbo Yang and Yang Gao and Yu Wang},
  doi          = {10.1111/exsy.13659},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13659},
  shortjournal = {Expert Syst.},
  title        = {Convolution-enhanced vision transformer method for lower limb exoskeleton locomotion mode recognition},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ImageVeriBypasser: An image verification code recognition
approach based on convolutional neural network. <em>EXSY</em>,
<em>41</em>(10), e13658. (<a
href="https://doi.org/10.1111/exsy.13658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent period has witnessed automated crawlers designed to automatically crack passwords, which greatly risks various aspects of our lives. To prevent passwords from being cracked, image verification codes have been implemented to accomplish the human–machine verification. It is important to note, however, that the most widely-used image verification codes, especially the visual reasoning Completely Automated Public Turing tests to tell Computers and Humans Apart (CAPTCHAs), are still susceptible to attacks by artificial intelligence. Taking the visual reasoning CAPTCHAs representing the image verification codes, this study introduces an enhanced approach for generating image verification codes and proposes an improved Convolutional Neural Network (CNN)-based recognition system. After we add a fully connected layer and briefly solve the edge of stability issue, the accuracy of the improved CNN model can smoothly approach 98.40% within 50 epochs on the image verification codes with four digits using a large initial learning rate of 0.01. Compared with the baseline model, it is approximately 37.82% better in accuracy without obvious curve oscillation. The improved CNN model can also smoothly reach the accuracy of 99.00% within 7500 epochs on the image verification codes with six characters, including digits, upper-case alphabets, lower-case alphabets, and symbols. A detailed comparison between our proposed approach and the baseline one is presented. The relationship between the time consumption and the length of the seeds is compared theoretically. Subsequently, we figure out the threat assignments on the visual reasoning CAPTCHAs with different lengths based on four machine learning models. Based on the threat assignments, the Kaplan-Meier (KM) curves are computed.},
  archive      = {J_EXSY},
  author       = {Tong Ji and Yuxin Luo and Yifeng Lin and Yuer Yang and Qian Zheng and Siwei Lian and Junjie Li},
  doi          = {10.1111/exsy.13658},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13658},
  shortjournal = {Expert Syst.},
  title        = {ImageVeriBypasser: An image verification code recognition approach based on convolutional neural network},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Marine predators optimization with deep learning model for
video-based facial expression recognition. <em>EXSY</em>,
<em>41</em>(10), e13657. (<a
href="https://doi.org/10.1111/exsy.13657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based facial expression recognition (VFER) technique intends to categorize an input video into different kinds of emotions. It remains a challenging issue because of the gap between visual features and emotions, problems in handling the delicate movement of muscles, and restricted datasets. One of the effective solutions to solve this problem is the exploitation of efficient features defining facial expressions to carry out FER. Generally, the VFER find useful in several areas like unmanned driving, venue management, urban safety management, and senseless attendance. Recent advances in computer vision and deep learning (DL) techniques enable the design of automated VFER models. In this aspect, this study establishes a new Marine Predators Optimization with Deep Learning Model for Video-based Facial Expression Recognition (MPODL-VFER) technique. The presented MPODL-VFER technique mainly aims to classify different kinds of facial emotions in the video. To accomplish this, the presented MPODL-VFER technique derives features using the deep convolutional neural network based densely connected network (DenseNet) model. The presented MPODL-VFER technique employs MPO technique for the hyperparameter adjustment of the DenseNet model. Finally, Elman Neural Network (ENN) model is exploited for emotion recognition purposes. For assuring the enhanced recognition performance of the MPODL-VFER approach, a comparison study was developed on benchmark dataset. The comprehensive results have shown the significant outcome of MPODL-VFER model over other approaches.},
  archive      = {J_EXSY},
  author       = {Mal Hari Prasad and P. Swarnalatha},
  doi          = {10.1111/exsy.13657},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13657},
  shortjournal = {Expert Syst.},
  title        = {Marine predators optimization with deep learning model for video-based facial expression recognition},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A supervised learning tool for heatwave predictions using
daily high summer temperatures. <em>EXSY</em>, <em>41</em>(10), e13656.
(<a href="https://doi.org/10.1111/exsy.13656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global temperature is increasing at an alarming rate, which increases the number of heatwaves. Heatwaves have significant impacts, both directly and indirectly, on human and natural systems and can create considerable risk to public health. Predicting the occurrence of a heatwave can save lives, increase the production of crops, improve water quality, and reduce transportation restrictions. Because of its geographical location, Bangladesh is particularly vulnerable to cyclones, droughts, earthquakes, floods, and heatwaves. The Bangladesh Meteorological Department collects temperature data at multiple weather stations, and we use data from 10 weather stations in this research. Data show that most heatwaves occur in the summer months, namely, April, May, and June. In this research, we develop Classification and Regression Tree (CART) models that use daily temperature data for the months of March, April, May, and June to predict the likelihood of a heatwave within the next 7 days, the next 28 days, and on any particular day based on daily high temperatures from the previous 14 days. We also use different model parameters to evaluate the accuracy of the models. Finally, we develop treed Stepwise Logistic Regression models to predict the probability of heatwaves occurring. Even though this research uses data from Bangladesh Meteorological Department, the developed modeling approach can be used in other geographic regions.},
  archive      = {J_EXSY},
  author       = {Gazi Md Daud Iqbal and Jay Rosenberger and Matthew Rosenberger and Muhammad Shah Alam and Lidan Ha and Emmanuel Anoruo and Sadie Gregory and Tom Mazzone},
  doi          = {10.1111/exsy.13656},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13656},
  shortjournal = {Expert Syst.},
  title        = {A supervised learning tool for heatwave predictions using daily high summer temperatures},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial attack vulnerability for multi-biometric
authentication system. <em>EXSY</em>, <em>41</em>(10), e13655. (<a
href="https://doi.org/10.1111/exsy.13655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on multi-biometric authentication systems using multiple biometric modalities to defend against adversarial attacks is actively being pursued. These systems authenticate users by combining two or more biometric modalities using score or feature-level fusion. However, research on adversarial attacks and defences against each biometric modality within these authentication systems has not been actively conducted. In this study, we constructed a multi-biometric authentication system using fingerprint, palmprint, and iris information from CASIA-BIT by employing score and feature-level fusion. We verified the system&#39;s vulnerability by deploying adversarial attacks on single and multiple biometric modalities based on the FGSM, with epsilon values ranging from 0 to 0.5. The experimental results show that when the epsilon value is 0.5, the accuracy of the multi-biometric authentication system against adversarial attacks on the palmprint and iris information decreases from 0.995 to 0.018 and 0.003, respectively, and the f1-score decreases from 0.995 to 0.007 and 0.000, respectively, demonstrating susceptibility to adversarial attacks. In the case of fingerprint data, however, the accuracy and f1-score decreased from 0.995 to 0.731 and from 0.995 to 0.741, respectively, indicating resilience against adversarial attacks.},
  archive      = {J_EXSY},
  author       = {MyeongHoe Lee and JunHo Yoon and Chang Choi},
  doi          = {10.1111/exsy.13655},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13655},
  shortjournal = {Expert Syst.},
  title        = {Adversarial attack vulnerability for multi-biometric authentication system},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When geoscience meets generative AI and large language
models: Foundations, trends, and future challenges. <em>EXSY</em>,
<em>41</em>(10), e13654. (<a
href="https://doi.org/10.1111/exsy.13654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Artificial Intelligence (GAI) represents an emerging field that promises the creation of synthetic data and outputs in different modalities. GAI has recently shown impressive results across a large spectrum of applications ranging from biology, medicine, education, legislation, computer science, and finance. As one strives for enhanced safety, efficiency, and sustainability, generative AI indeed emerges as a key differentiator and promises a paradigm shift in the field. This article explores the potential applications of generative AI and large language models in geoscience. The recent developments in the field of machine learning and deep learning have enabled the generative model&#39;s utility for tackling diverse prediction problems, simulation, and multi-criteria decision-making challenges related to geoscience and Earth system dynamics. This survey discusses several GAI models that have been used in geoscience comprising generative adversarial networks (GANs), physics-informed neural networks (PINNs), and generative pre-trained transformer (GPT)-based structures. These tools have helped the geoscience community in several applications, including (but not limited to) data generation/augmentation, super-resolution, panchromatic sharpening, haze removal, restoration, and land surface changing. Some challenges still remain, such as ensuring physical interpretation, nefarious use cases, and trustworthiness. Beyond that, GAI models show promises to the geoscience community, especially with the support to climate change, urban science, atmospheric science, marine science, and planetary science through their extraordinary ability to data-driven modelling and uncertainty quantification.},
  archive      = {J_EXSY},
  author       = {Abdenour Hadid and Tanujit Chakraborty and Daniel Busby},
  doi          = {10.1111/exsy.13654},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13654},
  shortjournal = {Expert Syst.},
  title        = {When geoscience meets generative AI and large language models: Foundations, trends, and future challenges},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-model deep learning system for screening human
monkeypox using skin images. <em>EXSY</em>, <em>41</em>(10), e13651. (<a
href="https://doi.org/10.1111/exsy.13651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Kapil Gupta and Varun Bajaj and Deepak Kumar Jain and Amir Hussain},
  doi          = {10.1111/exsy.13651},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13651},
  shortjournal = {Expert Syst.},
  title        = {Multi-model deep learning system for screening human monkeypox using skin images},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized hop-based approaches for identifying influential
nodes in social networks. <em>EXSY</em>, <em>41</em>(10), e13649. (<a
href="https://doi.org/10.1111/exsy.13649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locating a set of influential users within a social network, known as the Influence Maximization (IM) problem, can have significant implications for boosting the spread of positive information/news and curbing the spread of negative elements such as misinformation and disease. However, the traditional simulation-based spread computations under conventional diffusion models render existing algorithms inefficient in finding optimal solutions. In recent years, hop and path-based approaches have gained popularity, particularly under the cascade models to address the scalability issue. Nevertheless, these existing functions vary based on the considered hop-distance and provide no guidance on capturing spread sizes beyond two-hops. In this paper, we introduce Hop-based Expected Influence Maximization (HEIM), an approach utilizing generalized functions to compute influence spread across varying hop-distances in conventional diffusion models. We extend our investigation to the Linear Threshold (LT) model, in addition to the Independent Cascade (IC) and Weighted Cascade (WC) models, filling a gap in current literature. Our theoretical analysis shows that the proposed functions preserve both monotonicity and submodularity, and the proposed HEIM algorithm can achieve an approximation ratio of under a limited hop-measures, whereas a multiplicative -approximation under global measures. Furthermore, we show that expected spread methods can serve as a better benchmark approach than existing simulation-based methods. The performance of the HEIM algorithm is evaluated through experiments on three real-world networks, and is compared to six other existing algorithms. Results demonstrate that the three-hop based HEIM algorithm achieves superior solution quality, ranking first in statistical tests, and is notably faster than existing benchmark approaches. Conversely, the one-hop-based HEIM offers faster computation while still delivering competitive solutions, providing decision-makers with flexibility based on application needs.},
  archive      = {J_EXSY},
  author       = {Tarun Kumer Biswas and Alireza Abbasi and Ripon Kumar Chakrabortty},
  doi          = {10.1111/exsy.13649},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13649},
  shortjournal = {Expert Syst.},
  title        = {Generalized hop-based approaches for identifying influential nodes in social networks},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Crowdfunding performance prediction using
feature-selection-based machine learning models. <em>EXSY</em>,
<em>41</em>(10), e13646. (<a
href="https://doi.org/10.1111/exsy.13646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Yuanyue Feng and Yuhong Luo and Nianjiao Peng and Ben Niu},
  doi          = {10.1111/exsy.13646},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13646},
  shortjournal = {Expert Syst.},
  title        = {Crowdfunding performance prediction using feature-selection-based machine learning models},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data science methods for response, incremental response and
rate sensitivity to response modelling in banking. <em>EXSY</em>,
<em>41</em>(10), e13644. (<a
href="https://doi.org/10.1111/exsy.13644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work provides a review of data science methods that can be used to address a wide variety of business problems in the banking sector. The paper examines three modelling paradigms: the response, incremental response and the rate sensitivity to response approaches, emphasising the role they play to address these problems. These paradigms and the methods they involve are presented in combination with real cases to illustrate their potential in extracting valuable business insights from data. It is enhanced their usefulness to help business experts like risk managers, commercial managers, financial directors and chief executive officers to plan their strategies and guide decision making on the basis of the insights given by their outcomes. The scope of the work is twofold: it presents a unified view of the methods and how the fit the aforementioned paradigms while, at the same time, it examines some business cases for their application. Both issues will be of interest for technical and managerial teams involved in running data science projects in banking.},
  archive      = {J_EXSY},
  author       = {Jorge M. Arevalillo},
  doi          = {10.1111/exsy.13644},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13644},
  shortjournal = {Expert Syst.},
  title        = {Data science methods for response, incremental response and rate sensitivity to response modelling in banking},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Barzilai borwein incremental grey polynomial regression for
train delay prediction. <em>EXSY</em>, <em>41</em>(10), e13642. (<a
href="https://doi.org/10.1111/exsy.13642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The swift societal evolution and ceaseless advancement of human value of life have been set forth for reliability as well as rapidity of railway transportation. Latest advances in machine learning approaches as well as surging accessibility of numerous information sources is produced state-of-the-art probabilities for significant, precise train delay identification. In this method called, Barzilai Borwein Incremental Grey Polynomial Regression (BBI-GPR) is introduced for predicting train arrival/departure delays, which utilized for later delay management in an accurate manner with this method comprised into three sections such as, pre-processing, feature selection and classification. First, with the raw ETA train delay dataset as input, Barzilai–Borwein Feature Rescaling-based Pre-processing is applied to model computationally efficient feature rescaled and normalized values. Second with processed features as input, Incremental Maximum Relevance Minimum Redundant-based Feature Selection is applied to select error minimized optimal features. Finally, with optimal features selected as input, Grey Polynomial Regression-based Prediction algorithm is employed to analyse train delay. For confirming proposed BBI-GPR, as well as analyse its performance, compare standard train delay prediction method with existing machine learning-based regression method. Results show that new variants outperform existing train delay prediction method by minimizing train delay prediction time, error rate by 25% and 27% respectively, with improved accuracy rate of 7%, therefore paving ways for efficient train delay prediction.},
  archive      = {J_EXSY},
  author       = {Ajay Singh and Rajesh Kumar Dhanaraj and Seifedine Kadry},
  doi          = {10.1111/exsy.13642},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13642},
  shortjournal = {Expert Syst.},
  title        = {Barzilai borwein incremental grey polynomial regression for train delay prediction},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label logo recognition and retrieval based on weighted
fusion of neural features. <em>EXSY</em>, <em>41</em>(10), e13627. (<a
href="https://doi.org/10.1111/exsy.13627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying logo images is a challenging task as they contain elements such as text or shapes that can represent anything from known objects to abstract shapes. While the current state of the art for logo classification addresses the problem as a multi-class task focusing on a single characteristic, logos can have several simultaneous labels, such as different colours. This work proposes a method that allows visually similar logos to be classified and searched from a set of data according to their shape, colour, commercial sector, semantics, general characteristics, or a combination of features selected by the user. Unlike previous approaches, the proposal employs a series of multi-label deep neural networks specialized in specific attributes and combines the obtained features to perform the similarity search. To delve into the classification system, different existing logo topologies are compared and some of their problems are analysed, such as the incomplete labelling that trademark registration databases usually contain. The proposal is evaluated considering 76,000 logos (seven times more than previous approaches) from the European Union Trademarks dataset, which is organized hierarchically using the Vienna ontology. Overall, experimentation attains reliable quantitative and qualitative results, reducing the normalized average rank error of the state-of-the-art from 0.040 to 0.018 for the Trademark Image Retrieval task. Finally, given that the semantics of logos can often be subjective, graphic design students and professionals were surveyed. Results show that the proposed methodology provides better labelling than a human expert operator, improving the label ranking average precision from 0.53 to 0.68.},
  archive      = {J_EXSY},
  author       = {Marisa Bernabeu and Antonio Javier Gallego and Antonio Pertusa},
  doi          = {10.1111/exsy.13627},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13627},
  shortjournal = {Expert Syst.},
  title        = {Multi-label logo recognition and retrieval based on weighted fusion of neural features},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A compact artificial bee colony metaheuristic for global
optimization problems. <em>EXSY</em>, <em>41</em>(10), e13621. (<a
href="https://doi.org/10.1111/exsy.13621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computationally efficient and time-memory saving compact algorithms become a keystone for solving global optimization problems, particularly the real world problems; which involve devices with limited memory or restricted use of battery power. Compact optimization algorithms represent a probabilistic view of the population to simulate the population behaviour as they broadly explores the decision space at the beginning of the optimization process and keep focus on to search the most promising solution, therefore narrows the search space, moreover few number of parameters need be stored in the memory thus require less space and time to compute efficiently. Role of population-based algorithms remain inevitable as compact algorithms make use of the efficient search ability of these population based algorithms for optimization but only through a probabilistic representation of the population space in order to optimize the real world problems. Artificial bee colony (ABC) algorithm has shown to be competitive over other population-based algorithms for solving optimization problems, however its solution search equation contributes to its insufficiency due to poor exploitation phase coupled with low convergence rate. This paper, presents a compact Artificial bee colony (cABC) algorithm with an improved solution search equation, which will be able to search an optimal solution to improve its exploitation capabilities, moreover in order to increase the global convergence of the proposed algorithm, an improved approach for population sampling is introduced through a compact distribution which helps in maintaining a good balance between exploration and exploitation search abilities of the proposed compact algorithm with least memory requirements, thus became suitable for limited hardware access devices. The proposed algorithm is evaluated extensively on a standard set of benchmark functions proposed at IEEE CEC&#39;13 for large-scale global optimization (LSGO) problems. Numerical results prove that the proposed compact algorithm outperforms other standard optimization algorithms.},
  archive      = {J_EXSY},
  author       = {Palvinder Singh Mann and Shailesh D. Panchal and Satvir Singh and Simran Kaur},
  doi          = {10.1111/exsy.13621},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13621},
  shortjournal = {Expert Syst.},
  title        = {A compact artificial bee colony metaheuristic for global optimization problems},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient integration of perceptual variational autoencoder
into dynamic latent scale generative adversarial network. <em>EXSY</em>,
<em>41</em>(10), e13618. (<a
href="https://doi.org/10.1111/exsy.13618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic latent scale GAN is an architecture-agnostic encoder-based generative model inversion method. This paper introduces a method to efficiently integrate perceptual VAE into dynamic latent scale GAN to improve the performance of dynamic latent scale GAN. When dynamic latent scale GAN is trained with a normal i.i.d. latent random variable and the latent encoder is integrated into the discriminator, a sum of a predicted latent random variable of real data and a scaled normal noise follows the normal i.i.d. random variable. Since this random variable is paired with real data and follows the latent random variable, it can be used for both VAE and GAN training. Furthermore, by considering the intermediate layer output of the discriminator as the feature encoder output, the VAE can be trained to minimise the perceptual reconstruction loss. The forward propagation &amp; backpropagation for minimising this perceptual reconstruction loss can be integrated with those of GAN training. Therefore, the proposed method does not require additional computations compared to typical GAN or dynamic latent scale GAN. Integrating perceptual VAE to dynamic latent scale GAN improved the generative and inversion performance of the model.},
  archive      = {J_EXSY},
  author       = {Jeongik Cho and Adam Krzyzak},
  doi          = {10.1111/exsy.13618},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13618},
  shortjournal = {Expert Syst.},
  title        = {Efficient integration of perceptual variational autoencoder into dynamic latent scale generative adversarial network},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing depression detection: A multimodal approach with
text extension and content fusion. <em>EXSY</em>, <em>41</em>(10),
e13616. (<a href="https://doi.org/10.1111/exsy.13616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Jinyan Chen and Shuxian Liu and Meijia Xu and Peicheng Wang},
  doi          = {10.1111/exsy.13616},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13616},
  shortjournal = {Expert Syst.},
  title        = {Enhancing depression detection: A multimodal approach with text extension and content fusion},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Arabic text classification based on analogical proportions.
<em>EXSY</em>, <em>41</em>(10), e13609. (<a
href="https://doi.org/10.1111/exsy.13609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification is the process of labelling a given set of text documents with predefined classes or categories. Existing Arabic text classifiers are either applying classic Machine Learning algorithms such as k -NN and SVM or using modern deep learning techniques. The former are assessed using small text collections and their accuracy is still subject to improvement while the latter are efficient in classifying big data collections and show limited effectiveness in classifying small corpora with a large number of categories. This paper proposes a new approach to Arabic text classification to treat small and large data collections while improving the classification rates of existing classifiers. We first demonstrate the ability of analogical proportions (AP) (statements of the form ‘ x is to as is to ’), which have recently been shown to be effective in classifying ‘structured’ data, to classify ‘unstructured’ text documents requiring preprocessing. We design an analogical model to express the relationship between text documents and their real categories. Next, based on this principle, we develop two new analogical Arabic text classifiers. These rely on the idea that the category of a new document can be predicted from the categories of three others, in the training set, in case the four documents build together a ‘valid’ analogical proportion on all or on a large number of components extracted from each of them. The two proposed classifiers (denoted AATC1 and AATC2) differ mainly in terms of the keywords extracted for classification. To evaluate the proposed classifiers, we perform an extensive experimental study using five benchmark Arabic text collections with small or large sizes, namely ANT (Arabic News Texts) v2.1 and v1.1, BBC-Arabic, CNN-Arabic and AlKhaleej-2004. We also compare analogical classifiers with both classical ML-based and Deep Learning-based classifiers. Results show that AATC2 has the best average accuracy (78.78%) over all other classifiers and the best average precision (0.77) ranked first followed by AATC1 (0.73), NB (0.73) and SVM (0.72) for the ANT corpus v2.1. Besides, AATC1 shows the best average precisions (0.88) and (0.92), respectively for the BBC-Arabic corpus and AlKhaleej-2004, and the best average accuracy (85.64%) for CNN-Arabic over all other classifiers. Results demonstrate the utility of analogical proportions for text classification. In particular, the proposed analogical classifiers are shown to significantly outperform a number of existing Arabic classifiers, and in many cases, compare  favourably to the robust SVM classifier.},
  archive      = {J_EXSY},
  author       = {Myriam Bounhas and Bilel Elayeb and Amina Chouigui and Amir Hussain and Erik Cambria},
  doi          = {10.1111/exsy.13609},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13609},
  shortjournal = {Expert Syst.},
  title        = {Arabic text classification based on analogical proportions},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-armed bandit based online model selection for
concept-drift adaptation. <em>EXSY</em>, <em>41</em>(9), e13626. (<a
href="https://doi.org/10.1111/exsy.13626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble methods are among the most effective concept-drift adaptation techniques due to their high learning performance and flexibility. However, they are computationally expensive and pose a challenge in applications involving high-speed data streams. In this paper, we present a computationally efficient heterogeneous classifier ensemble entitled OMS-MAB which uses online model selection for concept-drift adaptation by posing it as a non-stationary multi-armed bandit (MAB) problem. We use a MAB to select a single adaptive learner within the ensemble for learning and prediction while systematically exploring promising alternatives. Each ensemble member is made drift resistant using explicit drift detection and is represented as an arm of the MAB. An exploration factor ϵ ϵ controls the trade-off between predictive performance and computational resource requirements, eliminating the need to continuously train and evaluate all the ensemble members. A rigorous evaluation on 20 benchmark datasets and 9 algorithms indicates that the accuracy of OMS-MAB is statistically at par with state-of-the-art (SOTA) ensembles. Moreover, it offers a significant reduction in execution time and model size in comparison to several SOTA ensemble methods, making it a promising ensemble for resource constrained stream-mining problems.},
  archive      = {J_EXSY},
  author       = {Jobin Wilson and Santanu Chaudhury and Brejesh Lall},
  doi          = {10.1111/exsy.13626},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13626},
  shortjournal = {Expert Syst.},
  title        = {Multi-armed bandit based online model selection for concept-drift adaptation},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ABANet: Attention boundary-aware network for image
segmentation. <em>EXSY</em>, <em>41</em>(9), e13625. (<a
href="https://doi.org/10.1111/exsy.13625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have attained substantial progress in various face-related tasks, such as face recognition, face inpainting, and facial expression recognition. To prevent infection or the spread of the virus, wearing of masks in public places has been mandated following the COVID-19 epidemic, which has led to face occlusion and posed significant challenges for face recognition systems. Most prominent masked face recognition solutions rely on mask segmentation tasks. Therefore, segmentation can be used to mitigate the negative impacts of wearing a mask and improve recognition accuracy. Mask region segmentation suffers from two main problems: there is no standard type of masks that people wear, they come in different colours and designs, and there is no publicly available masked face dataset with appropriate ground truth for the mask region. In order to address these issues, we propose an encoder–decoder framework that utilizes a boundary-aware attention network combined with a new hybrid loss to provide a map, patch, and pixel-level supervision. We also introduce a dataset called MFSD, with 11,601 images and 12,758 masked faces for masked face segmentation. Furthermore, we compare the performance of different cutting-edge deep learning semantic segmentation models on the presented dataset. Experimental results on the MSFD dataset reveal that the suggested approach outperforms state-of-the-art, algorithms with 97.623% accuracy, 93.814% IoU, and 96.817% F 1-score rate. Our dataset of masked faces with mask region labels and source code will be available online.},
  archive      = {J_EXSY},
  author       = {Sadjad Rezvani and Mansoor Fateh and Hossein Khosravi},
  doi          = {10.1111/exsy.13625},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13625},
  shortjournal = {Expert Syst.},
  title        = {ABANet: Attention boundary-aware network for image segmentation},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust anomaly detection in industrial images by blending
global–local features. <em>EXSY</em>, <em>41</em>(9), e13624. (<a
href="https://doi.org/10.1111/exsy.13624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial image anomaly detection achieves automated detection and localization of defects or abnormal regions in images through image processing and deep learning techniques. Currently, utilizing the approach of reverse knowledge distillation has yielded favourable outcomes. However, it is still a challenge in terms of the feature extraction capability of the image and the robustness of the decoding of the student network. This study first addresses the issue that the teacher network has not been able to extract global information more effectively. To acquire more global information, a vision transformer network is introduced to enhance the model&#39;s global information extraction capability, obtaining better features to further assist the student network in decoding. Second, for anomalous samples, to address the low similarity between features extracted by the teacher network and features restored by the student network, Gaussian noise is introduced. This further increases the probability that the features decoded by the student model match normal sample features, enhancing the robustness of the student model. Extensive experiments were conducted on industrial image datasets AeBAD, MvtecAD, and BTAD. In the AeBAD dataset, under the PRO performance metric, the result is 89.83%, achieving state-of-the-art performance. Under the AUROC performance metric, it reaches 83.35%. Similarly, good results were achieved on the MvtecAD and BTAD datasets. The proposed method&#39;s effectiveness and performance advantages were validated across multiple industrial datasets, providing a valuable reference for the application of industrial image anomaly detection methods.},
  archive      = {J_EXSY},
  author       = {Mingjing Pei and Ningzhong Liu and Shifeng Xia},
  doi          = {10.1111/exsy.13624},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13624},
  shortjournal = {Expert Syst.},
  title        = {Robust anomaly detection in industrial images by blending global–local features},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of nature-inspired algorithms in finite
element-based metaheuristic optimisation of laminated shells.
<em>EXSY</em>, <em>41</em>(9), e13620. (<a
href="https://doi.org/10.1111/exsy.13620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a unique technique for optimising composite laminates used as structural components, which is critical for situations where failure might result in disastrous effects. Unlike traditional surrogate-based optimisation approaches, this methodology combines the accurate modelling capabilities of finite element (FE) analysis with the iterative refining capacity of metaheuristic algorithms. By combining these two methodologies, our method intends to improve the design process of laminated shell structures, assuring robustness and dependability is crucial. Compared to existing benchmark solutions, the current FE shows a &lt;1% error for cylindrical and spherical shells. The prime objective of this study is to identify the optimum ply angles for attaining a high fundamental frequency. The problem is NP-hard because the possible ply angles span a wide range (±90°), making it difficult for optimisation algorithms to find a solution. Seven popular metaheuristic algorithms, namely the genetic algorithm (GA), the ant lion optimisation (ALO), the arithmetic optimisation algorithm (AOA), the dragonfly algorithm (DA), the grey wolf optimisation (GWO), the salp swarm optimisation (SSO), and the whale optimisation algorithm (WOA), are applied to and compared on a wide range of shell design problems. It assesses parameter sensitivity, discovering significant design elements that influence dynamic behaviour. Convergence studies demonstrate the superior performance of AOA, GWO, and WOA optimisers. Rigorous statistical comparisons assist practitioners in picking the best optimisation technique. FE-GWO, FE-DA, and FE-SSA methods surpass the other techniques as well as the layerwise optimisation strategy. The findings obtained, employing the GWO, DA, and SSA optimisers, demonstrate ~3% improvement over the existing literature. With respect to conventional layup designs (cross-ply and angle-ply), the current optimised designs are better by at least 0.43% and as much as 48.91%.},
  archive      = {J_EXSY},
  author       = {Subham Pal and Kanak Kalita and Salil Haldar},
  doi          = {10.1111/exsy.13620},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13620},
  shortjournal = {Expert Syst.},
  title        = {Comparison of nature-inspired algorithms in finite element-based metaheuristic optimisation of laminated shells},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonlinear dynamical system approximation and adaptive
control based on hybrid-feed-forward recurrent neural network:
Simulation and stability analysis. <em>EXSY</em>, <em>41</em>(9),
e13619. (<a href="https://doi.org/10.1111/exsy.13619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We proposed an online identification and adaptive control framework for the nonlinear dynamical systems using a novel hybrid-feed-forward recurrent neural network (HFRNN) model. The HFRNN is a combination of a feed-forward neural network (FFNN) and a local recurrent neural network (LRNN). We aim to leverage the simplicity of FFNN and the effectiveness of RNN to capture changing dynamics accurately and design an indirect adaptive control scheme. To derive the weights update equations, we have applied the gradient-descent-based Back-Propagation (BP) technique, and the stability of the proposed learning strategy is proven using the Lyapunov stability principles. We also compared the proposed method&#39;s results with those of the Jordan network-based controller (JNC) and the local recurrent network-based controller (LRNC) in the simulation examples. The results demonstrate that our approach performs satisfactorily, even in the presence of disturbance signals.},
  archive      = {J_EXSY},
  author       = {R. Shobana and Rajesh Kumar and Bhavnesh Jaint},
  doi          = {10.1111/exsy.13619},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13619},
  shortjournal = {Expert Syst.},
  title        = {Nonlinear dynamical system approximation and adaptive control based on hybrid-feed-forward recurrent neural network: Simulation and stability analysis},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doc-KG: Unstructured documents to knowledge graph
construction, identification and validation with wikidata.
<em>EXSY</em>, <em>41</em>(9), e13617. (<a
href="https://doi.org/10.1111/exsy.13617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of textual data in the digital era underlines the pivotal role of Knowledge Graphs (KGs) in effectively storing, managing, and utilizing this vast reservoir of information. Despite the copious amounts of text available on the web, a significant portion remains unstructured, presenting a substantial barrier to the automatic construction and enrichment of KGs. To address this issue, we introduce an enhanced Doc-KG model, a sophisticated approach designed to transform unstructured documents into structured knowledge by generating local KGs and mapping these to a target KG, such as Wikidata. Our model innovatively leverages syntactic information to extract entities and predicates efficiently, integrating them into triples with improved accuracy. Furthermore, the Doc-KG model&#39;s performance surpasses existing methodologies by utilizing advanced algorithms for both the extraction of triples and their subsequent identification within Wikidata, employing Wikidata&#39;s Unified Resource Identifiers for precise mapping. This dual capability not only facilitates the construction of KGs directly from unstructured texts but also enhances the process of identifying triple mentions within Wikidata, marking a significant advancement in the domain. Our comprehensive evaluation, conducted using the renowned WebNLG benchmark dataset, reveals the Doc-KG model&#39;s superior performance in triple extraction tasks, achieving an unprecedented accuracy rate of 86.64%. In the domain of triple identification, the model demonstrated exceptional efficacy by mapping 61.35% of the local KG to Wikidata, thereby contributing 38.65% of novel information for KG enrichment. A qualitative analysis based on a manually annotated dataset further confirms the model&#39;s excellence, outshining baseline methods in extracting high-fidelity triples. This research embodies a novel contribution to the field of knowledge extraction and management, offering a robust framework for the semantic structuring of unstructured data and paving the way for the next generation of KGs.},
  archive      = {J_EXSY},
  author       = {Muhammad Salman and Armin Haller and Sergio J. Rodríguez Méndez and Usman Naseem},
  doi          = {10.1111/exsy.13617},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13617},
  shortjournal = {Expert Syst.},
  title        = {Doc-KG: Unstructured documents to knowledge graph construction, identification and validation with wikidata},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization on selecting XGBoost hyperparameters using
meta-learning. <em>EXSY</em>, <em>41</em>(9), e13611. (<a
href="https://doi.org/10.1111/exsy.13611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With computational evolution, there has been a growth in the number of machine learning algorithms and they became more complex and robust. A greater challenge is upon faster and more practical ways to find hyperparameters that will set up each algorithm individually. This article aims to use meta-learning as a practicable solution for recommending hyperparameters from similar datasets, through their meta-features structures, than to adopt the already trained XGBoost parameters for a new database. This reduced computational costs and also aimed to make real-time decision-making feasible or reduce any extra costs for companies for new information. The experimental results, adopting 198 data sets, attested to the success of the heuristics application using meta-learning to compare datasets structure analysis. Initially, a characterization of the datasets was performed by combining three groups of meta-features (general, statistical, and info-theory), so that there would be a way to compare the similarity between sets and, thus, apply meta-learning to recommend the hyperparameters. Later, the appropriate number of sets to characterize the XGBoost turning was tested. The obtained results were promising, showing an improved performance in the accuracy of the XGBoost, k = {4 − 6}, using the average of the hyperparameters values and, comparing to the standard grid-search hyperparameters set by default, it was obtained that, in 78.28% of the datasets, the meta-learning methodology performed better. This study, therefore, shows that the adoption of meta-learning is a competitive alternative to generalize the XGBoost model, expecting better statistics performance (accuracy etc.) rather than adjusting to a single/particular model.},
  archive      = {J_EXSY},
  author       = {Tiago Lima Marinho and Diego Carvalho do Nascimento and Bruno Almeida Pimentel},
  doi          = {10.1111/exsy.13611},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13611},
  shortjournal = {Expert Syst.},
  title        = {Optimization on selecting XGBoost hyperparameters using meta-learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated learning model for twitter sentiment analysis
using ranger AdaBelief optimizer based bidirectional long short term
memory. <em>EXSY</em>, <em>41</em>(9), e13610. (<a
href="https://doi.org/10.1111/exsy.13610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is an automated approach which is utilized in process of analysing textual data to describe public opinion. The sentiment analysis has major role in creating impact in the day-to-day life of individuals. However, a precise interpretation of text still relies as a major concern in classifying sentiment. So, this research introduced Bidirectional Long Short Term Memory with Ranger AdaBelief Optimizer (Bi-LSTM RAO) to classify sentiment of tweets. Initially, data is obtained from Twitter API, Sentiment 140 and Stanford Sentiment Treebank-2 (SST-2). The raw data is pre-processed and it is subjected to feature extraction which is performed using Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF). The feature selection is performed using Gazelle Optimization Algorithm (GOA) which removes the irrelevant or redundant features that maximized model performance and classification is performed using Bi LSTM–RAO. The RAO optimizes the loss function of Bi-LSTM model that maximized accuracy. The classification accuracy of proposed method for Twitter API, Sentiment 140 and SST 2 dataset is obtained as 909.44%, 99.71% and 99.86%, respectively. These obtained results are comparably higher than ensemble framework, Robustly Optimized BERT and Gated Recurrent Unit (RoBERTa-GRU), Logistic Regression-Long Short Term Memory (LR-LSTM), Convolutional Bi-LSTM, Sentiment and Context Aware Attention-based Hybrid Deep Neural Network (SCA-HDNN) and Stochastic Gradient Descent optimization based Stochastic Gate Neural Network (SGD-SGNN).},
  archive      = {J_EXSY},
  author       = {Sasirekha Natarajan and Smitha Kurian and Parameshachari Bidare Divakarachari and Przemysław Falkowski-Gilski},
  doi          = {10.1111/exsy.13610},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13610},
  shortjournal = {Expert Syst.},
  title        = {An automated learning model for twitter sentiment analysis using ranger AdaBelief optimizer based bidirectional long short term memory},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting hotel cancellations through machine learning.
<em>EXSY</em>, <em>41</em>(9), e13608. (<a
href="https://doi.org/10.1111/exsy.13608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and reliable forecasting of cancellations is important for successful revenue management in the tourism industry. The objective of this study is to develop classification models to predict hotel booking cancellations. The work involves a number of key steps, such as data preprocessing to properly prepare the data; feature engineering to identify relevant attributes to help improve the predictive ability of the models; hyperparameter settings of the models, including choice of optimizers and incorporation of dropout layers to avoid overfitting in the neural networks; potential overfitting is evaluated using K-fold cross-validation; and performance is analysed using the confusion matrix and various performance metrics. The algorithms used are Multilayer Perceptron Neural Network, Radial Basis Function Neural Network, Deep Neural Network, Decision Tree Classifier, Random Forest Classifier, Ada Boost Classifier and XgBoost Classifier. Finally, the results of all models are compared, visualizing Deep Neural Network and XgBoost as the most suitable models for predicting hotel reservation cancellations.},
  archive      = {J_EXSY},
  author       = {Anita Herrera and Ángel Arroyo and Alfredo Jiménez and Álvaro Herrero},
  doi          = {10.1111/exsy.13608},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13608},
  shortjournal = {Expert Syst.},
  title        = {Forecasting hotel cancellations through machine learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lung cancer computed tomography image classification using
attention based capsule network with dispersed dynamic routing.
<em>EXSY</em>, <em>41</em>(9), e13607. (<a
href="https://doi.org/10.1111/exsy.13607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is relying as one of the significant and leading cause for the deaths which are based on cancer. So, an effective diagnosis is a crucial step to save the patients who are all dying due to lung cancer. Moreover, the diagnosis must be performed based on the severity of lung cancer and the severity can be addressed with the help of an optimal classification approach. So, this research introduced an Attention based Capsule Network (A-Caps Net) with dispersed dynamic routing to perform in-depth classification of the disease affected partitions of the image and results in better classification results. The attention layer with dispersed dynamic routing evaluates the digit capsule from feature vector in a constant manner. As the first stage, data acquisitioned from datasets such as Lung Nodule Analysis-16 (LUNA-16), The Cancer Imaging Archive (TCIA) dataset and Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI). After acquisitioning data, pre-processing is done to enhance the resolution of the image using Generative Adversarial Network. The pre-processed output is given as output for extraction of features that takes place using GLCM and VGG-16 which extracts the low level features and high level features respectively. Finally, categorization of lung cancer is performed using Attention based Capsule Network (A-Caps Net) with dispersed dynamic routing which categorize the lung cancer as benign and malignant. The results obtained through experimental analysis exhibits that proposed approach attained better accuracy of 99.57%, 99.91% and 99.29% for LUNA-16, LIDC-IDRI and TCIA dataset respectively. The classification accuracy achieved by the proposed approach for LUNA-16 dataset is 99.57% which is comparably higher than DBN, 3D CNN, Squeeze Nodule Net and 3D-DCNN with multi-layered filter with accuracies of 99.16%, 97.17% and 94.1% respectively.},
  archive      = {J_EXSY},
  author       = {Ramya Paramasivam and Sujata N. Patil and Srinivas Konda and K. L. Hemalatha},
  doi          = {10.1111/exsy.13607},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13607},
  shortjournal = {Expert Syst.},
  title        = {Lung cancer computed tomography image classification using attention based capsule network with dispersed dynamic routing},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local representation-based neighbourhood for robust
classification. <em>EXSY</em>, <em>41</em>(9), e13606. (<a
href="https://doi.org/10.1111/exsy.13606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation-based classification (RC) is an effective gauge of data similarity between a single instance and the whole dataset, which extends traditional individual-wise distance metrics using representation coefficients. These coefficients show remarkable discrimination nature via various regularisation terms, but the interference from potentially uncorrelated objects involved in this single-to-global relation can degrade the effectiveness of the coefficients. In order to filter out those unproductive, or even counter-productive, information from the decision making processes, this paper proposes a local representation-based classification (LRC) algorithm to improve the classification accuracy or the RC approach. LRC uses a single-to-local relation induced by the local representation-based neighbourhood (LRN) of each object, rather than the single-to-global relationship used by RC. Thanks to LRN, a compact and relevant dataset can be formed by selecting the most relevant data instances in the original dataset, to render a robust representation of a query. LRC was applied to multiple publicly available datasets, and the experimental results demonstrate the superiority of the proposed LRC algorithm as evidenced by the higher classification accuracy and more noise-tolerant capability in reference to alternative RC approaches. Moreover, the sampling ability of LRN is also verified via a comparative study.},
  archive      = {J_EXSY},
  author       = {Zihan Yao and Yanpeng Qu and Longzhi Yang and Yi Tan and Changjing Shang and Fei Chao and Qiang Shen},
  doi          = {10.1111/exsy.13606},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13606},
  shortjournal = {Expert Syst.},
  title        = {Local representation-based neighbourhood for robust classification},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A meta heuristic based deep learning classifier for
effective dengue disease prediction in IoT-fog system. <em>EXSY</em>,
<em>41</em>(9), e13605. (<a
href="https://doi.org/10.1111/exsy.13605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary aim of this article is to propose an effective dengue disease monitoring system by integrating deep learning models with Internet of Things (IoT) and fog computing. In this context, the disease related parameters are collected using IoT devices and further, the reports are securely transmitted to the healthcare facilities utilizing fog computing. In this article, the misdiagnosis error is reduced utilizing attention based long short term memory (ALSTM) network with coati optimization algorithm (COA). The Attention model allocates higher weight to significant parameters and pay additional consideration while training the model. Therefore, the conventional LSTM network is combined with a self-attention mechanism for enabling this model to focus on relevant parts of the input sequences for precise dengue disease prediction. The Attention Model differs from the conventional model since it delivers higher amount of data into the decoder and improves additional stages to the attention decoder before making its output. Furthermore, the COA is integrated with the ALSTM network for optimal hyper-parameter selection. The ALSTM-COA model evaluates the dengue related parameters like nausea, vomiting, skin rash, joint pain, muscle pain, soft bleeding, temperature of water-sites, humidity of water-sites, and so forth, for timely dengue disease diagnosis and clinical decision-making. The experiments performed on a real time dataset state that the ALSTM-COA model achieves significant prediction results by utilizing different performance measures like recall, accuracy, fall-out, miss rate, training time, and testing time. The ALSTM-COA model obtains an accuracy of 99.27%, latency of 0.03 s, and time complexity of 8 s for 500 health records, which are superior to the comparative deep learning models.},
  archive      = {J_EXSY},
  author       = {P. Britto Corthis and G. P. Ramesh and Ananda Babu Jayachandra},
  doi          = {10.1111/exsy.13605},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13605},
  shortjournal = {Expert Syst.},
  title        = {A meta heuristic based deep learning classifier for effective dengue disease prediction in IoT-fog system},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient customer behaviour prediction in indian
metropolitan cities for e-commerce applications. <em>EXSY</em>,
<em>41</em>(9), e13604. (<a
href="https://doi.org/10.1111/exsy.13604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary objective of this manuscript is to find the service quality factors, which influence customer satisfaction, promotional efforts taken by the retailer, and the perception of customers towards the organized retail sector. This investigation uses both offline and online retail sales datasets. Particularly, in the case of the offline retail sale dataset, an energy-efficient analysis is conducted to collect scalable, accurate, and real-time data. The collected data is used in enhancing several aspects of e-commerce operations, including customer experience optimization and inventory management, which results in better decision-making with increased efficiency. After data collection, the outliers are eliminated by implementing the z -score technique. The removal of outliers increases the variability of collected data which reduces statistical power. The statistically sufficient data are given to the Randomized Grasshopper Optimization Algorithm (RGOA) for optimal instance selection. Finally, the selected optimal instances are given to the deep neural network (DNN) model for future customer behaviour prediction. The efficacy of the RGOA-DNN model is analysed by using evaluation measures like mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), mean percentage error (MPE) and mean absolute percentage error (MAPE). The numerical analysis states that the RGOA-DNN model obtained a minimal error rate with MSE of 0.09 and 0.10, RMSE of 0.13 and 0.14, MAE of 0.12 and 0.16, MPE of 0.10 and 0.11 and MAPE of 0.08 and 0.10 on the offline and online retail sales datasets. The RGOA-DNN model has a minimal error rate in future customer behaviour prediction related to the conventional regression models. Furthermore, the elimination of inactive instances or selection of optimal instances reduces the model complexity to linear and computational time to 22.10 and 23.12 s on the offline and online retail sales datasets.},
  archive      = {J_EXSY},
  author       = {B. S. Suresh and M. Suresh},
  doi          = {10.1111/exsy.13604},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13604},
  shortjournal = {Expert Syst.},
  title        = {Efficient customer behaviour prediction in indian metropolitan cities for E-commerce applications},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combined use of radiomics and artificial neural networks for
the three-dimensional automatic segmentation of glioblastoma multiforme.
<em>EXSY</em>, <em>41</em>(9), e13598. (<a
href="https://doi.org/10.1111/exsy.13598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glioblastoma multiforme (GBM) is the most prevalent and aggressive primary brain tumour that has the worst prognosis in adults. Currently, the automatic segmentation of this kind of tumour is being intensively studied. Here, the automatic three-dimensional segmentation of the GBM is achieved with its related subzones (active tumour, inner necrosis, and peripheral oedema). Preliminary segmentations were first defined based on the four basic magnetic resonance imaging modalities and classic image processing methods (multithreshold Otsu, Chan–Vese active contours, and morphological erosion). After an automatic gap-filling post processing step, these preliminary segmentations were combined and corrected by a supervised artificial neural network of multilayer perceptron type with a hidden layer of 80 neurons, fed by 30 selected radiomic features of gray intensity and texture. Network classification has an overall accuracy of 83.9%, while the complete combined algorithm achieves average Dice similarity coefficients of 89.3%, 80.7%, 79.7%, and 66.4% for the entire region of interest, active tumour, oedema, and necrosis segmentations, respectively. These values are in the range of the best reported in the present bibliography, but even with better Hausdorff distances and lower computational costs. Results presented here evidence that it is possible to achieve the automatic segmentation of this kind of tumour by traditional radiomics. This has relevant clinical potential at the time of diagnosis, precision radiotherapy planning, or post-treatment response evaluation.},
  archive      = {J_EXSY},
  author       = {Alexander Mulet de los Reyes and Victoria Hyde Lord and Maria Elena Buemi and Daniel Gandía and Luis Gómez Déniz and Maikel Noriega Alemán and Cecilia Suárez},
  doi          = {10.1111/exsy.13598},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13598},
  shortjournal = {Expert Syst.},
  title        = {Combined use of radiomics and artificial neural networks for the three-dimensional automatic segmentation of glioblastoma multiforme},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rule based complex event processing for IoT applications:
Review, classification and challenges. <em>EXSY</em>, <em>41</em>(9),
e13597. (<a href="https://doi.org/10.1111/exsy.13597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, data aggregation and correlation have emerged as a significant and challenging area of research for finding useful relationships between different features of streaming data. Similarly, complex event processing (CEP) has emerged as a vital tool for aggregating and finding patterns from streaming data such as video streaming, real-time stock trades, sensor data and more. In this context, rule-based classifier algorithms are widely employed to discover valuable patterns from streaming data and cause-and-effect relationships between events. However, streaming data is dynamic and complex in nature, and it is not feasible to update the rules manually by domain experts continuously. On the contrary, dynamic data sometimes fails to adopt rules, which leads to sub optimal CEP implementation. Rule-based CEP systems come up with their own set of challenges, which primarily includes rules adaptability for streaming IoT data. In dynamic environment, shifting patterns in data streams can impact the performance of the CEP system, as well as the correlation between events, and finding useful patterns is very much challenging. In this article, our objective is to provide a comprehensive survey for CEP using rule-based algorithms applied across various domain and applications. We present a broad literature review using a sequential expansion of methods used for CEP, which primarily include event producers, preprocessing of events, robust rule implementation, and decision support. Additionally, we present a detailed classification of approaches used for different applications. Finally, after applying the rigorous review, it was feasible to present the state-of-the-art research, which focuses on designing robust rules, application specific insights, scalable event-driven architectures and finding the domain&#39;s trends with significant challenges and future scope.},
  archive      = {J_EXSY},
  author       = {Shashi Shekhar Kumar and Sonali Agarwal},
  doi          = {10.1111/exsy.13597},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13597},
  shortjournal = {Expert Syst.},
  title        = {Rule based complex event processing for IoT applications: Review, classification and challenges},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DFEF: Diversify feature enhancement and fusion for online
knowledge distillation. <em>EXSY</em>, <em>41</em>(9), e13593. (<a
href="https://doi.org/10.1111/exsy.13593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional knowledge distillation relies on high-capacity teacher models to supervise the training of compact student networks. To avoid the computational resource costs associated with pretraining high-capacity teacher models, teacher-free online knowledge distillation methods have achieved satisfactory performance. Among these methods, feature fusion methods have effectively alleviated the limitations of training without the strong guidance of a powerful teacher model. However, existing feature fusion methods often focus primarily on end-layer features, overlooking the efficient utilization of holistic knowledge loops and high-level information within the network. In this article, we propose a new feature fusion-based mutual learning method called Diversify Feature Enhancement and Fusion for Online Knowledge Distillation (DFEF). First, we enhance advanced semantic information by mapping multiple end-of-network features to obtain richer feature representations. Next, we design a self-distillation module to strengthen knowledge interactions between the deep and shallow network layers. Additionally, we employ attention mechanisms to provide deeper and more diversified enhancements to the input feature maps of the self-distillation module, allowing the entire network architecture to acquire a broader range of knowledge. Finally, we employ feature fusion to merge the enhanced features and generate a high-performance virtual teacher to guide the training of the student model. Extensive evaluations on the CIFAR-10, CIFAR-100, and CINIC-10 datasets demonstrate that our proposed method can significantly enhance performance compared to state-of-the-art feature fusion-based online knowledge distillation methods. Our code can be found at https://github.com/JSJ515-Group/DFEF-Liu .},
  archive      = {J_EXSY},
  author       = {Xingzhu Liang and Jian Zhang and Erhu Liu and Xianjin Fang},
  doi          = {10.1111/exsy.13593},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13593},
  shortjournal = {Expert Syst.},
  title        = {DFEF: Diversify feature enhancement and fusion for online knowledge distillation},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI based fuzzy MCDM models: Comparison and evaluation of
dissimilar outcomes, an application to enhance pilot recruitment
process. <em>EXSY</em>, <em>41</em>(9), e13590. (<a
href="https://doi.org/10.1111/exsy.13590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pilot recruitment is critical as they pose a multifaceted challenge for civilian and military organizations due to the complex traits impacting their missions and performance. In this study, a novel set of criteria and sub-criteria were determined to compare twelve candidate pilots. Numerically immeasurable, imprecise, and non-linear continuous fuzzy linguistic traits (variables) were studied which make the work unique and challenging due to individual preferences and disagreements between decision-makers (DMs). The outcomes of three distinct fuzzy multiple criteria decision-making (MCDM) approaches; fuzzy TOPSIS, fuzzy VIKOR, and fuzzy PROMETHEE were evaluated with trapezoidal fuzzy numbers (TFNs) to sort the positions of candidate pilots. Moreover, a unique defuzzification ranking method was employed to adjust the results of fuzzy MCDM methods for the synthesis and evaluation of outcomes of the pilot selection problem. All these efforts make the paper original and outstanding. Our findings and analysis suggested that fuzzy TOPSIS and PROMETHEE methods&#39; outcomes showed maximum close similarity for ranking positions. However, substantial distinctions were noted when comparing these outcomes with the fuzzy VIKOR approach. Yet, the mission of predicting and revealing the best candidates is related to several traits, their weights, and the methods selected. Therefore, since vague information and ambiguous preferences match fuzzy superiority, a comprehensive and unbiased evaluation was achieved, ensuring the integrity of the decision-making process. The results can be employed to enhance the safety and efficiency of airline operations and ensure that the most qualified and competent pilots are selected for the job.},
  archive      = {J_EXSY},
  author       = {Osman Taylan and Bulent Guloglu and Abdulaziz S. Alkabaa and Salih Sarp and Hassan M. Alidrisi and Ahmad H. Milyani and Hisham Alidrisi and Mohammed Balubaid},
  doi          = {10.1111/exsy.13590},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13590},
  shortjournal = {Expert Syst.},
  title        = {AI based fuzzy MCDM models: Comparison and evaluation of dissimilar outcomes, an application to enhance pilot recruitment process},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-focal channel attention for medical image
segmentation. <em>EXSY</em>, <em>41</em>(9), e13588. (<a
href="https://doi.org/10.1111/exsy.13588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation through the use of deep learning is becoming a trend targeting to automate disease detection and provide effective treatment. Traditionally huge manual efforts are associated with medical image processing by medical staff. The wide use of neural networks driven by the progressive advancement in computing power and the availability of training data provides instrumental means to automate medical image processing. A polyp refers to an abnormal growth that can occur in various parts of the body. While the majority of polyps are noncancerous or benign, there are instances where certain types can develop into cancer. Detecting and segmenting polyps is highly valuable for identifying early signs and potentially enabling more effective treatment of colon cancer. In this paper, we introduce a novel method for segmenting polyp images. The proposed method utilizes convolutional neural networks (CNNs) and employs an enhanced attention mechanism. To cater to a more granular attention view, this paper enhances the Convolutional Block Attention Module by introducing the multi-focal channel attention (MFCA) concept, which we call MFCA. In the proposed MFCA channel attention, focal attention spots allow us to consider scattered areas of interest more effectively. To evaluate the effectiveness of the proposed method, multiple experiments are conducted on five well-known benchmark datasets for polyp image segmentation: Kvasir, CVC-Clinic DB, CVC-Colon DB, CVC-T, and ETIS-Larib. Various testing scenarios are examined, including the impact of two focal attention spots and four focal attention spots. The experimental results demonstrated that the proposed method achieved superior performance compared to previous state-of-the-art techniques on two of the benchmark datasets, and ranked second on a third dataset.},
  archive      = {J_EXSY},
  author       = {Hamdan Al Jowair and Mansour Alsulaiman and Ghulam Muhammad},
  doi          = {10.1111/exsy.13588},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13588},
  shortjournal = {Expert Syst.},
  title        = {Multi-focal channel attention for medical image segmentation},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ultimate pose estimation: A comparative study.
<em>EXSY</em>, <em>41</em>(9), e13586. (<a
href="https://doi.org/10.1111/exsy.13586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose estimation is a computer vision task used to detect and estimate the pose of a person or an object in images or videos. It has some challenges that can leverage advances in computer vision research and others that require efficient solutions. In this paper, we provide a preliminary review of the state-of-the-art in pose estimation, including both traditional and deep learning approaches. Also, we implement and compare the performance of Hand Pose Estimation (HandPE), which uses PoseNet architecture for hand sign problems, for an ASL dataset by using different optimizers based on 10 common evaluation metrics on different datasets. Also, we discuss some related future research directions in the field of pose estimation and explore new architectures for pose estimation types. After applying the PoseNet model, the experiment results showed that the accuracy achieved was 99.9%, 89%, 97%, 79%, and 99% for the ASL alphabet, HARPET, Yoga, Animal, and Head datasets, comparing those with common optimizers and evaluation metrics on different dataset.},
  archive      = {J_EXSY},
  author       = {Esraa Hassan and M. Shamim Hossain and Samir Elmuogy and Ahmed Ghoneim and Khalid AlMutib and Abeer Saber},
  doi          = {10.1111/exsy.13586},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13586},
  shortjournal = {Expert Syst.},
  title        = {Ultimate pose estimation: A comparative study},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning approaches for neurological disease
prediction: A systematic review. <em>EXSY</em>, <em>41</em>(9), e13569.
(<a href="https://doi.org/10.1111/exsy.13569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a systematic and exhaustive review regarding the trends, datasets employed, as well as findings achieved in the last 11 years in neurological disorder prediction using machine learning models. In this work we present a comparison between the biomarkers used in ML field with the biomarkers that are obtained through other non-ml-based research fields. This will help in identifying the potential research gaps for ML domain. As the study of neurological disorders is a far-reaching task due to the wide variety of diseases, hence the scope of this study is restricted to the three most prevalent neurological diseases, that is, Alzheimer&#39;s, Parkinson&#39;s, and Autism Spectrum Disorder (ASD). From our analysis, it has been found that over time deep learning techniques especially Convolutional Neural Networks have proved to be beneficial for the disease prediction task. For this reason, Magnetic Resonance Imaging have been a popular modality across all three considered diseases. It is also notable that the employment of a transfer learning approach and maintenance of a global data centre helps in dealing with data scarcity problems for model training. The manuscript also discusses the potential challenges and future scope in this field. To the best of our knowledge, unlike other studies, this work attempts to put forth a conclusion of every article discussed highlighting the salient aspects of the major studies for a particular problem.},
  archive      = {J_EXSY},
  author       = {Ana Fatima and Sarfaraz Masood},
  doi          = {10.1111/exsy.13569},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e13569},
  shortjournal = {Expert Syst.},
  title        = {Machine learning approaches for neurological disease prediction: A systematic review},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bankruptcy prediction using optimal ensemble models under
balanced and imbalanced data. <em>EXSY</em>, <em>41</em>(8), e13599. (<a
href="https://doi.org/10.1111/exsy.13599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the performance of gradient boosting methods in bankruptcy prediction for a highly imbalanced dataset. We developed different heterogenous ensemble models based on three popular gradient boosting methods—XGBoost, LightGBM, and CatBoost. Our ensemble models were optimized using the cross-validation method and the results of the hold-out test sets showed that the optimized ensemble models not only outperform their base learners, but also improve the state-of-the-art benchmark results on the same dataset. Interestingly, we observed that the data oversampling technique that is commonly used to address the class imbalance issue had an adverse impact on our ensemble models&#39; performance. This indicates that our models are robust to the imbalanced dataset problem that typically degrades the classification performance of machine learning models.},
  archive      = {J_EXSY},
  author       = {Bahareh Amirshahi and Salim Lahmiri},
  doi          = {10.1111/exsy.13599},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13599},
  shortjournal = {Expert Syst.},
  title        = {Bankruptcy prediction using optimal ensemble models under balanced and imbalanced data},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A heuristic-based hybrid sampling method using a combination
of SMOTE and ENN for imbalanced health data. <em>EXSY</em>,
<em>41</em>(8), e13596. (<a
href="https://doi.org/10.1111/exsy.13596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health datasets often exhibit class imbalance, with healthy individuals being the majority class and patients being the minority class. As in many applications, machine learning methods are frequently used in the prediction and detection of diseases. The class imbalance presents a significant challenge for machine learning classifiers, particularly in their ability to accurately and effectively classify data from minority and majority classes. Therefore, data preprocessing is crucial before classifying the imbalanced data. In this study, we present GASMOTEPSO_ENN method that combines the synthetic minority oversampling technique (SMOTE) and edited nearest neighbour (ENN) algorithms using genetic algorithm (GA) and particle swarm optimization (PSO) heuristics as a preprocessing method to classify the imbalanced health datasets. In the experiments, chronic kidney disease (CKD), cerebral stroke prediction (CSP), and PIMA Indian diabetes (PID) datasets were utilized to assess the performance of the proposed method with metrics derived from the confusion matrix. The GASMOTEPSO_ENN method can classify the various diseases into different two classes of patients and healthy individuals with acceptable Matthews correlation coefficient (MCC) metric using the machine learning algorithms (Logistic regression (LR) 1.00 for CKD dataset, extreme gradient boosting (XGBoost) 0.94 for CSP dataset, and support vector machine (SVM) 0.87 for PID dataset). Moreover, the proposed method also performed well with other metrics in all datasets, and the analysis of the model results in relation to existing literature reveals that the proposed model demonstrably produces superior results.},
  archive      = {J_EXSY},
  author       = {Hatice Nizam-Ozogur and Zeynep Orman},
  doi          = {10.1111/exsy.13596},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13596},
  shortjournal = {Expert Syst.},
  title        = {A heuristic-based hybrid sampling method using a combination of SMOTE and ENN for imbalanced health data},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AXCF: Aspect-based collaborative filtering for explainable
recommendations. <em>EXSY</em>, <em>41</em>(8), e13594. (<a
href="https://doi.org/10.1111/exsy.13594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of the e-commerce market facilitated, users are often overwhelmed by the excessive online information, making item selection challenging. While recommendation services have significantly enhanced user experience and sales, these traditional models often overlook the complexity of user-item interactions and user preferences based on various item aspects. The proposed AXCF framework innovatively combines graph-based collaborative filtering (CF), which captures high-order connectivity between users and items, with aspect-based sentiment analysis (ABSA) to extract detailed user preferences from online reviews. This approach addresses the limitations of linear relationships in traditional CF models by incorporating deep neural networks and introduces a method to overcome the cold-start problem using online reviews as auxiliary information. By focusing on main aspects such as food, ambiance, and service derived from restaurant reviews, AXCF provides personalized recommendations with improved accuracy and explanatory power, demonstrating its superiority over existing models through experimental results. This study not only presents a significant advancement in recommender systems but also highlights the importance of high-order connectivity and aspect-based preferences in understanding and catering to user needs in the e-commerce platform.},
  archive      = {J_EXSY},
  author       = {Dongeon Kim and Qinglong Li and Dongsoo Jang and Jaekyeong Kim},
  doi          = {10.1111/exsy.13594},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13594},
  shortjournal = {Expert Syst.},
  title        = {AXCF: Aspect-based collaborative filtering for explainable recommendations},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta heuristic approaches for sentiment analysis.
<em>EXSY</em>, <em>41</em>(8), e13592. (<a
href="https://doi.org/10.1111/exsy.13592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the Internet, online microblogging and social networks have experienced tremendous growth. Millions of individuals share opinions on social networking platforms including Twitter, Facebook, YouTube, and Microblogging sites. Millions of individuals express their thoughts on social platforms namely Twitter, Facebook, YouTube, and microblogging sites. As user-generated content is growing to a huge extent on the internet, the analysis, extraction identification and classification of opinion and sentiment polarity of different aspects has now become a challenging issue in sentiment analysis. In sentiment analysis, aspect level opinion mining, opinion spam review detection, extraction and representation of most relevant accessed data from the user-generated data is a difficult process. The word vector representation using existing deep learning techniques affects the performance in aspect level opinion mining. Spammers propagate false opinion for payment exchange, which leads to degradation of the financial growth, business, and fame of the organization. To overcome the limitations aspect level opinion mining is used to recognize sentiment and polarity of aspects in the specified text. This work presents various input feature vectors of deep learning approaches aiming to improve performance in aspect level opinion mining. Next, three meta-heuristic algorithms with k-means clustering approach for classification of opinion spam reviews on social media transit tweets is presented. The proposed approaches are evaluated and compared with other existing approaches as well as other benchmark datasets namely Restaurant and Amazon reviews.},
  archive      = {J_EXSY},
  author       = {Meesala Shobha Rani and Sumathy Subramanian},
  doi          = {10.1111/exsy.13592},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13592},
  shortjournal = {Expert Syst.},
  title        = {Meta heuristic approaches for sentiment analysis},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The imbalance problem: A comparison of sampling approaches
using different parameters and feature selection methods in the context
of classification. <em>EXSY</em>, <em>41</em>(8), e13591. (<a
href="https://doi.org/10.1111/exsy.13591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common situation in classification tasks is to deal with unbalanced datasets, an issue that appears when the majority class(es) has a large number of samples compared to the minority class(es). This problem is even more significant when the datasets have a large number of features but only a few samples, as is the case with microarray datasets. Traditionally, an approach to alleviate this problem has been the application of sampling methods to obtain more balanced classes, increasing the number of samples in the minority class (replicating samples or generating new synthetic samples), or decreasing the number of samples in the majority class. In this study, we have compared different balancing methods, including a novel method that applies sampling in both the minority and majority classes. The interest in applying feature selection in combination with balancing methods has also been explored. In view of the results, a recommendation of sampling method, feature selection, and classifier is proposed to improve the classification results according to the type of dataset.},
  archive      = {J_EXSY},
  author       = {Jose L. Morillo-Salas and Verónica Bolón-Canedo and Amparo Alonso-Betanzos},
  doi          = {10.1111/exsy.13591},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13591},
  shortjournal = {Expert Syst.},
  title        = {The imbalance problem: A comparison of sampling approaches using different parameters and feature selection methods in the context of classification},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting word vectors for microtext. <em>EXSY</em>,
<em>41</em>(8), e13589. (<a
href="https://doi.org/10.1111/exsy.13589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of computer-mediated communication has resulted in a new form of written text called Microtext, which is very different from well-written text. Most previous approaches deal with microtext at the character level rather than just words resulting in increased processing time. In this paper, we propose to transform static word vectors to dynamic form by modelling the effect of neighbouring words and their sentiment strength in the AffectiveSpace. To evaluate the approach, we crawled Tweets from diverse topics and human annotation was used to label their sentiments. We also normalized the tweets to fix phonetic variations, spelling errors, and abbreviations manually. A total of 1432 out-of-vocabulary (OOV) texts and their IV texts made it to the final corpus with their corresponding polarity. To assess the quality of the corpus, we used several OOV classifiers such as linear regression and observed over 90% accuracy. Next, we inferred word vectors using a novel four-gram model based on sentiment intensity and reported accuracy on both open domain and closed domain sentiment classifiers. We observed an improvement in the range of 4–20 on Twitter, Movie and Airline reviews over baselines.},
  archive      = {J_EXSY},
  author       = {Iti Chaturvedi and Ranjan Satapathy and Curtis Lynch and Erik Cambria},
  doi          = {10.1111/exsy.13589},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13589},
  shortjournal = {Expert Syst.},
  title        = {Predicting word vectors for microtext},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linking negative herding effect to online public opinion: A
new BiGADG approach for sentiment classification. <em>EXSY</em>,
<em>41</em>(8), e13587. (<a
href="https://doi.org/10.1111/exsy.13587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The negative herding effect tends to cause the spread of negative emotions among netizens, resulting in significant public opinion crises. Therefore, linking the negative herding effect and online public opinion, we aim to exploring the connection between netizens&#39; emotions, opinions, and the negative herding effect. To better distinguish the emotions in texts, a new BiGADG ( Bi directional G ated recurrent unit based on A ttention with D istance-based dependency syntax G raph convolutional network) model is proposed in this article. This model includes primarily the proposed dependency syntax based on dependency distance, Bidirectional Gated Recurrent Unit (BiGRU), multi-head self-attention and Graph Convolutional Network (GCN). Particularly, the dependency syntax based on dependency distances incorporates word-to-word dependency distances into the traditional dependency syntax, which enriches the syntactic structural features of the text. After the experiments, the accuracy of BiGADG model is 86.53%, which is 3.11% higher compared to BERT model. It is evident that our model for sentiment classification has been improved. Based on the results of sentiment classification, we investigate the existence of negative herding behaviour and its underlying motivation in online public opinion. We have found that the negative herding effect, if not effectively managed, may trigger large-scale and uncontrollable public opinion crises.},
  archive      = {J_EXSY},
  author       = {Yiyin Zhou and Yanrong Hu and Hongjiu Liu and Ping Huang and Dan Dai},
  doi          = {10.1111/exsy.13587},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13587},
  shortjournal = {Expert Syst.},
  title        = {Linking negative herding effect to online public opinion: A new BiGADG approach for sentiment classification},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic interval prediction method based on
shape-adaptive quantile regression. <em>EXSY</em>, <em>41</em>(8),
e13585. (<a href="https://doi.org/10.1111/exsy.13585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces customized screening ensemble with shape-adaptive quantile regression (CseAQR), a novel probabilistic interval forecasting method built upon the quantile regression model. CseAQR utilizes ensemble learning to perform adaptive quantile regression prediction, which can handle the heteroscedasticity feature in time series data by using a weighted adaptive allocation loss function to enhance the adaptability of the basic quantile regression model on the dataset. The model performance predictor is used to select the optimal ensemble learner combination, assign reasonable adaptive weights to it, and obtain a preliminary prediction interval through weighted aggregation. Combining ensemble learners not only improves the accuracy and robustness of prediction intervals but also ensures the commutativity required for conformal prediction. Finally, the conformal prediction method is applied to locally adjust the prediction interval, constructing a more consistently aligned prediction interval with the actual data on a narrower basis.},
  archive      = {J_EXSY},
  author       = {Lin Li and Hua Wang and Yepeng Liu and Fan Zhang},
  doi          = {10.1111/exsy.13585},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13585},
  shortjournal = {Expert Syst.},
  title        = {Probabilistic interval prediction method based on shape-adaptive quantile regression},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semantic guide-based embedding method for knowledge graph
completion. <em>EXSY</em>, <em>41</em>(8), e13584. (<a
href="https://doi.org/10.1111/exsy.13584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding aims to map entities and relations into a low-dimensional vector space for easy manipulation. However, frequent entities are updated more often than infrequent ones during training, leading to inadequate representation of the latter&#39;s embeddings, which, in turn, affects the model&#39;s overall performance in downstream tasks. To address this issue, we propose a semantic information guide and enhance (SGE) method. The SGE tackles the heterogeneity in the frequency of entities through semantic reconstruction and a guidance network. The semantic reconstruction strengthens the semantic relevance among all entities and connects entities with different frequencies in semantic space. The guidance network extends these connections to knowledge space, enhancing the expression abilities of infrequent entities’ embeddings without compromising the embeddings of frequent entities. Experiments with four commonly used benchmark datasets show that the SGE method improves the performance of baseline models in most cases and that the method is model-independent.},
  archive      = {J_EXSY},
  author       = {Jinglin Zhang and Bo Shen and Tao Wang and Yu Zhong},
  doi          = {10.1111/exsy.13584},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13584},
  shortjournal = {Expert Syst.},
  title        = {A semantic guide-based embedding method for knowledge graph completion},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAFE: Unsupervised image feature extraction using
self-attention based feature extraction network. <em>EXSY</em>,
<em>41</em>(8), e13583. (<a
href="https://doi.org/10.1111/exsy.13583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to extract high-quality features from data is critical for machine learning applications. With the development of deep learning, various methods have been developed for image feature extraction, and unsupervised techniques have gained popularity due to their ability to operate without response variables. Autoencoders with encoder–decoder architectures are a common example of such techniques, but they are limited by a lack of proportional relationship between model reconstruction and encoder feature extraction performance. If the decoder is composed of multiple layers and mapping to a higher dimension is easier, the feature extraction performance of the encoder is likely to decrease. However, previous research has not adequately addressed this limitation. This study identifies the limitations of conventional unsupervised feature extraction techniques that utilize the encoder–decoder architecture, and proposes a novel feature extraction technique called SAFE, which utilizes a self-attention mechanism to eliminate decoder effects and improve the performance of encoder. To validate the effectiveness of the proposed model, we conducted experiments using diverse datasets (MNIST, Fashion MNIST, SVHN, and WM811K). The results of the experiments demonstrated that our proposed method exhibited, on average, 2%–10% higher performance in terms of accuracy and F-measure compared to the existing feature extraction techniques in the classification problem. While our research has limitations, specifically in its applicability only to the selection of image features, future studies should be undertaken to explore its potential application in various fields.},
  archive      = {J_EXSY},
  author       = {Yeoung Je Choi and Gyeong Taek Lee and Chang Ouk Kim},
  doi          = {10.1111/exsy.13583},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13583},
  shortjournal = {Expert Syst.},
  title        = {SAFE: Unsupervised image feature extraction using self-attention based feature extraction network},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effects of feature reduction on emotion recognition using
EEG signals and machine learning. <em>EXSY</em>, <em>41</em>(8), e13577.
(<a href="https://doi.org/10.1111/exsy.13577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography is a core technology of brain computer interfaces. Even a few number of electrodes can produce complex signals that are difficult to interpret. This is particularly true when trying to detect complex mental states, such as the identification of human emotions. This work analyzes the impact of feature reduction using the SJTU Emotion EEG data set, considering inter-subject and inter-session EEG classification. It is well established that EEG data tends to be subject dependent, and that the brain patterns produced by a person in response to a specific stimuli will tend to change over their lifetime, we can say that the above classification task is one of the most challenging test scenarios. We extract 1068 time and frequency domain features, single and multiple channels. Feature reduction was analyzed using both filter and wrapper based approaches. In the first group we use principal component analysis, Kernel PCA and Locally Linear Embedding, and in the second three meta-heuristic algorithms are employed. For classification, we apply multi-layer perceptron, quadratic discriminant analysis, random forest and support vector machines. Results show up to 100% accuracy on the median test error for some splits in the 10-fold cross-validation process. Best results were achieved by filter-based selection, with the best average accuracy reached by Random Forest (93.20%) and Multi-Layer Perceptron (92.71%), using components found with Kernel PCA. To the best of our knowledge, these classification results are the best ever achieved for the SEED data set on the subject-independent and session-independent problem. Comparisons using a Leave-One-Subject-Out cross-validation setting shows that the proposed methodology achieves state-of-the-art results using only 12 electrodes. These off-the-shelf ML techniques, coupled with filter based feature reduction, achieve the best results, outperforming complex classification models and wrapper based meta-heuristics.},
  archive      = {J_EXSY},
  author       = {Leonardo Trujillo and Daniel E. Hernandez and Adrian Rodriguez and Omar Monroy and Omar Villanueva},
  doi          = {10.1111/exsy.13577},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13577},
  shortjournal = {Expert Syst.},
  title        = {Effects of feature reduction on emotion recognition using EEG signals and machine learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An analysis to investigate plant disease identification
based on machine learning techniques. <em>EXSY</em>, <em>41</em>(8),
e13576. (<a href="https://doi.org/10.1111/exsy.13576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In agriculture, crops are severely affected by illnesses, which reduce their production every year. The detection of plant diseases during their initial stages is critical and thus needs to be addressed. Researchers have been making significant progress in the development of automatic plant disease recognition techniques through the utilization of machine learning (ML), image processing, and deep learning (DL). This study analyses the recent advancements made by researchers in the field of ML techniques for identifying plant diseases. This study also examines various methods used by researchers to produce ML solutions, such as image preprocessing, segmentation, and feature extraction. This study highlights the challenges encountered while creating plant disease identification systems, such as small datasets, image capture conditions, and the generalizability of the models, and discusses possible solutions to cater to these problems. Still, the development of a solution that automatically detects various plant diseases for various plant species remains a big challenge. To address these challenges, there is a need to create a system that is trained on an extensive dataset that contains images of various types of diseases a plant can suffer from, and plant images should be taken at various stages of the disease&#39;s development. This study further presents an analysis of various methods used at different stages of plant disease identification.},
  archive      = {J_EXSY},
  author       = {Sangeeta Duhan and Preeti Gulia and Nasib Singh Gill and Mohammad Yahya and Sangeeta Yadav and Mohamed M. Hassan and Hassan Alsberi and Piyush Kumar Shukla},
  doi          = {10.1111/exsy.13576},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13576},
  shortjournal = {Expert Syst.},
  title        = {An analysis to investigate plant disease identification based on machine learning techniques},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient multi-scale transformer for satellite image
dehazing. <em>EXSY</em>, <em>41</em>(8), e13575. (<a
href="https://doi.org/10.1111/exsy.13575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the impressive achievement of convolutional neural networks (CNNs) in grasping image priors from extensive datasets, they have been widely utilized for tasks related to image restoration. Recently, there is been significant progress in another category of neural architectures—Transformers. These models have demonstrated remarkable performance in natural language tasks and higher-level vision applications. Despite their ability to address some of CNNs limitations, such as restricted receptive fields and adaptability issues, Transformer models often face difficulties when processing images with a high level of detail. This is because the complexity of the computations required increases significantly with the image&#39;s spatial resolution. As a result, their application to most high-resolution image restoration tasks becomes impractical. In our research, we introduce a novel Transformer model, named DehFormer, by implementing specific design modifications in its fundamental components, for example, the multi-head attention and feed-forward network. Specifically, the proposed architecture consists of the three modules, that is, (a) multi-scale feature aggregation network (MSFAN), (b) the gated-Dconv feed-forward network (GFFN), (c) and the multi-Dconv head transposed attention (MDHTA). For the MDHTA module, our objective is to scrutinize the mechanics of scaled dot-product attention through the utilization of per-element product operations, thereby bypassing the need for matrix multiplications and operating directly in the frequency domain for enhanced efficiency. For the GFFN module, which enables only the relevant and valuable information to advance through the network hierarchy, thereby enhancing the efficiency of information flow within the model. Extensive experiments are conducted on the SateHazelk, RS-Haze, and RSID datasets, resulting in performance that significantly exceeds that of existing methods.},
  archive      = {J_EXSY},
  author       = {Lei Yang and Jianzhong Cao and Weining Chen and Hao Wang and Lang He},
  doi          = {10.1111/exsy.13575},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13575},
  shortjournal = {Expert Syst.},
  title        = {An efficient multi-scale transformer for satellite image dehazing},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of deep learning-based approaches for deepfake
content detection. <em>EXSY</em>, <em>41</em>(8), e13570. (<a
href="https://doi.org/10.1111/exsy.13570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning generative models have raised concerns as they can create highly convincing counterfeit images and videos. This poses a threat to people&#39;s integrity and can lead to social instability. To address this issue, there is a pressing need to develop new computational models that can efficiently detect forged content and alert users to potential image and video manipulations. This paper presents a comprehensive review of recent studies for deepfake content detection using deep learning-based approaches. We aim to broaden the state-of-the-art research by systematically reviewing the different categories of fake content detection. Furthermore, we report the advantages and drawbacks of the examined works, and prescribe several future directions towards the issues and shortcomings still unsolved on deepfake detection.},
  archive      = {J_EXSY},
  author       = {Leandro A. Passos and Danilo Jodas and Kelton A. P. Costa and Luis A. Souza Júnior and Douglas Rodrigues and Javier Del Ser and David Camacho and João Paulo Papa},
  doi          = {10.1111/exsy.13570},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13570},
  shortjournal = {Expert Syst.},
  title        = {A review of deep learning-based approaches for deepfake content detection},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Backtranslate what you are saying and i will tell who you
are. <em>EXSY</em>, <em>41</em>(8), e13568. (<a
href="https://doi.org/10.1111/exsy.13568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With this work, we hypothesize that semantically enriching a user&#39;s text corpus using backtranslation and expansion modules can improve performance for author profiling tasks. To perform this textual enrichment, we translate an author&#39;s representative text. Translations are made from one language—the source language—into another—the target language—and then back to the original one. Finally, we expand an author&#39;s text by integrating the original version with the back-translated one. Our framework includes these backtranslation and expansion modules followed by a SOTA classifier successfully employed for text classification. The framework is tested on three author profiling datasets from the last three years’ shared tasks on fake news, hate speech, irony and stereotypes detection hosted at the CLEF conference for the PAN Lab. This work is an extension of our previous one where we just presented our main idea. Here we improve our framework, and we also investigate more languages and more datasets. Finally, a qualitative analysis is provided. The results confirm that the backtranslation and expansion add-on modules improve model performance on all three datasets evaluated.},
  archive      = {J_EXSY},
  author       = {Marco Siino and Francesco Lomonaco and Paolo Rosso},
  doi          = {10.1111/exsy.13568},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13568},
  shortjournal = {Expert Syst.},
  title        = {Backtranslate what you are saying and i will tell who you are},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on computing, intelligence and data analytics
for wisdom (CIDA4Wisdom). <em>EXSY</em>, <em>41</em>(8), e13566. (<a
href="https://doi.org/10.1111/exsy.13566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Süleyman Eken and Serdar Solak and M. Hikmet Bilgehan Uçar and Zeynep Hilal Kilimci and Akhtar Jamil and Alaa Ali Hameed and Fausto Pedro Garcia Marquez},
  doi          = {10.1111/exsy.13566},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13566},
  shortjournal = {Expert Syst.},
  title        = {Special issue on computing, intelligence and data analytics for wisdom (CIDA4Wisdom)},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Airport take-off and landing optimization through genetic
algorithms. <em>EXSY</em>, <em>41</em>(8), e13565. (<a
href="https://doi.org/10.1111/exsy.13565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research addresses the crucial issue of pollution from aircraft operations, focusing on optimizing both gate allocation and runway scheduling simultaneously, a novel approach not previously explored. The study presents an innovative genetic algorithm-based method for minimizing pollution from fuel combustion during aircraft take-off and landing at airports. This algorithm uniquely integrates the optimization of both landing gates and take-off/landing runways, considering the correlation between engine operation time and pollutant levels. The approach employs advanced constraint handling techniques to manage the intricate time and resource limitations inherent in airport operations. Additionally, the study conducts a thorough sensitivity analysis of the model, with a particular emphasis on the mutation factor and the type of penalty function, to fine-tune the optimization process. This dual-focus optimization strategy represents a significant advancement in reducing environmental impact in the aviation sector, establishing a new standard for comprehensive and efficient airport operation management.},
  archive      = {J_EXSY},
  author       = {Fernando Guedan-Pecker and Cristian Ramirez-Atencia},
  doi          = {10.1111/exsy.13565},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13565},
  shortjournal = {Expert Syst.},
  title        = {Airport take-off and landing optimization through genetic algorithms},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel generative adversarial network-based
super-resolution approach for face recognition. <em>EXSY</em>,
<em>41</em>(8), e13564. (<a
href="https://doi.org/10.1111/exsy.13564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition is an essential feature required for a range of computer vision applications such as security, attendance systems, emotion detection, airport check-in, and many others. The super-resolution of subject images is an important and challenging element in numerous scenarios. At times the images are low resolution and need to be processed through super-resolution techniques to gain more accurate results. For the problem of image super-resolution, deep learning-based face recognition systems have been explored in recent years; however, low-resolution face recognition remains an arduous task. Generative adversarial network (GAN) based models are a promising approach to address this challenge. However, conventional GAN-based models may generate images that differ significantly from an original high-resolution image in the test set to the point that the identity of the target face may be changed. To address this shortcoming, we propose a novel U-Net style generator architecture, where skip-connections between the encoder and decoder layer can help in preserving the facial characteristics of the input image in the generated image, thus curbing the generator&#39;s ability to generate an entirely new image and training it to generate an image more similar in characteristics to the original image. In addition to statistical metrics like structural similarity index measure and Fréchet inception distance, we compute the pixel-wise distance between the original and model-generated images to ascertain that our model generates as close to the original images as possible. While we train the model for 4× super-resolution (64 × 64 images to 256 × 256), our architecture can also be trained for an arbitrary resizing scale. Finally, the number of faces detected over high-resolution images generated by our model is shown to be higher than state-of-the-art high-resolution image creation models for face recognition tasks.},
  archive      = {J_EXSY},
  author       = {Amit Chougule and Shreyas Kolte and Vinay Chamola and Amir Hussain},
  doi          = {10.1111/exsy.13564},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13564},
  shortjournal = {Expert Syst.},
  title        = {A novel generative adversarial network-based super-resolution approach for face recognition},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging quantum-inspired chimp optimization and deep
neural networks for enhanced profit forecasting in financial accounting
systems. <em>EXSY</em>, <em>41</em>(8), e13563. (<a
href="https://doi.org/10.1111/exsy.13563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning and metaheuristic algorithms have recently increased in various sciences, including financial accounting information systems (FAISs). However, the existence of large datasets has dramatically increased the complexity of these hybrid networks, so to address this shortcoming, this paper aims to develop a quantum-behaved chimp optimization algorithm (QCHOA) and deep neural network (DNN) for the prediction of the profit based on FAISs. Considering that there is no suitable dataset for the challenge, a novel dataset is developed utilizing the 15 features from the Chinese market dataset to compare more. This work designs QCHOA and five DNN-based predictors to forecast profit. These algorithms include the universal learning CHOA (ULCHOA), the niching CHOA (NCHOA) as the two best-modified versions of CHOA, the quantum-behaved whale optimization algorithm (QWOA), and the quantum-behaved grey wolf optimizer (QGWO) as the two best quantum-behaved optimizers as well as classic CHOA. The most effective deep learning-based predictors for forecasting the profit, ranked from highest to lowest, are DNN-QCHOA, DNN-NCHOA, DNN-QWOA, DNN-QGWO, DNN-ULCHOA, DNN-CHOA, and classic DNN, with corresponding ranking scores of 42, 36, 30, 24, 18, 12, and 6. As a final suggestion for profit prediction, the DNN-CHOA is shown to be the most accurate model.},
  archive      = {J_EXSY},
  author       = {Lin Zhang and Shtwai Alsubai and Abdullah Alqahtani and Abed Alanazi and Laith Abualigah},
  doi          = {10.1111/exsy.13563},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13563},
  shortjournal = {Expert Syst.},
  title        = {Leveraging quantum-inspired chimp optimization and deep neural networks for enhanced profit forecasting in financial accounting systems},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hate speech detection: A comprehensive review of recent
works. <em>EXSY</em>, <em>41</em>(8), e13562. (<a
href="https://doi.org/10.1111/exsy.13562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been surge in the usage of Internet as well as social media platforms which has led to rise in online hate speech targeted on individual or group. In the recent years, hate speech has resulted in one of the challenging problems that can unfurl at a fast pace on digital platforms leading to various issues such as prejudice, violence and even genocide. Considering the acceptance of Artificial Intelligence (AI) and Natural Language Processing (NLP) techniques in varied application domains, it would be intriguing to consider these techniques for automated hate speech detection. In literature, there have been efforts to recognize and categorize hate speech using varied Machine Learning (ML) and Deep Learning (DL) techniques. Hence, considering the need and provocations for hate speech detection we aim to present a comprehensive review that discusses fundamental taxonomy as well as recent advances in the field of online hate speech identification. There is a significant amount of literature related to the initial phases of hate speech detection. The background section provides a detailed explanation of the previous research. The subsequent section that follows is dedicated to examining the recent literature published from the year 2020 onwards. The paper presents some of the hate speech datasets considered for hate speech detection. Furthermore, the paper discusses different data modalities, namely, textual hate speech detection, multi-modal hate speech detection and multilingual hate speech detection. Apart from systematic review on hate speech detection, the paper also implement several multi-label models to compare the performance of hate speech detection by employing classic ML technique namely, Logistic Regression and DL technique namely, Long Short-Term Memory (LSTM) and a multiclass multi-label architecture. In the implemented architecture, we have derived two new elements to quantify the hatefulness and intensity of hatred to improve the results for hate speech detection using Indonesian tweet dataset. Empirical Analysis of the model reveals that the implemented approach outperforms and is able to achieve improved results for the underlying dataset.},
  archive      = {J_EXSY},
  author       = {Ankita Gandhi and Param Ahir and Kinjal Adhvaryu and Pooja Shah and Ritika Lohiya and Erik Cambria and Soujanya Poria and Amir Hussain},
  doi          = {10.1111/exsy.13562},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13562},
  shortjournal = {Expert Syst.},
  title        = {Hate speech detection: A comprehensive review of recent works},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). O-SCDR: Optimal cluster with attention based shared-account
cross-domain sequential recommendation using deep reinforcement learning
technique. <em>EXSY</em>, <em>41</em>(8), e13555. (<a
href="https://doi.org/10.1111/exsy.13555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation involves suggesting subsequent items in a series of user activities. When recommending relevant items to users within the same account, the challenge lies in discerning diverse user behaviours to provide tailored recommendations based on individual preferences and timing. Cross-domain sequential recommendation (CDSR) focuses on accurately extracting cross-domain user preferences from both within-sequence and between-sequence interactions among items. Current approaches typically concentrate on learning preferences within a single domain using intra-sequence item interactions, followed by a transfer module for cross-domain preferences. However, this sequential process and implicit method are constrained by the effectiveness of the transfer module and may overlook inter-sequence item associations. In this study, we propose an optimal cluster with attention-based shared-account cross-domain sequential recommendation (O-SCSR) system using deep reinforcement learning techniques. Our approach commences by formulating a modified hummingbird optimization (MHO) algorithm for clustering, effectively identifying latent users who share the same account to enhance the understanding of user interactions within shared-account scenarios. Additionally, we design a domain filter based on quantum classic deep reinforcement learning (QCDRL), intelligently selecting interactions contributing to O-SCSR. By quantifying rewards from transferred domain knowledge, the QCDRL-based filter retains only valuable interactions for the task of SCDR. Finally, we validate the efficacy of our proposed O-SCDR method using real-world datasets, namely HVIDEO and HAMAZON. Through simulation results comparing the O-SCDR system with existing state-of-the-art systems, we demonstrate its effectiveness and legitimacy.},
  archive      = {J_EXSY},
  author       = {M. Nanthini and K. Pradeep Mohan Kumar},
  doi          = {10.1111/exsy.13555},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13555},
  shortjournal = {Expert Syst.},
  title        = {O-SCDR: Optimal cluster with attention based shared-account cross-domain sequential recommendation using deep reinforcement learning technique},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved cooperative ant colony optimization for the
solution of binary combinatorial optimization applications.
<em>EXSY</em>, <em>41</em>(8), e13554. (<a
href="https://doi.org/10.1111/exsy.13554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary combinatorial optimization plays a crucial role in various scientific and engineering fields. While deterministic approaches have traditionally been used to solve these problems, stochastic methods, particularly metaheuristics, have gained popularity in recent years for efficiently handling large problem instances. Ant Colony Optimization (ACO) is among the most successful metaheuristics and is frequently employed in non-binary combinatorial problems due to its adaptability. Although for binary combinatorial problems ACO can suffer from issues such as rapid convergence to local minima, its eminently parallel structure means that it can be exploited to solve large and complex problems also in this field. In order to provide a versatile ACO implementation that achieves competitive results across a wide range of binary combinatorial optimization problems, we introduce a parallel multicolony strategy with an improved cooperation scheme to maintain search diversity. We evaluate our proposal (Binary Parallel Cooperative ACO, BiPCACO) using a comprehensive benchmark framework, showcasing its performance and, most importantly, its flexibility as a successful all-purpose solver for binary combinatorial problems.},
  archive      = {J_EXSY},
  author       = {Roberto Prado-Rodríguez and Patricia González and Julio R. Banga and Ramón Doallo},
  doi          = {10.1111/exsy.13554},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13554},
  shortjournal = {Expert Syst.},
  title        = {Improved cooperative ant colony optimization for the solution of binary combinatorial optimization applications},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced text classification through an improved discrete
laying chicken algorithm. <em>EXSY</em>, <em>41</em>(8), e13553. (<a
href="https://doi.org/10.1111/exsy.13553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of digital text documents presents a significant challenge for text classification algorithms, as the vast number of words in each document can hinder their efficiency. Feature selection (FS) is a crucial technique that aims to eliminate irrelevant features and enhance classification accuracy. In this study, we propose an improved version of the discrete laying chicken algorithm (IDLCA) that utilizes noun-based filtering to reduce the number of features and improve text classification performance. Although LCA is a newly proposed algorithm, it has not been systematically applied to discrete problems before. Our enhanced version of LCA employs different operators to improve both exploration and exploitation of this algorithm to find better solutions in discrete mode. To evaluate the effectiveness of the proposed method, we compared it with some conventional nature-inspired feature selection methods using various learning models such as decision trees (DT), K-nearest neighbor (KNN), Naive Bayes (NB), and support vector machine (SVM) on five benchmark datasets with three different evaluation metrics. The experimental results demonstrate the effectiveness of the proposed algorithm in comparison to the existing one. The code is available at https://github.com/m0javad/Improved-Discrete-Laying-Chicken-Algorithm .},
  archive      = {J_EXSY},
  author       = {Fatemeh Daneshfar and Mohammad Javad Aghajani},
  doi          = {10.1111/exsy.13553},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13553},
  shortjournal = {Expert Syst.},
  title        = {Enhanced text classification through an improved discrete laying chicken algorithm},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toxic language detection: A systematic review of arabic
datasets. <em>EXSY</em>, <em>41</em>(8), e13551. (<a
href="https://doi.org/10.1111/exsy.13551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of toxic language in the Arabic language has emerged as an active area of research in recent years, and reviewing the existing datasets employed for training the developed solutions has become a pressing need. This paper offers a comprehensive survey of Arabic datasets focused on online toxic language. We systematically gathered a total of 54 available datasets and their corresponding papers and conducted a thorough analysis, considering 18 criteria across four primary dimensions: availability details, content, annotation process, and reusability. This analysis enabled us to identify existing gaps and make recommendations for future research works. For the convenience of the research community, the list of the analysed datasets is maintained in a GitHub repository.},
  archive      = {J_EXSY},
  author       = {Imene Bensalem and Paolo Rosso and Hanane Zitouni},
  doi          = {10.1111/exsy.13551},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13551},
  shortjournal = {Expert Syst.},
  title        = {Toxic language detection: A systematic review of arabic datasets},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new computer-aided diagnostic method for classifying
anaemia disease: Hybrid use of tree bagger and metaheuristics.
<em>EXSY</em>, <em>41</em>(8), e13528. (<a
href="https://doi.org/10.1111/exsy.13528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anaemia occurs when the haemoglobin (Hgb) value falls below a certain reference range. It requires many blood tests, radiological images, and tests for diagnosis and treatment. By processing medical data from patients with artificial intelligence and machine learning methods, disease predictions can be made for newly ill individuals and decision-support mechanisms can be created for physicians with these predictions. Thanks to these methods, which are very important in reducing the margin of error in the diagnoses made by doctors, the evaluation of data records in health institutions is also important for patients and hospitals. In this study, six hybrid models are proposed to classify non-anaemia records, Hgb-anaemia, folate deficiency anaemia (FDA), iron deficiency anaemia (IDA), and B12 deficiency anaemia by combining artificial intelligence and machine learning methods TreeBagger, Crow Search Algorithm (CSA), Chicken Swarm Optimization Algorithm (CSO) and JAYA methods. The proposed hybrid models are analysed with two different approaches, with/without applying the SMOTE technique to achieve high performance by better emphasizing the importance of parameters. To solve the multiclass anaemia classification problem, fuzzy logic-based parameter optimization is applied to improve the class-based accuracy as well as the overall accuracy in the dataset. The proposed methods are evaluated using ROC criteria to build a prediction model to determine the anaemia type of anaemic patients. As a result of the study on the dataset taken from the Kaggle database, it is observed that the six proposed hybrid methods outperformed other studies using the same dataset and similar studies in the literature.},
  archive      = {J_EXSY},
  author       = {Nagihan Yagmur and Idiris Dag and Hasan Temurtas},
  doi          = {10.1111/exsy.13528},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e13528},
  shortjournal = {Expert Syst.},
  title        = {A new computer-aided diagnostic method for classifying anaemia disease: Hybrid use of tree bagger and metaheuristics},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data science approaches for sustainable development.
<em>EXSY</em>, <em>41</em>(7), e13613. (<a
href="https://doi.org/10.1111/exsy.13613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Serena Strazzullo and Paulo Cortez and Sérgio Moro},
  doi          = {10.1111/exsy.13613},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13613},
  shortjournal = {Expert Syst.},
  title        = {Data science approaches for sustainable development},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence for the practical assessment of
nutritional status in emergencies. <em>EXSY</em>, <em>41</em>(7),
e13550. (<a href="https://doi.org/10.1111/exsy.13550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a novel method for detecting child malnutrition based on artificial intelligence and facial photography. Estimates of severe and moderate acute malnutrition in children are critical for rapid emergency responses. However, the two traditional measurement methods, mid-upper arm circumference (MUAC) and weight-for-height (WFH), are impractical in conflict and catastrophic disaster situations. They require well-trained enumerators, cumbersome equipment, and close supervision. The Method for Extremely Rapid Observation of Nutritional Status (MERON) addresses the problem, using simple facial photographs. Facial features are extracted to predict Body Mass Index (BMI) in adults and Weight for Height Z Score (WFHZ) in children under five. MERON correctly predicts adult BMI classification with 78% accuracy. A variant of the model, trained on a sample of 3167 children in Kenya, successfully classified 60% of cases. On most measures, MERON was easier and more culturally acceptable to use than the traditional measurement methods. If MERON were to be trained and validated on a larger sample, with more extreme cases, it would provide a practical solution to a recurrent humanitarian problem.},
  archive      = {J_EXSY},
  author       = {Ben Watkins and Lameck Odallo and Jenny Yu},
  doi          = {10.1111/exsy.13550},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13550},
  shortjournal = {Expert Syst.},
  title        = {Artificial intelligence for the practical assessment of nutritional status in emergencies},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-label social short text classification method based
on contrastive learning and improved ml-KNN. <em>EXSY</em>,
<em>41</em>(7), e13547. (<a
href="https://doi.org/10.1111/exsy.13547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short texts on social platforms often have the problems of diverse categories and semantic sparsity, making it challenging to identify the diverse intentions of users. To address this issue, this article proposes a multi-label social short text classification method (IML-CL) based on contrastive learning and improved ml-KNN. First, a contrastive learning approach is employed to train a multi-label text classification model. This approach improves semantic sparsity by leveraging the knowledge from the existing samples to enrich the feature representation of short texts. Simultaneously, an improved ml-KNN algorithm is developed to enhance the accuracy of label prediction. This algorithm utilizes a two-layer nearest neighbor rule and introduces a penalty function and weight optimization. Next, the model generates the feature representation for the test sample and predicts its label. Additionally, the improved ml-KNN algorithm retrieves neighbors of the test sample and uses their label information for prediction. Finally, the two predictions are combined to obtain the final prediction, which accurately identifies the user&#39;s intention. The experimental results demonstrate that, on the dataset constructed in this article, the IML-CL method effectively boosts the performance of the baseline model.},
  archive      = {J_EXSY},
  author       = {Gang Tian and Jiachang Wang and Rui Wang and Guangxin Zhao and Cheng He},
  doi          = {10.1111/exsy.13547},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13547},
  shortjournal = {Expert Syst.},
  title        = {A multi-label social short text classification method based on contrastive learning and improved ml-KNN},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved minority attack detection in intrusion detection
system using efficient feature selection algorithms. <em>EXSY</em>,
<em>41</em>(7), e13546. (<a
href="https://doi.org/10.1111/exsy.13546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning and Data Mining algorithms are used extensively to enhance the performance of Intrusion Detection Systems. The number of training instances and the dimensionality of data are crucial factors affecting the performance of the model built during the training of any supervised learning algorithms. A sufficient proportion of instances having relevant features from all classes of attacks and normal traffic are considered most desirable while building the classification model that classifies the network traffic into attack and normal. This paper proposes a methodology to improve the accuracy of the model by giving importance to the relevant features that can contribute to model building. The feature selection using correlation-based and information gain-based techniques during training and testing contributes much to the detection of stealthier attacks and minority attacks. Then the features of the less detected attacks are identified as the second phase of the filter that is used to improve the performance. The relevant features of stealthy attacks are identified based on the correlation of corresponding features of the attack and normal data as the attacks are made stealthy mostly by making it resemble the normal traffic. Finally, the attacks that are rarely found in the training data are oversampled to improve their detection. CICIDS 2017 data set is employed as it comprises stealthier attacks generated using modern tools. NSL KDD data set is also used for evaluation to compare the proposed work with existing literature as it is used in most of the available literature. The results show superior performance with an accuracy of 99.8%, false positive rate of 0.2%, and a detection rate and 99.8%.},
  archive      = {J_EXSY},
  author       = {R. R. Rejimol Robinson and K. P. Anagha Madhav and Ciza Thomas},
  doi          = {10.1111/exsy.13546},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13546},
  shortjournal = {Expert Syst.},
  title        = {Improved minority attack detection in intrusion detection system using efficient feature selection algorithms},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel hybrid CNN methodology for automated leaf disease
detection and classification. <em>EXSY</em>, <em>41</em>(7), e13543. (<a
href="https://doi.org/10.1111/exsy.13543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant leaf diseases are challenging to categorize due to the complexity of the pattern variations and the high degrees of inter-class similarity. Plant ailments harm food quality and production. To ensure the quality and quantity of harvests, it is essential to protect plants from disease. Detection of diseases at an early stage is the main and the most complex task for farmers due to common morphological properties like colour, shape, texture, and edges. In this study, a Hybrid Deep Learning model named Hybrid-Convolutional Support Machine (H-CSM) based on ‘Support Vector Machine (SVM)’, ‘Convolutional Neural Network (CNN)’ and ‘Convolutional Block Attention Module (CBAM)’ is proposed for the early diagnosis and classification of leaf diseases in plants leaf. The suggested model can initially identify different plant leaf illnesses, although it is not constrained to these. A database of pictures of plant leaves is used to test the suggested method based on different evaluation parameters. The results were highly promising, with an accuracy of up to 98.72% which has been increased by applying better learning methods. Farmers can quickly identify 36 common diseases with a little instruction for 14 plant categories, enabling them to take prompt preventive measures using the proposed method.},
  archive      = {J_EXSY},
  author       = {Prabhjot Kaur and Anand Muni Mishra and Nitin Goyal and Sachin Kumar Gupta and Achyut Shankar and Wattana Viriyasitavat},
  doi          = {10.1111/exsy.13543},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13543},
  shortjournal = {Expert Syst.},
  title        = {A novel hybrid CNN methodology for automated leaf disease detection and classification},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting cardiovascular diseases from radiographic images
using deep learning techniques. <em>EXSY</em>, <em>41</em>(7), e13542.
(<a href="https://doi.org/10.1111/exsy.13542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease (CD) is one of the leading causes of death and disability across the globe. Chest x-rays (CXR) are crucial in detecting chest and CD. The CXR images present helpful information to the radiologist to identify a disease at an earlier stage. Several convolutional neural network (CNN) models for classifying the CXR images have been established. However, there is a demand for significant improvement in CNN models to generalize them in diverse datasets. In addition, healthcare centers require an effective model for identifying CD with limited resources. Therefore, the authors developed a CNN-based CD detector using CXR images. The proposed research employs the You Only Look Once, version 7 technique to extract features and DenseNet-161 for classifying the CXR images into normal and abnormal classes. The authors utilized datasets, including CheXpert and VinDr-CXR, for the performance evaluation. The findings reveal that the proposed study achieves an accuracy and F1-measure of 97.9, 97.47, 96.85, and 97.77 for the CheXpert and VinDr-CXR datasets, respectively. The recommended model required fewer parameters of 5.2 M and less computation time for predicting CD. The study&#39;s outcome can assist clinicians in detecting CD at the earliest stage.},
  archive      = {J_EXSY},
  author       = {Majed Alsanea and Ashit Kumar Dutta},
  doi          = {10.1111/exsy.13542},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13542},
  shortjournal = {Expert Syst.},
  title        = {Detecting cardiovascular diseases from radiographic images using deep learning techniques},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Judicial intelligent assistant system: Extracting events
from chinese divorce cases to detect disputes for the judge.
<em>EXSY</em>, <em>41</em>(7), e13540. (<a
href="https://doi.org/10.1111/exsy.13540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the formal procedure of Chinese civil cases, the textual materials provided by different parties describe the development process of the cases. It is a difficult but necessary task to extract the key information for the cases from these textual materials and to clarify the dispute focus of related parties. Currently, officers read the materials manually and use methods, such as keyword searching and regular matching, to get the target information. These approaches are time-consuming and heavily depend on prior knowledge and the carefulness of the officers. To assist the officers in enhancing working efficiency and accuracy, we conduct a case study of detecting disputes from Chinese divorce cases based on proposing a Two-Round-Labeling (TRL) event extracting technique in this article. We implement the Judicial Intelligent Assistant (JIA) system according to the proposed approach to (1) automatically extract focus events from divorce case materials, (2) align events by identifying co-reference among them, and (3) detect conflicts among events brought by the plaintiff and the defendant. With the JIA system, it is convenient for judges to determine the disputed issues in Chinese divorce cases. Experimental results demonstrate that the proposed approach and system can obtain the focus of Chinese divorce cases and detect conflicts more effectively and efficiently compared with the existing method.},
  archive      = {J_EXSY},
  author       = {Yuan Zhang and Chuanyi Li and Yu Sheng and Jidong Ge and Bin Luo},
  doi          = {10.1111/exsy.13540},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13540},
  shortjournal = {Expert Syst.},
  title        = {Judicial intelligent assistant system: Extracting events from chinese divorce cases to detect disputes for the judge},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hourly load prediction based feature selection scheme and
hybrid CNN-LSTM method for building’s smart solar microgrid.
<em>EXSY</em>, <em>41</em>(7), e13539. (<a
href="https://doi.org/10.1111/exsy.13539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The short-term load prediction is the critical operation in the peak demand administration and power generation scheduling of buildings that integrated the smart solar microgrid (SSM). Many research studies have proved that hybrid deep learning strategies achieve more accuracy and feasibility in practical applications than individual algorithms. Moreover, many buildings have integrated the SSM on the rooftop with the battery management system (BMS) to enhance energy efficiency management. However, the traditional methodologies only processed the weather parameters and power demand information for short-term load prediction, ignoring the collected data from SSM and BMS by the advanced metering infrastructures (AMI), which probably improved prediction accuracy. In this research, many accumulated data of building and SSM are collected before methodology implementation. Considering the diversities of accumulated parameters from SSM and BMS, an adaptive convolution neural network long short-term memory (CNN-LSTM) is proposed for hourly electrical load prediction. The CNN could extract the critical large-scale input feature, while the LSTM could achieve better accurate forecasts. The Pearson correlation matrix is calculated for the feature selection scheme from different data units. The hyperparameter tuning is utilized for obtaining the optimized hybrid CNN-LSTM algorithm. The K-fold cross-validation is employed for deep learning algorithm verification, which includes LSTM, GRU, CNN, and Bi-LSTM methodologies. The results prove that the hybrid CNN-LSTM achieved outperformed improvements, which are 20.57%, 29.63%, 19.06% in MSE, MAE, MAPE, and 21.24%, 22.02%, 3.82% in validating MSE, MAE, MAPE, respectively. The hybrid CNN and LSTM combined with the feature selection scheme achieve superior predicting accuracies, proving the adaptability ability for integrating into the energy management system (EMS) of the building&#39;s SSM.},
  archive      = {J_EXSY},
  author       = {Thao Nguyen Da and Ming-Yuan Cho and Phuong Nguyen Thanh},
  doi          = {10.1111/exsy.13539},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13539},
  shortjournal = {Expert Syst.},
  title        = {Hourly load prediction based feature selection scheme and hybrid CNN-LSTM method for building&#39;s smart solar microgrid},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reduction of financial tick big data for intraday trading.
<em>EXSY</em>, <em>41</em>(7), e13537. (<a
href="https://doi.org/10.1111/exsy.13537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various neural network architectures are often used to forecast movements in financial markets. Most research in quantitative analytics in finance uses interval financial data as this reduces the raw tick big data, but the averaging can lose key behaviour patterns. This work presents an alternative novel method to reduce raw tick data whilst retaining important information for training, as demonstrated with intraday trading using the EURO/USD currency pair. This time series reduction method focuses on short periods preceding significant movements in financial features and allows the most popular neural network architectures to be applied using less powerful but more readily available computer resources. It is shown that the proposed data preprocessing method for machine learning and other AI-techniques successfully reduced the size of the selected dataset covering a three-year period (2018–2021) by 275 times.},
  archive      = {J_EXSY},
  author       = {Vitaliy Milke and Cristina Luca and George B. Wilson},
  doi          = {10.1111/exsy.13537},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13537},
  shortjournal = {Expert Syst.},
  title        = {Reduction of financial tick big data for intraday trading},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expert systems supporting strategic decisions.
<em>EXSY</em>, <em>41</em>(7), e13531. (<a
href="https://doi.org/10.1111/exsy.13531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Maria José Sousa and Álvaro Rocha},
  doi          = {10.1111/exsy.13531},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13531},
  shortjournal = {Expert Syst.},
  title        = {Expert systems supporting strategic decisions},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved particle swarm optimization algorithm for
scheduling tasks in cloud environment. <em>EXSY</em>, <em>41</em>(7),
e13529. (<a href="https://doi.org/10.1111/exsy.13529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing provide services dynamically according to the contract between service providers and users. However, Inappropriateness of scheduling task on VMs can lead huge resource waste and load unbalance, which becomes a seriously challenging problem. Current Swarm intelligence algorithms like genetic algorithm (GA), particle swarm optimization (PSO) are combination of random initialization and local search algorithm. It avoids inconsistent results for different problem instances. However, existing Swarm intelligence works sometimes search the optima without analysing task scheduling situations comprehensively, global search efficiency is low and convergence is too early. In this paper, we propose SNSK-IPSO algorithm, which develops as a two-phases algorithm: enumerating all distributed solutions between VMs and tasks, finding the optimal solution through IPSO. It not only minimizes the execution time, but also improves resource utilization and load balance. Several experiments demonstrate that our novel algorithm outperforms others in terms of achieving load balance, higher resource utilization and lower execution times.},
  archive      = {J_EXSY},
  author       = {Zi-Ren Wang and Xiao-Xiang Hu and Peng Wei and Bo Yuan},
  doi          = {10.1111/exsy.13529},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13529},
  shortjournal = {Expert Syst.},
  title        = {An improved particle swarm optimization algorithm for scheduling tasks in cloud environment},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on international conference on computing and
communication networks (ICCCN2022). <em>EXSY</em>, <em>41</em>(7),
e13500. (<a href="https://doi.org/10.1111/exsy.13500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Deepak Gupta},
  doi          = {10.1111/exsy.13500},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13500},
  shortjournal = {Expert Syst.},
  title        = {Special issue on international conference on computing and communication networks (ICCCN2022)},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Magnetic resonance imaging features to evaluate the neonatal
hypoglycemia brain injury and investigation of related risk factors
under the fuzzy c-means clustering intelligent algorithm. <em>EXSY</em>,
<em>41</em>(7), e13491. (<a
href="https://doi.org/10.1111/exsy.13491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research was aimed to analyse the application value of magnetic resonance imaging based on Fuzzy C-means (FCM) algorithm in neonatal hypoglycemia brain injury (HBI), and explore the risk factors related to the occurrence of brain injury in children, to provide guidance for clinical diagnosis and treatment. 114 children with hypoglycemia were divided into brain injury group (58 cases) and no brain injury group (56 cases) according to whether they had brain injury or not. The MRI image signal performance, general data, average minimum blood glucose value, duration of hypoglycemia, first feeding time, age of onset of hypoglycemia, and algorithm segmentation performance of the two groups of patients were observed and compared. Furthermore, the Logistic factor analysis was carried out to summarize the MRI characteristics and related risk factors of neonatal hypoglycemic brain injury. The results showed that the average minimum blood glucose (1.09 ± 0.53 mmoL/L) in the brain injury group was lower than that in the non-brain injury group (1.75 ± 0.49 mmoL/L), and the duration of hypoglycemia (43.1 ± 21.07 h) was higher than that in the non-brain injury group (13.79 ± 6.81 h), p &lt; 0.05. The first feeding time and age of hypoglycemia in the brain injury group were higher than those in the non-brain injury group, showing a difference with p &lt; 0.05. In the brain injury group, all 58 cases showed high DWI (diffusion weighted imaging) signal at the damaged site at the early stage of MRI (magnetic resonance imaging), and 23 cases (39.66%) were involved in parieto-occipital lobe. Image segmentation coefficient of Vpc increased significantly under FCM clustering algorithm ( p &lt; 0.05). Late first feeding time, low blood sugar level, and long duration were high risk factors for hypoglycemic brain injury. In conclusion, MRI images based on FCM clustering algorithm had higher image quality. Late first feeding time, low blood sugar level, and long duration of hypoglycemia were high risk factors for hypoglycemic brain injury.},
  archive      = {J_EXSY},
  author       = {Dongmei Jin and Zhongxu Zhang and Yanru Ma and Zhushan Dai and Lili Zhao and Tongyao Ma and Guoping Chen},
  doi          = {10.1111/exsy.13491},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13491},
  shortjournal = {Expert Syst.},
  title        = {Magnetic resonance imaging features to evaluate the neonatal hypoglycemia brain injury and investigation of related risk factors under the fuzzy C-means clustering intelligent algorithm},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extractive text summarization for biomedical transcripts
using deep dense LSTM-CNN framework. <em>EXSY</em>, <em>41</em>(7),
e13490. (<a href="https://doi.org/10.1111/exsy.13490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Parminder Pal Singh Bedi and Manju Bala and Kapil Sharma},
  doi          = {10.1111/exsy.13490},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13490},
  shortjournal = {Expert Syst.},
  title        = {Extractive text summarization for biomedical transcripts using deep dense LSTM-CNN framework},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A SCORPAN-based data warehouse for digital soil mapping and
association rule mining in support of sustainable agriculture and
climate change analysis in the maghreb region. <em>EXSY</em>,
<em>41</em>(7), e13464. (<a
href="https://doi.org/10.1111/exsy.13464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable agriculture is becoming increasingly important in the face of growing environmental challenges. One key aspect of sustainable agriculture is managing soil resources effectively. In this context, digital soil mapping (DSM) has emerged as a powerful tool to understand soil variability better and inform land management decisions. This paper proposes a comprehensive data warehouse for DSM that supports climate change analysis. Our architecture integrates frequent itemset mining (FMI) and association rules mining (ARM) to extract insights from large-scale soil data. We review related studies in soil data warehousing and ARM, identify gaps, and propose a data warehouse architecture leveraging the galaxy multidimensional model for DSM based on the SCORPAN model, which incorporates all relevant soil forming factors. We employ and compare A-priori, FP-growth, and ECLAT algorithms to efficiently mine frequent itemsets and generate association rules. Our intensive experiments evaluation demonstrates that FP-growth outperforms the other algorithms in accuracy, scalability, and speed and requires less memory. Additionally, we utilized correlation metrics for ARM, such as lift, cosine, kulc, and Imbalance ratio, to obtain the most significant and relevant association rules. These rules provide valuable insights into the complex relationships between soil properties and environmental factors, which can inform land management decisions and improve sustainable agriculture practices. This work contributes to the growing body of research on DSM and data-driven approaches to sustainable agriculture.},
  archive      = {J_EXSY},
  author       = {Widad Hassina Belkadi and Yassine Drias and Habiba Drias and Mustapha Dali and Samira Hamdous and Nadjet Kamel and Djemai Aksa},
  doi          = {10.1111/exsy.13464},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13464},
  shortjournal = {Expert Syst.},
  title        = {A SCORPAN-based data warehouse for digital soil mapping and association rule mining in support of sustainable agriculture and climate change analysis in the maghreb region},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital innovation and transformation capabilities in a
large company. <em>EXSY</em>, <em>41</em>(7), e13452. (<a
href="https://doi.org/10.1111/exsy.13452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a dynamic and quickly changing world, digital innovation and transformation can help enhancing organizational efficiency and have competitive advantage. In addition, the digital innovation and transformation can facilitate the development of a growth-oriented culture of workforce. Therefore, this study aims at developing the digital innovation and transformation capabilities at large organization. The study uses a four-layered process for driving the digital innovation and transformation in the case organization. The study positions the digital innovation and transformation mechanism through the lens of the dynamic capability view and institutional theories. The findings indicate the emergence of digital champions as change agents, a digital council as a monitoring channel, a digital core team as technological enablers and reverse mentoring as an innovative cultural dimension of change facilitating dynamic capabilities of the organization. In terms of institutional pressures, coercive and mimetic pressures emerged as major contributors to the advancement of the digital skill set and digital mindset of the experienced workforce especially, supported by Hofstede&#39;s cultural dimensions. Study further delineates the insights for practice, theory and policy. To this end, the study showcases the future scope of work at the case company, since it is an ongoing project on digital innovation and transformation to make organization and related infrastructure resilient, digital and innovative.},
  archive      = {J_EXSY},
  author       = {Shivam Gupta and Sachin Modgil and Bharat Bhushan and Sachin Kamble and Juhi Mishra},
  doi          = {10.1111/exsy.13452},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13452},
  shortjournal = {Expert Syst.},
  title        = {Digital innovation and transformation capabilities in a large company},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nature-inspired artificial bee colony-based hyperparameter
optimization of CNN for anomaly detection in induction motor.
<em>EXSY</em>, <em>41</em>(7), e13407. (<a
href="https://doi.org/10.1111/exsy.13407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Induction Motor (IM) is one of the most frequently used prime movers in most industrial and transportation systems. The motor&#39;s stable and safe operation directly influences the secure and reliable operation of such prime movers. Developing an intelligent fault diagnosis system for such motors is very significant. This paper presents an intelligent fault diagnosis method based on the improved functionality of a Convolutional Neural Network (CNN) through its hyperparameter optimization using a nature-inspired Artificial Bee Colony Optimization (ABCO) algorithm. The proposed diagnostic method introduces and analyses various possible mechanical and electrical faults in the IM. The validation of the proposed method is presented with three different modalities, including vibration, acoustic, and infrared thermography, with their comparative performance analysis. Vibration and acoustic-based detection are done with time-frequency scalograms using Constant Q Transform (CQT), which provides enhanced time resolution for lower and higher frequencies. The obtained result indicates that the infrared thermography-based anomaly detection outperforms the vibration and acoustic-based diagnosis with 100% classification accuracy. The results signify the potential to diagnose different mechanical and electrical faults in IM with substantial reliability and robustness.},
  archive      = {J_EXSY},
  author       = {Anurag Choudhary and Tauheed Mian and Shahab Fatima and B. K. Panigrahi},
  doi          = {10.1111/exsy.13407},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13407},
  shortjournal = {Expert Syst.},
  title        = {Nature-inspired artificial bee colony-based hyperparameter optimization of CNN for anomaly detection in induction motor},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence governance: Ethical considerations
and implications for social responsibility. <em>EXSY</em>,
<em>41</em>(7), e13406. (<a
href="https://doi.org/10.1111/exsy.13406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of articles are increasingly raising awareness on the different uses of artificial intelligence (AI) technologies for customers and businesses. Many authors discuss about their benefits and possible challenges. However, for the time being, there is still limited research focused on AI principles and regulatory guidelines for the developers of expert systems like machine learning (ML) and/or deep learning (DL) technologies. This research addresses this knowledge gap in the academic literature. The objectives of this contribution are threefold: (i) It describes AI governance frameworks that were put forward by technology conglomerates, policy makers and by intergovernmental organizations, (ii) It sheds light on the extant literature on ‘AI governance’ as well as on the intersection of ‘AI’ and ‘corporate social responsibility’ (CSR), (iii) It identifies key dimensions of AI governance, and elaborates about the promotion of accountability and transparency; explainability, interpretability and reproducibility; fairness and inclusiveness; privacy and safety of end users, as well as on the prevention of risks and of cyber security issues from AI systems. This research implies that all those who are involved in the research, development and maintenance of AI systems, have social and ethical responsibilities to bear toward their consumers as well as to other stakeholders in society.},
  archive      = {J_EXSY},
  author       = {Mark Anthony Camilleri},
  doi          = {10.1111/exsy.13406},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13406},
  shortjournal = {Expert Syst.},
  title        = {Artificial intelligence governance: Ethical considerations and implications for social responsibility},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sustainable circular supply chain network design model for
electric vehicle battery production using internet of things and big
data. <em>EXSY</em>, <em>41</em>(7), e13395. (<a
href="https://doi.org/10.1111/exsy.13395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing and developing sustainable circular supply chain networks for electric vehicle (EV) lithium-ion battery recycling and production requires complex environmental sustainability and economic viability assessment. EVs use a lot of data for battery management and delivering optimum performance, and the Internet of Things (IoT) plays a major role in managing this data. This study develops a bi-objective mixed-integer linear programming model for designing a sustainable circular supply chain to manage the manufacturing, remanufacturing, and distribution of EV lithium-ion batteries under uncertainty using the IoT and big data. The proposed model simultaneously minimizes total costs and CO 2 emissions and uses IoT to improve network performance and create a traceable and secure environment. A fuzzy multi-objective method solves the bi-objective optimization model under uncertainty, and a simulation algorithm examines the effectiveness of the proposed model through simulated problems.},
  archive      = {J_EXSY},
  author       = {Madjid Tavana and Mahsa Sohrabi and Homa Rezaei and Shahryar Sorooshian and Hassan Mina},
  doi          = {10.1111/exsy.13395},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13395},
  shortjournal = {Expert Syst.},
  title        = {A sustainable circular supply chain network design model for electric vehicle battery production using internet of things and big data},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progress and prospects of quantum computing in sustainable
development: An analytical review. <em>EXSY</em>, <em>41</em>(7),
e13389. (<a href="https://doi.org/10.1111/exsy.13389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of quantum computing holds tremendous potential for advancing sustainable development through the design and development of sustainable materials and technologies. The domain has undergone significant growth and development in recent years, necessitating a quantitative and inclusive analysis of research advancements to comprehend research trends and directions. Henceforth, a scientometric analysis of scholarly literature from the Scopus database has been conducted to understand the research status of the discipline by analysing publication trends, most cited article analysis, document co-citation analysis, and author co-citation analysis. Furthermore, the CiteSpace tool is used to fulfil the visualization needs and comprehensive analysis of scientific literature to highlight emerging topics and quantum computing trends. The results of the study unveil spin-based quantum computing and lithographically defined quantum dots with exchange coupling as highly promising frontiers for exploration. These cutting-edge advancements exhibit exceptional potential for attaining remarkable fidelity in the quantum computing domain. Furthermore, it uncovers a trove of potential research areas for delving into the captivating phenomenon of quantum entanglement. Nuclear magnetic resonance, quantum dots, and spontaneous parametric down-conversion techniques emerge as promising areas, enticing scholars to embark on scientific quests aimed at unravelling the intricate complexities of entanglement. The study offers a comprehensive overview of quantum computing research advancements, serving as a valuable resource for researchers and policymakers driving sustainable development.},
  archive      = {J_EXSY},
  author       = {Vaishali Sood and Rishi Pal Chauhan},
  doi          = {10.1111/exsy.13389},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13389},
  shortjournal = {Expert Syst.},
  title        = {Progress and prospects of quantum computing in sustainable development: An analytical review},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid firefly particle swarm optimisation algorithm for
feature selection problems. <em>EXSY</em>, <em>41</em>(7), e13363. (<a
href="https://doi.org/10.1111/exsy.13363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection techniques play a vital role in the processes that deal with enormous amounts of data. These techniques have become extremely crucial and necessary for data mining and machine learning problems. Researchers have always been in a race to develop and provide libraries and frameworks to standardise this procedure. In this work, we propose a hybrid meta-heuristic algorithm to facilitate the problem of feature selection for classification problems in machine learning. It is a python based, lucid and efficient algorithm geared towards optimising and striking a balance between the number of features selected and accuracy. The proposed work is a binary hybrid of existing meta-heuristic algorithms, the particle swarm optimisation (PSO) algorithm, and the firefly algorithm (FA) such that it blends the best of each algorithm to provide an optimised and efficient way of solving the said problem. The suggested approach is assessed against six datasets from different domains that are publicly available at the UCI repository to demonstrate its validity. The datasets are Breast cancer, Iris, WBC, Mushroom, Glass ID, and Abalone. This approach has also been evaluated against similar, such evolutionary-based approaches to prove its superiority. Various metrics such as accuracy, precision, recall, f1 score, number of selected features, and run time have been analysed, measured, and compared. The hybrid firefly particle swarm optimisation algorithm is found to be suitable for feature selection problems.},
  archive      = {J_EXSY},
  author       = {Mahmoud Ragab},
  doi          = {10.1111/exsy.13363},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13363},
  shortjournal = {Expert Syst.},
  title        = {Hybrid firefly particle swarm optimisation algorithm for feature selection problems},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An empirical hybridized siamese network using hypercube
natural aggregation algorithm for handling imbalance data learning.
<em>EXSY</em>, <em>41</em>(7), e13338. (<a
href="https://doi.org/10.1111/exsy.13338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dealing with imbalanced data is a common challenge in machine learning, where one class has significantly fewer examples than another. Successfully addressing this challenge requires careful consideration of the data, algorithm, and evaluation metrics to ensure that the model accurately predicts the minority class. In this study, we present a hybrid approach called Siamese-HYNAA, which combines a Siamese network and a population-based optimizer hypercube natural aggregation algorithm (HYNAA) to generate candidate solutions for augmenting the minority class. We collected 10 imbalanced datasets ranging from 1.81 to 8.78 imbalanced ratios and built solution pairs based on correctly predicted candidate solutions using support vector machine (SVM). We then fed these solutions to the Siamese network, which employs a one-shot learning approach to improve predictions with fewer candidate solutions. However, we found that SVM predicted only a small number of minority class samples accurately, prompting us to optimize the number of candidate solution pairs using HYNAA to generate more synthetic samples for the Siamese network. We evaluated our proposed strategy against basic SMOTE and our previous work, SMOTE-PSOEV, using various performance measures, including ROC-AUC learning curves, sensitivity, specificity, accuracy, Characteristic stability index, balanced accuracy, F1-score, informedness, markedness, and execution time. Our results indicate that Siamese-HYNAA generates promising results for imbalanced data.},
  archive      = {J_EXSY},
  author       = {Subhashree Rout and Pradeep Kumar Mallick and Annapareddy V. N. Reddy and Meshal Alharbi and Ahmed Alkhayyat},
  doi          = {10.1111/exsy.13338},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13338},
  shortjournal = {Expert Syst.},
  title        = {An empirical hybridized siamese network using hypercube natural aggregation algorithm for handling imbalance data learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Genetic algorithm-based price and warranty optimization in
software systems. <em>EXSY</em>, <em>41</em>(7), e13334. (<a
href="https://doi.org/10.1111/exsy.13334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to increasing competition, the marketing of commercial software products is a challenging task. Diverse marketing strategies are adopted to tap the huge market share. Among these, pricing and warranty are the most significant ones. The warranty policies organizations adopt usually have two dimensions: duration and effort spent during the warranty phase. In this paper, two variables are broadly taken: testing, which is an essential phase during software development, and warranty, which is a crucial component of after-sales service. Here, we will formulate an optimization problem where returns and expenses will be influenced by price, testing resources spent, the duration for which testing is done, warranty duration and effort spent during warranty for the software product. The product sales are assumed to follow a Stochastic Bass model based on Non-Homogeneous Poisson Process (NHPP). A free on-site support warranty with an upper limit on duration and effort covers the 2D failure process. The numerical illustration is presented wherein profit is maximized by conjointly optimizing five variables, that is price, testing duration, testing effort, warranty duration and warranty effort. This study emphasizes on application of a popular soft computing, metaheuristic technique named genetic algorithm (GA) to solve an optimization problem for a software system. The solution to the Optimization problem is obtained using the GA tool in MATLAB. The results obtained have relevance in the industry for finding optimal values of significant variables before the release of newly developed software products to the ultimate client. The concept discussed in this paper has diverse applications in computer engineering, software and hardware industry.},
  archive      = {J_EXSY},
  author       = {Rajat Arora and Abhishek Tandon and Anu G. Aggarwal and Rubina Mittal},
  doi          = {10.1111/exsy.13334},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13334},
  shortjournal = {Expert Syst.},
  title        = {Genetic algorithm-based price and warranty optimization in software systems},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Marine radar monitoring IoT system and case study of target
detection based on PPI images. <em>EXSY</em>, <em>41</em>(7), e13333.
(<a href="https://doi.org/10.1111/exsy.13333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure the ship navigation safety, it is necessary to detect the targets in the background of sea clutter around the ship. The most commonly used target detection equipments for ship navigation are marine radar and automatic identification system (AIS) device. However, marine radar echoes are often mixed with multiple clutter, and weak targets are not always equipped with AIS device, neither provides AIS information with low accuracy, leading to difficulty with target detection. Moreover, existing marine monitoring systems are accustomed to using the traditional information fusion methods for target detection, and the accuracy of identifying the weak targets is relatively low. To make full use of radar echo and AIS information and improve the accuracy of target detection, a Marine radar monitoring Internet of Things system is proposed, in which marine radars work in both scanning and staring modes. By adopting image segmentation and deep learning methods, the proposed design can enable accurate perception of weak target detection based on plan-position indicator (PPI) images. A case study based on the selected X-band radar datasets shows that the proposed design can achieve high target identification accuracy.},
  archive      = {J_EXSY},
  author       = {Qiming Zhang and Yang Li and Chenguang Guo and Shibo Yin and Lin Ma and Yunrong Zhu},
  doi          = {10.1111/exsy.13333},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13333},
  shortjournal = {Expert Syst.},
  title        = {Marine radar monitoring IoT system and case study of target detection based on PPI images},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Code-mixed hindi-english text correction using fuzzy graph
and word embedding. <em>EXSY</em>, <em>41</em>(7), e13328. (<a
href="https://doi.org/10.1111/exsy.13328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interaction via social media involves frequent code-mixed text, spelling errors and noisy elements, which creates a bottleneck in the performance of natural language processing applications. This proposed work is the first approach for code-mixed Hindi-English social media text that comprises language identification, detection and correction of non-word (Out of Vocabulary) errors as well as real-word errors occurring simultaneously. Each identified language (Devanagari Hindi, Roman Hindi, and English) has its own complexities and challenges. Errors are detected individually for each language and a suggestive list of the erroneous words is created. After this, a fuzzy graph between different words of the suggestive lists is generated using various semantic relations in Hindi WordNet. Word embeddings and Fuzzy graph-based centrality measures are used to find the correct word. Several experiments are performed on different social media datasets taken from Instagram, Twitter, YouTube comments, Blogs, and WhatsApp. The experimental results demonstrate that the proposed system corrects out-of-vocabulary words as well as real-word errors with a maximum recall of 0.90 and 0.67, respectively for Dev_Hindi and 0.87 and 0.66, respectively for Rom_Hindi. The proposed method is also applied for state-of-art sentiment analysis approaches where the F1-score has been visibly improved.},
  archive      = {J_EXSY},
  author       = {Minni Jain and Rajni Jindal and Amita Jain},
  doi          = {10.1111/exsy.13328},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13328},
  shortjournal = {Expert Syst.},
  title        = {Code-mixed hindi-english text correction using fuzzy graph and word embedding},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep-SQA: A deep learning model using motor activity data
for objective sleep quality assessment assisting digital wellness in
healthcare 5.0. <em>EXSY</em>, <em>41</em>(7), e13321. (<a
href="https://doi.org/10.1111/exsy.13321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable sensor-based devices like actigraphs collect motor activity data which provide objective measures of physical activity. This research puts forward a novel methodology for assessment of objective sleep quality using actigraph recordings of motor activity. High level features of sequential motor activity data are extracted using Long-Short Term Memory (LSTM) model which are then paired with a significant statistical feature namely, zero percent which describes the percentage of events with zero activity over a series. Overlapping sliding window is used to input sequences into LSTM to capture superior features in activity recordings. The predictive ability of the combined feature vector is evaluated using support vector machine (SVM) classifier. This hybrid LSTM-SVM framework is validated on a benchmark dataset namely, the MESA Actigraphy dataset and achieves an accuracy of 85.62% for sleep quality prediction. Effectiveness of overlapping sliding window and statistical feature are evaluated, and their significance is validated. It is validated that the concept of overlapping sliding window improves the performance accuracy by 3.51% and the use of discriminative statistical feature improves sleep quality prediction task by 2.95%. Comparison with state of the art validates that this is the first study using objective sleep quality indicator for assessment of sleep quality via actigraph-based motor activity data.},
  archive      = {J_EXSY},
  author       = {Anshika Arora and Pinaki Chakraborty and M. P. S. Bhatia and Akshi Kumar},
  doi          = {10.1111/exsy.13321},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13321},
  shortjournal = {Expert Syst.},
  title        = {Deep-SQA: A deep learning model using motor activity data for objective sleep quality assessment assisting digital wellness in healthcare 5.0},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cervical precancerous lesion classification using quantum
invasive weed optimization with deep learning on biomedical pap smear
images. <em>EXSY</em>, <em>41</em>(7), e13308. (<a
href="https://doi.org/10.1111/exsy.13308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical imaging devices, in general, have been made and used a lot lately to examine the insides of the body during diagnostic and analytic procedures. Biomedical imaging gives accurate information about metabolites, which can be used to find and classify diseases because it is not invasive. For the study of cervical cancer (CC), the pap smear is a crucial type of biological imaging. CC is a crucial reason to enhance the rate of women&#39;s mortalities. Proper screening of pap smear images is critical for assisting in the early detection and analysis of CC. Computer-aided systems for cancerous cell recognition need well established artificial intelligence (AI) methods. In this study, we introduce an automated Cervical Precancerous Lesion Classification using Quantum Invasive Weed Optimization with Deep Learning (CPLC-QIWODL) on biomedical pap smear images. The presented CPLC-QIWODL technique examines the pap smear images for cervical cancer classification. To do so, the presented CPLC-QIWODL technique pre-processes the biomedical images using a Gabor filtering (GF) approach. Moreover, the CPLC-QIWODL technique uses a deep convolutional neural network-based SqueezeNet system for feature extraction. Furthermore, the hyperparameter tuning of the SqueezeNet methodology takes place using the QIWO technique, showing the novelty of the work. Finally, to classify CC, the deep variational autoencoder (DVAE) model is applied. The experimental result analysis of the CPLC-QIWODL technique is tested using a benchmark medical image database. Extensive comparative results demonstrated the enhanced outcomes of the CPLC-QIWODL technique over other existing algorithms, with a maximum accuracy of 99.07%.},
  archive      = {J_EXSY},
  author       = {Awanish Kumar Mishra and Indresh Kumar Gupta and Tarun Dhar Diwan and Swati Srivastava},
  doi          = {10.1111/exsy.13308},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13308},
  shortjournal = {Expert Syst.},
  title        = {Cervical precancerous lesion classification using quantum invasive weed optimization with deep learning on biomedical pap smear images},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge computing driven sustainable development: A case study
on professional farmer cultivation mechanism. <em>EXSY</em>,
<em>41</em>(7), e13285. (<a
href="https://doi.org/10.1111/exsy.13285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New-type professional farmers are new-type rural talents with scientific and cultural literacy and professional knowledge of agriculture under the background of rural revitalization. The new type of professional farmers is of great significance for accelerating agricultural modernization and promoting the construction of a powerful modern socialist country. In the process of cultivating new types of professional farmers in rural areas, it is necessary to break through the previous informatization constraints of poor information flow, and give full play to the role of advanced information technology to promote the sustainable economic development. This paper proposes a new type of professional farmer cultivation platform based on edge computing architecture to improve the cultivation efficiency of new type of professional farmers, so as to better promote the development of rural informatization. Aiming at the challenges of energy consumption under the edge platform architecture, this paper proposes a dynamic scheduling method for semi-online tasks for edge computing platforms. In order to evaluate the effectiveness of the algorithm, the method in this paper is compared with the classic scheduling algorithm, and simulated and verified on the CloudSim platform. Experimental results show that the proposed method outperforms other algorithms in the task completion time metric. With the expansion of task scale, more energy consumption can be saved by using the algorithm proposed in this paper.},
  archive      = {J_EXSY},
  author       = {Hui Yuan and Hong Nie},
  doi          = {10.1111/exsy.13285},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13285},
  shortjournal = {Expert Syst.},
  title        = {Edge computing driven sustainable development: A case study on professional farmer cultivation mechanism},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-feature gait analysis approach using deep learning in
constraint-free environment. <em>EXSY</em>, <em>41</em>(7), e13274. (<a
href="https://doi.org/10.1111/exsy.13274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A quantitative gait assessment system is crucial for clinical analysis and decision-making. Such rigorous evaluation involves costly clinical setups and domain experts for observation and analysis. To circumvent such constraints, the proposed work is conducted in a markerless environment and divided into three stages: First, we prepared a markerless gait database using videos from MNIT RAMAN LABORATORY in Jaipur. Second, we adapt the skeletal landmark data to generate kinematic gait characteristics comparable to gold-standard marker-based techniques. We provide a novel set of parameters based on video sequences and spatiotemporal and sagittal kinematic parameters to optimize accuracy and reliability. Third, we develop multi-feature based gait analysis, an ensemble model based on Convolutional Neural Networks + LSTM (Long-Short Term Memory), for gait classification. In addition, we deployed transfer learning models to correlate with our ensemble model. The findings indicate that gait analysis can be used successfully in a low-cost clinical gait monitoring system in a constraint-free environment. While considering the multiple gait variables, our proposed model attained an accuracy of 95.3%. Our model for quantifying gait analysis will improve access to quantitative gait analysis in clinics and rehabilitation centers and enable researchers to conduct large-scale studies for gait-related disorders.},
  archive      = {J_EXSY},
  author       = {Dimple Sethi and Chandra Prakash and Sourabh Bharti},
  doi          = {10.1111/exsy.13274},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13274},
  shortjournal = {Expert Syst.},
  title        = {Multi-feature gait analysis approach using deep learning in constraint-free environment},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bug severity prediction using LDA and sentiment scores: A
CNN approach. <em>EXSY</em>, <em>41</em>(7), e13264. (<a
href="https://doi.org/10.1111/exsy.13264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crucial part of the software development cycle is software maintenance. The demands included in the software management are fault fixes and request to change or bring a new feature. If priority is not given to these demands, then it may lead to customer dissatisfaction, inefficient planning, and software failure as well. Therefore, it is important to study the severity of the bug reports to maintain the efficiency of the software. Various research has been conducted in the past to predict the severity of the paper using text mining focusing only on the content of the bug reports. The sentiment of the user while reporting a bug also plays a vital role. In this study, we will be focusing on two aspects, that is, sentiment and content to improve the prediction. We propose a prediction model based on LDA to study the content aspect and emotion analysis to study the sentiment aspect. The model is validated on the datasets collected from the Eclipse project using Convolutional Neural Network (CNN). The results show that the CNN model effectively utilizes the content and sentiment aspect of the data to handle the severity prediction. CNN has weight sharing feature that decreases the number of parameters used for training. It also improves generalization and overfitting is avoided The Accuracy, Precision, Recall, and F-measure are improved when both aspects are taken into account rather than considering only content.},
  archive      = {J_EXSY},
  author       = {Ritu Bibyan and Sameer Anand and Ajay Jaiswal and Anu Gupta Aggarwal},
  doi          = {10.1111/exsy.13264},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e13264},
  shortjournal = {Expert Syst.},
  title        = {Bug severity prediction using LDA and sentiment scores: A CNN approach},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New bag of features using reinforcement aquila optimization
and weighted bayesian gaussian mixture modelling for dental images.
<em>EXSY</em>, <em>41</em>(6), e13453. (<a
href="https://doi.org/10.1111/exsy.13453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dentistry diseases are worldwide concern with the fact that 5% of medical budget is spent on it. Dental diagnosis requires frequent visits to clinic and multiple personal check-ups by an expert. This delays the process of diagnosis as well as introduces the danger of oral infection spread. Automation is boon to speed up the process of diagnosis, reduce expert&#39;s involvement and help in handling volume of patients. In situations like pandemic as seen in this decade, computer based automatic systems in health have proven their importance and necessity. For dental diagnosis, images are useful tool for better anatomical views and accurate decision of treatment. However, manual dental analysis increases load on dentists for initial check-up that can be easily performed with automated systems or self-kits. Oral cavity is a dental illness, if not identified and treated on time may lead to other serious ailments. This work presents a framework that performs tooth image classification for cavity and non-cavity with new bag of features (NBoF) method. NBoF method is an attempt to improve the performance of bag of features (BoF) using the proposed reinforcement Aquila optimization (RAO) and weighted Bayesian Gaussian mixture modelling (WBGMM). An analysis of the performance of the NBoF using the proposed RAO and WBGMM is conducted using standard metrics. The comparative study of results proves that the proposed NBoF method outperforms the existing state-of-the-art algorithms.},
  archive      = {J_EXSY},
  author       = {Surbhi Vijh and Neetu Gupta and Sumit Kumar and Priti Bansal},
  doi          = {10.1111/exsy.13453},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13453},
  shortjournal = {Expert Syst.},
  title        = {New bag of features using reinforcement aquila optimization and weighted bayesian gaussian mixture modelling for dental images},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effectiveness of deep learning in early-stage oral cancer
detections and classification using histogram of oriented gradients.
<em>EXSY</em>, <em>41</em>(6), e13439. (<a
href="https://doi.org/10.1111/exsy.13439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of oral cancer (OC) improves survival prospects. Artificial intelligence (AI) is gaining popularity in diagnostic medicine. Oral cancer is a primary global health concern, accounting for 177,384 deaths in 2018; most cases occur in low- and middle-income countries. Automated disease identification in the oral cavity may be facilitated by the ability to identify both possibly and definite malignant lesions. This study aimed to examine the evidence currently available on the effectiveness of AI in diagnosing OC. They highlighted the ability of AI to analyse and identify the early stages of OC. Furthermore, radial basis function networks (RBFN) were employed to develop automated systems to generate intricate patterns for this challenging operation. The stochastic gradient descent algorithm (SGDA) selected the model parameters that best matched the predicted and observed results. It can be used. The initial data was collected for this study to evaluate. Two deep learning-based computer vision algorithms have been developed to recognize and categorize oral lesions, which is necessary for the early detection of oral cancer. Several examples of HoG include the Canny edge detector, SIFT (scale invariant and feature transform), and SIFT (scale invariant and feature transform). In computer vision and image processing, it is used to find objects. We investigated the potential uses of deep learning-based computer vision techniques in oral cancer and the viability of an automated system for OC recognition based on photographic images. That made calculations to determine the accuracy, sensitivity, specificity, and receiver operating characteristic curve areas across all validation datasets, including internal, external, and clinical validation (AUC). The RBFN-SDC model outperformed all others. For 1000 data points, the accuracy of the RBFN-SDC model is 99.99%, while the accuracy of the R-CNN, CNN, DCNN, and SVM models is 91.54%, 90.14%, 93.89%, and 94.87%, respectively.},
  archive      = {J_EXSY},
  author       = {Chiranjit Dutta and Prasad Sandhya and Kandasamy Vidhya and Ramanathan Rajalakshmi and Devasahayam Ramya and Kotakonda Madhubabu},
  doi          = {10.1111/exsy.13439},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13439},
  shortjournal = {Expert Syst.},
  title        = {Effectiveness of deep learning in early-stage oral cancer detections and classification using histogram of oriented gradients},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel coarse-to-fine computational method for
three-dimensional landmark detection to perform hard-tissue
cephalometric analysis. <em>EXSY</em>, <em>41</em>(6), e13365. (<a
href="https://doi.org/10.1111/exsy.13365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cephalometric analysis has an important and essential role to treat the patients with craniofacial and dentofacial deformities. Cephalometric analysis is a relationship of human geometry which can be quantified and derived from the linear and angular measurements. To treat any patient, such analysis is required to be performed on the Head X-ray image of the patient. The objective of the proposed work is to detect cephalometric landmarks automatically on CT (computational tomography) images. Twenty cephalometric landmarks were automatically localized on 100 CT scans using hybrid coarse-to-fine computational method. The mean error for landmark detection was computed as 2.88 mm and standard deviation of 1.85 mm. The highest detection rate for cephalometric landmarks was received as 100% for Nasion landmark under 4-mm error and the highest detection rate was received as 99% for Nasion landmark under 3-mm error. The less number of datasets were used for the training and higher number of datasets were used for the testing. Compared to the literature methods, our method used higher number of datasets to demonstrate the accuracy of the proposed method.},
  archive      = {J_EXSY},
  author       = {Kusum Yadav and Kawther A. Al-Dhlan and Hamad A. Alreshidi and Gaurav Dhiman and Wattana Golf Viriyasitavat and Abdullah Zaid Almankory and Kadiyala Ramana and S. Vimal and Venkatesan Rajinikanth},
  doi          = {10.1111/exsy.13365},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13365},
  shortjournal = {Expert Syst.},
  title        = {A novel coarse-to-fine computational method for three-dimensional landmark detection to perform hard-tissue cephalometric analysis},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Faster region-based convolutional neural networks with you
only look once multi-stage caries lesion from oral panoramic x-ray
images. <em>EXSY</em>, <em>41</em>(6), e13326. (<a
href="https://doi.org/10.1111/exsy.13326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks and artificial intelligence find more applications in dentistry. It treats dental caries, the most prevalent type of dental illness worldwide. Even though dental caries can be prevented and treated, they typically cause dental discomfort and tooth loss. For dental caries to be treated quickly and effectively, comprehensive detection may be needed, a combination of techniques that include eye inspection, probing, using a dental probe, and using a hand-held mirror, and the individual application of each of these techniques, can quickly identify large caries cavities. Long-established caries detection techniques help to locate only partially hidden but still accessible holes. Deep learning (DL) techniques have produced remarkable diagnostic results in radiology. This study aimed to classify various radiographic extensions on panoramic films using DL techniques, identify caries lesions using these techniques, and compare the results to those of dentists with extensive training. Faster region-based convolutional neural networks (R-CNN) is a newly discovered field of medical research that is rapidly expanding and has produced outstanding results in diagnosing and prognosis of pathology and radiology conditions. In this study, dental cavities were detected and analysed using periapical radiographs to evaluate the accuracy of the Faster R-CNN algorithm. Because these three caries were derived from the oral panoramic images, we designed You Only Look Once Version 3 (YOLOv3) as a U-shaped network with a large-scale axial attention module. We also compare the effectiveness of YOLOv3&#39;s segmentation to that of other industrial standards. Experiments show that our proposed method, Fast R-CNN–YOLOv3, achieves higher accuracy in segmenting the three distinct caries level. The proposed model (R-CNN–YOLOv3) achieved an effective result with a precision of 97.183%.},
  archive      = {J_EXSY},
  author       = {Jayaraj Velusamy and T. Rajajegan and Sini Anna Alex and M. Ashok and A. V. R. Mayuri and Siripuri Kiran},
  doi          = {10.1111/exsy.13326},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13326},
  shortjournal = {Expert Syst.},
  title        = {Faster region-based convolutional neural networks with you only look once multi-stage caries lesion from oral panoramic X-ray images},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An information extraction method based on improved mixed
text density web pages. <em>EXSY</em>, <em>41</em>(6), e13267. (<a
href="https://doi.org/10.1111/exsy.13267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the effect of web page information extraction, this paper proposes an improved information extraction method of mixed text density grids. Under various power constraints of the relay node itself, this paper proposes a design scheme of joint beamforming and artificial noise based on safety and rate maximisation. Furthermore, with the help of semidefinite relaxation techniques and first-order approximations, it can be approximated as a semidefinite programming problem that is easy to solve. In addition, this paper uses an iterative algorithm based on a continuous convex approximation to process data to improve the accuracy of web page data extraction. The experimental results show that the information extraction method based on improved mixed text density webpages proposed in this paper has a good information extraction effect.},
  archive      = {J_EXSY},
  author       = {Yuan Zhou and Xiaojun Yin and Jingchen Yan},
  doi          = {10.1111/exsy.13267},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13267},
  shortjournal = {Expert Syst.},
  title        = {An information extraction method based on improved mixed text density web pages},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved differential bond energy algorithm with fuzzy
merging method to improve the document clustering for information
mining. <em>EXSY</em>, <em>41</em>(6), e13261. (<a
href="https://doi.org/10.1111/exsy.13261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast and diversified text materials on the internet in recent years have drastically increased the importance of information mining. By organizing documents into cohesive groupings, a document clustering method is a suitable tool for dealing with massive amounts of documents. Text documents, on the other hand, include sparse and uninformative features like noise, unrelated, and unneeded features, which reduce the efficiency of the document clustering methods. For noise removal, this work employs the hyperclique pattern-based data cleaner pre-processing approach. Then, differential bond energy algorithm (DBEA) is combined with a fuzzy merging approach and termed improved differential bond energy algorithm with fuzzy merging (IDBEFM) to handle the issues present in text document clustering. It seeks to discover and display natural variable clusters among large amounts of data. IDBEFM clusters are documented in three steps: in the first step, a cluster similarity matrix is instantiated by applying the DBEA. The second step develops an innovative technique for automatically dividing the cluster matrix into compact cohesive clusters, and in the third step, a fuzzy merging approach is used for combining identical clusters by the correlations and interrelationships between the resultant clusters. The test findings demonstrated that the accomplishment of the proposed technique is much superior to standard clustering algorithms.},
  archive      = {J_EXSY},
  author       = {S. Tejasree and B. Chandra Mohan},
  doi          = {10.1111/exsy.13261},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13261},
  shortjournal = {Expert Syst.},
  title        = {An improved differential bond energy algorithm with fuzzy merging method to improve the document clustering for information mining},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ELSTM: An improved long short-term memory network language
model for sequence learning. <em>EXSY</em>, <em>41</em>(6), e13211. (<a
href="https://doi.org/10.1111/exsy.13211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The gated structure of the long short-term memory (LSTM) alleviates the defects of gradient disappearance and explosion in the recurrent neural network (RNN). It has received widespread attention in sequence learning such as text analysis. Although LSTM has good performance in handling remote dependencies, information loss often occurs in long-distance transmission. We propose a new model called ELSTM based on the computational complexity and gradient dispersion in the traditional LSTM model. This model simplifies the input gate of LSTM, reduces some time complexity by reducing some components, and improves the output gate. By introducing the exponential linear unit activation layer, the problem of gradient dispersion is alleviated. Comparing the new model with multiple existing models, when predicting language sequences, the time used by the model has been greatly reduced, and the language confusion has been reduced, showing good performance.},
  archive      = {J_EXSY},
  author       = {Zhi Li and Qing Wang and Jia-Qiang Wang and Han-Bing Qu and Jichang Dong and Zhi Dong},
  doi          = {10.1111/exsy.13211},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13211},
  shortjournal = {Expert Syst.},
  title        = {ELSTM: An improved long short-term memory network language model for sequence learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-dimensional dental image segmentation and
classification using deep learning with tunicate swarm algorithm.
<em>EXSY</em>, <em>41</em>(6), e13198. (<a
href="https://doi.org/10.1111/exsy.13198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dentistry frequently makes use of intraoral scanning technologies to digitally acquire the three-dimensional (3D) geometry of teeth. In recent times, dental clinics over the globe utilize used computer aided diagnosis (CAD) models to make treatment plans, for example, orthodontics. Orthodontic CAD system acts as a vital part of the advanced dentistry field. A 3D dental model, computed by patient impression, as input and aids dentist in the extraction, moving, deletion, and rearranging of teeth to simulate treatment output. Tooth segmentation and labelling is the basic and foremost element of the CAD model which needs to be addressed. Automated segmentation and classification of 3D dental images using advanced machine learning and deep learning (DL) models become essential. This article introduces a new 3D dental image segmentation and classification using DL with tunicate swarm algorithm (3DDISC-DLTSA) model. The major intention of the 3DDISC-DLTSA system is to segment the tooth model and identify seven distinct tooth types. To accomplish this, the presented 3DDISC-DLTSA model performs image pre-processing in two stages namely image filtering and U-Net segmentation. In addition, the 3DDISC-DLTSA model derives DenseNet-169 model for feature extraction purposes. For the recognition and classification of tooth type, the TSA based hyperparameter tuning process is carried out which helps to accomplish maximum classification performance. A wide range of experimental analyses is performed and the outcomes are inspected under many aspects. On dataset-1, 3DDISC-DLTSA model accuracy rose by 96.67%. On dataset-3, 3DDISC-DLTSA model accuracy rose by 97.48% and algorithm accuracy by 97.35%. The 3DDISC-DLTSA model outperformed more modern models, according to the comparative investigation.},
  archive      = {J_EXSY},
  author       = {Harshavardhan Awari and Neelakandan Subramani and Avanija Janagaraj and Geetha Balasubramaniapillai Thanammal and Jackulin Thangarasu and Rachna Kohar},
  doi          = {10.1111/exsy.13198},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13198},
  shortjournal = {Expert Syst.},
  title        = {Three-dimensional dental image segmentation and classification using deep learning with tunicate swarm algorithm},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A predictive typological content retrieval method for
real-time applications using multilingual natural language processing.
<em>EXSY</em>, <em>41</em>(6), e13172. (<a
href="https://doi.org/10.1111/exsy.13172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) is widely used in multi-media real-time applications for understanding human interactions through computer aided-analysis. NLP is common in auto-filling, voice recognition, typo-checking applications, and so forth. Multilingual NLP requires vast data processing and interaction recognition features for leveraging content retrieval precision. To strengthen this concept, a predictive typological content retrieval method is introduced in this article. The proposed method maximizes and relies on distributed transfer learning for training multilingual interactions with pitch and tone features. The phonetic pronunciation and the previous content-based predictions are forwarded using knowledge transfer. This knowledge is modelled using the training data and precise contents identified in the previous processing instances. For this purpose, the auto-fill and error correction data are augmented with the training and multilingual processing databases. Depending on the current prediction and previous content, the knowledge base is updated, and further training relies on this feature. Therefore, the proposed method accurately identifies the content across multilingual NLP models.},
  archive      = {J_EXSY},
  author       = {S. Baskar and Sunita Dhote and Tejas Dhote and Gopalan Jayanandini and Duraisamy Akila and Srinath Doss},
  doi          = {10.1111/exsy.13172},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13172},
  shortjournal = {Expert Syst.},
  title        = {A predictive typological content retrieval method for real-time applications using multilingual natural language processing},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The refined management of medical finance combined with
information technology construction. <em>EXSY</em>, <em>41</em>(6),
e13151. (<a href="https://doi.org/10.1111/exsy.13151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the information construction and taking a hospital as an example, this paper analyzes the current situation, existing problems, and reasons for financial management. This paper puts forward the scheme of optimizing hospital financial management and points out that accounting, asset management, cost management, and performance are all based on accounting. Even though with the continuous improvement and development of hospital informatization, various infrastructures have been built, the problems are more prominent. This can be solved by improving the focus on the products, and processes by reducing waste, or by increasing the quality. In hospitals, the core layer of hospital information technology is the financial management construction and development, and it is also an indispensable factor. It is better to improve the hospital&#39;s financial fine management level and promote the hospital&#39;s sustainable development. It is not enough to improve the assessment management and comprehensive budget management. It is necessary to integrate the perfect Health Information System, enterprise resource planning system, scientific research system, and structured query language database, establish a comprehensive financial management platform, and obtain data information and indicators of various economic operations in hospitals. Experiments show that the consumables sorting optimization algorithm in this paper can improve the equipment management of medical finance to some extent. In other aspects, this system also incorporates information technology to make enhancements to the original financial management.},
  archive      = {J_EXSY},
  author       = {Dong-e Liu},
  doi          = {10.1111/exsy.13151},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13151},
  shortjournal = {Expert Syst.},
  title        = {The refined management of medical finance combined with information technology construction},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performance improvement and lyapunov stability analysis of
nonlinear systems using hybrid optimization techniques. <em>EXSY</em>,
<em>41</em>(6), e13140. (<a
href="https://doi.org/10.1111/exsy.13140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using Hybrid optimization algorithms for nonlinear systems analysis is a novel approach. It is a powerful technique that uses the exploitation ability of one algorithm and the exploration ability of another algorithm, to find the best solution. Literature survey reveals that hybrid algorithms not only show quality response but also give faster convergence of error for nonlinear systems. In this paper, hybrid optimization techniques based proportional integral derivative (PID) controller is used for benchmark problems: Continuous stirred tank reactor (CSTR), Inverted pendulum and blood glucose system. Two recent hybrid algorithms: Particle swarm optimization-Gravitational search algorithm (PSO-GSA) and Particle swarm optimization-Grey wolf algorithm (PSO-GWO) are implemented to control the temperature and concentration of CSTR, pendulum angle of inverted pendulum, glucose concentration and insulin level of blood glucose system. In PID and PSOGWO algorithms, the exploration abilities of GSA and GWO combined with the exploitation ability of PSO have been used. The performance of these algorithms is then compared with individual PSO, GSA, and GWO algorithms proving their superiority. Stability is ensured using the Lyapunov approach while the robustness of the systems is checked using the parameter perturbation technique. Simulation results show substantial improvement in the performance of these systems by using these meta-heuristic hybrid optimization techniques. A comparative analysis of these algorithms has also been done.},
  archive      = {J_EXSY},
  author       = {Vishal Srivastava and Smriti Srivastava and Gopal Chaudhary and Xiomara Patricia Blanco Valencia},
  doi          = {10.1111/exsy.13140},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13140},
  shortjournal = {Expert Syst.},
  title        = {Performance improvement and lyapunov stability analysis of nonlinear systems using hybrid optimization techniques},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Natural language processing with deep learning enabled
hybrid content retrieval model for digital library management.
<em>EXSY</em>, <em>41</em>(6), e13135. (<a
href="https://doi.org/10.1111/exsy.13135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, natural language processing (NLP) technique has received significant attention in content retrieval (CR) domain. The emergence of digital libraries, in recent years, enables people from across the globe to access and store books, documents, and literature of multiple kinds. The development of NLP models has considerably improved the performance in terms of digital library management. In this scenario, artificial intelligence-based expert systems are required to handle massive quantities of data that exist in digital libraries and achieve effective CR performance. In this background, the current study designs NLP with deep learning enabled hybrid content retrieval (NLPDL-HCR) model for digital library management. The aim of the presented NLPDL-HCR is to effectually retrieve the images as well as textual data from digital libraries based on a user&#39;s query. The proposed NLPDL-HCR model encompasses two major stages namely, text retrieval and image retrieval (IR). During text retrieval process, the proposed NLPDL-HCR model includes term frequency inverse document frequency vectorizer with optimal gated recurrent unit (GRU) model. The hyperparameters of the GRU model are optimally adjusted with the help of RMSProp approach. Besides, the IR process involves three sub-processes namely, densely connected networks-based feature extraction, butterfly optimization algorithm-based hyperparameter tuning, and Euclidean distance-based similarity measurement. The experimental analysis results, accomplished by the proposed NLPDL-HCR model using benchmark datasets, highlighted its superior performance over recent state-of-the-art approaches.},
  archive      = {J_EXSY},
  author       = {Mahmoud Ragab and Anas Almuhammadi and Romany F. Mansour and Seifedine Kadry},
  doi          = {10.1111/exsy.13135},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13135},
  shortjournal = {Expert Syst.},
  title        = {Natural language processing with deep learning enabled hybrid content retrieval model for digital library management},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotional expression in jewellery design under the
background of artificial intelligence. <em>EXSY</em>, <em>41</em>(6),
e13134. (<a href="https://doi.org/10.1111/exsy.13134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the emotional expression effect of jewellery, this article combines artificial intelligence technology to analyse the emotional expression in jewellery design. Aiming at the characteristics of high-gloss materials such as strong reflectivity and translucency, starting from the classic ray tracing algorithm, this article strengthens the reflected light and projected light on the surface of the material, and proposes a realistic rendering algorithm for high-gloss materials. Moreover, this article introduces the CUDA parallel computing architecture to give full play to the computing power of multi-core computers, accelerate the calculation speed of the ray tracing algorithm, save the consumption of storage space, and improve the speed of real-time rendering under the premise of ensuring realism. The experimental research results show that the emotional expression system for smart jewellery design proposed in this article has a certain effect, which can effectively improve the effect of jewellery design.},
  archive      = {J_EXSY},
  author       = {Meng Liang and Zhao Wanli},
  doi          = {10.1111/exsy.13134},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13134},
  shortjournal = {Expert Syst.},
  title        = {Emotional expression in jewellery design under the background of artificial intelligence},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling and control of fuzzy-based systems using
intelligent water drop algorithm. <em>EXSY</em>, <em>41</em>(6), e13124.
(<a href="https://doi.org/10.1111/exsy.13124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification and controlling of non-linear and complex dynamical systems have been a major concern and a serious topic of study in the field of adaptive control systems. Various techniques based on artificial intelligence have been used for this purpose. For modelling and control of plants, it is essential to optimize the values of the parameters associated with the respective technique. Searching the right or optimum value of the associated parameters is known as optimization. The intelligent water drop (IWD) algorithm is an optimization algorithm derived from the natural intelligence of water drop, which always chooses the shortest possible path while travelling from rivers and lakes towards oceans and seas. This paper introduces a new way of visualizing and understanding the IWD algorithm and presents its respective application in the field of modelling and controlling of non-linear dynamic systems with the help of illustrative examples. Fuzzy type-1 and recurrent fuzzy systems have been optimized using this algorithm. Its learning ability is compared with that of the grasshopper algorithm, a particle swarm optimization (PSO) based algorithm and the gradient descent method. The examples show the superiority of the IWD algorithm over the other optimization techniques.},
  archive      = {J_EXSY},
  author       = {Anuli Dass and Smriti Srivastava and Monika Gupta and Manju Khari and Javier Parra Fuente and Elena Verdú},
  doi          = {10.1111/exsy.13124},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13124},
  shortjournal = {Expert Syst.},
  title        = {Modelling and control of fuzzy-based systems using intelligent water drop algorithm},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of artificial intelligence-based chatbot for
smart aquafarm practices. <em>EXSY</em>, <em>41</em>(6), e13123. (<a
href="https://doi.org/10.1111/exsy.13123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study includes the development of a farm advisory framework known as the “AquaProFAN framework,” which is concerned with the study of various requirements by the aquafarmers and stakeholder community in fisheries. AquaProFAN (Aqua Professional Farm Advisory Network) advisory committee provided careful consideration to the aquafarmers&#39; inquiries and proper clarification with many innovative ideas, and the feasibility study on chatbot implementation was also performed. The end product, called AquaGent, is an AI-based chatbot designed for shrimp aquafarmers and support services. It was developed and deployed in social media platforms like Facebook Messenger and Telegram for research and dissemination purposes. The effectiveness of the chatbot was analysed and the findings indicated that: (i) informational requests are more satisfactory than emotional requests, (ii) the performance of chatbot is comparatively better than other software applications concerning informational requests, (iii) participants perceive ‘AquaGent’ chatbot to be more user friendly and time saving. It can be concluded that the implementation of this state-of-art technology in aquaculture sector will improve stakeholders&#39; understanding for future efficient and profitable production.},
  archive      = {J_EXSY},
  author       = {Ayesha Jasmin and Pradeep Ramesh and Mohammad Tanveer},
  doi          = {10.1111/exsy.13123},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13123},
  shortjournal = {Expert Syst.},
  title        = {Development of artificial intelligence-based chatbot for smart aquafarm practices},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ERIM: An ensemble of rare itemset mining and its application
in the automotive industry. <em>EXSY</em>, <em>41</em>(6), e13122. (<a
href="https://doi.org/10.1111/exsy.13122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering previously unknown anomalies that are rare and dramatically differ from the majority of the data is a critical need for the automotive industry. Rare itemset mining (RIM), one of the pattern-based methods, has been used for anomaly detection due to providing successful analysis results. However, several aspects still need to be explored, such as improving the mining process by identifying more targeted, valuable and reliable rare itemsets. Motivated by this fact, this study proposes a novel approach, named ensemble of rare itemset mining (ERIM), which investigates weak rare itemsets (WRIs) using different algorithms and aggregates these rules to obtain strong rare itemsets (SRIs). This study also combines four different RIM algorithms (Apriori Rare, Apriori Inverse, CORI and RP-Growth) as base learners for the first time. The proposed ERIM approach is a general methodology that can be applied to any field, but, in this study, it was used in the automotive industry as a case study. In the experiments, ERIM was applied to a real-world gear manufacturing dataset to discover anomalies in machine downtimes. The experimental results were evaluated in terms of the number of itemsets and the length of itemsets by giving some samples, as well. The results showed that the proposed ERIM approach gives more reliable common knowledge by jointly considering the relation between WRIs discovered by the base learners. The findings indicated that the proposed ERIM technique was successful in detecting anomalies whose support values are below 7.12. Furthermore, it is clear from the experimental results that the ERIM discovered the highest number of SRIs, 1403, each of which is a 3-itemset. Finally, the results showed that our method performed 43.37% better on average than state-of-the-art methods on the same dataset.},
  archive      = {J_EXSY},
  author       = {Devrim Naz Akdas and Derya Birant and Pelin Yildirim Taser},
  doi          = {10.1111/exsy.13122},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13122},
  shortjournal = {Expert Syst.},
  title        = {ERIM: An ensemble of rare itemset mining and its application in the automotive industry},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient direction-of-arrival estimation of multipath
signals with impulsive noise using satin bowerbird optimization-based
deep learning neural network. <em>EXSY</em>, <em>41</em>(6), e13108. (<a
href="https://doi.org/10.1111/exsy.13108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There exist numerous multi-path signals with impulsive noise (IN) in a multi-path propagation environment. The direction-of-arrival (DOA) is highly challenging to be assessed because of the strong IN. For various military along with civilian applications, DOA estimation has turned into a hopeful technology. But, estimating DOA for multipath signals with IN environments is extremely complicated. A desirable output is not attained by several existent techniques, namely subspace-centric approaches, maximum likelihood techniques, along with sparse representation-centric methods, for DOA assessment. Therefore, an effective satin bowerbird optimization-based deep learning neural network centered DOA estimation upon single snapshots is proposed in this paper for attaining robust and precise DOA estimation in multipath and IN environments. An arbitrary array configuration is employed by utilizing the proposed work as an input signal for establishing the received signal&#39;s model. Pre-processing, time-frequency domain conversion, feature extraction, feature reduction and DOA estimation are the proposed method&#39;s 5 disparate stages. To evince the proposed model&#39;s efficacy, its outcomes are analogized with other prevailing techniques regarding some performance metrics. The proposed model attains the root mean square error of 0.6484366-AZ and 0.6484366-EA, root mean squared percentage error (RMPSE) of 4.636107-AZ and 5.628599-EA and probability of resolution of 0.967999-AZ and 0.962841-EA, which are extremely less when compared to the prevailing methods; thus, depicting the superiority of the proposed model for an effective DOA estimation.},
  archive      = {J_EXSY},
  author       = {Harikrushna Gantayat and Trilochan Panigrahi and Pradyumna Patra},
  doi          = {10.1111/exsy.13108},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13108},
  shortjournal = {Expert Syst.},
  title        = {An efficient direction-of-arrival estimation of multipath signals with impulsive noise using satin bowerbird optimization-based deep learning neural network},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence of medical things for disease
detection using ensemble deep learning and attention mechanism.
<em>EXSY</em>, <em>41</em>(6), e13093. (<a
href="https://doi.org/10.1111/exsy.13093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel paradigm for disease detection. We build an artificial intelligence based system where various biomedical data are retrieved from distributed and homogeneous sensors. We use different deep learning architectures (VGG16, RESNET, and DenseNet) with ensemble learning and attention mechanisms to study the interactions between different biomedical data to detect and diagnose diseases. We conduct extensive testing on biomedical data. The results show the benefits of using deep learning technologies in the field of artificial intelligence of medical things to diagnose diseases in the healthcare decision-making process. For example, the disease detection rate using the proposed methodology achieves 92%, which is greatly improved compared to the higher-level disease detection models.},
  archive      = {J_EXSY},
  author       = {Youcef Djenouri and Asma Belhadi and Anis Yazidi and Gautam Srivastava and Jerry Chun-Wei Lin},
  doi          = {10.1111/exsy.13093},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13093},
  shortjournal = {Expert Syst.},
  title        = {Artificial intelligence of medical things for disease detection using ensemble deep learning and attention mechanism},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An e-commerce prediction system for product allocation to
bridge the gap between cultural analytics and data science.
<em>EXSY</em>, <em>41</em>(6), e13082. (<a
href="https://doi.org/10.1111/exsy.13082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emerging era of E-commerce and online shopping, people are also in a habit to receive default product recommendations on the web pages that they access. Google is already providing such suggestions. Till now recommendations were made only based on previous sentiments or feedback or ratings, but this research has improved the product recommendation method by including one more parameter for the same. This article represents two parameters for making predictions of product allocation to a new customer. These parameters are ratings given by the existing users for that particular product and the region to which the new customer belongs. Following these parameters, a prediction model and an algorithm, Improved_Collab_Similarity, have been implemented. The dataset has been developed where India as a country along with all its States has been considered for products which are popular for their creation based on regional and ancient skills of the people belonging to that area. Results for the mentioned prediction model have been discussed in this article where generally precision increases with the increase in a number of products but at some points, it does not increase when a smaller number of that product was purchased by the customers.},
  archive      = {J_EXSY},
  author       = {Shefali Singhal and Poonam Tanwar},
  doi          = {10.1111/exsy.13082},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13082},
  shortjournal = {Expert Syst.},
  title        = {An E-commerce prediction system for product allocation to bridge the gap between cultural analytics and data science},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Management and intelligent control of in-flight fuel
distribution in a commercial aircraft. <em>EXSY</em>, <em>41</em>(6),
e13075. (<a href="https://doi.org/10.1111/exsy.13075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuel management is an important issue in aviation for safety and efficiency reasons. This work develops an alternative solution based on an artificial intelligent technique to the fuel distribution management problem in commercial aircrafts. The fuel flow control amongst tanks during the flight is addressed. A fuzzy management system is implemented that decides the best fuel distribution based on safety criteria (keeping engines fed) and dynamic stability (placing the centre of gravity at the appropriate position), amongst other specifications. Expert knowledge is used to define the rules of the fuel fuzzy control, taking into account that the dynamics of the system changes, whilst fuel is being consumed. It has been simulated on a real long-range-type commercial aircraft with satisfactory results regarding stability, even in the case of internal malfunction (in pipes, pumps, or valves), and with external disturbances (engine failure). The knowledge-based fuzzy control is able to maintain the centre of gravity position within the stability and manoeuvrability margins along the flight. Besides, the intelligent control strategy minimizes the action of the actuators, providing some advantages to the control solution.},
  archive      = {J_EXSY},
  author       = {Elías Plaza and Matilde Santos},
  doi          = {10.1111/exsy.13075},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13075},
  shortjournal = {Expert Syst.},
  title        = {Management and intelligent control of in-flight fuel distribution in a commercial aircraft},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeuralConflict: Using neural networks to identify norm
conflicts in normative documents. <em>EXSY</em>, <em>41</em>(6), e13035.
(<a href="https://doi.org/10.1111/exsy.13035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of norms, which express constraints on people&#39;s behaviour within a specific range, are contained in normative documents. In writing and revising normative documents, conflicts between two norms often arise. The task Norm Conflict Identification (NCI) aims to identify such conflicts. The existing NCI methods based on statistical learning are all pipelines, which cause errors to accumulate, and cannot sufficiently extract helpful information. According to the characteristics of NCI, we propose a neural network model called NeuralConflict. This end-to-end model can avoid the accumulation of errors and makes it easier to obtain the optimal global solution. The model sets up an auxiliary task to predict whether two norms are semantically related and shares part of the information with the task NCI. In addition, the model uses a convolutional neural network with differently sized convolution kernels to extract local semantic information from the norms. Finally, the model inputs the shared and local semantic information into a fully connected neural network to predict whether the two norms conflict. We construct a Chinese dataset and use it with an existing English dataset for the experiments. Experimental results show that NeuralConflict achieves optimum results on both datasets.},
  archive      = {J_EXSY},
  author       = {Shaobin Huang and Jingyun Sun and Rongsheng Li},
  doi          = {10.1111/exsy.13035},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13035},
  shortjournal = {Expert Syst.},
  title        = {NeuralConflict: Using neural networks to identify norm conflicts in normative documents},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative robust adaptive control of multiple trains based
on RBFNN position output constraints. <em>EXSY</em>, <em>41</em>(6),
e13034. (<a href="https://doi.org/10.1111/exsy.13034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability of each train, high control accuracy, and minimum safe separation distance are important indexes to measure the performance of the cooperative control system of multiple trains. In this article, aiming at the problem of low accuracy of multiple trains cooperative control with nonlinear running resistance and external disturbance, the distributed cooperative robust adaptive control scheme for multiple trains with RBFNN position output constraints based on train running curve tracking is proposed. Multiple different control techniques are offered for different trains, and that they are based on local knowledge of position, speed, and acceleration. The leading train&#39;s speed and position precisely match the planned operation curve, while the following train keeps the tracking interval at the minimum safe distance between two trains. In order to reduce the influence of the uncertainty of basic resistance parameters and external interference on the cooperative control of multiple trains, the parameter uncertainties are compensated by adding a robust adaptive law to the multiple trains control based on position output constraints. The lumped exogenous disturbances (additional resistance, external interference, measurement noise, etc.) are estimated using an RBFNN approximator for the unknown term of the cooperative system. The stability of the cooperative operation of multiple trains is confirmed using the Lyapunov stability theorem. The performance of the proposed scheme was evaluated by the cooperative control system of multiple trains in predecessor following (PF) and bidirectional control (BC) modes.},
  archive      = {J_EXSY},
  author       = {Junxia Yang and Youpeng Zhang and Yuxiang Jin},
  doi          = {10.1111/exsy.13034},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13034},
  shortjournal = {Expert Syst.},
  title        = {Cooperative robust adaptive control of multiple trains based on RBFNN position output constraints},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mango wine making process optimization based on artificial
intelligence deep learning technology. <em>EXSY</em>, <em>41</em>(6),
e13032. (<a href="https://doi.org/10.1111/exsy.13032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper combines artificial intelligence deep learning technology to optimize the wine making process of mango wine. Moreover, in view of the shortcomings of traditional electronic nose data processing methods, a deep learning method based on SSAE-BPNN is proposed for electronic nose data processing. In addition, according to the characteristics of automatic learning features, this paper uses a deep learning method based on SSAE-BPNN to simplify the process of traditional data processing methods. Finally, this paper constructs an electronic nose system that can be used to identify mango wine making characteristics, and enhances the effect of electronic nose recognition through deep learning. Through the analysis, it can be seen that the mango wine making process optimization method based on artificial intelligence deep learning technology proposed in this paper has a certain effect, and it has optimized the traditional mango wine making process.},
  archive      = {J_EXSY},
  author       = {Hua Xubin and Lin Qiao and Gong Fayong and Cai Li and Liu Junhua},
  doi          = {10.1111/exsy.13032},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13032},
  shortjournal = {Expert Syst.},
  title        = {Mango wine making process optimization based on artificial intelligence deep learning technology},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An AI powered system call analysis with bag of word
approaches for the detection of intrusions and malware in australian
defence force academy and virtual machine monitor malware attack data
set. <em>EXSY</em>, <em>41</em>(6), e13029. (<a
href="https://doi.org/10.1111/exsy.13029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study propose the use of AI enabled machine learning algorithms with the Bag-of-Word (BoW) methods for the detection of intrusions by analysing the system call patterns. Host based Intrusion Detection System can make use of system call patterns to differentiate between normal and anomalous program behaviours. First, the system call patterns are pre-processed with different approaches like BoW, BoW with Boolean value, BoW with Probability value and BoW with TF-IDF. Next machine learning algorithms are used to evaluate the performance of classifier models. We used J48 (C4.5), Random Forrest, RIPPER, KNN, SVM, and NaiveBayes ML algorithms. This process was carried out on ADFA-LD and on our proposed virtual machine monitor (VMM) malware attack data set for analysis. The proposed work is evaluated based on detection accuracy and false alarm rate metrics. Random Forrest algorithm performs better compared with other ML algorithms in terms of intrusion detection accuracy and false alarm rate on ADFA and VMM malware data set. The proposed data set provide better results compared with ADFA-LD analysed using ML algorithms. The classifier model trained with ADFA and VMM malware system call data sets may do predictive analytics in detecting security issues for Industry 4.0 systems.},
  archive      = {J_EXSY},
  author       = {Appu Alfred Raja Melvin and Gnanaraj Jaspher W. Kathrine and Subbulakshmi Pasupathi and Vimal Shanmuganathan and Rajalingam Naganathan},
  doi          = {10.1111/exsy.13029},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13029},
  shortjournal = {Expert Syst.},
  title        = {An AI powered system call analysis with bag of word approaches for the detection of intrusions and malware in australian defence force academy and virtual machine monitor malware attack data set},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysing biosensor clinical pathogen information using
mayfly optimized convolute neural network approach. <em>EXSY</em>,
<em>41</em>(6), e13027. (<a
href="https://doi.org/10.1111/exsy.13027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viral and bacterial infection diseases are the most common things caused by microbes. Infection diseases are serious issues because of the growth of COVID-19. Because of the current living situation, clinical pathogens are difficult to identify. Therefore, biosensors have been widely utilized to sense the biomolecules relevant to viruses and bacteria. The biosensors observe the nanoparticles from the pathogens and help improve the infection analysis. The sensor information is processed using machine learning techniques because it consists of several learning patterns. However, the existing methods have multi-objective optimization problems while analysing the changes in the nanoparticles. This work utilizes a mayfly optimized convoluted neural network (MOCNN) to overcome this research issue. The grid uses the fully convolution layer that processes the extracted biosensor features to determine the infections. The network performance is optimized by applying the exploitation and exploration properties of nuptial dance that help to escape from the local optima solutions. The effective utilization of the optimized training patterns improves the convergence speed and convergence rate compared to traditional methods. From the results, MOCNN ensures 98.97% accuracy, 0.388 error rate, and 0.322833 convergence rate on various iterations with different learning rates.},
  archive      = {J_EXSY},
  author       = {Fahad Alblehai and Mohamed H. Mahmoud and M. Shaheer Akhtar and Ahmed Shaker and Abdallah A. Mohamed},
  doi          = {10.1111/exsy.13027},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13027},
  shortjournal = {Expert Syst.},
  title        = {Analysing biosensor clinical pathogen information using mayfly optimized convolute neural network approach},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handling uncertainty using optimal clustering with rough
sets-based rule generation model for data classification. <em>EXSY</em>,
<em>41</em>(6), e13026. (<a
href="https://doi.org/10.1111/exsy.13026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, MapReduce has become a popular tool for handling big data. At the same time, uncertainty is related to arbitrariness, fuzziness, ambiguity, irregularity and incomplete knowledge. In RS theory, the uncertainty behaviour of the data in the dataset of interest is managed by using upper and lower approximate sets and classification accuracy. The RS model is integrated with data clustering technique for optimal outcomes. With this motivation, this study designs an Optimal Clustering with RS-Based Rule Generation Model (OC-RSRGM) for data classification on MapReduce environment. The OC-RSRGM technique aims to generate an optimal set of rules using RST for the data classification process and it involves a two-stage process namely Optimal Fuzzy c-Means Clustering (OFCM) and RSRGM-based rule generation with classification. The OFCM technique is derived to eradicate the local optimal problem of the FCM (Fuzzy c-Means) model using Barnacles Mating Optimizer (BMO). It provides the decision-makers with all the information needed to design appropriate mechanisms to support their decision-making activities. The Hadoop MapReduce tool is used to handle big data. The proposed method combines an FCM, BMO, RS theory to accomplish effective decision-making. The OC-RSRGM technique can be employed to continuous value dataset where data point does not offer any class details and it might be uncertain. To validate the performance of OC-RSRGM technique, a detailed experimental analysis is carried out to highlights the betterment of OC-RSRGM technique. The proposed OC-RSRGM technique has obtained an effective outcome with the CT of 5.43 s.},
  archive      = {J_EXSY},
  author       = {Hanumanthu Bhukya and Manchala Sadanandam},
  doi          = {10.1111/exsy.13026},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13026},
  shortjournal = {Expert Syst.},
  title        = {Handling uncertainty using optimal clustering with rough sets-based rule generation model for data classification},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobile healthcare (m-health) based on artificial
intelligence in healthcare 4.0. <em>EXSY</em>, <em>41</em>(6), e13025.
(<a href="https://doi.org/10.1111/exsy.13025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare 4.0 is about collecting huge amounts of data and getting it to work in applications, enabling healthcare management decisions well-informed while providing for important gains in effectiveness and cost control. Diagnostics based on the digital footprint depend on wearable technology&#39;s ability to gather and extract essential patient data. Artificial intelligence (AI) technologies allow the analysis of real-time observed data and continuously developing from data to understand the world surrounding them. To connect and access intelligent healthcare services, people, and devices at any time, a secure wireless mobile communication system is essential. This article suggests a mHealth-based patient monitoring system (mHealth-PMS) based on AI for healthcare 4.0. Mobile healthcare applications motorized by Convolutional Neural Network (CNN) have enabled people to triage their conditions and preemptive treatment decisions. Information collected has been analysed for substantiating cause, and alert and preventive messages have been immediately sent through the mobile application. The performance analysis has been executed, and the proposed mobile application-based surveillance provided much-enhanced reporting of information quickly on diseases, symptoms, factors, and more. The mHealth-PMS strategy shows an accuracy ratio of 95.6%, monitoring ratio of 93.5%, data management ratio of 94.4%, data security ratio of 91.7%, data privacy ratio of 92.1%, prediction ratio of 95.3%, a cost-effective ratio of 25.5% compared to the existing methods.},
  archive      = {J_EXSY},
  author       = {Sunil Kumar Sharma and Mohammed Ibrahim Al-Wanain and Majed Alowaidi and Hisham Alsaghier},
  doi          = {10.1111/exsy.13025},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13025},
  shortjournal = {Expert Syst.},
  title        = {Mobile healthcare (m-health) based on artificial intelligence in healthcare 4.0},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting the suitable fertilizer for crop based on soil
and environmental factors using various feature selection techniques
with classifiers. <em>EXSY</em>, <em>41</em>(6), e13024. (<a
href="https://doi.org/10.1111/exsy.13024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture is an essential part of human life. The crop productivity is based on the soil and environmental factors. Different crops are cultivated in different areas. Nowadays, the crop productivity level is affected by the climate change and diseases in the crops. Due to this pest infestation, the crop growth is heavily affected. To overcome this problem, the right fertilizer for a particular crop has to be chosen and fertilizer helps farmers to improve the crop productivity rate. This process can be done by using various machine learning techniques. In this work, various features selection techniques with classifiers used to predict the suitable fertilizer for a crop. The experimental results show that recursive feature elimination along the proposed Heterogeneous Stacked Ensemble classifier gives better prediction rate than other methods.},
  archive      = {J_EXSY},
  author       = {Ganesan Mariammal and Andavar Suruliandi and Kharla Andreina Segovia-Bravo and Soosaimarian Peter Raja},
  doi          = {10.1111/exsy.13024},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13024},
  shortjournal = {Expert Syst.},
  title        = {Predicting the suitable fertilizer for crop based on soil and environmental factors using various feature selection techniques with classifiers},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on temporal and spatial distribution
characteristics of passenger flow of daxing airport line based on
automatic fare collection data. <em>EXSY</em>, <em>41</em>(6), e13009.
(<a href="https://doi.org/10.1111/exsy.13009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The airport line plays an important role in the airport landside traffic connection, which greatly relieves the urban road pressure. Studying the characteristics and laws of the time and space distribution of the airport line passenger flow will provide basic support for the airport line&#39;s capacity configuration, passenger flow control, and air-track coordinated operation, and will also provide quantitative basis for the layout of urban terminal buildings. This paper proposes a method to analyse the temporal and spatial distribution characteristics of airport line passenger flow based on AFC card swiping data. By mining and extracting the travel behaviour data of subway airport line air passengers, the distribution characteristics of airport line passenger flow are analysed and visualized from two aspects of passenger travel time and space. Finally, a case study is carried out based on the actual data of Daxing airport express. The results show that: (1) the travel time of departure and return passengers is concentrated in 60–90 min; (2) The departure and arrival points of passengers are concentrated within the Fourth Ring Road, and a large number of air passenger flows are gathered and evacuated at hubs such as Beijing South Railway Station and Beijing West Railway Station.},
  archive      = {J_EXSY},
  author       = {Zhao Jie and Ding Shuainan and Guo Bengang and Ma Zhenchao and Jin Jiamin},
  doi          = {10.1111/exsy.13009},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13009},
  shortjournal = {Expert Syst.},
  title        = {Research on temporal and spatial distribution characteristics of passenger flow of daxing airport line based on automatic fare collection data},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Green vehicle routing problem: Metaheuristic solution with
time window. <em>EXSY</em>, <em>41</em>(6), e13007. (<a
href="https://doi.org/10.1111/exsy.13007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of a Green Vehicle Routing Problem is to find an optimal route for alternative fuel vehicles to minimize the overall travelling distances while reducing energy consumption and CO 2 emissions. In this problem each vehicle handles a subset of customers/orders, leaving from and returning to the depot, with respect to maximum distance travelled, while minimizing energy consumptions. We proposed a time window-based GVRP solution (GVRP-TW) with exact routing approach. Wherein, each route serves a subset of customers/orders with minimum intermediate refuelling/recharges. To test our method, we conducted experiments using MATLAB software on a set of three reference problem specimens (C-101, R-101 and RC-11). Our approach, evaluated on these benchmark problems, par exceeds in performance in relation to the existing VRP methods (like tabu search, variable neighbourhood search, and GRASP). Moreover, the proposed GVRP-TW can be further optimized to solve other VRP problems with energy minimisations and less intermediate stops.},
  archive      = {J_EXSY},
  author       = {Ravi Prakash and Shashank Pushkar},
  doi          = {10.1111/exsy.13007},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13007},
  shortjournal = {Expert Syst.},
  title        = {Green vehicle routing problem: Metaheuristic solution with time window},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Developer load balancing bug triage: Developed load balance.
<em>EXSY</em>, <em>41</em>(6), e13006. (<a
href="https://doi.org/10.1111/exsy.13006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the software development process, numerous bugs are reported daily in the software bug repositories. Bug triage assigned these bugs to the most relevant and expert developer for resolution. Moreover, assigning bugs to an incompetent or an over-engaged developer causes repeated reassignment to other developers until it is resolved. This problem can be solved by devising a triage process that assigns bugs to not only expert developers but also to those who are either under-engaged or reasonably engaged but are not over-engaged in work. This paper has designed and implemented work engagement sensitive bug triage that resolves the issue of assigning bugs to developers considering their due work engagement, expertise as well as the current state of activity. For this purpose, a developer profile is built by using metrics to generate three types of scores: technical skill , work engagement and work experience . Metadata features like developer-name, email, developer-work-experience in bug resolution, last and present-work-activity, timestamp, component and priority are used for it. A multi-criteria-based Henry–Garret technique is used to generate a single ranked list of developers from three ranked lists. The performance of the proposed approach is evaluated on four large OOS projects: Mozilla, Eclipse, Netbeans, and Open Office covering 895,439 bug reports accumulated for 33 years of development. The overall system accuracy of the proposed triage using all four datasets is 91.96 ± 0.05% which is 5.87% better than previously published work. The proposed method is achieved up to 90.30 ± 0.05 and 97.1 ± 0.05 MRR and accuracy of reassignment respectively that indicates how significantly it re-assigns the bug to the relevant developers. The results demonstrate improvement in the accuracy of software-bugs triaging as well as a reduction in the probability of bug-tossing.},
  archive      = {J_EXSY},
  author       = {Asmita Yadav and Mohammed Baljon and Shailendra Mishra and Sandeep Kumar Singh and Sharad Saxena and Sunil Kumar Sharma},
  doi          = {10.1111/exsy.13006},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13006},
  shortjournal = {Expert Syst.},
  title        = {Developer load balancing bug triage: Developed load balance},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed hyperledger technology in FinTech with
artificial intelligence assisted internet of things platform.
<em>EXSY</em>, <em>41</em>(6), e13001. (<a
href="https://doi.org/10.1111/exsy.13001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) and distributed hyper ledger technology are both at the height of their hype cycles, which means they have the potential to be disruptive. This raises issues about new technologies&#39; influence on future business structures, particularly in service-based industries like finance. Even though numerous assumptions in reality point to the complementary use of these two technologies to produce new value creation potentials, there is little contemporary literature and study on the subject. Different perspectives on any of the two technologies mentioned above are insufficient for understanding potential synergies in financial services. Similarly, the internet of things (IoT) assistance for FinTech solutions has risen its significance in the upcoming transaction world. Therefore, this research proposes a distributed hyper ledger technology and AI for IoT-based FinTech platform (DHLT-AI-FTP), which incorporates the benefits of each technology for secured and energy-efficient financial transactions and communication. The AI technology and distributed hyper ledger technology handle the security part, and the communication part holds the efficient design of IoT implementation. The simulation evaluation of the proposed model ensures the improved security and efficiency of FinTech management and processing for real-time business scenarios. The experimental analysis shows the significant performance of the proposed DHLT-AI-FTP with a better adaptability ratio of 98.56% compared to other existing approaches.},
  archive      = {J_EXSY},
  author       = {Dengjia Li and Chaoqun Ma and Zhongding Zhou and Xianhua Kuang and Zijun Lin},
  doi          = {10.1111/exsy.13001},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e13001},
  shortjournal = {Expert Syst.},
  title        = {Distributed hyperledger technology in FinTech with artificial intelligence assisted internet of things platform},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flex-request: Library to make remote changes in the
communication of IoT devices. <em>EXSY</em>, <em>41</em>(6), e12994. (<a
href="https://doi.org/10.1111/exsy.12994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Internet of Things (IoT) systems have changed the way we live, work and do businesses in many areas, even those that until recently seemed unlikely. Some areas that can benefit from their application are, among others, healthcare, smart cities, industrial automation, smart agriculture, intelligent transportation systems, smart logistics, and emergency response. This research work proposes a novel alternative that allows the creation of IoT systems capable of making remote changes in devices&#39; communication in a fast and agile manner. Our proposal gives way to some of the most common changes in communication made during the development and maintenance phase in IoT systems, like changing the destination of data transmission, sending the data to multiple destinations, and changing the frequency of sending data. Our solution, which is used in the programs, is loaded on the device. When the device starts, it connects to a configuration server in the background and listens for changes. The changes are sent to the configuration server using specified commands. When a change is detected, the command is processed, and the change in communication is applied without stopping the running program. We designed experiments to evaluate the complexity of the programs developed using our proposal and the actions needed to make a change.},
  archive      = {J_EXSY},
  author       = {Karol Mateusz Ciok and Jordán Pascual Espada and Rubén González Crespo},
  doi          = {10.1111/exsy.12994},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e12994},
  shortjournal = {Expert Syst.},
  title        = {Flex-request: Library to make remote changes in the communication of IoT devices},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CIAMS—checkpoint-intrigued adversary mitigation scheme for
industrial internet of things. <em>EXSY</em>, <em>41</em>(6), e12972.
(<a href="https://doi.org/10.1111/exsy.12972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The industrial internet of things (IIoT) encompasses smart devices, manufacturing systems, humans, and networks for automated productive outcomes. The placement of devices and networks is vulnerable to distributed denial of service (DDOS) attacks that degrade the productivity and efficiency of IIoT. In this article, we propose a checkpoint-intrigued adversary mitigation scheme (CIAMS) for improving the security features and recommendations of the detection systems. Features that use the recommendation to provide relevant information maintain their security level. A DDoS attack is dealt with at the outset, resulting in increased productivity. The IIoT&#39;s smart devices are less productive and efficient because of this DDoS attack. This CIAMS method is designed to address vulnerability and the ability to survive the features checkpoints. The proposed scheme substantiates the security breach and lag in the checkpoint systems against DDOS attacks. The checkpoints&#39; vulnerability level and surviving features are assessed using a classified learning approach. In this assessment, the degrading features are reimbursed by improving the security functions, control, and access methods. Periodic checkpoint replacement and mutual security measures are used for mitigating the prolonging DDOS impact in the network. The proposed scheme&#39;s performance is verified using false positives, service distribution, lag, an efficiency score. Improvements have been made to the industrial environment&#39;s service delivery and efficiency. By reducing false positives by 10.35%, the proposed scheme improves service distribution ratio and efficiency score by 11.68% and 12.55% for different devices.},
  archive      = {J_EXSY},
  author       = {Jose Patris Donald and Linda Joseph},
  doi          = {10.1111/exsy.12972},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e12972},
  shortjournal = {Expert Syst.},
  title        = {CIAMS—Checkpoint-intrigued adversary mitigation scheme for industrial internet of things},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Next activity prediction of ongoing business processes based
on deep learning. <em>EXSY</em>, <em>41</em>(5), e13421. (<a
href="https://doi.org/10.1111/exsy.13421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next activity prediction of business processes (BPs) provides valid execution information of ongoing (i.e., unfinished) process instances, which enables process executors to rationally allocate resources and detect process deviations in advance. Current researches on next activity prediction, however, concentrate mostly on model construction without in-depth analysis of historical event logs. In this article, we are dedicated to proposing an approach to forecast the next activity effectively in BPs. After in-depth analysis of historical event logs, three types of candidate activity attributes are defined and calculated as additional input for the prediction based on three essential elements, that is, frequent activity patterns, trace similarity and position information. Furthermore, we construct an effective hybrid prediction model combining the popular convolutional neural network (CNN) and bidirectional long short-term memory (Bi-LSTM) with self-attention mechanism. Specifically, CNN is used to extract the temporal features before importing into Bi-LSTM for accurate prediction, and self-attention mechanism is applied to strengthen features that have decisive effects on the prediction results. Comparison experiments on four real-life datasets demonstrate that our hybrid model with selected attributes achieves better performance on next activity prediction than single models, and improves the prediction accuracy by 2.98%, 6.05%, 2.70% and 5.26% on Helpdesk , Sepsis , BPIC2013 Incidents and BPIC2012O datasets than the state-of-the-art methods, respectively.},
  archive      = {J_EXSY},
  author       = {Xiaoxiao Sun and Siqing Yang and Yuke Ying and Dongjin Yu},
  doi          = {10.1111/exsy.13421},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13421},
  shortjournal = {Expert Syst.},
  title        = {Next activity prediction of ongoing business processes based on deep learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The risk evaluation and management of the sports service
supply chain by introducing fuzzy comprehensive appraisal and artificial
intelligence technology. <em>EXSY</em>, <em>41</em>(5), e13279. (<a
href="https://doi.org/10.1111/exsy.13279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to improve the inefficiency of the sports service supply chain (SC) caused by the unscientific management of stakeholders in the sports service SC. First, combined with the literature, the identification and management of the service SC domestically and internationally are explained, providing a meaningful reference for improving sports service bases. Second, fuzzy comprehensive appraisal (FCA) algorithm process and artificial intelligence (AI) technology are used to evaluate and manage the risk of sports service SC. The FCA algorithm provides a comprehensive evaluation factor set for the sports service SC risk evaluation. Then, the complete evaluation set is established. To obtain the evaluation matrix, the ‘expert’ carries out fuzzy evaluation on each single factor. AI provides a data analysis platform for integrated SC risk management. The platform can realize the logical hierarchical relationship between indicators and rules, systematically standardize the sports service SC and rules, and conduct unified display, monitoring, and trend analysis. Finally, the FCA method is used to evaluate the risks of the sports service SC, and the results of the comprehensive management of the dangers of the SC are analysed. The results show that the evaluation indicators of sports service venues&#39; SC service capabilities have a good consistency. The highest weight score of government service capability in the sports service SC is 0.2413, and the lowest weight score of the service capability index of the sports operation platform is 0.1708. Sports service venues&#39; SC capacity in NANCHANG City is relatively average. This article deeply understands the structure of the sports service SC. It provides relevant basis and guidance for sports service integrators, service providers, and other market entities to adjust management strategies and improve market competitiveness. It also clarifies the risks and management results of the sports service SC, provides academic help for the researchers and enriches and develops the theory of the reform and construction of the Chinese sports service SC.},
  archive      = {J_EXSY},
  author       = {Ye Teng and Yuxuan Wang and Huan You},
  doi          = {10.1111/exsy.13279},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13279},
  shortjournal = {Expert Syst.},
  title        = {The risk evaluation and management of the sports service supply chain by introducing fuzzy comprehensive appraisal and artificial intelligence technology},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supply chain risk management of badminton supplies company
using decision tree model assisted by fuzzy comprehensive evaluation.
<em>EXSY</em>, <em>41</em>(5), e13275. (<a
href="https://doi.org/10.1111/exsy.13275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supply chain risk management is crucial for the management of sports goods companies, which can help enterprises better complete the transparent and efficient management of the whole process from production to sales. However, there are inevitably some potential risks. Thereby, to study the supply chain risk management of badminton supplies companies in the general environment, the supply chain risk evaluation index system of badminton supplies companies is established in this study by studying the connotation of supply chain risk management and combining the steps of fuzzy comprehensive evaluation (FCE) method. Then, according to these evaluation indexes, the modelling process of the classification and regression tree (CART) algorithm in the corporate overall supply chain risk decision tree is proposed. Moreover, the corporate overall supply chain risk decision tree is established. Moreover, the financial data of 10 A-share listed companies of badminton and other sports goods listed on the Shanghai Stock Exchange in 2020 are selected as the sample dataset for model training and testing. The potential risks of badminton enterprises are predicted. The results show that the scale of badminton enterprises has been increasing gradually since 2018, but the return on total assets has steadily decreased with the increase of years. It shows that the badminton supplies enterprise&#39;s input and output assets are unbalanced, and the overall asset operation effect is relatively poor. However, the growth rate of total assets shows a significant upward trend from 2018 to 2020, rising from 4% to 12.69%. It indicates that the corporate investment and domestic and foreign investment expansion in recent years are high-speed. The FCE method is employed to obtain the maximum eigenvalue of the enterprise, which is 5.4698. Then, the C.I value is 0.1205, the R.I value is 1.12, and the consistency ratio value is 0.0958 &lt; 0.1, indicating that the judgement matrix is acceptable. It suggests that under the influence of the macro environment and economic situation, the sales situation of the badminton supplies company is subtly affected by the macro environment, and there is a great business risk.},
  archive      = {J_EXSY},
  author       = {Daolin Zhang and Yong Tang and Xianliang Yan},
  doi          = {10.1111/exsy.13275},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13275},
  shortjournal = {Expert Syst.},
  title        = {Supply chain risk management of badminton supplies company using decision tree model assisted by fuzzy comprehensive evaluation},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The genetic algorithm and BP neural network in financial
supply chain management under information sharing. <em>EXSY</em>,
<em>41</em>(5), e13273. (<a
href="https://doi.org/10.1111/exsy.13273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s global business competition environment, the significant changes in the market environment make it very difficult for supply chain enterprises to gain a place in the fierce market only by their strength. This article will do the following research work to effectively play the role of supply chain management. First, the concepts, and characteristics, related applications of supply chain information sharing (IS) and genetic algorithm back propagation (GABP) neural networks, are expounded. Then, the feasibility of applying BP neural network (BPNN) to evaluate supply chain IS is analysed. Next, the assessment indicator system for IS is constructed from four aspects: IS infrastructure, IS member characteristics, IS content, and the quality of supply chain enterprises. Finally, the principle and algorithm of BPNN are analysed, the network model is constructed, the BPNN process based on GA is designed, and every step in the process is explained in detail. The sample input is provided for the network model by obtaining and sorting data. The supply chain IS assessment model based on the GABP neural network is constructed. The results show that the assessment model of supply chain IS based on the GABP neural network has high accuracy, and the highest network accuracy reaches 95.01%, which proves that the assessment model based on BPNN has certain advantages for the supply chain IS of the financial industry.},
  archive      = {J_EXSY},
  author       = {Chenxi Li and Zhaohui Li and Meng Wu},
  doi          = {10.1111/exsy.13273},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13273},
  shortjournal = {Expert Syst.},
  title        = {The genetic algorithm and BP neural network in financial supply chain management under information sharing},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The design for supply chain management of intelligent
logistics system using cloud computing and the internet of things.
<em>EXSY</em>, <em>41</em>(5), e13271. (<a
href="https://doi.org/10.1111/exsy.13271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image recognition is the key to smart logistics systems. Traditional handwriting feature extraction is difficult to meet the requirements of image recognition. Deep learning is used for image recognition. Firstly, convolutional neural network (CNN) and deep Boltzmann machines under deep learning are introduced. Second, cellular neural networks are used to perform feature recognition and extraction on images. Finally, a Parzen classifier is used to classify the obtained image features. The novelty is that through the structural design and research of the intelligent logistics system, the CNN is combined to construct a management system of supply chain logistics of image recognition and information processing. The experimental results show that the recognition accuracy time of the proposed improved fusion algorithm on the Mixed National Institute of Standards and Technology data set is 198.85 s. When the improved algorithm achieves the same recognition accuracy, it takes 159.65 s. The recognition efficiency of the improved algorithm is 19.71% higher than that of the unimproved algorithm. In addition, when the unimproved algorithm reaches the maximum number of iterations, the error rate is 2.47%. The error rate of the improved algorithm is only 0.74%. This study provides a basis for improving the image recognition accuracy and has certain practical value.},
  archive      = {J_EXSY},
  author       = {Huan Wang and Yuanxing Yin and Xinyu Wang},
  doi          = {10.1111/exsy.13271},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13271},
  shortjournal = {Expert Syst.},
  title        = {The design for supply chain management of intelligent logistics system using cloud computing and the internet of things},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The energy storage and optimal dispatch supply chain for new
energy grids using edge computing and the internet of things.
<em>EXSY</em>, <em>41</em>(5), e13266. (<a
href="https://doi.org/10.1111/exsy.13266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intermittent and uncertainty of new energy in the grid connection process affects the overall quality of the grid. To resolve the scattered geographical locations, small individual capacities and poor controllability of distributed energy storage (DES) devices, edge computing is applied in conjunction with aggregate service providers to build an edge computing-based aggregate model of DES. Regarding its characteristics, the schedulable potential of the energy storage device is analysed through modelling. A multi-objective scheduling model with multiple parties participating in the subject&#39;s behaviour is established using the Internet of Things (IoT) optimization strategy. The simulation example of concrete data further verifies the validity of the model. Results demonstrate that the edge computing-based aggregate model of DES effectively reduces the calculation load&#39;s peak-valley load and reduces the wind power abandonment. In the meantime, the multiple starts and stops of the thermal power unit reduce the operation cost of the unit. The multi-objective dispatch model can reduce the opportunity cost and payment of DES effectively. This model achieves load peak reduction and valley filling and reduces the peak dispatch cost of the power grid. The research results can provide some ideas for storing and utilizing the new energy.},
  archive      = {J_EXSY},
  author       = {Jun Liu},
  doi          = {10.1111/exsy.13266},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13266},
  shortjournal = {Expert Syst.},
  title        = {The energy storage and optimal dispatch supply chain for new energy grids using edge computing and the internet of things},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The design of intelligent fuzzy cognitive system of music
emotion by product supply chain management. <em>EXSY</em>,
<em>41</em>(5), e13265. (<a
href="https://doi.org/10.1111/exsy.13265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of musical emotion is a challenging problem due to the subjectivity of human perception. In order to analyse the emotion contained in music from the perspective of product supply chain management, the psychological model and characteristics of music emotion are introduced. Additionally, the current product supply chain and the composition of the two contracts are analysed based on the characteristics of online music products. Finally, a speech emotion recognition model based on a Fuzzy Cognitive Map (FCM) is proposed. Two corpora containing five emotions (sadness, anger, happiness, surprise, and boredom) are selected to test the performance and characteristics of the network. In order to evaluate the system performance, different acoustic features are extracted, including prosodic features, formant features, marginal spectral features, and Mel-Frequency Cepstral Coefficients (MFCC). The results show that the average recognition rate of MFCC features on the Berlin emotional speech database is 70.36%, which is 0.75% higher than the second-highest edge spectrum features and achieves better results. Meanwhile, on this database, the Musical Emotion Recognition System based on FCM (MERS-FCM) is improved by 20.36%, 7.34%, and 4.12% compared with Back Propagation, K-nearest neighbour, and support vector machine, respectively, which shows that MERS-FCM has better performance. The research of the music emotion recognition system based on FCM can provide more reference paths for the use and development of music emotion information.},
  archive      = {J_EXSY},
  author       = {Fanfan Li and Rong Jiang and Jiabao Li},
  doi          = {10.1111/exsy.13265},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13265},
  shortjournal = {Expert Syst.},
  title        = {The design of intelligent fuzzy cognitive system of music emotion by product supply chain management},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The evaluation of enterprise supply chain intelligent
manufacturing system for agricultural interconnection data based on
machine learning. <em>EXSY</em>, <em>41</em>(5), e13259. (<a
href="https://doi.org/10.1111/exsy.13259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work aims to promote the modernization of the real economy and facilitate the manufacturing industry to construct an intelligent manufacturing system. Analysis and evaluation are performed on the intelligent manufacturing system (IMS) of enterprises under the Internet of Things (IoT). First, the connotation and characteristics of machine learning (ML) in the era of IoT are explained, followed by the introduction to neural networks. Second, the optimization process of the neural network is described. Since it has no memory, the traditional deep neural network (DNN) needs to be improved. Then, the Generative Adversarial Network (GAN) is utilized to expand the sample data. Moreover, the confusion matrix is used in the indicator evaluation. The backpropagation neural network (BPNN) is optimized by the genetic algorithm (GA) to improve the network configuration and model performance. In addition, the connotation and development status of IMS are analysed, and the feasibility of applying deep learning to the evaluation of IMS is discussed. Finally, the evaluation model of IMS is designed and built via a neural network, which is ultimately verified in the simulation experiment. The experimental results indicate that the BPNN-GA algorithm has advantages in predicting fitting accuracy, taking up less memory, and shortening training time; it can achieve generalization and accuracy in the forefront of optimal Pareto value, and the fitting accuracy attains 95.3%. The horizontal evaluation of IMS proves the effectiveness and feasibility of the evaluation model. This shows that the BPNN-GA algorithm has an excellent evaluation effect on IMS. And the IMS evaluation model is applied to assess the IMS of sample enterprises. The evaluation results demonstrate that there are problems in the current manufacturing industry. For example, the transactions are challenging to be completed smoothly, information technology is updated slowly, and IMS is dependent on collecting resources.},
  archive      = {J_EXSY},
  author       = {Caili Yu and Liang Li and Junlong Li and Peng Qin and Bei Zhang},
  doi          = {10.1111/exsy.13259},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13259},
  shortjournal = {Expert Syst.},
  title        = {The evaluation of enterprise supply chain intelligent manufacturing system for agricultural interconnection data based on machine learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The analysis of green advertisement communication strategy
based on deep factorization machine deep learning model under supply
chain management. <em>EXSY</em>, <em>41</em>(5), e13258. (<a
href="https://doi.org/10.1111/exsy.13258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) technology has brought new reconstruction opportunities for the intelligence of the advertisement industry through the help of AI technologies such as machine learning and deep learning. First, the relationship between AI and the attractiveness of green advertisements is investigated, and the influence of different AI technologies in green advertisements on consumers&#39; perception of the attractiveness of green advertisements is summarized. Second, based on the green advertisement dissemination rate data set, the data visualization exploration is carried out, and the data deletion and coding processing are carried out aiming at different characteristic variables. Finally, according to the problems existing in the current green advertisement communication and the high-dimensional and sparse characteristics of the communication rate data set. In this paper, based on Deep FM (Factorization Machine), Gradient Boost Decision Tree (GBDT) is added to assist the experiment, and the prediction performance of green advertising communication is tested. The results are as follows. (1) Different AI expressions in green advertisements will affect consumers&#39; perception of the attractiveness of green advertisements. (2) The prediction ability of Deep FM model after feature engineering is better than that of data cleaning only. The prediction effect of the model is obviously improved. The purpose of this paper is to integrate green advertising media communication into the ecological concept of harmonious coexistence between man and nature, strengthen the political belief of ecological civilization construction, and conform to the communication trend of today&#39;s severe ecological situation.},
  archive      = {J_EXSY},
  author       = {Xue Yu and Yunfei Zhu and Congcong Jia and Wanqiu Lu and Hao Xu},
  doi          = {10.1111/exsy.13258},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13258},
  shortjournal = {Expert Syst.},
  title        = {The analysis of green advertisement communication strategy based on deep factorization machine deep learning model under supply chain management},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Carbon emission reduction policy simulation based on the CGE
model. <em>EXSY</em>, <em>41</em>(5), e13251. (<a
href="https://doi.org/10.1111/exsy.13251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carbon peaking and carbon neutrality will be important policies to constrain China&#39;s economic and social development in the future. Based on the theory of computable general equilibrium, CGE model of carbon emission was constructed in this paper to simulate the influence of different scenarios of carbon emission reduction on economic and social development. The major contribution is to subdivide the energy sector according to primary energy and secondary energy, and construct the macro SAM table and micro SAM table of 21 energy sectors in 2017 to quantitatively simulate the impact of carbon emission reduction policies on China&#39;s economy and society, and put forward some policy suggestions based on this.},
  archive      = {J_EXSY},
  author       = {Yu Lin},
  doi          = {10.1111/exsy.13251},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13251},
  shortjournal = {Expert Syst.},
  title        = {Carbon emission reduction policy simulation based on the CGE model},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impacts of the carbon tax on green shipping supply chain
under the port competition. <em>EXSY</em>, <em>41</em>(5), e13229. (<a
href="https://doi.org/10.1111/exsy.13229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shipping industry plays an important role in international trade and world economic development. In particular, the rapid growth of the shipping industry since 2020 has become an important factor in supporting and stimulating the rapid recovery of the global economy in the post epidemic era. However, under the background that climate warming has increasingly become the theme of common concern for all mankind, greenhouse gas emission reduction has increasingly attracted the attention of governments of all countries, and has also become the requirement for production and operation of all industries, the green, environmental protection and sustainable development of the shipping industry have also become the focus of participants in all sectors. As environmental problems put forward requirements of a green shipping industry, some policies aimed at reducing pollution and greenhouse gas emissions, such as carbon tax, have brought considerations to the strategies of ports and enterprises, especially concerning their competition and cooperation. This study established a shipping supply chain system composed of two ports and one shipping company in order to analyse the impact of carbon tax on the shipping supply chain, and to optimize the decision-making of government, ports and shipping companies. Through three-level Stackelberg game of four models including ports cooperating or not cooperating with each other, and carbon tax levied on ports or the shipping company, the study finds that the collection of carbon tax will reduce the container handling volume of ports thus reducing the profits of both ports and the shipping company, but the handling volume when the government puts carbon tax on ports is lower than that when it puts carbon tax on the shipping company, and the handling volume under the port cooperation mode is lower than that under the port non-cooperation mode. As the collector of carbon tax, the government&#39;s income when taxing ports is higher than that when taxing shipping companies, and when ports cooperate, the government&#39;s income is lower than that when ports do not cooperate. Moreover, no matter the ports cooperate with each other or not, the impact of carbon tax on port container handling volume under the mode of carbon tax levied by the government on ports is more significant than that when carbon tax is levied on the shipping company, leaving the former mode a lower container handling volume and lower profit. The findings of this study have the following significance and suggestions for the participants of shipping supply industry: In order to maximize the government&#39;s target value, the government should give priority to taxing shipping companies rather than ports; Under the background of carbon tax, ports should choose limited non-cooperation strategies.},
  archive      = {J_EXSY},
  author       = {Jingyao Song and Changyan Xu and Chuanxu Wang},
  doi          = {10.1111/exsy.13229},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13229},
  shortjournal = {Expert Syst.},
  title        = {Impacts of the carbon tax on green shipping supply chain under the port competition},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Normalized hellinger feature selection and soft margin
boosting classification for water quality prediction. <em>EXSY</em>,
<em>41</em>(5), e13221. (<a
href="https://doi.org/10.1111/exsy.13221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several research works have been introduced using classification methods for predicting water quality. However, the accuracy level with which the prediction was made and the time consumed in the prediction was not found to be satisfactory. In order to solve these problems, water quality forecasting is done using machine learning and deep learning techniques. The study introduces an efficient water quality prediction called, Normalized Hellinger Feature Selection based Soft Margin Boosting (NHFS-SMB) is developed based on pre-processing, feature selection, and classification with higher accuracy and lesser time consumption. The Hellinger Similarity Feature selection model is used to select relevant features for water quality prediction. SMBC is an ensemble classifier that combines several weak classifiers (i.e., associative classifier) to provide the final prediction results as normal or contaminated conditions with lesser error rates. Experimental evaluation is determined by the factors such as accuracy, precision, recall, F1-score, and ROC Curve with respect to the number of data points.},
  archive      = {J_EXSY},
  author       = {Kalaivanan Karuppannan and Vellingiri Jayagopal},
  doi          = {10.1111/exsy.13221},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13221},
  shortjournal = {Expert Syst.},
  title        = {Normalized hellinger feature selection and soft margin boosting classification for water quality prediction},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applying information ecological theory to analyse the green
supply chain management system in universities. <em>EXSY</em>,
<em>41</em>(5), e13219. (<a
href="https://doi.org/10.1111/exsy.13219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to create higher benefits for enterprises in the university supply chain and make them meet the requirements of sustainable development, this study uses information ecology theory to study the cognitive management and transformation path of the green supply chain. Firstly, the related concepts of university supply chain management and green supply chain management are expounded, and the differences are mainly discussed. Then, the evaluation of supply chain greenness is studied. The fuzzy hierarchical comprehensive evaluation method combining quantitative and qualitative is used to establish the evaluation model. Finally, the proposed evaluation model is tested. The test results show that Suzhou University&#39;s greenness fuzzy evaluation score is 58.2 points, which is a medium level. The resource factor scored 61 points, the energy factor scored 52.9 points, the environmental factors scored 59.8 points, the economic factor scored 62.7 points, and the social factor scored 55.5 points. The company has obvious advantages in terms of resources and economy, but the level is not very high. The higher the cost synergy between enterprises and customers, the greater the environmental cost synergy. Therefore, manufacturing enterprises should strengthen communication and cooperation with customers. The higher the cost synergy control of manufacturing and waste recycling enterprises, the greater the synergy effect of environmental cost control. Therefore, increasing cooperation with waste recycling enterprises is conducive to developing environmental cost control and coordinated benefits. The proposed scheme provides some ideas for applying information ecology theory in the university supply chain.},
  archive      = {J_EXSY},
  author       = {Qi Wang and Wenjie Zhang and Lan Ma},
  doi          = {10.1111/exsy.13219},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13219},
  shortjournal = {Expert Syst.},
  title        = {Applying information ecological theory to analyse the green supply chain management system in universities},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent cognitive evaluation of ice and snow sports
training by fuzzy comprehensive evaluation from the perspective of
supply chain management. <em>EXSY</em>, <em>41</em>(5), e13212. (<a
href="https://doi.org/10.1111/exsy.13212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This exploration aims to promote the development of ice and snow sports (ISS), strengthen people&#39;s physiques and improve people&#39;s health level. First, the fuzzy comprehensive evaluation method is adopted to determine the importance weight of the intelligent cognitive factors of ISS. Then, based on the evaluation grade, the membership matrix of intelligent cognitive factors of ISS is constructed, and an individual cognitive ability evaluation model combining qualitative analysis with quantitative analysis is established. Statistical Product and Service Solutions (SPSS) 25.0 and Smartpls software are further used to determine the questionnaire items about the participation intention in ISS training. A structural equation model among individual participation intention, theoretical factors of planned behaviour and participation behaviour is constructed. After analysing the data obtained from the survey, the final results are as follows. (1) Harbin&#39;s largest number of ISS participants is 18–29 years old, accounting for 44.8%. Undergraduate education accounts for 47.5%. The number of participants whose monthly income is 6001–12,000 is the largest, accounting for 25%. (2) The number of people participating in skating is the highest, reaching 59.7%. The proportion of people who have participated in ISS for one year or less is the highest, reaching 45.8%. From the consumption amount perspective, the people spending 1000 yuan and below account for the highest proportion, so the consumption degree of ISS is mainly experiential consumption. (3) The correlation coefficient between subjective norms and individual participation intention is 0.598, indicating that the relationship between the two is significantly positive, that is, subjective norms will have a certain impact on individual participation intention. The correlation coefficient between behaviour attitude and participation intention is 0.697, indicating that the relationship between them is significantly positive. The correlation coefficient between perceived behaviour control and participation intention is 0.745, indicating a significant positive correlation between them. The research content provides a theoretical basis for the follow-up ISS training intelligent cognition. It can also provide a reference for the further development of ISS in China.},
  archive      = {J_EXSY},
  author       = {Ping Hu and Peng Zhang},
  doi          = {10.1111/exsy.13212},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13212},
  shortjournal = {Expert Syst.},
  title        = {Intelligent cognitive evaluation of ice and snow sports training by fuzzy comprehensive evaluation from the perspective of supply chain management},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact assessment and mechanism of water conservancy policy
on carbon emission performance under the background of artificial
intelligence. <em>EXSY</em>, <em>41</em>(5), e13190. (<a
href="https://doi.org/10.1111/exsy.13190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence technology is constantly developing and changing. Due to the continuous development of technology and production of carbon emissions, various problems have occurred, forcing various regions to start promoting low-carbon work. What kind of method can effectively and quickly improve the performance of carbon emissions has become one of the current research topics that has attracted much attention. In response to this problem, it is of great significance to study in the field of carbon emission performance improvement methods. With the in-depth research on carbon emissions, the research on the intervention of water conservancy policies in carbon emissions has gradually been carried out, and its functional effects are of great significance to solve the problem of improving carbon emissions performance. This paper has aimed to study the impact and mechanism of water conservancy policy on carbon emission performance in the context of artificial intelligence. Through the analysis and research of the water conservancy policy under artificial intelligence, the double difference model (DID) is used to evaluate it, so that it can show the impact and mechanism on carbon emission performance, so as to solve the problem of improving carbon emission performance. This paper has analysed water policy, carbon emission performance and DID model under artificial intelligence. Its influence and mechanism have been experimentally analysed, and related theoretical formulas have been used to explain. The results have shown that the water conservancy policy has a significant and continuous driving effect on the reduction of carbon emission intensity, and its interaction coefficient under the control variable is −0.934, which is significantly negative. Further analysis of variables has shown that the energy intensity coefficient is 0.714, and the secondary industry coefficient is 0.924, which are both positive and significant. Water conservancy policies mainly reduce carbon intensity through the improvement of energy efficiency and industrial structure upgrading. It can be seen that water conservancy policies under artificial intelligence can meet the needs of improving carbon emission performance, and the level of economic growth and green development has been greatly improved.},
  archive      = {J_EXSY},
  author       = {Junjie Xiang and Haitao Mao and Bin Yang},
  doi          = {10.1111/exsy.13190},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13190},
  shortjournal = {Expert Syst.},
  title        = {Impact assessment and mechanism of water conservancy policy on carbon emission performance under the background of artificial intelligence},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adoption of blockchain + internet of things in demand
forecasting of farm supply chain. <em>EXSY</em>, <em>41</em>(5), e13187.
(<a href="https://doi.org/10.1111/exsy.13187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It aims to improve the application efficiency of modern science and technology in farm operation, enhance the prediction effect of machine learning and blockchain technology on farm supply and demand issues, and help enterprise managers to better manage and operate enterprise projects. First, the agricultural super business model under the blockchain + Internet of Things (IoT) is analysed, and the farm management system of the IoT + blockchain is constructed. Then, the demand forecasting model of agricultural products is established. Autoregressive Integrated Moving Average model (ARIMA) and Support Vector Machine (SVM) models are used to predict demand. Finally, the characteristics of the two models in prediction are discussed. The results show that the ARIMA model can reach 99.2% in the application efficiency evaluation of farm operation. Moreover, in the case of proper selection of model indicators, the prediction result is close to the actual value, the prediction error fluctuation is small, and the maximum error value is 0.4318. The local error of SVM model in predicting the demand of farm supply chain is slightly larger, and the maximum error value is 8.5430. By comparing the error results of the two models, the prediction accuracy of ARIMA model is higher than that of SVM model. The ARIMA model designed in this work can provide effective product demand prediction for farm operation and provide a strong technical reference for farm operation. The research not only provides technical support for machine learning and blockchain technology to predict farm supply and demand, but also contributes to the integrated management and sustainable development of enterprises.},
  archive      = {J_EXSY},
  author       = {Hao Wang and Tao Liu and Zhongyang Yu},
  doi          = {10.1111/exsy.13187},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13187},
  shortjournal = {Expert Syst.},
  title        = {Adoption of blockchain + internet of things in demand forecasting of farm supply chain},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classification of open source software bug report based on
transfer learning. <em>EXSY</em>, <em>41</em>(5), e13184. (<a
href="https://doi.org/10.1111/exsy.13184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the feature richness of text encoding vectors in the bug report classification model based on deep learning is limited by the size of the domain dataset and the quality of the text. However, it is difficult to further enrich the features of text encoding vectors. At the same time, most existing bug report classification methods ignore the submitter&#39;s personal information. To solve these problems, we construct nine personal information characteristics of bug report submitters in GitHub by survey. Then, we propose a GitHub bug report classification method named personal information fine-tuning network (PIFTNet) based on transfer learning and the submitter&#39;s personal information. PIFTNet transfers the general text feature vectors in bidirectional encoder representation from transformers (BERT) to the domain of bug report classification by fine-tuning the pre-training parameters in BERT. It also combines the text characteristics and the characteristics of the submitter&#39;s personal information to construct the classification model. In addition, we propose a two-stage training method to alleviate the catastrophic changes in the pre-training parameters and loss of the initially learned knowledge caused by direct training of PIFTNet. We verify the proposed PIFTNet on the dataset extracted from GitHub and empirical results prove the effectiveness of PIFTNet.},
  archive      = {J_EXSY},
  author       = {Liao Zhifang and Wang Kun and Zeng Qi and Liu Shengzong and Zhang Yan and He Jianbiao},
  doi          = {10.1111/exsy.13184},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13184},
  shortjournal = {Expert Syst.},
  title        = {Classification of open source software bug report based on transfer learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expert system for automatic microservices identification
using API similarity graph. <em>EXSY</em>, <em>41</em>(5), e13158. (<a
href="https://doi.org/10.1111/exsy.13158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new software design paradigm, microservices structure an application as a collection of services that are independently deployable and loosely coupled. A key step of migrating non-microservices-based systems to microservices-based systems is the identification of microservices in the target application. Traditional approaches to identify microservices, however, usually suffer from lack of full automation and low effectiveness. This paper puts forward an expert system to identify microservices automatically from legacy systems by leveraging the similarity of RESTful APIs. The system consists of three major parts. The first part calculates the candidate topic similarity and the response message similarity of APIs, and the overall similarity is obtained through their combination. Afterwards, the second part constructs a graph of API similarities with API as the node and the overall similarity as the weight. The third part employs a graph-based clustering algorithm to identify candidate microservices from the API similarity graph. Experiments conducted on open-source projects demonstrate the effectiveness of our system.},
  archive      = {J_EXSY},
  author       = {Xiaoxiao Sun and Salamat Boranbaev and Shicong Han and Huanqiang Wang and Dongjin Yu},
  doi          = {10.1111/exsy.13158},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13158},
  shortjournal = {Expert Syst.},
  title        = {Expert system for automatic microservices identification using API similarity graph},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive control and supply chain management of intelligent
agricultural greenhouse by intelligent fuzzy auxiliary cognitive system.
<em>EXSY</em>, <em>41</em>(5), e13117. (<a
href="https://doi.org/10.1111/exsy.13117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to avoid the complex control process caused by complex coupling and serious interference of control variables in traditional greenhouse environmental control and achieve the adaptive control of agricultural greenhouse. This work studies smart agricultural greenhouses&#39; adaptive control and supply chain management based on the intelligent fuzzy auxiliary cognitive system (IFACS). This work firstly expresses the intelligent fuzzy control theory and analyses data according to the fuzzy control set. Secondly, the adaptive control and supply chain management of the intelligent agricultural greenhouse are studied by using an intelligent fuzzy aided cognitive system. Finally, the light intensity and carbon dioxide content of the intelligent agricultural greenhouse are analysed; the temperature and humidity content of the intelligent agricultural greenhouse of the traditional control method and adaptive control are compared; the impact of carbon dioxide emissions on the environment under supply chain management is analysed. The research results indicate that the adaptive control of the intelligent agricultural greenhouse based on the IFACS can keep the greenhouse temperature within a specific range and reduce the impact of temperature on the development of agricultural greenhouses. The IFACS is effectively managed in the adaptive control of the intelligent agricultural greenhouse. From 2015 to 2020, the smart agricultural greenhouse supply chain management gradually reduced the content of greenhouse gas (GHG) emitted by the production and suppliers through the IFACS. The content of greenhouse gases emitted by product transporters did not change significantly, and the greenhouse gases emitted by non-manufacturing suppliers gradually decreased. In 2020, supply chain GHG emission intensity decreased by 20% compared with 2015, and GHG emissions from product transportation also decreased by 6% compared with the previous year. This research has reference significance for the adaptive control of intelligent agricultural greenhouse by IFACS and the environmental control of agricultural greenhouse by supply chain management.},
  archive      = {J_EXSY},
  author       = {Yongjie Tian},
  doi          = {10.1111/exsy.13117},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13117},
  shortjournal = {Expert Syst.},
  title        = {Adaptive control and supply chain management of intelligent agricultural greenhouse by intelligent fuzzy auxiliary cognitive system},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic relationship network and international management of
enterprise supply chain by particle swarm optimization algorithm under
deep learning. <em>EXSY</em>, <em>41</em>(5), e13081. (<a
href="https://doi.org/10.1111/exsy.13081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional enterprise decision evaluation model based on neural network has the problems of mismatch with the optimal solution and slow convergence speed. In order to enable companies to make decisions that are in line with changes in the market, the particle swarm optimization (PSO) algorithm is used to optimize deep learning neural networks. Firstly, the model parameter setting is improved, and the inertia weight strategy of normal distribution attenuation is combined. On this basis, a normal distribution decay inertial weight particle swarm optimization (NDPSO) is proposed. The inertia weight of the optimized algorithm maintains a large value in the initial stage, which makes the PSO algorithm maintain a large step size in the optimization process and a small value in the later stage. Through experimental analysis, the trend parameter of the best normal distribution of the algorithm is obtained as 0.4433 and then using the detection function, the NDPSO algorithm is tested by two types of test functions. The NDPSO algorithm is compared with the optimization results of other algorithms which are optimized on the Sphere function. The minimum value of 554.29, the average value of 2032.11, and the standard deviation of 918.47, all of them are at the leading level. Taking into account other experimental results, it is proved that the normal distribution decay inertia weight can balance the global search and local development capabilities from the perspective of parameter improvement. It can speed up the convergence with ensuring the convergence accuracy. The improved PSO algorithm has certain optimization capabilities for neural network models. The use of optimized neural network models can enable companies to make decisions in line with changes in the market and optimize the dynamic relationship network of the company&#39;s supply chain, which is of great significance to the implementation of the company&#39;s international management.},
  archive      = {J_EXSY},
  author       = {Min Chen and Wenhu Du},
  doi          = {10.1111/exsy.13081},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13081},
  shortjournal = {Expert Syst.},
  title        = {Dynamic relationship network and international management of enterprise supply chain by particle swarm optimization algorithm under deep learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supply chain benefit distribution and economic sustainable
development by shapley model in smart city. <em>EXSY</em>,
<em>41</em>(5), e13078. (<a
href="https://doi.org/10.1111/exsy.13078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Things, the Internet, and artificial intelligence, people&#39;s consumption patterns have undergone major changes. Consumers&#39; consumption channels are no longer monolithic. A new retail model was born. Additionally, how to improve the revenue of the new retail supply chain and the scientific and reasonable distribution of revenue is particularly important. On this background, the concept of new retail is defined, and the principle of supply chain synergy is analysed. Stackelberg game is used to analyse and determine the optimal selling and wholesale prices of retailers and suppliers in the supply chain. The defects of the traditional supply chain model and the necessity of optimization are further analysed and demonstrated. Because of the traditional retail supply chain, new retail channels are introduced, and the second-order supply chain is established. New retail channels are added to the supply chain, and the profitability before and after the addition is analysed. Stackelberg game and Shapley value method are used for modelling. Finally, Matlab is used for numerical analysis. The results show that adding new retail channels to the traditional supply chain dominated by suppliers will increase the market share of products. The market share f is positively correlated with the profits of suppliers and retailers. Under the multi-channel retail model, product demand and supply chain profits are higher than a single channel. The Shapley value method can enable supply chain members to obtain corresponding benefits on their input, thereby achieving a win–win situation for retail supply chain members.},
  archive      = {J_EXSY},
  author       = {Ziyu Xu},
  doi          = {10.1111/exsy.13078},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13078},
  shortjournal = {Expert Syst.},
  title        = {Supply chain benefit distribution and economic sustainable development by shapley model in smart city},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Software business process adaptive approach supporting
organization architecture evolution. <em>EXSY</em>, <em>41</em>(5),
e13071. (<a href="https://doi.org/10.1111/exsy.13071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software maintenance and evolution play an important role in the software engineering field, especially when current software becomes more and more complex and powerful. As an entity to implement business processes and gain revenue, valuable software is composed of business logic and corresponding organization role interaction interfaces. With the enterprise development, the organization architecture also evolves, like expanding, cross department cooperation, and so on. However, existing software process adaptive approaches mainly focus on handling the change of the business (program) logic instead of organization structure. Therefore, we propose an adaptive software business process approach that supports organization architecture evolution and automatically migrates the run-time process instances to the latest version. First, a business process adaptation model is designed, which includes the organization layer, business process layer and event layer that connects the two. Based on the model, the organization changing impact and business process model modification are formalized. Besides, the business process adaptation approach is designed. According to the dependence between the organization architecture and the business process activities, the affected domain detection algorithms for three basic business process structures and the business process instance migration algorithm are developed. Finally, the feasibility and stability of the proposed system are comprehensively evaluated with the synthetic data sets.},
  archive      = {J_EXSY},
  author       = {Youhuizi Li and Yuyu Yin and Yu Li and Haijie Hu and Linyang Lu and Jie Cao},
  doi          = {10.1111/exsy.13071},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13071},
  shortjournal = {Expert Syst.},
  title        = {Software business process adaptive approach supporting organization architecture evolution},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis and design of financial data mining system based on
fuzzy clustering. <em>EXSY</em>, <em>41</em>(5), e13031. (<a
href="https://doi.org/10.1111/exsy.13031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the economy, a large amount of financial data will be generated during the continuous growth of enterprises. However, due to the explosive growth of the financial data range index, the use of machine learning methods to mine and analyse financial data is extremely important. Among them, accurate financial risk evaluation is an effective measure to prevent and resolve corporate financial crises. In this article, we use fuzzy clustering method to establish a financial risk early warning and evaluation model. Specifically, we use fuzzy C-mean (FCM), half-suppressed FCM, and interval FCM clustering algorithms-based state construction financial risk early warning and evaluation models, to give an evaluation from two aspects of corporate financial indicators and non-financial indicators system. In order to verify the feasibility and effectiveness of the fuzzy clustering algorithms used in financial data mining, we conducted experiments in financial data mining and early warning in real estate companies and ST companies. The experimental results show that the fuzzy clustering algorithms represented by the FCM clustering algorithm has achieved good results in financial data mining, and can achieve good results in financial risk analysis and financial risk early warning.},
  archive      = {J_EXSY},
  author       = {Huwei Li},
  doi          = {10.1111/exsy.13031},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13031},
  shortjournal = {Expert Syst.},
  title        = {Analysis and design of financial data mining system based on fuzzy clustering},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supply chain operation evaluation and management decision by
fuzzy cognitive map model. <em>EXSY</em>, <em>41</em>(5), e13022. (<a
href="https://doi.org/10.1111/exsy.13022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The components of the logistics and supply chain are analysed to meet the needs of supply chain management and decision support, aiming at the complexity and uncertainty of the supply chain system. First, the characteristics of logistics demand and logistics service types of some enterprises are analysed. The factors affecting supplier selection are obtained through the questionnaire survey of some enterprises. Second, the mutual feedback relationship between the influencing factors of logistics service demand is analysed, and a fuzzy cognitive graph model of the selection process of enterprise logistics service providers is established. Finally, the fuzzy cognitive map (FCM) model is used to conduct an empirical study on the choice of logistics service providers. The results show that the ranking results of the three candidate logistics service companies based on FCM are WL1 (0.7233) &gt; WL3 (0.6823) &gt; WL2 (0.6764). When each node of the concept does not affect each other, the ranking result of the three candidate logistics service companies based on analytic hierarchy process (AHP) is WL1 (0.7265) &gt; WL2 (0.6949) &gt; WL3 (0.6920). The final evaluation result will be affected by the mutual influence relationship between the concept nodes. Overall, the lowest evaluation score is WL2, and the highest is WL1. The most suitable logistics service provider for the enterprise is WL1. The FCM model selected by the manufacturing enterprise logistics service provider is reasonable and feasible. The mutual feedback between evaluation factors affects the selection of logistics service providers in a company. The FCM can get a more accurate evaluation weight value and can realize the optimal choice of logistics suppliers. It provides reference and decision support for other enterprises.},
  archive      = {J_EXSY},
  author       = {Puyang Zheng},
  doi          = {10.1111/exsy.13022},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e13022},
  shortjournal = {Expert Syst.},
  title        = {Supply chain operation evaluation and management decision by fuzzy cognitive map model},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The model of tibetan thangka sales under blockchain
technology. <em>EXSY</em>, <em>41</em>(5), e12989. (<a
href="https://doi.org/10.1111/exsy.12989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the reliability of Tibetan Thangka sales channels, this paper combines blockchain technology to construct an intelligent model that can be used for Tibetan Thangka sales. This paper uses cryptographic encryption technology to encrypt the uploaded data to prevent Thangka producers or logistics turnover parties from tampering with Thangka-related information, and uses the timestamp in blockchain technology to realize the traceability of Thangka-related information. In addition, this paper uses the timestamp technology in the blockchain system to generate a traceable, query and supervised blockchain, which runs through the whole process of thangka from planting to selling. Finally, a sales model of Tibet Thangka based on blockchain technology is designed, and system performance analysis is carried out through simulation experiments. The experimental research results verify that the designed sales model has a certain effect.},
  archive      = {J_EXSY},
  author       = {Yuan Fang},
  doi          = {10.1111/exsy.12989},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12989},
  shortjournal = {Expert Syst.},
  title        = {The model of tibetan thangka sales under blockchain technology},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The analysis of optimized path selection for management mode
of coastal regional circular economy based on fuzzy decision algorithm.
<em>EXSY</em>, <em>41</em>(5), e12985. (<a
href="https://doi.org/10.1111/exsy.12985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to achieve economic growth without destroying the environment and consuming resources, it is necessary to reasonably predict the development trend of circular economy. The evolution path of circular economy in the coastal regions was analysed based on the fuzzy technique for order of preference by similarity to ideal solution (TOPSIS) method. The dynamic investigation was carried out through the variations of moving trajectory of model space points in a certain period of time, the evolution path of circular economy development in coastal regions was numerically simulated to conclude the evolution law of sustainable growth path in coastal regions, and the future development trend of coastal regional circular economy was predicted. Finally, the composite indicator is synthesized accordingly as an example to quantify the development of the Center of around Bohai Sea Economic Rim (BER). On the basis of introducing the basic situation of BER as well as its circular economy development status, comprehensive economic performance dataset of BER from 2005 to 2018 is used with specified TOPSIS method to improve the accuracy of evaluation. In addition, on the basis of the rule, the future development direction of the circular economy of the BER was predicted, and the policy suggestions were put forward in view of the development status and problems of circular economy.},
  archive      = {J_EXSY},
  author       = {Wenwan Wei and De Xiao and Wan Liu},
  doi          = {10.1111/exsy.12985},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12985},
  shortjournal = {Expert Syst.},
  title        = {The analysis of optimized path selection for management mode of coastal regional circular economy based on fuzzy decision algorithm},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Auxiliary cognition system-based management strategy
optimization of supply chain of new energy in oil–gas enterprises.
<em>EXSY</em>, <em>41</em>(5), e12974. (<a
href="https://doi.org/10.1111/exsy.12974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The work focuses on the optimization of management strategies of energy supply chain in oil–gas enterprises to find out the optimal management strategy and overcome the fragility of the traditional energy supply system. Back propagation neural network (BPNN) is applied to energy supply predicting and energy security early warning in oil–gas enterprises, and the security early warning system is designed based on the auxiliary cognitive system. The security early warning system is used to predict the peak value of oil and gas supply, and then the detection performances and the data security transmission performances of different models are compared and analysed. Finally, a comprehensive evaluation model for security management of oil–gas supply chain based on the neural network is established. It is found that compared with convolutional neural network (CNNs), the improved BPNN model algorithm proposed shows increased accuracy by 5.4%, reduced model training time and test time, and improved accuracy and recall rate. The accuracy, recall rate, and F1 value of the proposed model are 88.48%, 75.88%, and 96.21%, respectively, which were obviously higher than those of the CNN. It suggests that the improved security management model of oil–gas supply chain shows better recognition and prediction accuracy. Based on the quantitative and qualitative analysis results, the optimization suggestions for management strategies in oil–gas enterprises are put forward, which is of strategic significance for improving the security of oil–gas supply.},
  archive      = {J_EXSY},
  author       = {Qian Sun and Sha He},
  doi          = {10.1111/exsy.12974},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12974},
  shortjournal = {Expert Syst.},
  title        = {Auxiliary cognition system-based management strategy optimization of supply chain of new energy in oil–gas enterprises},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linking AI supply chain strength to sustainable development
and innovation: A country-level analysis. <em>EXSY</em>, <em>41</em>(5),
e12973. (<a href="https://doi.org/10.1111/exsy.12973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To help enterprises improve their sustainable development as well as the competition ability in their supply chain, a research is conducted on the efficiency of the supply chain embedded with artificial intelligence (AI) and issues in sustainable development. At first, the present work determines the connection between efficiency of the supply chain with the environmental performance. Then, it illustrates how the efficiency of the AI supply chain influences the environment of enterprises. Next, an evaluation is made on the influence of AI supply chain efficiency on the social sustainable development. In the empirical analysis, the data of Global Competitiveness Report and Environmental Performance Index Report from 2008 to 2017 are collected. Descriptive statistics and correlation analysis are used to determine the relationship between AI supply chain efficiency and sustainable development and innovation. The Pearson Product Moment Correlation Coefficient (PPMCC) has been computed to ascertain if there are significant bivariate relationships between the variables of interest. There are two components of reducing waste and consumption pressures that are significantly and positively correlated with AI supply chain efficiency. These are ecological footprint per capita ( r = .662, p = .000) and waste recycling rates ( r = .631, p = .000). The present work identifies the connection between AI supply chain efficiency and sustainable development and innovation. Moreover, the AI supply chain promotes technological innovation among enterprises and the sustainable development of enterprises themselves.},
  archive      = {J_EXSY},
  author       = {Hao Wang},
  doi          = {10.1111/exsy.12973},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12973},
  shortjournal = {Expert Syst.},
  title        = {Linking AI supply chain strength to sustainable development and innovation: A country-level analysis},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring systemic and systematic risk in the financial
markets using artificial intelligence. <em>EXSY</em>, <em>41</em>(5),
e12971. (<a href="https://doi.org/10.1111/exsy.12971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial markets are exposed to sharp price volatility, and defaults and errors, which results in high risks for relevant stakeholders. In such markets, success depends upon the quality and quantity of information available to assist the decision-making. Artificial intelligence (AI), in this regard, can measure or predict systemic and systematic risk in the financial markets. This research study aims to highlight how the risks can be measured and controlled with the support and integration of modern AI and machine learning mechanisms. By performing a review-based methodology, the study first presents an explanation of the models, which is followed by proposing a new AI-based model. The model relies on several financial system&#39;s inputs, which include portfolio data, trade data, market data, financial reports, market condition, and sector-wise data. The systemic and systematic risk is then assessed through a number of outputs that the AI algorithm will process, in the form of an interactive dashboard. The article further tests that model and presents benefits and implications.},
  archive      = {J_EXSY},
  author       = {M. M. Kamruzzaman and Omar Alruwaili and Dhiyaa Aldaghmani},
  doi          = {10.1111/exsy.12971},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12971},
  shortjournal = {Expert Syst.},
  title        = {Measuring systemic and systematic risk in the financial markets using artificial intelligence},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of local configuration pattern for automated
detection of schizophrenia with electroencephalogram signals.
<em>EXSY</em>, <em>41</em>(5), e12957. (<a
href="https://doi.org/10.1111/exsy.12957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a mix of traditional and modern approaches have been proposed to detect brain abnormalities using bio-signal/bio-image-assisted methods. In hospitals, most of the initial/scheduled assessments consider the bio-signal-based appraisal, due to its non-invasive nature and low cost. Further, brain bio-signal scans can be recorded using a single/multi-channel electrode setup, which is further evaluated by an experienced doctor, as well as computer software, to identify the nature and severity of abnormality. In this paper, we describe the development of a system for computer supported detection (CSD) of schizophrenia using the electroencephalogram (EEG) signal collected with a 19-channel electrode array. Schizophrenia is a mental illness that interferes with the way an individual thinks and behaves. It is characterised by psychotic symptoms such as hallucinations or delusions, negative symptoms such as decreased motivation or a lack of interest in daily activities and cognitive symptoms such challenges in processing information to make informed decisions or staying focused. This research has utilized 1142 EEGs (516 normal and 626 schizophrenia) with a frame length of 25 s (6250 samples) for investigation. The work initially converts the EEG signals to images using a spectrogram. Local configuration pattern features were extracted from the images thereafter, and 10-fold validation technique was used wherein Student&#39;s t- test and z -score standardization were computed per fold. The highest accuracy of 97.20% was achieved with the K-nearest neighbour (KNN) classifier. The results obtained confirm that the KNN classifier is helpful in the rapid detection of schizophrenia. This work is one of the first studies to extract local configuration pattern features from spectrogram images, yielding a high accuracy of 97.20%, with reduced computational complexity.},
  archive      = {J_EXSY},
  author       = {Joel En WeiKoh and Venkatesan Rajinikanth and Jahmunah Vicnesh and The-Hanh Pham and Shu Lih Oh and Chai Hong Yeong and Meena Sankaranarayanan and Aditya Kamath and Giliyar Muralidhar Bairy and Prabal Datta Barua and Kang Hao Cheong},
  doi          = {10.1111/exsy.12957},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12957},
  shortjournal = {Expert Syst.},
  title        = {Application of local configuration pattern for automated detection of schizophrenia with electroencephalogram signals},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An expert system for automated classification of phases in
cyclic alternating patterns of sleep using optimal wavelet-based entropy
features. <em>EXSY</em>, <em>41</em>(5), e12939. (<a
href="https://doi.org/10.1111/exsy.12939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans spend a significant portion of their time in the state of sleep, and therefore one&#39;s’sleep health’ is an important indicator of the overall health of an individual. Non-invasive methods such as electroencephalography (EEG) are used to evaluate the ’sleep health’ as well as associated disorders such as nocturnal front lobe epilepsy, insomnia, and narcolepsy. A long-duration and repetitive activity, known as a cyclic alternating pattern (CAP), is observed in the EEG waveforms which reflect the cortical electrical activity during non-rapid eye movement (NREM) sleep. The CAP sequences involve various, continuing periods of phasic activation (phase-A) and deactivation (phase-B). The manual analysis of these signals performed by clinicians are prone to errors, and may lead to the wrong diagnosis. Hence, automated systems that can classify the two phases (viz. Phase A and Phase B accurately can eliminate any human involvement in the diagnosis. The pivotal aim of this study is to evaluate the usefulness of stopband energy minimized biorthogonal wavelet filter bank (BOWFB) based entropy features in the identification of CAP phases. We have employed entropy features obtained from six wavelet subbands of EEG signals to develop a machine learning (ML) based model using various supervised ML algorithms. The proposed model by us yielded an average classification accuracy of 74.40% with 10% hold-out validation with the balanced dataset, and maximum accuracy of 87.83% with the unbalanced dataset using ensemble bagged tree classifier. The developed expert system can assist the medical practitioners to assess the person&#39;s cerebral activity and quality of sleep accurately.},
  archive      = {J_EXSY},
  author       = {Manish Sharma and Ankit A. Bhurane and U. Rajendra Acharya},
  doi          = {10.1111/exsy.12939},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12939},
  shortjournal = {Expert Syst.},
  title        = {An expert system for automated classification of phases in cyclic alternating patterns of sleep using optimal wavelet-based entropy features},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on the improvement path of international
competitiveness of china’s agricultural product supply chain from the
perspective of machine learning. <em>EXSY</em>, <em>41</em>(5), e12935.
(<a href="https://doi.org/10.1111/exsy.12935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to explore the international competitiveness of China&#39;s agricultural product supply chain, this paper combines machine learning algorithms to optimize the data processing process of agricultural product supply chain, and proposes a data processing method for agricultural product supply chain based on the intra-column rules and inter-column rules. According to the problems of my country&#39;s agricultural product supply chain, this paper establishes a new agricultural product supply chain management model based on the various modules of the supply chain link, and constructs a corresponding intelligent analysis model. Moreover, this paper combines machine learning algorithms to study the improvement path of the international competitiveness of China&#39;s agricultural product supply chain, and builds an intelligent model to improve the international competitiveness of China&#39;s agricultural products. Finally, after constructing the model, this paper obtains the path to improve the international competitiveness of China&#39;s agricultural products through the simulation operation model, which also verifies the effectiveness of the intelligent algorithm of this paper from the side.},
  archive      = {J_EXSY},
  author       = {Juan Lei},
  doi          = {10.1111/exsy.12935},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12935},
  shortjournal = {Expert Syst.},
  title        = {Research on the improvement path of international competitiveness of china&#39;s agricultural product supply chain from the perspective of machine learning},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved exponential metric space approach for c-mean
clustering analysing. <em>EXSY</em>, <em>41</em>(5), e12896. (<a
href="https://doi.org/10.1111/exsy.12896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present two resilient algorithms, the improved alternative hard c-means (IAHCM) and the improved alternative fuzzy c-means (IAFCM). We implement the Gaussian distance-dependent function proposed by Zhang and Chen (D.-Q. Zhang and Chen, 2004). In some cases, Zhang and Chen&#39;s metric distance does not account for the clustering centroid effect predicted by the large value. R* is employed in IAHCM and IAFCM to discover robust results while minimizing its sensitivity. Experiments are conducted using two-and three-dimensional data, including Diamond and Iris real-world data. The results are based on demonstrating the robust simplicity and applicability of the offered algorithms. Similarly, computational complexity is assessed.},
  archive      = {J_EXSY},
  author       = {Rakesh Kumar and Varun Joshi and Gaurav Dhiman and Wattana Viriyasitavat},
  doi          = {10.1111/exsy.12896},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12896},
  shortjournal = {Expert Syst.},
  title        = {An improved exponential metric space approach for C-mean clustering analysing},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The economic globalization for sustainable management of
overseas trade enterprise logistics. <em>EXSY</em>, <em>41</em>(5),
e12887. (<a href="https://doi.org/10.1111/exsy.12887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study expects to explore the sustainable logistics management of foreign trade enterprises, help to further adapt to the development mode of economic globalization at this stage, reduce the international trade cost of enterprises and promote the overall trade development of enterprises. Logistics service providers of foreign trade enterprises are set as the research object to analyzes the problems existing in internal logistics management from the perspective of enterprise logistics. According to the relevant principles of analytic hierarchy process (AHP), the corresponding solutions are proposed. Through the actual investigation and analysis, first, the main problems of modern logistics enterprises are underdeveloped logistics management technology, low degree of information, low work efficiency and high logistics risk. Second, the enterprise logistics management index is divided into five indicators, namely, enterprise service level, enterprise operation qualification, logistics management ability, risk control ability and transportation location advantage. Among the five indicators, service level accounts for the largest proportion, for 30.23%. Four different service providers are scored, and the highest score is 7.5717, which proves the practicability of AHP. Finally, the optimization management strategy is proposed for similar logistics enterprises. The accomplishment of the study can help enterprises reduce costs, continuously improve the level of logistics management, and provide a theoretical basis for the sustainable development of enterprises, which has certain practical significance and can provide scientific and effective reference for subsequent research.},
  archive      = {J_EXSY},
  author       = {Wurong Zuo and Xing Zhang and Yuemeng Ge},
  doi          = {10.1111/exsy.12887},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12887},
  shortjournal = {Expert Syst.},
  title        = {The economic globalization for sustainable management of overseas trade enterprise logistics},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective high-quality economic growth based on human
capital structure. <em>EXSY</em>, <em>41</em>(5), e12863. (<a
href="https://doi.org/10.1111/exsy.12863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, economic growth is influenced by many domestic and foreign global issues, concerning the decrease of industrial output for the rise in economic inequality and the weakening of the overall condition in the countries. The role of economic growth is based on human capital and investment. Therefore, in this paper, the human capital and economic management system (HC-EMS) has been proposed to enhance the country&#39;s quality based on the capital structure investment. HC explores the role of command and its significance in the production and creation aspects of human capital such that an area can grow creatively. HC-EMS analysed critical monitoring systems for developing human resources and the high-tech and creative economic sectors and presented suggestions for change. Besides, there is a need to prove the shortcomings and benefits of human resource development systems and statistical approaches for gathering and analysing information. The impact of financial services and financial transactions may vary; therefore, it is critically important for fostering economic growth. The experimental result suggests that the proposed HC-EMS achieves the highest growth development ratio than other existing methods.},
  archive      = {J_EXSY},
  author       = {Jinli Ma and Carlos Enrique Montenegro-Marin and Rubén González Crespo},
  doi          = {10.1111/exsy.12863},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e12863},
  shortjournal = {Expert Syst.},
  title        = {Effective high-quality economic growth based on human capital structure},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence for heart sound classification: A
review. <em>EXSY</em>, <em>41</em>(4), e13535. (<a
href="https://doi.org/10.1111/exsy.13535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart sound signal analysis is very important for the early identification and treatment of cardiovascular illness. With rapid advancements in science and technology, artificial intelligence technologies are providing tremendous opportunities to enhance diagnosis and clinical decision-making. Instruments can now perform clinical diagnoses that previously could only be handled by human experts more conveniently and efficiently. Despite multiple works on automatic heart sound analysis, there are few summarization and review works. This article attempts to give a thorough overview of various heart sound analysis subtasks and examine the improvements made in each subtask by both machine learning techniques and deep learning algorithms. It goals to highlight the potential of AI to revolutionize cardiovascular healthcare by enabling accurate and automated analysis of heart sounds. The findings of this review are beneficial for researchers, clinicians, and engineers in the development and application of AI-based solutions for improved heart sound classification and diagnosis.},
  archive      = {J_EXSY},
  author       = {Junxin Chen and Zhihuan Guo and Xu Xu and Gwanggil Jeon and David Camacho},
  doi          = {10.1111/exsy.13535},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13535},
  shortjournal = {Expert Syst.},
  title        = {Artificial intelligence for heart sound classification: A review},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sarcasm-based tweet-level stress detection. <em>EXSY</em>,
<em>41</em>(4), e13534. (<a
href="https://doi.org/10.1111/exsy.13534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psychological stress has evolved as an important health concern across the globe. The vulnerability to stress and the ramifications of it have only worsened during the time of the COVID-19 pandemic. This necessitates a timely diagnosis of stress before the condition progresses to chronicity. In this context, the popularity of social media like Twitter, where large numbers of users share opinions without any social stigma, has emerged as a major resource of human opinions. This has led to an increased research interest in social media-based stress detection techniques. However, tweet-level stress detection techniques in the literature have left a void in leveraging the text information in tweets, especially the presence of sarcastic expressions in the tweet&#39;s text content. To this end, a novel method called “Sarcasm-based Tweet-Level Stress Detection” (STSD) is proposed in this work with the modification of the logistic loss function to detect tweet-level stress by availing the information of sarcasm that exists in the tweet-content. The principle of the STSD model is to minimize the loss for non-sarcastic tweets while maximising the loss for sarcastic tweets. Furthermore, extensive preprocessing and dimensionality reduction are performed using kernel principal component analysis (kernel PCA) to improve the performance by reducing the dimensions. The experimental results show that the proposed STSD model, when applied along with kernel PCA, records a significant improvement in accuracy by a minimum of 5.25% and a maximum of 9.19% over baseline models. Also, there is an increment in F1-score by at least 0.085 points and a maximum of 0.164 points when compared to the baseline models.},
  archive      = {J_EXSY},
  author       = {KVTKN Prashanth and Tene Ramakrishnudu},
  doi          = {10.1111/exsy.13534},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13534},
  shortjournal = {Expert Syst.},
  title        = {Sarcasm-based tweet-level stress detection},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing diversity for logical table-to-text generation
with mixture of experts. <em>EXSY</em>, <em>41</em>(4), e13533. (<a
href="https://doi.org/10.1111/exsy.13533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logical table-to-text generation is a task within the realm of natural language generation (NLG) that aims to generate coherent and logically faithful sentences based on tables. Unlike conventional NLG tasks, this task demands not only surface-level fluency but also a high degree of logic-level fidelity in the generated outputs. Current table-to-text systems grapple with various quality issues, such as repetitive generation, insufficient reasoning and limited complexity. Therefore, we introduce LogicMoE, a dedicated Mixture-of-Experts (MoE) model tailored for logical table-to-text generation. The primary objective of LogicMoE is to enrich the diversity of generated sentences from both semantic and logical perspectives. In particular, each expert within the model serves as a specialized generator responsible for generating sentences of a specific logical type. Additionally, we propose and employ novel evaluation metrics to comprehensively assess the diversity of generated outputs. Our experimental results showcase LogicMoE&#39;s superiority with absolute improvements of 0.8 and 2.2 in BLEU-3 over the strong baselines on LogicNLG and Logic2Text datasets, respectively, driving the state-of-the-art performance to a new level. Furthermore, we highlight its inherent advantages in terms of diversity and controllability, signifying its potential to spearhead advancements in logical table-to-text generation applications.},
  archive      = {J_EXSY},
  author       = {Jie Wu and Mengshu Hou},
  doi          = {10.1111/exsy.13533},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13533},
  shortjournal = {Expert Syst.},
  title        = {Enhancing diversity for logical table-to-text generation with mixture of experts},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient hybrid extreme learning machine and
evolutionary framework with applications for medical diagnosis.
<em>EXSY</em>, <em>41</em>(4), e13532. (<a
href="https://doi.org/10.1111/exsy.13532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating machine learning techniques into medical diagnostic systems holds great promise for enhancing disease identification and treatment. Among the various options for training such systems, the extreme learning machine (ELM) stands out due to its rapid learning capability and computational efficiency. However, the random selection of input weights and hidden neuron biases in the ELM can lead to suboptimal performance. To address this issue, our study introduces a novel approach called modified Harris hawks optimizer (MHHO) to optimize these parameters in ELM for medical classification tasks. By applying the MHHO-based method to seven medical datasets, our experimental results demonstrate its superiority over seven other evolutionary-based ELM trainer models. The findings strongly suggest that the MHHO approach can serve as a valuable tool for enhancing the performance of ELM in medical diagnosis.},
  archive      = {J_EXSY},
  author       = {Ali Al Bataineh and Seyed Mohammad Jafar Jalali and Seyed Jalaleddin Mousavirad and Amirmehdi Yazdani and Syed Mohammed Shamsul Islam and Abbas Khosravi},
  doi          = {10.1111/exsy.13532},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13532},
  shortjournal = {Expert Syst.},
  title        = {An efficient hybrid extreme learning machine and evolutionary framework with applications for medical diagnosis},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empowering EEG motor imagery classification with deep
transfer learning approach. <em>EXSY</em>, <em>41</em>(4), e13530. (<a
href="https://doi.org/10.1111/exsy.13530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain-computer interface (BCI) enables individuals with impairments to interact with the real world without relying on the neuromuscular pathway. BCI leverages artificial intelligence (AI) models for control. It can capture brain activity patterns associated with mental processes and convert them into commands for actuators. One promising application of BCI is in rehabilitation centres. When compared to traditional methods integrated with machine learning (ML) approaches that were initially explored in the past decade, there is a relatively limited number of studies utilizing deep learning (DL) approaches in BCI applications. A novel approach has been developed for the automatic recognition of motor imagery (MI) tasks. This article introduces the improved sparrow search algorithm with deep transfer learning-based EEG motor imagery classification (ISSADTL-EEGMIC) algorithm for BCIs. The presented ISSADTL-EEGMIC algorithm primarily focuses on the automated classification of motor imagery tasks. To achieve this, the ISSADTL-EEGMIC method first preprocesses EEG signals using the wavelet packet decomposition (WPD) technique. Additionally, it employs the neural architecture search network (NASNet) technique for feature extraction. Moreover, the ISSADTL-EEGMIC model performs the classification process using the stacked sparse autoencoder (SSAE) model. Furthermore, hyperparameter optimization of the SSAE model is performed using the ISSA technique, which helps achieve maximum classification performance. The results from the simulation of the ISSADTL-EEGMIC algorithm have been examined using two benchmark datasets. The experimental findings suggest that it outperforms recent approaches in terms of performance.},
  archive      = {J_EXSY},
  author       = {Awanish Kumar Mishra and Indresh Kumar Gupta and Swati Srivastava and Sultan Alfarhood and Mejdl Safran},
  doi          = {10.1111/exsy.13530},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13530},
  shortjournal = {Expert Syst.},
  title        = {Empowering EEG motor imagery classification with deep transfer learning approach},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying professional photographers through image quality
and aesthetics in flickr. <em>EXSY</em>, <em>41</em>(4), e13526. (<a
href="https://doi.org/10.1111/exsy.13526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our generation, there is an undoubted rise in the use of social media and specifically photo and video sharing platforms. These sites have proved their ability to yield rich data sets through the users’ interaction which can be used to perform a data-driven evaluation of capabilities. Nevertheless, this study reveals the lack of suitable data sets in photo and video sharing platforms and evaluation processes across them. In this way, our first contribution is the creation of one of the largest labelled data sets in Flickr with the multimodal data which has been open sourced as part of this contribution. It incorporates multimodal data, combining information from various sources such as user profiles, photo metadata, and crowdsourced features. Predicated on these data, we explored machine learning models and concluded that it is feasible to properly predict whether a user is a professional photographer or not based on self-reported occupation labels and several feature representations out of the user, photo and crowdsourced sets. We also examined the relationship between the aesthetics and technical quality of a picture and the social activity of that picture. Finally, we depicted which characteristics differentiate professional photographers from non-professionals. As far as we know, the results presented in this work represent an important novelty for identifying expertise in the domain of photography, which researchers from various domains can utilise for related applications.},
  archive      = {J_EXSY},
  author       = {Sofia Strukova and Rubén Gaspar Marco and Félix Gómez Mármol and José A. Ruipérez-Valiente},
  doi          = {10.1111/exsy.13526},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13526},
  shortjournal = {Expert Syst.},
  title        = {Identifying professional photographers through image quality and aesthetics in flickr},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient ensembles of distance-based label ranking trees.
<em>EXSY</em>, <em>41</em>(4), e13525. (<a
href="https://doi.org/10.1111/exsy.13525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble of label ranking trees (LRTs) are currently the state-of-the-art approaches to the label ranking problem. Recently, bagging, boosting, and random forest methods have been proposed, all based on the LRT algorithm, which adapts regression/classification trees to the label classification problem. The LRT algorithm uses theoretically grounded Mallows probability distribution to select the best split when growing the tree, and an EM-type process to complete the rankings on the training data when they are incomplete. These two steps have proven to be accurate, but require a large computational effort. This article proposes two alternative methods that replace the use of the Mallows distribution with distance-based criteria to select the best split at each inner node of the tree. Moreover, these distance-based criteria allow dealing with incomplete rankings natively, so avoiding the completion process. We have carried out an extensive experimental evaluation, which shows that (1) the integration of the two proposed modifications to the LRT algorithm into ensemble methods (bagging and random forest) are an order of magnitude faster than using the original Mallows-based LRT algorithm; (2) ensembles using the proposed LRT methods are significantly more accurate in the presence of incomplete rankings, while they are at least as accurate in the complete case; and (3) the two modified LRT algorithms are also an order of magnitude faster than the Mallows-based LRT, while they are at least as accurate as the Mallows-based LRT on both complete and incomplete rankings.},
  archive      = {J_EXSY},
  author       = {Enrique G. Rodrigo and Juan C. Alfaro and Juan A. Aledo and Jose A. Gámez},
  doi          = {10.1111/exsy.13525},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13525},
  shortjournal = {Expert Syst.},
  title        = {Efficient ensembles of distance-based label ranking trees},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bare-bones particle swarm optimization-based quantization
for fast and energy efficient convolutional neural networks.
<em>EXSY</em>, <em>41</em>(4), e13522. (<a
href="https://doi.org/10.1111/exsy.13522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network quantization is a critical method for reducing memory usage and computational complexity in deep learning models, making them more suitable for deployment on resource-constrained devices. In this article, we propose a method called BBPSO-Quantizer, which utilizes an enhanced Bare-Bones Particle Swarm Optimization algorithm, to address the challenging problem of mixed precision quantization of convolutional neural networks (CNNs). Our proposed algorithm leverages a new population initialization, a robust screening process, and a local search strategy to improve the search performance and guide the population towards a feasible region. Additionally, Deb&#39;s constraint handling method is incorporated to ensure that the optimized solutions satisfy the functional constraints. The effectiveness of our BBPSO-Quantizer is evaluated on various state-of-the-art CNN architectures, including VGG, DenseNet, ResNet, and MobileNetV2, using CIFAR-10, CIFAR-100, and Tiny ImageNet datasets. Comparative results demonstrate that our method delivers an excellent tradeoff between accuracy and computational efficiency.},
  archive      = {J_EXSY},
  author       = {Jihene Tmamna and Emna Ben Ayed and Rahma Fourati and Amir Hussain and Mounir Ben Ayed},
  doi          = {10.1111/exsy.13522},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13522},
  shortjournal = {Expert Syst.},
  title        = {Bare-bones particle swarm optimization-based quantization for fast and energy efficient convolutional neural networks},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated and intelligent system for world health
organization data forecasting. <em>EXSY</em>, <em>41</em>(4), e13521.
(<a href="https://doi.org/10.1111/exsy.13521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advances and social transformations have enabled the circulation of a large amount of data in the health area. Analyzing this data becomes more critical and more challenging as the volume of data increases. An alternative to performing this analysis is to use data analysis techniques to process input data sets and build consistent databases for input to machine learning algorithms. Thus, it can forecast future scenarios and collaborate with knowledge discovery. In this context, this work aims to develop a parameterizable system with automated decisions capable of collecting and analyzing many indicators provided by the World Health Organization (WHO). After these analysis steps, the system applies machine learning algorithms for predictions of different indicators to process information automatically, finding different knowledge discovery scenarios. Thus, the contribution of this article is an automated and intelligent system for WHO data forecasting. The efficiency of the system&#39;s choices and forecasts was proven with experiments in five different areas of health, obtaining assertiveness by up to 99.92%, root-mean-square error (RMSE) by up to 0.0286, and Kling-Gupta Efficiency (KGE) by up to 0.9988, hitting even the most complex cases, as shown in the confusion matrices. Finally, three case studies were carried out to expand the studies and present the potential of the system in different contexts: anemia in children, age-standardized suicide rates in men, and number of road traffic deaths.},
  archive      = {J_EXSY},
  author       = {Felipe Augusto Lara Soares and Henrique Cota de Freitas},
  doi          = {10.1111/exsy.13521},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13521},
  shortjournal = {Expert Syst.},
  title        = {Automated and intelligent system for world health organization data forecasting},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data augmentation using virtual word insertion techniques in
text classification tasks. <em>EXSY</em>, <em>41</em>(4), e13519. (<a
href="https://doi.org/10.1111/exsy.13519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labelling multiple t r aining examples for text classification models is usually time-consuming and complex. Data augmentation can be used to automatically expand the dataset by transforming the original data. However, it may cause semantic changes without modifying the labels, which reduces the effectiveness of the classifiers. In this paper, we propose a data-augmentation method called the virtual word insertion technique, which generates new sentences by randomly inserting virtual words into existing sentences. Two methods are used to achieve virtual word embedding: unweighted average and weighted average. Furthermore, a new concept of weight is proposed: the class deviation factor, which demonstrates the correlation between words and classes. Based on this new concept, different weights are assigned to words of different classes. Experiments are conducted on five different classification tasks. Ablation experiments are also performed to explore the effects of random operation and number of augmented sentences for classification results. The results of these experiments show that our method improves the classification performance and outperforms two other contrasting data-augmentation methods in automatically augmenting the dataset. Compared to raw datasets, the average accuracy improvement of our method is 3.5% for a small-scale dataset and 1% for a large-scale dataset.},
  archive      = {J_EXSY},
  author       = {Zhigao Long and Hong Li and Jiawen Shi and Xin Ma},
  doi          = {10.1111/exsy.13519},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13519},
  shortjournal = {Expert Syst.},
  title        = {Data augmentation using virtual word insertion techniques in text classification tasks},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network intrusion detection system by learning jointly from
tabular and text-based features. <em>EXSY</em>, <em>41</em>(4), e13518.
(<a href="https://doi.org/10.1111/exsy.13518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network intrusion detection systems (NIDS) play a critical role in maintaining the security and integrity of computer networks. These systems are designed to detect and respond to anomalous activities that may indicate malicious intent or unauthorized access. The need for robust NIDS solutions has never been more pressing in today&#39;s digital landscape, characterized by constantly evolving cyber threats. Deploying effective NIDS can be challenging, particularly in accurately identifying network anomalies amid the ever-increasing sophisticated and difficult-to-detect cyber threats. The motivation for our research stems from the recognition that while NIDS studies have made significant strides, there remains a crucial need for more effective and accurate methods to detect network anomalies. Commonly used features in NIDS studies include network logs, with some studies exploring text-based features such as payload. However, traditional machine and deep learning models may need to be improved in learning jointly from tabular and text-based features. Here, we present a new approach that integrates both tabular and text-based features to improve the performance of NIDS. Our research aims to address the existing limitations of NIDS and contribute to the development of more reliable and efficient network security solutions by introducing more effective and accurate methods for detecting network anomalies. Our internal experiments have revealed that the deep learning approach utilizing tabular features produces favourable results, whereas the pre-trained transformer approach needs to perform sufficiently. Hence, our proposed approach, which integrates both feature types using deep learning and pre-trained transformer approaches, achieves superior performance. These findings indicate that integrating both feature types using deep learning and pre-trained transformer approaches can significantly improve the accuracy of network anomaly detection. Moreover, our proposed approach outperforms the state-of-the-art methods in terms of accuracy, F1-score, and recall on commonly used NIDS datasets consisting of ISCX-IDS2012, UNSW-NB15, and CIC-IDS2017, with F1-scores of 99.80%, 92.37%, and 99.69%, respectively, indicating its effectiveness in detecting network anomalies.},
  archive      = {J_EXSY},
  author       = {Berkant Düzgün and Aykut Çayır and Uğur Ünal and Hasan Dağ},
  doi          = {10.1111/exsy.13518},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13518},
  shortjournal = {Expert Syst.},
  title        = {Network intrusion detection system by learning jointly from tabular and text-based features},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-scale feature fusion convolutional neural network
for facial expression recognition. <em>EXSY</em>, <em>41</em>(4),
e13517. (<a href="https://doi.org/10.1111/exsy.13517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper designs a new facial Expression recognition network called a multi-scale feature Fusion Convolutional neural Network (EFCN). This network is proposed to solve two problems in the facial expression recognition task. First, there are many commonalities between faces of different expression categories, and the recognition task cannot be precisely performed when the commonality is greater than the individuality. Secondly, facial detail features have a significant impact on the final results of expression recognition, while the image detail features extracted by traditional convolutional neural networks are not sufficient. In order to address the above issues, the feature enhancement network (FEN) and the detail information enhancement module (DEM) are designed. The FEN fuses deep and shallow features. Accordingly, the feature map contains richer information, making it easy to identify the samples. The DEM extracts and fuses the features passed by the backbone network with multi-scale features to enhance the network&#39;s ability to extract features from small regions of the face. We validated the proposed method on three datasets, RAF-DB, CK+, and JAFFE, and achieved 84.50%, 97.86%, and 91.05% accuracy, respectively, and the experimental results showed the effectiveness of the proposed method in this paper. For example, on the JAFFE dataset, the recognition accuracy of this method surpasses the MLT method by 1.87%.},
  archive      = {J_EXSY},
  author       = {Xiufeng Zhang and Xingkui Fu and Guobin Qi and Ning Zhang},
  doi          = {10.1111/exsy.13517},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13517},
  shortjournal = {Expert Syst.},
  title        = {A multi-scale feature fusion convolutional neural network for facial expression recognition},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depthwise channel attention network (DWCAN): An efficient
and lightweight model for single image super-resolution and metaverse
gaming. <em>EXSY</em>, <em>41</em>(4), e13516. (<a
href="https://doi.org/10.1111/exsy.13516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR) has gained significant attention in image processing and computer vision, driven by deep learning-based models like convolutional neural networks (CNN). Yet, the resource-intensive nature of these models poses challenges when deploying them on edge devices. To address this issue, resource-constrained models need to be developed. While recent models like the information distillation network (IDN), the information multi-distillation network (IMDN), the residual feature distillation network (RFDN), and so on, have attempted to reduce parameters and computational complexity, further optimization remains vital. Therefore, this paper presents an approach to enhancing the efficiency and lightweight nature of the SISR. We introduce a novel lightweight SR model by building upon the RFDN architecture, the winner of the AIM2020 and NTIRE2022 SR challenges. The proposed depthwise channel attention network (DWCAN) model makes some key changes to RFDN. First, it replaces the main residual feature distillation block (RFDB) with a depthwise channel attention block (DWCAB). Additionally, DWCAN includes a shallow residual block (SRB) with depthwise separable convolution (DW) and a channel attention (CA) block. The primary goal of our work is to significantly reduce model parameters, computational operations, inference time, and memory size while maintaining or improving a peak signal-to-noise ratio (PSNR) of 29 dB. The experimental results demonstrate the effectiveness of the proposed model. By applying our modifications, we achieve a notable reduction in model complexity, leading to an improved PSNR of 29.07 dB, up from RFDN&#39;s 29.04 dB on a diverse 2 K resolution (DIV2K) dataset. This underscores the potential of our lightweight model to balance computational efficiency and SR quality. Additionally, the proposed work is essential for the metaverse for two key reasons: (1) Enhancing visual quality by adding complex details to textures and objects, making the digital world feel more like reality. (2) Ensuring device compatibility across a range of gadgets, from smartphones to VR headsets, optimizing the metaverse experience for all users. In short, a lightweight single image super-resolution model for image reconstruction is proposed in this paper.},
  archive      = {J_EXSY},
  author       = {Muhammad Yasir and Inam Ullah and Chang Choi},
  doi          = {10.1111/exsy.13516},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13516},
  shortjournal = {Expert Syst.},
  title        = {Depthwise channel attention network (DWCAN): An efficient and lightweight model for single image super-resolution and metaverse gaming},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal deep transfer learning driven computer-aided breast
cancer classification using ultrasound images. <em>EXSY</em>,
<em>41</em>(4), e13515. (<a
href="https://doi.org/10.1111/exsy.13515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) is regarded as the second leading type of cancer among women globally. Ultrasound images are typically used for the identification and classification of abnormalities that exist in the breast. To enhance diagnosis performance, the computer assisted diagnosis (CAD) model finds it effective for identifying and classifying BC. Generally, the CAD technique contains distinct procedures like feature extraction, preprocessing, segmentation, and classification. The recent developments of deep learning (DL) algorithms in the form of CAD system helps to minimize the cost and enhance the ability of radiologists to interpret medical images. Therefore, this study develops an optimal deep transfer learning driven computer aided BC classification (ODTLD-CABCC) technique on ultrasound images. The presented ODTLD-CABCC algorithm undergoes pre-processing in two levels such as median filtering based noise removal and graph cut segmentation. Furthermore, the residual network (ResNet101) model can be used as a feature extractor. Finally, the sailfish optimizer (SFO) with a labelled weighted extreme learning machine (LWELM) algorithm is used for the classification process. The SFO technique is employed to choose optimal parameters involved in the LWELM algorithm. A comprehensive set of simulations are conducted on the benchmark data and the experimental outcomes are examined under numerous aspects. The comparative examination represents the supremacy of the ODTLD-CABCC technique over the other approaches.},
  archive      = {J_EXSY},
  author       = {Mahmoud Ragab and Alaa O. Khadidos and Abdulrhman M. Alshareef and Adil O. Khadidos and Mohammed Altwijri and Nawaf Alhebaishi},
  doi          = {10.1111/exsy.13515},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13515},
  shortjournal = {Expert Syst.},
  title        = {Optimal deep transfer learning driven computer-aided breast cancer classification using ultrasound images},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-dimensional atrous conv-net based architecture for
automatic diagnosis of epilepsy using electroencephalography signals and
its brain–computer interface applications. <em>EXSY</em>,
<em>41</em>(4), e13514. (<a
href="https://doi.org/10.1111/exsy.13514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise monitoring and diagnosis of epilepsy by manual analysis of EEG signals are challenging due to the low doctor-to-patient ratio, and shortage of medical resources. To automate this diagnosis in real-time, EEG based Brain–Computer Interface (BCI) system with integration of artificial intelligence techniques will prove to be propitious. This work proposes an end-to-end, one-dimensional atrous conv-net-based architecture for automatic epilepsy diagnosis using EEG signals with a conceptual framework of the EEG-BCI system for routine monitoring and clinical use. The proposed architecture has a robust backbone of six blocks of atrous convolutional layers activated with exponential linear unit functions. The six blocks are followed by the addition of a long short-term memory layer for automatic feature extraction and sequential EEG data analysis. The efficacy of the proposed architecture has been verified on three publicly available EEG datasets using various evaluation metrics, feature maps, test set evaluation, and ablation studies. An average training and validation accuracy of 96.16% and 90.80% has been achieved upon multiple runs for the three datasets. Ablation experiments indicate that the addition of each block contributed to increasing 17%–25% accuracy scores during the classification of epileptic and non-epileptic EEG signals. The real-time EEG-BCI has been analyzed using weight optimization of the proposed architecture through the NVIDIA Tensor RT framework on a 40 GB DGX A100 NVIDIA workstation. The proposed architecture has generalized well in comparison with the existing techniques for the three EEG datasets and achieved a low training and validation loss with optimum evaluation metrics. This makes the proposed architecture suitable for future EEG-BCI system deployment in the automatic diagnosis of epilepsy.},
  archive      = {J_EXSY},
  author       = {Palak Handa and Muskan Gupta and Esha Gupta and Nidhi Goel},
  doi          = {10.1111/exsy.13514},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13514},
  shortjournal = {Expert Syst.},
  title        = {One-dimensional atrous conv-net based architecture for automatic diagnosis of epilepsy using electroencephalography signals and its brain–computer interface applications},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PowerPulse: Power energy chat model with LLaMA model
fine-tuned on chinese and power sector domain knowledge. <em>EXSY</em>,
<em>41</em>(3), e13513. (<a
href="https://doi.org/10.1111/exsy.13513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, large-scale language models (LLMs) such as chat generative pre-trained transformer and generative pre-trained transformer 4 have demonstrated remarkable performance in the general domain. However, inadaptability in a particular domain has led to hallucination for these LLMs when responding in specific domain contexts. The issue has attracted widespread attention, existing domain-centered fine-tuning efforts have predominantly focused on sectors like medical, financial, and legal, leaving critical areas such as power energy relatively unexplored. To bridge this gap, this paper introduces a novel power energy chat model called PowerPulse. Built upon the open and efficient foundation language models (LLaMA) architecture, PowerPulse is fine-tuned specifically on Chinese Power Sector Domain Knowledge. This work marks the inaugural application of the LLaMA model in the field of power energy. By leveraging pertinent pre-training data and instruction fine-tuning datasets tailored for the power energy domain, the PowerPulse model showcases exceptional performance in tasks such as text generation, summary extraction, and topic classification. Experimental results validate the efficacy of the PowerPulse model, making significant contributions to the advancement of specialized language models in specific domains.},
  archive      = {J_EXSY},
  author       = {ChunLin Yin and KunPeng Du and Qiong Nong and HongCheng Zhang and Li Yang and Bin Yan and Xiang Huang and XiaoBo Wang and Xuan Zhang},
  doi          = {10.1111/exsy.13513},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13513},
  shortjournal = {Expert Syst.},
  title        = {PowerPulse: Power energy chat model with LLaMA model fine-tuned on chinese and power sector domain knowledge},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing text classification with attention matrices based
on BERT. <em>EXSY</em>, <em>41</em>(3), e13512. (<a
href="https://doi.org/10.1111/exsy.13512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification is a critical task in the field of natural language processing. While pre-trained language models like BERT have made significant strides in improving performance in this area, the distinctive dependency information that is present in text has not been fully exploited. Besides, BERT mostly captures phrase-level information in lower layers, which becomes progressively weaker with the increasing depth of layers. To address these limitations, our work focuses on enhancing text classification through the incorporation of Attention Matrices, particularly in the fine-tuning process of pre-trained models like BERT. Our approach, named AM-BERT, leverages learned dependency relationships as external knowledge to enhance the pre-trained model by generating attention matrices. In addition, we introduce a new learning strategy that enables the model to retain learned phrase-level structure information. Extensive experiments and detailed analysis on multiple benchmark datasets demonstrate the effectiveness of our approach in text classification tasks. Furthermore, we show that AM-BERT achieves stable performance improvements also in named entity recognition tasks.},
  archive      = {J_EXSY},
  author       = {Zhiyi Yu and Hong Li and Jialin Feng},
  doi          = {10.1111/exsy.13512},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13512},
  shortjournal = {Expert Syst.},
  title        = {Enhancing text classification with attention matrices based on BERT},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text classification based on PEGCN: Graph convolution
classification using location information and edge features.
<em>EXSY</em>, <em>41</em>(3), e13511. (<a
href="https://doi.org/10.1111/exsy.13511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of text classification is to label the text with known labels. In recent years, the method based on graph neural network (GNN) has achieved good results. However, the existing methods based on GNN only regard the text as the set of co-occurring words, without considering the position information of each word in the statement. At the same time, the method mainly extracts the node features in the graph, and the edge features between the nodes are not used enough. To solve these problems, a new text classification method, graph convolutional network using positions and edges, is proposed. In the word embedding section, a positional encoding input representation is employed to enable the neural network to learn the relative positional information among words. Meanwhile, the dimension of the adjacency matrix is increased to extract the multi-dimensional edge features. Through experiments on multiple text classification datasets, the proposed method is shown to be superior to the traditional text classification method, and has achieved a maximum improvement of more than 4%.},
  archive      = {J_EXSY},
  author       = {Ruidong Zhang and Zelin Guo and Hai Huan},
  doi          = {10.1111/exsy.13511},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13511},
  shortjournal = {Expert Syst.},
  title        = {Text classification based on PEGCN: Graph convolution classification using location information and edge features},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A swarm-optimized microbial colony counter. <em>EXSY</em>,
<em>41</em>(3), e13510. (<a
href="https://doi.org/10.1111/exsy.13510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of bacterial colonies is deemed to be crucial in microbiology as it helps in identifying specific categories of bacteria. The careful examination of colony morphology plays a crucial role in microbiology laboratories for the identification of microorganisms. Quantifying bacterial colonies on culture plates is a necessary task in Clinical Microbiology Laboratories, but it can be time-consuming and susceptible to inaccuracies. Therefore, there is a need to develop an automated system that is both dependable and cost-effective. Advancements in Deep Learning have played a crucial role in improving processes by providing maximum accuracy with a negligible amount of error. This research proposes an automated technique to extract the bacterial colonies using SegNet, a semantic segmentation network. The segmented colonies are then counted with the assistance of blob counter to accomplish the activity of colony counting. Furthermore, to ameliorate the proficiency of the segmentation network, the network weights are optimized using a swarm optimizer. The proposed methodology is both cost-effective and time-efficient, while also providing better accuracy and precise colony counts, ensuring the elimination of human errors involved in traditional colony counting techniques. The investigative assessments were carried out on three distinct sets of data: Microorganism, DIBaS, and tailored datasets. The results obtained from these assessments revealed that the suggested framework attained an accuracy rate of 88.32%, surpassing other conventional methodologies with the utilization of an optimizer.},
  archive      = {J_EXSY},
  author       = {Sannidhan M S and Jason Elroy Martis and Senka Krivic and Sudeepa K B and Pradeep Nazareth},
  doi          = {10.1111/exsy.13510},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13510},
  shortjournal = {Expert Syst.},
  title        = {A swarm-optimized microbial colony counter},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cosine modulated filter bank-based architecture for
extracting and fusing saliency features. <em>EXSY</em>, <em>41</em>(3),
e13508. (<a href="https://doi.org/10.1111/exsy.13508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many academics are interested in content-based image retrieval techniques like image segmentation. In computer vision, the most popular method for segmenting a digital image into different parts is known as image segmentation. We assigned the artificially intelligent algorithm to the image&#39;s critical areas by modeling human features in specific regions. In order to detect the object and identify the key parts in the ‘RGB’ photographs, we combined scenes based on a colour and depth map, or ‘RGB-D’, and used cosine modulated filter bank (CMFB), which conducts cross-scale extraction of joint features from the images during feature extraction. The proposed ‘CMFB’ combines the discovered collaborative elements with the discovered supplementary data. The features in multi-scale images is combined using fusion blocks with the goal of producing additional features (FB). Then, a saliency mapping calculation is made for the loss linked to two blocks. The suggested ‘CMFB’ is tested with the aid of five data sets, and it is shown that, the proposed ‘CMFB’ outperforms other conventional techniques.},
  archive      = {J_EXSY},
  author       = {Md. Yousuf Ali and Bin Jiang and Oindrila Chowdhury and Md. Harun-Ar-Rashid and M. Shamim Hossain and Khalid AlMutib},
  doi          = {10.1111/exsy.13508},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13508},
  shortjournal = {Expert Syst.},
  title        = {Cosine modulated filter bank-based architecture for extracting and fusing saliency features},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated face mask detection system using transfer
learning based neural network to preventing viral infection.
<em>EXSY</em>, <em>41</em>(3), e13507. (<a
href="https://doi.org/10.1111/exsy.13507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the “Internet of Medical Things (IoMT)” grows, healthcare systems can collect and process data. It is also challenging to study public health prevention requirements. Virus transmission can be prevented by wearing a mask. The World Health Organization (WHO) recommends wearing a facemask to protect against the COVID-19 pandemic—the levels of a pandemic rise across almost all regions of the world. By following the WHO rules, we support the development of face mask-detecting technologies and determine whether or not people are using masks in public locations. The proposed paradigm in this paper will work in three stages. Firstly, we use an Image data generator to import the images. In addition to using a Haar cascade (HC) classifier for detecting faces, residual learning (ResNet152V2) trains a model that detects whether someone is wearing a face mask. Detection and classification are carried out in real-time with high precision. Compared with other recently proposed methods, the model achieved 99.65% accuracy during training and 99.63% during validation.},
  archive      = {J_EXSY},
  author       = {Sonia Verma and Preeti Rani and Shelly Gupta and Richa Sharma and Kusum Yadav and Arwa N. Aledaily and Meshal Alharbi},
  doi          = {10.1111/exsy.13507},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13507},
  shortjournal = {Expert Syst.},
  title        = {An automated face mask detection system using transfer learning based neural network to preventing viral infection},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint object contour points and semantics for instance
segmentation. <em>EXSY</em>, <em>41</em>(3), e13504. (<a
href="https://doi.org/10.1111/exsy.13504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edges of objects are of great significance to the task of instance segmentation. However, most of the current popular deep neural networks do not pay much attention to the object edge information. More importantly, using the down-sampling pooling layer in the deep learning network, the edge detail information of the object will be lost. To address this issue, inspired by the manual annotation process, we propose Mask Point R-CNN aiming at promoting the neural network&#39;s attention to the object boundary. Specifically, we introduce the auxiliary task of object contour point detection on the Mask R-CNN framework, which can effectively improve the gradient flow between different tasks by multi-task learning and repairing objects&#39; boundary information via feature fusion. Consequently, the model can be more sensitive to the edges of the object and capture more geometric features. Quantitatively, the experimental results show that our Mask Point R-CNN outperforms vanilla Mask R-CNN by 3.8% on the Cityscapes dataset and 0.8% on the COCO dataset.},
  archive      = {J_EXSY},
  author       = {Wenchao Zhang and Chong Fu and Mai Zhu and Lin Cao and Ming Tie and Chiu-Wing Sham},
  doi          = {10.1111/exsy.13504},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13504},
  shortjournal = {Expert Syst.},
  title        = {Joint object contour points and semantics for instance segmentation},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NarmViz: A novel method for visualization of time series
numerical association rules for smart agriculture. <em>EXSY</em>,
<em>41</em>(3), e13503. (<a
href="https://doi.org/10.1111/exsy.13503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical association rule mining (NARM) is a popular method under the umbrella of data mining, focused on finding relationships between attributes in transaction databases. Numerical association rules for time series are a new paradigm that extends the applicability of NARM to the domain of time series. Association rule mining algorithms result in numerous rules, the interpretation of which is sometimes not easy for human experts. Therefore, various visualization methods have been developed to improve the explanation results of the rule mining process. This article is a novel contribution to the development of a new visualization method capable of presenting the association rules for time series developed according to the principles of explainable artificial intelligence. The experiments are conducted in the context of smart agriculture (i.e., agricultural time series data), and show the great potential of the proposed visualization method for the future.},
  archive      = {J_EXSY},
  author       = {Iztok Fister and Vili Podgorelec and Sancho Salcedo-Sanz and Andreas Holzinger},
  doi          = {10.1111/exsy.13503},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13503},
  shortjournal = {Expert Syst.},
  title        = {NarmViz: A novel method for visualization of time series numerical association rules for smart agriculture},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-efficiency evaluation of the data envelopment analysis
with conflict behaviour and beneficial relationship perspectives.
<em>EXSY</em>, <em>41</em>(3), e13501. (<a
href="https://doi.org/10.1111/exsy.13501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data envelopment analysis, cross-efficiency evaluation stands out as a valuable tool for ranking the effectiveness of decision-making units (DMUs). However, existing research commonly assume that DMUs are randomly classified as either collaborators or opponents of the evaluated DMUs. Unfortunately, few studies have considered the presence of conflict behaviour and beneficial relationships among DMUs during cross-efficiency evaluation. To address this research gap, this study proposes an innovative approach. Firstly, the proposed framework incorporates an interval cross-efficiency environment to accommodate the inherent uncertainty and fuzziness in the efficiency scores of DMUs. Secondly, two definitions, namely task conflict cross-efficiency and relationship conflict cross-efficiency, are introduced by combining the characteristics of the interval cross-efficiency matrix with conflict behaviour. To measure the strength of the conflict between DMUs, a novel measurement method is proposed, forming the foundation for constructing two cross-efficiency secondary programming models based on conflict behaviour. Thirdly, the Spearman&#39;s rank correlation coefficient is employed to measure the strength of beneficial relationships among DMUs. Subsequently, two cross-efficiency secondary programming models are developed based on beneficial relationships. To obtain the final scores, the efficiency scores of the four models are integrated using the Shannon entropy method. Finally, two illustrative applications are presented to demonstrate the effectiveness and robustness of the proposed method.},
  archive      = {J_EXSY},
  author       = {Hua Zhuang and Xueying Luo},
  doi          = {10.1111/exsy.13501},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13501},
  shortjournal = {Expert Syst.},
  title        = {Cross-efficiency evaluation of the data envelopment analysis with conflict behaviour and beneficial relationship perspectives},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancements in medical diagnosis and treatment through
machine learning: A review. <em>EXSY</em>, <em>41</em>(3), e13499. (<a
href="https://doi.org/10.1111/exsy.13499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aptness of machine learning (ML) to learn from large datasets, discover trends, and make predictions has demonstrated its potential to metamorphose the medical field. Medical data analysis with ML algorithms can improve patient outcomes in terms of both treatment and diagnosis. This paper investigates the numerous possibilities of ML in the medical industries, including radiology, pathology, genomics, and clinical decision-making. It also goes over the benefits and drawbacks of ML in various sectors as well as the limitations that come with its application. It illustrates the potential advantages of ML, such as better accuracy and efficiency in diagnosis and individualized treatment programs, through a review of previous studies. Lastly, it provides perspectives on prospective advancements and prospects for the discipline. This study also intends to investigate the applications of deep learning (DL) in the medical field. DL algorithms have performed exceptionally satisfactorily in several healthcare-related fields. The main conclusions of the study are summarized, and their ramifications for the healthcare sector are discussed in this paper&#39;s conclusion. This paper intends to contribute to a greater understanding of the prevailing state of the discipline and the possibility for future developments by emphasizing the prospects of these methodologies to alter medical study and clinical practice.},
  archive      = {J_EXSY},
  author       = {Mohammad Ahsan and Anam Khan and Kaif Rehman Khan and Bam Bahadur Sinha and Anamika Sharma},
  doi          = {10.1111/exsy.13499},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13499},
  shortjournal = {Expert Syst.},
  title        = {Advancements in medical diagnosis and treatment through machine learning: A review},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic segmentation and classification of polycystic
ovarian disease using attention UNet, pyspark, and ensemble learning
model. <em>EXSY</em>, <em>41</em>(3), e13498. (<a
href="https://doi.org/10.1111/exsy.13498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ovarian abnormality like polycystic ovarian disease (PCOD) is one of the most common diseases among women worldwide. PCOD not only has an impact on infertility but also hurts the psychological well-being of women affecting their quality of life. In this study, a two-class pattern learning problem is designed for the classification of PCOD. In total, 37 clinical parameters and abdominal ultrasound images of women are collected under the proper ethical protocol. Using only clinical data, an accuracy of 93.7% is obtained using Random Forest as the classifier which is further improved to 95.54% by using a Randomized Search CV during Random Forest classification. The ultrasound images are classified using the proposed Attention-UNet architecture and a mean Dice score of 0.945 is obtained indicating more accurate segmentation. The segmented images are passed through the state-of-the-art EfficientNet B6 for the classification of PCOS and non-PCOS and recorded an accuracy of 95.47%. Using big data architecture Pyspark, the performance is further enhanced to 96.8% and 96.3% for clinical and ultrasound images respectively along with the reduced computational speed. The results of these classifiers are then used to create metadata and a customized Artificial Neural Network is applied for the final prediction of PCOD and non-PCOD. From the results, it can be seen that the stacking model outperformed with an accuracy of 98.12% when compared to the single classifier. Our proposed method has very good performance with less computation, contributing a new architecture to evaluate PCOD and hence helping to improve the wellness of women.},
  archive      = {J_EXSY},
  author       = {Ashwini Kodipalli and Susheela Devi and Santosh Dasar},
  doi          = {10.1111/exsy.13498},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13498},
  shortjournal = {Expert Syst.},
  title        = {Semantic segmentation and classification of polycystic ovarian disease using attention UNet, pyspark, and ensemble learning model},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PaaS platform security enhancement using fuzzy based access
control and trust based signature. <em>EXSY</em>, <em>41</em>(3),
e13496. (<a href="https://doi.org/10.1111/exsy.13496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform-as-a-Service (PaaS) is one of the cloud computing services which will be offered to the clients through the virtualization platform. The problem is constructed in the PaaS platform effectively excluding internet service suppliers which do not meet highly secure standards, and so security issues can be a significant a hurdle to online computation. To address the aforementioned issues, the suggested method primarily concentrated proceeding the safety of PaaS infrastructure by using a novel fuzzy based access control technique, in which the information source is realized by using CDO security feature. To further enhance the security, security using trust based signature has been used in which the class and packet filter for each specified authorization were mapped to our class by modifying the security feature code. This public key will be utilized for authenticate that transaction relevant signature if the identity provider (IdP) has provided public safety data about this same two major components represents this little token on whose trusted trademark components are built put, that is, the entire item or even the claims contained. The suggested technique outperforms other current methods in terms of several performance parameters, with a high throughput of 14, CPU utilization of 12.5, and a low execution time of 5.2 s. The proposed ‘PaaS platform security enhancement with the use of fuzzy and trust-based signatures’ provides high throughput and low execution time. This research work proposes a novel approach to enhancing the security of PaaS infrastructure, which is offered to clients through the virtualization platform in cloud computing. The current problem with PaaS is that it excludes internet service providers that do not meet high-security standards, which poses a significant challenge to online computation. The suggested approach focuses on improving the safety of the PaaS platform by employing a fuzzy-based access control technique, using the connectionless data objects security feature as the information source. To further strengthen security, the approach utilizes a trust-based signature technique where the class and packet filter for each authorization are mapped to the class of the proposed method by modifying the security feature code. The proposed public key is then used to authenticate the transaction relevant signature, provided the IdP has given public safety data about these two components of the token on which Trusted Trademark components are built, that is, the entire item or even the claims contained within it. The proposed approach outperforms current methods in terms of several performance parameters, such as a high throughput of 14, a high CPU utilization of 12.5, and a low execution time of 5.2 s. In summary, the proposed PaaS platform security enhancement using fuzzy and trust-based signatures provides an effective way to achieve high throughput and low execution time while ensuring platform security.},
  archive      = {J_EXSY},
  author       = {Srinivasulu Pathakamuri and B. V. Ramana Reddy and A. P. Siva Kumar},
  doi          = {10.1111/exsy.13496},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13496},
  shortjournal = {Expert Syst.},
  title        = {PaaS platform security enhancement using fuzzy based access control and trust based signature},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WegFormer: Transformers for weakly supervised semantic
segmentation. <em>EXSY</em>, <em>41</em>(3), e13495. (<a
href="https://doi.org/10.1111/exsy.13495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although convolutional neural networks (CNNs) have achieved remarkable progress in weakly supervised semantic segmentation (WSSS), there are still deficiencies of object incompleteness due to the lack of receptive field and insufficient utilization of global context information of CNN. Based on the above observations, we propose a simple and effective method, WegFormer. Specifically, WegFormer captures the global context information with the Vision Transformer (ViT) as the classification network and is equipped with Deep Taylor Decomposition (DTD) principle and Soft Erase (SE) module to generate more integral pseudo labels and smooth further. However, we observe that although the generated pseudo-labels are more complete, they intrude into the background region, that is, background incompleteness problem. The Efficient Potential Object Mining (EPOM) module we propose solves this problem well. Extensive experiments on the challenging PASCAL VOC 2012 and MS COCO 2014 demonstrate the effectiveness of WegFormer, where superior results of and are obtained on the PASCAL VOC 2012 and MS COCO 2014 validation sets, respectively.},
  archive      = {J_EXSY},
  author       = {Chunmeng Liu and Guangyao Li},
  doi          = {10.1111/exsy.13495},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13495},
  shortjournal = {Expert Syst.},
  title        = {WegFormer: Transformers for weakly supervised semantic segmentation},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-aspect permutation tests for model selection.
<em>EXSY</em>, <em>41</em>(3), e13492. (<a
href="https://doi.org/10.1111/exsy.13492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In typical machine learning frameworks, model selection is of fundamental importance: commonly, multiple models have to be trained and compared in order to identify the one with the best predictive performances. The aim of this study is to provide a new tool to improve the model selection process, allowing the user to identify the algorithm which significantly outperforms the other candidates. It proposes a robust model selection procedure based on a multi-aspect permutation test which makes it possible to detect differences in both location and variability between two paired samples of prediction errors. A new extension of the nonparametric combination (NPC) methodology is therefore introduced and is integrated with an appropriate ranking procedure in order to deal with the comparison of C ≥ 2 candidate models. A simulation study is conducted to evaluate the performances of this testing procedure in 2-sample and -sample problems, by generating data from various well-known distributions and simulating several possible null and alternative scenarios. The adoption of the proposed technique in machine learning model selection problems is then discussed by means of multiple real data applications. These applications confirm what emerges from the simulation study: the introduced NPC-based approach performs well under several different scenarios and represents a valuable tool for robust machine learning model selection.},
  archive      = {J_EXSY},
  author       = {Elena Barzizza and Nicolò Biasetton and Riccardo Ceccato},
  doi          = {10.1111/exsy.13492},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13492},
  shortjournal = {Expert Syst.},
  title        = {Multi-aspect permutation tests for model selection},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid evolutionary intelligent network for sentiment
analysis using twitter data during COVID-19 pandemic. <em>EXSY</em>,
<em>41</em>(3), e13489. (<a
href="https://doi.org/10.1111/exsy.13489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 pandemic has impacted many nations, causing physical as well as mental health concerns globally. In most countries, governments enforced strict lockdowns and social distancing, thus affecting people&#39;s daily lives. People usually tweet their views on online platforms that is unstructured text with implicit meaning. With the evolution of artificial intelligence in the natural language processing domain, the prediction of sentiments accurately has become a challenge. To contribute as a solution to this, a hybrid approach is proposed for sentiment prediction with the use of an evolutionary-based approach, transfer-based learning and machine learning. The proposed approach uses bidirectional encoder representations from transformers (BERT) with genetic algorithm (GA) and support vector machine (SVM), namely, hybrid evolutionary intelligent model (GA-BERT-SVM). These approaches aid in extracting important features considering semantics and context present in the text. To avoid the limitations of the backpropagation approach, such as trapping in local minima and overfitting the data, the initial parameters (weights and biases) of the dense layers has been optimized using GA. Additionally, the pretrained BERT layers are utilized without any modification, following a standard transfer learning approach. The BERT embeddings are concatenated with the SVM for training and classification. GridSearchCV and GeneticSearchCV is used for obtaining optimal parameters of SVM. A multi-classification problem is tackled using a benchmark COVID-19 dataset, which comprises of Twitter data and is categorized into COVIDSENTI-A, COVIDSENTI-B, COVIDSENTI-C and a combined dataset called COVIDSENTI. Experimental evaluation demonstrates promising results of the proposed model in terms of accuracy, F1-score, precision and recall, surpassing state-of-the-art approaches.},
  archive      = {J_EXSY},
  author       = {Harnain Kour and Manoj Kumar Gupta},
  doi          = {10.1111/exsy.13489},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13489},
  shortjournal = {Expert Syst.},
  title        = {Hybrid evolutionary intelligent network for sentiment analysis using twitter data during COVID-19 pandemic},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent computational methods for economics.
<em>EXSY</em>, <em>41</em>(2), e13523. (<a
href="https://doi.org/10.1111/exsy.13523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Takfarinas Saber and Dominik Naeher and Malika Bendechache},
  doi          = {10.1111/exsy.13523},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13523},
  shortjournal = {Expert Syst.},
  title        = {Intelligent computational methods for economics},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recent trends and advances in machine learning challenges
and applications for industry 4.0. <em>EXSY</em>, <em>41</em>(2),
e13506. (<a href="https://doi.org/10.1111/exsy.13506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This editorial summarizes and analyzes 17 articles selected for a special issue on machine learning advances for Industry 4.0 applications. The diverse articles cover fault detection, deep learning optimisation, IoT networking, vehicle control, recommendation systems and domain knowledge integration. Key methods represented include neural networks, deep learning, reinforcement learning and explainable AI. Real-world industrial case studies showcase machine learning&#39;s versatility in enabling intelligent automation, control, and decision-making across manufacturing, healthcare, transportation and other sectors. While highlighting theoretical innovations, the contributions also demonstrate machine learning&#39;s transformative potential for intelligent, connected, self-optimising next generation production systems. This editorial concisely overviews the latest trends represented in this special issue.},
  archive      = {J_EXSY},
  author       = {Victor Rodriguez-Fernandez and David Camacho},
  doi          = {10.1111/exsy.13506},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13506},
  shortjournal = {Expert Syst.},
  title        = {Recent trends and advances in machine learning challenges and applications for industry 4.0},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-criteria evolutionary optimization of a traffic light
using genetics algorithm and teaching-learning based optimization.
<em>EXSY</em>, <em>41</em>(2), e13487. (<a
href="https://doi.org/10.1111/exsy.13487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, the development of urbanization and increasing the number of vehicles has resulted in displeased consequences like traffic congestion and vehicle queuing. The vast majority of countries in the world encounter the challenge of the explosive rise in traffic demand. In this regard, it is necessary to meet traffic demand in transport networks, especially in metropolitans. In traffic management and shortening the trip duration, traffic lights on the signalized intersections play an essential role in urban pathways. This work provides a multi-criteria decision-making method for optimum traffic light control in an isolated corner. The main idea involves establishing a set of sub-optimal solutions for traffic light timing and selecting the best one among the diverse solutions. We have mathematically modelled the problem as an optimization problem to achieve an optimal solution with less waiting time for vehicles in intersections and the lowest cost. Genetic algorithm (GA) and Teaching-Learning-based Optimization (TLBO) are utilized for each phase to create a set of suitable timing scenarios. The Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) method is used to identify the best scenario, considering both waiting vehicles and traffic capacity as decision criteria. Its efficiency has been demonstrated over three different traffic volumes. Also, in a real-world implementation, its practical capability has been approved at a crossroads in Mashhad, Iran. The simulations indicate the improvement in the number of vehicles waiting behind the crossroad and the traffic capacity by 10% and 6.76% compared to the existing signal timing of the studied intersection, respectively.},
  archive      = {J_EXSY},
  author       = {Hossein Yektamoghadam and Amirhossein Nikoofard and Masoumeh Behzadi and Mahdi Khosravy and Nilanjan Dey and Olaf Witkowski},
  doi          = {10.1111/exsy.13487},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13487},
  shortjournal = {Expert Syst.},
  title        = {Multi-criteria evolutionary optimization of a traffic light using genetics algorithm and teaching-learning based optimization},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted distance classification method based on data
intelligence. <em>EXSY</em>, <em>41</em>(2), e13486. (<a
href="https://doi.org/10.1111/exsy.13486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s world, data is essential for enhancing an organization&#39;s development and decision-making processes. Implementing artificial intelligence is necessary to analyse data and make meaningful recommendations. Machine learning distance classification methods are used to classify observations in various algorithms, such as K -nearest neighbours (KNN), learning vector quantization and support vector machines, and are commonly used in academia and industry. However, this procedure faces a significant challenge in finding optimal parameters (i.e., distance metrics and the desired number of neighbours) in multidimensional datasets. This study presents a novel variation of a general method for classifying new observations. The method defines a new measure called closeness , which represents the proximity between an observation and the distribution. The advantages of this method are the use of both parametric and non-parametric distance metrics and the ability to classify observations in cases where the simple method does not provide a clear answer. This method was demonstrated using KNN over three datasets and was observed to succeed in providing correct classifications, while the simple KNN method did not. The results showed that the proposed method increased the accuracy score to 40.7% in two of the three cases and that the closeness values were well defined by the proximity between the new observation and the given distribution. In addition, the F1 score increased up to 47.97%. The innovative method introduced here may be examined and used in various distance classification algorithms.},
  archive      = {J_EXSY},
  author       = {Michal Koren and Oded Koren and Or Peretz},
  doi          = {10.1111/exsy.13486},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13486},
  shortjournal = {Expert Syst.},
  title        = {Weighted distance classification method based on data intelligence},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutions in machine learning technology for financial
distress prediction: A comprehensive review and comparative analysis.
<em>EXSY</em>, <em>41</em>(2), e13485. (<a
href="https://doi.org/10.1111/exsy.13485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, financial distress prediction (FDP), also known as corporate failure prediction or bankruptcy prediction, has gained significant importance due to its impact on organizations, especially during unexpected events like pandemics and wars. Machine learning (ML) models have emerged as innovative and essential tools in predicting financial distress, leveraging the ever-increasing volume of databases and computing power. This study utilizes bibliographic techniques to contribute to the field&#39;s literature review to address the disorganized nature of the existing literature on FDP, reduce confusion, and provide clarity to domain researchers. These techniques enable identifying the progress of articles published over the years, influential authors, and highly cited articles. Additionally, the study examines crucial aspects of data preprocessing, such as missing data, imbalanced data, feature selection, and outliers, as they significantly impact the robustness and performance of ML models. Furthermore, it discusses essential models employed in FDP, focusing on recent advancements that represent promising trends. In conclusion, this study contributes to the field by uncovering novel trends and proposing possible directions for advancing FDP research. These findings will guide researchers, practitioners, and stakeholders in their quest for improved prediction and decision-making in financial distress.},
  archive      = {J_EXSY},
  author       = {Kaoutar El Madou and Said Marso and Moad El Kharrim and Mohamed El Merouani},
  doi          = {10.1111/exsy.13485},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13485},
  shortjournal = {Expert Syst.},
  title        = {Evolutions in machine learning technology for financial distress prediction: A comprehensive review and comparative analysis},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid input shaping and fuzzy logic-based position and
oscillation control of tower crane system. <em>EXSY</em>,
<em>41</em>(2), e13484. (<a
href="https://doi.org/10.1111/exsy.13484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective control of the tower crane system (TCS) is crucial for the safe and quick transport of goods from one point to another in industries and construction sites. However, the existing literature either depends on the dynamic model of the system, which is prone to errors and assumptions, or requires the feedback of the states to be controlled, which can be costly and or complex. In particular, the TCS has many states and is highly nonlinear. Thus, a non-model-based control approach that depends on minimal feedback sensors is needed. The primary contribution of this work is that the potential hybrid configurations of the input shaping control (ISC) and fuzzy logic control (FLC) were proposed and investigated. The study provides the best hybrid configuration of the ISC and FLC (non-model-based controllers) for optimal positioning and anti-swing control of the TCS. Three configurations of the ISC + FLC, namely shaper as reference input (SARI), shaper as input to the FLC (SAI2F), and shaper as input to the plant (SAI2P) were investigated to assess the optimal hybrid configuration of the ISC + FLC. The results demonstrated that the hybrid ISC + FLC scheme had improved the response of ISC by at least 150% and enhanced the oscillation reduction of FLC by 72%. Finally, these analyses showed that the ISC + FLC could achieve reasonable control of the tower crane by only considering the output of one state (position). This makes the control scheme cheaper and reduces the complexity and computational time compared to other full-state feedback controllers.},
  archive      = {J_EXSY},
  author       = {Ahmad Bala Alhassan and Wudhichai Assawinchaichote and Huiyan Zhang and Yan Shi},
  doi          = {10.1111/exsy.13484},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13484},
  shortjournal = {Expert Syst.},
  title        = {Hybrid input shaping and fuzzy logic-based position and oscillation control of tower crane system},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cascaded deep learning framework for iris centre
localization in facial image. <em>EXSY</em>, <em>41</em>(2), e13483. (<a
href="https://doi.org/10.1111/exsy.13483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate iris centre localization is crucial in many computer vision and facial biometric applications such as gaze estimation, human–computer interaction, iris recognition, and liveness detection. However, it is challenging in an uncontrolled environment due to variations like pose, scale, rotation, specular reflection, and image quality. Therefore, a cascaded deep learning framework for iris centre localization in facial images is proposed that is robust to the abovementioned variations. The proposed approach consists of (i) YOLOv3 for eye detection, (ii) UNet for iris segmentation, and (iii) statistical modelling for iris centre localization. The eyes are first detected using the YOLOv3, and subsequently, iris segmentation is performed within the detected eyes using the UNet. Following iris segmentation, statistical modelling is employed to enhance the localization accuracy of the iris centre. Experiments were performed on benchmark databases, resulting in a standardized error measure S ED of 3.405 pixels for BioID and 3.259 pixels for GI4E databases. In addition, the robustness of the proposed eye detection model was further evaluated on the Yale B for illumination variations and the CAS-PEAL for pose variations.},
  archive      = {J_EXSY},
  author       = {Naseem Ahmad and Ghulam Muhammad and Kuldeep Singh Yadav and Rabul Hussain Laskar and Ashraf Hossain and Zulfiqar Ali},
  doi          = {10.1111/exsy.13483},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13483},
  shortjournal = {Expert Syst.},
  title        = {A cascaded deep learning framework for iris centre localization in facial image},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced meta-heuristic methods for industrial winding
process modelling. <em>EXSY</em>, <em>41</em>(2), e13438. (<a
href="https://doi.org/10.1111/exsy.13438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear industrial system modelling entails two critical phases: The first is selecting a method in order to estimate the parameter list values, and the second is selecting a proper model structure with a relatively short parameter list. Developing a comprehensive model for an industrial design process is critical for the model-based control system. This article presents a model-based strategy that aims to develop three linear and three nonlinear dynamic models using three well-known meta-heuristic optimization algorithms to simulate a challenging plant-wide process. As a case study, an industrial real winding process (WP) is targeted to accomplish the aim of this study. The algorithms have been optimized to find the best weights of the inputs of the WP with a key issue to effectively describe the behaviour aspects of the process. To test the validity of the developed models, a series of experiments were carried out on each of the developed linear and nonlinear models. Several relevant evaluation metric measures are used to demonstrate the models&#39; performance level. The experimental results for training and test sets of 1250 independent samples for each set based upon the proposed modelling schemes show that the mean square error to correctly model the WP occurred in less than 0.001. A comparison of the developed intelligent linear and nonlinear models with the Auto-Regressive Integrated Moving Average (ARIMA) and Multiple Linear Regression (MLR) models obtained through the evaluation criteria asserts the effectiveness of the proposed models-based approaches.},
  archive      = {J_EXSY},
  author       = {Dheeb Albashish and Hossam M. J. Mustafa and Ruba Abu Khurma and Basela Hasan and Sulieman Bani-Ahmad and Azizi Abdullah and Anas Arram},
  doi          = {10.1111/exsy.13438},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13438},
  shortjournal = {Expert Syst.},
  title        = {Enhanced meta-heuristic methods for industrial winding process modelling},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimized extreme learning machine-based novel model for
bearing fault classification. <em>EXSY</em>, <em>41</em>(2), e13432. (<a
href="https://doi.org/10.1111/exsy.13432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the rolling element bearing (REB) fault classification problem by tackling the issue of identifying the appropriate parameters for the extreme learning machine (ELM) and enhancing its effectiveness. This study introduces a memetic algorithm (MA) to identify the optimal ELM parameter set for compact ELM architecture alongside better ELM performance. The goal of using MA is to investigate the promising solution space and systematically exploit the facts in the viable solution space. In the proposed method, the local search method is proposed along with link-based and node-based genetic operators to provide a tight ELM structure. A vibration data set simulated from the bearing of rotating machinery has been used to assess the performance of the optimized ELM with the REB fault categorization problem. The complexity involved in choosing a promising feature set is eliminated because the vibration data has been transformed into kurtograms to reflect the input of the model. The experimental results demonstrate that MA efficiently optimizes the ELM to improve the fault classification accuracy by around 99.0% and reduces the requirement of hidden nodes by 17.0% for both data sets. As a result, the proposed scheme is demonstrated to be a practically acceptable and well-organized solution that offers a compact ELM architecture in comparison to the state-of-the-art methods for the fault classification problem.},
  archive      = {J_EXSY},
  author       = {Sandeep S. Udmale and Aneesh G. Nath and Durgesh Singh and Aman Singh and Xiaochun Cheng and Divya Anand and Sanjay Kumar Singh},
  doi          = {10.1111/exsy.13432},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13432},
  shortjournal = {Expert Syst.},
  title        = {An optimized extreme learning machine-based novel model for bearing fault classification},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention based hybrid parametric and neural network models
for non-stationary time series prediction. <em>EXSY</em>,
<em>41</em>(2), e13419. (<a
href="https://doi.org/10.1111/exsy.13419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates non-stationary time series analysis and forecasting techniques for financial datasets. We focus on the use of a popular non-stationary parametric model namely GARCH and neural network model LSTM, with an attention mechanism to capture the complex temporal dynamics and dependencies in the data. We propose a hybrid GARCH-ATT-LSTM model where the GARCH model is employed for volatility forecasting, attention mechanism is applied to capture the more important parts of the data sequence and enhance the interpretability of the model, and the LSTM model is used for price forecasting. Our experiments are conducted on real-world financial datasets, that is, Apple stock price, Dow Jones index, and gold futures price. We compare the performance of GARCH-ATT-LSTM against the sole LSTM model, ATT-LSTM model, and LSTM-GARCH model. Our results show that GARCH-ATT-LSTM outperforms the baseline methods and achieves high accuracy in price forecasting. It implies the effectiveness of the attention mechanism in improving the interpretability and stability of the model and the success of combining parametric models with neural network models. The findings suggest that GARCH-ATT-LSTM can be a valuable tool for non-stationary time series analysis and forecasting in financial applications.},
  archive      = {J_EXSY},
  author       = {Zidi Gao and Ercan Engin Kuruoğlu},
  doi          = {10.1111/exsy.13419},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13419},
  shortjournal = {Expert Syst.},
  title        = {Attention based hybrid parametric and neural network models for non-stationary time series prediction},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forgetting curve models: A systematic review aimed at
consolidating the main models and outlining possibilities for future
research in production. <em>EXSY</em>, <em>41</em>(2), e13405. (<a
href="https://doi.org/10.1111/exsy.13405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research surveys current knowledge about forgetting curves and their application in production, aiming to identify the main characteristics and tendencies and research gaps on this topic. Faced with the need to improve tools that allow production planners to predict programmed batches with greater precision, it was found that there are still gaps to be filled that allow the application of learning and forgetting techniques in the production process. To compose the scope of this research, a systematization of the existing literature was carried out, using the keywords ‘forgetting curves’, ‘total forgetting’, ‘learning and forgetting curve’ and ‘forgetting effects’, in the databases of Science Direct, Scielo, Scopus, Web of Science and Google Academics, which allowed classifying and organizing the developed models into 3 groups: Deterministic models, Statistical models and Functional models. This systematic process consisted of selecting databases, filtering publications, reviewing information, and analysing models, providing a detailed analysis on a topic that, despite being promising, is poorly explored in the industry, demonstrating and indicating gaps in research and application. To be filled.},
  archive      = {J_EXSY},
  author       = {José Ângelo Ferreira and Edson Luiz Valmorbida and Bruno Goulart Sato and Bruno Pontes Fuentes and Renan Botti},
  doi          = {10.1111/exsy.13405},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13405},
  shortjournal = {Expert Syst.},
  title        = {Forgetting curve models: A systematic review aimed at consolidating the main models and outlining possibilities for future research in production},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). …And the pursuit of happiness: International migratory flows
and countries’ subjective well-being. <em>EXSY</em>, <em>41</em>(2),
e13402. (<a href="https://doi.org/10.1111/exsy.13402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research considers observed migratory flows as a source of information for the preferences of individuals. Using data on global migration flows from 2005 onwards, the extent to which the structure of the international migration system is consistent with an implicit ranking of countries is examined. Moreover, by assuming that migrants&#39; decision-making is guided by utility maximization, we interpret the scores of the ranking as a measure of subjective well-being or happiness. After this, the estimated scores are associated with a set of factors usually considered key drivers of happiness. Empirical results suggest that income is the most relevant predictor of happiness and this relationship is non-linear.},
  archive      = {J_EXSY},
  author       = {José Miguel Navarro-Azorín and José María Ramos-Parreño},
  doi          = {10.1111/exsy.13402},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13402},
  shortjournal = {Expert Syst.},
  title        = {…And the pursuit of happiness: International migratory flows and countries&#39; subjective well-being},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven fault diagnosis approaches for industrial
equipment: A review. <em>EXSY</em>, <em>41</em>(2), e13360. (<a
href="https://doi.org/10.1111/exsy.13360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Undetected and unpredicted faults in heavy industrial machines/equipment can lead to unwanted failures. Therefore, prediction of faults puts paramount importance on maintaining the reliability and availability of capital-intensive equipment. Due to large number of interconnected and interdependent mechanical and electrical components in the machines, fault analysis becomes a complex and challenging task. Under these circumstances, data-driven fault diagnosis (DDFD) is one of the most powerful, reliable and cost-effective artificial intelligence tools to detect, isolate, identify and classify the occurrence of faults. This article aims to make a comprehensive literature survey of various DDFD approaches used for analysing faults in industrial machines/equipment; and summarizing the strengths, limitations, and possible applications of existing fault diagnosis methods. Analysing and synthesizing 190 research works conducted on DDFD in last two decades revealed three types of DDFD approaches: supervised-learning, semi-supervised-learning and unsupervised-learning-based fault diagnosis. The supervised-learning is predominantly applied for fault diagnosis contributing to 82% of research works. Therefore, this article puts special emphasis on two supervised-learning-based approaches for fault diagnosis: (i) classification-based artificial neural network approach, and (ii) inference-based Bayesian network approach. Finally, these fault diagnosis approaches have been briefly discussed with effectiveness of the models and their possible inclusion in future industrial applications.},
  archive      = {J_EXSY},
  author       = {Atma Ram Sahu and Sanjay Kumar Palei and Aishwarya Mishra},
  doi          = {10.1111/exsy.13360},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13360},
  shortjournal = {Expert Syst.},
  title        = {Data-driven fault diagnosis approaches for industrial equipment: A review},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic data mining-based decision support for quality
assessment in steel industry. <em>EXSY</em>, <em>41</em>(2), e13319. (<a
href="https://doi.org/10.1111/exsy.13319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study evaluates quality management practices in Industry 4.0 in a specific case of steel manufacturing. We formulate a novel proposal based on Semantic Data Mining techniques a step towards knowledge-driven decision support based on the industrial Six Sigma approach. In our research we combine machine learning classifiers, and explanation generation algorithms with the Six Sigma practice to automate the evaluation of quality of steel products and determine origins of their defects. We describe our original method, and provide evaluation of the results with real–life data from our industrial partner.},
  archive      = {J_EXSY},
  author       = {Maciej Szelążek and Szymon Bobek and Grzegorz J. Nalepa},
  doi          = {10.1111/exsy.13319},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13319},
  shortjournal = {Expert Syst.},
  title        = {Semantic data mining-based decision support for quality assessment in steel industry},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is attention all you need for intraday forex trading?
<em>EXSY</em>, <em>41</em>(2), e13317. (<a
href="https://doi.org/10.1111/exsy.13317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of this paper is to analyse whether the Transformer neural network, which has become one of the most influential algorithms in Artificial Intelligence over the last few years, exhibits predictive capabilities for high-frequency Forex data. The prediction task is to classify short-term Forex movements for six currency pairs and five different time intervals from 60 to 720 min. We find that the Transformer exhibits high predictive power in the context of intraday Forex trading. This performance is slightly better than for the carefully selected benchmark – ResNet-LSTM, which currently is a state-of-the-art algorithm. Since intraday Forex trading based on deep learning models is largely unexplored, we offer insight on which currency pair and time interval are amenable to devising a profitable trading strategy. We also show that high predictive accuracy can be misleading in real world trading for short time intervals, as models trained on OHLC data tend to report the highest accuracy when the spread cost is the highest. This renders assessment based on typical machine learning metrics overly optimistic. Therefore, it is critical to backtest frequent intraday Forex trading strategies with realistic cost assumptions, which is rarely the case in empirical literature. Lastly, sensitivity analysis shows that the length of the time interval used for training does not play a critical role in the Transformer&#39;s predictive capabilities, whereas features derived from technical analysis are essential.},
  archive      = {J_EXSY},
  author       = {Przemysław Grądzki and Piotr Wójcik},
  doi          = {10.1111/exsy.13317},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13317},
  shortjournal = {Expert Syst.},
  title        = {Is attention all you need for intraday forex trading?},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An attention-based logistic-CNN-BiLSTM hybrid neural network
for credit risk prediction of listed real estate enterprises.
<em>EXSY</em>, <em>41</em>(2), e13299. (<a
href="https://doi.org/10.1111/exsy.13299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise credit risk prediction is to predict whether enterprises will default in the future, according to a variety of historical data by establishing a corresponding relationship between historical operating conditions and default status. To improve the accuracy of credit risk prediction of listed real estate enterprises and effectively reduce difficulty of government management, we propose an attention-based CNN-BiLSTM hybrid neural network enhanced with features of results of logistic regression, and constructs the credit risk prediction index system of listed real estate enterprises from five characteristic dimensions: profitability, debt-paying ability, growth ability, operating ability and enterprise basic information. This study uses data from the 2017–2020 annual reports of listed real estate enterprises on China&#39;s Shanghai and Shenzhen stock exchanges. A five different verifications yields average sensitivity, specificity, and quality index of 99.28%, 94.57% and 97.15%, respectively. The results show that our approach achieves better experimental results than previous works, by comparing PSO-SVM model, RS-PSO-SVR model and PSO-BP model. We conclude that the Logistic-CNN-BiLSTM-att model is more effective for the credit risk prediction of listed real estate enterprises.},
  archive      = {J_EXSY},
  author       = {Xinsheng Zhang and Yulong Ma and Minghu Wang},
  doi          = {10.1111/exsy.13299},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13299},
  shortjournal = {Expert Syst.},
  title        = {An attention-based logistic-CNN-BiLSTM hybrid neural network for credit risk prediction of listed real estate enterprises},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning tuning by diversity oriented firefly
metaheuristics for industry 4.0. <em>EXSY</em>, <em>41</em>(2), e13293.
(<a href="https://doi.org/10.1111/exsy.13293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progress of Industrial Revolution 4.0 has been supported by recent advances in several domains, and one of the main contributors is the Internet of Things. Smart factories and healthcare have both benefited in terms of leveraged quality of service and productivity rate. However, there is always a trade-off and some of the largest concerns include security, intrusion, and failure detection, due to high dependence on the Internet of Things devices. To overcome these and other challenges, artificial intelligence, especially machine learning algorithms, are employed for fault prediction, intrusion detection, computer-aided diagnostics, and so forth. However, efficiency of machine learning models heavily depend on feature selection, predetermined values of hyper-parameters and training to deliver a desired result. This paper proposes a swarm intelligence-based approach to tune the machine learning models. A novel version of the firefly algorithm, that overcomes known deficiencies of original method by employing diversification-based mechanism, has been proposed and applied to both feature selection and hyper-parameter optimization of two machine learning models—XGBoost and extreme learning machine. The proposed approach has been tested on four real-world Industry 4.0 data sets, namely distributed transformer monitoring, elderly fall prediction, BoT-IoT, and UNSW-NB 15. Achieved results have been compared to the results of eight other cutting-edge metaheuristics, that have been implemented and tested under the same conditions. The experimental outcomes strongly indicate that the proposed approach significantly outperformed all other competitor metaheuristics in terms of convergence speed and results&#39; quality measured with standard metrics—accuracy, precision, recall, and f1-score.},
  archive      = {J_EXSY},
  author       = {Luka Jovanovic and Nebojsa Bacanin and Miodrag Zivkovic and Milos Antonijevic and Bojan Jovanovic and Marija Bogicevic Sretenovic and Ivana Strumberger},
  doi          = {10.1111/exsy.13293},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13293},
  shortjournal = {Expert Syst.},
  title        = {Machine learning tuning by diversity oriented firefly metaheuristics for industry 4.0},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The orchestration of machine learning frameworks with data
streams and GPU acceleration in kafka-ML: A deep-learning performance
comparative. <em>EXSY</em>, <em>41</em>(2), e13287. (<a
href="https://doi.org/10.1111/exsy.13287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) applications need large volumes of data to train their models so that they can make high-quality predictions. Given digital revolution enablers such as the Internet of Things (IoT) and the Industry 4.0, this information is generated in large quantities in terms of continuous data streams and not in terms of static datasets as it is the case with most AI (Artificial Intelligence) frameworks. Kafka-ML is a novel open-source framework that allows the complete management of ML/AI pipelines through data streams. In this article, we present new features for the Kafka-ML framework, such as the support for the well-known ML/AI framework PyTorch, as well as for GPU acceleration at different points along the pipeline. This pipeline will be described by taking a real Industry 4.0 use case in the Petrochemical Industry. Finally, a comprehensive evaluation with state-of-the-art deep learning models will be carried out to demonstrate the feasibility of the platform.},
  archive      = {J_EXSY},
  author       = {Antonio Jesús Chaves and Cristian Martín and Manuel Díaz},
  doi          = {10.1111/exsy.13287},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13287},
  shortjournal = {Expert Syst.},
  title        = {The orchestration of machine learning frameworks with data streams and GPU acceleration in kafka-ML: A deep-learning performance comparative},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy optimization in path arbitrary wireless sensor
network. <em>EXSY</em>, <em>41</em>(2), e13282. (<a
href="https://doi.org/10.1111/exsy.13282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A network of wireless sensors is a self-infrastructure approach with many sensory nodes. The distributed sensory nodes communicate with each other via sensory points. In wireless sensor network (WSN), the sensory nodes collect information for healthcare, military and monitoring systems. Such networks require an exclusive arrangement of the nodes to challenge inherent limitations and energy deficiency. The conventional design of a communication system consumes more energy with high latency causing degraded performance. This study provided a machine learning-based path optimization mechanism using the least energy resources in designing an effective wireless network system with enhanced three measures of network performance, including throughput, packet delivery efficiency and energy usage. The proposed methodology is validated through network simulation tools.},
  archive      = {J_EXSY},
  author       = {B. Harish Goud and T. N. Shankar and Basant Sah and Rajanikanth Aluvalu},
  doi          = {10.1111/exsy.13282},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13282},
  shortjournal = {Expert Syst.},
  title        = {Energy optimization in path arbitrary wireless sensor network},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel intelligent approach for man-in-the-middle attacks
detection over internet of things environments based on message queuing
telemetry transport. <em>EXSY</em>, <em>41</em>(2), e13263. (<a
href="https://doi.org/10.1111/exsy.13263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most common attacks is man-in-the-middle (MitM) which, due to its complex behaviour, is difficult to detect by traditional cyber-attack detection systems. MitM attacks on internet of things systems take advantage of special features of the protocols and cause system disruptions, making them invisible to legitimate elements. In this work, an intrusion detection system (IDS), where intelligent models can be deployed, is the approach to detect this type of attack considering network alterations. Therefore, this paper presents a novel method to develop the intelligent model used by the IDS, being this method based on a hybrid process. The first stage of the process implements a feature extraction method, while the second one applies different supervised classification techniques, both over a message queuing telemetry transport (MQTT) dataset compiled by authors in previous works. The contribution shows excellent performance for any compared classification methods. Likewise, the best results are obtained using the method with the highest computational cost. Thanks to this, a functional IDS will be able to prevent MQTT attacks.},
  archive      = {J_EXSY},
  author       = {Álvaro Michelena and José Aveleira-Mata and Esteban Jove and Martín Bayón-Gutiérrez and Paulo Novais and Oscar Fontenla Romero and José Luis Calvo-Rolle and Héctor Aláiz-Moretón},
  doi          = {10.1111/exsy.13263},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13263},
  shortjournal = {Expert Syst.},
  title        = {A novel intelligent approach for man-in-the-middle attacks detection over internet of things environments based on message queuing telemetry transport},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-parametric machine learning approach using
authentication trees for the healthcare industry. <em>EXSY</em>,
<em>41</em>(2), e13202. (<a
href="https://doi.org/10.1111/exsy.13202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Health Things (IoHT) has grown in importance for developing medical applications with the support of wireless communication systems. IoHT is integrated with many sensors to capture the patients&#39; records and transmits them to hospital centres for analysis and reporting. Controlling and managing health records has been addressed in several ways, however, it is noted that two key research problems for vital communication systems are reliability and reducing data loss. To enhance the sustainability of health applications and effectively use the network infrastructure when transferring sensitive data, this research provides a machine learning approach. Moreover, data collected from the IoHTs are protected and can be securely received for physical process in hospitals using authentication trees. Firstly, the undirected graphs are explored based on the multi-parametric machine learning approach to minimize the computation overheads and traffic congestion. Secondly, it evaluates the nodes&#39; level behaviour over the heterogeneous traffic load with efficient identification of redundant links. Finally, in-depth analysis and simulation results have shown that the proposed protocol is more effective than existing approaches for data accuracy and security analysis.},
  archive      = {J_EXSY},
  author       = {Ibrahim Abunadi and Amjad Rehman and Khalid Haseeb and Teg Alam and Gwanggil Jeon},
  doi          = {10.1111/exsy.13202},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13202},
  shortjournal = {Expert Syst.},
  title        = {A multi-parametric machine learning approach using authentication trees for the healthcare industry},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatiotemporal-distributed deep-learning framework for KPI
estimation of chemical processes with cascaded reactors. <em>EXSY</em>,
<em>41</em>(2), e13199. (<a
href="https://doi.org/10.1111/exsy.13199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online and accurate estimation of key performance indicators (KPI) is the foundation for operational optimization of a chemical process. However, a chemical process usually consists of multiple reactors, and the factors influencing KPI are spatially distributed in the long process flow. In addition, due to the distinct time lags between KPI and each reactor, temporal relationships among KPI and its influence factors are a mixture of short-term and long-term relationships. In this regard, a deep distributed KPI estimator with a self-attention mechanism is proposed in this paper. First, considering the process topology, a cascaded long short-term memory network is developed to simulate the process topology and capture the short-term effects. Then, to extract the long-term dependencies, a de-noise self-attention layer is employed to model interactions of all the influence factors explicitly and dynamically. Lastly, the proposed method is compared with typical state-of-the-art methods using real industrial data. The comparison results illustrate the performance and effectiveness of the proposed KPI estimation method.},
  archive      = {J_EXSY},
  author       = {Jiaxin Li and Yonggang Li and Zhenxiang Feng and Bei Sun and Shuang Long and Yanting Luo},
  doi          = {10.1111/exsy.13199},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13199},
  shortjournal = {Expert Syst.},
  title        = {A spatiotemporal-distributed deep-learning framework for KPI estimation of chemical processes with cascaded reactors},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Job offers recommender system based on virtual
organizations. <em>EXSY</em>, <em>41</em>(2), e13152. (<a
href="https://doi.org/10.1111/exsy.13152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human interaction has changed considerably with the emergence of the Internet. Today, a large percentage of daily communication takes place on instant messaging applications and social networks. In fact, there has been a considerable increase in the use of social networks because new social networks are being created for specific purposes, such as the search for employment or tourism. When the amount of content on a social network is large, it is necessary to help users find content of their interest. In this regard, artificial intelligence techniques can greatly facilitate the task of searching for relevant content. This paper presents a recommender system for a business and employment oriented social network, on which users are recommended job offers and other user profiles to follow. The presented system is based on virtual organizations of agents, and uses artificial neural networks to determine whether job offers and users should be recommended or not. The system has been evaluated on a real social network; its recommendations regarding job offers and user profiles have had a high acceptance rate.},
  archive      = {J_EXSY},
  author       = {Alfonso González-Briones and Pablo Chamoso and Juan Pavón and Fernando De La Prieta and Juan M. Corchado},
  doi          = {10.1111/exsy.13152},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13152},
  shortjournal = {Expert Syst.},
  title        = {Job offers recommender system based on virtual organizations},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based clustering of processes and their visual
exploration: An industry 4.0 use case for small, medium-sized
enterprises. <em>EXSY</em>, <em>41</em>(2), e13139. (<a
href="https://doi.org/10.1111/exsy.13139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a multi-stage approach consisting of deep learning-based image classification, process trace clustering, and visual/statistical knowledge discovery of process data. The proposed decision augmentation solution aims to facilitate the production planners in estimating the process-specific production parameters such as activity duration, idle time, or machine utilization. This study focuses on ‘one-of-a-kind production’ (OKP). Planning in OKP is especially challenging due to the increasing individualization of customer requirements. Furthermore, the uniqueness of products adds complexity to data and information structuring. To tackle this issue, we first train deep convolutional neural networks (CNN) with image data of production parts obtained from computer-aided design (CAD) systems to extract meaningful features. After cross-validation, uncertainty, and robustness assessment of the adopted deep learning approach, we use the data representation from the penultimate layer as input for clustering production parts. The goodness of clustering results is evaluated using a series of internal clustering validation indices. Finally, process event log data provided by manufacturing execution systems (MES) is mapped to each production part, allowing us to conduct statistical and visual knowledge discovery of process parameters for each cluster. The relevance of our proposed approach has been validated by studying a real-world use case in a small, medium-sized enterprise (SME) operating in the fixture and jig manufacturing industry.},
  archive      = {J_EXSY},
  author       = {Nijat Mehdiyev and Lea Mayer and Johannes Lahann and Peter Fettke},
  doi          = {10.1111/exsy.13139},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13139},
  shortjournal = {Expert Syst.},
  title        = {Deep learning-based clustering of processes and their visual exploration: An industry 4.0 use case for small, medium-sized enterprises},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predictive maintenance for offshore oil wells by means of
deep learning features extraction. <em>EXSY</em>, <em>41</em>(2),
e13128. (<a href="https://doi.org/10.1111/exsy.13128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the great diffusion of the Internet of Things and the improvements in Artificial Intelligence techniques have given a rise in the development and application of data-driven approaches for Predictive Maintenance to reduce the costs linked to the maintenance of industrial machinery. Due to the wide real-life applications and the strong interest by even more industries, this field is highly attractive for academics and practitioners. So, constructing efficient frameworks to address the Predictive Maintenance problem is an open debate. In this work, we propose a Deep Learning approach for the feature extraction in the offshore oil wells monitoring context, exploiting the public 3 W dataset, which is well-known in the literature. The dataset is made up of about 2000 multivariate time series labelled according to the corresponding functioning of the well. So, there is a classification task with eight classes, each related to a particular machinery condition. Thanks to the peculiarities of the labels, the proposed framework is valid both for diagnostics and prognostics. In more detail, we compare two different approaches in feature extraction. The first is a statistical approach, widely used in the literature related to the considered dataset; the second is based on Convolutional 1D AutoEncoder. The extracted features are then used as input for several Machine Learning algorithms, namely the Random Forest, Nearest Neighbours, Gaussian Naive Bayes and Quadratic Discriminant Analysis. Different experiments on various time horizons prove the worthiness of the Convolutional AutoEncoder.},
  archive      = {J_EXSY},
  author       = {Federico Gatta and Fabio Giampaolo and Diletta Chiaro and Francesco Piccialli},
  doi          = {10.1111/exsy.13128},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13128},
  shortjournal = {Expert Syst.},
  title        = {Predictive maintenance for offshore oil wells by means of deep learning features extraction},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A slimmer and deeper approach to deep network structures for
low-level vision tasks. <em>EXSY</em>, <em>41</em>(2), e13092. (<a
href="https://doi.org/10.1111/exsy.13092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep network design is a fundamental challenge. A right trade-off between depth and complexity of convolutional neural networks is of significant importance to applications in low-level vision tasks. Wider feature maps could be beneficial to performance and generality but would increase computational complexity. In this paper, we rethink the balance between width of the feature maps and depth of the network especially for image restoration tasks including deblurring, dehazing, super-resolution, and denoising. We explore a new approach to network structure by encouraging more depth to deal with restoration requirements while decreasing the width of some feature maps. Such a slimmer and deeper approach can enhance the performance while maintaining the same level of computational costs. We have experimentally evaluated the performances of the proposed approach on four image restoration tasks and obtained state-of-the-art results on quantitative measures and qualitative assessments, demonstrating the effectiveness of the approach.},
  archive      = {J_EXSY},
  author       = {Boyan Xu and Hujun Yin},
  doi          = {10.1111/exsy.13092},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13092},
  shortjournal = {Expert Syst.},
  title        = {A slimmer and deeper approach to deep network structures for low-level vision tasks},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic graph embedding-based anomaly detection on internet
of things time series. <em>EXSY</em>, <em>41</em>(2), e13083. (<a
href="https://doi.org/10.1111/exsy.13083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is critical in the internet of things (IoT) environment. To this issue, this study provides a novel approach for detecting anomalies in multivariate IoT time series. The proposed approach identified relationships between IoT time series to establish a dynamic graph and estimated the graph entropy to detect anomalies. The presented approach was applied to industrial IoT datasets. The results have shown that the presented method outperformed other models by 0.21 with respect to F1-score. In addition, we used three distinct algorithms to detect the anomalies from the multivariate IoT time series. According to the results, the local outlier factor approach outperformed the others by 0.18 with respect to F1-score.},
  archive      = {J_EXSY},
  author       = {Gen Li and Jason J. Jung},
  doi          = {10.1111/exsy.13083},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13083},
  shortjournal = {Expert Syst.},
  title        = {Dynamic graph embedding-based anomaly detection on internet of things time series},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining reinforcement learning and conventional control to
improve automatic guided vehicles tracking of complex trajectories.
<em>EXSY</em>, <em>41</em>(2), e13076. (<a
href="https://doi.org/10.1111/exsy.13076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of logistics transportation in the framework of Industry 4.0, automated guided vehicle (AGV) technologies have developed speedily. These systems present two coupled control problems: the control of the longitudinal velocity, essential to ensure the application requirements such as throughput and tag time, and the trajectory tracking control, necessary to ensure the proper accuracy in loading and unloading manoeuvres. When the paths are very short or have abrupt changes, the kinematic constraints play a restrictive role, and the tracking control becomes more challenging. In this case, advanced control strategies such as those based on intelligent techniques, including machine learning (ML) can be useful. Hence, in this work, we present an intelligent hybrid control scheme that combines reinforcement learning-based control (RLC) with conventional PI regulators to face both control problems simultaneously. On the one hand, PIs are used to control the speed of each wheel. On the other hand, the input reference of these regulators is calculated by the RLC in order to reduce the guiding error of the path tracking and to maintain the longitudinal speed. The latter is compared with a PID path following controller. The PID regulators have been tuned by genetic algorithms. The RLC allows the vehicle to learn how to improve the trajectory tracking in an adaptive way and thus, the AGV can face disturbances or unknown physical system parameters that may change due to friction and degradation of AGV mechanical components. Extensive simulation experiments of the proposed intelligent control strategy on a hybrid tricycle and differential AGV model, that considers the kinematics and the dynamics of the vehicle, prove the efficiency of the approach when following different demanding trajectories. The performance of the RL tracking controller in comparison with the optimized PID gives errors around 70% smaller, and the average maximum error is also 48% lower.},
  archive      = {J_EXSY},
  author       = {J. Enrique Sierra-Garcia and Matilde Santos},
  doi          = {10.1111/exsy.13076},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13076},
  shortjournal = {Expert Syst.},
  title        = {Combining reinforcement learning and conventional control to improve automatic guided vehicles tracking of complex trajectories},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel adaptive vehicle speed recommender fuzzy system for
autonomous vehicles on conventional two-lane roads. <em>EXSY</em>,
<em>41</em>(2), e13046. (<a
href="https://doi.org/10.1111/exsy.13046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an intelligent speed adaption system for vehicles on conventional roads. The fuzzy logic based expert system outputs a recommended speed to ensure both safety and passenger comfort. This intelligent system includes geometrical features of the road, as well as subjective perceptions of the drivers. It has been developed and checked with real data that were measured with an instrumental system incorporated in a vehicle, on several two-lane roads located in the Madrid Region, Spain. Along with the road geometrical characteristics, other input variables to the system are external factors, such as weather conditions, distance to the preceding vehicle, tire pressure, and other subjective criteria, such as the desired comfort level, selected by the driver. The expert system output is the most suitable speed for the specific road type, considering real factors that may modify the category of the road and thus, the appropriate speed. This information could be added to the adaptive cruise control of the vehicle. The recommended speed can be a very useful input for both, drivers and the autonomous vehicles, to improve safety on the road system.},
  archive      = {J_EXSY},
  author       = {Felipe Barreno and Matilde Santos and Manuel G. Romana},
  doi          = {10.1111/exsy.13046},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13046},
  shortjournal = {Expert Syst.},
  title        = {A novel adaptive vehicle speed recommender fuzzy system for autonomous vehicles on conventional two-lane roads},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel approach for the detection of anomalous energy
consumption patterns in industrial cyber-physical systems.
<em>EXSY</em>, <em>41</em>(2), e12959. (<a
href="https://doi.org/10.1111/exsy.12959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most scenarios emerging from the Industry 4.0 paradigm rely on the concept of cyber-physical production systems (CPPS), which allow them to synergistically connect physical to digital setups so as to integrate them over all stages of product development. Unfortunately, endowing CPPS with AI-based functionalities poses its own challenges: although advances in the performance of AI models keep blossoming in the community, their penetration in real-world industrial solutions has not so far developed at the same pace. Currently, 90% of AI-based models never reach production due to a manifold of assorted reasons not only related to complexity and performance: decisions issued by AI-based systems must be explained, understood and trusted by their end users. This study elaborates on a novel tool designed to characterize, in a non-supervised, human-understandable fashion, the nominal performance of a factory in terms of production and energy consumption. The traceability and analysis of energy consumption data traces and the monitoring of the factory&#39;s production permit to detect anomalies and inefficiencies in the working regime of the overall factory. By virtue of the transparency of the detection process, the proposed approach elicits understandable information about the root cause from the perspective of the production line, process and/or machine that generates the identified inefficiency. This methodology allows for the identification of the machines and/or processes that cause energy inefficiencies in the manufacturing system, and enables significant energy consumption savings by acting on these elements. We assess the performance of our designed method over a real-world case study from the automotive sector, comparing it to an extensive benchmark comprising state-of-the-art unsupervised and semi-supervised anomaly detection algorithms, from classical algorithms to modern generative neural counterparts. The superior quantitative results attained by our proposal complements its better interpretability with respect to the rest of algorithms in the comparison, which emphasizes the utmost relevance of considering the available domain knowledge and the target audience when design AI-based industrial solutions of practical value. Finally, the work described in this paper has been successfully deployed on a large scale in several industrial factories with significant international projection.},
  archive      = {J_EXSY},
  author       = {Izaskun Mendia and Sergio Gil-Lopez and Iñaki Grau and Javier Del Ser},
  doi          = {10.1111/exsy.12959},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e12959},
  shortjournal = {Expert Syst.},
  title        = {A novel approach for the detection of anomalous energy consumption patterns in industrial cyber-physical systems},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sentiment analysis from email pattern using feature
selection algorithm. <em>EXSY</em>, <em>41</em>(2), e12867. (<a
href="https://doi.org/10.1111/exsy.12867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today number of applications are available on mobile devices and computers for electronic mail (email) conversations. The demand for email communication is increasing day-by-day. Therefore the incoming and outgoing messages are also getting increased. However, extracting the sentiments from the emails is now demanding. Therefore in the proposed method, the pattern classification and sentiment clustering are carried out in two phases. Initially, the pattern classification is performed using support vector regression, then the sentiments from such classified patterns are clustered using a unsupervised fuzzy-model-based Gaussian clustering algorithm. Finally, the experimental analysis is performed in Python tool. The proposed sentiment clustering from email patterns has attained a better accuracy result of 97.13%, which is found higher than other existing techniques. Along with the parametric analysis, non-parametric statistical analysis using the Wilcoxon rank-sum test is also carried out to identify the proposed sentiment analysis architecture&#39;s effectiveness.},
  archive      = {J_EXSY},
  author       = {Ulligaddala Srinivasarao and Aakanksha Sharaff},
  doi          = {10.1111/exsy.12867},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e12867},
  shortjournal = {Expert Syst.},
  title        = {Sentiment analysis from email pattern using feature selection algorithm},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of detecting malware in android devices based on
machine learning techniques. <em>EXSY</em>, <em>41</em>(1), e13482. (<a
href="https://doi.org/10.1111/exsy.13482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware developers install malware on mobile users&#39; devices and steal their personal information without their knowledge. According to recent studies, it has been observed that malware developers are now targeting Android mobile devices. Researchers have examined the issues of detecting malware in these devices and proposed different methods and techniques. This study&#39;s main goal is to aid researchers in gaining a basic understanding of Android malware and its numerous detection methods. Earlier experiments that used machine learning to detect Android malware will be carefully reviewed in this paper. This in-depth review article thoroughly examines the origins, evolution, and sustainability of Android malware detection. It offers an in-depth literature review that includes the most recent approaches and research trends for detecting malware, from static analysis to dynamic analysis, machine learning, and deep learning. Additionally, we review current approaches&#39; shortcomings and difficulties and suggest possible paths for further investigation. The paper aims to stimulate further innovation in this essential field by providing researchers and practitioners with a comprehensive overview of the current status of Android malware detection.},
  archive      = {J_EXSY},
  author       = {Monika Sharma and Ajay Kaul},
  doi          = {10.1111/exsy.13482},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13482},
  shortjournal = {Expert Syst.},
  title        = {A review of detecting malware in android devices based on machine learning techniques},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A precise method of identifying android application family.
<em>EXSY</em>, <em>41</em>(1), e13481. (<a
href="https://doi.org/10.1111/exsy.13481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementing the necessary countermeasures to detect the growing and highly destructive family of malware is an urgent obligation. The proliferation and diversity of malware make these problems more challenging. For beginners, it is arduous to attain crucial features for multi-class family classification and extract valuable information from the obtained features. Another issue is that building a classification model that effectively absorbs multi-class samples and adapts to various features is challenging. This work indicates a precise identification method for Android application families (ANDF) to tackle these issues. It perceptively analyzes the features that multi-class families can utilize to identify members and further excavates the relationship between implicit information and the severity of those distinctions. A more appropriate classification model is developed for the heterogeneous file formats, and a more beneficial feature with a diverse array of heterogeneous information is chosen as the replacement representation of the sample. It is capable of upgrading learning ability and mastering the multi-modal traits of the family malware. The application of ANDF to real data sets yields effective classification results. It is capable of 0.9800 in f1-macro and has a classification accuracy of 98.61%. It performs, respectively, 0.0088 points better than the two-feature comparison classification model and 0.0872 points better than the single-feature comparison classification model. The kappa coefficient can also exceed 0.9830, which is at least 0.1044 higher than other contrasting classifiers and is 0.0105 greater than that of the contrasted model containing two features, which is 0.1046 larger than the classifier with a contrasting single feature.},
  archive      = {J_EXSY},
  author       = {Dan Li and Ning Lu and Siyu Wang and Wenbo Shi and Chang Choi},
  doi          = {10.1111/exsy.13481},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13481},
  shortjournal = {Expert Syst.},
  title        = {A precise method of identifying android application family},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Examining the two-dimensional perceived marketplace
influence and the role of financial incentives by SEM and ANN.
<em>EXSY</em>, <em>41</em>(1), e13480. (<a
href="https://doi.org/10.1111/exsy.13480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, research on sustainable consumption has been particularly relevant, highlighting the importance of the collective over the individual to reduce pollution. This study focuses on the study of the perceived marketplace influence (PMI) concept in its organizational and consumer dimensions, together with the financial incentives that exist in the adoption of electric cars and their effect on green customer engagement. A sample of 382 potential buyers of electric vehicles was obtained. A new hybrid analytical approach was taken structural equation modelling and artificial neural network. The research found the most significant variables affecting purchase intention were financial incentives, followed by PMI Organization and finally PMI Consumer. The results of artificial neural network analysis confirmed all the findings of the structural equation modelling, although the importance of each PMI dimension is different for each technique used. The conclusions point to new business opportunities that can be exploited by companies selling this green technology.},
  archive      = {J_EXSY},
  author       = {Elena Higueras-Castillo and Dineshwar Ramdhony and Zoran Kalinic and Francisco Liébana-Cabanillas},
  doi          = {10.1111/exsy.13480},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13480},
  shortjournal = {Expert Syst.},
  title        = {Examining the two-dimensional perceived marketplace influence and the role of financial incentives by SEM and ANN},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Android ransomware detection using binary jaya optimization
algorithm. <em>EXSY</em>, <em>41</em>(1), e13479. (<a
href="https://doi.org/10.1111/exsy.13479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ransomware is a serious security concern to mobile devices, as it prevents the use of the device and its contents until a ransom is paid, resulting in considerable financial losses for both people and corporations. The existing anti-malware measures have shown to be inadequate in combatting new malware variants that utilize advanced evasion strategies like Polymorphic, Metamorphic, Dynamic Code Loading, Time-based evasion, and Reflection. Furthermore, these primary defences have also suffered from low detection rates, significant false positives, high processing times, and excessive processing and power consumption that is inappropriate for smartphones. This paper offers the binary JAYA (BJAYA) for ransomware detection in Android mobile devices using the BJAYA optimization-based algorithm. The developed algorithm&#39;s effectiveness has been assessed against two datasets, the 0–1 knapsack, and real ransomware dataset. The proposed BJAYA method surpassed the other algorithms on 85% of the 0–1 knapsack datasets. The suggested BJAYA method was also tested on a ransomware dataset in two phases. In the first stage of testing, BJAYA outperformed other standard classifiers with sensitivity and Gmean values of 97% and 98.2%, respectively. In the second stage of testing, BJAYA outperformed other GA, FPA, and PSO metaheuristic algorithms in terms of specificity, sensitivity, and Gmean. These findings indicate the applicability of the suggested BJAYA algorithm for ransomware detection.},
  archive      = {J_EXSY},
  author       = {Moutaz Alazab},
  doi          = {10.1111/exsy.13479},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13479},
  shortjournal = {Expert Syst.},
  title        = {Android ransomware detection using binary jaya optimization algorithm},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning and blockchain-based intelligent and
secure vaccine recommender system. <em>EXSY</em>, <em>41</em>(1),
e13478. (<a href="https://doi.org/10.1111/exsy.13478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By combining blockchain technology (BT) and reinforcement learning (RL), the proposed work addresses the difficulties associated with vaccine recommendations. The need for individualized recommendations is obvious as vaccine schedules become more complicated and there are more vaccines available. The proposed work presents a novel approach that combines the adaptability of RL with the security, privacy, and transparency of BT. Layers for data processing, application, consensus, and smart contracts are included in the system architecture. It provides user-managed secure access, individualized vaccine recommendations, and decentralized data storage. The system aims to improve public health outcomes by using smart contracts to automate procedures and RL to improve recommendations. Using 10-fold cross-validation on the Vaccine Adverse Event Reporting System (VAERS) dataset, the experimental study verifies the performance of the system. The study focuses on metrics like accuracy, sensitivity, specificity, and F-measure when contrasting the proposed model with current solutions. The ability of proposed model to offer precise and well-informed vaccine recommendations is demonstrated by its consistent outperformance of competitors in accuracy and sensitivity.},
  archive      = {J_EXSY},
  author       = {M. Sreenu and Nitin Gupta and Chandrashekar Jatoth and Deepak Gupta},
  doi          = {10.1111/exsy.13478},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13478},
  shortjournal = {Expert Syst.},
  title        = {Reinforcement learning and blockchain-based intelligent and secure vaccine recommender system},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GA-based feature selection method for oversized data
analysis in digital economy. <em>EXSY</em>, <em>41</em>(1), e13477. (<a
href="https://doi.org/10.1111/exsy.13477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the promotion and development of oversized data technology, many data analysis platforms based on super large data storage and computing frameworks have emerged in the industry. While the platforms with oversized economic data analysis combined with machine learning models are still relatively lacking. And oversized data also brings a new problem, that is the security of economic development. It is an important and difficult task to analyse and detect risks from oversized economic data. Based on machine learning, data analysis, economic market and other multidisciplinary fields, this paper proposes a machine learning method, which is a genetic algorithm (GA) based feature selection method: FSGA. This method abstracts every possible feature selection result into an individual in GA, generates a population through genetic operation, and measures the merits of the individual through fitness. In addition, this paper has conducted multitudinous simulation experiments on the GA-based FSGA method and the traditional LSTM data analysis method respectively. The accuracy rate and other indicators are obtained by comparing the training. The experimental results show that the GA-based FSGA machine learning method has higher prediction accuracy when analysing oversized economic data. And it is practical to accelerate the development of digital economy.},
  archive      = {J_EXSY},
  author       = {Yao Lv and Peng Liu and Juan Wang and Yao Zhang and Adam Slowik and Jianhui Lv},
  doi          = {10.1111/exsy.13477},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13477},
  shortjournal = {Expert Syst.},
  title        = {GA-based feature selection method for oversized data analysis in digital economy},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fight detection with spatial and channel wise
attention-based ConvLSTM model. <em>EXSY</em>, <em>41</em>(1), e13474.
(<a href="https://doi.org/10.1111/exsy.13474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An automated detection of aggressive and violent behaviour in videos has immense potential. It enables efficient online content filtering by restricting access to extreme content and also, when integrated with security systems, helps to monitor violence in surveillance videos. In this work, a convolutional neural network is combined with the proposed Spatial and Channel wise Attention-based ConvLSTM encoder (SCan-ConvLSTM). The proposed architecture performs an efficient spatiotemporal fusion of the features extracted from the video sequences containing fight scenes. In order to focus selectively on regions of utmost importance, this blended attention mechanism adjusts the weights of outputs in different locations and across different channels. This recurrent attention mechanism enhances the sequential refinement of activation maps and boosts the model performance. Finally, the experimental results have been presented that show the proposed architecture achieves superior results on the benchmark datasets (RWF-2000, Violent-flow, Hockey-fights, and Movies).},
  archive      = {J_EXSY},
  author       = {Kunal Chaturvedi and Chhavi Dhiman and Dinesh Kumar Vishwakarma},
  doi          = {10.1111/exsy.13474},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13474},
  shortjournal = {Expert Syst.},
  title        = {Fight detection with spatial and channel wise attention-based ConvLSTM model},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 6G-enabled internet of medical things. <em>EXSY</em>,
<em>41</em>(1), e13472. (<a
href="https://doi.org/10.1111/exsy.13472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare is the foremost concern for any country. UN has also fixed ‘good health’ and well-being’ as its third sustainable development goal. CoVID-19 era has shown the vulnerability of health infrastructure worldwide. As per WHO, aging population of the world would put enormous pressure on health infrastructure in coming decades. With lack of healthcare professionals and limited budget to achieve UNSDG goal-3, IoMT with its huge umbrella of medical services and applications can address these issues. 5G has been deployed in many countries and vision for 6G have been finalized by different research groups. 6G will help Internet of Medical Things to realize its full potential. In this work, an open ‘6G enabled IoMT’ architecture has been presented. It can integrate a wide number of services. The limitations of 5G in catering to such a network are also highlighted. The challenges and open issues are presented in the light of services and applications that can be provided by the purposed architecture. A brief overview of 6G is also provided.},
  archive      = {J_EXSY},
  author       = {Sumit Singh Dhanda and Brahmjit Singh and Poonam Jindal and Tarun Kumar Sharma and Deepak Panwar},
  doi          = {10.1111/exsy.13472},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13472},
  shortjournal = {Expert Syst.},
  title        = {6G-enabled internet of medical things},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Soft computing approaches for prediction of specific heat
capacity of hybrid nanofluids. <em>EXSY</em>, <em>41</em>(1), e13471.
(<a href="https://doi.org/10.1111/exsy.13471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nanofluids are thermoelectric substance with a greater thermal transmission effectiveness than traditional thermal fluids. Nanofluid&#39;s specific heat capacity (SHC) is a crucial thermophysical parameter since it controls the fluid&#39;s heat transfer coefficient. Surprisingly few research investigations have been done on the prognostic modelling of the specific heat capacity of fusion nanofluids, despite the fact that numerous experiments have been conducted on the heat transfer capacitance of blended nanofluids. This study focuses on the use of algorithms based on machine learning procedures (MLP) to estimate the SHC of blended nanofluids. Numerous numbers of hybrid nanofluids are investigated in this investigation. Total of 984 samples of hybrid nanofluids for specific heat capacity has been collected from nine experimental research papers. Various MLP techniques were utilized in this analysis, including extreme boost gradient boost regression (XGB), support vector regression augmented with a genetic algorithm (support vector regression [SVR]-genetic algorithm [GA]), grid search optimized based gradient boost regression algorithm (GBR) (GBR-GSO), and voting ensemble procedure (VE). The obtained correlation coefficients of the SVR-GA, XGB, VE and GBR-GSO models for the testing dataset are 98.43%, 96.29%, 94.4% and 96.55% respectively. The SVR-GA model showed a better predictive accuracy relative to other ML models. This SVR-GA anticipated model could be used for quick and reliable prediction of the SHC of blended nanofluids which reduces the burden associated with experimental measurement possible future work includes applying a machine-learning strategy to the problem of determining the diffusivity of hybrid nanofluids.},
  archive      = {J_EXSY},
  author       = {Priya Mathur and Amit Kumar Gupta and Deepak Panwar and Tarun Kumar Sharma},
  doi          = {10.1111/exsy.13471},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13471},
  shortjournal = {Expert Syst.},
  title        = {Soft computing approaches for prediction of specific heat capacity of hybrid nanofluids},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing complex networks: Extracting key characteristics
and measuring structural similarities. <em>EXSY</em>, <em>41</em>(1),
e13470. (<a href="https://doi.org/10.1111/exsy.13470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses the importance of feature extraction and structure similarity measurement in the analysis of complex networks. Social networks, biological systems, and transportation networks are just a few examples of the many phenomena that have been modeled using complex networks. However, analyzing these networks can be challenging due to their large size and complexity. Feature extraction techniques can help to simplify the network by identifying key nodes or substructures. Structure similarity measurement techniques can be used to compare different networks and identify similarities and differences between them. Previous research has suggested that real-world complex networks are influenced by multiplex features and either local or global features. However, the interaction between these characteristics is not well understood. The proposed approach outperforms other graph similarity methods on publicly available datasets, with accurate estimations of overall complex network structures. Specifically, the approach based on cosine similarity outperforms as compared to existing methods. Overall, this study highlights the importance of considering various graph features–local and global features and their interactions in the analysis of complex networks.},
  archive      = {J_EXSY},
  author       = {Haji Gul and Feras Al-Obeidat and Adnan Amin and Fernando Moreira},
  doi          = {10.1111/exsy.13470},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13470},
  shortjournal = {Expert Syst.},
  title        = {Analyzing complex networks: Extracting key characteristics and measuring structural similarities},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning and internet of things applications in
enterprise architectures: Solutions, challenges, and open issues.
<em>EXSY</em>, <em>41</em>(1), e13467. (<a
href="https://doi.org/10.1111/exsy.13467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of the Internet of Things (IoT) has led to its widespread adoption in various industries, enabling enhanced productivity and efficient services. Integrating IoT systems with existing enterprise application systems has become common practice. However, this integration necessitates reevaluating and reworking current Enterprise Architecture (EA) models and Expert Systems (ES) to accommodate IoT and cloud technologies. Enterprises must adopt a multifaceted view and automate various aspects, including operations, data management, and technology infrastructure. Machine Learning (ML) is a powerful IoT and smart automation tool within EA. Despite its potential, a need for dedicated work focuses on ML applications for IoT services and systems. With IoT being a significant field, analyzing IoT-generated data and IoT-based networks is crucial. Many studies have explored how ML can solve specific IoT-related challenges. These mutually reinforcing technologies allow IoT applications to leverage sensor data for ML model improvement, leading to enhanced IoT operations and practices. Furthermore, ML techniques empower IoT systems with knowledge and enable suspicious activity detection in smart systems and objects. This survey paper conducts a comprehensive study on the role of ML in IoT applications, particularly in the domains of automation and security. It provides an in-depth analysis of the state-of-the-art ML approaches within the context of IoT, highlighting their contributions, challenges, and potential applications.},
  archive      = {J_EXSY},
  author       = {Zubaida Rehman and Noshina Tariq and Syed Atif Moqurrab and Joon Yoo and Gautam Srivastava},
  doi          = {10.1111/exsy.13467},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13467},
  shortjournal = {Expert Syst.},
  title        = {Machine learning and internet of things applications in enterprise architectures: Solutions, challenges, and open issues},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDbQfSum: Query-focused summarization framework based on
diversity and text semantic analysis. <em>EXSY</em>, <em>41</em>(1),
e13462. (<a href="https://doi.org/10.1111/exsy.13462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query-focused multi-document summarization (Qf-MDS) is a sub-task of automatic text summarization that aims to extract a substitute summary from a document cluster of the same topic and based on a user query. Unlike other summarization tasks, Qf-MDS has specific research challenges including the differences and similarities across related document sets, the high degree of redundancy inherent in the summaries created from multiple related sources, relevance to the given query, topic diversity in the produced summary and the small source-to-summary compression ratio. In this work, we propose a semantic diversity feature based query-focused extractive summarizer (SDbQfSum) built on powerful text semantic representation techniques underpinned with Wikipedia commonsense knowledge in order to address the query-relevance, centrality, redundancy and diversity challenges. Specifically, a semantically parsed document text is combined with knowledge-based vectorial representation to extract effective sentence importance and query-relevance features. The proposed monolingual summarizer is evaluated on a standard English dataset for automatic query-focused summarization tasks, that is, the DUC2006 dataset. The obtained results show that our summarizer outperforms most state-of-the-art related approaches on one or more ROUGE measures achieving 0.418, 0.092 and 0.152 in ROUGE-1, ROUGE-2, and ROUGE-SU4 respectively. It also attains competitive performance with the slightly outperforming system(s), for example, the difference between our system&#39;s result and best system in ROUGE-1 is just 0.006. We also found through the conducted experiments that our proposed custom cluster merging algorithm significantly reduces information redundancy while maintaining topic diversity across documents.},
  archive      = {J_EXSY},
  author       = {Muhidin Mohamed and Mourad Oussalah and Victor Chang},
  doi          = {10.1111/exsy.13462},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13462},
  shortjournal = {Expert Syst.},
  title        = {SDbQfSum: Query-focused summarization framework based on diversity and text semantic analysis},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CogNLG: Cognitive graph for KG-to-text generation.
<em>EXSY</em>, <em>41</em>(1), e13461. (<a
href="https://doi.org/10.1111/exsy.13461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) has been fully considered in natural language generation (NLG) tasks. A KG can help models generate controllable text and achieve better performance. However, most existing related approaches still lack explainability and scalability in large-scale knowledge reasoning. In this work, we propose a novel CogNLG framework for KG-to-text generation tasks. Our CogNLG is implemented based on the dual-process theory in cognitive science. It consists of two systems: one system acts as the analytic system for knowledge extraction, and another is the perceptual system for text generation by using existing knowledge. During text generation, CogNLG provides a visible and explainable reasoning path. Our framework shows excellent performance on all datasets and achieves a BLEU score of 36.7, which increases by 6.7 compared to the best competitor.},
  archive      = {J_EXSY},
  author       = {Peichao Lai and Feiyang Ye and Yanggeng Fu and Zhiwei Chen and Yingjie Wu and Yilei Wang and Victor Chang},
  doi          = {10.1111/exsy.13461},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13461},
  shortjournal = {Expert Syst.},
  title        = {CogNLG: Cognitive graph for KG-to-text generation},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of US 30-years-treasury-bonds movement and
trading entry point using the robust 1DCNN-BiLSTM-XGBoost algorithm.
<em>EXSY</em>, <em>41</em>(1), e13459. (<a
href="https://doi.org/10.1111/exsy.13459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel algorithm that accurately predicts market trends and identifies trading entry points for US 30-year Treasury bonds. The proposed method employs a hybrid approach, integrating a 1-dimensional convolutional neural network (1DCNN), long-short term memory (LSTM), and XGBoost algorithms. The 1DCNN is used to learn local and short-term patterns, while LSTM is employed to capture both short and long-term dependencies. Furthermore, we have implemented an algorithm that utilizes hull moving average (HMA) and simple moving average (SMA) crossover data to detect trading entry points and major trends in the market. The combination of the SMA–HMA crossover algorithm and predictions provided by the 1DCNN-BiLSTM-XGBoost algorithm yields exceptional results in terms of prediction accuracy and profitability. Additionally, these integrated techniques effectively filter out noise and mitigate false breakouts, which are often observed with US 30-year Treasury bonds. In the field of financial time series prediction, the effectiveness of 1DCNN and LSTM in identifying trading entry points and market perturbations has not been comprehensively studied. Therefore, our work fills this gap by demonstrating through experiments that the proposed 1DCNN-BiLSTM-XGBoost algorithm, in combination with moving average crossovers, effectively reduces noise and market perturbations. This leads to the precise identification of trading entry points and accurate recognition of trend signals for US 30-year Treasury bonds. We demonstrate through experiments that our proposed approach achieves an average root mean squared error of 0.0001 and an R-square value of 0.9999, highlighting its promise as a method for predicting market trends and trading entry points for US 30-year Treasury bonds.},
  archive      = {J_EXSY},
  author       = {Abdellah El Zaar and Nabil Benaya and Toufik Bakir and Amine Mansouri and Abderrahim El Allati},
  doi          = {10.1111/exsy.13459},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13459},
  shortjournal = {Expert Syst.},
  title        = {Prediction of US 30-years-treasury-bonds movement and trading entry point using the robust 1DCNN-BiLSTM-XGBoost algorithm},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved binary grey wolf optimizer for constrained
engineering design problems. <em>EXSY</em>, <em>41</em>(1), e13458. (<a
href="https://doi.org/10.1111/exsy.13458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An Improved binary Non-Linear Convergent Bi-phase Mutated Grey Wolf Optimizer (IbGWO) is proposed for solving feature selection problems with two main goals reducing irrelevant features and maximizing accuracy. We used stratified k -fold cross-validation that performs stratified sampling on the data to avoid overfitting problems. The fitness function used in the proposed algorithm allows choosing the solution with the minimum number of features if more than one feature has the same highest accuracy. When stratified cross-validation is performed, the split datasets contain the same share of the feature of interest as the actual dataset. During stratified sampling, the cross-validation result minimizes the generalization error to a considerable extent, with a smaller variance. Feature selection could be seen as an optimization problem that efficiently removes irrelevant data from high-dimensional data to reduce computation time and improve learning accuracy. This paper proposes an improved Non-Linear Convergent Bi-Phase Mutated Binary Grey Wolf Optimizer (IbGWO) algorithm for feature selection. The bi-phase mutation enhances the rate of exploitation of GWO, where the first mutation phase minimizes the number of features and the second phase adds more informative features for accurate feature selection. A non-linear tangent trigonometric function is used for convergence to generalize better while handling heterogeneous data. To accelerate the global convergence speed, an inertia weight is added to control the position updating of the grey wolves. Feature-weighted K-Nearest Neighbor is used to enhance classification accuracy, where only relevant features are used for feature selection. Experimental results confirm that IbGWO outperforms other algorithms in terms of average accuracy of 0.8716, average number of chosen features of 6.13, average fitness of 0.1717, and average standard deviation of 0.0072 tested on different datasets and in terms of statistical analysis. IbGWO is also benchmarked using unimodal, multimodal, and IEEE CEC 2019 functions, where it outperforms other algorithms in most cases. Three classical engineering design problems are also solved using IbGWO, which significantly outperforms other algorithms. Moreover, the overtaking percentage of the proposed algorithm is .},
  archive      = {J_EXSY},
  author       = {Parijata Majumdar and Diptendu Bhattacharya and Sanjoy Mitra and Leonardo Ramos Rodrigues and Diego Oliva},
  doi          = {10.1111/exsy.13458},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13458},
  shortjournal = {Expert Syst.},
  title        = {An improved binary grey wolf optimizer for constrained engineering design problems},
  volume       = {41},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
