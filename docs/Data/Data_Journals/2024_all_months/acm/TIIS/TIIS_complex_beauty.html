<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tiis---30">TIIS - 30</h2>
<ul>
<li><details>
<summary>
(2024). Insights into natural language database query errors: From
attention misalignment to user handling strategies. <em>TIIS</em>,
<em>14</em>(4), 1–32. (<a
href="https://doi.org/10.1145/3650114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Querying structured databases with natural language (NL2SQL) has remained a difficult problem for years. Recently, the advancement of machine learning (ML), natural language processing (NLP), and large language models (LLM) have led to significant improvements in performance, with the best model achieving ∼85% percent accuracy on the benchmark Spider dataset. However, there is a lack of a systematic understanding of the types, causes, and effectiveness of error-handling mechanisms of errors for erroneous queries nowadays. To bridge the gap, a taxonomy of errors made by four representative NL2SQL models was built in this work, along with an in-depth analysis of the errors. Second, the causes of model errors were explored by analyzing the model-human attention alignment to the natural language query. Last, a within-subjects user study with 26 participants was conducted to investigate the effectiveness of three interactive error-handling mechanisms in NL2SQL. Findings from this article shed light on the design of model structure and error discovery and repair strategies for natural language data query interfaces in the future.},
  archive      = {J_TIIS},
  doi          = {10.1145/3650114},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-32},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Insights into natural language database query errors: From attention misalignment to user handling strategies},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating scientific paper skimming with augmented
intelligence through customizable faceted highlights. <em>TIIS</em>,
<em>14</em>(4), 1–30. (<a
href="https://doi.org/10.1145/3665648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim , a novel intelligent interface that helps scholars skim papers to rapidly review and gain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient content within a paper, directing a scholar’s attention. These automatically-extracted highlights are faceted by content type, evenly-distributed across a paper, and have a density configurable by scholars. We evaluate Scim with an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. Finally, we describe the process of scaling highlights from their conception within Scim , a research prototype, to production on over 521,000 papers within the Semantic Reader, a publicly-available augmented reading interface for scientific papers. We conclude by discussing design considerations and tensions for the design of future skimming tools with augmented intelligence.},
  archive      = {J_TIIS},
  doi          = {10.1145/3665648},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-30},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Accelerating scientific paper skimming with augmented intelligence through customizable faceted highlights},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoRL x: Automated reinforcement learning on the web.
<em>TIIS</em>, <em>14</em>(4), 1–30. (<a
href="https://doi.org/10.1145/3670692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) is crucial in decision optimization, but its inherent complexity often presents challenges in interpretation and communication. Building upon AutoDOViz—an interface that pushed the boundaries of Automated RL for Decision Optimization—this article unveils an open-source expansion with a web-based platform for RL. Our work introduces a taxonomy of RL visualizations and launches a dynamic web platform, leveraging backend flexibility for AutoRL frameworks like ARLO and Svelte.js for a smooth interactive user experience in the front end. Since AutoDOViz is not open-source, we present AutoRL X, a new interface designed to visualize RL processes. AutoRL X is shaped by the extensive user feedback and expert interviews from AutoDOViz studies, and it brings forth an intelligent interface with real-time, intuitive visualization capabilities that enhance understanding, collaborative efforts, and personalization of RL agents. Addressing the gap in accurately representing complex real-world challenges within standard RL environments, we demonstrate our tool’s application in healthcare, explicitly optimizing brain stimulation trajectories. A user study contrasts the performance of human users optimizing electric fields via a 2D interface with RL agents’ behavior that we visually analyze in AutoRL X, assessing the practicality of automated RL. All our data and code is openly available at: https://github.com/lorifranke/autorlx .},
  archive      = {J_TIIS},
  doi          = {10.1145/3670692},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-30},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {AutoRL x: Automated reinforcement learning on the web},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Categorical and continuous features in counterfactual
explanations of AI systems. <em>TIIS</em>, <em>14</em>(4), 1–37. (<a
href="https://doi.org/10.1145/3673907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, eXplainable AI (XAI) research has focused on the use of counterfactual explanations to address interpretability, algorithmic recourse, and bias in AI system decision-making. The developers of these algorithms claim they meet user requirements in generating counterfactual explanations with “plausible,” “actionable” or “causally important” features. However, few of these claims have been tested in controlled psychological studies. Hence, we know very little about which aspects of counterfactual explanations really help users understand the decisions of AI systems. Nor do we know whether counterfactual explanations are an advance on more traditional causal explanations that have a longer history in AI (e.g., in expert systems). Accordingly, we carried out three user studies to (1) test a fundamental distinction in feature-types, between categorical and continuous features, and (2) compare the relative effectiveness of counterfactual and causal explanations. The studies used a simulated, automated decision-making app that determined safe driving limits after drinking alcohol, based on predicted blood alcohol content, where users’ responses were measured objectively (using predictive accuracy) and subjectively (using satisfaction and trust judgments). Study 1 ( N \({=}\) 127) showed that users understand explanations referring to categorical features more readily than those referring to continuous features. It also discovered a dissociation between objective and subjective measures: counterfactual explanations elicited higher accuracy than no-explanation controls but elicited no more accuracy than causal explanations, yet counterfactual explanations elicited greater satisfaction and trust than causal explanations. In Study 2 ( N \({=}\) 136) we transformed the continuous features of presented items to be categorical (i.e., binary) and found that these converted features led to highly accurate responding. Study 3 ( N \({=}\) 211) explicitly compared matched items involving either mixed features (i.e., a mix of categorical and continuous features) or categorical features (i.e., categorical and categorically-transformed continuous features), and found that users were more accurate when categorically-transformed features were used instead of continuous ones. It also replicated the dissociation between objective and subjective effects of explanations. The findings delineate important boundary conditions for current and future counterfactual explanation methods in XAI.},
  archive      = {J_TIIS},
  doi          = {10.1145/3673907},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-37},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Categorical and continuous features in counterfactual explanations of AI systems},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding trust and reliance development in AI advice:
Assessing model accuracy, model explanations, and experiences from
previous interactions. <em>TIIS</em>, <em>14</em>(4), 1–30. (<a
href="https://doi.org/10.1145/3686164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People are increasingly interacting with AI systems, but successful interactions depend on people trusting these systems only when appropriate. Since neither gaining trust in AI advice nor restoring lost trust after AI mistakes is warranted, we seek to better understand the development of trust and reliance in sequential human-AI interaction scenarios. In a 2 \({\times}\) 2 between-subject simulated AI experiment, we tested how model accuracy (high vs. low) and explanation type (human-like vs. abstract) affect trust and reliance on AI advice for repeated interactions. In the experiment, participants estimated jail times for 20 criminal law cases, first without and then with AI advice. Our results show that trust and reliance are significantly higher for high model accuracy. In addition, reliance does not decline over the trial sequence, and trust increases significantly with high accuracy. Human-like (vs. abstract) explanations only increased reliance on the high-accuracy condition. We furthermore tested the extent to which trust and reliance in a trial round can be explained by trust and reliance experiences from prior rounds. We find that trust assessments in prior trials correlate with trust in subsequent ones. We also find that the cumulative trust experience of a person in all earlier trial rounds correlates with trust in subsequent ones. Furthermore, we find that the two trust measures, trust and reliance, impact each other: prior trust beliefs not only influence subsequent trust beliefs but likewise influence subsequent reliance behavior, and vice versa. Executing a replication study yielded comparable results to our original study, thereby enhancing the validity of our findings.},
  archive      = {J_TIIS},
  doi          = {10.1145/3686164},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-30},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Understanding trust and reliance development in AI advice: Assessing model accuracy, model explanations, and experiences from previous interactions},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Developing an AI-based explainable expert support system for
art therapy. <em>TIIS</em>, <em>14</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3689649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch-based drawing assessments in art therapy are widely used to understand individuals’ cognitive and psychological states, such as cognitive impairments or mental disorders. Along with self-reported measures based on questionnaires, psychological drawing assessments can augment information regarding an individual’s psychological state. Interpreting drawing assessments demands significant time and effort, particularly for large groups such as schools or companies, and relies on the expertise of art therapists. To address this issue, we propose an artificial intelligence (AI)-based expert support system called AlphaDAPR to support art therapists and psychologists in conducting large-scale automatic drawing assessments. In Study 1, we first investigated user experience in AlphaDAPR . Through surveys involving 64 art therapists, we observed a substantial willingness (64.06% of participants) in using the proposed system. Structural equation modeling highlighted the pivotal role of explainable AI in the interface design, affecting perceived usefulness, trust, satisfaction, and intention to use. However, our interviews unveiled a nuanced perspective: while many art therapists showed a strong inclination to use the proposed system, they also voiced concerns about potential AI limitations and risks. Since most concerns arose from insufficient trust, which was the focal point of our attention, we conducted Study 2 with the aim of enhancing trust. Study 2 delved deeper into the necessity of clear communication regarding the division of roles between AI and users for elevating trust. Through experimentation with another 26 art therapists, we demonstrated that clear communication enhances users’ trust in our system. Our work not only highlights the potential of AlphaDAPR to streamline drawing assessments but also underscores broader implications for human-AI collaboration in psychological domains. By addressing concerns and optimizing communication, we pave the way for a symbiotic relationship between AI and human expertise, ultimately enhancing the efficacy and accessibility of psychological assessment tools.},
  archive      = {J_TIIS},
  doi          = {10.1145/3689649},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Developing an AI-based explainable expert support system for art therapy},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring user experience inclusivity in human-AI
interaction via five user problem-solving styles. <em>TIIS</em>,
<em>14</em>(3), 1–90. (<a
href="https://doi.org/10.1145/3663740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivations : Recent research has emerged on generally how to improve AI products’ human-AI interaction (HAI) user experience (UX), but relatively little is known about HAI-UX inclusivity. For example, what kinds of users are supported, and who are left out? What product changes would make it more inclusive? Objectives : To help fill this gap, we present an approach to measuring what kinds of diverse users an AI product leaves out and how to act upon that knowledge. To bring actionability to the results, the approach focuses on users’ problem-solving diversity. Thus, our specific objectives were (1) to show how the measure can reveal which participants with diverse problem-solving styles were left behind in a set of AI products and (2) to relate participants’ problem-solving diversity to their demographic diversity, specifically gender and age. Methods : We performed 18 experiments, discarding two that failed manipulation checks. Each experiment was a 2 \(\times\) 2 factorial experiment with online participants, comparing two AI products: one deliberately violating 1 of 18 HAI guidelines and the other applying the same guideline. For our first objective, we used our measure to analyze how much each AI product gained/lost HAI-UX inclusivity compared to its counterpart, where inclusivity meant supportiveness to participants with particular problem-solving styles. For our second objective, we analyzed how participants’ problem-solving styles aligned with their gender identities and ages. Results and Implications : Participants’ diverse problem-solving styles revealed six types of inclusivity results: (1) the AI products that followed an HAI guideline were almost always more inclusive across diversity of problem-solving styles than the products that did not follow that guideline—but “who” got most of the inclusivity varied widely by guideline and by problem-solving style; (2) when an AI product had risk implications, four variables’ values varied in tandem: participants’ feelings of control, their (lack of) suspicion, their trust in the product, and their certainty while using the product; (3) the more control an AI product offered users, the more inclusive it was; (4) whether an AI product was learning from “my” data or other people’s affected how inclusive that product was; (5) participants’ problem-solving styles skewed differently by gender and age group; and (6) almost all of the results suggested actions that HAI practitioners could take to improve their products’ inclusivity further. Together, these results suggest that a key to improving the demographic inclusivity of an AI product (e.g., across a wide range of genders, ages) can often be obtained by improving the product’s support of diverse problem-solving styles.},
  archive      = {J_TIIS},
  doi          = {10.1145/3663740},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-90},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Measuring user experience inclusivity in human-AI interaction via five user problem-solving styles},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). “I want it that way”: Enabling interactive decision support
using large language models and constraint programming. <em>TIIS</em>,
<em>14</em>(3), 1–33. (<a
href="https://doi.org/10.1145/3685053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical factor in the success of many decision support systems is the accurate modeling of user preferences. Psychology research has demonstrated that users often develop their preferences during the elicitation process, highlighting the pivotal role of system-user interaction in developing personalized systems. This paper introduces a novel approach, combining Large Language Models (LLMs) with Constraint Programming to facilitate interactive decision support. We study this hybrid framework through the lens of meeting scheduling, a time-consuming daily activity faced by a multitude of information workers. We conduct three studies to evaluate the novel framework, including a diary study to characterize contextual scheduling preferences, a quantitative evaluation of the system’s performance, and a user study to elicit insights with a technology probe that encapsulates our framework. Our work highlights the potential for a hybrid LLM and optimization approach for iterative preference elicitation, and suggests design considerations for building systems that support human-system collaborative decision-making processes.},
  archive      = {J_TIIS},
  doi          = {10.1145/3685053},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-33},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {“I want it that way”: Enabling interactive decision support using large language models and constraint programming},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the effects of self-correction behavior of an
intelligent virtual character during a jigsaw puzzle co-solving task.
<em>TIIS</em>, <em>14</em>(3), 1–33. (<a
href="https://doi.org/10.1145/3688006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although researchers have explored how humans perceive the intelligence of virtual characters, few studies have focused on the ability of intelligent virtual characters to fix their mistakes. Thus, we explored the self-correction behavior of a virtual character with different intelligence capabilities in a within-group design ( \(N=23\) ) study. For this study, we developed a virtual character that can solve a jigsaw puzzle whose self-correction behavior is controlled by two parameters, namely, Intelligence and Accuracy of Self-correction . Then, we integrated the virtual character into our virtual reality experience and asked participants to co-solve a jigsaw puzzle. During the study, our participants were exposed to five experimental conditions resulting from combinations of the Intelligence and Accuracy of Self-correction parameters. In each condition, we asked our participants to respond to a survey examining their perceptions of the virtual character’s intelligence and awareness (private, public, and surroundings awareness) and user experiences, including trust, enjoyment, performance, frustration, and desire for future interaction. We also collected application logs, including participants’ dwell gaze data, completion times, and the number of puzzle pieces they placed to co-solve the jigsaw puzzle. The results of all the survey ratings and the completion time were statistically significant. Our results indicated that higher levels of Intelligence and Accuracy of Self-correction enhanced not only our participants’ perceptions of the virtual character’s intelligence, awareness (private, public, and surroundings), trustworthiness, and performance but also increased their enjoyment and desire for future interaction with the virtual character while reducing their frustration and completion time. Moreover, we found that as the Intelligence and Accuracy of Self-correction increased, participants had to place fewer puzzle pieces and needed less time to complete the jigsaw puzzle. Finally, regardless of the experimental condition to which we exposed our participants, they gazed at the virtual character for more time compared to the puzzle pieces and puzzle goal in the virtual environment.},
  archive      = {J_TIIS},
  doi          = {10.1145/3688006},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-33},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Exploring the effects of self-correction behavior of an intelligent virtual character during a jigsaw puzzle co-solving task},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2023 TiiS best paper announcement. <em>TIIS</em>,
<em>14</em>(3), 1. (<a href="https://doi.org/10.1145/3690000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIIS},
  doi          = {10.1145/3690000},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {2023 TiiS best paper announcement},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactions for socially shared regulation in collaborative
learning: An interdisciplinary multimodal dataset. <em>TIIS</em>,
<em>14</em>(3), 1–34. (<a
href="https://doi.org/10.1145/3658376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Socially shared regulation plays a pivotal role in the success of collaborative learning. However, evaluating socially shared regulation of learning (SSRL) proves challenging due to the dynamic and infrequent cognitive and socio-emotional interactions, which constitute the focal point of SSRL. To address this challenge, this article gathers interdisciplinary researchers to establish a multimodal dataset with cognitive and socio-emotional interactions for SSRL study. Firstly, to induce cognitive and socio-emotional interactions, learning science researchers designed a special collaborative learning task with regulatory trigger events among triadic people for the SSRL study. Secondly, this dataset includes various modalities like video, Kinect data, audio, and physiological data (accelerometer, EDA, heart rate) from 81 high school students in 28 groups, offering a comprehensive view of the SSRL process. Thirdly, three-level verbal interaction annotations and nonverbal interactions including facial expression, eye gaze, gesture, and posture are provided, which could further contribute to interdisciplinary fields such as computer science, sociology, and education. In addition, comprehensive analysis verifies the dataset’s effectiveness. As far as we know, this is the first multimodal dataset for studying SSRL among triadic group members.},
  archive      = {J_TIIS},
  doi          = {10.1145/3658376},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3},
  pages        = {1-34},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Interactions for socially shared regulation in collaborative learning: An interdisciplinary multimodal dataset},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unpacking human-AI interactions: From interaction primitives
to a design space. <em>TIIS</em>, <em>14</em>(3), 1–51. (<a
href="https://doi.org/10.1145/3664522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to develop a semi-formal representation for Human-AI (HAI) interactions, by building a set of interaction primitives which can specify the information exchanges between users and AI systems during their interaction. We show how these primitives can be combined into a set of interaction patterns which can capture common interactions between humans and AI/ML models. The motivation behind this is twofold: firstly, to provide a compact generalization of existing practices for the design and implementation of HAI interactions; and secondly, to support the creation of new interactions by extending the design space of HAI interactions. Taking into consideration frameworks, guidelines, and taxonomies related to human-centered design and implementation of AI systems, we define a vocabulary for describing information exchanges based on the model’s characteristics and interactional capabilities. Based on this vocabulary, a message passing model for interactions between humans and models is presented, which we demonstrate can account for existing HAI interaction systems and approaches. Finally, we build this into design patterns which can describe common interactions between users and models, and we discuss how this approach can be used toward a design space for HAI interactions that creates new possibilities for designs as well as keeping track of implementation issues and concerns.},
  archive      = {J_TIIS},
  doi          = {10.1145/3664522},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3},
  pages        = {1-51},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Unpacking human-AI interactions: From interaction primitives to a design space},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reassuring, misleading, debunking: Comparing effects of XAI
methods on human decisions. <em>TIIS</em>, <em>14</em>(3), 1–36. (<a
href="https://doi.org/10.1145/3665647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust calibration is essential in AI-assisted decision-making. If human users understand the rationale on which an AI model has made a prediction, they can decide whether they consider this prediction reasonable. Especially in high-risk tasks such as mushroom hunting (where a wrong decision may be fatal), it is important that users make correct choices to trust or overrule the AI. Various explainable AI (XAI) methods are currently being discussed as potentially useful for facilitating understanding and subsequently calibrating user trust. So far, however, it remains unclear which approaches are most effective. In this article, the effects of XAI methods on human AI-assisted decision-making in the high-risk task of mushroom picking were tested. For that endeavor, the effects of (i) Grad-CAM attributions, (ii) nearest-neighbor examples, and (iii) network-dissection concepts were compared in a between-subjects experiment with \(N=501\) participants representing end-users of the system. In general, nearest-neighbor examples improved decision correctness the most. However, varying effects for different task items became apparent. All explanations seemed to be particularly effective when they revealed reasons to (i) doubt a specific AI classification when the AI was wrong and (ii) trust a specific AI classification when the AI was correct. Our results suggest that well-established methods, such as Grad-CAM attribution maps, might not be as beneficial to end users as expected and that XAI techniques for use in real-world scenarios must be chosen carefully.},
  archive      = {J_TIIS},
  doi          = {10.1145/3665647},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3},
  pages        = {1-36},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Reassuring, misleading, debunking: Comparing effects of XAI methods on human decisions},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reasoning and value alignment test to assess advanced GPT
reasoning. <em>TIIS</em>, <em>14</em>(3), 1–37. (<a
href="https://doi.org/10.1145/3670691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to diverse perspectives on artificial general intelligence (AGI), ranging from potential safety and ethical concerns to more extreme views about the threats it poses to humanity, this research presents a generic method to gauge the reasoning capabilities of artificial intelligence (AI) models as a foundational step in evaluating safety measures. Recognizing that AI reasoning measures cannot be wholly automated, due to factors such as cultural complexity, we conducted an extensive examination of five commercial generative pre-trained transformers (GPTs), focusing on their comprehension and interpretation of culturally intricate contexts. Utilizing our novel “Reasoning and Value Alignment Test,” we assessed the GPT models’ ability to reason in complex situations and grasp local cultural subtleties. Our findings have indicated that, although the models have exhibited high levels of human-like reasoning, significant limitations remained, especially concerning the interpretation of cultural contexts. This article also explored potential applications and use-cases of our Test, underlining its significance in AI training, ethics compliance, sensitivity auditing, and AI-driven cultural consultation. We concluded by emphasizing its broader implications in the AGI domain, highlighting the necessity for interdisciplinary approaches, wider accessibility to various GPT models, and a profound understanding of the interplay between GPT reasoning and cultural sensitivity.},
  archive      = {J_TIIS},
  doi          = {10.1145/3670691},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3},
  pages        = {1-37},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {A reasoning and value alignment test to assess advanced GPT reasoning},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visualization for recommendation explainability: A survey
and new perspectives. <em>TIIS</em>, <em>14</em>(3), 1–40. (<a
href="https://doi.org/10.1145/3672276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing system-generated explanations for recommendations represents an important step toward transparent and trustworthy recommender systems. Explainable recommender systems provide a human-understandable rationale for their outputs. Over the past two decades, explainable recommendation has attracted much attention in the recommender systems research community. This paper aims to provide a comprehensive review of research efforts on visual explanation in recommender systems. More concretely, we systematically review the literature on explanations in recommender systems based on four dimensions, namely explanation aim, explanation scope, explanation method, and explanation format. Recognizing the importance of visualization, we approach the recommender system literature from the angle of explanatory visualizations, that is using visualizations as a display style of explanation. As a result, we derive a set of guidelines that might be constructive for designing explanatory visualizations in recommender systems and identify perspectives for future work in this field. The aim of this review is to help recommendation researchers and practitioners better understand the potential of visually explainable recommendation research and to support them in the systematic design of visual explanations in current and future recommender systems.},
  archive      = {J_TIIS},
  doi          = {10.1145/3672276},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3},
  pages        = {1-40},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Visualization for recommendation explainability: A survey and new perspectives},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ID.8: Co-creating visual stories with generative AI.
<em>TIIS</em>, <em>14</em>(3), 1–29. (<a
href="https://doi.org/10.1145/3672277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storytelling is an integral part of human culture and significantly impacts cognitive and socio-emotional development and connection. Despite the importance of interactive visual storytelling, the process of creating such content requires specialized skills and is labor-intensive. This article introduces ID.8, an open-source system designed for the co-creation of visual stories with generative AI. We focus on enabling an inclusive storytelling experience by simplifying the content creation process and allowing for customization. Our user evaluation confirms a generally positive user experience in domains such as enjoyment and exploration while highlighting areas for improvement, particularly in immersiveness, alignment, and partnership between the user and the AI system. Overall, our findings indicate promising possibilities for empowering people to create visual stories with generative AI. This work contributes a novel content authoring system, ID.8, and insights into the challenges and potential of using generative AI for multimedia content creation.},
  archive      = {J_TIIS},
  doi          = {10.1145/3672277},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {ID.8: Co-creating visual stories with generative AI},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). -generAItor: Tree-in-the-loop text generation for language
model explainability and adaptation. <em>TIIS</em>, <em>14</em>(2),
1–32. (<a href="https://doi.org/10.1145/3652028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods. Additionally, we demonstrate the applicability of our approach in a qualitative user study. Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases.},
  archive      = {J_TIIS},
  doi          = {10.1145/3652028},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {6},
  number       = {2},
  pages        = {1-32},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {-generAItor: Tree-in-the-loop text generation for language model explainability and adaptation},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatial constraint model for manipulating static
visualizations. <em>TIIS</em>, <em>14</em>(2), 1–29. (<a
href="https://doi.org/10.1145/3657642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a spatial constraint model to characterize the positioning and interactions in visualizations, thereby facilitating the activation of static visualizations. Our model provides users with the capability to manipulate visualizations through operations such as selection, filtering, navigation, arrangement, and aggregation. Building upon this conceptual framework, we propose a prototype system designed to activate pre-existing visualizations by imbuing them with intelligent interactions. This augmentation is accomplished through the integration of visual objects with forces. The instantiation of our spatial constraint model enables seamless animated transitions between distinct visualization layouts. To demonstrate the efficacy of our approach, we present usage scenarios that involve the activation of visualizations within real-world contexts.},
  archive      = {J_TIIS},
  doi          = {10.1145/3657642},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {6},
  number       = {2},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {A spatial constraint model for manipulating static visualizations},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative multi-objective bayesian design optimization.
<em>TIIS</em>, <em>14</em>(2), 1–28. (<a
href="https://doi.org/10.1145/3657643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational methods can potentially facilitate user interface design by complementing designer intuition, prior experience, and personal preference. Framing a user interface design task as a multi-objective optimization problem can help with operationalizing and structuring this process at the expense of designer agency and experience. While offering a systematic means of exploring the design space, the optimization process cannot typically leverage the designer’s expertise in quickly identifying that a given “bad” design is not worth evaluating. We here examine a cooperative approach where both the designer and optimization process share a common goal and work in partnership by establishing a shared understanding of the design space. We tackle the research question: How can we foster cooperation between the designer and a systematic optimization process in order to best leverage their combined strength? We introduce and present an evaluation of a cooperative approach that allows the user to express their design insight and work in concert with a multi-objective design process. We find that the cooperative approach successfully encourages designers to explore more widely in the design space than when they are working without assistance from an optimization process. The cooperative approach also delivers design outcomes that are comparable to an optimization process run without any direct designer input but achieves this with greater efficiency and substantially higher designer engagement levels.},
  archive      = {J_TIIS},
  doi          = {10.1145/3657643},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {6},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Cooperative multi-objective bayesian design optimization},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). “It would work for me too”: How online communities shape
software developers’ trust in AI-powered code generation tools.
<em>TIIS</em>, <em>14</em>(2), 1–39. (<a
href="https://doi.org/10.1145/3651990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While revolutionary AI-powered code generation tools have been rising rapidly, we know little about how and how to help software developers form appropriate trust in those AI tools. Through a two-phase formative study, we investigate how online communities shape developers’ trust in AI tools and how we can leverage community features to facilitate appropriate user trust. Through interviewing 17 developers, we find that developers collectively make sense of AI tools using the experiences shared by community members and leverage community signals to evaluate AI suggestions. We then surface design opportunities and conduct 11 design probe sessions to explore the design space of using community features to support user trust in AI code generation systems. We synthesize our findings and extend an existing model of user trust in AI technologies with sociotechnical factors. We map out the design considerations for integrating user community into the AI code generation experience.},
  archive      = {J_TIIS},
  doi          = {10.1145/3651990},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {5},
  number       = {2},
  pages        = {1-39},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {“It would work for me too”: How online communities shape software developers’ trust in AI-powered code generation tools},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Entity footprinting: Modeling contextual user states via
digital activity monitoring. <em>TIIS</em>, <em>14</em>(2), 1–27. (<a
href="https://doi.org/10.1145/3643893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our digital life consists of activities that are organized around tasks and exhibit different user states in the digital contexts around these activities. Previous works have shown that digital activity monitoring can be used to predict entities that users will need to perform digital tasks. There have been methods developed to automatically detect the tasks of a user. However, these studies typically support only specific applications and tasks, and relatively little research has been conducted on real-life digital activities. This article introduces user state modeling and prediction with contextual information captured as entities, recorded from real-world digital user behavior, called entity footprinting —a system that records users’ digital activities on their screens and proactively provides useful entities across application boundaries without requiring explicit query formulation. Our methodology is to detect contextual user states using latent representations of entities occurring in digital activities. Using topic models and recurrent neural networks, the model learns the latent representation of concurrent entities and their sequential relationships. We report a field study in which the digital activities of 13 people were recorded continuously for 14 days. The model learned from this data is used to (1) predict contextual user states and (2) predict relevant entities for the detected states. The results show improved user state detection accuracy and entity prediction performance compared to static, heuristic, and basic topic models. Our findings have implications for the design of proactive recommendation systems that can implicitly infer users’ contextual state by monitoring users’ digital activities and proactively recommending the right information at the right time.},
  archive      = {J_TIIS},
  doi          = {10.1145/3643893},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Entity footprinting: Modeling contextual user states via digital activity monitoring},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Talk2Data: A natural language interface for exploratory
visual analysis via question decomposition. <em>TIIS</em>,
<em>14</em>(2), 1–24. (<a
href="https://doi.org/10.1145/3643894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through a natural language interface (NLI) for exploratory visual analysis, users can directly “ask” analytical questions about the given tabular data. This process greatly improves user experience and lowers the technical barriers of data analysis. Existing techniques focus on generating a visualization from a concrete question. However, complex questions, requiring multiple data queries and visualizations to answer, are frequently asked in data exploration and analysis, which cannot be easily solved with the existing techniques. To address this issue, in this article, we introduce Talk2Data, a natural language interface for exploratory visual analysis that supports answering complex questions. It leverages an advanced deep-learning model to resolve complex questions into a series of simple questions that could gradually elaborate on the users’ requirements. To present answers, we design a set of annotated and captioned visualizations to represent the answers in a form that supports interpretation and narration. We conducted an ablation study and a controlled user study to evaluate the Talk2Data’s effectiveness and usefulness.},
  archive      = {J_TIIS},
  doi          = {10.1145/3643894},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Talk2Data: A natural language interface for exploratory visual analysis via question decomposition},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Man and the machine: Effects of AI-assisted human labeling
on interactive annotation of real-time video streams. <em>TIIS</em>,
<em>14</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3649457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI-assisted interactive annotation is a powerful way to facilitate data annotation—a prerequisite for constructing robust AI models. While AI-assisted interactive annotation has been extensively studied in static settings, less is known about its usage in dynamic scenarios where the annotators operate under time and cognitive constraints, e.g., while detecting suspicious or dangerous activities from real-time surveillance feeds. Understanding how AI can assist annotators in these tasks and facilitate consistent annotation is paramount to ensure high performance for AI models trained on these data. We address this gap in interactive machine learning (IML) research, contributing an extensive investigation of the benefits, limitations, and challenges of AI-assisted annotation in dynamic application use cases. We address both the effects of AI on annotators and the effects of (AI) annotations on the performance of AI models trained on annotated data in real-time video annotations. We conduct extensive experiments that compare annotation performance at two annotator levels (expert and non-expert) and two interactive labeling techniques (with and without AI assistance). In a controlled study with \(N=34\) annotators and a follow-up study with 51,963 images and their annotation labels being input to the AI model, we demonstrate that the benefits of AI-assisted models are greatest for non-expert users and for cases where targets are only partially or briefly visible. The expert users tend to outperform or achieve similar performance as the AI model. Labels combining AI and expert annotations result in the best overall performance as the AI reduces overflow and latency in the expert annotations. We derive guidelines for the use of AI-assisted human annotation in real-time dynamic use cases.},
  archive      = {J_TIIS},
  doi          = {10.1145/3649457},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Man and the machine: Effects of AI-assisted human labeling on interactive annotation of real-time video streams},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I know this looks bad, but i can explain: Understanding when
AI should explain actions in human-AI teams. <em>TIIS</em>,
<em>14</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3635474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explanation of artificial intelligence (AI) decision-making has become an important research area in human–computer interaction (HCI) and computer-supported teamwork research. While plenty of research has investigated AI explanations with an intent to improve AI transparency and human trust in AI, how AI explanations function in teaming environments remains unclear. Given that a major benefit of AI giving explanations is to increase human trust understanding how AI explanations impact human trust is crucial to effective human-AI teamwork. An online experiment was conducted with 156 participants to explore this question by examining how a teammate’s explanations impact the perceived trust of the teammate and the effectiveness of the team and how these impacts vary based on whether the teammate is a human or an AI. This study shows that explanations facilitate trust in AI teammates when explaining why AI disobeyed humans’ orders but hindered trust when explaining why an AI lied to humans. In addition, participants’ personal characteristics (e.g., their gender and the individual’s ethical framework) impacted their perceptions of AI teammates both directly and indirectly in different scenarios. Our study contributes to interactive intelligent systems and HCI by shedding light on how an AI teammate’s actions and corresponding explanations are perceived by humans while identifying factors that impact trust and perceived effectiveness. This work provides an initial understanding of AI explanations in human-AI teams, which can be used for future research to build upon in exploring AI explanation implementation in collaborative environments.},
  archive      = {J_TIIS},
  doi          = {10.1145/3635474},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {I know this looks bad, but i can explain: Understanding when AI should explain actions in human-AI teams},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting group choices from group profiles. <em>TIIS</em>,
<em>14</em>(1), 1–27. (<a
href="https://doi.org/10.1145/3639710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group recommender systems (GRSs) identify items to recommend to a group of people by aggregating group members’ individual preferences into a group profile and selecting the items that have the largest score in the group profile. The GRS predicts that these recommendations would be chosen by the group by assuming that the group is applying the same preference aggregation strategy as the one adopted by the GRS. However, predicting the choice of a group is more complex since the GRS is not aware of the exact preference aggregation strategy that is going to be used by the group. To this end, the aim of this article is to validate the research hypothesis that, by using a machine learning approach and a dataset of observed group choices, it is possible to predict a group’s final choice better than by using a standard preference aggregation strategy. Inspired by the Decision Scheme theory, which first tried to address the group choice prediction problem, we search for a group profile definition that, in conjunction with a machine learning model, can be used to accurately predict a group choice. Moreover, to cope with the data scarcity problem, we propose two data augmentation methods, which add synthetic group profiles to the training data, and we hypothesize that they can further improve the choice prediction accuracy. We validate our research hypotheses by using a dataset containing 282 participants organized in 79 groups. The experiments indicate that the proposed method outperforms baseline aggregation strategies when used for group choice prediction. The method we propose is robust with the presence of missing preference data and achieves a performance superior to what humans can achieve on the group choice prediction task. Finally, the proposed data augmentation method can also improve the prediction accuracy. Our approach can be exploited in novel GRSs to identify the items that the group is likely to choose and to help groups to make even better and fairer choices.},
  archive      = {J_TIIS},
  doi          = {10.1145/3639710},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Predicting group choices from group profiles},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward addressing ambiguous interactions and inferring user
intent with dimension reduction and clustering combinations in visual
analytics. <em>TIIS</em>, <em>14</em>(1), 1–35. (<a
href="https://doi.org/10.1145/3588565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct manipulation interactions on projections are often incorporated in visual analytics applications. These interactions enable analysts to provide incremental feedback to the system in a semi-supervised manner, demonstrating relationships that the analyst wishes to find within the data. However, determining the precise intent of the analyst is a challenge. When an analyst interacts with a projection, the inherent ambiguity of interactions can lead to a variety of possible interpretations that the system can infer. Previous work has demonstrated the utility of clusters as an interaction target to address this “With Respect to What” problem in dimension-reduced projections. However, the introduction of clusters introduces interaction inference challenges as well. In this work, we discuss the interaction space for the simultaneous use of semi-supervised dimension reduction and clustering algorithms. We introduce a novel pipeline representation to disambiguate between interactions on observations and clusters, as well as which underlying model is responding to those analyst interactions. We use a prototype visual analytics tool to demonstrate the effects of these ambiguous interactions, their properties, and the insights that an analyst can glean from each.},
  archive      = {J_TIIS},
  doi          = {10.1145/3588565},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-35},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Toward addressing ambiguous interactions and inferring user intent with dimension reduction and clustering combinations in visual analytics},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simulation-based optimization of user interfaces for
quality-assuring machine learning model predictions. <em>TIIS</em>,
<em>14</em>(1), 1–32. (<a
href="https://doi.org/10.1145/3594552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality-sensitive applications of machine learning (ML) require quality assurance (QA) by humans before the predictions of an ML model can be deployed. QA for ML (QA4ML) interfaces require users to view a large amount of data and perform many interactions to correct errors made by the ML model. An optimized user interface (UI) can significantly reduce interaction costs. While UI optimization can be informed by user studies evaluating design options, this approach is not scalable, because there are typically numerous small variations that can affect the efficiency of a QA4ML interface. Hence, we propose using simulation to evaluate and aid the optimization of QA4ML interfaces. In particular, we focus on simulating the combined effects of human intelligence in initiating appropriate interaction commands and machine intelligence in providing algorithmic assistance for accelerating QA4ML processes. As QA4ML is usually labor-intensive, we use the simulated task completion time as the metric for UI optimization under different interface and algorithm setups. We demonstrate the usage of this UI design method in several QA4ML applications.},
  archive      = {J_TIIS},
  doi          = {10.1145/3594552},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-32},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Simulation-based optimization of user interfaces for quality-assuring machine learning model predictions},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VERB: Visualizing and interpreting bias mitigation
techniques geometrically for word representations. <em>TIIS</em>,
<em>14</em>(1), 1–34. (<a
href="https://doi.org/10.1145/3604433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word vector embeddings have been shown to contain and amplify biases in the data they are extracted from. Consequently, many techniques have been proposed to identify, mitigate, and attenuate these biases in word representations. In this article, we utilize interactive visualization to increase the interpretability and accessibility of a collection of state-of-the-art debiasing techniques. To aid this, we present the Visualization of Embedding Representations for deBiasing (VERB) system, an open-source web-based visualization tool that helps users gain a technical understanding and visual intuition of the inner workings of debiasing techniques, with a focus on their geometric properties. In particular, VERB offers easy-to-follow examples that explore the effects of these debiasing techniques on the geometry of high-dimensional word vectors. To help understand how various debiasing techniques change the underlying geometry, VERB decomposes each technique into interpretable sequences of primitive transformations and highlights their effect on the word vectors using dimensionality reduction and interactive visual exploration. VERB is designed to target natural language processing (NLP) practitioners who are designing decision-making systems on top of word embeddings and researchers working with the fairness and ethics of machine learning systems in NLP. It can also serve as a visual medium for education, which helps an NLP novice understand and mitigate biases in word embeddings.},
  archive      = {J_TIIS},
  doi          = {10.1145/3604433},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-34},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {VERB: Visualizing and interpreting bias mitigation techniques geometrically for word representations},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrity-based explanations for fostering appropriate trust
in AI agents. <em>TIIS</em>, <em>14</em>(1), 1–36. (<a
href="https://doi.org/10.1145/3610578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Appropriate trust is an important component of the interaction between people and AI systems, in that “inappropriate” trust can cause disuse, misuse, or abuse of AI. To foster appropriate trust in AI, we need to understand how AI systems can elicit appropriate levels of trust from their users. Out of the aspects that influence trust, this article focuses on the effect of showing integrity. In particular, this article presents a study of how different integrity-based explanations made by an AI agent affect the appropriateness of trust of a human in that agent. To explore this, (1) we provide a formal definition to measure appropriate trust, (2) present a between-subject user study with 160 participants who collaborated with an AI agent in such a task. In the study, the AI agent assisted its human partner in estimating calories on a food plate by expressing its integrity through explanations focusing on either honesty, transparency, or fairness. Our results show that (a) an agent who displays its integrity by being explicit about potential biases in data or algorithms achieved appropriate trust more often compared to being honest about capability or transparent about the decision-making process, and (b) subjective trust builds up and recovers better with honesty-like integrity explanations. Our results contribute to the design of agent-based AI systems that guide humans to appropriately trust them, a formal method to measure appropriate trust, and how to support humans in calibrating their trust in AI.},
  archive      = {J_TIIS},
  doi          = {10.1145/3610578},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-36},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Integrity-based explanations for fostering appropriate trust in AI agents},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How should an AI trust its human teammates? Exploring
possible cues of artificial trust. <em>TIIS</em>, <em>14</em>(1), 1–26.
(<a href="https://doi.org/10.1145/3635475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In teams composed of humans, we use trust in others to make decisions, such as what to do next, who to help and who to ask for help. When a team member is artificial, they should also be able to assess whether a human teammate is trustworthy for a certain task. We see trustworthiness as the combination of (1) whether someone will do a task and (2) whether they can do it. With building beliefs in trustworthiness as an ultimate goal, we explore which internal factors (krypta) of the human may play a role (e.g., ability, benevolence, and integrity) in determining trustworthiness, according to existing literature. Furthermore, we investigate which observable metrics (manifesta) an agent may take into account as cues for the human teammate’s krypta in an online 2D grid-world experiment ( n = 54). Results suggest that cues of ability, benevolence and integrity influence trustworthiness. However, we observed that trustworthiness is mainly influenced by human’s playing strategy and cost-benefit analysis, which deserves further investigation. This is a first step towards building informed beliefs of human trustworthiness in human-AI teamwork.},
  archive      = {J_TIIS},
  doi          = {10.1145/3635475},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {How should an AI trust its human teammates? exploring possible cues of artificial trust},
  volume       = {14},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
