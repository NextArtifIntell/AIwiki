<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tog---272">TOG - 272</h2>
<ul>
<li><details>
<summary>
(2024). Learn to create simple LEGO micro buildings. <em>TOG</em>,
<em>43</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3687755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the first learning-based generative pipeline for effectively creating 3D LEGO® 1 models. This task is very challenging due to the lack of dedicated representations and datasets for learning coherently-connected bricks arrangements, as well as an immense design space that is combinatorial in nature. We approach this task by focusing on creating LEGO® micro buildings. Our contributions are four-fold. First, we propose the LEGO® semantic volume representation to encode LEGO® models, considering the bricks types and bricks connections, while allowing back-propagation learning. Second, we further consider the transformative nature of LEGO ® to atomize the semantic volume and formulate a generative model to learn the representation. Third, we build a rich dataset of micro buildings for model learning. Last, we design the progressive reconstructor to create 3D LEGO® models from the generated representations, while ensuring bricks connections. We employed our pipeline to create LEGO ® micro buildings with a wide array of bricks types, demonstrating its strong capability of learning diverse micro-building styles and producing assemble-able LEGO® models. Further, we performed various quantitative evaluations, ablations, and a user study to show the compelling capability of our approach in terms of generative quality, fidelity, and diversity.},
  archive      = {J_TOG},
  doi          = {10.1145/3687755},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learn to create simple LEGO micro buildings},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time large-scale deformation of gaussian splatting.
<em>TOG</em>, <em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene. Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in a real-time fashion. Gaussian Splatting (GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views. However, it cannot be easily deformed due to the use of discrete Gaussians and the lack of explicit topology. To address this, we develop a novel GS-based method (GaussianMesh) that enables interactive deformation. Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation. 3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians. Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians ( e.g. , misaligned Gaussians, long-narrow shaped Gaussians), thus enhancing visual quality and reducing artifacts during deformation. Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh. Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation. Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate (65 FPS on average on a single commodity GPU).},
  archive      = {J_TOG},
  doi          = {10.1145/3687756},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time large-scale deformation of gaussian splatting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NU-NeRF: Neural reconstruction of nested transparent objects
with uncontrolled capture environment. <em>TOG</em>, <em>43</em>(6),
1–14. (<a href="https://doi.org/10.1145/3687757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The geometry reconstruction of transparent objects is a challenging problem due to the highly noncontinuous and rapidly changing surface color caused by refraction. Existing methods rely on special capture devices, dedicated backgrounds, or ground-truth object masks to provide more priors and reduce the ambiguity of the problem. However, it is hard to apply methods with these special requirements to real-life reconstruction tasks, like scenes captured in the wild using mobile devices. Moreover, these methods can only cope with solid and homogeneous materials, greatly limiting the scope of the application. To solve the problems above, we propose NU-NeRF to reconstruct nested transparent objects without requiring a dedicated capture environment or additional input. NU-NeRF is built upon a neural signed distance field formulation and leverages neural rendering techniques. It consists of two main stages. In Stage I, the surface color is separated into reflection and refraction. The reflection is decomposed using physically based material and rendering. The refraction is modeled using a single MLP given the refraction and view directions, which is a simple yet effective solution of refraction modeling. This step produces high-fidelity geometry of the outer surface. In stage II, we use explicit ray tracing on the reconstructed outer surface for accurate light transport simulation. The surface reconstruction is executed again inside the outer geometry to obtain any inner surface geometry. In this process, a novel transparent interface formulation is used to cope with different types of transparent surfaces. Experiments conducted on synthetic scenes and real captured scenes show that NU-NeRF is capable of producing better reconstruction results than previous methods and achieves accurate nested surface reconstruction under an uncontrolled capture environment.},
  archive      = {J_TOG},
  doi          = {10.1145/3687757},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {NU-NeRF: Neural reconstruction of nested transparent objects with uncontrolled capture environment},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MiNNIE: A mixed multigrid method for real-time simulation of
nonlinear near-incompressible elastics. <em>TOG</em>, <em>43</em>(6),
1–15. (<a href="https://doi.org/10.1145/3687758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose MiNNIE, a simple yet comprehensive framework for real-time simulation of nonlinear near-incompressible elastics. To avoid the common volumetric locking issues at high Poisson&#39;s ratios of linear finite element methods (FEM), we build MiNNIE upon a mixed FEM framework and further incorporate a pressure stabilization term to ensure excellent convergence of multigrid solvers. Our pressure stabilization strategy injects bounded influence on nodal displacement which can be eliminated using a quasiNewton method. MiNNIE has a specially tailored GPU multigrid solver including a modified skinning-space interpolation scheme, a novel vertex Vanka smoother, and an efficient dense solver using Schur complement. MiNNIE supports various elastic material models and simulates them in real-time, supporting a full range of Poisson&#39;s ratios up to 0.5 while handling large deformations, element inversions, and self-collisions at the same time.},
  archive      = {J_TOG},
  doi          = {10.1145/3687758},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {MiNNIE: A mixed multigrid method for real-time simulation of nonlinear near-incompressible elastics},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GaussianObject: High-quality 3D object reconstruction from
four views with gaussian splatting. <em>TOG</em>, <em>43</em>(6), 1–13.
(<a href="https://doi.org/10.1145/3687759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination, which explicitly inject structure priors into the initial optimization process to help build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. We further design a COLMAP-free variant, where pre-given accurate camera poses are not required, which achieves competitive quality and facilitates wider applications. GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, OpenIllumination, and our-collected unposed images, achieving superior performance from only four views and significantly outperforming previous SOTA methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3687759},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {GaussianObject: High-quality 3D object reconstruction from four views with gaussian splatting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient GPU cloth simulation with non-distance barriers
and subspace reuse. <em>TOG</em>, <em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper pushes the performance of cloth simulation, making the simulation interactive even for high-resolution garment models while keeping every triangle untangled. The penetration-free guarantee is inspired by the interior point method, which converts the inequality constraints to barrier potentials. We propose a major overhaul of this modality within the projective dynamics framework by leveraging an adaptive weighting mechanism inspired by barrier formulation. This approach does not depend on the distance between mesh primitives, but on the virtual life span of a collision event and thus keeps all the vertices within feasible region. Such a non-distance barrier model allows a new way to integrate collision resolution into the simulation pipeline. Another contributor to the performance boost comes from the subspace reuse strategy. This is based on the observation that low-frequency strain propagation is near orthogonal to the deformation induced by collisions or self-collisions, often of high frequency. Subspace reuse then takes care of low-frequency residuals, while high-frequency residuals can also be effectively smoothed by GPU-based iterative solvers. We show that our method outperforms existing fast cloth simulators by at least one order while producing high-quality animations of high-resolution models.},
  archive      = {J_TOG},
  doi          = {10.1145/3687760},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient GPU cloth simulation with non-distance barriers and subspace reuse},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ToonCrafter: Generative cartoon interpolation. <em>TOG</em>,
<em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce ToonCrafter, a novel approach that transcends traditional correspondence-based cartoon video interpolation, paving the way for generative interpolation. Traditional methods, that implicitly assume linear motion and the absence of complicated phenomena like dis-occlusion, often struggle with the exaggerated non-linear and large motions with occlusion commonly found in cartoons, resulting in implausible or even failed interpolation results. To overcome these limitations, we explore the potential of adapting live-action video priors to better suit cartoon interpolation within a generative framework. ToonCrafter effectively addresses the challenges faced when applying live-action video motion priors to generative cartoon interpolation. First, we design a toon rectification learning strategy that seamlessly adapts live-action video priors to the cartoon domain, resolving the domain gap and content leakage issues. Next, we introduce a dual-reference-based 3D decoder to compensate for lost details due to the highly compressed latent prior spaces, ensuring the preservation of fine details in interpolation results. Finally, we design a flexible sketch encoder that empowers users with interactive control over the interpolation results. Experimental results demonstrate that our proposed method not only produces visually convincing and more natural dynamics, but also effectively handles dis-occlusion. The comparative evaluation demonstrates the notable superiority of our approach over existing competitors. Code and model weights are available at https://doubiiu.github.io/projects/ToonCrafter},
  archive      = {J_TOG},
  doi          = {10.1145/3687761},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {ToonCrafter: Generative cartoon interpolation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LetsGo: Large-scale garage modeling and rendering via
LiDAR-assisted gaussian primitives. <em>TOG</em>, <em>43</em>(6), 1–18.
(<a href="https://doi.org/10.1145/3687762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large garages are ubiquitous yet intricate scenes that present unique challenges due to their monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction often fail in these environments due to poor correspondence construction. To address these challenges, we introduce LetsGo, a LiDAR-assisted Gaussian splatting framework for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate data acquisition. Using this Polar device, we present the GarageWorld dataset, consisting of eight expansive garage scenes with diverse geometric structures, which will be made publicly available for further research. Our approach demonstrates that LiDAR point clouds collected by the Polar device significantly enhance a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We introduce a novel depth regularizer that effectively eliminates floating artifacts in rendered images. Additionally, we propose a multi-resolution 3D Gaussian representation designed for Level-of-Detail (LOD) rendering. This includes adapted scaling factors for individual levels and a random-resolution-level training scheme to optimize the Gaussians across different resolutions. This representation enables efficient rendering of large-scale garage scenes on lightweight devices via a web-based renderer. Experimental results on our GarageWorld dataset, as well as on ScanNet++ and KITTI-360, demonstrate the superiority of our method in terms of rendering quality and resource efficiency.},
  archive      = {J_TOG},
  doi          = {10.1145/3687762},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {LetsGo: Large-scale garage modeling and rendering via LiDAR-assisted gaussian primitives},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MATTopo: Topology-preserving medial axis transform with
restricted power diagram. <em>TOG</em>, <em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel topology-preserving 3D medial axis computation framework based on volumetric restricted power diagram (RPD), while preserving the medial features and geometric convergence simultaneously, for both 3D CAD and organic shapes. The volumetric RPD discretizes the input 3D volume into sub-regions given a set of medial spheres. With this intermediate structure, we convert the homotopy equivalency between the generated medial mesh and the input 3D shape into a localized contractibility checking for each restricted element (power cell, power face, power edge), by checking their connected components and Euler characteristics. We further propose a fractional Euler characteristic algorithm for efficient GPU-based computation of Euler characteristic for each restricted element on the fly while computing the volumetric RPD. Compared with existing voxel-based or point-cloud-based methods, our approach is the first to adaptively and directly revise the medial mesh without globally modifying the dependent structure, such as voxel size or sampling density, while preserving its topology and medial features. In comparison with the feature preservation method MATFP [Wang et al. 2022], our method provides geometrically comparable results with fewer spheres and more robustly captures the topology of the input 3D shape.},
  archive      = {J_TOG},
  doi          = {10.1145/3687763},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {MATTopo: Topology-preserving medial axis transform with restricted power diagram},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable owen scrambling. <em>TOG</em>,
<em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-Monte Carlo integration is at the core of rendering. This technique estimates the value of an integral by evaluating the integrand at well-chosen sample locations. These sample points are designed to cover the domain as uniformly as possible to achieve better convergence rates than purely random points. Deterministic low-discrepancy sequences have been shown to outperform many competitors by guaranteeing good uniformity as measured by the so-called discrepancy metric, and, indirectly, by an integer t value relating the number of points falling into each domain stratum with the stratum area (lower t is better). To achieve randomness, scrambling techniques produce multiple realizations preserving the t value, making the construction stochastic. Among them, Owen scrambling is a popular approach that recursively permutes intervals for each dimension. However, relying on permutation trees makes it incompatible with smooth optimization frameworks. We present a differentiable Owen scrambling that regularizes permutations. We show that it can effectively be used with automatic differentiation tools for optimizing low-discrepancy sequences to improve metrics such as optimal transport uniformity, integration error, designed power spectra or projective properties, while maintaining their initial t -value as guaranteed by Owen scrambling. In some rendering settings, we show that our optimized sequences improve the rendering error.},
  archive      = {J_TOG},
  doi          = {10.1145/3687764},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable owen scrambling},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized shock-protecting microstructures. <em>TOG</em>,
<em>43</em>(6), 1–21. (<a
href="https://doi.org/10.1145/3687765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mechanical shock is a common occurrence in various settings, there are two different scenarios for shock protection: catastrophic protection (e.g. car collisions and falls) and routine protection (e.g. shoe soles and mattresses). The former protects against one-time events, the latter against periodic shocks and loads. Common shock absorbers based on plasticity and fracturing materials are suitable for the former, while our focus is on the latter, where elastic structures are useful. Further, we optimize the effective elastic material properties which control the critical shock parameter, maximal stress, with energy dissipation by viscous forces assumed adequate. Improved elastic materials protecting against shock can be used in applications such as automotive suspension, furniture like sofas and mattresses, landing gear systems, etc. Materials offering optimal protection against shock have a highly non-linear elastic response: their reaction force needs to be as close as possible to constant with respect to deformation. In this paper, we use shape optimization and topology search to design 2D families of microstructures approximating the ideal behavior across a range of deformations, leading to superior shock protection. We present an algorithmic pipeline for the optimal design of such families combining differentiable nonlinear homogenization with self-contact and an optimization algorithm. We validate the effectiveness of our advanced 2D designs by extruding and fabricating them with 3D printing technologies and performing material and drop testing.},
  archive      = {J_TOG},
  doi          = {10.1145/3687765},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optimized shock-protecting microstructures},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPU coroutines for flexible splitting and scheduling of
rendering tasks. <em>TOG</em>, <em>43</em>(6), 1–24. (<a
href="https://doi.org/10.1145/3687766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce coroutines into GPU kernel programming, providing an automated solution for flexible splitting and scheduling of rendering tasks. This approach addresses a prevalent challenge in harnessing the power of modern GPUs for complex, imbalanced graphics workloads like path tracing. Usually, to accommodate the SIMT execution model and latency-hiding architecture, developers have to decompose a monolithic mega-kernel into smaller sub-tasks for improved thread coherence and reduced register pressure. However, involving the handling of intricate nested control flows and numerous interdependent program states, this process can be exceedingly tedious and error-prone when performed manually. Coroutines, a building block for asynchronous programming in many high-level CPU languages, exhibit untapped potential for restructuring GPU kernels due to their versatility in control representation. By extending Luisa [Zheng et al. 2022], we implement an asymmetric, stackless coroutine model with programming language support and multiple built-in schedulers for modern GPUs. To showcase the effectiveness of our model and implementation, we examine them in different application scenarios, including path tracing, SDF rendering, and incorporation with custom passes.},
  archive      = {J_TOG},
  doi          = {10.1145/3687766},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {ACM Trans. Graph.},
  title        = {GPU coroutines for flexible splitting and scheduling of rendering tasks},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Polarimetric BSSRDF acquisition of dynamic faces.
<em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquisition and modeling of polarized light reflection and scattering help reveal the shape, structure, and physical characteristics of an object, which is increasingly important in computer graphics. However, current polarimetric acquisition systems are limited to static and opaque objects. Human faces, on the other hand, present a particularly difficult challenge, given their complex structure and reflectance properties, the strong presence of spatially-varying subsurface scattering, and their dynamic nature. We present a new polarimetric acquisition method for dynamic human faces, which focuses on capturing spatially varying appearance and precise geometry, across a wide spectrum of skin tones and facial expressions. It includes both single and heterogeneous subsurface scattering, index of refraction, and specular roughness and intensity, among other parameters, while revealing biophysically-based components such as inner- and outer-layer hemoglobin, eumelanin and pheomelanin. Our method leverages such components&#39; unique multispectral absorption profiles to quantify their concentrations, which in turn inform our model about the complex interactions occurring within the skin layers. To our knowledge, our work is the first to simultaneously acquire polarimetric and spectral reflectance information alongside biophysically-based skin parameters and geometry of dynamic human faces. Moreover, our polarimetric skin model integrates seamlessly into various rendering pipelines.},
  archive      = {J_TOG},
  doi          = {10.1145/3687767},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Polarimetric BSSRDF acquisition of dynamic faces},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GroomCap: High-fidelity prior-free hair capture.
<em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent advances in multi-view hair reconstruction, achieving strand-level precision remains a significant challenge due to inherent limitations in existing capture pipelines. We introduce GroomCap , a novel multi-view hair capture method that reconstructs faithful and high-fidelity hair geometry without relying on external data priors. To address the limitations of conventional reconstruction algorithms, we propose a neural implicit representation for hair volume that encodes high-resolution 3D orientation and occupancy from input views. This implicit hair volume is trained with a new volumetric 3D orientation rendering algorithm, coupled with 2D orientation distribution supervision, to effectively prevent the loss of structural information caused by undesired orientation blending. We further propose a Gaussian-based hair optimization strategy to refine the traced hair strands with a novel chained Gaussian representation, utilizing direct photometric supervision from images. Our results demonstrate that GroomCap is able to capture high-quality hair geometries that are not only more precise and detailed than existing methods but also versatile enough for a range of applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3687768},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {GroomCap: High-fidelity prior-free hair capture},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DreamUDF: Generating unsigned distance fields from a single
image. <em>TOG</em>, <em>43</em>(6), 1–21. (<a
href="https://doi.org/10.1145/3687769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in diffusion models and neural implicit surfaces have shown promising progress in generating 3D models. However, existing generative frameworks are limited to closed surfaces, failing to cope with a wide range of commonly seen shapes that have open boundaries. In this work, we present DreamUDF, a novel framework for generating high-quality 3D objects with arbitrary topologies from a single image. To address the challenge of generating proper topology given sparse and ambiguous observations, we propose to incorporate both the data priors from a multi-view diffusion model and the geometry priors brought by an unsigned distance field (UDF) reconstructor. In particular, we leverage a joint framework that consists of 1) a generation module that produces a neural radiance field for photorealistic renderings from arbitrary views; and 2) a reconstruction module that distills the learnable radiance field into surfaces with arbitrary topologies. We further introduce a field coupler that bridges the radiance field and UDF under a novel optimization scheme. This allows the two modules to mutually boost each other during training. Extensive experiments and evaluations demonstrate that DreamUDF achieves high-quality reconstruction and robust 3D generation on both closed and open surfaces with arbitrary topologies, compared to the previous works.},
  archive      = {J_TOG},
  doi          = {10.1145/3687769},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {DreamUDF: Generating unsigned distance fields from a single image},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MpcMech: Multi-point conjugation mechanisms. <em>TOG</em>,
<em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mechanism is an assembly of moving parts interconnected by joints to transfer an input motion to a desired output motion. Traditionally, to generate a complex motion, mechanisms are modeled by selecting and combining a number of mechanical parts with simple shapes such as links, gears, and cams. Combining multiple mechanical parts results in a mechanism with an intricate topology, which not only complicates assembly and maintenance but also deteriorates the functionality of generating motions due to accumulation of manufacturing imprecisions. To get rid of these limitations, we study mechanisms with a single pair of moving parts for generating complex motions. We model the pair of moving parts as a pair of conjugate surfaces with multiple conjugation points, forming a multi-point conjugation mechanism. To study this new mechanism, we establish a connection between conjugate surface pairs and form-closure grasps to formulate a dynamic form closure condition under which one conjugate surface is able to continuously transfer the motion to the other conjugate surface by utilizing multiple conjugation points. Guided by the condition, we propose an optimization-based approach to model the geometry of a multi-point conjugation mechanism for exactly generating a user-specified motion, in 1-, 2-, or 3-DOF motion space. The core of our approach is to model multiple conjugate curve pairs that satisfy various requirements in multi-point conjugation, dynamic form closure, and surface fabricability. We demonstrate the effectiveness of our approach by modeling different classes of multi-point conjugation mechanisms to generate various motions, evaluating the mechanisms&#39; kinematic performance with 3D printed prototypes, and presenting three applications of these mechanisms.},
  archive      = {J_TOG},
  doi          = {10.1145/3687770},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {MpcMech: Multi-point conjugation mechanisms},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PuzzleAvatar: Assembling 3D avatars from personal albums.
<em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating personalized 3D avatars is crucial for AR/VR. However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people. Methods for faithful reconstruction typically require full-body images in controlled settings. What if users could just upload their personal &quot;OOTD&quot; (Outfit Of The Day) photo collection and get a faithful avatar in return? The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle). We address this novel &quot; Album2Human &quot; task by developing PuzzleAvatar , a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, bypassing the challenging estimation of body and camera pose. To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into separate learned tokens, instilling these cues into the VLM. In effect, we exploit the learned tokens as &quot;puzzle pieces&quot; from which we assemble a faithful, personalized 3D avatar. Importantly, we can customize avatars by simply inter-changing tokens. As a benchmark for this new task, we create a new dataset, called PuzzleIOI , with 41 subjects in a total of nearly 1k OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and demonstrating strong robustness. Our code and data are publicly available for research purpose at puzzleavatar.is.tue.mpg.de},
  archive      = {J_TOG},
  doi          = {10.1145/3687771},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {PuzzleAvatar: Assembling 3D avatars from personal albums},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Look ma, no markers: Holistic performance capture without
the hassle. <em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the problem of highly-accurate, holistic performance capture for the face, body and hands simultaneously. Motion-capture technologies used in film and game production typically focus only on face, body or hand capture independently, involve complex and expensive hardware and a high degree of manual intervention from skilled operators. While machine-learning-based approaches exist to overcome these problems, they usually only support a single camera, often operate on a single part of the body, do not produce precise world-space results, and rarely generalize outside specific contexts. In this work, we introduce the first technique for markerfree, high-quality reconstruction of the complete human body, including eyes and tongue, without requiring any calibration, manual intervention or custom hardware. Our approach produces stable world-space results from arbitrary camera rigs as well as supporting varied capture environments and clothing. We achieve this through a hybrid approach that leverages machine learning models trained exclusively on synthetic data and powerful parametric models of human shape and motion. We evaluate our method on a number of body, face and hand reconstruction benchmarks and demonstrate state-of-the-art results that generalize on diverse datasets.},
  archive      = {J_TOG},
  doi          = {10.1145/3687772},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Look ma, no markers: Holistic performance capture without the hassle},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and globally consistent normal orientation based on the
winding number normal consistency. <em>TOG</em>, <em>43</em>(6), 1–19.
(<a href="https://doi.org/10.1145/3687895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating consistently oriented normals for point clouds enables a number of important applications in computer graphics such as surface reconstruction. While local normal estimation is possible with simple techniques like principal component analysis (PCA), orienting these normals to be globally consistent has been a notoriously difficult problem. Some recent methods exploit various properties of the winding number formula to achieve global consistency with state-of-the-art performance. Despite their exciting progress, these algorithms either have high space/time complexity, or do not produce accurate and consistently oriented normals for imperfect data. In this paper, we propose a novel property from the winding number formula, termed Winding Number Normal Consistency (WNNC ), to tackle this problem. The derived property is based on the simple observation that the normals (negative gradients) sampled from the winding number field should be codirectional to the normals used to compute the winding number field. Since the WNNC property itself does not resolve the inside/outside orientation ambiguity, we further propose to incorporate an objective function from Parametric Gauss Reconstruction (PGR). We propose to iteratively update normals by alternating between WNNC-based normal updates and PGR-based gradient descents, which leads to an embarrassingly simple yet effective iterative algorithm that allows fast and high-quality convergence to a globally consistent normal vector field. Furthermore, our proposed algorithm only involves repeatedly evaluating the winding number formula and its derivatives, which can be accelerated and parallelized using a treecode-based approximation algorithm due to their special structures. Exploiting this fact, we implement a GPU-accelerated treecode-based solver. Our GPU (and even CPU) implementation can be significantly faster than the recent state-of-the-art methods for normal orientation from raw points. Our code is integrated with the popular PyTorch framework to facilitate further research into winding numbers, and is publicly available at https://jsnln.github.io/wnnc/index.html.},
  archive      = {J_TOG},
  doi          = {10.1145/3687895},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast and globally consistent normal orientation based on the winding number normal consistency},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PVP-recon: Progressive view planning via warping consistency
for sparse-view surface reconstruction. <em>TOG</em>, <em>43</em>(6),
1–13. (<a href="https://doi.org/10.1145/3687896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural implicit representations have revolutionized dense multi-view surface reconstruction, yet their performance significantly diminishes with sparse input views. A few pioneering works have sought to tackle this challenge by leveraging additional geometric priors or multi-scene generalizability. However, they are still hindered by the imperfect choice of input views, using images under empirically determined viewpoints. We propose PVP-Recon , a novel and effective sparse-view surface reconstruction method that progressively plans the next best views to form an optimal set of sparse viewpoints for image capturing. PVP-Recon starts initial surface reconstruction with as few as 3 views and progressively adds new views which are determined based on a novel warping score that reflects the information gain of each newly added view. This progressive view planning progress is interleaved with a neural SDF-based reconstruction module that utilizes multi-resolution hash features, enhanced by a progressive training scheme and a directional Hessian loss. Quantitative and qualitative experiments on three benchmark datasets show that our system achieves high-quality reconstruction with a constrained input budget and outperforms existing baselines.},
  archive      = {J_TOG},
  doi          = {10.1145/3687896},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {PVP-recon: Progressive view planning via warping consistency for sparse-view surface reconstruction},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DirectL: Efficient radiance fields rendering for 3D light
field displays. <em>TOG</em>, <em>43</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3687897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autostereoscopic display technology, despite decades of development, has not achieved extensive application, primarily due to the daunting challenge of three-dimensional (3D) content creation for non-specialists. The emergence of Radiance Field as an innovative 3D representation has markedly revolutionized the domains of 3D reconstruction and generation, simplifying 3D content creation for common users and broadening the applicability of Light Field Displays (LFDs). However, the combination of these two technologies remains largely unexplored. The standard paradigm to create optimal content for parallax-based light field displays demands rendering at least 45 slightly shifted views preferably at high resolution per frame, a substantial hurdle for real-time rendering. We introduce DirectL, a novel rendering paradigm for Radiance Fields on autostereoscopic displays with lenticular lens. By thoroughly analyzing the interleaved mapping of spatial rays to screen sub-pixels, we accurately render only the light rays entering the human eye and propose subpixel repurposing to significantly reduce the pixel count required for rendering. Tailored for the two predominant radiance fields---Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS), we propose corresponding optimized rendering pipelines that directly render the light field images instead of multi-view images, achieving state-of-the-art rendering speeds on autostereoscopic displays. Extensive experiments across various autostereoscopic displays and user visual perception assessments demonstrate that DirectL accelerates rendering by up to 40 times compared to the standard paradigm without sacrificing visual quality. Its rendering process-only modification allows seamless integration into subsequent radiance field tasks. Finally, we incorporate DirectL into diverse applications, showcasing the stunning visual experiences and the synergy between Light Field Displays and Radiance Fields, which reveals the immense potential for application prospects. DirectL Project Homepage: direct-l.github.io},
  archive      = {J_TOG},
  doi          = {10.1145/3687897},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {DirectL: Efficient radiance fields rendering for 3D light field displays},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alignable lamella gridshells. <em>TOG</em>, <em>43</em>(6),
1–21. (<a href="https://doi.org/10.1145/3687898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alignable lamella gridshells are 3D grid structures capable of collapsing into a planar strip. This feature significantly simplifies on-site assembly and also ensures compactness for efficient transport and storage. However, designing these structures still remains a challenge. This paper tackles the inverse design problem of alignable lamella gridshells leveraging concepts from differential geometry and Cartan&#39;s theory of moving frames. The study unveils that geodesic alignable gridshells, where lamellae are disposed tangentially to the surface, are limited to forming shapes isometric to surfaces of revolution. Furthermore, it demonstrates that alignable gridshells with lamellae arranged orthogonally to a surface can be realized only on a specific class of surfaces that meet a particular curvature condition along their principal curvature lines. Finally, drawing on these theoretical findings, this work introduces novel computational tools tailored for the design of these structures.},
  archive      = {J_TOG},
  doi          = {10.1145/3687898},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Alignable lamella gridshells},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational biomimetics of winged seeds. <em>TOG</em>,
<em>43</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3687899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a computational pipeline to facilitate the biomimetic design of winged seeds. Our approach leverages 3D scans of natural winged seeds to construct a bio-inspired design space by interpolating them with geodesic coordinates in the 3D diffeomorphism group. We formulate aerodynamic design tasks with probabilistic performance objectives and adapt a gradient-free optimizer to explore the design space and minimize the expectation of performance objectives efficiently and effectively. Our pipeline discovers novel winged seed designs that outperform natural counterparts in aerodynamic tasks, including long-distance dispersal and guided flight. We validate the physical fidelity of our pipeline by showcasing paper models of selected winged seeds in the design space and reporting their similar aerodynamic behaviors in simulation and reality.},
  archive      = {J_TOG},
  doi          = {10.1145/3687899},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational biomimetics of winged seeds},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural differential appearance equations. <em>TOG</em>,
<em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method to reproduce dynamic appearance textures with space-stationary but time-varying visual statistics. While most previous work decomposes dynamic textures into static appearance and motion, we focus on dynamic appearance that results not from motion but variations of fundamental properties, such as rusting, decaying, melting, and weathering. To this end, we adopt the neural ordinary differential equation (ODE) to learn the underlying dynamics of appearance from a target exemplar. We simulate the ODE in two phases. At the &quot;warm-up&quot; phase, the ODE diffuses a random noise to an initial state. We then constrain the further evolution of this ODE to replicate the evolution of visual feature statistics in the exemplar during the generation phase. The particular innovation of this work is the neural ODE achieving both denoising and evolution for dynamics synthesis, with a proposed temporal training scheme. We study both relightable (BRDF) and non-relightable (RGB) appearance models. For both we introduce new pilot datasets, allowing, for the first time, to study such phenomena: For RGB we provide 22 dynamic textures acquired from free online sources; For BRDFs, we further acquire a dataset of 21 flash-lit videos of time-varying materials, enabled by a simple-to-construct setup. Our experiments show that our method consistently yields realistic and coherent results, whereas prior works falter under pronounced temporal appearance variations. A user study confirms our approach is preferred to previous work for such exemplars.},
  archive      = {J_TOG},
  doi          = {10.1145/3687900},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural differential appearance equations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural laplacian operator for 3D point clouds. <em>TOG</em>,
<em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrete Laplacian operator holds a crucial role in 3D geometry processing, yet it is still challenging to define it on point clouds. Previous works mainly focused on constructing a local triangulation around each point to approximate the underlying manifold for defining the Laplacian operator, which may not be robust or accurate. In contrast, we simply use the K- nearest neighbors (KNN) graph constructed from the input point cloud and learn the Laplacian operator on the KNN graph with graph neural networks (GNNs). However, the ground-truth Laplacian operator is defined on a manifold mesh with a different connectivity from the KNN graph and thus cannot be directly used for training. To train the GNN, we propose a novel training scheme by imitating the behavior of the ground-truth Laplacian operator on a set of probe functions so that the learned Laplacian operator behaves similarly to the ground-truth Laplacian operator. We train our network on a subset of ShapeNet and evaluate it across a variety of point clouds. Compared with previous methods, our method reduces the error by an order of magnitude and excels in handling sparse point clouds with thin structures or sharp features. Our method also demonstrates a strong generalization ability to unseen shapes. With our learned Laplacian operator, we further apply a series of Laplacian-based geometry processing algorithms directly to point clouds and achieve accurate results, enabling many exciting possibilities for geometry processing on point clouds. The code and trained models are available at https://github.com/IntelligentGeometry/NeLo.},
  archive      = {J_TOG},
  doi          = {10.1145/3687901},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural laplacian operator for 3D point clouds},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalized ray formulation for wave-optical light
transport. <em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ray optics is the foundation of modern path tracing and sampling algorithms for computer graphics; crucially, it allows high-performance implementations based on ray tracing. However, many applications of interest in computer graphics and computational optics demand a more precise understanding of light: as waves. For example, accurately modelling scattering effects like diffraction or interference requires a model that provides the coherence of light waves arriving at surfaces. While recent work in Physical Light Transport [Steinberg et al. 2022; Steinberg and Yan 2021] has introduced such a model, it requires tracing light paths starting from the light sources, which is often less efficient than tracing them from the sensor, and does not allow the use of many effective importance sampling techniques. We introduce a new model for wave optical light transport that is based on the fact that sensors aggregate the measurement of many light waves when capturing an image. This allows us to compactly represent the statistics of light waves in a generalized ray. Generalized rays allow sampling light paths starting from the sensor and applying sophisticated path tracing sampling techniques while still accurately modelling the wave nature of light. Our model is computationally efficient and straightforward to add to an existing path tracer; this offers the prospect of wave optics becoming the foundation of most renderers in the future. Using our model, we show that it is possible to render complex scenes under wave optics with high performance, which has not been possible with any existing method.},
  archive      = {J_TOG},
  doi          = {10.1145/3687902},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A generalized ray formulation for wave-optical light transport},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative portrait shadow removal. <em>TOG</em>,
<em>43</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3687903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a high-fidelity portrait shadow removal model that can effectively enhance the image of a portrait by predicting its appearance under disturbing shadows and highlights. Portrait shadow removal is a highly ill-posed problem where multiple plausible solutions can be found based on a single image. For example, disentangling complex environmental lighting from original skin color is a non-trivial problem. While existing works have solved this problem by predicting the appearance residuals that can propagate local shadow distribution, such methods are often incomplete and lead to unnatural predictions, especially for portraits with hard shadows. We overcome the limitations of existing local propagation methods by formulating the removal problem as a generation task where a diffusion model learns to globally rebuild the human appearance from scratch as a condition of an input portrait image. For robust and natural shadow removal, we propose to train the diffusion model with a compositional repurposing framework: a pre-trained text-guided image generation model is first fine-tuned to harmonize the lighting and color of the foreground with a background scene by using a background harmonization dataset; and then the model is further fine-tuned to generate a shadow-free portrait image via a shadow-paired dataset. To overcome the limitation of losing fine details in the latent diffusion model, we propose a guided-upsampling network to restore the original high-frequency details (e.g. , wrinkles and dots) from the input image. To enable our compositional training framework, we construct a high-fidelity and large-scale dataset using a lightstage capturing system and synthetic graphics simulation. Our generative framework effectively removes shadows caused by both self and external occlusions while maintaining original lighting distribution and high-frequency details. Our method also demonstrates robustness to diverse subjects captured in real environments.},
  archive      = {J_TOG},
  doi          = {10.1145/3687903},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generative portrait shadow removal},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CBIL: Collective behavior imitation learning for fish from
real videos. <em>TOG</em>, <em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reproducing realistic collective behaviors presents a captivating yet formidable challenge. Traditional rule-based methods rely on hand-crafted principles, limiting motion diversity and realism in generated collective behaviors. Recent imitation learning methods learn from data but often require ground-truth motion trajectories and struggle with authenticity, especially in high-density groups with erratic movements. In this paper, we present a scalable approach, Collective Behavior Imitation Learning (CBIL), for learning fish schooling behavior directly from videos , without relying on captured motion trajectories. Our method first leverages Video Representation Learning, in which a Masked Video AutoEncoder (MVAE) extracts implicit states from video inputs in a self-supervised manner. The MVAE effectively maps 2D observations to implicit states that are compact and expressive for following the imitation learning stage. Then, we propose a novel adversarial imitation learning method to effectively capture complex movements of the schools of fish, enabling efficient imitation of the distribution of motion patterns measured in the latent space. It also incorporates bio-inspired rewards alongside priors to regularize and stabilize training. Once trained, CBIL can be used for various animation tasks with the learned collective motion priors. We further show its effectiveness across different species. Finally, we demonstrate the application of our system in detecting abnormal fish behavior from in-the-wild videos.},
  archive      = {J_TOG},
  doi          = {10.1145/3687904},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {CBIL: Collective behavior imitation learning for fish from real videos},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Architectural co-LOD generation. <em>TOG</em>,
<em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Managing the level-of-detail (LOD) in architectural models is crucial yet challenging, particularly for effective representation and visualization of buildings. Traditional approaches often fail to deliver controllable detail alongside semantic consistency, especially when dealing with noisy and inconsistent inputs. We address these limitations with Co-LOD , a new approach specifically designed for effective LOD management in architectural modeling. Co-LOD employs shape co-analysis to standardize geometric structures across multiple buildings, facilitating the progressive and consistent generation of LODs. This method allows for precise detailing in both individual models and model collections, ensuring semantic integrity. Extensive experiments demonstrate that Co-LOD effectively applies accurate LOD across a variety of architectural inputs, consistently delivering superior detail and quality in LOD representations.},
  archive      = {J_TOG},
  doi          = {10.1145/3687905},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Architectural co-LOD generation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A flexible mold for facade panel fabrication. <em>TOG</em>,
<em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Architectural surface panelling often requires fabricating molds for panels, a process that can be cost-inefficient and material-wasteful when using traditional methods such as CNC milling. In this paper, we introduce a novel solution to generating molds for efficiently fabricating architectural panels. At the core of our method is a machine that utilizes a deflatable membrane as a flexible mold. By adjusting the deflation level and boundary element positions, the membrane can be reconfigured into various shapes, allowing for mass customization with significantly lower overhead costs. We devise an efficient algorithm that works in sync with our flexible mold machine that optimizes the placement of customizable boundary element positions, ensuring the fabricated panel matches the geometry of a given input shape: (1) Using a quadratic Weingarten surface arising from a natural assumption on the membrane&#39;s stress, we can approximate the initial placement of the boundary element from the input shape&#39;s geometry; (2) we solve the inverse problem with a simulator-in-the-loop optimizer by searching for the optimal placement of boundary curves with sensitivity analysis. We validate our approach by fabricating baseline panels and a facade with a wide range of curvature profiles, providing a detailed numerical analysis on simulation and fabrication, demonstrating significant advantages in cost and flexibility.},
  archive      = {J_TOG},
  doi          = {10.1145/3687906},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {A flexible mold for facade panel fabrication},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EgoHDM: A real-time egocentric-inertial human motion
capture, localization, and dense mapping system. <em>TOG</em>,
<em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present EgoHDM, an online egocentric-inertial human motion capture (mocap), localization, and dense mapping system. Our system uses 6 inertial measurement units (IMUs) and a commodity head-mounted RGB camera. EgoHDM is the first human mocap system that offers dense scene mapping in near real-time. Further, it is fast and robust to initialize and fully closes the loop between physically plausible map-aware global human motion estimation and mocap-aware 3D scene reconstruction. To achieve this, we design a tightly coupled mocap-aware dense bundle adjustment and physics-based body pose correction module leveraging a local body-centric elevation map. The latter introduces a novel terrain-aware contact PD controller, which enables characters to physically contact the given local elevation map thereby reducing human floating or penetration. We demonstrate the performance of our system on established synthetic and real-world benchmarks. The results show that our method reduces human localization, camera pose, and mapping accuracy error by 41%, 71%, 46%, respectively, compared to the state of the art. Our qualitative evaluations on newly captured data further demonstrate that EgoHDM can cover challenging scenarios in non-flat terrain including stepping over stairs and outdoor scenes in the wild. Our project page: https://handiyin.github.io/EgoHDM/},
  archive      = {J_TOG},
  doi          = {10.1145/3687907},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {EgoHDM: A real-time egocentric-inertial human motion capture, localization, and dense mapping system},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cubic barrier with elasticity-inclusive dynamic stiffness.
<em>TOG</em>, <em>43</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3687908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new cubic barrier with elasticity-inclusive dynamic stiffness for penetration-free contact resolution and strain limiting. We show that our method enlarges tight strain-limiting gaps where logarithmic barriers struggle and enables highly scalable contact-rich simulation.},
  archive      = {J_TOG},
  doi          = {10.1145/3687908},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {A cubic barrier with elasticity-inclusive dynamic stiffness},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TEXGen: A generative diffusion model for mesh textures.
<em>TOG</em>, <em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While high-quality texture maps are essential for realistic 3D asset rendering, few studies have explored learning directly in the texture space, especially on large-scale datasets. In this work, we depart from the conventional approach of relying on pre-trained 2D diffusion models for testtime optimization of 3D textures. Instead, we focus on the fundamental problem of learning in the UV texture space itself. For the first time, we train a large diffusion model capable of directly generating high-resolution texture maps in a feed-forward manner. To facilitate efficient learning in high-resolution UV spaces, we propose a scalable network architecture that interleaves convolutions on UV maps with attention layers on point clouds. Leveraging this architectural design, we train a 700 million parameter diffusion model that can generate UV texture maps guided by text prompts and single-view images. Once trained, our model naturally supports various extended applications, including text-guided texture inpainting, sparse-view texture completion, and text-driven texture synthesis. The code is available at https://github.com/CVMI-Lab/TEXGen.},
  archive      = {J_TOG},
  doi          = {10.1145/3687909},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {TEXGen: A generative diffusion model for mesh textures},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Volumetric homogenization for knitwear simulation.
<em>TOG</em>, <em>43</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3687911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents volumetric homogenization, a spatially varying homogenization scheme for knitwear simulation. We are motivated by the observation that macro-scale fabric dynamics is strongly correlated with its underlying knitting patterns. Therefore, homogenization towards a single material is less effective when the knitting is complex and non-repetitive. Our method tackles this challenge by homogenizing the yarn-level material locally at volumetric elements. Assigning a virtual volume of a knitting structure enables us to model bending and twisting effects via a simple volume-preserving penalty and thus effectively alleviates the material nonlinearity. We employ an adjoint Gauss-Newton formulation[Zehnder et al. 2021] to battle the dimensionality challenge of such per-element material optimization. This intuitive material model makes the forward simulation GPU-friendly. To this end, our pipeline also equips a novel domain-decomposed subspace solver crafted for GPU projective dynamics, which makes our simulator hundreds of times faster than the yarn-level simulator. Experiments validate the capability and effectiveness of volumetric homogenization. Our method produces realistic animations of knitwear matching the quality of full-scale yarn-level simulations. It is also orders of magnitude faster than existing homogenization techniques in both the training and simulation stages.},
  archive      = {J_TOG},
  doi          = {10.1145/3687911},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Volumetric homogenization for knitwear simulation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D reconstruction with fast dipole sums. <em>TOG</em>,
<em>43</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3687914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a method for high-quality 3D reconstruction from multi-view images. Our method uses a new point-based representation, the regularized dipole sum, which generalizes the winding number to allow for interpolation of per-point attributes in point clouds with noisy or outlier points. Using regularized dipole sums, we represent implicit geometry and radiance fields as per-point attributes of a dense point cloud, which we initialize from structure from motion. We additionally derive Barnes-Hut fast summation schemes for accelerated forward and adjoint dipole sum queries. These queries facilitate the use of ray tracing to efficiently and differentiably render images with our point-based representations, and thus update their point attributes to optimize scene geometry and appearance. We evaluate our method in inverse rendering applications against state-of-the-art alternatives, based on ray tracing of neural representations or rasterization of Gaussian point-based representations. Our method significantly improves 3D reconstruction quality and robustness at equal runtimes, while also supporting more general rendering methods such as shadow rays for direct illumination.},
  archive      = {J_TOG},
  doi          = {10.1145/3687914},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {3D reconstruction with fast dipole sums},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stripe embedding: Efficient maps with exact numeric
computation. <em>TOG</em>, <em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the fundamental problem of injectively mapping a surface mesh with disk topology onto a boundary constrained convex domain. We start from the basic observation that mapping a strip of triangles onto a rectangular shape always yields a valid embedding, if the vertices that bound the strip are sorted coherently along the sides of the rectangle. Based on this intuition, we propose a straightforward algorithm, called Stripe Embedding, that operates by decomposing the input mesh into a set of triangle strips and then embeds each strip into the target domain by means of linear interpolation between two previously embedded vertices. Thanks to its simplicity, Stripe Embedding is extremely efficient and permits to switch to an exact implementation without almost increasing its running times. Stripe Embedding is up to three orders of magnitude faster than the Tutte embedding for same numerical model and, even when implemented with costly rational numbers, it is faster than any floating point implementation of prior methods at any scale.},
  archive      = {J_TOG},
  doi          = {10.1145/3687915},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Stripe embedding: Efficient maps with exact numeric computation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PCO: Precision-controllable offset surfaces with sharp
features. <em>TOG</em>, <em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface offsetting is a crucial operation in digital geometry processing and computer-aided design, where an offset is defined as an iso-value surface of the distance field. A challenge emerges as even smooth surfaces can exhibit sharp features in their offsets due to the non-differentiable characteristics of the underlying distance field. Prevailing approaches to the offsetting problem involve approximating the distance field and then extracting the iso-surface. However, even with dual contouring (DC), there is a risk of degrading sharp feature points/lines due to the inaccurate discretization of the distance field. This issue is exacerbated when the input is a piecewise-linear triangle mesh. This study is inspired by the observation that a triangle-based distance field, unlike the complex distance field rooted at the entire surface, remains smooth across the entire 3D space except at the triangle itself. With a polygonal surface comprising n triangles, the final distance field for accommodating the offset surface is determined by minimizing these n triangle-based distance fields. In implementation, our approach starts by tetrahedralizing the space around the offset surface, enabling a tetrahedron-wise linear approximation for each triangle-based distance field. The final offset surface within a tetrahedral range can be traced by slicing the tetrahedron with planes. As illustrated in the teaser figure, a key advantage of our algorithm is its ability to precisely preserve sharp features. Furthermore, this paper addresses the problem of simplifying the offset surface&#39;s complexity while preserving sharp features, formulating it as a maximal-clique problem.},
  archive      = {J_TOG},
  doi          = {10.1145/3687920},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {PCO: Precision-controllable offset surfaces with sharp features},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GarVerseLOD: High-fidelity 3D garment reconstruction from a
single in-the-wild image using a dataset with levels of details.
<em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD) , spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches while being robust against a large variation of pose, illumination, occlusion, and deformation. Code and dataset are available at garverselod.github.io.},
  archive      = {J_TOG},
  doi          = {10.1145/3687921},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {GarVerseLOD: High-fidelity 3D garment reconstruction from a single in-the-wild image using a dataset with levels of details},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ParSEL: Parameterized shape editing with language.
<em>TOG</em>, <em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to edit 3D assets with natural language presents a compelling paradigm to aid in the democratization of 3D content creation. However, while natural language is often effective at communicating general intent, it is poorly suited for specifying exact manipulation. To address this gap, we introduce ParSEL, a system that enables controllable editing of high-quality 3D assets with natural language. Given a segmented 3D mesh and an editing request, ParSEL produces a parameterized editing program. Adjusting these parameters allows users to explore shape variations with exact control over the magnitude of the edits. To infer editing programs which align with an input edit request, we leverage the abilities of large-language models (LLMs). However, we find that although LLMs excel at identifying the initial edit operations, they often fail to infer complete editing programs, resulting in outputs that violate shape semantics. To overcome this issue, we introduce Analytical Edit Propagation (AEP), an algorithm which extends a seed edit with additional operations until a complete editing program has been formed. Unlike prior methods, AEP searches for analytical editing operations compatible with a range of possible user edits through the integration of computer algebra systems for geometric analysis. Experimentally, we demonstrate ParSEL&#39;s effectiveness in enabling controllable editing of 3D objects through natural language requests over alternative system designs.},
  archive      = {J_TOG},
  doi          = {10.1145/3687922},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {ParSEL: Parameterized shape editing with language},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GFFE: G-buffer free frame extrapolation for low-latency
real-time rendering. <em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time rendering has been embracing ever-demanding effects, such as ray tracing. However, rendering such effects in high resolution and high frame rate remains challenging. Frame extrapolation methods, which do not introduce additional latency as opposed to frame interpolation methods such as DLSS 3 and FSR 3, boost the frame rate by generating future frames based on previous frames. However, it is a more challenging task because of the lack of information in the disocclusion regions and complex future motions, and recent methods also have a high engine integration cost due to requiring G-buffers as input. We propose a G-buffer free frame extrapolation method, GFFE, with a novel heuristic framework and an efficient neural network, to plausibly generate new frames in real time without introducing additional latency. We analyze the motion of dynamic fragments and different types of disocclusions, and design the corresponding modules of the extrapolation block to handle them. After that, a light-weight shading correction network is used to correct shading and improve overall quality. GFFE achieves comparable or better results than previous interpolation and G-buffer dependent extrapolation methods, with more efficient performance and easier integration.},
  archive      = {J_TOG},
  doi          = {10.1145/3687923},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {GFFE: G-buffer free frame extrapolation for low-latency real-time rendering},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse rendering for tomographic volumetric additive
manufacturing. <em>TOG</em>, <em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tomographic Volumetric Additive Manufacturing (TVAM) is an emerging 3D printing technology that can create complex objects in under a minute. The key idea is to project intense light patterns onto a rotating vial of photo-sensitive resin, causing polymerization where the cumulative dose of these patterns reaches the polymerization threshold. We formulate the pattern calculation as an inverse light transport problem and solve it via physically based differentiable rendering. In doing so, we address longstanding limitations of prior work by accurately modeling and correcting for scattering in composite resins, printing in non-symmetric vials, and supporting unusual printing geometries. We also introduce an improved discretization scheme that exploits the ray tracing operation to mitigate resolution-related artifacts in prints. We demonstrate the benefits of our method in real-world experiments, where our computed patterns produce prints with an improved fidelity.},
  archive      = {J_TOG},
  doi          = {10.1145/3687924},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Inverse rendering for tomographic volumetric additive manufacturing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exact and efficient intersection resolution for mesh
arrangements. <em>TOG</em>, <em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to exactly and efficiently resolve intersections and self-intersections in triangle meshes. Our method contains two key components. First, we present a new concept of geometric predicates, called indirect offset predicates , to represent all intersection points through a new formulation and establish all necessary geometric predicates. Consequently, we reduce numerical errors in floating-point evaluations and improve the success rate of early stages of arithmetic filtering. Second, we develop localization and dimension reduction techniques for sorting, deduplicating, and locating the intersection points, thereby boosting efficiency and parallelism while maintaining accuracy. Rigorous testing confirms the robustness of our algorithm and consistency with previous methods. Comprehensive testing across diverse datasets further highlights the speed improvement achieved by our method, which is one order of magnitude faster than the state-of-the-art methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3687925},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Exact and efficient intersection resolution for mesh arrangements},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust dual gaussian splatting for immersive human-centric
volumetric videos. <em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed DualGS , for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers&#39; fingertips. Project page: https://nowheretrix.github.io/DualGS/.},
  archive      = {J_TOG},
  doi          = {10.1145/3687926},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Robust dual gaussian splatting for immersive human-centric volumetric videos},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GaussianHeads: End-to-end learning of drivable gaussian head
avatars from coarse-to-fine representations. <em>TOG</em>,
<em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time rendering of human head avatars is a cornerstone of many computer graphics applications, such as augmented reality, video games, and films, to name a few. Recent approaches address this challenge with computationally efficient geometry primitives in a carefully calibrated multi-view setup. Albeit producing photorealistic head renderings, they often fail to represent complex motion changes, such as the mouth interior and strongly varying head poses. We propose a new method to generate highly dynamic and deformable human head avatars from multi-view imagery in real time. At the core of our method is a hierarchical representation of head models that can capture the complex dynamics of facial expressions and head movements. First, with rich facial features extracted from raw input frames, we learn to deform the coarse facial geometry of the template mesh. We then initialize 3D Gaussians on the deformed surface and refine their positions in a fine step. We train this coarse-to-fine facial avatar model along with the head pose as learnable parameters in an end-to-end framework. This enables not only controllable facial animation via video inputs but also high-fidelity novel view synthesis of challenging facial expressions, such as tongue deformations and fine-grained teeth structure under large motion changes. Moreover, it encourages the learned head avatar to generalize towards new facial expressions and head poses at inference time. We demonstrate the performance of our method with comparisons against the related methods on different datasets, spanning challenging facial expression sequences across multiple identities. We also show the potential application of our approach by demonstrating a cross-identity facial performance transfer application. We make the code available on our project page.},
  archive      = {J_TOG},
  doi          = {10.1145/3687927},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {GaussianHeads: End-to-end learning of drivable gaussian head avatars from coarse-to-fine representations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chebyshev parameterization for woven fabric modeling.
<em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distortion-minimizing surface parameterization is an essential step for computing 2D pieces necessary to fabricate a target 3D shape from flat material. Garment design and textile fabrication are a prominent application example. Common distortion measures quantify length, angle or area preservation in an isotropic manner, so that when applied to woven textile fabrication, they implicitly assume fabric behaves like paper, which is inextensible in all directions and does not permit shearing. However, woven fabric differs significantly from paper: it exhibits anisotropy along the yarn directions and allows for some degree of shearing. We propose a novel distortion energy based on Chebyshev nets that anisotropically penalizes shearing and stretching. Our energy formulation can be used as an optimization objective for surface parameterization and is simple to minimize via a local-global algorithm. We demonstrate its advantages in modeling nets or woven fabric behavior over the commonly used isotropic distortion energies.},
  archive      = {J_TOG},
  doi          = {10.1145/3687928},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Chebyshev parameterization for woven fabric modeling},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying behavioral correlates to visual discomfort.
<em>TOG</em>, <em>43</em>(6), 1–10. (<a
href="https://doi.org/10.1145/3687929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outside of self-report surveys, there are no proven, reliable methods to quantify visual discomfort or visually induced motion sickness symptoms when using head-mounted displays. While valuable tools, self-report surveys suffer from potential biases and low sensitivity due to variability in how respondents may assess and report their experience. Consequently, extreme visual-vestibular conflicts are generally used to induce discomfort symptoms large enough to measure reliably with surveys (e.g., stationary participants riding virtual roller coasters). An emerging area of research is the prediction of discomfort survey results from physiological and behavioral markers. However, the signals derived from experimental paradigms that are explicitly designed to be uncomfortable may not generalize to more naturalistic experiences where comfort is prioritized. In this work we introduce a custom VR headset designed to introduce significant near-eye optical distortion (i.e., pupil swim) to induce visual discomfort during more typical VR experiences. We evaluate visual comfort in our headset while users play the popular VR title Job Simulator and show that eye-tracked dynamic distortion correction improves visual comfort in a multi-session, within-subjects user study. We additionally use representational similarity analysis to highlight changes in head and gaze behavior that are potentially more sensitive to visual discomfort than surveys.},
  archive      = {J_TOG},
  doi          = {10.1145/3687929},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Identifying behavioral correlates to visual discomfort},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DARTS: Diffusion approximated residual time sampling for
time-of-flight rendering in homogeneous scattering media. <em>TOG</em>,
<em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-of-flight (ToF) devices have greatly propelled the advancement of various multi-modal perception applications. However, achieving accurate rendering of time-resolved information remains a challenge, particularly in scenes involving complex geometries, diverse materials and participating media. Existing ToF rendering works have demonstrated notable results, yet they struggle with scenes involving scattering media and camera-warped settings. Other steady-state volumetric rendering methods exhibit significant bias or variance when directly applied to ToF rendering tasks. To address these challenges, we integrate transient diffusion theory into path construction and propose novel sampling methods for free-path distance and scattering direction, via resampled importance sampling and offline tabulation. An elliptical sampling method is further adapted to provide controllable vertex connection satisfying any required photon traversal time. In contrast to the existing temporal uniform sampling strategy, our method is the first to consider the contribution of transient radiance to importance-sample the full path, and thus enables improved temporal path construction under multiple scattering settings. The proposed method can be integrated into both path tracing and photon-based frameworks, delivering significant improvements in quality and efficiency with at least a 5x MSE reduction versus SOTA methods in equal rendering time.},
  archive      = {J_TOG},
  doi          = {10.1145/3687930},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {DARTS: Diffusion approximated residual time sampling for time-of-flight rendering in homogeneous scattering media},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StyleTex: Style image-guided texture generation for 3D
models. <em>TOG</em>, <em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Style-guided texture generation aims to generate a texture that is harmonious with both the style of the reference image and the geometry of the input mesh, given a reference style image and a 3D mesh with its text description. Although diffusion-based 3D texture generation methods, such as distillation sampling, have numerous promising applications in stylized games and films, it requires addressing two challenges: 1) decouple style and content completely from the reference image for 3D models, and 2) align the generated texture with the color tone, style of the reference image, and the given text prompt. To this end, we introduce StyleTex, an innovative diffusion-model-based framework for creating stylized textures for 3D models. Our key insight is to decouple style information from the reference image while disregarding content in diffusion-based distillation sampling. Specifically, given a reference image, we first decompose its style feature from the image CLIP embedding by subtracting the embedding&#39;s orthogonal projection in the direction of the content feature, which is represented by a text CLIP embedding. Our novel approach to disentangling the reference image&#39;s style and content information allows us to generate distinct style and content features. We then inject the style feature into the cross-attention mechanism to incorporate it into the generation process, while utilizing the content feature as a negative prompt to further dissociate content information. Finally, we incorporate these strategies into StyleTex to obtain stylized textures. We utilize Interval Score Matching to address over-smoothness and over-saturation, in combination with a geometry-aware ControlNet that ensures consistent geometry throughout the generative process. The resulting textures generated by StyleTex retain the style of the reference image, while also aligning with the text prompts and intrinsic details of the given 3D mesh. Quantitative and qualitative experiments show that our method outperforms existing baseline methods by a significant margin.},
  archive      = {J_TOG},
  doi          = {10.1145/3687931},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {StyleTex: Style image-guided texture generation for 3D models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CPoser: An optimization-after-parsing approach for
text-to-pose generation using large language models. <em>TOG</em>,
<em>43</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3687932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-pose generation is challenging due to the complexity of natural language and human posture semantics. Utilizing large language models (LLMs) for text-to-pose generation is appealing due to their strong capabilities in text understanding and reasoning. However, as LLMs are designed for general-purpose language processing and not specifically trained for pose generation, it remains nontrivial to generate precise articulation targets for the full body using LLMs directly. To this end, we propose CPoser, a novel approach to harness the power of LLMs for text-to-pose generation, featuring a prompt parsing stage and a pose optimization stage. The parsing stage utilizes LLMs to turn text prompts into pose intermediate representations (Pose-IRs) through a set of predefined structured queries. These Pose-IRs explicitly describe specific pose conditions, such as squatting depth and knee bending angle, naturally forming an objective function that a target pose should satisfy. The optimization stage solves for expressive poses and hand gestures based on the Pose-IR objective function via robust optimization in a quantized pose prior space. The results are further refined to enhance naturalness and incorporate facial expressions. Experiments show that our approach effectively understands diverse text prompts for pose generation, surpassing existing text-to-pose methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3687932},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {CPoser: An optimization-after-parsing approach for text-to-pose generation using large language models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning based toolpath planner on diverse graphs for 3D
printing. <em>TOG</em>, <em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a learning based planner for computing optimized 3D printing toolpaths on prescribed graphs, the challenges of which include the varying graph structures on different models and the large scale of nodes &amp; edges on a graph. We adopt an on-the-fly strategy to tackle these challenges, formulating the planner as a Deep Q-Network (DQN) based optimizer to decide the next &#39;best&#39; node to visit. We construct the state spaces by the Local Search Graph (LSG) centered at different nodes on a graph, which is encoded by a carefully designed algorithm so that LSGs in similar configurations can be identified to re-use the earlier learned DQN priors for accelerating the computation of toolpath planning. Our method can cover different 3D printing applications by defining their corresponding reward functions. Toolpath planning problems in wire-frame printing, continuous fiber printing, and metallic printing are selected to demonstrate its generality. The performance of our planner has been verified by testing the resultant toolpaths in physical experiments. By using our planner, wire-frame models with up to 4.2k struts can be successfully printed, up to 93.3% of sharp turns on continuous fiber toolpaths can be avoided, and the thermal distortion in metallic printing can be reduced by 24.9%.},
  archive      = {J_TOG},
  doi          = {10.1145/3687933},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning based toolpath planner on diverse graphs for 3D printing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D gaussian ray tracing: Fast tracing of particle scenes.
<em>TOG</em>, <em>43</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3687934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle-based representations of radiance fields such as 3D Gaussian Splatting have found great success for reconstructing and re-rendering of complex scenes. Most existing methods render particles via rasterization, projecting them to screen space tiles for processing in a sorted order. This work instead considers ray tracing the particles, building a bounding volume hierarchy and casting a ray for each pixel using high-performance GPU ray tracing hardware. To efficiently handle large numbers of semi-transparent particles, we describe a specialized rendering algorithm which encapsulates particles with bounding meshes to leverage fast ray-triangle intersections, and shades batches of intersections in depth-order. The benefits of ray tracing are well-known in computer graphics: processing incoherent rays for secondary lighting effects such as shadows and reflections, rendering from highly-distorted cameras common in robotics, stochastically sampling rays, and more. With our renderer, this flexibility comes at little cost compared to rasterization. Experiments demonstrate the speed and accuracy of our approach, as well as several applications in computer graphics and vision. We further propose related improvements to the basic Gaussian representation, including a simple use of generalized kernel functions which significantly reduces particle hit counts.},
  archive      = {J_TOG},
  doi          = {10.1145/3687934},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {3D gaussian ray tracing: Fast tracing of particle scenes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). V^3: Viewing volumetric videos on mobiles via streamable 2D
dynamic gaussians. <em>TOG</em>, <em>43</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3687935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experiencing high-fidelity volumetric video as seamlessly as 2D videos is a long-held dream. However, current dynamic 3DGS methods, despite their high rendering quality, face challenges in streaming on mobile devices due to computational and bandwidth constraints. In this paper, we introduce V 3 (Viewing Volumetric Videos), a novel approach that enables high-quality mobile rendering through the streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as 2D videos, facilitating the use of hardware video codecs. Additionally, we propose a two-stage training strategy to reduce storage requirements with rapid training speed. The first stage employs hash encoding and shallow MLP to learn motion, then reduces the number of Gaussians through pruning to meet the streaming requirements, while the second stage fine tunes other Gaussian attributes using residual entropy loss and temporal loss to improve temporal continuity. This strategy, which disentangles motion and appearance, maintains high rendering quality with compact storage requirements. Meanwhile, we designed a multi-platform player to decode and render 2D Gaussian videos. Extensive experiments demonstrate the effectiveness of V 3 , outperforming other methods by enabling high-quality rendering and streaming on common devices, which is unseen before. As the first to stream dynamic Gaussians on mobile devices, our companion player offers users an unprecedented volumetric video experience, including smooth scrolling and instant sharing. Our project page with source code is available at https://authoritywang.github.io/v3/.},
  archive      = {J_TOG},
  doi          = {10.1145/3687935},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {V^3: Viewing volumetric videos on mobiles via streamable 2D dynamic gaussians},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direct manipulation of procedural implicit surfaces.
<em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural implicit surfaces are a popular representation for shape modeling. They provide a simple framework for complex geometric operations such as Booleans, blending and deformations. However, their editability remains a challenging task: as the definition of the shape is purely implicit, direct manipulation of the shape cannot be performed. Thus, parameters of the model are often exposed through abstract sliders, which have to be nontrivially created by the user and understood by others for each individual model to modify. Further, each of these sliders needs to be set one by one to achieve the desired appearance. To circumvent this laborious process while preserving editability, we propose to directly manipulate the implicit surface in the viewport. We let the user naturally interact with the output shape, leveraging points on a co-parameterization we design specifically for implicit surfaces, to guide the parameter updates and reach the desired appearance faster. We leverage our automatic differentiation of the procedural implicit surface to propagate interactions made by the user in the viewport to the shape parameters themselves. We further design a solver that uses such information to guide an intuitive and smooth user workflow. We demonstrate different editing processes across multiple implicit shapes and parameters that would be tedious by tuning sliders.},
  archive      = {J_TOG},
  doi          = {10.1145/3687936},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Direct manipulation of procedural implicit surfaces},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian opacity fields: Efficient adaptive surface
reconstruction in unbounded scenes. <em>TOG</em>, <em>43</em>(6), 1–13.
(<a href="https://doi.org/10.1145/3687937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and adaptive surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing Marching Tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene&#39;s complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to or even outperforms, neural implicit methods in both quality and speed.},
  archive      = {J_TOG},
  doi          = {10.1145/3687937},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Gaussian opacity fields: Efficient adaptive surface reconstruction in unbounded scenes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online neural denoising with cross-regression for
interactive rendering. <em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating a rendered image sequence through Monte Carlo ray tracing is an appealing option when one aims to accurately simulate various lighting effects. Unfortunately, interactive rendering scenarios limit the allowable sample size for such sampling-based light transport algorithms, resulting in an unbiased but noisy image sequence. Image denoising has been widely adopted as a post-sampling process to convert such noisy image sequences into biased but temporally stable ones. The state-of-the-art strategy for interactive image denoising involves devising a deep neural network and training this network via supervised learning, i.e., optimizing the network parameters using training datasets that include an extensive set of image pairs (noisy and ground truth images). This paper adopts the prevalent approach for interactive image denoising, which relies on a neural network. However, instead of supervised learning, we propose a different learning strategy that trains our network parameters on the fly, i.e., updating them online using runtime image sequences. To achieve our denoising objective with online learning, we tailor local regression to a cross-regression form that can guide robust training of our denoising neural network. We demonstrate that our denoising framework effectively reduces noise in input image sequences while robustly preserving both geometric and non-geometric edges, without requiring the manual effort involved in preparing an external dataset.},
  archive      = {J_TOG},
  doi          = {10.1145/3687938},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Online neural denoising with cross-regression for interactive rendering},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quad mesh mechanisms. <em>TOG</em>, <em>43</em>(6), 1–17.
(<a href="https://doi.org/10.1145/3687939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides computational tools for the modeling and design of quad mesh mechanisms, which are meshes allowing continuous flexions under the assumption of rigid faces and hinges in the edges. We combine methods and results from different areas, namely differential geometry of surfaces, rigidity and flexibility of bar and joint frameworks, algebraic geometry, and optimization. The basic idea to achieve a time-continuous flexion is time-discretization justified by an algebraic degree argument. We are able to prove computationally feasible bounds on the number of required time instances we need to incorporate in our optimization. For optimization to succeed, an informed initialization is crucial. We present two computational pipelines to achieve that: one based on remeshing isometric surface pairs, another one based on iterative refinement. A third manner of initialization proved very effective: We interactively design meshes which are close to a narrow known class of flexible meshes, but not contained in it. Having enjoyed sufficiently many degrees of freedom during design, we afterwards optimize towards flexibility.},
  archive      = {J_TOG},
  doi          = {10.1145/3687939},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Quad mesh mechanisms},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing triangle meshes with controlled roughness.
<em>TOG</em>, <em>43</em>(6), 1–20. (<a
href="https://doi.org/10.1145/3687940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the emergence of rough surfaces in various areas of design, we address the computational design of triangle meshes with controlled roughness. Our focus lies on small levels of roughness. There, roughness or smoothness mainly arises through the local positioning of the mesh edges and faces with respect to the curvature behavior of the reference surface. The analysis of this interaction between curvature and roughness is simplified by a 2D dual diagram and its generation within so-called isotropic geometry, which may be seen as a structure-preserving simplification of Euclidean geometry. Isotropic dihedral angles of the mesh are close to the Euclidean angles and appear as Euclidean edge lengths in the dual diagram, which also serves as a tool for visualization and interactive local design. We present a computational framework that includes appearance-aware remeshing, optimization-based automatic roughening, and control of dihedral angles.},
  archive      = {J_TOG},
  doi          = {10.1145/3687940},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Designing triangle meshes with controlled roughness},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SKEL-betweener: A neural motion rig for interactive motion
authoring. <em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authoring 3D motions is a laborious process that requires manipulating and coordinating many control handles over time. Neural motion representations learned from large motion datasets have recently shown impressive capabilities in many motion completion tasks. However, current methods are not designed for interactive motion authoring workflows. The reasons being their requirement of a dense context of full poses, which takes considerable time to author, as well as their lack of joint-level controls for refinement. In this paper, we introduce a Neural Motion Rig called SKEL-Betweener, tailored to interactive motion authoring. SKEL-Betweener is able to generate long motion sequences from two poses only, and enables intermediate motion authoring via neural motion curves---intuitive joint-level controls for positions and orientations. Through user evaluations, we demonstrate the effectiveness of our Neural Motion Rig for efficiently creating and editing motions.},
  archive      = {J_TOG},
  doi          = {10.1145/3687941},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {SKEL-betweener: A neural motion rig for interactive motion authoring},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximation by meshes with spherical faces. <em>TOG</em>,
<em>43</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3687942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meshes with spherical faces and circular edges are an attractive alternative to polyhedral meshes for applications in architecture and design. Approximation of a given surface by such a mesh needs to consider the visual appearance, approximation quality, the position and orientation of circular intersections of neighboring faces and the existence of a torsion free support structure that is formed by the planes of circular edges. The latter requirement implies that the mesh simultaneously defines a second mesh whose faces lie on the same spheres as the faces of the first mesh. It is a discretization of the two envelopes of a sphere congruence, i.e., a two-parameter family of spheres. We relate such sphere congruences to torsal parameterizations of associated line congruences. Turning practical requirements into properties of such a line congruence, we optimize line and sphere congruence as a basis for computing a mesh with spherical triangular or quadrilateral faces that approximates a given reference surface.},
  archive      = {J_TOG},
  doi          = {10.1145/3687942},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Approximation by meshes with spherical faces},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient image-space shape splatting for monte carlo
rendering. <em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A typical Monte Carlo rendering method contributes one light path only to a single pixel at a time. Reusing light paths across multiple pixels, however, can amortize the cost and improve the efficiency. The state of the art of path reuse is to employ shift mapping to reduce the cost of path reuse, while its computation cost is still proportional to the number of pixels processed in shift mapping. We propose a general framework for efficiently reusing light paths to multiple pixels arranged in arbitrary two-dimensional shapes. Our shape is defined as a set of multiple pixels, and the framework allows us to reuse light paths among pixels in a shape faster than simply evaluating all pixels via shift mapping. The key idea is to sparsely evaluate the contribution of shifted paths at random pixels within the shape and interpolate the contribution to the other pixels. We apply a debiasing estimator to ensure unbiasedness. Our method can be integrated with many existing rendering methods and brings consistent improvement over its single-pixel counterpart.},
  archive      = {J_TOG},
  doi          = {10.1145/3687943},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient image-space shape splatting for monte carlo rendering},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic normal orientation for point clouds.
<em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple yet effective method to orient normals for point clouds. Central to our approach is a novel optimization objective function defined from global and local perspectives. Globally, we introduce a signed uncertainty function that distinguishes the inside and outside of the underlying surface. Moreover, benefiting from the statistics of our global term, we present a local orientation term instead of a global one. The optimization problem can be solved by the commonly used numerical optimization solver, such as L-BFGS. The capability and feasibility of our approach are demonstrated over various complex point clouds. We achieve higher practical robustness and normal quality than the state-of-the-art methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3687944},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Stochastic normal orientation for point clouds},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Still-moving: Customized video generation without customized
video data. <em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customizing text-to-image (T2I) models has seen tremendous progress recently, particularly in areas such as personalization, stylization, and conditional generation. However, expanding this progress to video generation is still in its infancy, primarily due to the lack of customized video data. In this work, we introduce Still-Moving, a novel generic framework for customizing a text-to-video (T2V) model, without requiring any customized video data. The framework applies to the prominent T2V design where the video model is built over a T2I model (e.g., via inflation). We assume access to a customized version of the T2I model, trained only on still image data (e.g., using DreamBooth). Naively plugging in the weights of the customized T2I model into the T2V model often leads to significant artifacts or insufficient adherence to the customization data. To overcome this issue, we train lightweight Spatial Adapters that adjust the features produced by the injected T2I layers. Importantly, our adapters are trained on &quot;frozen videos&quot; (i.e., repeated images), constructed from image samples generated by the customized T2I model. This training is facilitated by a novel Motion Adapter module, which allows us to train on such static videos while preserving the motion prior of the video model. At test time, we remove the Motion Adapter modules and leave in only the trained Spatial Adapters. This restores the motion prior of the T2V model while adhering to the spatial prior of the customized T2I model. We demonstrate the effectiveness of our approach on diverse tasks including personalized, stylized, and conditional generation. In all evaluated scenarios, our method seamlessly integrates the spatial prior of the customized T2I model with a motion prior supplied by the T2V model.},
  archive      = {J_TOG},
  doi          = {10.1145/3687945},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Still-moving: Customized video generation without customized video data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trading spaces: Adaptive subspace time integration for
contacting elastodynamics. <em>TOG</em>, <em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a subspace simulator that adaptively balances solution improvement against system size. The core components of our simulator are an adaptive subspace oracle, model, and parallel time-step solver algorithm. Our in-time-step adaptivity oracle continually assesses subspace solution quality and candidate update proposals while accounting for temporal variations in deformation and spatial variations in material. In turn our adaptivity model is subspace agnostic. It allows application across subspace representations and expresses unrestricted deformations independent of subspace choice. We couple our oracle and model with a custom-constructed parallel time-step solver for our enriched systems that exposes a pair of user tolerances which provide controllable simulation quality. As tolerances are tightened our model converges to full-space solutions (with expected cost increases). On the other hand, as tolerances are relaxed we obtain output-bound simulation costs. We demonstrate the efficacy of our approach across a wide range of challenging nonlinear materials models, material stiffnesses, heterogeneities, dynamic behaviors, and frictionally contacting conditions, obtaining scalable and efficient simulations of complex elastodynamic scenarios.},
  archive      = {J_TOG},
  doi          = {10.1145/3687946},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Trading spaces: Adaptive subspace time integration for contacting elastodynamics},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). All you need is rotation: Construction of developable
strips. <em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach to generate developable strips along a space curve. The key idea of the new method is to use the rotation angle between the Frenet frame of the input space curve, and its Darboux frame of the curve on the resulting developable strip as a free design parameter, thereby revolving the strip around the tangential axis of the input space curve. This angle is not restricted to be constant but it can be any differentiable function defined on the curve, thereby creating a large design space of developable strips that share a common directrix curve. The range of possibilities for choosing the rotation angle is diverse, encompassing constant angles, linearly varying angles, sinusoidal patterns, and even solutions derived from initial value problems involving ordinary differential equations. This enables the potential of the proposed method to be used for a wide range of practical applications, spanning fields such as architectural design, industrial design, and papercraft modeling. In our computational and physical examples, we demonstrate the flexibility of the method by constructing, among others, toroidal and helical windmill blades for papercraft models, curved foldings, triply orthogonal structures, and developable strips featuring a log-aesthetic directrix curve.},
  archive      = {J_TOG},
  doi          = {10.1145/3687947},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {All you need is rotation: Construction of developable strips},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UFO instruction graphs are machine knittable. <em>TOG</em>,
<em>43</em>(6), 1–22. (<a
href="https://doi.org/10.1145/3687948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming low-level controls for knitting machines is a meticulous, time-consuming task that demands specialized expertise. Recently, there has been a shift towards automatically generating low-level knitting machine programs from high-level knit representations that describe knit objects in a more intuitive, user-friendly way. Current high-level systems trade off expressivity for ease-of-use, requiring ad-hoc trapdoors to access the full space of machine capabilities, or eschewing completeness in the name of utility. Thus, advanced techniques either require ad-hoc extensions from domain experts, or are entirely unsupported. Furthermore, errors may emerge during the compilation from knit object representations to machine instructions. While the generated program may describe a valid machine control sequence, the fabricated object is topologically different from the specified input, with little recourse for understanding and fixing the issue. To address these limitations, we introduce instruction graphs , an intermediate representation capable of capturing the full range of machine knitting programs. We define a semantic mapping from instruction graphs to fenced tangles, which make them compatible with the established formal semantics for machine knitting instructions. We establish a semantics-preserving bijection between machine knittable instruction graphs and knit programs that proves three properties - upward, forward, and ordered (UFO) - are both necessary and sufficient to ensure the existence of a machine knitting program that can fabricate the fenced tangle denoted by the graph. As a proof-of-concept, we implement an instruction graph editor and compiler that allows a user to transform an instruction graph into UFO presentation and then compile it to a machine program, all while maintaining semantic equivalence. In addition, we use the UFO properties to more precisely characterize the limitations of existing compilers. This work lays the groundwork for more expressive and reliable automated knitting machine programming systems by providing a formal characterization of machine knittability.},
  archive      = {J_TOG},
  doi          = {10.1145/3687948},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {ACM Trans. Graph.},
  title        = {UFO instruction graphs are machine knittable},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural kernel regression for consistent monte carlo
denoising. <em>TOG</em>, <em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unbiased Monte Carlo path tracing that is extensively used in realistic rendering produces undesirable noise, especially with low samples per pixel (spp). Recently, several methods have coped with this problem by importing unbiased noisy images and auxiliary features to neural networks to either predict a fixed-sized kernel for convolution or directly predict the denoised result. Since it is impossible to produce arbitrarily high spp images as the training dataset, the network-based denoising fails to produce high-quality images under high spp. More specifically, network-based denoising is inconsistent and does not converge to the ground truth as the sampling rate increases. On the other hand, the post-correction estimators yield a blending coefficient for a pair of biased and unbiased images influenced by image errors or variances to ensure the consistency of the denoised image. As the sampling rate increases, the blending coefficient of the unbiased image converges to 1, that is, using the unbiased image as the denoised results. However, these estimators usually produce artifacts due to the difficulty of accurately predicting image errors or variances with low spp. To address the above problems, we take advantage of both kernel-predicting methods and post-correction denoisers. A novel kernel-based denoiser is proposed based on distribution-free kernel regression consistency theory, which does not explicitly combine the biased and unbiased results but constrain the kernel bandwidth to produce consistent results under high spp. Meanwhile, our kernel regression method explores bandwidth optimization in the robust auxiliary feature space instead of the noisy image space. This leads to consistent high-quality denoising at both low and high spp. Experiment results demonstrate that our method outperforms existing denoisers in accuracy and consistency.},
  archive      = {J_TOG},
  doi          = {10.1145/3687949},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural kernel regression for consistent monte carlo denoising},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bijective volumetric mapping via star decomposition.
<em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method for the construction of bijective volumetric maps between 3D shapes is presented. Arbitrary shapes of ball-topology are supported, overcoming restrictions of previous methods to convex or star-shaped targets. In essence, the mapping problem is decomposed into a set of simpler mapping problems, each of which can be solved with previous methods for discrete star-shaped mapping problems. Addressing the key challenges in this endeavor, algorithms are described to reliably construct structurally compatible partitions of two shapes with constraints regarding star-shapedness and to compute a parsimonious common refinement of two triangulations.},
  archive      = {J_TOG},
  doi          = {10.1145/3687950},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Bijective volumetric mapping via star decomposition},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MaskedMimic: Unified physics-based character control through
masked motion inpainting. <em>TOG</em>, <em>43</em>(6), 1–21. (<a
href="https://doi.org/10.1145/3687951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crafting a single, versatile physics-based controller that can breathe life into interactive characters across a wide spectrum of scenarios represents an exciting frontier in character animation. An ideal controller should support diverse control modalities, such as sparse target keyframes, text instructions, and scene information. While previous works have proposed physically simulated, scene-aware control models, these systems have predominantly focused on developing controllers that each specializes in a narrow set of tasks and control modalities. This work presents MaskedMimic, a novel approach that formulates physics-based character control as a general motion inpainting problem. Our key insight is to train a single unified model to synthesize motions from partial (masked) motion descriptions, such as masked keyframes, objects, text descriptions, or any combination thereof. This is achieved by leveraging motion tracking data and designing a scalable training method that can effectively utilize diverse motion descriptions to produce coherent animations. Through this process, our approach learns a physics-based controller that provides an intuitive control interface without requiring tedious reward engineering for all behaviors of interest. The resulting controller supports a wide range of control modalities and enables seamless transitions between disparate tasks. By unifying character control through motion inpainting, MaskedMimic creates versatile virtual characters. These characters can dynamically adapt to complex scenes and compose diverse motions on demand, enabling more interactive and immersive experiences.},
  archive      = {J_TOG},
  doi          = {10.1145/3687951},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {MaskedMimic: Unified physics-based character control through masked motion inpainting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3DGSR: Implicit surface reconstruction with 3D gaussian
splatting. <em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is to incorporate an implicit signed distance field (SDF) within 3D Gaussians for surface modeling, and to enable the alignment and joint optimization of both SDF and 3D Gaussians. To achieve this, we design coupling strategies that align and associate the SDF with 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. With alignment, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only offers sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with that derived from 3DGS. In sum, these two designs allow SDF and 3DGS to be aligned, jointly optimized, and mutually boosted. Our extensive experimental results demonstrate that our 3DGSR enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities.},
  archive      = {J_TOG},
  doi          = {10.1145/3687952},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {3DGSR: Implicit surface reconstruction with 3D gaussian splatting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quark: Real-time, high-resolution, and general neural view
synthesis. <em>TOG</em>, <em>43</em>(6), 1–20. (<a
href="https://doi.org/10.1145/3687953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel neural algorithm for performing high-quality, highresolution, real-time novel view synthesis. From a sparse set of input RGB images or videos streams, our network both reconstructs the 3D scene and renders novel views at 1080p resolution at 30fps on an NVIDIA A100. Our feed-forward network generalizes across a wide variety of datasets and scenes and produces state-of-the-art quality for a real-time method. Our quality approaches, and in some cases surpasses, the quality of some of the top offline methods. In order to achieve these results we use a novel combination of several key concepts, and tie them together into a cohesive and effective algorithm. We build on previous works that represent the scene using semi-transparent layers and use an iterative learned render-and-refine approach to improve those layers. Instead of flat layers, our method reconstructs layered depth maps (LDMs) that efficiently represent scenes with complex depth and occlusions. The iterative update steps are embedded in a multi-scale, UNet-style architecture to perform as much compute as possible at reduced resolution. Within each update step, to better aggregate the information from multiple input views, we use a specialized Transformer-based network component. This allows the majority of the per-input image processing to be performed in the input image space, as opposed to layer space, further increasing efficiency. Finally, due to the real-time nature of our reconstruction and rendering, we dynamically create and discard the internal 3D geometry for each frame, generating the LDM for each view. Taken together, this produces a novel and effective algorithm for view synthesis. Through extensive evaluation, we demonstrate that we achieve state-of-the-art quality at real-time rates.},
  archive      = {J_TOG},
  doi          = {10.1145/3687953},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Quark: Real-time, high-resolution, and general neural view synthesis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing the aesthetics of 3D shapes via reference-based
editing. <em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While there have been previous works that explored methods to enhance the aesthetics of images, the automated beautification of 3D shapes has been limited to specific shapes such as 3D face models. In this paper, we introduce a framework to automatically enhance the aesthetics of general 3D shapes. Our approach employs a reference-based beautification strategy. We first performed data collection to gather the aesthetics ratings of various 3D shapes to create a 3D shape aesthetics dataset. Then we perform reference-based editing to edit the input shape and beautify it by making it look more like some reference shape that is aesthetic. Specifically, we propose a reference-guided global deformation framework to coherently deform the input shape such that its structural proportions will be closer to those of the reference shape. We then optionally transplant some local aesthetic parts from the reference to the input to obtain the beautified output shapes. Comparisons show that our reference-guided 3D deformation algorithm outperforms existing techniques. Furthermore, quantitative and qualitative evaluations demonstrate that the performance of our aesthetics enhancement framework is consistent with both human perception and existing 3D shape aesthetics assessment.},
  archive      = {J_TOG},
  doi          = {10.1145/3687954},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Enhancing the aesthetics of 3D shapes via reference-based editing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skeleton-driven inbetweening of bitmap character drawings.
<em>TOG</em>, <em>43</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3687955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the primary reasons for the high cost of traditional animation is the inbetweening process, where artists manually draw each intermediate frame necessary for smooth motion. Making this process more efficient has been at the core of computer graphics research for years, yet the industry has adopted very few solutions. Most existing solutions either require vector input or resort to tight inbetweening; often, they attempt to fully automate the process. In industry, however, keyframes are often spaced far apart, drawn in raster format, and contain occlusions. Moreover, inbetweening is fundamentally an artistic process, so the artist should maintain high-level control over it. We address these issues by proposing a novel inbetweening system for bitmap character drawings, supporting both tight and far inbetweening. In our setup, the artist can control motion by animating a skeleton between the keyframe poses. Our system then performs skeleton-based deformation of the bitmap drawings into the same pose and employs discrete optimization and deep learning to blend the deformed images. Besides the skeleton and the two drawn bitmap keyframes, we require very little annotation. However, deforming drawings with occlusions is complex, as it requires a piecewise smooth deformation field. To address this, we observe that this deformation field is smooth when the drawing is lifted into 3D. Our system therefore optimizes topology of a 2.5D partially layered template that we use to lift the drawing into 3D and get the final piecewise-smooth deformaton, effectively resolving occlusions. We validate our system through a series of animations, qualitative and quantitative comparisons, and user studies, demonstrating that our approach consistently outperforms the state of the art and our results are consistent with the viewers&#39; perception. Code and data for our paper are available at http://www-labs.iro.umontreal.ca/~bmpix/inbetweening/.},
  archive      = {J_TOG},
  doi          = {10.1145/3687955},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Skeleton-driven inbetweening of bitmap character drawings},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Surface reconstruction using rotation systems. <em>TOG</em>,
<em>43</em>(6), 1–22. (<a
href="https://doi.org/10.1145/3687956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the seminal result that a graph and an associated rotation system uniquely determine the topology of a closed manifold, we propose a combinatorial method for reconstruction of surfaces from points. Our method constructs a spanning tree and a rotation system. Since the tree is trivially a planar graph, its rotation system determines a genus zero surface with a single face which we proceed to incrementally refine by inserting edges to split faces. In order to raise the genus, special handles are added in a later stage by inserting edges between different faces and thus merging them. We apply our method to a wide range of input point clouds in order to investigate its effectiveness, and we compare our method to several other surface reconstruction methods. It turns out that our approach has two specific benefits over these other methods. First, the output mesh preserves the most information from the input point cloud. Second, our method provides control over the topology of the reconstructed surface. Code is available on https://github.com/cuirq3/RsR.},
  archive      = {J_TOG},
  doi          = {10.1145/3687956},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {ACM Trans. Graph.},
  title        = {Surface reconstruction using rotation systems},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SGEdit: Bridging LLM with Text2Image generative model for
scene graph-based image editing. <em>TOG</em>, <em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graphs offer a structured, hierarchical representation of images, with nodes and edges symbolizing objects and the relationships among them. It can serve as a natural interface for image editing, dramatically improving precision and flexibility. Leveraging this benefit, we introduce a new framework that integrates large language model (LLM) with Text2Image generative model for scene graph-based image editing. This integration enables precise modifications at the object level and creative recomposition of scenes without compromising overall image integrity. Our approach involves two primary stages: 1) Utilizing a LLM-driven scene parser, we construct an image&#39;s scene graph, capturing key objects and their interrelationships, as well as parsing fine-grained attributes such as object masks and descriptions. These annotations facilitate concept learning with a fine-tuned diffusion model, representing each object with an optimized token and detailed description prompt. 2) During the image editing phase, a LLM editing controller guides the edits towards specific areas. These edits are then implemented by an attention-modulated diffusion editor, utilizing the fine-tuned model to perform object additions, deletions, replacements, and adjustments. Through extensive experiments, we demonstrate that our framework significantly outperforms existing image editing methods in terms of editing precision and scene aesthetics. Our code is available at https://bestzzhang.github.io/SGEdit.},
  archive      = {J_TOG},
  doi          = {10.1145/3687957},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {SGEdit: Bridging LLM with Text2Image generative model for scene graph-based image editing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable photon mapping using generalized path
gradients. <em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photon mapping is a fundamental and practical Monte Carlo rendering technique for efficiently simulating global illumination effects, especially for caustics and specular-diffuse-specular (SDS) paths. In this paper, we present the first differentiable rendering method for photon mapping. The core of our method is a newly introduced concept named generalized path gradients. Based on the extended path space manifolds (EPSMs) [Xing et al. 2023], the generalized path gradients define the derivatives of the vertex positions and color contributions of a path with respect to scene parameters under given geometric constraints. By formalizing photon mapping as a path sampling technique through vertex merging [Georgiev et al. 2012] and incorporating a smooth differentiable density estimation kernel, we enable the differentiation of the photon mapping algorithms based on the theoretical results of generalized path gradients. Experiments demonstrate that our method is more effective than state-of-the-art physics-based differentiable rendering methods in inverse rendering applications involving difficult illumination paths, especially SDS paths.},
  archive      = {J_TOG},
  doi          = {10.1145/3687958},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable photon mapping using generalized path gradients},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solid-fluid interaction on particle flow maps. <em>TOG</em>,
<em>43</em>(6), 1–20. (<a
href="https://doi.org/10.1145/3687959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel solid-fluid interaction method for coupling elastic solids with impulse flow maps. Our key idea is to unify the representation of fluid and solid components as particle flow maps with different lengths and dynamics. The solid-fluid coupling is enabled by implementing two novel mechanisms: first, we developed an impulse-to-velocity transfer mechanism to unify the exchanged physical quantities; second, we devised a particle path integral mechanism to accumulate coupling forces along each flow-map trajectory. Our framework integrates these two mechanisms into an Eulerian-Lagrangian impulse fluid simulator to accommodate traditional coupling models, exemplified by the Material Point Method (MPM) and Immersed Boundary Method (IBM), within a particle flow map framework. We demonstrate our method&#39;s efficacy by simulating solid-fluid interactions exhibiting strong vortical dynamics, including various vortex shedding and interaction examples across swimming, falling, breezing, and combustion.},
  archive      = {J_TOG},
  doi          = {10.1145/3687959},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Solid-fluid interaction on particle flow maps},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A time-dependent inclusion-based method for continuous
collision detection between parametric surfaces. <em>TOG</em>,
<em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous collision detection (CCD) between parametric surfaces is typically formulated as a five-dimensional constrained optimization problem. In the field of CAD and computer graphics, common approaches to solving this problem rely on linearization or sampling strategies. Alternatively, inclusion-based techniques detect collisions by employing 5D inclusion functions, which are typically designed to represent the swept volumes of parametric surfaces over a given time span, and narrowing down the earliest collision moment through subdivision in both spatial and temporal dimensions. However, when high detection accuracy is required, all these approaches significantly increases computational consumption due to the high-dimensional searching space. In this work, we develop a new time-dependent inclusion-based CCD framework that eliminates the need for temporal subdivision and can speedup conventional methods by a factor ranging from 36 to 138. To achieve this, we propose a novel time-dependent inclusion function that provides a continuous representation of a moving surface, along with a corresponding intersection detection algorithm that quickly identifies the time intervals when collisions are likely to occur. We validate our method across various primitive types, demonstrate its efficacy within the simulation pipeline and show that it significantly improves CCD efficiency while maintaining accuracy.},
  archive      = {J_TOG},
  doi          = {10.1145/3687960},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {A time-dependent inclusion-based method for continuous collision detection between parametric surfaces},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerate neural subspace-based reduced-order solver of
deformable simulation by lipschitz optimization. <em>TOG</em>,
<em>43</em>(6), 1–10. (<a
href="https://doi.org/10.1145/3687961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reduced-order simulation is an emerging method for accelerating physical simulations with high DOFs, and recently developed neural-network-based methods with nonlinear subspaces have been proven effective in diverse applications as more concise subspaces can be detected. However, the complexity and landscape of simulation objectives within the subspace have not been optimized, which leaves room for enhancement of the convergence speed. This work focuses on this point by proposing a general method for finding optimized subspace mappings, enabling further acceleration of neural reduced-order simulations while capturing comprehensive representations of the configuration manifolds. We achieve this by optimizing the Lipschitz energy of the elasticity term in the simulation objective, and incorporating the cubature approximation into the training process to manage the high memory and time demands associated with optimizing the newly introduced energy. Our method is versatile and applicable to both supervised and unsupervised settings for optimizing the parameterizations of the configuration manifolds. We demonstrate the effectiveness of our approach through general cases in both quasi-static and dynamics simulations. Our method achieves acceleration factors of up to 6.83 while consistently preserving comparable simulation accuracy in various cases, including large twisting, bending, and rotational deformations with collision handling. This novel approach offers significant potential for accelerating physical simulations, and can be a good add-on to existing neural-network-based solutions in modeling complex deformable objects.},
  archive      = {J_TOG},
  doi          = {10.1145/3687961},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Accelerate neural subspace-based reduced-order solver of deformable simulation by lipschitz optimization},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometry-aware retargeting for two-skinned characters
interaction. <em>TOG</em>, <em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive motion between multiple characters is widely utilized in games and movies. However, the method for generating interactive motions considering the character&#39;s diverse mesh shape has yet to be studied. We propose a Spatio Cooperative Transformer (SCT) to retarget the interacting motions of two characters having arbitrary mesh connectivity. SCT predicts the residual of root position and joint rotations considering the shape difference between the source and target of interacting characters. In addition, we introduce an anchor loss function for SCT to maintain the geometric distance between the interacting characters when they are retargeted. We also propose a motion augmentation method with deformation-based adaptation to prepare a source-target paired dataset with an identical mesh connectivity for training. In experiments, our method achieved higher accuracy for semantic preservation and produced less artifacts of inter-penetration between the interacting characters for unseen characters and motions than the baselines. Moreover, we conducted a user evaluation using characters with various shapes, spanning low-to-high interaction levels to prove better semantic preservation of our method compared to previous studies.},
  archive      = {J_TOG},
  doi          = {10.1145/3687962},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Geometry-aware retargeting for two-skinned characters interaction},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An impulse ghost fluid method for simulating two-phase
flows. <em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a two-phase interfacial fluid model based on the impulse variable to capture complex vorticity-interface interactions. Our key idea is to leverage bidirectional flow map theory to enhance the transport accuracy of both vorticity and interfaces simultaneously and address their coupling within a unified Eulerian framework. At the heart of our framework is an impulse ghost fluid method to solve the two-phase incompressible fluid characterized by its interfacial dynamics. To deal with the history-dependent jump of gauge variables across a dynamic interface, we develop a novel path integral formula empowered by spatiotemporal buffers to convert the history-dependent jump condition into a geometry-dependent jump condition when projecting impulse to velocity. We demonstrate the efficacy of our approach in simulating and visualizing several interface-vorticity interaction problems with cross-phase vortical evolution, including interfacial whirlpool, vortex ring reflection, and leapfrogging bubble rings.},
  archive      = {J_TOG},
  doi          = {10.1145/3687963},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {An impulse ghost fluid method for simulating two-phase flows},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medial skeletal diagram: A generalized medial axis approach
for compact 3D shape representation. <em>TOG</em>, <em>43</em>(6), 1–23.
(<a href="https://doi.org/10.1145/3687964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the Medial Skeletal Diagram, a novel skeletal representation that tackles the prevailing issues around skeleton sparsity and reconstruction accuracy in existing skeletal representations. Our approach augments the continuous elements in the medial axis representation to effectively shift the complexity away from the discrete elements. To that end, we introduce generalized enveloping primitives, an enhancement over the standard primitives in the medial axis, which ensure efficient coverage of intricate local features of the input shape and substantially reduce the number of discrete elements required. Moreover, we present a computational framework for constructing a medial skeletal diagram from an arbitrary closed manifold mesh. Our optimization pipeline ensures that the resulting medial skeletal diagram comprehensively covers the input shape with the fewest primitives. Additionally, each optimized primitive undergoes a post-refinement process to guarantee an accurate match with the source mesh in both geometry and tessellation. We validate our approach on a comprehensive benchmark of 100 shapes, demonstrating the sparsity of the discrete elements and superior reconstruction accuracy across a variety of cases. Finally, we exemplify the versatility of our representation in downstream applications such as shape generation, mesh decomposition, shape optimization, mesh alignment, mesh compression, and user-interactive design.},
  archive      = {J_TOG},
  doi          = {10.1145/3687964},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {ACM Trans. Graph.},
  title        = {Medial skeletal diagram: A generalized medial axis approach for compact 3D shape representation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dense server design for immersion cooling. <em>TOG</em>,
<em>43</em>(6), 1–20. (<a
href="https://doi.org/10.1145/3687965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing demands for computational power in cloud computing have led to a significant increase in the deployment of high-performance servers. The growing power consumption of servers and the heat they produce is on track to outpace the capacity of conventional air cooling systems, necessitating more efficient cooling solutions such as liquid immersion cooling. The superior heat exchange capabilities of immersion cooling both eliminates the need for bulky heat sinks, fans, and air flow channels while also unlocking the potential go beyond conventional 2D blade servers to three-dimensional designs. In this work, we present a computational framework to explore designs of servers in three-dimensional space, specifically targeting the maximization of server density within immersion cooling tanks. Our tool is designed to handle a variety of physical and electrical server design constraints. We demonstrate our optimized designs can reduce server volume by 25--52% compared to traditional flat server designs. This increased density reduces land usage as well as the amount of liquid used for immersion, with significant reduction in the carbon emissions embodied in datacenter buildings. We further create physical prototypes to simulate dense server designs and perform real-world experiments in an immersion cooling tank demonstrating they operate at safe temperatures. This approach marks a critical step forward in sustainable and efficient datacenter management.},
  archive      = {J_TOG},
  doi          = {10.1145/3687965},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dense server design for immersion cooling},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational design of a kit of parts for bending active
structures. <em>TOG</em>, <em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bending-active structures are composed of elastic elements that deform to achieve a desired target shape. To support effective design, inverse algorithms have been proposed that optimize the geometry of each element specifically for each design. This makes it difficult to reuse elements across designs or gain efficiency in fabrication through mass production. We address this issue and propose a computational framework to rationalize bending-active structures into a sparse kit of parts. Our method solves for the optimal part geometry such that multiple input designs can be faithfully realized with the same kit of parts. Assigning parts to different assemblies leads to a combinatorial explosion that makes exhaustive search intractable. Instead, we propose a relaxed continuous optimization incorporating a physics-based simulation in its inner loop to model the elastic deformation of the bending-active structure accurately. Our algorithm allows analyzing different design trade-offs of a kit of parts to tune the balance between fabrication complexity and fidelity to the original designs. We demonstrate our method on three different classes of bending-active structures, showcasing the effectiveness of our approach for part reuse and sustainable practices in fabrication-driven design.},
  archive      = {J_TOG},
  doi          = {10.1145/3687966},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of a kit of parts for bending active structures},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tencers: Tension-constrained elastic rods. <em>TOG</em>,
<em>43</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3687967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study ensembles of elastic rods that are tensioned by a small set of inextensible cables. The cables induce forces that deform the initially straight, but flexible rods into 3D space curves at equilibrium. Rods can be open or closed, knotted, and arranged in arbitrary topologies. We specifically focus on equilibrium states with no contacts among rods. Our setup can thus be seen as a generalization of classical tensegrities that are composed of rigid rods and tensile cables, to also support rods that elastically deform. We show how this generalization leads to a rich design space, where complex target shapes can be achieved with a small set of elastic rods. To explore this space, we present an inverse design optimization algorithm that solves for the length and placement of cables such that the equilibrium state of the rod network best approximates a given set of input curves. We introduce appropriate sparsity terms to minimize the number of required cables, which significantly simplifies fabrication. Using our algorithm, we explore new classes of bending-active 3D structures, including elastic tensegrity knots that only require a few internal cables. We design and fabricate several physical models from basic materials that attain complex 3D shapes with unique structural properties.},
  archive      = {J_TOG},
  doi          = {10.1145/3687967},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Tencers: Tension-constrained elastic rods},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deformation recovery: Localized learning for
detail-preserving deformations. <em>TOG</em>, <em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel data-driven approach aimed at designing high-quality shape deformations based on a coarse localized input signal. Unlike previous data-driven methods that require a global shape encoding, we observe that detail-preserving deformations can be estimated reliably without any global context in certain scenarios. Building on this intuition, we leverage Jacobians defined in a one-ring neighborhood as a coarse representation of the deformation. Using this as the input to our neural network, we apply a series of MLPs combined with feature smoothing to learn the Jacobian corresponding to the detail-preserving deformation, from which the embedding is recovered by the standard Poisson solve. Crucially, by removing the dependence on a global encoding, every point becomes a training example, making the supervision particularly lightweight. Moreover, when trained on a class of shapes, our approach demonstrates remarkable generalization across different object categories. Equipped with this novel network, we explore three main tasks: refining an approximate shape correspondence, unsupervised deformation and mapping, and shape editing. Our code is made available at https://github.com/sentient07/LJN.},
  archive      = {J_TOG},
  doi          = {10.1145/3687968},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deformation recovery: Localized learning for detail-preserving deformations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AR-DAVID: Augmented reality display artifact video dataset.
<em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perception of visual content in optical-see-through augmented reality (AR) devices is affected by the light coming from the environment. This additional light interacts with the content in a non-trivial manner because of the illusion of transparency, different focal depths, and motion parallax. To investigate the impact of environment light on display artifact visibility (such as blur or color fringes), we created the first subjective quality dataset targeted toward augmented reality displays. Our study consisted of 6 scenes, each affected by one of 6 distortions at two strength levels, seen against one of 3 background patterns shown at 2 luminance levels: 432 conditions in total. Our dataset shows that environment light has a much smaller masking effect than expected. Further, we show that this effect cannot be explained by compositing of the AR-content with the background using optical blending models. As a consequence, we demonstrate that existing video quality metrics perform worse than expected when predicting the perceived magnitude of degradation in AR displays, motivating further research.},
  archive      = {J_TOG},
  doi          = {10.1145/3687969},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {AR-DAVID: Augmented reality display artifact video dataset},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fluid implicit particles on coadjoint orbits. <em>TOG</em>,
<em>43</em>(6), 1–38. (<a
href="https://doi.org/10.1145/3687970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Coadjoint Orbit FLIP (CO-FLIP), a high order accurate, structure preserving fluid simulation method in the hybrid Eulerian-Lagrangian framework. We start with a Hamiltonian formulation of the incompressible Euler Equations, and then, using a local, explicit, and high order divergence free interpolation, construct a modified Hamiltonian system that governs our discrete Euler flow. The resulting discretization, when paired with a geometric time integration scheme, is energy and circulation preserving (formally the flow evolves on a coadjoint orbit) and is similar to the Fluid Implicit Particle (FLIP) method. CO-FLIP enjoys multiple additional properties including that the pressure projection is exact in the weak sense, and the particle-to-grid transfer is an exact inverse of the grid-to-particle interpolation. The method is demonstrated numerically with outstanding stability, energy, and Casimir preservation. We show that the method produces benchmarks and turbulent visual effects even at low grid resolutions.},
  archive      = {J_TOG},
  doi          = {10.1145/3687970},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-38},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fluid implicit particles on coadjoint orbits},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StableNormal: Reducing diffusion variance for stable and
sharp normal. <em>TOG</em>, <em>43</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3687971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the challenge of high-quality surface normal estimation from monocular colored inputs (i.e., images and videos), a field which has recently been revolutionized by repurposing diffusion priors. However, previous attempts still struggle with stochastic inference, conflicting with the deterministic nature of the Image2Normal task, and costly ensembling step, which slows down the estimation process. Our method, StableNormal, mitigates the stochasticity of the diffusion process by reducing inference variance, thus producing &quot;Stable-and-Sharp&quot; normal estimates without any additional ensembling process. StableNormal works robustly under challenging imaging conditions, such as extreme lighting, blurring, and low quality. It is also robust against transparent and reflective surfaces, as well as cluttered scenes with numerous objects. Specifically, StableNormal employs a coarse-to-fine strategy, which starts with a one-step normal estimator (YOSO) to derive an initial normal guess, that is relatively coarse but reliable, then followed by a semantic-guided refinement process (SG-DRN) that refines the normals to recover geometric details. The effectiveness of StableNormal is demonstrated through competitive performance in standard datasets such as DIODE-indoor, iBims, ScannetV2 and NYUv2, and also in various downstream tasks, such as surface reconstruction and normal enhancement. These results evidence that StableNormal retains both the &quot;stability&quot; and &quot;sharpness&quot; for accurate normal estimation. StableNormal represents a baby attempt to repurpose diffusion priors for deterministic estimation. To democratize this, code and models have been publicly available in hf.co/Stable-X.},
  archive      = {J_TOG},
  doi          = {10.1145/3687971},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {StableNormal: Reducing diffusion variance for stable and sharp normal},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). C^0 generalized coons patches for high-order cage-based
deformation. <em>TOG</em>, <em>43</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3687972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space deformations deform the ambient space and thus implicitly deform the embedded objects. Free-Form Deformation allows high-order deformation to the embedding space, yet the lattice may fail to conform to the object and involves many internal control points. Cage-based Deformation utilizes a cage space that conforms to the object, obviating the need for additional internal control points, but it is typically linear at edges. In this paper, we propose a simple and general method with both advantages while avoiding their drawbacks, allowing users to implement high-order cage-based deformation. To achieve this goal, we introduce a new parametric transfinite interpolation scheme based on generalized barycentric coordinates, which unifies and generalizes the rectangular and triangular Coons patch. This C 0 Generalized Coons patch can be defined not only over 2D domains but also 3D domains or even higher-dimensional domains, with arbitrary polytopes, even including non-manifold topologies. Moreover, the C 0 Generalized Coons patch has an elegant mathematical expression.},
  archive      = {J_TOG},
  doi          = {10.1145/3687972},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {C^0 generalized coons patches for high-order cage-based deformation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MVImgNet2.0: A larger-scale dataset of multi-view images.
<em>TOG</em>, <em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MVImgNet is a large-scale dataset that contains multi-view images of ~220k real-world objects in 238 classes. As a counterpart of ImageNet, it introduces 3D visual signals via multi-view shooting, making a soft bridge between 2D and 3D vision. This paper constructs the MVImgNet2.0 dataset that expands MVImgNet into a total of ~520k objects and 515 categories, which derives a 3D dataset with a larger scale that is more comparable to ones in the 2D domain. In addition to the expanded dataset scale and category range, MVImgNet2.0 is of a higher quality than MVImgNet owing to four new features: (i) most shoots capture 360° views of the objects, which can support the learning of object reconstruction with completeness; (ii) the segmentation manner is advanced to produce foreground object masks of higher accuracy; (iii) a more powerful structure-from-motion method is adopted to derive the camera pose for each frame of a lower estimation error; (iv) higher-quality dense point clouds are reconstructed via advanced methods for objects captured in 360 ° views, which can serve for downstream applications. Extensive experiments confirm the value of the proposed MVImgNet2.0 in boosting the performance of large 3D reconstruction models. MVImgNet2.0 will be public at luyues.github.io/mvimgnet2 , including multi-view images of all 520k objects, the reconstructed high-quality point clouds, and data annotation codes, hoping to inspire the broader vision community.},
  archive      = {J_TOG},
  doi          = {10.1145/3687973},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {MVImgNet2.0: A larger-scale dataset of multi-view images},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VOODOO XP: Expressive one-shot head reenactment for VR
telepresence. <em>TOG</em>, <em>43</em>(6), 1–26. (<a
href="https://doi.org/10.1145/3687974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce VOODOO XP: a 3D-aware one-shot head reenactment method that can generate highly expressive facial expressions driven by an input video from a single 2D portrait. Our approach is real-time, view-consistent, and can be instantly used without calibration or fine-tuning. We demonstrate our solution in a monocular video setting and an end-to-end VR telepresence system for two-way communication. Compared to 2D head reenactment methods, 3D-aware approaches aim to preserve the identity of the subject and ensure view-consistent facial geometry for novel camera poses, which makes them suitable for immersive applications. While various facial disentanglement techniques have been introduced, cutting-edge 3D-aware neural reenactment techniques still lack expressiveness and fail to reproduce complex and fine-scale facial expressions. We present a novel cross-reenactment architecture that directly transfers the driver&#39;s facial expressions to transformer blocks of the input source&#39;s 3D lifting module. We show that highly effective disentanglement is possible using a new multi-stage self-supervision approach. It relies on a coarse-to-fine training strategy, which is combined with explicit face neutralization and 3D lifted frontalization during its initial training stage. We further integrate our novel head reenactment solution into an accessible high-fidelity VR telepresence system, where any person can instantly build a personalized neural head avatar from any photo and bring it to life using the headset. Furthermore, our proposed method demonstrates state-of-the-art expressiveness and likeness preservation on diverse subjects and capture conditions.},
  archive      = {J_TOG},
  doi          = {10.1145/3687974},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-26},
  shortjournal = {ACM Trans. Graph.},
  title        = {VOODOO XP: Expressive one-shot head reenactment for VR telepresence},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StyleCrafter: Taming artistic video diffusion with
reference-augmented adapter learning. <em>TOG</em>, <em>43</em>(6),
1–10. (<a href="https://doi.org/10.1145/3687975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-video (T2V) models have shown remarkable capabilities in generating diverse videos. However, they struggle to produce user-desired artistic videos due to (i) text&#39;s inherent clumsiness in expressing specific styles and (ii) the generally degraded style fidelity. To address these challenges, we introduce StyleCrafter, a generic method that enhances pretrained T2V models with a style control adapter, allowing video generation in any style by feeding a reference image. Considering the scarcity of artistic video data, we propose to first train a style control adapter using style-rich image datasets, then transfer the learned stylization ability to video generation through a tailor-made finetuning paradigm. To promote content-style disentanglement, we employ carefully designed data augmentation strategies to enhance decoupled learning. Additionally, we propose a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features, which helps generalization across various text and style combinations. StyleCrafter efficiently generates high-quality stylized videos that align with the content of the texts and resemble the style of the reference images. Experiments demonstrate that our approach is more flexible and efficient than existing competitors. Project page: https://gongyeliu.github.io/StyleCrafter.github.io/},
  archive      = {J_TOG},
  doi          = {10.1145/3687975},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {StyleCrafter: Taming artistic video diffusion with reference-augmented adapter learning},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learned multi-aperture color-coded optics for snapshot
hyperspectral imaging. <em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned optics, which incorporate lightweight diffractive optics, coded-aperture modulation, and specialized image-processing neural networks, have recently garnered attention in the field of snapshot hyperspectral imaging (HSI). While conventional methods typically rely on a single lens element paired with an off-the-shelf color sensor, these setups, despite their widespread availability, present inherent limitations. First, the Bayer sensor&#39;s spectral response curves are not optimized for HSI applications, limiting spectral fidelity of the reconstruction. Second, single lens designs rely on a single diffractive optical element (DOE) to simultaneously encode spectral information and maintain spatial resolution across all wavelengths, which constrains spectral encoding capabilities. This work investigates a multi-channel lens array combined with aperture-wise color filters, all co-optimized alongside an image reconstruction network. This configuration enables independent spatial encoding and spectral response for each channel, improving optical encoding across both spatial and spectral dimensions. Specifically, we validate that the method achieves over a 5dB improvement in PSNR for spectral reconstruction compared to existing single-diffractive lens and coded-aperture techniques. Experimental validation further confirmed that the method is capable of recovering up to 31 spectral bands within the 429--700 nm range in diverse indoor and outdoor environments.},
  archive      = {J_TOG},
  doi          = {10.1145/3687976},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learned multi-aperture color-coded optics for snapshot hyperspectral imaging},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MV2MV: Multi-view image translation via view-consistent
diffusion models. <em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image translation has various applications in computer graphics and computer vision, aiming to transfer images from one domain to another. Thanks to the excellent generation capability of diffusion models, recent single-view image translation methods achieve realistic results. However, directly applying diffusion models for multi-view image translation remains challenging for two major obstacles: the need for paired training data and the limited view consistency. To overcome the obstacles, we present a first unified multi-view image to multi-view image translation framework based on diffusion models, called MV2MV. Firstly, we propose a novel self-supervised training strategy that exploits the success of off-the-shelf single-view image translators and the 3D Gaussian Splatting (3DGS) technique to generate pseudo ground truths as supervisory signals, leading to enhanced consistency and fine details. Additionally, we propose a latent multi-view consistency block, which utilizes the latent-3DGS as the underlying 3D representation to facilitate information exchange across multi-view images and inject 3D prior into the diffusion model to enforce consistency. Finally, our approach simultaneously optimizes the diffusion model and 3DGS to achieve a better trade-off between consistency and realism. Extensive experiments across various translation tasks demonstrate that MV2MV outperforms task-specific specialists in both quantitative and qualitative.},
  archive      = {J_TOG},
  doi          = {10.1145/3687977},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {MV2MV: Multi-view image translation via view-consistent diffusion models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NFPLight: Deep SVBRDF estimation via the combination of near
and far field point lighting. <em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering spatial-varying bi-directional reflectance distribution function (SVBRDF) from a few hand-held captured images has been a challenging task in computer graphics. Benefiting from the learned priors from data, single-image methods can obtain plausible SVBRDF estimation results. However, the extremely limited appearance information in a single image does not suffice for high-quality SVBRDF reconstruction. Although increasing the number of inputs can improve the reconstruction quality, it also affects the efficiency of real data capture and adds significant computational burdens. Therefore, the key challenge is to minimize the required number of inputs, while keeping high-quality results. To address this, we propose maximizing the effective information in each input through a novel co-located capture strategy that combines near-field and far-field point lighting. To further enhance effectiveness, we theoretically investigate the inherent relation between two images. The extracted relation is strongly correlated with the slope of specular reflectance, substantially enhancing the precision of roughness map estimation. Additionally, we designed the registration and denoising modules to meet the practical requirements of hand-held capture. Quantitative assessments and qualitative analysis have demonstrated that our method achieves superior SVBRDF estimations compared to previous approaches. All source codes will be publicly released.},
  archive      = {J_TOG},
  doi          = {10.1145/3687978},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {NFPLight: Deep SVBRDF estimation via the combination of near and far field point lighting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Procedural material generation with reinforcement learning.
<em>TOG</em>, <em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern 3D content creation heavily relies on procedural assets. In particular, procedural materials are ubiquitous in the industry, but their manipulation remains challenging. Previous work [Hu et al. 2023] conditionally generates procedural graphs that match a given input image. However, the parameter generation step limits how accurately the generated graph matches the input image, due to a reliance on supervision with scarcely available procedural data. We propose to improve parameter prediction accuracy for image-conditioned procedural material generation by leveraging reinforcement learning (RL) and present the first RL approach for procedural materials. RL circumvents the limited availability of procedural data, the domain gap between real and synthetic materials, and the need for end-to-end differentiable loss functions. Given a target image, we retrieve a procedural material and use an RL-trained transformer model to predict a set of parameters that reconstruct the target image as closely as possible. We show that using RL significantly improves parameter prediction to match a given target image compared to supervised methods on both synthetic and real target images.},
  archive      = {J_TOG},
  doi          = {10.1145/3687979},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Procedural material generation with reinforcement learning},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 360-degree human video generation with 4D diffusion
transformer. <em>TOG</em>, <em>43</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3687980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for generating 360-degree high-quality, spatiotemporally coherent human videos from a single image. Our framework combines the strengths of diffusion transformers for capturing global correlations across viewpoints and time, and CNNs for accurate condition injection. The core is a hierarchical 4D transformer architecture that factorizes self-attention across views, time steps, and spatial dimensions, enabling efficient modeling of the 4D space. Precise conditioning is achieved by injecting human identity, camera parameters, and temporal signals into the respective transformers. To train this model, we collect a multi-dimensional dataset spanning images, videos, multi-view data, and limited 4D footage, along with a tailored multi-dimensional training strategy. Our approach overcomes the limitations of previous methods based on generative adversarial networks or vanilla diffusion models, which struggle with complex motions, viewpoint changes, and generalization. Through extensive experiments, we demonstrate our method&#39;s ability to synthesize 360-degree realistic, coherent human motion videos, paving the way for advanced multimedia applications in areas such as virtual reality and animation.},
  archive      = {J_TOG},
  doi          = {10.1145/3687980},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {360-degree human video generation with 4D diffusion transformer},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Content-aware tile generation using exterior boundary
inpainting. <em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel and flexible learning-based method for generating tileable image sets. Our method goes beyond simple self-tiling, supporting sets of mutually tileable images that exhibit a high degree of diversity. To promote diversity we decouple structure from content by foregoing explicit copying of patches from an exemplar image. Instead we leverage the prior knowledge of natural images and textures embedded in large-scale pretrained diffusion models to guide tile generation constrained by exterior boundary conditions and a text prompt to specify the content. By carefully designing and selecting the exterior boundary conditions, we can reformulate the tile generation process as an inpainting problem, allowing us to directly employ existing diffusion-based inpainting models without the need to retrain a model on a custom training set. We demonstrate the flexibility and efficacy of our content-aware tile generation method on different tiling schemes, such as Wang tiles, from only a text prompt. Furthermore, we introduce a novel Dual Wang tiling scheme that provides greater texture continuity and diversity than existing Wang tile variants.},
  archive      = {J_TOG},
  doi          = {10.1145/3687981},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Content-aware tile generation using exterior boundary inpainting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Volume scattering probability guiding. <em>TOG</em>,
<em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating the light transport of volumetric effects poses significant challenges and costs, especially in the presence of heterogeneous volumes. Generating stochastic paths for volume rendering involves multiple decisions, and previous works mainly focused on directional and distance sampling, where the volume scattering probability (VSP), i.e., the probability of scattering inside a volume, is indirectly determined as a byproduct of distance sampling. We demonstrate that direct control over the VSP can significantly improve efficiency and present an unbiased volume rendering algorithm based on an existing resampling framework for precise control over the VSP. Compared to previous state-of-the-art, which can only increase the VSP without guaranteeing to reach the desired value, our method also supports decreasing the VSP. We further present a data-driven guiding framework to efficiently learn and query an approximation of the optimal VSP everywhere in the scene without the need for user control. Our approach can easily be combined with existing path-guiding methods for directional sampling at minimal overhead and shows significant improvements over the state-of-the-art in various complex volumetric lighting scenarios.},
  archive      = {J_TOG},
  doi          = {10.1145/3687982},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Volume scattering probability guiding},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Appearance modeling of iridescent feathers with diverse
nanostructures. <em>TOG</em>, <em>43</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3687983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many animals exhibit structural colors, which are often iridescent, meaning that the perceived colors change with illumination conditions and viewing perspectives. Biological iridescence is usually caused by multilayers or other periodic structures in animal tissues, which selectively reflect light of certain wavelengths and often result in a shiny appearance---which almost always comes with spatially varying highlights, thanks to randomness and irregularities in the structures. Previous models for biological iridescence tend to each target one specific structure, and most models only compute large-area averages, overlooking spatial variation in iridescent appearance. In this work, we build appearance models for biological iridescence using bird feathers as our case study, investigating different types of feathers with a variety of structural coloration mechanisms. We propose an approximate wave simulation method that takes advantage of quasi-regular structures while efficiently modeling the effects of natural structural irregularities. We further propose a method to distill our simulation results into distributions of BRDFs, generated using noise functions, that preserve relevant statistical properties of the simulated BRDFs. This allows us to model the spatially varying, glittery appearance commonly seen on feathers. Our BRDFs are practical and efficient, and we present renderings of multiple types of iridescent feathers with comparisons to photographic images.},
  archive      = {J_TOG},
  doi          = {10.1145/3687983},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Appearance modeling of iridescent feathers with diverse nanostructures},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Colorful diffuse intrinsic image decomposition in the wild.
<em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic image decomposition aims to separate the surface reflectance and the effects from the illumination given a single photograph. Due to the complexity of the problem, most prior works assume a single-color illumination and a Lambertian world, which limits their use in illumination-aware image editing applications. In this work, we separate an input image into its diffuse albedo, colorful diffuse shading, and specular residual components. We arrive at our result by gradually removing first the single-color illumination and then the Lambertian-world assumptions. We show that by dividing the problem into easier sub-problems, in-the-wild colorful diffuse shading estimation can be achieved despite the limited ground-truth datasets. Our extended intrinsic model enables illumination-aware analysis of photographs and can be used for image editing applications such as specularity removal and per-pixel white balancing.},
  archive      = {J_TOG},
  doi          = {10.1145/3687984},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Colorful diffuse intrinsic image decomposition in the wild},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-resolution real-time deep pose-space deformation.
<em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hard-real-time multi-resolution mesh shape deformation technique for skeleton-driven soft-body characters. Producing mesh deformations at multiple levels of detail is very important in many applications in computer graphics. Our work targets applications where the multi-resolution shapes must be generated at fast speeds (&quot;hard-real-time&quot;, e.g., a few milliseconds at most and preferably under 1 millisecond), as commonly needed in computer games, virtual reality and Metaverse applications. We assume that the character mesh is driven by a skeleton, and that high-quality character shapes are available in a set of training poses originating from a high-quality (slow) rig such as volumetric FEM simulation. Our method combines multi-resolution analysis, mesh partition of unity, and neural networks, to learn the pre-skinning shape deformations in an arbitrary character pose. Combined with linear blend skinning, this makes it possible to reconstruct the training shapes, as well as interpolate and extrapolate them. Crucially, we simultaneously achieve this at hard real-time rates and at multiple mesh resolution levels. Our technique makes it possible to trade deformation quality for memory and computation speed, to accommodate the strict requirements of modern real-time systems. Furthermore, we propose memory layout and code improvements to boost computation speeds. Previous methods for realtime approximations of quality shape deformations did not focus on hard real-time, or did not investigate the multi-resolution aspect of the problem. Compared to a &quot;naive&quot; approach of separately processing each hierarchical level of detail, our method offers a substantial memory reduction as well as computational speedups. It also makes it possible to construct the shape progressively level by level and interrupt the computation at any time, enabling graceful degradation of the deformation detail. We demonstrate our technique on several examples, including a stylized human character, human hands, and an inverse-kinematics-driven quadruped animal.},
  archive      = {J_TOG},
  doi          = {10.1145/3687985},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Multi-resolution real-time deep pose-space deformation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mesh-based simulation framework using automatic code
generation. <em>TOG</em>, <em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimized parallel implementations on GPU or CPU have dramatically enhanced the fidelity, resolution and accuracy of physical simulations and mesh-based algorithms. However, attaining optimal performance requires expert knowledge and might demand complex code and memory layout optimizations. This adds to the fact that physical simulation algorithms require the implementation of derivatives, which can be a tedious and error-prone process. In recent years, researchers and practitioners have investigated the concept of designing systems that allow for a more expressive definition of mesh-based simulation code. These systems leverage domain-specific languages (DSL), automatic differentiation or symbolic computing to enhance readability of implementations without compromising performance. We follow this line of work and propose a symbolic code generation approach tailored to mesh-based computations on parallel devices. Our system extends related work by incorporating collision handling and a data access synchronization approach, enabling rapid sparse matrix assembly.},
  archive      = {J_TOG},
  doi          = {10.1145/3687986},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A mesh-based simulation framework using automatic code generation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A class of new tuned primal subdivision schemes with
high-quality limit surface in extraordinary regions. <em>TOG</em>,
<em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a unified tuning framework for primal subdivision schemes that are generalizations of odd-degree uniform B-spline surfaces for unstructured quadrilateral meshes of arbitrary topology. The subdivision of the resulting tuned primal subdivision (TPS) schemes is performed through efficient repeated local refinement operations. One level of subdivision of TPS-schemes is decomposed into one step of simple topological splitting plus an additional series of repeated local smoothing operations. The unified tuning framework optimizes subdivision rules for both topological splitting and smoothing operations near extraordinary vertices by minimizing the curvature fluctuation of the second-order characteristic maps of the respective TPS-scheme. To validate the limit surface quality of TPS-schemes in extraordinary regions, a mesh-independent metric is also devised to estimate local curvature variation in addition to highlight lines and direct curvature evaluation. With (p - 1)/2 steps of smoothing operations in each level of subdivision, the respective TPS-scheme is termed as a degree-p scheme that produces global C p -1 limit surfaces everywhere except at a finite number of extraordinary positions where near- G 2 continuity is achieved. The limit surface of TPS-schemes in extraordinary regions also exhibits appealing highlight lines, and the larger the number of smoothing operations applied in each level of subdivision, the better the final limit surface quality. One of the key advantages of the proposed tuning framework for TPS-schemes is that the optimization of relevant subdivision rules is performed by tuning structured operations of topological splitting and repeated smoothing that involve direct neighbors of an extraordinary vertex only without the need of tuning large subdivision stencils. All subdivision operations of TPS-schemes are local, involving one-ring of neighboring vertices only, which is efficient for high-degree surface subdivision and convenient for practical implementation. Numerical examples also validate the superiority of TPS-schemes over other state-of-the-art subdivision methods in terms of both highlight lines and various curvature measures.},
  archive      = {J_TOG},
  doi          = {10.1145/3687987},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A class of new tuned primal subdivision schemes with high-quality limit surface in extraordinary regions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Barrier-augmented lagrangian for GPU-based elastodynamic
contact. <em>TOG</em>, <em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a GPU-based iterative method for accelerated elastodynamic simulation with the log-barrier-based contact model. While Newton&#39;s method is a conventional choice for solving the interior-point system, the presence of ill-conditioned log barriers often necessitates a direct solution at each linearized substep and costs substantial storage and computational overhead. Moreover, constraint sets that vary in each iteration present additional challenges in algorithm convergence. Our method employs a novel barrier-augmented Lagrangian method to improve system conditioning and solver efficiency by adaptively updating an augmentation constraint sets. This enables the utilization of a scalable, inexact Newton-PCG solver with sparse GPU storage, eliminating the need for direct factorization. We further enhance PCG convergence speed with a domain-decomposed warm start strategy based on an eigenvalue spectrum approximated through our in-time assembly. Demonstrating significant scalability improvements, our method makes simulations previously impractical on 128 GB of CPU memory feasible with only 8 GB of GPU memory and orders-of-magnitude faster. Additionally, our method adeptly handles stiff problems, surpassing the capabilities of existing GPU-based interior-point methods. Our results, validated across various complex collision scenarios involving intricate geometries and large deformations, highlight the exceptional performance of our approach.},
  archive      = {J_TOG},
  doi          = {10.1145/3687988},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Barrier-augmented lagrangian for GPU-based elastodynamic contact},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-level partition of unity on differentiable moving
particles. <em>TOG</em>, <em>43</em>(6), 1–21. (<a
href="https://doi.org/10.1145/3687989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a differentiable moving particle representation based on the multi-level partition of unity (MPU) to represent dynamic implicit geometries. At the core of our representation are two groups of particles, named feature particles and sample particles, which can move in space and produce dynamic surfaces according to external velocity fields or optimization gradients. These two particle groups iteratively guide and correct each other by alternating their roles as inputs and outputs. Each feature particle carries a set of coefficients for a local quadratic patch. These particle patches are assembled with partition-of-unity weights to derive a continuous implicit global shape. Each sampling particle carries its position and orientation, serving as dense surface samples for optimization tasks. Based on these moving particles, we develop a fully differentiable framework to infer and evolve highly detailed implicit geometries, enhanced by a multi-level background grid for particle adaptivity, across different inverse tasks. We demonstrated the efficacy of our representation through various benchmark comparisons with state-of-the-art neural representations, achieving lower memory consumption, fewer training iterations, and orders of magnitude higher accuracy in handling topologically complex objects and dynamic tracking tasks.},
  archive      = {J_TOG},
  doi          = {10.1145/3687989},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Multi-level partition of unity on differentiable moving particles},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving inverse PDE problems using grid-free monte carlo
estimators. <em>TOG</em>, <em>43</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3687990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial differential equations can model diverse physical phenomena including heat diffusion, incompressible flows, and electrostatic potentials. Given a description of an object&#39;s boundary and interior, traditional methods solve such PDEs by densely meshing the interior and then solving a large and sparse linear system derived from this mesh. Recent grid-free solvers take an alternative approach and avoid this complexity in exchange for randomness: they compute stochastic solution estimates and generally bear a striking resemblance to physically-based rendering algorithms. In this article, we develop algorithms targeting the inverse form of this problem: given an already existing solution of a PDE, we infer parameters characterizing the boundary and interior. In the grid-free setting, there are again significant connections to rendering, and we show how insights from both fields can be combined to compute unbiased derivative estimates that enable gradient-based optimization. In this process, we encounter new challenges that must be addressed to obtain practical solutions. We introduce acceleration and variance reduction strategies and show how to differentiate branching random walks in reverse mode. We finally demonstrate our approach on both simulated data and a real-world electrical impedance tomography experiment, where we reconstruct the position of a conducting object from voltage measurements taken in a saline-filled tank.},
  archive      = {J_TOG},
  doi          = {10.1145/3687990},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Solving inverse PDE problems using grid-free monte carlo estimators},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ELMO: Enhanced real-time LiDAR motion capture through
upsampling. <em>TOG</em>, <em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces ELMO, a real-time upsampling motion capture framework designed for a single LiDAR sensor. Modeled as a conditional autoregressive transformer-based upsampling motion generator, ELMO achieves 60 fps motion capture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is the coupling of the self-attention mechanism with thoughtfully designed embedding modules for motion and point clouds, significantly elevating the motion quality. To facilitate accurate motion capture, we develop a one-time skeleton calibration model capable of predicting user skeleton off-sets from a single-frame point cloud. Additionally, we introduce a novel data augmentation technique utilizing a LiDAR simulator, which enhances global root tracking to improve environmental understanding. To demonstrate the effectiveness of our method, we compare ELMO with state-of-the-art methods in both image-based and point cloud-based motion capture. We further conduct an ablation study to validate our design principles. ELMO&#39;s fast inference time makes it well-suited for real-time applications, exemplified in our demo video featuring live streaming and interactive gaming scenarios. Furthermore, we contribute a high-quality LiDAR-mocap synchronized dataset comprising 20 different subjects performing a range of motions, which can serve as a valuable resource for future research. The dataset and evaluation code are available at https://movin3d.github.io/ELMO_SIGASIA2024/},
  archive      = {J_TOG},
  doi          = {10.1145/3687991},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {ELMO: Enhanced real-time LiDAR motion capture through upsampling},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A progressive embedding approach to bijective tetrahedral
maps driven by cluster mesh topology. <em>TOG</em>, <em>43</em>(6),
1–14. (<a href="https://doi.org/10.1145/3687992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel algorithm to map ball-topology tetrahedral meshes onto star-shaped domains with guarantees regarding bijectivity. Our algorithm is based on the recently introduced idea of Shrink-and-Expand, where images of interior vertices are initially clustered at one point (Shrink-), before being sequentially moved to non-degenerate positions yielding a bijective map (-and-Expand). In this context, we introduce the concept of the cluster mesh , i.e. the unexpanded interior mesh consisting of geometrically degenerate simplices. Using local, per-vertex connectivity information solely from the cluster mesh, we show that a viable expansion sequence guaranteed to produce a bijective map can always be found as long as the mesh is shellable. In addition to robustness guarantees for this ubiquitous class of inputs, other practically relevant benefits include improved parsimony and reduced algorithmic complexity. While inheriting some of the worst-case high run time requirements of the state of the art, significant acceleration for the average case is experimentally demonstrated.},
  archive      = {J_TOG},
  doi          = {10.1145/3687992},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A progressive embedding approach to bijective tetrahedral maps driven by cluster mesh topology},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian surfel splatting for live human performance
capture. <em>TOG</em>, <em>43</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3687993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality real-time rendering using user-affordable capture rigs is an essential property of human performance capture systems for real-world applications. However, state-of-the-art performance capture methods may not yield satisfactory rendering results under a very sparse (e.g., four) capture setting. Specifically, neural radiance field (NeRF)-based methods and 3D Gaussian Splatting (3DGS)-based methods tend to produce local geometry errors for unseen performers, while occupancy field (PIFu)-based methods often produce unrealistic rendering results. In this paper, we propose a novel generalizable neural approach to reconstruct and render the performers from very sparse RGBD streams in high quality. The core of our method is a novel point-based generalizable human (PGH) representation conditioned on the pixel-aligned RGBD features. The PGH representation learns a surface implicit function for the regression of surface points and a Gaussian implicit function for parameterizing the radiance fields of the regressed surface points with 2D Gaussian surfels, and uses surfel splatting for fast rendering. We learn this hybrid human representation via two novel networks. First, we propose a novel point-regressing network (PRNet) with a depth-guided point cloud initialization (DPI) method to regress an accurate surface point cloud based on the denoised depth information. Second, we propose a novel neural blending-based surfel splatting network (SPNet) to render high-quality geometries and appearances in novel views based on the regressed surface points and high-resolution RGBD features of adjacent views. Our method produces free-view human performance videos of 1K resolution at 12 fps on average. Experiments on two benchmarks show that our method outperforms state-of-the-art human performance capture methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3687993},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Gaussian surfel splatting for live human performance capture},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HFH-font: Few-shot chinese font synthesis with higher
quality, faster speed, and higher resolution. <em>TOG</em>,
<em>43</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3687994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of automatically synthesizing high-quality vector fonts, particularly for writing systems (e.g., Chinese) consisting of huge amounts of complex glyphs, remains unsolved. Existing font synthesis techniques fall into two categories: 1) methods that directly generate vector glyphs, and 2) methods that initially synthesize glyph images and then vectorize them. However, the first category often fails to construct complete and correct shapes for complex glyphs, while the latter struggles to efficiently synthesize high-resolution (i.e., 1024 × 1024 or higher) glyph images while preserving local details. In this paper, we introduce HFH-Font, a few-shot font synthesis method capable of efficiently generating high-resolution glyph images that can be converted into high-quality vector glyphs. More specifically, our method employs a diffusion model-based generative framework with component-aware conditioning to learn different levels of style information adaptable to varying input reference sizes. We also design a distillation module based on Score Distillation Sampling for 1-step fast inference, and a style-guided super-resolution module to refine and upscale low-resolution synthesis results. Extensive experiments, including a user study with professional font designers, have been conducted to demonstrate that our method significantly outperforms existing font synthesis approaches. Experimental results show that our method produces high-fidelity, high-resolution raster images which can be vectorized into high-quality vector fonts. Using our method, for the first time, large-scale Chinese vector fonts of a quality comparable to those manually created by professional font designers can be automatically generated.},
  archive      = {J_TOG},
  doi          = {10.1145/3687994},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {HFH-font: Few-shot chinese font synthesis with higher quality, faster speed, and higher resolution},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perspective-aligned AR mirror with under-display camera.
<em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) mirrors are novel displays that have great potential for commercial applications such as virtual apparel try-on. Typically the camera is placed beside the display, leading to distorted perspectives during user interaction. In this paper, we present a novel approach to address this problem by placing the camera behind a transparent display, thereby providing users with a perspective-aligned experience. Simply placing the camera behind the display can compromise image quality due to optical effects. We meticulously analyze the image formation process, and present an image restoration algorithm that benefits from physics-based data synthesis and network design. Our method significantly improves image quality and outperforms existing methods especially on the underexplored wire and backscatter artifacts. We then carefully design a full AR mirror system including display and camera selection, real-time processing pipeline, and mechanical design. Our user study demonstrates that the system is exceptionally well-received by users, highlighting its advantages over existing camera configurations not only as an AR mirror, but also for video conferencing. Our work represents a step forward in the development of AR mirrors, with potential applications in retail, cosmetics, fashion, etc. The image restoration dataset and code are available at https://perspective-armirror.github.io/.},
  archive      = {J_TOG},
  doi          = {10.1145/3687995},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Perspective-aligned AR mirror with under-display camera},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An eulerian vortex method on flow maps. <em>TOG</em>,
<em>43</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3687996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an Eulerian vortex method based on the theory of flow maps to simulate the complex vortical motions of incompressible fluids. Central to our method is the novel incorporation of the flow-map transport equations for line elements , which, in combination with a bi-directional marching scheme for flow maps, enables the high-fidelity Eulerian advection of vorticity variables. The fundamental motivation is that, compared to impulse m , which has been recently bridged with flow maps to encouraging results, vorticity ω promises to be preferable for its numerical stability and physical interpretability. To realize the full potential of this novel formulation, we develop a new Poisson solving scheme for vorticity-to-velocity reconstruction that is both efficient and able to accurately handle the coupling near solid boundaries. We demonstrate the efficacy of our approach with a range of vortex simulation examples, including leapfrog vortices, vortex collisions, cavity flow, and the formation of complex vortical structures due to solid-fluid interactions.},
  archive      = {J_TOG},
  doi          = {10.1145/3687996},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {An eulerian vortex method on flow maps},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LVCD: Reference-based lineart video colorization with
diffusion models. <em>TOG</em>, <em>43</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3687910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the first video diffusion framework for reference-based lineart video colorization. Unlike previous works that rely solely on image generative models to colorize lineart frame by frame, our approach leverages a large-scale pretrained video diffusion model to generate colorized animation videos. This approach leads to more temporally consistent results and is better equipped to handle large motions. Firstly, we introduce Sketch-guided ControlNet which provides additional control to finetune an image-to-video diffusion model for controllable video synthesis, enabling the generation of animation videos conditioned on lineart. We then propose Reference Attention to facilitate the transfer of colors from the reference frame to other frames containing fast and expansive motions. Finally, we present a novel scheme for sequential sampling, incorporating the Overlapped Blending Module and Prev-Reference Attention , to extend the video diffusion model beyond its original fixed-length limitation for long video colorization. Both qualitative and quantitative results demonstrate that our method significantly outperforms state-of-the-art techniques in terms of frame and video quality, as well as temporal consistency. Moreover, our method is capable of generating high-quality, long temporal-consistent animation videos with large motions, which is not achievable in previous works. Our code and model are available at https://luckyhzt.github.io/lvcd.},
  archive      = {J_TOG},
  doi          = {10.1145/3687910},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {LVCD: Reference-based lineart video colorization with diffusion models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating visual perception of object motion in dynamic
environments. <em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precisely understanding how objects move in 3D is essential for broad scenarios such as video editing, gaming, driving, and athletics. With screen-displayed computer graphics content, users only perceive limited cues to judge the object motion from the on-screen optical flow. Conventionally, visual perception is studied with stationary settings and singular objects. However, in practical applications, we---the observer---also move within complex scenes. Therefore, we must extract object motion from a combined optical flow displayed on screen, which can often lead to mis-estimations due to perceptual ambiguities. We measure and model observers&#39; perceptual accuracy of object motions in dynamic 3D environments, a universal but under-investigated scenario in computer graphics applications. We design and employ a crowdsourcing-based psychophysical study, quantifying the relationships among patterns of scene dynamics and content, and the resulting perceptual judgments of object motion direction. The acquired psychophysical data underpins a model for generalized conditions. We then demonstrate the model&#39;s guidance ability to significantly enhance users&#39; understanding of task object motion in gaming and animation design. With applications in measuring and compensating for object motion errors in video and rendering, we hope the research establishes a new frontier for understanding and mitigating perceptual errors caused by the gap between screen-displayed graphics and the physical world.},
  archive      = {J_TOG},
  doi          = {10.1145/3687912},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Evaluating visual perception of object motion in dynamic environments},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential walk on spheres. <em>TOG</em>, <em>43</em>(6),
1–18. (<a href="https://doi.org/10.1145/3687913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a Monte Carlo method for computing derivatives of the solution to a partial differential equation (PDE) with respect to problem parameters (such as domain geometry or boundary conditions). Derivatives can be evaluated at arbitrary points, without performing a global solve or constructing a volumetric grid or mesh. The method is hence well suited to inverse problems with complex geometry, such as PDE-constrained shape optimization. Like other walk on spheres (WoS) algorithms, our method is trivial to parallelize, and is agnostic to boundary representation (meshes, splines, implicit surfaces, etc. ), supporting large topological changes. We focus in particular on screened Poisson equations, which model diverse problems from scientific and geometric computing. As in differentiable rendering, we jointly estimate derivatives with respect to all parameters---hence, cost does not grow significantly with parameter count. In practice, even noisy derivative estimates exhibit fast, stable convergence for stochastic gradient-based optimization, as we show through examples from thermal design, shape from diffusion, and computer graphics.},
  archive      = {J_TOG},
  doi          = {10.1145/3687913},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differential walk on spheres},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Particle-laden fluid on flow maps. <em>TOG</em>,
<em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework for simulating ink as a particle-laden flow using particle flow maps. Our method addresses the limitations of existing flow-map techniques, which struggle with dissipative forces like viscosity and drag, thereby extending the application scope from solving the Euler equations to solving the Navier-Stokes equations with accurate viscosity and laden-particle treatment. Our key contribution lies in a coupling mechanism for two particle systems, coupling physical sediment particles and virtual flow-map particles on a background grid by solving a Poisson system. We implemented a novel path integral formula to incorporate viscosity and drag forces into the particle flow map process. Our approach enables state-of-the-art simulation of various particle-laden flow phenomena, exemplified by the bulging and breakup of suspension drop tails, torus formation, torus disintegration, and the coalescence of sedimenting drops. In particular, our method delivered high-fidelity ink diffusion simulations by accurately capturing vortex bulbs, viscous tails, fractal branching, and hierarchical structures.},
  archive      = {J_TOG},
  doi          = {10.1145/3687916},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Particle-laden fluid on flow maps},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large scale farm scene modeling from remote sensing imagery.
<em>TOG</em>, <em>43</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3687918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a scalable framework for large-scale farm scene modeling that utilizes remote sensing data, specifically satellite images. Our approach begins by accurately extracting and categorizing the distributions of various scene elements from satellite images into four distinct layers: fields, trees, roads, and grasslands. For each layer, we introduce a set of controllable Parametric Layout Models (PLMs). These models are capable of learning layout parameters from satellite images, enabling them to generate complex, large-scale farm scenes that closely reproduce reality across multiple scales. Additionally, our framework provides intuitive control for users to adjust layout parameters to simulate different stages of crop growth and planting patterns. This adaptability makes our model an excellent tool for graphics and virtual reality applications. Experimental results demonstrate that our approach can rapidly generate a variety of realistic and highly detailed farm scenes with minimal inputs.},
  archive      = {J_TOG},
  doi          = {10.1145/3687918},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Large scale farm scene modeling from remote sensing imagery},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representing long volumetric video with temporal gaussian
hierarchy. <em>TOG</em>, <em>43</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3687919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to address the challenge of reconstructing long volumetric videos from multi-view RGB videos. Recent dynamic view synthesis methods leverage powerful 4D representations, like feature grids or point cloud sequences, to achieve high-quality rendering results. However, they are typically limited to short (1~2s) video clips and often suffer from large memory footprints when dealing with longer videos. To solve this issue, we propose a novel 4D representation, named Temporal Gaussian Hierarchy, to compactly model long volumetric videos. Our key observation is that there are generally various degrees of temporal redundancy in dynamic scenes, which consist of areas changing at different speeds. Motivated by this, our approach builds a multi-level hierarchy of 4D Gaussian primitives, where each level separately describes scene regions with different degrees of content change, and adaptively shares Gaussian primitives to represent unchanged scene content over different temporal segments, thus effectively reducing the number of Gaussian primitives. In addition, the tree-like structure of the Gaussian hierarchy allows us to efficiently represent the scene at a particular moment with a subset of Gaussian primitives, leading to nearly constant GPU memory usage during the training or rendering regardless of the video length. Moreover, we design a Compact Appearance Model that mixes diffuse and view-dependent Gaussians to further minimize the model size while maintaining the rendering quality. We also develop a rasterization pipeline of Gaussian primitives based on the hardware-accelerated technique to improve rendering speed. Extensive experimental results demonstrate the superiority of our method over alternative methods in terms of training cost, rendering speed, and storage usage. To our knowledge, this work is the first approach capable of efficiently handling hours of volumetric video data while maintaining state-of-the-art rendering quality.},
  archive      = {J_TOG},
  doi          = {10.1145/3687919},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Representing long volumetric video with temporal gaussian hierarchy},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resolving collisions in dense 3D crowd animations.
<em>TOG</em>, <em>43</em>(5), 1–14. (<a
href="https://doi.org/10.1145/3687266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel contact-aware method to synthesize highly-dense 3D crowds of animated characters. Existing methods animate crowds by, first, computing the 2D global motion approximating subjects as 2D particles and, then, introducing individual character motions without considering their surroundings. This creates the illusion of a 3D crowd, but, with density, characters frequently intersect each other since character-to-character contact is not modeled. We tackle this issue and propose a general method that considers any crowd animation and resolves existing residual collisions. To this end, we take a physics-based approach to model contacts between articulated characters. This enables the real-time synthesis of 3D high-density crowds with dozens of individuals that do not intersect each other, producing an unprecedented level of physical correctness in animations. Under the hood, we model each individual using a parametric human body incorporating a set of 3D proxies to approximate their volume. We then build a large system of articulated rigid bodies, and use an efficient physics-based approach to solve for individual body poses that do not collide with each other while maintaining the overall motion of the crowd. We first validate our approach objectively and quantitatively. We then explore relations between physical correctness and perceived realism based on an extensive user study that evaluates the relevance of solving contacts in dense crowds. Results demonstrate that our approach outperforms existing methods for crowd animation in terms of geometric accuracy and overall realism.},
  archive      = {J_TOG},
  doi          = {10.1145/3687266},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Resolving collisions in dense 3D crowd animations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ControlMat: A controlled generative approach to material
capture. <em>TOG</em>, <em>43</em>(5), 1–17. (<a
href="https://doi.org/10.1145/3688830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material reconstruction from a photograph is a key component of 3D content creation democratization. We propose to formulate this ill-posed problem as a controlled synthesis one, leveraging the recent progress in generative deep networks. We present ControlMat, a method which, given a single photograph with uncontrolled illumination as input, conditions a diffusion model to generate plausible, tileable, high-resolution physically-based digital materials. We carefully analyze the behavior of diffusion models for multi-channel outputs, adapt the sampling process to fuse multi-scale information and introduce rolled diffusion to enable both tileability and patched diffusion for high-resolution outputs. Our generative approach further permits exploration of a variety of materials that could correspond to the input image, mitigating the unknown lighting conditions. We show that our approach outperforms recent inference and latent-space optimization methods, and we carefully validate our diffusion process design choices. 1},
  archive      = {J_TOG},
  doi          = {10.1145/3688830},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {ControlMat: A controlled generative approach to material capture},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analytic rotation-invariant modelling of anisotropic finite
elements. <em>TOG</em>, <em>43</em>(5), 1–20. (<a
href="https://doi.org/10.1145/3666086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anisotropic hyperelastic distortion energies are used to solve many problems in fields like computer graphics and engineering with applications in shape analysis, deformation, design, mesh parameterization, biomechanics, and more. However, formulating a robust anisotropic energy that is low order and yet sufficiently non-linear remains a challenging problem for achieving the convergence promised by Newton-type methods in numerical optimization. In this article, we propose a novel analytic formulation of an anisotropic energy that is smooth everywhere, low order, rotationally invariant, and at least twice differentiable. At its core, our approach utilizes implicit rotation factorizations with invariants of the Cauchy-Green tensor that arises from the deformation gradient. The versatility and generality of our analysis is demonstrated through a variety of examples, where we also show that the constitutive law suggested by the anisotropic version of the well-known As-Rigid-As-Possible energy is the foundational parametric description of both passive and active elastic materials. The generality of our approach means that we can systematically derive the force and force-Jacobian expressions for use in implicit and quasistatic numerical optimization schemes, and we can also use our analysis to rewrite, simplify, and speed up several existing anisotropic and isotropic distortion energies with guaranteed inversion safety.},
  archive      = {J_TOG},
  doi          = {10.1145/3666086},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Analytic rotation-invariant modelling of anisotropic finite elements},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Near-realtime facial animation by deep 3D simulation
super-resolution. <em>TOG</em>, <em>43</em>(5), 1–20. (<a
href="https://doi.org/10.1145/3670687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a neural network-based simulation super-resolution framework that can efficiently and realistically enhance a facial performance produced by a low-cost, real-time physics-based simulation to a level of detail that closely approximates that of a reference-quality off-line simulator with much higher resolution (27× element count in our examples) and accurate physical modeling. Our approach is rooted in our ability to construct a training set of paired frames, from the low- and high-resolution simulators respectively, that are in semantic correspondence with each other. We use face animation as an exemplar of such a simulation domain, where creating this semantic congruence is achieved by simply dialing in the same muscle actuation controls and skeletal pose in the two simulators. Our proposed neural network super-resolution framework generalizes from this training set to unseen expressions, compensates for modeling discrepancies between the two simulations due to limited resolution or cost-cutting approximations in the real-time variant, and does not require any semantic descriptors or parameters to be provided as input, other than the result of the real-time simulation. We evaluate the efficacy of our pipeline on a variety of expressive performances and provide comparisons and ablation experiments for plausible variations and alternatives to our proposed scheme. Our code is available at https://github.com/hjoonpark/3d-sim-super- res.git.},
  archive      = {J_TOG},
  doi          = {10.1145/3670687},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Near-realtime facial animation by deep 3D simulation super-resolution},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A closest point method for PDEs on manifolds with interior
boundary conditions for geometry processing. <em>TOG</em>,
<em>43</em>(5), 1–26. (<a
href="https://doi.org/10.1145/3673652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many geometry processing techniques require the solution of partial differential equations (PDEs) on manifolds embedded in ℝ 2 or ℝ 3 , such as curves or surfaces. Such manifold PDEs often involve boundary conditions (e.g., Dirichlet or Neumann) prescribed at points or curves on the manifold’s interior or along the geometric (exterior) boundary of an open manifold. However, input manifolds can take many forms (e.g., triangle meshes, parametrizations, point clouds, implicit functions, etc.). Typically, one must generate a mesh to apply finite element-type techniques or derive specialized discretization procedures for each distinct manifold representation. We propose instead to address such problems in a unified manner through a novel extension of the closest point method (CPM) to handle interior boundary conditions. CPM solves the manifold PDE by solving a volumetric PDE defined over the Cartesian embedding space containing the manifold and requires only a closest point representation of the manifold. Hence, CPM supports objects that are open or closed, orientable or not, and of any codimension. To enable support for interior boundary conditions, we derive a method that implicitly partitions the embedding space across interior boundaries. CPM’s finite difference and interpolation stencils are adapted to respect this partition while preserving second-order accuracy. Additionally, we develop an efficient sparse-grid implementation and numerical solver that can scale to tens of millions of degrees of freedom, allowing PDEs to be solved on more complex manifolds. We demonstrate our method’s convergence behavior on selected model PDEs and explore several geometry processing problems: diffusion curves on surfaces, geodesic distance, tangent vector field design, harmonic map construction, and reaction-diffusion textures. Our proposed approach thus offers a powerful and flexible new tool for a range of geometry processing tasks on general manifold representations.},
  archive      = {J_TOG},
  doi          = {10.1145/3673652},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {1-26},
  shortjournal = {ACM Trans. Graph.},
  title        = {A closest point method for PDEs on manifolds with interior boundary conditions for geometry processing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MERCI: Mixed curvature-based elements for computing
equilibria of thin elastic ribbons. <em>TOG</em>, <em>43</em>(5), 1–26.
(<a href="https://doi.org/10.1145/3674502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thin elastic ribbons represent a class of intermediary objects lying in-between thin elastic plates and thin elastic rods. Although the two latter families of thin structures have received much interest from the Computer Graphics community over the last decades, ribbons have seldom been considered and modelled numerically so far, in spite of a growing number of applications in Computer Design. In this article, starting from the reduced developable ribbon models [Sadowsky 1929 ; Wunderlich 1962 ] recently popularised in Soft Matter Physics, we propose a both accurate and efficient algorithm for computing the statics of thin elastic ribbons. Inspired by the super-clothoid model for thin elastic rods, our method relies on compact ribbon elements whose normal curvature varies linearly with respect to arc length s , while their geodesic torsion is quadratic in s . In contrast, however, for the sake of efficiency, our algorithm avoids building a fully reduced kinematic chain and instead treats each element independently, gluing them only at the final solving stage through well-chosen bilateral constraints. Thanks to this mixed variational strategy, which yields a banded Hessian, our algorithm recovers the linear complexity of low-order models while preserving the high-order convergence of curvature-based models. As a result, our approach is scalable to a large number of elements, and suitable for various boundary conditions and unilateral contact constraints, making it possible to handle challenging scenarios such as confined buckling experiments or Möbius bands with contact. Remarkably, our mixed algorithm proves an order of magnitude faster compared to Discrete Elastic Ribbon models of the literature while achieving, in a few seconds only, high accuracy levels that remain out of reach for such low-order models. Additionally, our numerical model can incorporate various ribbon energies, including the RibExt model for quasi-developable ribbons recently introduced in Physics [Audoly and Neukirch 2021 ], which allows to transition smoothly between a rectangular Kirchhoff rod and a (developable) Sadowsky ribbon. Our numerical scheme is carefully validated against demanding experiments of the Physics literature, which demonstrates its accuracy, efficiency, robustness, and versatility. Our Merci code is publicly available at https://gitlab.inria.fr/elan-public-code/merci for the sake of reproducibility and future benchmarking.},
  archive      = {J_TOG},
  doi          = {10.1145/3674502},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {1-26},
  shortjournal = {ACM Trans. Graph.},
  title        = {MERCI: Mixed curvature-based elements for computing equilibria of thin elastic ribbons},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identity-preserving face swapping via dual surrogate
generative models. <em>TOG</em>, <em>43</em>(5), 1–19. (<a
href="https://doi.org/10.1145/3676165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we revisit the fundamental setting of face-swapping models and reveal that only using implicit supervision for training leads to the difficulty of advanced methods to preserve the source identity. We propose a novel reverse pseudo-input generation approach to offer supplemental data for training face-swapping models, which addresses the aforementioned issue. Unlike the traditional pseudo-label-based training strategy, we assume that arbitrary real facial images could serve as the ground-truth outputs for the face-swapping network and try to generate corresponding input &lt;source, target&gt; pair data. Specifically, we involve a source-creating surrogate that alters the attributes of the real image while keeping the identity, and a target-creating surrogate intends to synthesize attribute-preserved target images with different identities. Our framework, which utilizes proxy-paired data as explicit supervision to direct the face-swapping training process, partially fulfills a credible and effective optimization direction to boost the identity-preserving capability. We design explicit and implicit adaption strategies to better approximate the explicit supervision for face swapping. Quantitative and qualitative experiments on FF++, FFHQ, and wild images show that our framework could improve the performance of various face-swapping pipelines in terms of visual fidelity and ID preserving. Furthermore, we display applications with our method on re-aging, swappable attribute customization, cross-domain, and video face swapping. Code is available under https://github.com/ ICTMCG/CSCS.},
  archive      = {J_TOG},
  doi          = {10.1145/3676165},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Identity-preserving face swapping via dual surrogate generative models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReN human: Learning relightable neural implicit surfaces for
animatable human rendering. <em>TOG</em>, <em>43</em>(5), 1–22. (<a
href="https://doi.org/10.1145/3678002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, implicit neural representation has been widely used to learn the appearance of human bodies in the canonical space, which can be further animated using a parametric human model. However, how to decompose the material properties from the implicit representation for relighting has not yet been investigated thoroughly. We propose to address this problem with a novel framework, ReN Human, that takes sparse or even monocular input videos collected in unconstrained lighting to produce a 3D human representation that can be rendered with novel views, poses, and lighting. Our method represents humans as deformable implicit neural representation and decomposes the geometry, material of humans as well as environment illumination for capturing a relightable and animatable human model. Moreover, we introduce a volumetric lighting grid consisting of spherical Gaussian mixtures to learn the spatially varying illumination and animatable visibility probes to model the dynamic self-occlusion caused by human motion. Specifically, we learn the material property fields and illumination using a physically-based rendering layer that uses Monte Carlo importance sampling to facilitate differentiation of the complex rendering integral. We demonstrate that our approach outperforms recent novel views and poses synthesis methods in a challenging benchmark with sparse videos, enabling high-fidelity human relighting.},
  archive      = {J_TOG},
  doi          = {10.1145/3678002},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {1-22},
  shortjournal = {ACM Trans. Graph.},
  title        = {ReN human: Learning relightable neural implicit surfaces for animatable human rendering},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IMESH: A DSL for mesh processing. <em>TOG</em>,
<em>43</em>(5), 1–17. (<a
href="https://doi.org/10.1145/3662181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh processing algorithms are often communicated via concise mathematical notation (e.g., summation over mesh neighborhoods). However, conversion of notation into working code remains a time-consuming and error-prone process, which requires arcane knowledge of low-level data structures and libraries—impeding rapid exploration of high-level algorithms. We address this problem by introducing a domain-specific language (DSL) for mesh processing called I MESH, which resembles notation commonly used in visual and geometric computing and automates the process of converting notation into code. The centerpiece of our language is a flexible notation for specifying and manipulating neighborhoods of a cell complex, internally represented via standard operations on sparse boundary matrices. This layered design enables natural expression of algorithms while minimizing demands on a code generation backend. In particular, by integrating I MESH with the linear algebra features of the I LA DSL and adding support for automatic differentiation, we can rapidly implement a rich variety of algorithms on point clouds, surface meshes, and volume meshes.},
  archive      = {J_TOG},
  doi          = {10.1145/3662181},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {5},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {IMESH: A DSL for mesh processing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Plug-and-play algorithms for dynamic non-line-of-sight
imaging. <em>TOG</em>, <em>43</em>(5), 1–12. (<a
href="https://doi.org/10.1145/3665139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-line-of-sight (NLOS) imaging has the ability to recover 3D images of scenes outside the direct line of sight, which is of growing interest for diverse applications. Despite the remarkable progress, NLOS imaging of dynamic objects is still challenging. It requires a large amount of multibounce photons for the reconstruction of single-frame data. To overcome this obstacle, we develop a computational framework for dynamic time-of-flight NLOS imaging based on plug-and-play (PnP) algorithms. By combining imaging forward model with the deep denoising network from the computer vision community, we show a 4 frames-per-second (fps) 3D NLOS video recovery (128 × 128 × 512) in post-processing. Our method leverages the temporal similarity among adjacent frames and incorporates sparse priors and frequency filtering. This enables higher-quality reconstructions for complex scenes. Extensive experiments are conducted to verify the superior performance of our proposed algorithm both through simulations and real data.},
  archive      = {J_TOG},
  doi          = {10.1145/3665139},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {5},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Plug-and-play algorithms for dynamic non-line-of-sight imaging},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for solving parabolic partial differential
equations on discrete domains. <em>TOG</em>, <em>43</em>(5), 1–14. (<a
href="https://doi.org/10.1145/3666087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for solving a class of parabolic partial differential equations on triangle mesh surfaces, including the Hamilton-Jacobi equation and the Fokker-Planck equation. PDE in this class often have nonlinear or stiff terms that cannot be resolved with standard methods on curved triangle meshes. To address this challenge, we leverage a splitting integrator combined with a convex optimization step to solve these PDE. Our machinery can be used to compute entropic approximation of optimal transport distances on geometric domains, overcoming the numerical limitations of the state-of-the-art method. In addition, we demonstrate the versatility of our method on a number of linear and nonlinear PDE that appear in diffusion and front propagation tasks in geometry processing.},
  archive      = {J_TOG},
  doi          = {10.1145/3666087},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {5},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A framework for solving parabolic partial differential equations on discrete domains},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SketchDream: Sketch-based text-to-3D generation and editing.
<em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing text-based 3D generation methods generate attractive results but lack detailed geometry control. Sketches, known for their conciseness and expressiveness, have contributed to intuitive 3D modeling but are confined to producing texture-less mesh models within predefined categories. Integrating sketch and text simultaneously for 3D generation promises enhanced control over geometry and appearance but faces challenges from 2D-to-3D translation ambiguity and multi-modal condition integration. Moreover, further editing of 3D models in arbitrary views will give users more freedom to customize their models. However, it is difficult to achieve high generation quality, preserve unedited regions, and manage proper interactions between shape components. To solve the above issues, we propose a text-driven 3D content generation and editing method, SketchDream, which supports NeRF generation from given hand-drawn sketches and achieves free-view sketch-based local editing. To tackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view image generation diffusion model, which leverages depth guidance to establish spatial correspondence. A 3D ControlNet with a 3D attention module is utilized to control multi-view images and ensure their 3D consistency. To support local editing, we further propose a coarse-to-fine editing approach: the coarse phase analyzes component interactions and provides 3D masks to label edited regions, while the fine stage generates realistic results with refined details by local enhancement. Extensive experiments validate that our method generates higher-quality results compared with a combination of 2D ControlNet and image-to-3D generation techniques and achieves detailed control compared with existing diffusion-based 3D editing approaches.},
  archive      = {J_TOG},
  doi          = {10.1145/3658120},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {SketchDream: Sketch-based text-to-3D generation and editing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From microfacets to participating media: A unified theory of
light transport with stochastic geometry. <em>TOG</em>, <em>43</em>(4),
1–17. (<a href="https://doi.org/10.1145/3658121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic geometry models have enjoyed immense success in graphics for modeling interactions of light with complex phenomena such as participating media, rough surfaces, fibers, and more. Although each of these models operates on the same principle of replacing intricate geometry by a random process and deriving the average light transport across all instances thereof, they are each tailored to one specific application and are fundamentally distinct. Each type of stochastic geometry present in the scene is firmly encapsulated in its own appearance model, with its own statistics and light transport average, and no cross-talk between different models or deterministic and stochastic geometry is possible. In this paper, we derive a theory of light transport on stochastic implicit surfaces , a geometry model capable of expressing deterministic geometry, microfacet surfaces, participating media, and an exciting new continuum in between containing aggregate appearance, non-classical media, and more. Our model naturally supports spatial correlations , missing from most existing stochastic models. Our theory paves the way for tractable rendering of scenes in which all geometry is described by the same stochastic model, while leaving ample future work for developing efficient sampling and rendering algorithms.},
  archive      = {J_TOG},
  doi          = {10.1145/3658121},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {From microfacets to participating media: A unified theory of light transport with stochastic geometry},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable geodesic distance for intrinsic minimization
on triangle meshes. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing intrinsic distances on discrete surfaces is at the heart of many minimization problems in geometry processing and beyond. Solving these problems is extremely challenging as it demands the computation of on-surface distances along with their derivatives. We present a novel approach for intrinsic minimization of distance-based objectives defined on triangle meshes. Using a variational formulation of shortest-path geodesics, we compute first and second-order distance derivatives based on the implicit function theorem, thus opening the door to efficient Newton-type minimization solvers. We demonstrate our differentiable geodesic distance framework on a wide range of examples, including geodesic networks and membranes on surfaces of arbitrary genus, two-way coupling between hosting surface and embedded system, differentiable geodesic Voronoi diagrams, and efficient computation of Karcher means on complex shapes. Our analysis shows that second-order descent methods based on our differentiable geodesics outperform existing first-order and quasi-Newton methods by large margins.},
  archive      = {J_TOG},
  doi          = {10.1145/3658122},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable geodesic distance for intrinsic minimization on triangle meshes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solid knitting. <em>TOG</em>, <em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce solid knitting, a new fabrication technique that combines the layer-by-layer volumetric approach of 3D printing with the topologically-entwined stitch structure of knitting to produce solid 3D objects. We define the basic building blocks of solid knitting and demonstrate a working prototype of a solid knitting machine controlled by a low-level instruction language, along with a volumetric design tool for creating machine-knittable patterns. Solid knitting uses a course-wale-layer structure, where every loop in a solid-knit object passes through both a loop from the previous layer and a loop from the previous course. Our machine uses two beds of latch needles to create stitches like a conventional V-bed knitting machine, but augments these needles with a pair of rotating hook arrays to provide storage locations for all of the loops in one layer of the object. It can autonomously produce solid-knit prisms of arbitrary length, although it requires manual intervention to cast on the first layer and bind off the final row. Our design tool allows users to create solid knitting patterns by connecting elementary stitches; objects designed in our interface can---after basic topological checks and constraint propagation---be exported as a sequence of instructions for fabrication on the solid knitting machine. We validate our solid knitting hardware and software on prism examples, detail the mechanical errors which we have encountered, and discuss potential extensions to the capability of our solid knitting machine.},
  archive      = {J_TOG},
  doi          = {10.1145/3658123},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Solid knitting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An induce-on-boundary magnetostatic solver for grid-based
ferrofluids. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel Induce-on-Boundary (IoB) solver designed to address the magnetostatic governing equations of ferrofluids. The IoB solver is based on a single-layer potential and utilizes only the surface point cloud of the object, offering a lightweight, fast, and accurate solution for calculating magnetic fields. Compared to existing methods, it eliminates the need for complex linear system solvers and maintains minimal computational complexities. Moreover, it can be seamlessly integrated into conventional fluid simulators without compromising boundary conditions. Through extensive theoretical analysis and experiments, we validate both the convergence and scalability of the IoB solver, achieving state-of-the-art performance. Additionally, a straightforward coupling approach is proposed and executed to showcase the solver&#39;s effectiveness when integrated into a grid-based fluid simulation pipeline, allowing for realistic simulations of representative ferrofluid instabilities.},
  archive      = {J_TOG},
  doi          = {10.1145/3658124},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {An induce-on-boundary magnetostatic solver for grid-based ferrofluids},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational homogenization for inverse design of
surface-based inflatables. <em>TOG</em>, <em>43</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3658125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface-based inflatables are composed of two thin layers of nearly inextensible sheet material joined together along carefully selected fusing curves. During inflation, pressure forces separate the two sheets to maximize the enclosed volume. The fusing curves restrict this expansion, leading to a spatially varying in-plane contraction and hence metric frustration. The inflated structure settles into a 3D equilibrium that balances pressure forces with the internal elastic forces of the sheets. We present a computational framework for analyzing and designing surface-based inflatable structures with arbitrary fusing patterns. Our approach employs numerical homogenization to characterize the behavior of parametric families of periodic inflatable patch geometries, which can then be combined to tessellate the sheet with smoothly varying patterns. We propose a novel parametrization of the underlying deformation space that allows accurate, efficient, and systematical analysis of the stretching and bending behavior of inflated patches with potentially open boundaries. We apply our homogenization algorithm to create a database of geometrically diverse fusing patterns spanning a wide range of material properties and deformation characteristics. This database is employed in an inverse design algorithm that solves for fusing curves to best approximate a given input target surface. Local patches are selected and blended to form a global network of curves based on a geometric flattening algorithm. These fusing curves are then further optimized to minimize the distance of the deployed structure to target surface. We show that this approach offers greater flexibility to approximate given target geometries compared to previous work while significantly improving structural performance.},
  archive      = {J_TOG},
  doi          = {10.1145/3658125},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational homogenization for inverse design of surface-based inflatables},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PEA-PODs: Perceptual evaluation of algorithms for power
optimization in XR displays. <em>TOG</em>, <em>43</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3658126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Display power consumption is an emerging concern for untethered devices. This goes double for augmented and virtual extended reality (XR) displays, which target high refresh rates and high resolutions while conforming to an ergonomically light form factor. A number of image mapping techniques have been proposed to extend battery usage. However, there is currently no comprehensive quantitative understanding of how the power savings provided by these methods compare to their impact on visual quality. We set out to answer this question. To this end, we present a perceptual evaluation of algorithms (PEA) for power optimization in XR displays (PODs). Consolidating a portfolio of six power-saving display mapping approaches, we begin by performing a large-scale perceptual study to understand the impact of each method on perceived quality in the wild. This results in a unified quality score for each technique, scaled in just-objectionable-difference (JOD) units. In parallel, each technique is analyzed using hardware-accurate power models. The resulting JOD-to-Milliwatt transfer function provides a first-of-its-kind look into tradeoffs offered by display mapping techniques, and can be directly employed to make architectural decisions for power budgets on XR displays. Finally, we leverage our study data and power models to address important display power applications like the choice of display primary, power implications of eye tracking, and more 1 .},
  archive      = {J_TOG},
  doi          = {10.1145/3658126},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {PEA-PODs: Perceptual evaluation of algorithms for power optimization in XR displays},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mesh neural cellular automata. <em>TOG</em>, <em>43</em>(4),
1–16. (<a href="https://doi.org/10.1145/3658127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture modeling and synthesis are essential for enhancing the realism of virtual environments. Methods that directly synthesize textures in 3D offer distinct advantages to the UV-mapping-based methods as they can create seamless textures and align more closely with the ways textures form in nature. We propose Mesh Neural Cellular Automata (MeshNCA), a method that directly synthesizes dynamic textures on 3D meshes without requiring any UV maps. MeshNCA is a generalized type of cellular automata that can operate on a set of cells arranged on non-grid structures such as the vertices of a 3D mesh. MeshNCA accommodates multi-modal supervision and can be trained using different targets such as images, text prompts, and motion vector fields. Only trained on an Icosphere mesh, MeshNCA shows remarkable test-time generalization and can synthesize textures on unseen meshes in real time. We conduct qualitative and quantitative comparisons to demonstrate that MeshNCA outperforms other 3D texture synthesis methods in terms of generalization and producing high-quality textures. Moreover, we introduce a way of grafting trained MeshNCA instances, enabling interpolation between textures. MeshNCA allows several user interactions including texture density/orientation controls, grafting/regenerate brushes, and motion speed/direction controls. Finally, we implement the forward pass of our MeshNCA model using the WebGL shading language and showcase our trained models in an online interactive demo, which is accessible on personal computers and smartphones and is available at https://meshnca.github.io/.},
  archive      = {J_TOG},
  doi          = {10.1145/3658127},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Mesh neural cellular automata},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic digital garment initialization from sewing
patterns. <em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of digital fashion and generative AI technology calls for an automated approach to transform digital sewing patterns into well-fitted garments on human avatars. When given a sewing pattern with its associated sewing relationships, the primary challenge is to establish an initial arrangement of sewing pieces that is free from folding and intersections. This setup enables a physics-based simulator to seamlessly stitch them into a digital garment, avoiding undesirable local minima. To achieve this, we harness AI classification, heuristics, and numerical optimization. This has led to the development of an innovative hybrid system that minimizes the need for user intervention in the initialization of garment pieces. The seeding process of our system involves the training of a classification network for selecting seed pieces, followed by solving an optimization problem to determine their positions and shapes. Subsequently, an iterative selection-arrangement procedure automates the selection of pattern pieces and employs a phased initialization approach to mitigate local minima associated with numerical optimization. Our experiments confirm the reliability, efficiency, and scalability of our system when handling intricate garments with multiple layers and numerous pieces. According to our findings, 68 percent of garments can be initialized with zero user intervention, while the remaining garments can be easily corrected through user operations.},
  archive      = {J_TOG},
  doi          = {10.1145/3658128},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Automatic digital garment initialization from sewing patterns},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BrepGen: A b-rep generative diffusion model with structured
latent geometry. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents BrepGen , a diffusion-based generative approach that directly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD) model. BrepGen represents a B-rep model as a novel structured latent geometry in a hierarchical tree. With the root node representing a whole CAD solid, each element of a B-rep model (i.e., a face, an edge, or a vertex) progressively turns into a child-node from top to bottom. B-rep geometry information goes into the nodes as the global bounding box of each primitive along with a latent code describing the local geometric shape. The B-rep topology information is implicitly represented by node duplication. When two faces share an edge, the edge curve will appear twice in the tree, and a T-junction vertex with three incident edges appears six times in the tree with identical node features. Starting from the root and progressing to the leaf, BrepGen employs Transformer-based diffusion models to sequentially denoise node features while duplicated nodes are detected and merged, recovering the B-Rep topology information. Extensive experiments show that BrepGen advances the task of CAD B-rep generation, surpassing existing methods on various benchmarks. Results on our newly collected furniture dataset further showcase its exceptional capability in generating complicated geometry. While previous methods were limited to generating simple prismatic shapes, BrepGen incorporates free-form and doubly-curved surfaces for the first time. Additional applications of BrepGen include CAD autocomplete and design interpolation. The code, pretrained models, and dataset are available at https://github.com/samxuxiang/BrepGen.},
  archive      = {J_TOG},
  doi          = {10.1145/3658129},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {BrepGen: A B-rep generative diffusion model with structured latent geometry},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Binary opacity grids: Capturing fine geometric detail for
mesh-based view synthesis. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While surface-based view synthesis algorithms are appealing due to their low computational requirements, they often struggle to reproduce thin structures. In contrast, more expensive methods that model the scene&#39;s geometry as a volumetric density field (e.g. NeRF) excel at reconstructing fine geometric detail. However, density fields often represent geometry in a &quot;fuzzy&quot; manner, which hinders exact localization of the surface. In this work, we modify density fields to encourage them to converge towards surfaces, without compromising their ability to reconstruct thin structures. First, we employ a discrete opacity grid representation instead of a continuous density field, which allows opacity values to discontinuously transition from zero to one at the surface. Second, we anti-alias by casting multiple rays per pixel, which allows occlusion boundaries and subpixel structures to be modelled without using semi-transparent voxels. Third, we minimize the binary entropy of the opacity values, which facilitates the extraction of surface geometry by encouraging opacity values to binarize towards the end of training. Lastly, we develop a fusion-based meshing strategy followed by mesh simplification and appearance model fitting. The compact meshes produced by our model can be rendered in real-time on mobile devices and achieve significantly higher view synthesis quality compared to existing mesh-based approaches. Our interactive webdemo is available at https://binary-opacity-grid.github.io.},
  archive      = {J_TOG},
  doi          = {10.1145/3658130},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Binary opacity grids: Capturing fine geometric detail for mesh-based view synthesis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic computation of barycentric coordinates.
<em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a practical and general approach for computing barycentric coordinates through stochastic sampling. Our key insight is a reformulation of the kernel integral defining barycentric coordinates into a weighted least-squares minimization that enables Monte Carlo integration without sacrificing linear precision. Our method can thus compute barycentric coordinates directly at the points of interest, both inside and outside the cage, using just proximity queries to the cage such as closest points and ray intersections. As a result, we can evaluate barycentric coordinates for a large variety of cage representations (from quadrangulated surface meshes to parametric curves) seamlessly, bypassing any volumetric discretization or custom solves. To address the archetypal noise induced by sample-based estimates, we also introduce a denoising scheme tailored to barycentric coordinates. We demonstrate the efficiency and flexibility of our formulation by implementing a stochastic generation of harmonic coordinates, mean-value coordinates, and positive mean-value coordinates.},
  archive      = {J_TOG},
  doi          = {10.1145/3658131},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Stochastic computation of barycentric coordinates},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Specular polynomials. <em>TOG</em>, <em>43</em>(4), 1–13.
(<a href="https://doi.org/10.1145/3658132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding valid light paths that involve specular vertices in Monte Carlo rendering requires solving many non-linear, transcendental equations in high-dimensional space. Existing approaches heavily rely on Newton iterations in path space, which are limited to obtaining at most a single solution each time and easily diverge when initialized with improper seeds. We propose specular polynomials , a Newton iteration-free methodology for finding a complete set of admissible specular paths connecting two arbitrary endpoints in a scene. The core is a reformulation of specular constraints into polynomial systems, which makes it possible to reduce the task to a univariate root-finding problem. We first derive bivariate systems utilizing rational coordinate mapping between the coordinates of consecutive vertices. Subsequently, we adopt the hidden variable resultant method for variable elimination, converting the problem into finding zeros of the determinant of univariate matrix polynomials. This can be effectively solved through Laplacian expansion for one bounce and a bisection solver for more bounces. Our solution is generic, completely deterministic, accurate for the case of one bounce, and GPU-friendly. We develop efficient CPU and GPU implementations and apply them to challenging glints and caustic rendering. Experiments on various scenarios demonstrate the superiority of specular polynomial-based solutions compared to Newton iteration-based counterparts. Our implementation is available at https://github.com/mollnn/spoly.},
  archive      = {J_TOG},
  doi          = {10.1145/3658132},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Specular polynomials},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional mixture path guiding for differentiable
rendering. <em>TOG</em>, <em>43</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3658133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of inverse optimization in physically based differentiable rendering heavily depends on the variance of Monte Carlo estimation. Despite recent advancements emphasizing the necessity of tailored differential sampling strategies, the general approaches remain unexplored. In this paper, we investigate the interplay between local sampling decisions and the estimation of light path derivatives. Considering that modern differentiable rendering algorithms share the same path for estimating differential radiance and ordinary radiance, we demonstrate that conventional guiding approaches, conditioned solely on the last vertex, cannot attain this density. Instead, a mixture of different sampling distributions is required, where the weights are conditioned on all the previously sampled vertices in the path. To embody our theory, we implement a conditional mixture path guiding that explicitly computes optimal weights on the fly. Furthermore, we show how to perform positivization to eliminate sign variance and extend to scenes with millions of parameters. To the best of our knowledge, this is the first generic framework for applying path guiding to differentiable rendering. Extensive experiments demonstrate that our method achieves nearly one order of magnitude improvements over state-of-the-art methods in terms of variance reduction in gradient estimation and errors of inverse optimization. The implementation of our proposed method is available at https://github.com/mollnn/conditional-mixture.},
  archive      = {J_TOG},
  doi          = {10.1145/3658133},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Conditional mixture path guiding for differentiable rendering},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic gesticulator: Semantics-aware co-speech gesture
synthesis. <em>TOG</em>, <em>43</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3658134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present Semantic Gesticulator , a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT&#39;s output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin. We will release the code and dataset for academic research.},
  archive      = {J_TOG},
  doi          = {10.1145/3658134},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Semantic gesticulator: Semantics-aware co-speech gesture synthesis},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A neural network model for efficient musculoskeletal-driven
skin deformation. <em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive neural network to model the deformation of human soft tissues including muscle, tendon, fat and skin. Our approach provides kinematic and active correctives to linear blend skinning [Magnenat-Thalmann et al. 1989] that enhance the realism of soft tissue deformation at modest computational cost. Our network accounts for deformations induced by changes in the underlying skeletal joint state as well as the active contractile state of relevant muscles. Training is done to approximate quasistatic equilibria produced from physics-based simulation of hyperelastic soft tissues in close contact. We use a layered approach to equilibrium data generation where deformation of muscle is computed first, followed by an inner skin/fascia layer, and lastly a fat layer between the fascia and outer skin. We show that a simple network model which decouples the dependence on skeletal kinematics and muscle activation state can produce compelling behaviors with modest training data burden. Active contraction of muscles is estimated using inverse dynamics where muscle moment arms are accurately predicted using the neural network to model kinematic musculotendon geometry. Results demonstrate the ability to accurately replicate compelling musculoskeletal and skin deformation behaviors over a representative range of motions, including the effects of added weights in body building motions.},
  archive      = {J_TOG},
  doi          = {10.1145/3658135},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {A neural network model for efficient musculoskeletal-driven skin deformation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analogist: Out-of-the-box visual in-context learning with
image diffusion model. <em>TOG</em>, <em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual In-Context Learning (ICL) has emerged as a promising research area due to its capability to accomplish various tasks with limited example pairs through analogical reasoning. However, training-based visual ICL has limitations in its ability to generalize to unseen tasks and requires the collection of a diverse task dataset. On the other hand, existing methods in the inference-based visual ICL category solely rely on textual prompts, which fail to capture fine-grained contextual information from given examples and can be time-consuming when converting from images to text prompts. To address these challenges, we propose Analogist, a novel inference-based visual ICL approach that exploits both visual and textual prompting techniques using a text-to-image diffusion model pretrained for image inpainting. For visual prompting, we propose a self-attention cloning (SAC) method to guide the fine-grained structural-level analogy between image examples. For textual prompting, we leverage GPT-4V&#39;s visual reasoning capability to efficiently generate text prompts and introduce a cross-attention masking (CAM) operation to enhance the accuracy of semantic-level analogy guided by text prompts. Our method is out-of-the-box and does not require fine-tuning or optimization. It is also generic and flexible, enabling a wide range of visual tasks to be performed in an in-context manner. Extensive experiments demonstrate the superiority of our method over existing approaches, both qualitatively and quantitatively. Our project webpage is available at https://analogist2d.github.io.},
  archive      = {J_TOG},
  doi          = {10.1145/3658136},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Analogist: Out-of-the-box visual in-context learning with image diffusion model},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoConVQ: Unified physics-based motion control via scalable
discrete representations. <em>TOG</em>, <em>43</em>(4), 1–21. (<a
href="https://doi.org/10.1145/3658137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present MoConVQ, a novel unified framework for physics-based motion control leveraging scalable discrete representations. Building upon vector quantized variational autoencoders (VQ-VAE) and model-based reinforcement learning, our approach effectively learns motion embeddings from a large, unstructured dataset spanning tens of hours of motion examples. The resultant motion representation not only captures diverse motion skills but also offers a robust and intuitive interface for various applications. We demonstrate the versatility of MoConVQ through several applications: universal tracking control from various motion sources, interactive character control with latent motion representations using supervised learning, physics-based motion generation from natural language descriptions using the GPT framework, and, most interestingly, seamless integration with large language models (LLMs) with in-context learning to tackle complex and abstract tasks.},
  archive      = {J_TOG},
  doi          = {10.1145/3658137},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {MoConVQ: Unified physics-based motion control via scalable discrete representations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cybersickness reduction via gaze-contingent image
deformation. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality has ushered in a revolutionary era of immersive content perception. However, a persistent challenge in dynamic environments is the occurrence of cybersickness arising from a conflict between visual and vestibular cues. Prior techniques have demonstrated that limiting illusory self-motion, so-called vection, by blurring the peripheral part of images, introducing tunnel vision, or altering the camera path can effectively reduce the problem. Unfortunately, these methods often alter the user&#39;s experience with visible changes to the content. In this paper, we propose a new technique for reducing vection and combating cybersickness by subtly lowering the screen-space speed of objects in the user&#39;s peripheral vision. The method is motivated by our hypothesis that small modifications to the objects&#39; velocity in the periphery and geometrical distortions in the peripheral vision can remain unnoticeable yet lead to reduced vection. This paper describes the experiments supporting this hypothesis and derives its limits. Furthermore, we present a method that exploits these findings by introducing subtle, screen-space geometrical distortions to animation frames to counteract the motion contributing to vection. We implement the method as a realtime post-processing step that can be integrated into existing rendering frameworks. The final validation of the technique and comparison to an alternative approach confirms its effectiveness in reducing cybersickness.},
  archive      = {J_TOG},
  doi          = {10.1145/3658138},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Cybersickness reduction via gaze-contingent image deformation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spin-weighted spherical harmonics for polarized light
transport. <em>TOG</em>, <em>43</em>(4), 1–24. (<a
href="https://doi.org/10.1145/3658139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of polarization rendering is to simulate the interaction of light with materials exhibiting polarization-dependent behavior. However, integrating polarization into rendering is challenging and increases computational costs significantly. The primary difficulty lies in efficiently modeling and computing the complex reflection phenomena associated with polarized light. Specifically, frequency-domain analysis, essential for efficient environment lighting and storage of complex light interactions, is lacking. To efficiently simulate and reproduce polarized light interactions using frequency-domain techniques, we address the challenge of maintaining continuity in polarized light transport represented by Stokes vectors within angular domains. The conventional spherical harmonics method cannot effectively handle continuity and rotation invariance for Stokes vectors. To overcome this, we develop a new method called polarized spherical harmonics (PSH) based on the spin-weighted spherical harmonics theory. Our method provides a rotation-invariant representation of Stokes vector fields. Furthermore, we introduce frequency domain formulations of polarized rendering equations and spherical convolution based on PSH. We first define spherical convolution on Stokes vector fields in the angular domain, and it also provides efficient computation of polarized light transport, nearly on an entry-wise product in the frequency domain. Our frequency domain formulation, including spherical convolution, led to the development of the first real-time polarization rendering technique under polarized environmental illumination, named precomputed polarized radiance transfer, using our polarized spherical harmonics. Results demonstrate that our method can effectively and accurately simulate and reproduce polarized light interactions in complex reflection phenomena, including polarized environmental illumination and soft shadows.},
  archive      = {J_TOG},
  doi          = {10.1145/3658139},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Spin-weighted spherical harmonics for polarized light transport},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive character control with auto-regressive motion
diffusion models. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time character control is an essential component for interactive experiences, with a broad range of applications, including physics simulations, video games, and virtual reality. The success of diffusion models for image synthesis has led to the use of these models for motion synthesis. However, the majority of these motion diffusion models are primarily designed for offline applications, where space-time models are used to synthesize an entire sequence of frames simultaneously with a pre-specified length. To enable real-time motion synthesis with diffusion model that allows time-varying controls, we propose A-MDM (Auto-regressive Motion Diffusion Model). Our conditional diffusion model takes an initial pose as input, and auto-regressively generates successive motion frames conditioned on the previous frame. Despite its streamlined network architecture, which uses simple MLPs, our framework is capable of generating diverse, long-horizon, and high-fidelity motion sequences. Furthermore, we introduce a suite of techniques for incorporating interactive controls into A-MDM, such as task-oriented sampling, in-painting, and hierarchical reinforcement learning (See Figure 1). These techniques enable a pre-trained A-MDM to be efficiently adapted for a variety of new downstream tasks. We conduct a comprehensive suite of experiments to demonstrate the effectiveness of A-MDM, and compare its performance against state-of-the-art auto-regressive methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3658140},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive character control with auto-regressive motion diffusion models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards motion metamers for foveated rendering.
<em>TOG</em>, <em>43</em>(4), 1–10. (<a
href="https://doi.org/10.1145/3658141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foveated rendering takes advantage of the reduced spatial sensitivity in peripheral vision to greatly reduce rendering cost without noticeable spatial quality degradation. Due to its benefits, it has emerged as a key enabler for real-time high-quality virtual and augmented realities. Interestingly though, a large body of work advocates that a key role of peripheral vision may be motion detection, yet foveated rendering lowers the image quality in these regions, which may impact our ability to detect and quantify motion. The problem is critical for immersive simulations where the ability to detect and quantify movement drives actions and decisions. In this work, we diverge from the contemporary approach towards the goal of foveated graphics, and demonstrate that a loss of high-frequency spatial details in the periphery inhibits motion perception, leading to underestimating motion cues such as velocity. Furthermore, inspired by an interesting visual illusion, we design a perceptually motivated real-time technique that synthesizes controlled spatio-temporal motion energy to offset the loss in motion perception. Finally, we perform user experiments demonstrating our method&#39;s effectiveness in recovering motion cues without introducing objectionable quality degradation.},
  archive      = {J_TOG},
  doi          = {10.1145/3658141},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Towards motion metamers for foveated rendering},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alignment conditions for NURBS-based design of mixed
tension-compression grid shells. <em>TOG</em>, <em>43</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3658142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In architecture, shapes of surfaces that can withstand gravity with no bending action are considered ideal for shell structures. Those shells have special geometries through which they can stream gravitational force toward the ground via stresses strictly tangent to the surface, making them highly efficient. The process of finding these special forms is called form-finding. Recently, [Miki and Mitchell 2022] presented a method to reliably produce mixed tension-compression continuum shells, a type of shells known to be especially difficult to form-find. The key to this method was to use the concept of the Airy stress function to derive a valid bending-free shell shape by iterating on both the shell shape and the Airy stress function; this turns a problem that is over-constrained in general into a problem with many solutions. In [Miki and Mitchell 2022], it was proposed that the method could also be used to design grid shells by tracing curves on a continuum shell such that the resulting grid has bars that are both bending-free and form flat panels, a property useful for construction of real grid shells made of glass and steel. However, this special type of grid is not guaranteed to exist in general on a mixed-tension compression shell, even when the shell is in bending-free equilibrium [Miki and Mitchell 2023]. Additional conditions must be imposed on the shell shape to guarantee the existence of simultaneously bending-free and conjugate grid directions. The current study resolves the existence issue by adding alignment conditions. We consider several practical curve alignment conditions: alignment with the lines of curvature of the shell, approximate alignment with a bidirectional set of user-prescribed guide curves, and exact alignment with a single direction of user-prescribed guide curves. We report that the variable projection method originally used to solve the form-finding problem in the work of [Miki and Mitchell 2022] can be successfully extended to solve the newly introduced alignment conditions, and conclude with results for several practical design examples. To our knowledge, this is the first method that can take a user-input grid and find a &quot;nearby&quot; grid that is both flat-panelled and in bending-free equilibrium for the general case of mixed tension-compression grid shells.},
  archive      = {J_TOG},
  doi          = {10.1145/3658142},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Alignment conditions for NURBS-based design of mixed tension-compression grid shells},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Super-resolution cloth animation with spatial and temporal
coherence. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating super-resolution cloth animations, which refine coarse cloth meshes with fine wrinkle details, faces challenges in preserving spatial consistency and temporal coherence across frames. In this paper, we introduce a general framework to address these issues, leveraging two core modules. The first module interleaves a simulator and a corrector. The simulator handles cloth dynamics, while the corrector rectifies differences in low-frequency features across various resolutions. This interleaving ensures prompt correction of spatial errors from the coarse simulation, effectively preventing their temporal propagation. The second module performs mesh-based super-resolution for detailed wrinkle enhancements. We decompose garment meshes into overlapping patches for adaptability to various styles and geometric continuity. Our method achieves an 8× improvement in resolution for cloth animations. We showcase the effectiveness of our method through diverse animation examples, including simple cloth pieces and intricate garments.},
  archive      = {J_TOG},
  doi          = {10.1145/3658143},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Super-resolution cloth animation with spatial and temporal coherence},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ColorVideoVDP: A visual difference predictor for image,
video and display distortions. <em>TOG</em>, <em>43</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3658144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ColorVideoVDP is a video and image quality metric that models spatial and temporal aspects of vision for both luminance and color. The metric is built on novel psychophysical models of chromatic spatiotemporal contrast sensitivity and cross-channel contrast masking. It accounts for the viewing conditions, geometric, and photometric characteristics of the display. It was trained to predict common video-streaming distortions (e.g., video compression, rescaling, and transmission errors) and also 8 new distortion types related to AR/VR displays (e.g., light source and waveguide non-uniformities). To address the latter application, we collected our novel XR-Display-Artifact-Video quality dataset (XR-DAVID), comprised of 336 distorted videos. Extensive testing on XR-DAVID, as well as several datasets from the literature, indicate a significant gain in prediction performance compared to existing metrics. ColorVideoVDP opens the doors to many novel applications that require the joint automated spatiotemporal assessment of luminance and color distortions, including video streaming, display specification, and design, visual comparison of results, and perceptually-guided quality optimization. The code for the metric can be found at https://github.com/gfxdisp/ColorVideoVDP.},
  archive      = {J_TOG},
  doi          = {10.1145/3658144},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {ColorVideoVDP: A visual difference predictor for image, video and display distortions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Curvature-driven conformal deformations. <em>TOG</em>,
<em>43</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3658145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel approach for computing conformal deformations in ℝ 3 while minimizing curvature-based energies. Curvature-based energies serve as fundamental tools in geometry processing, essential for tasks such as surface fairing, deformation, and approximation using developable or cone metric surfaces. However, accurately computing the geometric embedding, especially for the latter, has been a challenging endeavor. The complexity arises from inherent numerical instabilities in curvature estimation and the intricate nature of differentiating these energies. To address these challenges, we concentrate on conformal deformations, leveraging the curvature tensor as the primary variable in our model. This strategic choice renders curvature-based energies easily applicable, mitigating previous manipulation difficulties. Our key contribution lies in identifying a previously unknown integrability condition that establishes a connection between conformal deformations and changes in curvature. We use this insight to deform surfaces of arbitrary genus, aiming to minimize bending energies or prescribe Gaussian curvature while sticking to positional constraints.},
  archive      = {J_TOG},
  doi          = {10.1145/3658145},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Curvature-driven conformal deformations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLAY: A controllable large-scale generative model for
creating high-quality 3D assets. <em>TOG</em>, <em>43</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3658146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of digital creativity, our potential to craft intricate 3D worlds from imagination is often hampered by the limitations of existing digital tools, which demand extensive expertise and efforts. To narrow this disparity, we introduce CLAY, a 3D geometry and material generator designed to effortlessly transform human imagination into intricate 3D digital structures. CLAY supports classic text or image inputs as well as 3D-aware controls from diverse primitives (multi-view images, voxels, bounding boxes, point clouds, implicit representations, etc). At its core is a large-scale generative model composed of a multi-resolution Variational Autoencoder (VAE) and a minimalistic latent Diffusion Transformer (DiT), to extract rich 3D priors directly from a diverse range of 3D geometries. Specifically, it adopts neural fields to represent continuous and complete surfaces and uses a geometry generative module with pure transformer blocks in latent space. We present a progressive training scheme to train CLAY on an ultra large 3D model dataset obtained through a carefully designed processing pipeline, resulting in a 3D native geometry generator with 1.5 billion parameters. For appearance generation, CLAY sets out to produce physically-based rendering (PBR) textures by employing a multi-view material diffusion model that can generate 2K resolution textures with diffuse, roughness, and metallic modalities. We demonstrate using CLAY for a range of controllable 3D asset creations, from sketchy conceptual designs to production ready assets with intricate details. Even first time users can easily use CLAY to bring their vivid 3D imaginations to life, unleashing unlimited creativity.},
  archive      = {J_TOG},
  doi          = {10.1145/3658146},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {CLAY: A controllable large-scale generative model for creating high-quality 3D assets},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DressCode: Autoregressively sewing and generating garments
from text guidance. <em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apparel&#39;s significant role in human appearance underscores the importance of garment digitalization for digital human creation. Recent advances in 3D content creation are pivotal for digital human creation. Nonetheless, garment generation from text guidance is still nascent. We introduce a text-driven 3D garment generation framework, DressCode, which aims to democratize design for novices and offer immense potential in fashion design, virtual try-on, and digital human creation. We first introduce SewingGPT, a GPT-based architecture integrating cross-attention with text-conditioned embedding to generate sewing patterns with text guidance. We then tailor a pre-trained Stable Diffusion to generate tile-based Physically-based Rendering (PBR) textures for the garments. By leveraging a large language model, our framework generates CG-friendly garments through natural language interaction. It also facilitates pattern completion and texture editing, streamlining the design process through user-friendly interaction. This framework fosters innovation by allowing creators to freely experiment with designs and incorporate unique elements into their work. With comprehensive evaluations and comparisons with other state-of-the-art methods, our method showcases superior quality and alignment with input prompts. User studies further validate our high-quality rendering results, highlighting its practical utility and potential in production settings. Our project page is https://IHe-KaiI.github.io/DressCode/.},
  archive      = {J_TOG},
  doi          = {10.1145/3658147},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {DressCode: Autoregressively sewing and generating garments from text guidance},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bilateral guided radiance field processing. <em>TOG</em>,
<em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) achieves unprecedented performance in synthesizing novel view synthesis, utilizing multi-view consistency. When capturing multiple inputs, image signal processing (ISP) in modern cameras will independently enhance them, including exposure adjustment, color correction, local tone mapping, etc. While these processings greatly improve image quality, they often break the multi-view consistency assumption, leading to &quot;floaters&quot; in the reconstructed radiance fields. To address this concern without compromising visual aesthetics, we aim to first disentangle the enhancement by ISP at the NeRF training stage and re-apply user-desired enhancements to the reconstructed radiance fields at the finishing stage. Furthermore, to make the re-applied enhancements consistent between novel views, we need to perform imaging signal processing in 3D space (i.e. &quot;3D ISP&quot;). For this goal, we adopt the bilateral grid, a locally-affine model, as a generalized representation of ISP processing. Specifically, we optimize per-view 3D bilateral grids with radiance fields to approximate the effects of camera pipelines for each input view. To achieve user-adjustable 3D finishing, we propose to learn a low-rank 4D bilateral grid from a given single view edit, lifting photo enhancements to the whole 3D scene. We demonstrate our approach can boost the visual quality of novel view synthesis by effectively removing floaters and performing enhancements from user retouching. The source code and our data are available at: https://bilarfpro.github.io.},
  archive      = {J_TOG},
  doi          = {10.1145/3658148},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Bilateral guided radiance field processing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyclogenesis: Simulating hurricanes and tornadoes.
<em>TOG</em>, <em>43</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3658149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyclones are large-scale phenomena that result from complex heat and water transfer processes in the atmosphere, as well as from the interaction of multiple hydrometeors , i.e., water and ice particles. When cyclones make landfall, they are considered natural disasters and spawn dread and awe alike. We propose a physically-based approach to describe the 3D development of cyclones in a visually convincing and physically plausible manner. Our approach allows us to capture large-scale heat and water continuity, turbulent microphysical dynamics of hydrometeors, and mesoscale cyclonic processes within the planetary boundary layer. Modeling these processes enables us to simulate multiple hurricane and tornado phenomena. We evaluate our simulations quantitatively by comparing to real data from storm soundings and observations of hurricane landfall from climatology research. Additionally, qualitative comparisons to previous methods are performed to validate the different parts of our scheme. In summary, our model simulates cyclogenesis in a comprehensive way that allows us to interactively render animations of some of the most complex weather events.},
  archive      = {J_TOG},
  doi          = {10.1145/3658149},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Cyclogenesis: Simulating hurricanes and tornadoes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transparent image layer diffusion using latent transparency.
<em>TOG</em>, <em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach enabling large-scale pretrained latent diffusion models to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a &quot;latent transparency&quot; that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model. It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc. A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock.},
  archive      = {J_TOG},
  doi          = {10.1145/3658150},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Transparent image layer diffusion using latent transparency},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fabric tessellation: Realizing freeform surfaces by
smocking. <em>TOG</em>, <em>43</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3658151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method for realizing freeform surfaces with pieces of flat fabric, where curvature is created by stitching together points on the fabric using a technique known as smocking. Smocking is renowned for producing intricate geometric textures with voluminous pleats. However, it has been mostly used to realize flat shapes or manually designed, limited classes of curved surfaces. Our method combines the computation of directional fields with continuous optimization of a Tangram graph in the plane, which together allow us to realize surfaces of arbitrary topology and curvature with smocking patterns of diverse symmetries. Given a target surface and the desired smocking pattern, our method outputs a corresponding 2D smocking pattern that can be fabricated by sewing specified points together. The resulting textile fabrication approximates the target shape and exhibits visually pleasing pleats. We validate our method through physical fabrication of various smocked examples.},
  archive      = {J_TOG},
  doi          = {10.1145/3658151},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fabric tessellation: Realizing freeform surfaces by smocking},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable voronoi diagrams for simulation of cell-based
mechanical systems. <em>TOG</em>, <em>43</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3658152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigating topological transitions in cellular mechanical systems is a significant challenge for existing simulation methods. While abstract models lack predictive capabilities at the cellular level, explicit network representations struggle with topology changes, and per-cell representations are computationally too demanding for large-scale simulations. To address these challenges, we propose a novel cell-centered approach based on differentiable Voronoi diagrams. Representing each cell with a Voronoi site, our method defines shape and topology of the interface network implicitly. In this way, we substantially reduce the number of problem variables, eliminate the need for explicit contact handling, and ensure continuous geometry changes during topological transitions. Closed-form derivatives of network positions facilitate simulation with Newton-type methods for a wide range of per-cell energies. Finally, we extend our differentiable Voronoi diagrams to enable coupling with arbitrary rigid and deformable boundaries. We apply our approach to a diverse set of examples, highlighting splitting and merging of cells as well as neighborhood changes. We illustrate applications to inverse problems by matching soap foam simulations to real-world images. Comparative analysis with explicit cell models reveals that our method achieves qualitatively comparable results at significantly faster computation times.},
  archive      = {J_TOG},
  doi          = {10.1145/3658152},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable voronoi diagrams for simulation of cell-based mechanical systems},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Walkin’ robin: Walk on stars with robin boundary conditions.
<em>TOG</em>, <em>43</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3658153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous scientific and engineering applications require solutions to boundary value problems (BVPs) involving elliptic partial differential equations, such as the Laplace or Poisson equations, on geometrically intricate domains. We develop a Monte Carlo method for solving such BVPs with arbitrary first-order linear boundary conditions---Dirichlet, Neumann, and Robin. Our method directly generalizes the walk on stars (WoSt) algorithm, which previously tackled only the first two types of boundary conditions, with a few simple modifications. Unlike conventional numerical methods, WoSt does not need finite element meshing or global solves. Similar to Monte Carlo rendering, it instead computes pointwise solution estimates by simulating random walks along star-shaped regions inside the BVP domain, using efficient ray-intersection and distance queries. To ensure WoSt produces bounded-variance estimates in the presence of Robin boundary conditions, we show that it is sufficient to modify how WoSt selects the size of these star-shaped regions. Our generalized WoSt algorithm reduces estimation error by orders of magnitude relative to alternative grid-free methods such as the walk on boundary algorithm. We also develop bidirectional and boundary value caching strategies to further reduce estimation error. Our algorithm is trivial to parallelize, scales sublinearly with increasing geometric detail, and enables progressive and view-dependent evaluation.},
  archive      = {J_TOG},
  doi          = {10.1145/3658153},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Walkin’ robin: Walk on stars with robin boundary conditions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Position-based nonlinear gauss-seidel for quasistatic
hyperelasticity. <em>TOG</em>, <em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Position based dynamics [Müller et al. 2007] is a powerful technique for simulating a variety of materials. Its primary strength is its robustness when run with limited computational budget. Even though PBD is based on the projection of static constraints, it does not work well for quasistatic problems. This is particularly relevant since the efficient creation of large data sets of plausible, but not necessarily accurate elastic equilibria is of increasing importance with the emergence of quasistatic neural networks [Bailey et al. 2018; Chentanez et al. 2020; Jin et al. 2022; Luo et al. 2020]. Recent work [Macklin et al. 2016] has shown that PBD can be related to the Gauss-Seidel approximation of a Lagrange multiplier formulation of backward Euler time stepping, where each constraint is solved/projected independently of the others in an iterative fashion. We show that a position-based, rather than constraint-based nonlinear Gauss-Seidel approach resolves a number of issues with PBD, particularly in the quasistatic setting. Our approach retains the essential PBD feature of stable behavior with constrained computational budgets, but also allows for convergent behavior with expanded budgets. We demonstrate the efficacy of our method on a variety of representative hyperelastic problems and show that both successive over relaxation (SOR), Chebyshev and multiresolution-based acceleration can be easily applied.},
  archive      = {J_TOG},
  doi          = {10.1145/3658154},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Position-based nonlinear gauss-seidel for quasistatic hyperelasticity},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Split-and-fit: Learning b-reps via structure-aware voronoi
partitioning. <em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel method for acquiring boundary representations (B-Reps) of 3D CAD models which involves a two-step process: it first applies a spatial partitioning , referred to as the &quot;split&quot;, followed by a &quot;fit&quot; operation to derive a single primitive within each partition. Specifically, our partitioning aims to produce the classical Voronoi diagram of the set of ground-truth (GT) B-Rep primitives. In contrast to prior B-Rep constructions which were bottom-up, either via direct primitive fitting or point clustering, our Split-and-Fit approach is top-down and structure-aware , since a Voronoi partition explicitly reveals both the number of and the connections between the primitives. We design a neural network to predict the Voronoi diagram from an input point cloud or distance field via a binary classification. We show that our network, coined NVD-Net for neural Voronoi diagrams, can effectively learn Voronoi partitions for CAD models from training data and exhibits superior generalization capabilities. Extensive experiments and evaluation demonstrate that the resulting B-Reps, consisting of parametric surfaces, curves, and vertices, are more plausible than those obtained by existing alternatives, with significant improvements in reconstruction quality. Code will be released on https://github.com/yilinliu77/NVDNet.},
  archive      = {J_TOG},
  doi          = {10.1145/3658155},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Split-and-fit: Learning B-reps via structure-aware voronoi partitioning},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3Doodle: Compact abstraction of objects with 3D strokes.
<em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While free-hand sketching has long served as an efficient representation to convey characteristics of an object, they are often subjective, deviating significantly from realistic representations. Moreover, sketches are not consistent for arbitrary viewpoints, making it hard to catch 3D shapes. We propose 3Dooole, generating descriptive and view-consistent sketch images given multi-view images of the target object. Our method is based on the idea that a set of 3D strokes can efficiently represent 3D structural information and render view-consistent 2D sketches. We express 2D sketches as a union of view-independent and view-dependent components. 3D cubic Bézier curves indicate view-independent 3D feature lines, while contours of superquadrics express a smooth outline of the volume of varying viewpoints. Our pipeline directly optimizes the parameters of 3D stroke primitives to minimize perceptual losses in a fully differentiable manner. The resulting sparse set of 3D strokes can be rendered as abstract sketches containing essential 3D characteristic shapes of various objects. We demonstrate that 3Doodle can faithfully express concepts of the original images compared with recent sketch generation approaches. 1},
  archive      = {J_TOG},
  doi          = {10.1145/3658156},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {3Doodle: Compact abstraction of objects with 3D strokes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training-free consistent text-to-image generation.
<em>TOG</em>, <em>43</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3658157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy persubject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory , a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.},
  archive      = {J_TOG},
  doi          = {10.1145/3658157},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Training-free consistent text-to-image generation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Surface-filling curve flows via implicit medial axes.
<em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a fast, robust, and user-controllable algorithm to generate surface-filling curves. We compute these curves through the gradient flow of a simple sparse energy, making our method several orders of magnitude faster than previous works. Our algorithm makes minimal assumptions on the topology and resolution of the input surface, achieving improved robustness. Our framework provides tuneable parameters that guide the shape of the output curve, making it ideal for interactive design applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3658158},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Surface-filling curve flows via implicit medial axes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CWF: Consolidating weak features in high-quality mesh
simplification. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mesh simplification, common requirements like accuracy, triangle quality, and feature alignment are often considered as a trade-off. Existing algorithms concentrate on just one or a few specific aspects of these requirements. For example, the well-known Quadric Error Metrics (QEM) approach [Garland and Heckbert 1997] prioritizes accuracy and can preserve strong feature lines/points as well, but falls short in ensuring high triangle quality and may degrade weak features that are not as distinctive as strong ones. In this paper, we propose a smooth functional that simultaneously considers all of these requirements. The functional comprises a normal anisotropy term and a Centroidal Voronoi Tessellation (CVT) [Du et al. 1999] energy term, with the variables being a set of movable points lying on the surface. The former inherits the spirit of QEM but operates in a continuous setting, while the latter encourages even point distribution, allowing various surface metrics. We further introduce a decaying weight to automatically balance the two terms. We selected 100 CAD models from the ABC dataset [Koch et al. 2019], along with 21 organic models, to compare the existing mesh simplification algorithms with ours. Experimental results reveal an important observation: the introduction of a decaying weight effectively reduces the conflict between the two terms and enables the alignment of weak features. This distinctive feature sets our approach apart from most existing mesh simplification methods and demonstrates significant potential in shape understanding. Please refer to the teaser figure for illustration.},
  archive      = {J_TOG},
  doi          = {10.1145/3658159},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {CWF: Consolidating weak features in high-quality mesh simplification},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical 3D gaussian representation for real-time
rendering of very large datasets. <em>TOG</em>, <em>43</em>(4), 1–15.
(<a href="https://doi.org/10.1145/3658160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel view synthesis has seen major advances in recent years, with 3D Gaussian splatting offering an excellent level of visual quality, fast training and real-time rendering. However, the resources needed for training and rendering inevitably limit the size of the captured scenes that can be represented with good visual quality. We introduce a hierarchy of 3D Gaussians that preserves visual quality for very large scenes, while offering an efficient Level-of-Detail (LOD) solution for efficient rendering of distant content with effective level selection and smooth transitions between levels. We introduce a divide-and-conquer approach that allows us to train very large scenes in independent chunks. We consolidate the chunks into a hierarchy that can be optimized to further improve visual quality of Gaussians merged into intermediate nodes. Very large captures typically have sparse coverage of the scene, presenting many challenges to the original 3D Gaussian splatting training method; we adapt and regularize training to account for these issues. We present a complete solution, that enables real-time rendering of very large scenes and can adapt to available resources thanks to our LOD method. We show results for captured scenes with up to tens of thousands of images with a simple and affordable rig, covering trajectories of up to several kilometers and lasting up to one hour.},
  archive      = {J_TOG},
  doi          = {10.1145/3658160},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A hierarchical 3D gaussian representation for real-time rendering of very large datasets},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stylized rendering as a function of expectation.
<em>TOG</em>, <em>43</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3658161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a generalization of the rendering equation that captures both the realistic light transport of physically-based rendering (PBR) and a subset of non-photorealistic rendering (NPR) stylizations in a principled manner. The proposed formulation is based on the key observation that both classical transport and certain NPR stylizations can be modeled as a function of expectation. Given this observation, we generalize the recursive integrals of the rendering equation to recursive functions of expectation. As estimating functions of expectation can be challenging, especially recursive ones, we provide a toolkit for unbiased and biased estimation comprising prior work, general strategies, and a novel build-your-own strategy for constructing more complex unbiased estimators from simpler unbiased estimators. We then use this toolkit to construct a complete estimator for the proposed recursive formulation, and implement a sampling algorithm that is both conceptually simple and leverages many of the components of an ordinary path tracer. To demonstrate the practicality of the proposed method we showcase how it captures several existing stylizations like color mapping, cel shading, and cross-hatching, fuses NPR and PBR visuals, and allows us to explore visuals that were previously challenging under existing formulations.},
  archive      = {J_TOG},
  doi          = {10.1145/3658161},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Stylized rendering as a function of expectation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Portrait3D: Text-guided high-quality 3D portrait generation
using pyramid representation and GANs prior. <em>TOG</em>,
<em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing neural rendering-based text-to-3D-portrait generation methods typically make use of human geometry prior and diffusion models to obtain guidance. However, relying solely on geometry information introduces issues such as the Janus problem, over-saturation, and over-smoothing. We present Portrait3D , a novel neural rendering-based framework with a novel joint geometry-appearance prior to achieve text-to-3D-portrait generation that overcomes the aforementioned issues. To accomplish this, we train a 3D portrait generator, 3DPortraitGAN, as a robust prior. This generator is capable of producing 360° canonical 3D portraits, serving as a starting point for the subsequent diffusion-based generation process. To mitigate the &quot;grid-like&quot; artifact caused by the high-frequency information in the feature-map-based 3D representation commonly used by most 3D-aware GANs, we integrate a novel pyramid tri-grid 3D representation into 3DPortraitGAN. To generate 3D portraits from text, we first project a randomly generated image aligned with the given prompt into the pre-trained 3DPortraitGAN&#39;s latent space. The resulting latent code is then used to synthesize a pyramid tri-grid. Beginning with the obtained pyramid tri-grid , we use score distillation sampling to distill the diffusion model&#39;s knowledge into the pyramid tri-grid. Following that, we utilize the diffusion model to refine the rendered images of the 3D portrait and then use these refined images as training data to further optimize the pyramid tri-grid , effectively eliminating issues with unrealistic color and unnatural artifacts. Our experimental results show that Portrait3D can produce realistic, high-quality, and canonical 3D portraits that align with the prompt.},
  archive      = {J_TOG},
  doi          = {10.1145/3658162},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Portrait3D: Text-guided high-quality 3D portrait generation using pyramid representation and GANs prior},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural gaussian scale-space fields. <em>TOG</em>,
<em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian scale spaces are a cornerstone of signal representation and processing, with applications in filtering, multiscale analysis, anti-aliasing, and many more. However, obtaining such a scale space is costly and cumbersome, in particular for continuous representations such as neural fields. We present an efficient and lightweight method to learn the fully continuous, anisotropic Gaussian scale space of an arbitrary signal. Based on Fourier feature modulation and Lipschitz bounding, our approach is trained self-supervised, i.e., training does not require any manual filtering. Our neural Gaussian scale-space fields faithfully capture multiscale representations across a broad range of modalities, and support a diverse set of applications. These include images, geometry, light-stage data, texture anti-aliasing, and multiscale optimization.},
  archive      = {J_TOG},
  doi          = {10.1145/3658163},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural gaussian scale-space fields},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Going with the flow. <em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a sequence of poses of a body we study the motion resulting when the body is immersed in a (possibly) moving, incompressible medium. With the poses given, say, by an animator, the governing second-order ordinary differential equations are those of a rigid body with time-dependent inertia acted upon by various forces. Some of these forces, like lift and drag, depend on the motion of the body in the surrounding medium. Additionally, the inertia must encode the effect of the medium through its added mass. We derive the corresponding dynamics equations which generalize the standard rigid body dynamics equations. All forces are based on local computations using only physical parameters such as mass density. Notably, we approximate the effect of the medium on the body through local computations avoiding any global simulation of the medium. Consequently, the system of equations we must integrate in time is only 6 dimensional (rotation and translation). Our proposed algorithm displays linear complexity and captures intricate natural phenomena that depend on body-fluid interactions.},
  archive      = {J_TOG},
  doi          = {10.1145/3658164},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Going with the flow},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A vortex particle-on-mesh method for soap film simulation.
<em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel physically-based vortex fluid model for films, aimed at accurately simulating cascading vortical structures on deforming thin films. Central to our approach is a novel mechanism decomposing the film&#39;s tangential velocity into circulation and dilatation components. These components are then evolved using a hybrid particle-mesh method, enabling the effective reconstruction of three-dimensional tangential velocities and seamlessly integrating surfactant and thickness dynamics into a unified framework. By coupling with its normal component and surface-tension model, our method is particularly adept at depicting complex interactions between in-plane vortices and out-of-plane physical phenomena, such as gravity, surfactant dynamics, and solid boundary, leading to highly realistic simulations of complex thin-film dynamics, achieving an unprecedented level of vortical details and physical realism.},
  archive      = {J_TOG},
  doi          = {10.1145/3658165},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A vortex particle-on-mesh method for soap film simulation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A free-space diffraction BSDF. <em>TOG</em>, <em>43</em>(4),
1–15. (<a href="https://doi.org/10.1145/3658166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free-space diffractions are an optical phenomenon where light appears to &quot;bend&quot; around the geometric edges and corners of scene objects. In this paper we present an efficient method to simulate such effects. We derive an edge-based formulation of Fraunhofer diffraction, which is well suited to the common (triangular) geometric meshes used in computer graphics. Our method dynamically constructs a free-space diffraction BSDF by considering the geometry around the intersection point of a ray of light with an object, and we present an importance sampling strategy for these BSDFs. Our method is unique in requiring only ray tracing to produce free-space diffractions, works with general meshes, requires no geometry preprocessing, and is designed to work with path tracers with a linear rendering equation. We show that we are able to reproduce accurate diffraction lobes, and, in contrast to any existing method, are able to handle complex, real-world geometry. This work serves to connect free-space diffractions to the efficient path tracing tools from computer graphics.},
  archive      = {J_TOG},
  doi          = {10.1145/3658166},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A free-space diffraction BSDF},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Creating LEGO figurines from single images. <em>TOG</em>,
<em>43</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3658167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a computational pipeline for creating personalized, physical LEGO ®1 figurines from user-input portrait photos. The generated figurine is an assembly of coherently-connected LEGO ® bricks detailed with uv-printed decals, capturing prominent features such as hairstyle, clothing style, and garment color, and also intricate details such as logos, text, and patterns. This task is non-trivial, due to the substantial domain gap between unconstrained user photos and the stylistically-consistent LEGO ® figurine models. To ensure assemble-ability by LEGO ® bricks while capturing prominent features and intricate details, we design a three-stage pipeline: (i) we formulate a CLIP-guided retrieval approach to connect the domains of user photos and LEGO ® figurines, then output physically-assemble-able LEGO ® figurines with decals excluded; (ii) we then synthesize decals on the figurines via a symmetric U-Nets architecture conditioned on appearance features extracted from user photos; and (iii) we next reproject and uv-print the decals on associated LEGO ® bricks for physical model production. We evaluate the effectiveness of our method against eight hundred expert-designed figurines, using a comprehensive set of metrics, which include a novel GPT-4V-based evaluation metric, demonstrating superior performance of our method in visual quality and resemblance to input photos. Also, we show our method&#39;s robustness by generating LEGO ® figurines from diverse inputs and physically fabricating and assembling several of them.},
  archive      = {J_TOG},
  doi          = {10.1145/3658167},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Creating LEGO figurines from single images},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Holographic parallax improves 3D perceptual realism.
<em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holographic near-eye displays are a promising technology to solve long-standing challenges in virtual and augmented reality display systems. Over the last few years, many different computer-generated holography (CGH) algorithms have been proposed that are supervised by different types of target content, such as 2.5D RGB-depth maps, 3D focal stacks, and 4D light fields. It is unclear, however, what the perceptual implications are of the choice of algorithm and target content type. In this work, we build a perceptual testbed of a full-color, high-quality holographic near-eye display. Under natural viewing conditions, we examine the effects of various CGH supervision formats and conduct user studies to assess their perceptual impacts on 3D realism. Our results indicate that CGH algorithms designed for specific viewpoints exhibit noticeable deficiencies in achieving 3D realism. In contrast, holograms incorporating parallax cues consistently outperform other formats across different viewing conditions, including the center of the eyebox. This finding is particularly interesting and suggests that the inclusion of parallax cues in CGH rendering plays a crucial role in enhancing the overall quality of the holographic experience. This work represents an initial stride towards delivering a perceptually realistic 3D experience with holographic near-eye displays.},
  archive      = {J_TOG},
  doi          = {10.1145/3658168},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Holographic parallax improves 3D perceptual realism},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial and surface correspondence field for interaction
transfer. <em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new method for the task of interaction transfer. Given an example interaction between a source object and an agent, our method can automatically infer both surface and spatial relationships for the agent and target objects within the same category, yielding more accurate and valid transfers. Specifically, our method characterizes the example interaction using a combined spatial and surface representation. We correspond the agent points and object points related to the representation to the target object space using a learned spatial and surface correspondence field, which represents objects as deformed and rotated signed distance fields. With the corresponded points, an optimization is performed under the constraints of our spatial and surface interaction representation and additional regularization. Experiments conducted on human-chair and hand-mug interaction transfer tasks show that our approach can handle larger geometry and topology variations between source and target shapes, significantly outperforming state-of-the-art methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3658169},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Spatial and surface correspondence field for interaction transfer},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DreamMat: High-quality PBR material generation with
geometry- and light-aware diffusion models. <em>TOG</em>,
<em>43</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3658170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in 2D diffusion models allow appearance generation on untextured raw meshes. These methods create RGB textures by distilling a 2D diffusion model, which often contains unwanted baked-in shading effects and results in unrealistic rendering effects in the downstream applications. Generating Physically Based Rendering (PBR) materials instead of just RGB textures would be a promising solution. However, directly distilling the PBR material parameters from 2D diffusion models still suffers from incorrect material decomposition, such as baked-in shading effects in albedo. We introduce DreamMat , an innovative approach to resolve the aforementioned problem, to generate high-quality PBR materials from text descriptions. We find out that the main reason for the incorrect material distillation is that large-scale 2D diffusion models are only trained to generate final shading colors, resulting in insufficient constraints on material decomposition during distillation. To tackle this problem, we first finetune a new light-aware 2D diffusion model to condition on a given lighting environment and generate the shading results on this specific lighting condition. Then, by applying the same environment lights in the material distillation, DreamMat can generate high-quality PBR materials that are not only consistent with the given geometry but also free from any baked-in shading effects in albedo. Extensive experiments demonstrate that the materials produced through our methods exhibit greater visual appeal to users and achieve significantly superior rendering quality compared to baseline methods, which are preferable for downstream tasks such as game and film production.},
  archive      = {J_TOG},
  doi          = {10.1145/3658170},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {DreamMat: High-quality PBR material generation with geometry- and light-aware diffusion models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeurCADRecon: Neural representation for reconstructing CAD
surfaces by enforcing zero gaussian curvature. <em>TOG</em>,
<em>43</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3658171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent advances in reconstructing an organic model with the neural signed distance function (SDF), the high-fidelity reconstruction of a CAD model directly from low-quality unoriented point clouds remains a significant challenge. In this paper, we address this challenge based on the prior observation that the surface of a CAD model is generally composed of piecewise surface patches, each approximately developable even around the feature line. Our approach, named NeurCADRecon , is self-supervised, and its loss includes a developability term to encourage the Gaussian curvature toward 0 while ensuring fidelity to the input points (see the teaser figure). Noticing that the Gaussian curvature is non-zero at tip points, we introduce a double-trough curve to tolerate the existence of these tip points. Furthermore, we develop a dynamic sampling strategy to deal with situations where the given points are incomplete or too sparse. Since our resulting neural SDFs can clearly manifest sharp feature points/lines, one can easily extract the feature-aligned triangle mesh from the SDF and then decompose it into smooth surface patches, greatly reducing the difficulty of recovering the parametric CAD design. A comprehensive comparison with existing state-of-the-art methods shows the significant advantage of our approach in reconstructing faithful CAD shapes.},
  archive      = {J_TOG},
  doi          = {10.1145/3658171},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeurCADRecon: Neural representation for reconstructing CAD surfaces by enforcing zero gaussian curvature},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). S3: Speech, script and scene driven head and eye animation.
<em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present S 3 , a novel approach to generating expressive, animator-centric 3D head and eye animation of characters in conversation. Given speech audio, a Directorial script and a cinematographic 3D scene as input, we automatically output the animated 3D rotation of each character&#39;s head and eyes. S 3 distills animation and psycho-linguistic insights into a novel modular framework for conversational gaze capturing: audio-driven rhythmic head motion; narrative script-driven emblematic head and eye gestures; and gaze trajectories computed from audio-driven gaze focus/aversion and 3D visual scene salience. Our evaluation is four-fold: we quantitatively validate our algorithm against ground truth data and baseline alternatives; we conduct a perceptual study showing our results to compare favourably to prior art; we present examples of animator control and critique of S 3 output; and present a large number of compelling and varied animations of conversational gaze.},
  archive      = {J_TOG},
  doi          = {10.1145/3658172},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {S3: Speech, script and scene driven head and eye animation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ZeroGrads: Learning local surrogates for non-differentiable
graphics. <em>TOG</em>, <em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a &quot;surrogate&quot; that has similar minima but is differentiable. Our proposed framework, ZeroGrads , automates this process by learning a neural approximation of the objective function, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate&#39;s capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competitive performance at little overhead. We demonstrate optimizing diverse non-convex, non-differentiable black-box problems in graphics, such as visibility in rendering, discrete parameter spaces in procedural modelling or optimal control in physics-driven animation. In contrast to other derivative-free algorithms, our approach scales well to higher dimensions, which we demonstrate on problems with up to 35k interlinked variables.},
  archive      = {J_TOG},
  doi          = {10.1145/3658173},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {ZeroGrads: Learning local surrogates for non-differentiable graphics},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Repulsive shells. <em>TOG</em>, <em>43</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3658174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a shape space framework for collision-aware geometric modeling, where basic geometric operations automatically avoid inter-penetration. Shape spaces are a powerful tool for surface modeling, shape analysis, nonrigid motion planning, and animation, but past formulations permit nonphysical intersections. Our framework augments an existing shape space using a repulsive energy such that collision avoidance becomes a first-class property, encoded in the Riemannian metric itself. In turn, tasks like intersection-free shape interpolation or motion extrapolation amount to simply computing geodesic paths via standard numerical algorithms. To make optimization practical, we develop an adaptive collision penalty that prevents mesh self-intersection, and converges to a meaningful limit energy under refinement. The final algorithms apply to any category of shape, and do not require a dataset of examples, training, rigging, nor any other prior information. For instance, to interpolate between two shapes we need only a single pair of meshes with the same connectivity. We evaluate our method on a variety of challenging examples from modeling and animation.},
  archive      = {J_TOG},
  doi          = {10.1145/3658174},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. Graph.},
  title        = {Repulsive shells},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FlexScale: Modeling and characterization of flexible scaled
sheets. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a computational approach for modeling the mechanical behavior of flexible scaled sheet materials---3D-printed hard scales embedded in a soft substrate. Balancing strength and flexibility, these structured materials find applications in protective gear, soft robotics, and 3D-printed fashion. To unlock their full potential, however, we must unravel the complex relation between scale pattern and mechanical properties. To address this problem, we propose a contact-aware homogenization approach that distills native-level simulation data into a novel macromechanical model. This macro-model combines piecewise-quadratic uniaxial fits with polar interpolation using circular harmonics, allowing for efficient simulation of large-scale patterns. We apply our approach to explore the space of isohedral scale patterns, revealing a diverse range of anisotropic and nonlinear material behaviors. Through an extensive set of experiments, we show that our models reproduce various scale-level effects while offering good qualitative agreement with physical prototypes on the macro-level.},
  archive      = {J_TOG},
  doi          = {10.1145/3658175},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {FlexScale: Modeling and characterization of flexible scaled sheets},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time physically guided hair interpolation.
<em>TOG</em>, <em>43</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3658176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strand-based hair simulations have recently become increasingly popular for a range of real-time applications. However, accurately simulating the full number of hair strands remains challenging. A commonly employed technique involves simulating a subset of guide hairs to capture the overall behavior of the hairstyle. Details are then enriched by interpolation using linear skinning. Hair interpolation enables fast real-time simulations but frequently leads to various artifacts during runtime. As the skinning weights are often pre-computed, substantial variations between the initial and deformed shapes of the hair can cause severe deviations in fine hair geometry. Straight hairs may become kinked, and curly hairs may become zigzags. This work introduces a novel physical-driven hair interpolation scheme that utilizes existing simulated guide hair data. Instead of directly operating on positions, we interpolate the internal forces from the guide hairs before efficiently reconstructing the rendered hairs based on their material model. We formulate our problem as a constraint satisfaction problem for which we present an efficient solution. Further practical considerations are addressed using regularization terms that regulate penetration avoidance and drift correction. We have tested various hairstyles to illustrate that our approach can generate visually plausible rendered hairs with only a few guide hairs and minimal computational overhead, amounting to only about 20% of conventional linear hair interpolation. This efficiency underscores the practical viability of our method for real-time applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3658176},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time physically guided hair interpolation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proxy asset generation for cloth simulation in games.
<em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating high-resolution cloth poses computational challenges in real-time applications. In the gaming industry, the proxy mesh technique offers an alternative, simulating a simplified low-resolution cloth geometry, proxy mesh. This proxy mesh&#39;s dynamics drive the detailed high-resolution geometry, visual mesh , through Linear Blended Skinning (LBS). However, generating a suitable proxy mesh with appropriate skinning weights from a given visual mesh is non-trivial, often requiring skilled artists several days for fine-tuning. This paper presents an automatic pipeline to convert an ill-conditioned highresolution visual mesh into a single-layer low-poly proxy mesh. Given that the input visual mesh may not be simulation-ready, our approach then simulates the proxy mesh based on specific use scenarios and optimizes the skinning weights, relying on differential skinning with several well-designed loss functions to ensure the skinned visual mesh appears plausible in the final simulation. We have tested our method on various challenging cloth models, demonstrating its robustness and effectiveness.},
  archive      = {J_TOG},
  doi          = {10.1145/3658177},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Proxy asset generation for cloth simulation in games},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinetic simulation of turbulent multifluid flows.
<em>TOG</em>, <em>43</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3658178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its visual appeal, the simulation of separated multiphase flows (i.e., streams of fluids separated by interfaces) faces numerous challenges in accurately reproducing complex behaviors such as guggling, wetting, or bubbling. These difficulties are especially pronounced for high Reynolds numbers and large density variations between fluids, most likely explaining why they have received comparatively little attention in Computer Graphics compared to single- or two-phase flows. In this paper, we present a full LBM solver for multifluid simulation. We derive a conservative phase field model with which the spatial presence of each fluid or phase is encoded to allow for the simulation of miscible, immiscible and even partially-miscible fluids, while the temporal evolution of the phases is performed using a D3Q7 lattice-Boltzmann discretization. The velocity field, handled through the recent high-order moment-encoded LBM (HOME-LBM) framework to minimize its memory footprint, is simulated via a velocity-based distribution stored on a D3Q27 or D3Q19 discretization to offer accuracy and stability to large density ratios even in turbulent scenarios, while coupling with the phases through pressure, viscosity, and interfacial forces is achieved by leveraging the diffuse encoding of interfaces. The resulting solver addresses a number of limitations of kinetic methods in both computational fluid dynamics and computer graphics: it offers a fast, accurate, and low-memory fluid solver enabling efficient turbulent multiphase simulations free of the typical oscillatory pressure behavior near boundaries. We present several numerical benchmarks, examples and comparisons of multiphase flows to demonstrate our solver&#39;s visual complexity, accuracy, and realism.},
  archive      = {J_TOG},
  doi          = {10.1145/3658178},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Kinetic simulation of turbulent multifluid flows},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vertex block descent. <em>TOG</em>, <em>43</em>(4), 1–16.
(<a href="https://doi.org/10.1145/3658179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce vertex block descent, a block coordinate descent solution for the variational form of implicit Euler through vertex-level Gauss-Seidel iterations. It operates with local vertex position updates that achieve reductions in global variational energy with maximized parallelism. This forms a physics solver that can achieve numerical convergence with unconditional stability and exceptional computation performance. It can also fit in a given computation budget by simply limiting the iteration count while maintaining its stability and superior convergence rate. We present and evaluate our method in the context of elastic body dynamics, providing details of all essential components and showing that it outperforms alternative techniques. In addition, we discuss and show examples of how our method can be used for other simulation systems, including particle-based simulations and rigid bodies.},
  archive      = {J_TOG},
  doi          = {10.1145/3658179},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Vertex block descent},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eulerian-lagrangian fluid simulation on particle flow maps.
<em>TOG</em>, <em>43</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3658180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Particle Flow Map (PFM) method to enable accurate long-range advection for incompressible fluid simulation. The foundation of our method is the observation that a particle trajectory generated in a forward simulation naturally embodies a perfect flow map. Centered on this concept, we have developed an Eulerian-Lagrangian framework comprising four essential components: Lagrangian particles for a natural and precise representation of bidirectional flow maps; a dual-scale map representation to accommodate the mapping of various flow quantities; a particle-to-grid interpolation scheme for accurate quantity transfer from particles to grid nodes; and a hybrid impulse-based solver to enforce incompressibility on the grid. The efficacy of PFM has been demonstrated through various simulation scenarios, highlighting the evolution of complex vortical structures and the details of turbulent flows. Notably, compared to NFM, PFM reduces computing time by up to 49 times and memory consumption by up to 41%, while enhancing vorticity preservation as evidenced in various tests like leapfrog, vortex tube, and turbulent flow.},
  archive      = {J_TOG},
  doi          = {10.1145/3658180},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Eulerian-lagrangian fluid simulation on particle flow maps},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implicit swept volume SDF: Enabling continuous
collision-free trajectory generation for arbitrary shapes. <em>TOG</em>,
<em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of trajectory generation for objects, ensuring continuous collision-free motion remains a huge challenge, especially for non-convex geometries and complex environments. Previous methods either oversimplify object shapes, which results in a sacrifice of feasible space or rely on discrete sampling, which suffers from the &quot;tunnel effect&quot;. To address these limitations, we propose a novel hierarchical trajectory generation pipeline, which utilizes the Swept Volume Signed Distance Field (SVSDF) to guide trajectory optimization for Continuous Collision Avoidance (CCA). Our interdisciplinary approach, blending techniques from graphics and robotics, exhibits outstanding effectiveness in solving this problem. We formulate the computation of the SVSDF as a Generalized Semi-Infinite Programming model, and we solve for the numerical solutions at query points implicitly, thereby eliminating the need for explicit reconstruction of the surface. Our algorithm has been validated in a variety of complex scenarios and applies to robots of various dynamics, including both rigid and deformable shapes. It demonstrates exceptional universality and superior CCA performance compared to typical algorithms. The code will be released at https://github.com/ZJU-FAST-Lab/Implicit-SVSDF-Planner for the benefit of the community.},
  archive      = {J_TOG},
  doi          = {10.1145/3658181},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Implicit swept volume SDF: Enabling continuous collision-free trajectory generation for arbitrary shapes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Target-aware image denoising for inverse monte carlo
rendering. <em>TOG</em>, <em>43</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3658182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically based differentiable rendering allows an accurate light transport simulation to be differentiated with respect to the rendering input, i.e., scene parameters, and it enables inferring scene parameters from target images, e.g., photos or synthetic images, via an iterative optimization. However, this inverse Monte Carlo rendering inherits the fundamental problem of the Monte Carlo integration, i.e., noise, resulting in a slow optimization convergence. An appealing approach to addressing such noise is exploiting an image denoiser to improve optimization convergence. Unfortunately, the direct adoption of existing image denoisers designed for ordinary rendering scenarios can drive the optimization into undesirable local minima due to denoising bias. It motivates us to reformulate a new image denoiser specialized for inverse rendering. Unlike existing image denoisers, we conduct our denoising by considering the target images, i.e., specific information in inverse rendering. For our target-aware denoising, we determine our denoising weights via a linear regression technique using the target. We demonstrate that our denoiser enables inverse rendering optimization to infer scene parameters robustly through a diverse set of tests.},
  archive      = {J_TOG},
  doi          = {10.1145/3658182},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Target-aware image denoising for inverse monte carlo rendering},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D-layers: Bringing layer-based color editing to VR
painting. <em>TOG</em>, <em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to represent artworks as stacks of layers is fundamental to modern graphics design, as it allows artists to easily separate visual elements, edit them in isolation, and blend them to achieve rich visual effects. Despite their ubiquity in 2D painting software, layers have not yet made their way to VR painting, where users paint strokes directly in 3D space by gesturing a 6-degrees-of-freedom controller. But while the concept of a stack of 2D layers was inspired by real-world layers in cell animation, what should 3D layers be? We propose to define 3D-Layers as groups of 3D strokes, and we distinguish the ones that represent 3D geometry from the ones that represent color modifications of the geometry. We call the former substrate layers and the latter appearance layers. Strokes in appearance layers modify the color of the substrate strokes they intersect. Thanks to this distinction, artists can define sequences of color modifications as stacks of appearance layers, and edit each layer independently to finely control the final color of the substrate. We have integrated 3D-Layers into a VR painting application and we evaluate its flexibility and expressiveness by conducting a usability study with experienced VR artists.},
  archive      = {J_TOG},
  doi          = {10.1145/3658183},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {3D-layers: Bringing layer-based color editing to VR painting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simplicits: Mesh-free, geometry-agnostic elastic simulation.
<em>TOG</em>, <em>43</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3658184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of 3D representations, from explicit meshes to implicit neural fields and more, motivates the need for simulators agnostic to representation. We present a data-, mesh-, and grid-free solution for elastic simulation for any object in any geometric representation undergoing large, nonlinear deformations. We note that every standard geometric representation can be reduced to an occupancy function queried at any point in space, and we define a simulator atop this common interface. For each object, we fit a small implicit neural network encoding spatially varying weights that act as a reduced deformation basis. These weights are trained to learn physically significant motions in the object via random perturbations. Our loss ensures we find a weight-space basis that best minimizes deformation energy by stochastically evaluating elastic energies through Monte Carlo sampling of the deformation volume. At runtime, we simulate in the reduced basis and sample the deformations back to the original domain. Our experiments demonstrate the versatility, accuracy, and speed of this approach on data including signed distance functions, point clouds, neural primitives, tomography scans, radiance fields, Gaussian splats, surface meshes, and volume meshes, as well as showing a variety of material energies, contact models, and time integration schemes.},
  archive      = {J_TOG},
  doi          = {10.1145/3658184},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Simplicits: Mesh-free, geometry-agnostic elastic simulation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Capacitive touch sensing on general 3D surfaces.
<em>TOG</em>, <em>43</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3658185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutual-capacitive sensing is the most common technology for detecting multi-touch, especially on flat and simple curvature surfaces. Its extension to a more complex shape is still challenging, as a uniform distribution of sensing electrodes is required for consistent touch sensitivity across the surface. To overcome this problem, we propose a method to adapt the sensor layout of common capacitive multi-touch sensors to more complex 3D surfaces, ensuring high-resolution, robust multi-touch detection. The method automatically computes a grid of transmitter and receiver electrodes with as regular distribution as possible over a general 3D shape. It starts with the computation of a proxy geometry by quad meshing used to place the electrodes through the dual-edge graph. It then arranges electrodes on the surface to minimize the number of touch controllers required for capacitive sensing and the number of input/output pins to connect the electrodes with the controllers. We reach these objectives using a new simplification and clustering algorithm for a regular quad-patch layout. The reduced patch layout is used to optimize the routing of all the structures (surface grooves and internal pipes) needed to host all electrodes on the surface and inside the object&#39;s volume, considering the geometric constraints of the 3D shape. Finally, we print the 3D object prototype ready to be equipped with the electrodes. We analyze the performance of the proposed quad layout simplification and clustering algorithm using different quad meshing and characterize the signal quality and accuracy of the capacitive touch sensor for different non-planar geometries. The tested prototypes show precise and robust multi-touch detection with good Signal-to-Noise Ratio and spatial accuracy of about 1mm.},
  archive      = {J_TOG},
  doi          = {10.1145/3658185},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Capacitive touch sensing on general 3D surfaces},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeuralTO: Neural reconstruction and view synthesis of
translucent objects. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from multi-view images using neural implicit signed distance functions shows impressive performance on 3D Reconstruction of opaque objects. However, existing methods struggle to reconstruct accurate geometry when applied to translucent objects due to the non-negligible bias in their rendering function. To address the inaccuracies in the existing model, we have reparameterized the density function of the neural radiance field by incorporating an estimated constant extinction coefficient. This modification forms the basis of our innovative framework, which is geared towards highfidelity surface reconstruction and the novel-view synthesis of translucent objects. Our framework contains two stages. In the reconstruction stage, we introduce a novel weight function to achieve accurate surface geometry reconstruction. Following the recovery of geometry, the second phase involves learning the distinct scattering properties of the participating media to enhance rendering. A comprehensive dataset, comprising both synthetic and real translucent objects, has been built for conducting extensive experiments. Experiments reveal that our method outperforms existing approaches in terms of reconstruction and novel-view synthesis.},
  archive      = {J_TOG},
  doi          = {10.1145/3658186},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeuralTO: Neural reconstruction and view synthesis of translucent objects},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StopThePop: Sorted gaussian splatting for view-consistent
real-time rendering. <em>TOG</em>, <em>43</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3658187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains. However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications. Notably, reducing Gaussian to 2D splats with a single viewspace depth introduces popping and blending artifacts during view rotation. Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation. In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead. Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements. Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation. Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion. Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting. Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency. Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements. Our renderer is publicly available at https://github.com/r4dl/StopThePop.},
  archive      = {J_TOG},
  doi          = {10.1145/3658187},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {StopThePop: Sorted gaussian splatting for view-consistent real-time rendering},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BlockFusion: Expandable 3D scene generation using latent
tri-plane extrapolation. <em>TOG</em>, <em>43</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3658188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature samples from the overlapping tri-planes during the denoising iterations. Latent tri-plane extrapolation produces semantically and geometrically meaningful transitions that harmoniously blend with the existing scene. A 2D layout conditioning mechanism is used to control the placement and arrangement of scene elements. Experimental results indicate that BlockFusion is capable of generating diverse, geometrically consistent and unbounded large 3D scenes with unprecedented high-quality shapes in both indoor and outdoor scenarios.},
  archive      = {J_TOG},
  doi          = {10.1145/3658188},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {BlockFusion: Expandable 3D scene generation using latent tri-plane extrapolation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a generalized physical face model from data.
<em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically-based simulation is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits. Today&#39;s methods are data-driven, where the actuations for finite elements are inferred from captured skin geometry. Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training. In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset. Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically. Fitting is as easy as providing a single 3D face scan, or even a single face image. After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters. All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more.},
  archive      = {J_TOG},
  doi          = {10.1145/3658189},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning a generalized physical face model from data},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning images across scales using adversarial training.
<em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real world exhibits rich structure and detail across many scales of observation. It is difficult, however, to capture and represent a broad spectrum of scales using ordinary images. We devise a novel paradigm for learning a representation that captures an orders-of-magnitude variety of scales from an unstructured collection of ordinary images. We treat this collection as a distribution of scale-space slices to be learned using adversarial training, and additionally enforce coherency across slices. Our approach relies on a multiscale generator with carefully injected procedural frequency content, which allows to interactively explore the emerging continuous scale space. Training across vastly different scales poses challenges regarding stability, which we tackle using a supervision scheme that involves careful sampling of scales. We show that our generator can be used as a multiscale generative model, and for reconstructions of scale spaces from unstructured patches. Significantly outperforming the state of the art, we demonstrate zoom-in factors of up to 256x at high quality and scale consistency.},
  archive      = {J_TOG},
  doi          = {10.1145/3658190},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning images across scales using adversarial training},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contact detection between curved fibres: High order makes a
difference. <em>TOG</em>, <em>43</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3658191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer Graphics has a long history in the design of effective algorithms for handling contact and friction between solid objects. For the sake of simplicity and versatility, most methods rely on low-order primitives such as line segments or triangles, both for the detection and the response stages. In this paper we carefully analyse, in the case of fibre systems, the impact of such choices on the retrieved contact forces. We highlight the presence of artifacts in the force response that are tightly related to the low-order geometry used for contact detection. Our analysis draws upon thorough comparisons between the high-order super-helix model and the low-order discrete elastic rod model. These reveal that when coupled to a low-order, segment-based detection scheme, both models yield spurious jumps in the contact force profile. Moreover, these artifacts are shown to be all the more visible as the geometry of fibres at contact is curved. In order to remove such artifacts we develop an accurate high-order detection scheme between two smooth curves, which relies on an efficient adaptive pruning strategy. We use this algorithm to detect contact between super-helices at high precision, allowing us to recover, in the range of wavy to highly curly fibres, much smoother force profiles during sliding motion than with a classical segment-based strategy. Furthermore, we show that our approach offers better scaling properties in terms of efficiency vs. precision compared to segment-based approaches, making it attractive for applications where accurate and reliable forces are desired. Finally, we demonstrate the robustness and accuracy of our fully high-order approach on a challenging hair combing scenario.},
  archive      = {J_TOG},
  doi          = {10.1145/3658191},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. Graph.},
  title        = {Contact detection between curved fibres: High order makes a difference},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scintilla: Simulating combustible vegetation for wildfires.
<em>TOG</em>, <em>43</em>(4), 1–21. (<a
href="https://doi.org/10.1145/3658192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildfires are a complex physical phenomenon that involves the combustion of a variety of flammable materials ranging from fallen leaves and dried twigs to decomposing organic material and living flora. All these materials can potentially act as fuel with different properties that determine the progress and severity of a wildfire. In this paper, we propose a novel approach for simulating the dynamic interaction between the varying components of a wildfire, including processes of convection, combustion and heat transfer between vegetation, soil and atmosphere. We propose a novel representation of vegetation that includes detailed branch geometry, fuel moisture, and distribution of grass, fine fuel, and duff. Furthermore, we model the ignition, generation, and transport of fire by firebrands and embers. This allows simulating and rendering virtual 3D wildfires that realistically capture key aspects of the process, such as progressions from ground to crown fires, the impact of embers carried by wind, and the effects of fire barriers and other human intervention methods. We evaluate our approach through numerous experiments and based on comparisons to real-world wildfire data.},
  archive      = {J_TOG},
  doi          = {10.1145/3658192},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Scintilla: Simulating combustible vegetation for wildfires},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SMERF: Streamable memory efficient radiance fields for
real-time large-scene exploration. <em>TOG</em>, <em>43</em>(4), 1–13.
(<a href="https://doi.org/10.1145/3658193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent techniques for real-time view synthesis have rapidly advanced in fidelity and speed, and modern methods are capable of rendering near-photorealistic scenes at interactive frame rates. At the same time, a tension has arisen between explicit scene representations amenable to rasterization and neural fields built on ray marching, with state-of-the-art instances of the latter surpassing the former in quality while being prohibitively expensive for real-time applications. We introduce SMERF, a view synthesis approach that achieves state-of-the-art accuracy among real-time methods on large scenes with footprints up to 300 m 2 at a volumetric resolution of 3.5 mm 3 . Our method is built upon two primary contributions: a hierarchical model partitioning scheme, which increases model capacity while constraining compute and memory consumption, and a distillation training strategy that simultaneously yields high fidelity and internal consistency. Our method enables full six degrees of freedom navigation in a web browser and renders in real-time on commodity smartphones and laptops. Extensive experiments show that our method exceeds the state-of-the-art in real-time novel view synthesis by 0.78 dB on standard benchmarks and 1.78 dB on large scenes, renders frames three orders of magnitude faster than state-of-the-art radiance field models, and achieves real-time performance across a wide variety of commodity devices, including smartphones. We encourage readers to explore these models interactively at our project website: https://smerf-3d.github.io.},
  archive      = {J_TOG},
  doi          = {10.1145/3658193},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {SMERF: Streamable memory efficient radiance fields for real-time large-scene exploration},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spin-it faster: Quadrics solve all topology optimization
problems that depend only on mass moments. <em>TOG</em>, <em>43</em>(4),
1–13. (<a href="https://doi.org/10.1145/3658194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The behavior of a rigid body primarily depends on its mass moments, which consist of the mass, center of mass, and moments of inertia. It is possible to manipulate these quantities without altering the geometric appearance of an object by introducing cavities in its interior. Algorithms that find cavities of suitable shapes and sizes have enabled the computational design of spinning tops, yo-yos, wheels, buoys, and statically balanced objects. Previous work is based, for example, on topology optimization on voxel grids, which introduces a large number of optimization variables and box constraints, or offset surface computation, which cannot guarantee that solutions to a feasible problem will always be found. In this work, we provide a mathematical analysis of constrained topology optimization problems that depend only on mass moments. This class of problems covers, among others, all applications mentioned above. Our main result is to show that no matter the outer shape of the rigid body to be optimized or the optimization objective and constraints considered, the optimal solution always features a quadric-shaped interface between material and cavities. This proves that optimal interfaces are always ellipsoids, hyperboloids, paraboloids, or one of a few degenerate cases, such as planes. This insight lets us replace a difficult topology optimization problem with a provably equivalent non-linear equation system in a small number (&lt;10) of variables, which represent the coefficients of the quadric. This system can be solved in a few seconds for most examples, provides insights into the geometric structure of many specific applications, and lets us describe their solution properties. Finally, our method integrates seamlessly into modern fabrication workflows because our solutions are analytical surfaces that are native to the CAD domain.},
  archive      = {J_TOG},
  doi          = {10.1145/3658194},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Spin-it faster: Quadrics solve all topology optimization problems that depend only on mass moments},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One noise to rule them all: Learning a unified model of
spatially-varying noise patterns. <em>TOG</em>, <em>43</em>(4), 1–21.
(<a href="https://doi.org/10.1145/3658195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural noise is a fundamental component of computer graphics pipelines, offering a flexible way to generate textures that exhibit &quot;natural&quot; random variation. Many different types of noise exist, each produced by a separate algorithm. In this paper, we present a single generative model which can learn to generate multiple types of noise as well as blend between them. In addition, it is capable of producing spatially-varying noise blends despite not having access to such data for training. These features are enabled by training a denoising diffusion model using a novel combination of data augmentation and network conditioning techniques. Like procedural noise generators, the model&#39;s behavior is controllable via interpretable parameters plus a source of randomness. We use our model to produce a variety of visually compelling noise textures. We also present an application of our model to improving inverse procedural material design; using our model in place of fixed-type noise nodes in a procedural material graph results in higher-fidelity material reconstructions without needing to know the type of noise in advance. Open-sourced materials can be found at https://armanmaesumi.github.io/onenoise/},
  archive      = {J_TOG},
  doi          = {10.1145/3658195},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {One noise to rule them all: Learning a unified model of spatially-varying noise patterns},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cricket: A self-powered chirping pixel. <em>TOG</em>,
<em>43</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3658196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a sensor that can measure light and wirelessly communicate the measurement, without the need for an external power source or a battery. Our sensor, called cricket, harvests energy from incident light. It is asleep for most of the time and transmits a short and strong radio frequency chirp when its harvested energy reaches a specific level. The carrier frequency of each cricket is fixed and reveals its identity, and the duration between consecutive chirps is a measure of the incident light level. We have characterized the radiometric response function, signal-to-noise ratio and dynamic range of cricket. We have experimentally verified that cricket can be miniaturized at the expense of increasing the duration between chirps. We show that a cube with a cricket on each of its sides can be used to estimate the centroid of any complex illumination, which has value in applications such as solar tracking. We also demonstrate the use of crickets for creating untethered sensor arrays that can produce video and control lighting for energy conservation. Finally, we modified cricket&#39;s circuit to develop battery-free electronic sunglasses that can instantly adapt to environmental illumination.},
  archive      = {J_TOG},
  doi          = {10.1145/3658196},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Cricket: A self-powered chirping pixel},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep sketch vectorization via implicit surface extraction.
<em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an algorithm for sketch vectorization with state-of-the-art accuracy and capable of handling complex sketches. We approach sketch vectorization as a surface extraction task from an unsigned distance field, which is implemented using a two-stage neural network and a dual contouring domain post processing algorithm. The first stage consists of extracting unsigned distance fields from an input raster image. The second stage consists of an improved neural dual contouring network more robust to noisy input and more sensitive to line geometry. To address the issue of under-sampling inherent in grid-based surface extraction approaches, we explicitly predict undersampling and keypoint maps. These are used in our post-processing algorithm to resolve sharp features and multi-way junctions. The keypoint and undersampling maps are naturally controllable, which we demonstrate in an interactive topology refinement interface. Our proposed approach produces far more accurate vectorizations on complex input than previous approaches with efficient running time.},
  archive      = {J_TOG},
  doi          = {10.1145/3658197},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep sketch vectorization via implicit surface extraction},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lifting directional fields to minimal sections.
<em>TOG</em>, <em>43</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3658198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directional fields, including unit vector, line, and cross fields, are essential tools in the geometry processing toolkit. The topology of directional fields is characterized by their singularities. While singularities play an important role in downstream applications such as meshing, existing methods for computing directional fields either require them to be specified in advance, ignore them altogether, or treat them as zeros of a relaxed field. While fields are ill-defined at their singularities, the graphs of directional fields with singularities are well-defined surfaces in a circle bundle. By lifting optimization of fields to optimization over their graphs, we can exploit a natural convex relaxation to a minimal section problem over the space of currents in the bundle. This relaxation treats singularities as first-class citizens, expressing the relationship between fields and singularities as an explicit boundary condition. As curvature frustrates finite element discretization of the bundle, we devise a hybrid spectral method for representing and optimizing minimal sections. Our method supports field optimization on both flat and curved domains and enables more precise control over singularity placement.},
  archive      = {J_TOG},
  doi          = {10.1145/3658198},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Lifting directional fields to minimal sections},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightning-fast method of fundamental solutions.
<em>TOG</em>, <em>43</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3658199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The method of fundamental solutions (MFS) and its associated boundary element method (BEM) have gained popularity in computer graphics due to the reduced dimensionality they offer: for three-dimensional linear problems, they only require variables on the domain boundary to solve and evaluate the solution throughout space, making them a valuable tool in a wide variety of applications. However, MFS and BEM have poor computational scalability and huge memory requirements for large-scale problems, limiting their applicability and efficiency in practice. By leveraging connections with Gaussian Processes and exploiting the sparse structure of the inverses of boundary integral matrices, we introduce a variational preconditioner that can be computed via a sparse inverse-Cholesky factorization in a massively parallel manner. We show that applying our preconditioner to the Preconditioned Conjugate Gradient algorithm greatly improves the efficiency of MFS or BEM solves, up to four orders of magnitude in our series of tests.},
  archive      = {J_TOG},
  doi          = {10.1145/3658199},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Lightning-fast method of fundamental solutions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Terrain amplification using multi scale erosion.
<em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling high-resolution terrains is a perennial challenge in the creation of virtual worlds. In this paper, we focus on the amplification of a low-resolution input terrain into a high-resolution, hydrologically consistent terrain featuring complex patterns by a multi-scale approach. Our framework combines the best of both worlds, relying on physics-inspired erosion models producing consistent erosion landmarks and introducing control at different scales, thus bridging the gap between physics-based erosion simulations and multi-scale procedural modeling. The method uses a fast and accurate approximation of different simulations, including thermal, stream power erosion and deposition performed at different scales to obtain a range of effects. Our approach provides landscape designers with tools for amplifying mountain ranges and valleys with consistent details.},
  archive      = {J_TOG},
  doi          = {10.1145/3658200},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Terrain amplification using multi scale erosion},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ray tracing harmonic functions. <em>TOG</em>,
<em>43</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3658201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sphere tracing is a fast and high-quality method for visualizing surfaces encoded by signed distance functions (SDFs). We introduce a similar method for a completely different class of surfaces encoded by harmonic functions , opening up rich new possibilities for visual computing. Our starting point is similar in spirit to sphere tracing: using conservative Harnack bounds on the growth of harmonic functions, we develop a Harnack tracing algorithm for visualizing level sets of harmonic functions, including those that are angle-valued and exhibit singularities. The method takes much larger steps than naïve ray marching, avoids numerical issues common to generic root finding methods and, like sphere tracing, needs only perform pointwise evaluation of the function at each step. For many use cases, the method is fast enough to run real time in a shader program. We use it to visualize smooth surfaces directly from point clouds (via Poisson surface reconstruction) or polygon soup (via generalized winding numbers) without linear solves or mesh extraction. We also use it to visualize nonplanar polygons (possibly with holes), surfaces from architectural geometry, mesh &quot;exoskeletons&quot;, and key mathematical objects including knots, links, spherical harmonics, and Riemann surfaces. Finally we show that, at least in theory, Harnack tracing provides an alternative mechanism for visualizing arbitrary implicit surfaces.},
  archive      = {J_TOG},
  doi          = {10.1145/3658201},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Ray tracing harmonic functions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Seamless parametrization in penner coordinates.
<em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a conceptually simple and efficient algorithm for seamless parametrization, a key element in constructing quad layouts and texture charts on surfaces. More specifically, we consider the construction of parametrizations with prescribed holonomy signatures i.e., a set of angles at singularities, and rotations along homology loops, preserving which is essential for constructing parametrizations following an input field, as well as for user control of the parametrization structure. Our algorithm performs exceptionally well on a large dataset based on Thingi10k [Zhou and Jacobson 2016], (16156 meshes) as well as on a challenging smaller dataset of [Myles et al. 2014], converging, on average, in 9 iterations. Although the algorithm lacks a formal mathematical guarantee, presented empirical evidence and the connections between convex optimization and closely related algorithms, suggest that a similar formulation can be found for this algorithm in the future.},
  archive      = {J_TOG},
  doi          = {10.1145/3658202},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Seamless parametrization in penner coordinates},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time path guiding using bounding voxel sampling.
<em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a real-time path guiding method, Voxel Path Guiding (VXPG), that significantly improves fitting efficiency under limited sampling budget. Our key idea is to use a spatial irradiance voxel data structure across all shading points to guide the location of path vertices. For each frame, we first populate the voxel data structure with irradiance and geometry information. To sample from the data structure for a shading point, we need to select a voxel with high contribution to that point. To importance sample the voxels while taking visibility into consideration, we adapt techniques from offline many-lights rendering by clustering pairs of shading points and voxels. Finally, we unbiasedly sample within the selected voxel while taking the geometry inside into consideration. Our experiments show that VXPG achieves significantly lower perceptual error compared to other real-time path guiding and virtual point light methods under equal-time comparison. Furthermore, our method does not rely on temporal information, but can be used together with other temporal reuse sampling techniques such as ReSTIR to further improve sampling efficiency.},
  archive      = {J_TOG},
  doi          = {10.1145/3658203},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time path guiding using bounding voxel sampling},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-to-vector generation with neural path representation.
<em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector graphics are widely used in digital art and highly favored by designers due to their scalability and layer-wise properties. However, the process of creating and editing vector graphics requires creativity and design expertise, making it a time-consuming task. Recent advancements in text-to-vector (T2V) generation have aimed to make this process more accessible. However, existing T2V methods directly optimize control points of vector graphics paths, often resulting in intersecting or jagged paths due to the lack of geometry constraints. To overcome these limitations, we propose a novel neural path representation by designing a dual-branch Variational Autoencoder (VAE) that learns the path latent space from both sequence and image modalities. By optimizing the combination of neural paths, we can incorporate geometric constraints while preserving expressivity in generated SVGs. Furthermore, we introduce a two-stage path optimization method to improve the visual and topological quality of generated SVGs. In the first stage, a pre-trained text-to-image diffusion model guides the initial generation of complex vector graphics through the Variational Score Distillation (VSD) process. In the second stage, we refine the graphics using a layer-wise image vectorization strategy to achieve clearer elements and structure. We demonstrate the effectiveness of our method through extensive experiments and showcase various applications. The project page is https://intchous.github.io/T2V-NPR.},
  archive      = {J_TOG},
  doi          = {10.1145/3658204},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Text-to-vector generation with neural path representation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TIP-editor: An accurate 3D editor following both
text-prompts and image-prompts. <em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIP-Editor, that accepts both text and image prompts and a 3D bounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIP-Editor utilizes explicit and flexible 3D Gaussian splatting (GS) as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively.},
  archive      = {J_TOG},
  doi          = {10.1145/3658205},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {TIP-editor: An accurate 3D editor following both text-prompts and image-prompts},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive invigoration: Volumetric modeling of trees with
strands. <em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating realistic models of trees and plants is a complex problem because of the vast variety of shapes trees can form. Procedural modeling algorithms are popular for defining branching structures and steadily increasing their expressive power by considering more biological findings. Most existing methods focus on defining the branching structure of trees based on skeletal graphs, while the surface mesh of branches is most commonly defined as simple cylinders. One critical open problem is defining and controlling the complex details observed in real trees. This paper aims to advance tree modeling by proposing a strand-based volumetric representation for tree models. Strands are fixed-size volumetric pipes that define the branching structure. By leveraging strands, our approach captures the lateral development of trees. We combine the strands with a novel branch development formulation that allows us to locally inject vigor and reshape the tree model. Moreover, we define a set of editing operators for tree primary and lateral development that enables users to interactively generate complex tree models with unprecedented detail with minimal effort.},
  archive      = {J_TOG},
  doi          = {10.1145/3658206},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive invigoration: Volumetric modeling of trees with strands},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smooth bijective projection in a high-order shell.
<em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new structure called a higher-order shell, which is composed of a set of triangular prisms. Each triangular prism is enveloped by three Bézier triangles (top, middle, and bottom) and three side surfaces, each of which is trimmed from a bilinear surface. Moreover, we define a continuous vector field to smoothly and bijectively transfer attributes between two surfaces inside the shell. Since the higher-order shell has several hard construction constraints, we apply an interior-point strategy to robustly and automatically construct a high-order shell for an input mesh. Specifically, the strategy starts from a valid linear shell with a small thickness. Then, the shell is optimized until the specified thickness is reached, where explicit checks ensure that the constraints are always satisfied. We extensively test our method on more than 8300 models, demonstrating its robustness and performance. Compared to state-of-the-art methods, our bijective projection is smoother, and the space between the shell and input mesh is more uniform.},
  archive      = {J_TOG},
  doi          = {10.1145/3658207},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Smooth bijective projection in a high-order shell},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biharmonic coordinates and their derivatives for triangular
3D cages. <em>TOG</em>, <em>43</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3658208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a natural extension to the harmonic coordinates, the biharmonic coordinates have been found superior for planar shape and image manipulation with an enriched deformation space. However, the 3D biharmonic coordinates and their derivatives have remained unexplored. In this work, we derive closed-form expressions for biharmonic coordinates and their derivatives for 3D triangular cages. The core of our derivation lies in computing the closed-form expressions for the integral of the Euclidean distance over a triangle and its derivatives. The derived 3D biharmonic coordinates not only fill a missing component in methods of generalized barycentric coordinates but also pave the way for various interesting applications in practice, including producing a family of biharmonic deformations, solving variational shape deformations, and even unlocking the closed-form expressions for recently-introduced Somigliana coordinates for both fast and accurate evaluations.},
  archive      = {J_TOG},
  doi          = {10.1145/3658208},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Biharmonic coordinates and their derivatives for triangular 3D cages},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Categorical codebook matching for embodied character
controllers. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Translating motions from a real user onto a virtual embodied avatar is a key challenge for character animation in the metaverse. In this work, we present a novel generative framework that enables mapping from a set of sparse sensor signals to a full body avatar motion in real-time while faithfully preserving the motion context of the user. In contrast to existing techniques that require training a motion prior and its mapping from control to motion separately, our framework is able to learn the motion manifold as well as how to sample from it at the same time in an end-to-end manner. To achieve that, we introduce a technique called codebook matching which matches the probability distribution between two categorical codebooks for the inputs and outputs for synthesizing the character motions. We demonstrate this technique can successfully handle ambiguity in motion generation and produce high quality character controllers from unstructured motion capture data. Our method is especially useful for interactive applications like virtual reality or video games where high accuracy and responsiveness are needed.},
  archive      = {J_TOG},
  doi          = {10.1145/3658209},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Categorical codebook matching for embodied character controllers},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Area ReSTIR: Resampling for real-time defocus and
antialiasing. <em>TOG</em>, <em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in spatiotemporal reservoir resampling (ReSTIR) leverage sample reuse from neighbors to efficiently evaluate the path integral. Like rasterization, ReSTIR methods implicitly assume a pinhole camera and evaluate the light arriving at a pixel through a single predetermined subpixel location at a time (e.g., the pixel center). This prevents efficient path reuse in and near pixels with high-frequency details. We introduce Area ReSTIR , extending ReSTIR reservoirs to also integrate each pixel&#39;s 4D ray space, including 2D areas on the film and lens. We design novel subpixel-tracking temporal reuse and shift mappings that maximize resampling quality in such regions. This robustifies ReSTIR against high-frequency content, letting us importance sample subpixel and lens coordinates and efficiently render antialiasing and depth of field.},
  archive      = {J_TOG},
  doi          = {10.1145/3658210},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Area ReSTIR: Resampling for real-time defocus and antialiasing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TensoSDF: Roughness-aware tensorial representation for
robust geometry and material reconstruction. <em>TOG</em>,
<em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing objects with realistic materials from multi-view images is problematic, since it is highly ill-posed. Although the neural reconstruction approaches have exhibited impressive reconstruction ability, they are designed for objects with specific materials (e.g., diffuse or specular materials). To this end, we propose a novel framework for robust geometry and material reconstruction, where the geometry is expressed with the implicit signed distance field (SDF) encoded by a tensorial representation, namely TensoSDF. At the core of our method is the roughness-aware incorporation of the radiance and reflectance fields, which enables a robust reconstruction of objects with arbitrary reflective materials. Furthermore, the tensorial representation enhances geometry details in the reconstructed surface and reduces the training time. Finally, we estimate the materials using an explicit mesh for efficient intersection computation and an implicit SDF for accurate representation. Consequently, our method can achieve more robust geometry reconstruction, outperform the previous works in terms of relighting quality, and reduce 50% training times and 70% inference time. Codes and datasets are available at https://github.com/Riga2/TensoSDF.},
  archive      = {J_TOG},
  doi          = {10.1145/3658211},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {TensoSDF: Roughness-aware tensorial representation for robust geometry and material reconstruction},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural slicer for multi-axis 3D printing. <em>TOG</em>,
<em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel neural network-based computational pipeline as a representation-agnostic slicer for multi-axis 3D printing. This advanced slicer can work on models with diverse representations and intricate topology. The approach involves employing neural networks to establish a deformation mapping, defining a scalar field in the space surrounding an input model. Isosurfaces are subsequently extracted from this field to generate curved layers for 3D printing. Creating a differentiable pipeline enables us to optimize the mapping through loss functions directly defined on the field gradients as the local printing directions. New loss functions have been introduced to meet the manufacturing objectives of support-free and strength reinforcement. Our new computation pipeline relies less on the initial values of the field and can generate slicing results with significantly improved performance.},
  archive      = {J_TOG},
  doi          = {10.1145/3658212},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural slicer for multi-axis 3D printing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient debris-flow simulation for steep terrain erosion.
<em>TOG</em>, <em>43</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3658213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erosion simulation is a common approach used for generating and authoring mountainous terrains. While water is considered the primary erosion factor, its simulation fails to capture steep slopes near the ridges. In these low-drainage areas, erosion is often approximated with slope-reducing erosion, which yields unrealistically uniform slopes. However, geomorphology observed that another process dominates the low-drainage areas: erosion by debris flow, which is a mixture of mud and rocks triggered by strong climatic events. We propose a new method to capture the interactions between debris flow and fluvial erosion thanks to a new mathematical formulation for debris flow erosion derived from geomorphology and a unified GPU algorithm for erosion and deposition. In particular, we observe that sediment and debris deposition tend to intersect river paths, which motivates the design of a new, approximate flow routing algorithm on the GPU to estimate the water path out of these newly formed depressions. We demonstrate that debris flow carves distinct patterns in the form of erosive scars on steep slopes and cones of deposited debris competing with fluvial erosion downstream.},
  archive      = {J_TOG},
  doi          = {10.1145/3658213},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient debris-flow simulation for steep terrain erosion},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive dynamics for cloth and shell animation.
<em>TOG</em>, <em>43</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3658214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Progressive Dynamics, a coarse-to-fine, level-of-detail simulation method for the physics-based animation of complex frictionally contacting thin shell and cloth dynamics. Progressive Dynamics provides tight-matching consistency and progressive improvement across levels, with comparable quality and realism to high-fidelity, IPC-based shell simulations [Li et al. 2021] at finest resolutions. Together these features enable an efficient animation-design pipeline with predictive coarse-resolution previews providing rapid design iterations for a final, to-be-generated, high-resolution animation. In contrast, previously, to design such scenes with comparable dynamics would require prohibitively slow design iterations via repeated direct simulations on high-resolution meshes. We evaluate and demonstrate Progressive Dynamics&#39;s features over a wide range of challenging stress-tests, benchmarks, and animation design tasks. Here Progressive Dynamics efficiently computes consistent previews at costs comparable to coarsest-level direct simulations. Its matching progressive refinements across levels then generate rich, high-resolution animations with high-speed dynamics, impacts, and the complex detailing of the dynamic wrinkling, folding, and sliding of frictionally contacting thin shells and fabrics.},
  archive      = {J_TOG},
  doi          = {10.1145/3658214},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Progressive dynamics for cloth and shell animation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive grid generation for discretizing implicit
complexes. <em>TOG</em>, <em>43</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3658215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for generating a simplicial (e.g., triangular or tetrahedral) grid to enable adaptive discretization of implicit shapes defined by a vector function. Such shapes, which we call implicit complexes, are generalizations of implicit surfaces and useful for representing non-smooth and non-manifold structures. While adaptive grid generation has been extensively studied for polygonizing implicit surfaces, few methods are designed for implicit complexes. Our method can generate adaptive grids for several implicit complexes, including arrangements of implicit surfaces, CSG shapes, material interfaces, and curve networks. Importantly, our method adapts the grid to the geometry of not only the implicit surfaces but also their lower-dimensional intersections. We demonstrate how our method enables efficient and detail-preserving discretization of non-trivial implicit shapes.},
  archive      = {J_TOG},
  doi          = {10.1145/3658215},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Adaptive grid generation for discretizing implicit complexes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proxy tracing: Unbiased reciprocal estimation for optimized
sampling in BDPT. <em>TOG</em>, <em>43</em>(4), 1–21. (<a
href="https://doi.org/10.1145/3658216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust light transport algorithms, particularly bidirectional path tracing (BDPT), face significant challenges when dealing with specular or highly glossy involved paths. BDPT constructs the full path by connecting sub-paths traced individually from the light source and camera. However, it remains difficult to sample by connecting vertices on specular and glossy surfaces with narrow-lobed BSDF, as it poses severe constraints on sampling in the feasible direction. To address this issue, we propose a novel approach, called proxy sampling , that enables efficient sub-path connection of these challenging paths. When a low-contribution specular/glossy connection occurs, we drop out the problematic neighboring vertex next to this specular/glossy vertex from the original path, then retrace an alternative sub-path as a proxy to complement this incomplete path. This newly constructed complete path ensures that the connection adheres to the constraint of the narrow lobe within the BSDF of the specular/glossy surface. Unbiased reciprocal estimation is the key to our method to obtain a probability density function (PDF) reciprocal to ensure unbiased rendering. We derive the reciprocal estimation method and provide an efficiency-optimized setting for efficient sampling and connection. Our method provides a robust tool for substituting problematic paths with favorable alternatives while ensuring unbiasedness. We validate this approach in the probabilistic connections BDPT for addressing specular-involved difficult paths. Experimental results have proved the effectiveness and efficiency of our approach, showcasing high-performance rendering capabilities across diverse settings.},
  archive      = {J_TOG},
  doi          = {10.1145/3658216},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Proxy tracing: Unbiased reciprocal estimation for optimized sampling in BDPT},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CharacterGen: Efficient 3D character generation from single
images with multi-view pose canonicalization. <em>TOG</em>,
<em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BNRist, Department of Computer Science and Technology, Tsinghua University, China In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity. In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters. CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view diffusion model. This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses. A transformer-based, generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images. We also adopt a texture-back-projection strategy to produce high-quality texture maps. Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model. Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation.},
  archive      = {J_TOG},
  doi          = {10.1145/3658217},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {CharacterGen: Efficient 3D character generation from single images with multi-view pose canonicalization},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporally stable metropolis light transport denoising using
recurrent transformer blocks. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metropolis Light Transport (MLT) is a global illumination algorithm that is well-known for rendering challenging scenes with intricate light paths. However, MLT methods tend to produce unpredictable correlation artifacts in images, which can introduce visual inconsistencies for animation rendering. This drawback also makes it challenging to denoise MLT renderings while maintaining temporal stability. We tackle this issue with modern learning-based methods and build a sequence denoiser combining the recurrent connections with the cutting-edge vision transformer architecture. We demonstrate that our sophisticated denoiser can consistently improve the quality and temporal stability of MLT renderings with difficult light paths. Our method is efficient and scalable for complex scene renderings that require high sample counts.},
  archive      = {J_TOG},
  doi          = {10.1145/3658218},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Temporally stable metropolis light transport denoising using recurrent transformer blocks},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational feature extraction in scientific visualization.
<em>TOG</em>, <em>43</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3658219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Across many scientific disciplines, the pursuit of even higher grid resolutions leads to a severe scalability problem in scientific computing. Feature extraction is a commonly chosen approach to reduce the amount of information from dense fields down to geometric primitives that further enable a quantitative analysis. Examples of common features are isolines, extremal lines, or vortex corelines. Due to the rising complexity of the observed phenomena, or in the event of discretization issues with the data, a straightforward application of textbook feature definitions is unfortunately insufficient. Thus, feature extraction from spatial data often requires substantial pre- or post-processing to either clean up the results or to include additional domain knowledge about the feature in question. Such a separate pre- or post-processing of features not only leads to suboptimal and incomparable solutions, it also results in many specialized feature extraction algorithms arising in the different application domains. In this paper, we establish a mathematical language that not only encompasses commonly used feature definitions, it also provides a set of regularizers that can be applied across the bounds of individual application domains. By using the language of variational calculus, we treat features as variational minimizers, which can be combined and regularized as needed. Our formulation not only encompasses existing feature definitions as special case, it also opens the path to novel feature definitions. This work lays the foundations for many new research directions regarding formal definitions, data representations, and numerical extraction algorithms.},
  archive      = {J_TOG},
  doi          = {10.1145/3658219},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Variational feature extraction in scientific visualization},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A heat method for generalized signed distance. <em>TOG</em>,
<em>43</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3658220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a method for approximating the signed distance function (SDF) of geometry corrupted by holes, noise, or self-intersections. The method implicitly defines a completed version of the shape, rather than explicitly repairing the given input. Our starting point is a modified version of the heat method for geodesic distance, which diffuses normal vectors rather than a scalar distribution. This formulation provides robustness akin to generalized winding numbers (GWN) , but provides distance function rather than just an inside/outside classification. Our formulation also offers several features not common to classic distance algorithms, such as the ability to simultaneously fit multiple level sets, a notion of distance for geometry that does not topologically bound any region, and the ability to mix and match signed and unsigned distance. The method can be applied in any dimension and to any spatial discretization, including triangle meshes, tet meshes, point clouds, polygonal meshes, voxelized surfaces, and regular grids. We evaluate the method on several challenging examples, implementing normal offsets and other morphological operations directly on imperfect curve and surface data. In many cases we also obtain an inside/outside classification dramatically more robust than the one obtained provided by GWN.},
  archive      = {J_TOG},
  doi          = {10.1145/3658220},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {A heat method for generalized signed distance},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiffPoseTalk: Speech-driven stylistic 3D facial animation
and head pose generation via diffusion models. <em>TOG</em>,
<em>43</em>(4), 1–9. (<a href="https://doi.org/10.1145/3658221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generation of stylistic 3D facial animations driven by speech presents a significant challenge as it requires learning a many-to-many mapping between speech, style, and the corresponding natural facial motion. However, existing methods either employ a deterministic model for speech-to-motion mapping or encode the style using a one-hot encoding scheme. Notably, the one-hot encoding approach fails to capture the complexity of the style and thus limits generalization ability. In this paper, we propose DiffPoseTalk, a generative framework based on the diffusion model combined with a style encoder that extracts style embeddings from short reference videos. During inference, we employ classifier-free guidance to guide the generation process based on the speech and style. In particular, our style includes the generation of head poses, thereby enhancing user perception. Additionally, we address the shortage of scanned 3D talking face data by training our model on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset. Extensive experiments and user study demonstrate that our approach outperforms state-of-the-art methods. The code and dataset are at https://diffposetalk.github.io.},
  archive      = {J_TOG},
  doi          = {10.1145/3658221},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-9},
  shortjournal = {ACM Trans. Graph.},
  title        = {DiffPoseTalk: Speech-driven stylistic 3D facial animation and head pose generation via diffusion models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EASI-tex: Edge-aware mesh texturing from single image.
<em>TOG</em>, <em>43</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3658222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for single-image mesh texturing , which employs a diffusion model with judicious conditioning to seamlessly transfer an object&#39;s texture from a single RGB image to a given 3D mesh object. We do not assume that the two objects belong to the same category, and even if they do, there can be significant discrepancies in their geometry and part proportions. Our method aims to rectify the discrepancies by conditioning a pre-trained Stable Diffusion generator with edges describing the mesh through ControlNet, and features extracted from the input image using IP-Adapter to generate textures that respect the underlying geometry of the mesh and the input texture without any optimization or training. We also introduce Image Inversion , a novel technique to quickly personalize the diffusion model for a single concept using a single image , for cases where the pre-trained IP-Adapter falls short in capturing all the details from the input image faithfully. Experimental results demonstrate the efficiency and effectiveness of our edge-aware single-image mesh texturing approach, coined EASI-Tex, in preserving the details of the input texture on diverse 3D objects, while respecting their geometry. Code : https://github.com/sairajk/easi-tex},
  archive      = {J_TOG},
  doi          = {10.1145/3658222},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {EASI-tex: Edge-aware mesh texturing from single image},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-material mesh-based surface tracking with implicit
topology changes. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a multi-material non-manifold mesh-based surface tracking algorithm that converts self-intersections into topological changes. Our algorithm generalizes prior work on manifold surface tracking with topological changes: it preserves surface features like mesh-based methods, and it robustly handles topological changes like level set methods. Our method also offers improved efficiency and robustness over the state of the art. We demonstrate the effectiveness of the approach on a range of examples, including complex soap film simulations with thousands of interacting bubbles, and boolean unions of non-manifold meshes consisting of millions of triangles.},
  archive      = {J_TOG},
  doi          = {10.1145/3658223},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Multi-material mesh-based surface tracking with implicit topology changes},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fully-correlated anisotropic micrograin BSDF model.
<em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an improved version of the micrograin BSDF model [Lucas et al. 2023] for the rendering of anisotropic porous layers. Our approach leverages the properties of micrograins to take into account the correlation between their height and normal, as well as the correlation between the light and view directions. This allows us to derive an exact analytical expression for the Geometrical Attenuation Factor (GAF), summarizing shadowing and masking inside the porous layer. This fully-correlated GAF is then used to define appropriate mixing weights to blend the BSDFs of the porous and base layers. Furthermore, by generalizing the micrograins shape to anisotropy, combined with their fully-correlated GAF, our improved BSDF model produces effects specific to porous layers such as retro-reflection visible on dust layers at grazing angles or height and color correlation that can be found on rusty materials. Finally, we demonstrate very close matches between our BSDF model and light transport simulations realized with explicit instances of micrograins, thus validating our model.},
  archive      = {J_TOG},
  doi          = {10.1145/3658224},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A fully-correlated anisotropic micrograin BSDF model},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Split-aperture 2-in-1 computational cameras. <em>TOG</em>,
<em>43</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3658225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While conventional cameras offer versatility for applications ranging from amateur photography to autonomous driving, computational cameras allow for domain-specific adaption. Cameras with co-designed optics and image processing algorithms enable high-dynamic-range image recovery, depth estimation, and hyperspectral imaging through optically encoding scene information that is otherwise undetected by conventional cameras. However, this optical encoding creates a challenging inverse reconstruction problem for conventional image recovery, and often lowers the overall photographic quality. Thus computational cameras with domain-specific optics have only been adopted in a few specialized applications where the captured information cannot be acquired in other ways. In this work, we investigate a method that combines two optical systems into one to tackle this challenge. We split the aperture of a conventional camera into two halves: one which applies an application-specific modulation to the incident light via a diffractive optical element to produce a coded image capture, and one which applies no modulation to produce a conventional image capture. Co-designing the phase modulation of the split aperture with a dual-pixel sensor allows us to simultaneously capture these coded and uncoded images without increasing physical or computational footprint. With an uncoded conventional image alongside the optically coded image in hand, we investigate image reconstruction methods that are conditioned on the conventional image, making it possible to eliminate artifacts and compute costs that existing methods struggle with. We assess the proposed method with 2-in-1 cameras for optical high-dynamic-range reconstruction, monocular depth estimation, and hyperspectral imaging, comparing favorably to all tested methods in all applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3658225},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Split-aperture 2-in-1 computational cameras},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FVDB: A deep-learning framework for sparse, large scale, and
high performance spatial intelligence. <em>TOG</em>, <em>43</em>(4),
1–15. (<a href="https://doi.org/10.1145/3658226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present f VDB, a novel GPU-optimized framework for deep learning on large-scale 3D data. f VDB provides a complete set of differentiable primitives to build deep learning architectures for common tasks in 3D learning such as convolution, pooling, attention, ray-tracing, meshing, etc. f VDB simultaneously provides a much larger feature set (primitives and operators) than established frameworks with no loss in efficiency: our operators match or exceed the performance of other frameworks with narrower scope. Furthermore, f VDB can process datasets with much larger footprint and spatial resolution than prior works, while providing a competitive memory footprint on small inputs. To achieve this combination of versatility and performance, f VDB relies on a single novel VDB index grid acceleration structure paired with several key innovations including GPU accelerated sparse grid construction, convolution using tensorcores, fast ray tracing kernels using a Hierarchical Digital Differential Analyzer algorithm (HDDA), and jagged tensors. Our framework is fully integrated with PyTorch enabling interoperability with existing pipelines, and we demonstrate its effectiveness on a number of representative tasks such as large-scale point-cloud segmentation, high resolution 3D generative modeling, unbounded scale Neural Radiance Fields, and large-scale point cloud reconstruction.},
  archive      = {J_TOG},
  doi          = {10.1145/3658226},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {FVDB: A deep-learning framework for sparse, large scale, and high performance spatial intelligence},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive design of stylized walking gaits for robotic
characters. <em>TOG</em>, <em>43</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3658227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural animation has seen widespread use in the design of expressive walking gaits for virtual characters. While similar tools could breathe life into robotic characters, existing techniques are largely unaware of the kinematic and dynamic constraints imposed by physical robots. In this paper, we propose a system for the artist-directed authoring of stylized bipedal walking gaits, tailored for execution on robotic characters. The artist interfaces with an interactive editing tool that generates the desired character motion in realtime, either on the physical or simulated robot, using a model-based control stack. Each walking style is encoded as a set of sample parameters which are translated into whole-body reference trajectories using the proposed procedural animation technique. In order to generalize the stylized gait over a continuous range of input velocities, we employ a phase-space blending strategy that interpolates a set of example walk cycles authored by the animator while preserving contact constraints. To demonstrate the utility of our approach, we animate gaits for a custom, free-walking robotic character, and show, with two additional in-simulation examples, how our procedural animation technique generalizes to bipeds with different degrees of freedom, proportions, and mass distributions.},
  archive      = {J_TOG},
  doi          = {10.1145/3658227},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive design of stylized walking gaits for robotic characters},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust containment queries over collections of rational
parametric curves via generalized winding numbers. <em>TOG</em>,
<em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point containment queries for regions bound by watertight geometric surfaces, i.e., closed and without self-intersections, can be evaluated straightforwardly with a number of well-studied algorithms. When this assumption on domain geometry is not met, such methods are either unusable, or prone to misclassifications that can lead to cascading errors in downstream applications. More robust point classification schemes based on generalized winding numbers have been proposed, as they are indifferent to these imperfections. However, existing algorithms are limited to point clouds and collections of linear elements. We extend this methodology to encompass more general curved shapes with an algorithm that evaluates the winding number scalar field over unstructured collections of rational parametric curves. In particular, we evaluate the winding number for each curve independently, making the derived containment query robust to how the curves are arranged. We ensure geometric fidelity in our queries by treating each curve as equivalent to an adaptively constructed polyline that provably has the same generalized winding number at the point of interest. Our algorithm is numerically stable for points that are arbitrarily close to the model, and explicitly treats points that are coincident with curves. We demonstrate the improvements in computational performance granted by this method over conventional techniques as well as the robustness induced by its application.},
  archive      = {J_TOG},
  doi          = {10.1145/3658228},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Robust containment queries over collections of rational parametric curves via generalized winding numbers},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LightFormer: Light-oriented global neural rendering in
dynamic scene. <em>TOG</em>, <em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generation of global illumination in real time has been a long-standing challenge in the graphics community, particularly in dynamic scenes with complex illumination. Recent neural rendering techniques have shown great promise by utilizing neural networks to represent the illumination of scenes and then decoding the final radiance. However, incorporating object parameters into the representation may limit their effectiveness in handling fully dynamic scenes. This work presents a neural rendering approach, dubbed LightFormer , that can generate realistic global illumination for fully dynamic scenes, including dynamic lighting, materials, cameras, and animated objects, in real time. Inspired by classic many-lights methods, the proposed approach focuses on the neural representation of light sources in the scene rather than the entire scene, leading to the overall better generalizability. The neural prediction is achieved by leveraging the virtual point lights and shading clues for each light. Specifically, two stages are explored. In the light encoding stage, each light generates a set of virtual point lights in the scene, which are then encoded into an implicit neural light representation, along with screen-space shading clues like visibility. In the light gathering stage, a pixel-light attention mechanism composites all light representations for each shading point. Given the geometry and material representation, in tandem with the composed light representations of all lights, a lightweight neural network predicts the final radiance. Experimental results demonstrate that the proposed LightFormer can yield reasonable and realistic global illumination in fully dynamic scenes with real-time performance.},
  archive      = {J_TOG},
  doi          = {10.1145/3658229},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {LightFormer: Light-oriented global neural rendering in dynamic scene},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NICER: A new and improved consumed endurance and recovery
metric to quantify muscle fatigue of mid-air interactions. <em>TOG</em>,
<em>43</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3658230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural gestures are crucial for mid-air interaction, but predicting and managing muscle fatigue is challenging. Existing torque-based models are limited in their ability to model above-shoulder interactions and to account for fatigue recovery. We introduce a new hybrid model, NICER , which combines a torque-based approach with a new term derived from the empirical measurement of muscle contraction and a recovery factor to account for decreasing fatigue during rest. We evaluated NICER in a mid-air selection task using two interaction methods with different degrees of perceived fatigue. Results show that NICER can accurately model above-shoulder interactions as well as reflect fatigue recovery during rest periods. Moreover, both interaction methods show a stronger correlation with subjective fatigue measurement ( ρ = 0.978/0.976) than a previous model, Cumulative Fatigue ( ρ = 0.966/0.923), confirming that NICER is a powerful analytical tool to predict fatigue across a variety of gesture-based interactive applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3658230},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {NICER: A new and improved consumed endurance and recovery metric to quantify muscle fatigue of mid-air interactions},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational illusion knitting. <em>TOG</em>,
<em>43</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3658231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Illusion-knit fabrics reveal distinct patterns or images depending on the viewing angle. Artists have manually achieved this effect by exploiting &quot;microgeometry,&quot; i.e., small differences in stitch heights. However, past work in computational 3D knitting does not model or exploit designs based on stitch height variation. This paper establishes a foundation for exploring illusion knitting in the context of computational design and fabrication. We observe that the design space is highly constrained, elucidate these constraints, and derive strategies for developing effective, machine-knittable illusion patterns. We partially automate these strategies in a new interactive design tool that reduces difficult patterning tasks to familiar image editing tasks. Illusion patterns also uncover new fabrication challenges regarding mixed colorwork and texture; we describe new algorithms for mitigating fabrication failures and ensuring high-quality knit results.},
  archive      = {J_TOG},
  doi          = {10.1145/3658231},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational illusion knitting},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Theory of human tetrachromatic color experience and
printing. <em>TOG</em>, <em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic studies indicate that more than 50% of women are genetically tetrachromatic, expressing four distinct types of color photoreceptors (cone cells) in the retina. At least one functional tetrachromat has been identified in laboratory tests. We hypothesize that there is a large latent group in the population capable of fundamentally richer color experience, but we are not yet aware of this group because of a lack of tetrachromatic colors in the visual environment. This paper develops theory and engineering practice for fabricating tetrachromatic colors and potentially identifying tetrachromatic color vision in the wild. First, we apply general d -dimensional color theory to derive and compute all the key color structures of human tetrachromacy for the first time, including its 4D space of possible object colors, 3D space of chromaticities, and yielding a predicted 2D sphere of tetrachromatic hues. We compare this predicted hue sphere to the familiar hue circle of trichromatic color, extending the theory to predict how the higher dimensional topology produces an expanded color experience for tetrachromats. Second, we derive the four reflectance functions for the ideal tetrachromatic inkset, analogous to the well-known CMY printing basis for trichromacy. Third, we develop a method for prototyping tetrachromatic printers using a library of fountain pen inks and a multi-pass inkjet printing platform. Fourth, we generalize existing color tests - sensitive hue ordering tests and rapid isochromatic plate screening tests - to higher-dimensional vision, and prototype variants of these tests for identifying and characterizing tetrachromacy in the wild.},
  archive      = {J_TOG},
  doi          = {10.1145/3658232},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Theory of human tetrachromatic color experience and printing},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-SLAM: Scalable dense SLAM for task-aware optimization
using CSFD. <em>TOG</em>, <em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present X-SLAM, a real-time dense differentiable SLAM system that leverages the complex-step finite difference (CSFD) method for efficient calculation of numerical derivatives, bypassing the need for a large-scale computational graph. The key to our approach is treating the SLAM process as a differentiable function, enabling the calculation of the derivatives of important SLAM parameters through Taylor series expansion within the complex domain. Our system allows for the real-time calculation of not just the gradient, but also higher-order differentiation. This facilitates the use of high-order optimizers to achieve better accuracy and faster convergence. Building on X-SLAM, we implemented end-to-end optimization frameworks for two important tasks: camera relocalization in wide outdoor scenes and active robotic scanning in complex indoor environments. Comprehensive evaluations on public benchmarks and intricate real scenes underscore the improvements in the accuracy of camera relocalization and the efficiency of robotic navigation achieved through our task-aware optimization. The code and data are available at https://gapszju.github.io/X-SLAM.},
  archive      = {J_TOG},
  doi          = {10.1145/3658233},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {X-SLAM: Scalable dense SLAM for task-aware optimization using CSFD},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Universal facial encoding of codec avatars from VR headsets.
<em>TOG</em>, <em>43</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3658234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Faithful real-time facial animation is essential for avatar-mediated telepresence in Virtual Reality (VR). To emulate authentic communication, avatar animation needs to be efficient and accurate: able to capture both extreme and subtle expressions within a few milliseconds to sustain the rhythm of natural conversations. The oblique and incomplete views of the face, variability in the donning of headsets, and illumination variation due to the environment are some of the unique challenges in generalization to unseen faces. In this paper, we present a method that can animate a photorealistic avatar in realtime from head-mounted cameras (HMCs) on a consumer VR headset. We present a self-supervised learning approach, based on a cross-view reconstruction objective, that enables generalization to unseen users. We present a lightweight expression calibration mechanism that increases accuracy with minimal additional cost to run-time efficiency. We present an improved parameterization for precise ground-truth generation that provides robustness to environmental variation. The resulting system produces accurate facial animation for unseen users wearing VR headsets in realtime. We compare our approach to prior face-encoding methods demonstrating significant improvements in both quantitative metrics and qualitative results.},
  archive      = {J_TOG},
  doi          = {10.1145/3658234},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. Graph.},
  title        = {Universal facial encoding of codec avatars from VR headsets},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Audio matters too! Enhancing markerless motion capture with
audio signals for string performance capture. <em>TOG</em>,
<em>43</em>(4), 1–10. (<a
href="https://doi.org/10.1145/3658235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection. It holds significant implications and guidance for string instrument pedagogy, animation, and virtual concerts, as well as for both musical performance analysis and generation. Our code and SPD dataset are available at https://github.com/Yitongishere/string_performance.},
  archive      = {J_TOG},
  doi          = {10.1145/3658235},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Audio matters too! enhancing markerless motion capture with audio signals for string performance capture},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiffCAD: Weakly-supervised probabilistic CAD model retrieval
and alignment from an RGB image. <em>TOG</em>, <em>43</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3658236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perceiving 3D structures from RGB images based on CAD model primitives can enable an effective, efficient 3D object-based representation of scenes. However, current approaches rely on supervision from expensive yet imperfect annotations of CAD models associated with real images, and encounter challenges due to the inherent ambiguities in the task - both in depth-scale ambiguity in monocular perception, as well as inexact matches of CAD database models to real observations. We thus propose DiffCAD, the first weakly-supervised probabilistic approach to CAD retrieval and alignment from an RGB image. We learn a probabilistic model through diffusion, modeling likely distributions of shape, pose, and scale of CAD objects in an image. This enables multi-hypothesis generation of different plausible CAD reconstructions, requiring only a few hypotheses to characterize ambiguities in depth/scale and inexact shape matches. Our approach is trained only on synthetic data, leveraging monocular depth and mask estimates to enable robust zero-shot adaptation to various real target domains. Despite being trained solely on synthetic data, our multi-hypothesis approach can even surpass the supervised state-of-the-art on the Scan2CAD dataset by 5.9% with 8 hypotheses.},
  archive      = {J_TOG},
  doi          = {10.1145/3658236},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {DiffCAD: Weakly-supervised probabilistic CAD model retrieval and alignment from an RGB image},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RealFill: Reference-driven generation for authentic image
completion. <em>TOG</em>, <em>43</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3658237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions. However, the content these models hallucinate is necessarily inauthentic, since they are unaware of the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging scenarios, and find that it outperforms existing approaches by a large margin. Project page: https://realfill.github.io.},
  archive      = {J_TOG},
  doi          = {10.1145/3658237},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {RealFill: Reference-driven generation for authentic image completion},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating gesture generation in a large-scale open
challenge: The GENEA challenge 2022. <em>TOG</em>, <em>43</em>(3), 1–28.
(<a href="https://doi.org/10.1145/3656374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reports on the second GENEA Challenge to benchmark data-driven automatic co-speech gesture generation. Participating teams used the same speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was rendered to video using a standardised visualisation pipeline and evaluated in several large, crowdsourced user studies. Unlike when comparing different research articles, differences in results are here only due to differences between methods, enabling direct comparison between systems. The dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in a dyadic conversation. Ten teams participated in the challenge across two tiers: full-body and upper-body gesticulation. For each tier, we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech signal. Our evaluations decouple human-likeness from gesture appropriateness, which has been a difficult problem in the field. The evaluation results show some synthetic gesture conditions being rated as significantly more human-like than 3D human motion capture. To the best of our knowledge, this has not been demonstrated before. On the other hand, all synthetic motion is found to be vastly less appropriate for the speech than the original motion-capture recordings. We also find that conventional objective metrics do not correlate well with subjective human-likeness ratings in this large evaluation. The one exception is the Fréchet gesture distance (FGD), which achieves a Kendall’s tau rank correlation of around -0.5. Based on the challenge results we formulate numerous recommendations for system building and evaluation.},
  archive      = {J_TOG},
  doi          = {10.1145/3656374},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {1-28},
  shortjournal = {ACM Trans. Graph.},
  title        = {Evaluating gesture generation in a large-scale open challenge: The GENEA challenge 2022},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time neural appearance models. <em>TOG</em>,
<em>43</em>(3), 1–17. (<a
href="https://doi.org/10.1145/3659577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a complete system for real-time rendering of scenes with complex appearance previously reserved for offline use. This is achieved with a combination of algorithmic and system level innovations. Our appearance model utilizes learned hierarchical textures that are interpreted using neural decoders, which produce reflectance values and importance-sampled directions. To best utilize the modeling capacity of the decoders, we equip the decoders with two graphics priors. The first prior—transformation of directions into learned shading frames—facilitates accurate reconstruction of mesoscale effects. The second prior—a microfacet sampling distribution—allows the neural decoder to perform importance sampling efficiently. The resulting appearance model supports anisotropic sampling and level-of-detail rendering, and allows baking deeply layered material graphs into a compact unified neural representation. By exposing hardware accelerated tensor operations to ray tracing shaders, we show that it is possible to inline and execute the neural decoders efficiently inside a real-time path tracer. We analyze scalability with increasing number of neural materials and propose to improve performance using code optimized for coherent and divergent execution. Our neural material shaders can be over an order of magnitude faster than non-neural layered materials. This opens up the door for using film-quality visuals in real-time applications such as games and live previews.},
  archive      = {J_TOG},
  doi          = {10.1145/3659577},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time neural appearance models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ConceptLab: Creative concept generation using VLM-guided
diffusion prior constraints. <em>TOG</em>, <em>43</em>(3), 1–14. (<a
href="https://doi.org/10.1145/3659578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent text-to-image generative models have enabled us to transform our words into vibrant, captivating imagery. The surge of personalization techniques that has followed has also allowed us to imagine unique concepts in new scenes. However, an intriguing question remains: How can we generate a new , imaginary concept that has never been seen before? In this article, we present the task of creative text-to-image generation , where we seek to generate new members of a broad category (e.g., generating a pet that differs from all existing pets). We leverage the under-studied Diffusion Prior models and show that the creative generation problem can be formulated as an optimization process over the output space of the diffusion prior, resulting in a set of “prior constraints.” To keep our generated concept from converging into existing members, we incorporate a question-answering Vision-Language Model that adaptively adds new constraints to the optimization problem, encouraging the model to discover increasingly more unique creations. Finally, we show that our prior constraints can also serve as a strong mixing mechanism allowing us to create hybrids between generated concepts, introducing even more flexibility into the creative process.},
  archive      = {J_TOG},
  doi          = {10.1145/3659578},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {ConceptLab: Creative concept generation using VLM-guided diffusion prior constraints},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable solver for time-dependent deformation
problems with contact. <em>TOG</em>, <em>43</em>(3), 1–30. (<a
href="https://doi.org/10.1145/3657648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a general differentiable solver for time-dependent deformation problems with contact and friction. Our approach uses a finite element discretization with a high-order time integrator coupled with the recently proposed incremental potential contact method for handling contact and friction forces to solve ODE- and PDE-constrained optimization problems on scenes with complex geometry. It supports static and dynamic problems and differentiation with respect to all physical parameters involved in the physical problem description, which include shape, material parameters, friction parameters, and initial conditions. Our analytically derived adjoint formulation is efficient, with a small overhead (typically less than 10% for nonlinear problems) over the forward simulation, and shares many similarities with the forward problem, allowing the reuse of large parts of existing forward simulator code. We implement our approach on top of the open-source PolyFEM library and demonstrate the applicability of our solver to shape design, initial condition optimization, and material estimation on both simulated results and physical validations.},
  archive      = {J_TOG},
  doi          = {10.1145/3657648},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {3},
  pages        = {1-30},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable solver for time-dependent deformation problems with contact},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). View-independent adjoint light tracing for lighting design
optimization. <em>TOG</em>, <em>43</em>(3), 1–16. (<a
href="https://doi.org/10.1145/3662180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable rendering methods promise the ability to optimize various parameters of three-dimensional (3D) scenes to achieve a desired result. However, lighting design has so far received little attention in this field. In this article, we introduce a method that enables continuous optimization of the arrangement of luminaires in a 3D scene via differentiable light tracing. Our experiments show two major issues when attempting to apply existing methods from differentiable path tracing to this problem: First, many rendering methods produce images, which restricts the ability of a designer to define lighting objectives to image space. Second, most previous methods are designed for scene geometry or material optimization and have not been extensively tested for the case of optimizing light sources. Currently available differentiable ray-tracing methods do not provide satisfactory performance, even on fairly basic test cases in our experience. In this article, we propose, to the best of our knowledge, a novel adjoint light tracing method that overcomes these challenges and enables gradient-based lighting design optimization in a view-independent (camera-free) way. Thus, we allow the user to paint illumination targets directly onto the 3D scene or use existing baked illumination data (e.g., light maps). Using modern ray-tracing hardware, we achieve interactive performance. We find light tracing advantageous over path tracing in this setting, as it naturally handles irregular geometry, resulting in less noise and improved optimization convergence. We compare our adjoint gradients to state-of-the-art image-based differentiable rendering methods. We also demonstrate that our gradient data works with various common optimization algorithms, providing good convergence behaviour. Qualitative comparisons with real-world scenes underline the practical applicability of our method.},
  archive      = {J_TOG},
  doi          = {10.1145/3662180},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {3},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {View-independent adjoint light tracing for lighting design optimization},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Importance sampling BRDF derivatives. <em>TOG</em>,
<em>43</em>(3), 1–21. (<a
href="https://doi.org/10.1145/3648611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a set of techniques to efficiently importance sample the derivatives of a wide range of Bidirectional Reflectance Distribution Function (BRDF) models. In differentiable rendering, BRDFs are replaced by their differential BRDF counterparts, which are real-valued and can have negative values. This leads to a new source of variance arising from their change in sign. Real-valued functions cannot be perfectly importance sampled by a positive-valued PDF, and the direct application of BRDF sampling leads to high variance. Previous attempts at antithetic sampling only addressed the derivative with the roughness parameter of isotropic microfacet BRDFs. Our work generalizes BRDF derivative sampling to anisotropic microfacet models, mixture BRDFs, Oren-Nayar, Hanrahan-Krueger, among other analytic BRDFs. Our method first decomposes the real-valued differential BRDF into a sum of single-signed functions, eliminating variance from a change in sign. Next, we importance sample each of the resulting single-signed functions separately. The first decomposition, positivization, partitions the real-valued function based on its sign, and is effective at variance reduction when applicable. However, it requires analytic knowledge of the roots of the differential BRDF, and for it to be analytically integrable too. Our key insight is that the single-signed functions can have overlapping support, which significantly broadens the ways we can decompose a real-valued function. Our product and mixture decompositions exploit this property, and they allow us to support several BRDF derivatives that positivization could not handle. For a wide variety of BRDF derivatives, our method significantly reduces the variance (up to 58× in some cases) at equal computation cost and enables better recovery of spatially varying textures through gradient-descent-based inverse rendering.},
  archive      = {J_TOG},
  doi          = {10.1145/3648611},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Importance sampling BRDF derivatives},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online neural path guiding with normalized anisotropic
spherical gaussians. <em>TOG</em>, <em>43</em>(3), 1–18. (<a
href="https://doi.org/10.1145/3649310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Importance sampling techniques significantly reduce variance in physically based rendering. In this article, we propose a novel online framework to learn the spatial-varying distribution of the full product of the rendering equation, with a single small neural network using stochastic ray samples. The learned distributions can be used to efficiently sample the full product of incident light. To accomplish this, we introduce a novel closed-form density model, called the Normalized Anisotropic Spherical Gaussian mixture, that can model a complex light field with a small number of parameters and that can be directly sampled. Our framework progressively renders and learns the distribution, without requiring any warm-up phases. With the compact and expressive representation of our density model, our framework can be implemented entirely on the GPU, allowing it to produce high-quality images with limited computational resources. The results show that our framework outperforms existing neural path guiding approaches and achieves comparable or even better performance than state-of-the-art online statistical path guiding techniques.},
  archive      = {J_TOG},
  doi          = {10.1145/3649310},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Online neural path guiding with normalized anisotropic spherical gaussians},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual-particle approach for incompressible SPH fluids.
<em>TOG</em>, <em>43</em>(3), 1–18. (<a
href="https://doi.org/10.1145/3649888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensile instability is one of the major obstacles to particle methods in fluid simulation, which would cause particles to clump in pairs under tension and prevent fluid simulation to generate small-scale thin features. To address this issue, previous particle methods either use a background pressure or a finite difference scheme to alleviate the particle clustering artifacts, yet still fail to produce small-scale thin features in free-surface flows. In this article, we propose a dual-particle approach for simulating incompressible fluids. Our approach involves incorporating supplementary virtual particles designed to capture and store particle pressures. These pressure samples undergo systematic redistribution at each time step, grounded in the initial positions of the fluid particles. By doing so, we effectively reduce tensile instability in standard SPH by narrowing down the unstable regions for particles experiencing tensile stress. As a result, we can accurately simulate free-surface flows with rich small-scale thin features, such as droplets, streamlines, and sheets, as demonstrated by experimental results.},
  archive      = {J_TOG},
  doi          = {10.1145/3649888},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {A dual-particle approach for incompressible SPH fluids},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HQ3DAvatar: High-quality implicit 3D head avatar.
<em>TOG</em>, <em>43</em>(3), 1–24. (<a
href="https://doi.org/10.1145/3649889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view volumetric rendering techniques have recently shown great potential in modeling and synthesizing high-quality head avatars. A common approach to capture full head dynamic performances is to track the underlying geometry using a mesh-based template or 3D cube-based graphics primitives. While these model-based approaches achieve promising results, they often fail to learn complex geometric details such as the mouth interior, hair, and topological changes over time. This article presents a novel approach to building highly photorealistic digital head avatars. Our method learns a canonical space via an implicit function parameterized by a neural network. It leverages multiresolution hash encoding in the learned feature space, allowing for high quality, faster training, and high-resolution rendering. At test time, our method is driven by a monocular RGB video. Here, an image encoder extracts face-specific features that also condition the learnable canonical space. This encourages deformation-dependent texture variations during training. We also propose a novel optical flow-based loss that ensures correspondences in the learned canonical space, thus encouraging artifact-free and temporally consistent renderings. We show results on challenging facial expressions and show free-viewpoint renderings at interactive real-time rates for a resolution of 480 x 270. Our method outperforms related approaches both visually and numerically. We will release our multiple-identity dataset to encourage further research.},
  archive      = {J_TOG},
  doi          = {10.1145/3649889},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Graph.},
  title        = {HQ3DAvatar: High-quality implicit 3D head avatar},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint stroke tracing and correspondence for 2D animation.
<em>TOG</em>, <em>43</em>(3), 1–17. (<a
href="https://doi.org/10.1145/3649890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To alleviate human labor in redrawing keyframes with ordered vector strokes for automatic inbetweening, we for the first time propose a joint stroke tracing and correspondence approach. Given consecutive raster keyframes along with a single vector image of the starting frame as a guidance, the approach generates vector drawings for the remaining keyframes while ensuring one-to-one stroke correspondence. Our framework trained on clean line drawings generalizes to rough sketches, and the generated results can be imported into inbetweening systems to produce inbetween sequences. Hence, the method is compatible with standard 2D animation workflow. An adaptive spatial transformation module (ASTM) is introduced to handle non-rigid motions and stroke distortion. We collect a dataset for training with 10k+ pairs of raster frames and their vector drawings with stroke correspondence. Comprehensive validations on real clean and rough animated frames manifest the effectiveness of our method and superiority to existing methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3649890},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Joint stroke tracing and correspondence for 2D animation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DMHomo: Learning homography with diffusion models.
<em>TOG</em>, <em>43</em>(3), 1–16. (<a
href="https://doi.org/10.1145/3652207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised homography estimation methods face a challenge due to the lack of adequate labeled training data. To address this issue, we propose DMHomo , a diffusion model-based framework for supervised homography learning. This framework generates image pairs with accurate labels, realistic image content, and realistic interval motion, ensuring that they satisfy adequate pairs. We utilize unlabeled image pairs with pseudo labels such as homography and dominant plane masks, computed from existing methods, to train a diffusion model that generates a supervised training dataset. To further enhance performance, we introduce a new probabilistic mask loss, which identifies outlier regions through supervised training, and an iterative mechanism to optimize the generative and homography models successively. Our experimental results demonstrate that DMHomo effectively overcomes the scarcity of qualified datasets in supervised homography learning and improves generalization to real-world scenes. The code and dataset are available at GitHub ( https://github.com/lhaippp/DMHomo ).},
  archive      = {J_TOG},
  doi          = {10.1145/3652207},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {DMHomo: Learning homography with diffusion models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised high dynamic range imaging: What can be
learned from a single 8-bit video? <em>TOG</em>, <em>43</em>(2),
24:1–16. (<a href="https://doi.org/10.1145/3648570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Deep Learning-based methods for inverse tone mapping standard dynamic range (SDR) images to obtain high dynamic range (HDR) images have become very popular. These methods manage to fill over-exposed areas convincingly both in terms of details and dynamic range. To be effective, deep learning-based methods need to learn from large datasets and transfer this knowledge to the network weights. In this work, we tackle this problem from a completely different perspective. What can we learn from a single SDR 8-bit video? With the presented self-supervised approach, we show that, in many cases, a single SDR video is sufficient to generate an HDR video of the same quality or better than other state-of-the-art methods.},
  archive      = {J_TOG},
  author       = {Francesco Banterle and Demetris Marnerides and Thomas Bashford-rogers and Kurt Debattista},
  doi          = {10.1145/3648570},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {2},
  pages        = {24:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Self-supervised high dynamic range imaging: What can be learned from a single 8-bit video?},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GIPC: Fast and stable gauss-newton optimization of IPC
barrier energy. <em>TOG</em>, <em>43</em>(2), 23:1–18. (<a
href="https://doi.org/10.1145/3643028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Barrier functions are crucial for maintaining an intersection- and inversion-free simulation trajectory but existing methods, which directly use distance can restrict implementation design and performance. We present an approach to rewriting the barrier function for arriving at an efficient and robust approximation of its Hessian. The key idea is to formulate a simplicial geometric measure of contact using mesh boundary elements, from which analytic eigensystems are derived and enhanced with filtering and stiffening terms that ensure robustness with respect to the convergence of a Project-Newton solver. A further advantage of our rewriting of the barrier function is that it naturally caters to the notorious case of nearly parallel edge-edge contacts for which we also present a novel analytic eigensystem. Our approach is thus well suited for standard second-order unconstrained optimization strategies for resolving contacts, minimizing nonlinear nonconvex functions where the Hessian may be indefinite. The efficiency of our eigensystems alone yields a 3× speedup over the standard Incremental Potential Contact (IPC) barrier formulation. We further apply our analytic proxy eigensystems to produce an entirely GPU-based implementation of IPC with significant further acceleration.},
  archive      = {J_TOG},
  author       = {Kemeng Huang and Floyd M. Chitalu and Huancheng Lin and Taku Komura},
  doi          = {10.1145/3643028},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {2},
  pages        = {23:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {GIPC: Fast and stable gauss-newton optimization of IPC barrier energy},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral total-variation processing of shapes—theory and
applications. <em>TOG</em>, <em>43</em>(2), 22:1–20. (<a
href="https://doi.org/10.1145/3641845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive analysis of total variation (TV) on non-Euclidean domains and its eigenfunctions. We specifically address parameterized surfaces, a natural representation of the shapes used in 3D graphics. Our work sheds new light on the celebrated Beltrami and Anisotropic TV flows and explains experimental findings from recent years on shape spectral TV [Fumero et al. 2020 ] and adaptive anisotropic spectral TV [Biton and Gilboa 2022 ]. A new notion of convexity on surfaces is derived by characterizing structures that are stable throughout the TV flow, performed on surfaces. We establish and numerically demonstrate quantitative relationships between TV, area, eigenvalue, and eigenfunctions of the TV operator on surfaces. Moreover, we expand the shape spectral TV toolkit to include zero-homogeneous flows, leading to efficient and versatile shape processing methods. These methods are exemplified through applications in smoothing, enhancement, and exaggeration filters. We introduce a novel method that, for the first time, addresses the shape deformation task using TV. This deformation technique is characterized by the concentration of deformation along geometrical bottlenecks, shown to coincide with the discontinuities of eigenfunctions. Overall, our findings elucidate recent experimental observations in spectral TV, provide a diverse framework for shape filtering, and present the first TV-based approach to shape deformation.},
  archive      = {J_TOG},
  author       = {Jonathan Brokman and Martin Burger and Guy Gilboa},
  doi          = {10.1145/3641845},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {2},
  pages        = {22:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Spectral total-variation processing of Shapes—Theory and applications},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeadWood: Including disturbance and decay in the depiction
of digital nature. <em>TOG</em>, <em>43</em>(2), 21:1–19. (<a
href="https://doi.org/10.1145/3641816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation of truly believable simulated natural environments remains an unsolved problem in Computer Graphics. This is, in part, due to a lack of visual variety. In nature, apart from variation due to abiotic and biotic growth factors, a significant role is played by disturbance events, such as fires, windstorms, disease, and death and decay processes, which give rise to both standing dead trees (snags) and downed woody debris (logs). For instance, snags constitute on average 10% of unmanaged forests by basal area, and logs account for 2 \(\frac{1}{2}\) times this quantity. While previous systems have incorporated individual elements of disturbance (e.g., forest fires) and decay (e.g., the formation of humus), there has been no unifying treatment, perhaps because of the challenge of matching simulation results with generated geometric models. In this paper, we present a framework that combines an ecosystem simulation, which explicitly incorporates disturbance events and decay processes, with a model realization process, which balances the uniqueness arising from life history with the need for instancing due to memory constraints. We tested our hypothesis concerning the visual impact of disturbance and decay with a two-alternative forced-choice experiment ( n = 116). Our findings are that the presence of dead wood in various forms, as snags or logs, significantly improves the believability of natural scenes, while, surprisingly, general variation in the number of model instances, with up to 8 models per species, and a focus on disturbance events, does not.},
  archive      = {J_TOG},
  author       = {Adrien Peytavie and James Gain and Eric Guérin and Oscar Argudo and Eric Galin},
  doi          = {10.1145/3641816},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {2},
  pages        = {21:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeadWood: Including disturbance and decay in the depiction of digital nature},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeuralVDB: High-resolution sparse volume representation
using hierarchical neural networks. <em>TOG</em>, <em>43</em>(2),
20:1–21. (<a href="https://doi.org/10.1145/3641817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce NeuralVDB, which improves on an existing industry standard for efficient storage of sparse volumetric data, denoted VDB [Museth 2013 ], by leveraging recent advancements in machine learning. Our novel hybrid data structure can reduce the memory footprints of VDB volumes by orders of magnitude, while maintaining its flexibility and only incurring small (user-controlled) compression errors. Specifically, NeuralVDB replaces the lower nodes of a shallow and wide VDB tree structure with multiple hierarchical neural networks that separately encode topology and value information by means of neural classifiers and regressors respectively. This approach is proven to maximize the compression ratio while maintaining the spatial adaptivity offered by the higher-level VDB data structure. For sparse signed distance fields and density volumes, we have observed compression ratios on the order of 10× to more than 100× from already compressed VDB inputs, with little to no visual artifacts. Furthermore, NeuralVDB is shown to offer more effective compression performance compared to other neural representations such as Neural Geometric Level of Detail [Takikawa et al. 2021 ], Variable Bitrate Neural Fields [Takikawa et al. 2022a ], and Instant Neural Graphics Primitives [Müller et al. 2022 ]. Finally, we demonstrate how warm-starting from previous frames can accelerate training, i.e., compression, of animated volumes as well as improve temporal coherency of model inference, i.e., decompression.},
  archive      = {J_TOG},
  author       = {Doyub Kim and Minjae Lee and Ken Museth},
  doi          = {10.1145/3641817},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {2},
  pages        = {20:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeuralVDB: High-resolution sparse volume representation using hierarchical neural networks},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified MPM framework supporting phase-field models and
elastic-viscoplastic phase transition. <em>TOG</em>, <em>43</em>(2),
19:1–19. (<a href="https://doi.org/10.1145/3638047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the rapid deployment of numerous physics-based modeling and simulation algorithms and techniques for fluids, solids, and their delicate coupling in computer animation. However, it still remains a challenging problem to model the complex elastic-viscoplastic behaviors during fluid–solid phase transitions and facilitate their seamless interactions inside the same framework. In this article, we propose a practical method capable of simulating granular flows, viscoplastic liquids, elastic-plastic solids, rigid bodies, and interacting with each other, to support novel phenomena all heavily involving realistic phase transitions, including dissolution, melting, cooling, expansion, shrinking, and so on. At the physics level, we propose to combine and morph von Mises with Drucker–Prager and Cam–Clay yield models to establish a unified phase-field-driven EVP model, capable of describing the behaviors of granular, elastic, plastic, viscous materials, liquid, non-Newtonian fluids, and their smooth evolution. At the numerical level, we derive the discretization form of Cahn–Hilliard and Allen–Cahn equations with the material point method to effectively track the phase-field evolution, so as to avoid explicit handling of the boundary conditions at the interface. At the application level, we design a novel heuristic strategy to control specialized behaviors via user-defined schemes, including chemical potential, density curve, and so on. We exhibit a set of numerous experimental results consisting of challenging scenarios to validate the effectiveness and versatility of the new unified approach. This flexible and highly stable framework, founded upon the unified treatment and seamless coupling among various phases, and effective numerical discretization, has its unique advantage in animation creation toward novel phenomena heavily involving phase transitions with artistic creativity and guidance.},
  archive      = {J_TOG},
  author       = {Zaili Tu and Chen Li and Zipeng Zhao and Long Liu and Chenhui Wang and Changbo Wang and Hong Qin},
  doi          = {10.1145/3638047},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {19:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {A unified MPM framework supporting phase-field models and elastic-viscoplastic phase transition},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Creation of dihedral escher-like tilings based on
as-rigid-as-possible deformation. <em>TOG</em>, <em>43</em>(2), 18:1–18.
(<a href="https://doi.org/10.1145/3638048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An Escher-like tiling is a tiling consisting of one or a few artistic shapes of tile. This article proposes a method for generating Escher-like tilings consisting of two distinct shapes (dihedral Escher-like tilings) that are as similar as possible to the two goal shapes specified by the user. This study is an extension of a previous study that successfully generated Escher-like tilings consisting of a single tile shape for a single goal shape. Building upon the previous study, our method attempts to exhaustively search for which parts of the discretized tile contours are adjacent to each other to form a tiling. For each configuration, two tile shapes are optimized to be similar to the given two goal shapes. By evaluating the similarity based on as-rigid-as possible deformation energy, the optimized tile shapes preserve the local structures of the goal shapes, even if substantial deformations are necessary to form a tiling. However, in the dihedral case, this approach is seemingly unrealistic because it suffers from the complexity of the energy function and the combinatorial explosion of the possible configurations. We developed a method to address these issues and show that the proposed algorithms can generate satisfactory dihedral Escher-like tilings in a realistic computation time, even for somewhat complex goal shapes.},
  archive      = {J_TOG},
  author       = {Yuichi Nagata and Shinji Imahori},
  doi          = {10.1145/3638048},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {18:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Creation of dihedral escher-like tilings based on as-rigid-as-possible deformation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Promptable game models: Text-guided game simulation via
masked diffusion models. <em>TOG</em>, <em>43</em>(2), 17:1–16. (<a
href="https://doi.org/10.1145/3635705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural video game simulators emerged as powerful tools to generate and edit videos. Their idea is to represent games as the evolution of an environment’s state driven by the actions of its agents. While such a paradigm enables users to play a game action-by-action, its rigidity precludes more semantic forms of control. To overcome this limitation, we augment game models with prompts specified as a set of natural language actions and desired states . The result—a Promptable Game Model (PGM)—makes it possible for a user to play the game by prompting it with high- and low-level action sequences. Most captivatingly, our PGM unlocks the director’s mode , where the game is played by specifying goals for the agents in the form of a prompt. This requires learning “game AI,” encapsulated by our animation model, to navigate the scene using high-level constraints, play against an adversary, and devise a strategy to win a point. To render the resulting state, we use a compositional NeRF representation encapsulated in our synthesis model. To foster future research, we present newly collected, annotated and calibrated Tennis and Minecraft datasets. Our method significantly outperforms existing neural video game simulators in terms of rendering quality and unlocks applications beyond the capabilities of the current state-of-the-art. Our framework, data, and models are available at snap-research.github.io/promptable-game-models.},
  archive      = {J_TOG},
  author       = {Willi Menapace and Aliaksandr Siarohin and Stéphane Lathuilière and Panos Achlioptas and Vladislav Golyanik and Sergey Tulyakov and Elisa Ricci},
  doi          = {10.1145/3635705},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {17:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Promptable game models: Text-guided game simulation via masked diffusion models},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural wavelet-domain diffusion for 3D shape generation,
inversion, and manipulation. <em>TOG</em>, <em>43</em>(2), 16:1–18. (<a
href="https://doi.org/10.1145/3635304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new approach for 3D shape generation, inversion, and manipulation, through a direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets. Then, we design a pair of neural networks: a diffusion-based generator to produce diverse shapes in the form of the coarse coefficient volumes and a detail predictor to produce compatible detail coefficient volumes for introducing fine structures and details. Further, we may jointly train an encoder network to learn a latent space for inverting shapes, allowing us to enable a rich variety of whole-shape and region-aware shape manipulations. Both quantitative and qualitative experimental results manifest the compelling shape generation, inversion, and manipulation capabilities of our approach over the state-of-the-art methods.},
  archive      = {J_TOG},
  author       = {Jingyu Hu and Ka-Hei Hui and Zhengzhe Liu and Ruihui Li and Chi-Wing Fu},
  doi          = {10.1145/3635304},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {16:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural wavelet-domain diffusion for 3D shape generation, inversion, and manipulation},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Haisor: Human-aware indoor scene optimization via deep
reinforcement learning. <em>TOG</em>, <em>43</em>(2), 15:1–17. (<a
href="https://doi.org/10.1145/3632947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D scene synthesis facilitates and benefits many real-world applications. Most scene generators focus on making indoor scenes plausible via learning from training data and leveraging extra constraints such as adjacency and symmetry. Although the generated 3D scenes are mostly plausible with visually realistic layouts, they can be functionally unsuitable for human users to navigate and interact with furniture. Our key observation is that human activity plays a critical role and sufficient free space is essential for human-scene interactions. This is exactly where many existing synthesized scenes fail—the seemingly correct layouts are often not fit for living. To tackle this, we present a human-aware optimization framework Haisor for 3D indoor scene arrangement via reinforcement learning, which aims to find an action sequence to optimize the indoor scene layout automatically. Based on the hierarchical scene graph representation, an optimal action sequence is predicted and performed via Deep Q-Learning with Monte Carlo Tree Search (MCTS), where MCTS is our key feature to search for the optimal solution in long-term sequences and large action space. Multiple human-aware rewards are designed as our core criteria of human-scene interaction, aiming to identify the next smart action by leveraging powerful reinforcement learning. Our framework is optimized end-to-end by giving the indoor scenes with part-level furniture layout including part mobility information. Furthermore, our methodology is extensible and allows utilizing different reward designs to achieve personalized indoor scene synthesis. Extensive experiments demonstrate that our approach optimizes the layout of 3D indoor scenes in a human-aware manner, which is more realistic and plausible than original state-of-the-art generator results, and our approach produces superior smart actions, outperforming alternative baselines.},
  archive      = {J_TOG},
  author       = {Jia-Mu Sun and Jie Yang and Kaichun Mo and Yu-Kun Lai and Leonidas Guibas and Lin Gao},
  doi          = {10.1145/3632947},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {15:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Haisor: Human-aware indoor scene optimization via deep reinforcement learning},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital three-dimensional smocking design. <em>TOG</em>,
<em>43</em>(2), 14:1–17. (<a
href="https://doi.org/10.1145/3631945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an optimization-based method to model smocking , a surface embroidery technique that provides decorative geometric texturing while maintaining stretch properties of the fabric. During smocking, multiple pairs of points on the fabric are stitched together, creating non-manifold geometric features and visually pleasing textures. Designing smocking patterns is challenging, because the outcome of stitching is unpredictable: The final texture is often revealed only when the whole smocking process is completed, necessitating painstaking physical fabrication and time consuming trial-and-error experimentation. This motivates us to seek a digital smocking design method. Straightforward attempts to compute smocked fabric geometry using surface deformation or cloth simulation methods fail to produce realistic results, likely due to the intricate structure of the designs, the large number of contacts and high-curvature folds. We instead formulate smocking as a graph embedding and shape deformation problem. We extract a coarse graph representing the fabric and the stitching constraints and then derive the graph structure of the smocked result. We solve for the three-dimensional embedding of this graph, which in turn reliably guides the deformation of the high-resolution fabric mesh. Our optimization based method is simple, efficient, and flexible, which allows us to build an interactive system for smocking pattern exploration. To demonstrate the accuracy of our method, we compare our results to real fabrications on a large set of smocking patterns.},
  archive      = {J_TOG},
  author       = {Jing Ren and Aviv Segall and Olga Sorkine-Hornung},
  doi          = {10.1145/3631945},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {14:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Digital three-dimensional smocking design},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decorrelating ReSTIR samplers via MCMC mutations.
<em>TOG</em>, <em>43</em>(1), 10:1–15. (<a
href="https://doi.org/10.1145/3629166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte Carlo rendering algorithms often utilize correlations between pixels to improve efficiency and enhance image quality. For real-time applications in particular, repeated reservoir resampling offers a powerful framework to reuse samples both spatially in an image and temporally across multiple frames. While such techniques achieve equal-error up to 100× faster for real-time direct lighting [Bitterli et al. 2020 ] and global illumination [Ouyang et al. 2021 ; Lin et al. 2021 ], they are still far from optimal. For instance, spatiotemporal resampling often introduces noticeable correlation artifacts, while reservoirs holding more than one sample suffer from impoverishment in the form of duplicate samples. We demonstrate how interleaving Markov Chain Monte Carlo (MCMC) mutations with reservoir resampling helps alleviate these issues, especially in scenes with glossy materials and difficult-to-sample lighting. Moreover, our approach does not introduce any bias, and in practice, we find considerable improvement in image quality with just a single mutation per reservoir sample in each frame.},
  archive      = {J_TOG},
  author       = {Rohan Sawhney and Daqi Lin and Markus Kettunen and Benedikt Bitterli and Ravi Ramamoorthi and Chris Wyman and Matt Pharr},
  doi          = {10.1145/3629166},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {1},
  pages        = {10:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Decorrelating ReSTIR samplers via MCMC mutations},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In the quest for scale-optimal mappings. <em>TOG</em>,
<em>43</em>(1), 8:1–16. (<a
href="https://doi.org/10.1145/3627102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal mapping is one of the longest-standing problems in computational mathematics. It is natural to measure the relative curve length error under map to assess its quality. The maximum of such error is called the quasi-isometry constant, and its minimization is a nontrivial max-norm optimization problem. We present a physics-based quasi-isometric stiffening (QIS) algorithm for the max-norm minimization of hyperelastic distortion. QIS perfectly equidistributes distortion over the entire domain for the ground-truth test (unit hemisphere flattening) and, when it is not possible, tends to create zones where all cells have the same distortion. Such zones correspond to fragments of elastic material that became rigid under stiffening, reaching the deformation limit. As such, maps built by QIS are related to the de Boor equidistribution principle, which asks for an integral of a certain error indicator function to be the same over each mesh cell. Under certain assumptions on the minimization toolbox, we prove that our method can build, in a finite number of steps, a deformation whose maximum distortion is arbitrarily close to the (unknown) minimum. We performed extensive testing: on more than 10,000 domains QIS was reliably better than the competing methods. In summary, we reliably build 2D and 3D mesh deformations with the smallest known distortion estimates for very stiff problems.},
  archive      = {J_TOG},
  author       = {Vladimir Garanzha and Igor Kaporin and Liudmila Kudryavtseva and Francois Protais and Dmitry Sokolov},
  doi          = {10.1145/3627102},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {1},
  pages        = {8:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {In the quest for scale-optimal mappings},
  volume       = {43},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
