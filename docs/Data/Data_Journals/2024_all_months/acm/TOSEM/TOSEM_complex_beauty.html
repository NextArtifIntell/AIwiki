<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOSEM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tosem---184">TOSEM - 184</h2>
<ul>
<li><details>
<summary>
(2024). DiPri: Distance-based seed prioritization for greybox
fuzzing. <em>TOSEM</em>, <em>34</em>(1), 1–39. (<a
href="https://doi.org/10.1145/3654440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Greybox fuzzing is a powerful testing technique. Given a set of initial seeds, greybox fuzzing continuously generates new test inputs to execute the program under test and drives executions with code coverage as feedback. Seed prioritization is an important step of greybox fuzzing that helps greybox fuzzing choose promising seeds for input generation in priority. However, mainstream greybox fuzzers like AFL++ and Zest tend to neglect the importance of seed prioritization. They may pick seeds plainly according to the sequential order of the seeds being queued or an order produced with a random-based approach, which may consequently degrade their performance in exploring code and exposing bugs. In the meantime, existing state-of-the-art techniques like Alphuzz and K-Scheduler adopt complex strategies to schedule seeds. Although powerful, such strategies also inevitably incur great overhead and will reduce the scalability of the proposed technique. In this article, we propose a novel distance-based seed prioritization approach named DiPri to facilitate greybox fuzzing. Specifically, DiPri evaluates the queued seeds according to seed distances and chooses the outlier ones, which are the farthest from the others, in priority to improve the probabilities of discovering previously unexplored code regions. To make a profound evaluation of DiPri , we prototype DiPri on AFL++ and conduct large-scale experiments with four baselines and 24 C/C++ fuzz targets, where eight are from widely adopted real-world projects, eight are from the coverage-based benchmark FuzzBench, and eight are from the bug-based benchmark Magma. The results obtained through a fuzzing exceeding 50,000 CPU hours suggest that DiPri can (1) insignificantly influence the host fuzzer’s capability of code coverage by slightly improving the branch coverage on the eight targets from real-world projects and slightly reducing the branch coverage on the eight targets from FuzzBench, and (2) improve the host fuzzer’s capability of finding bugs by triggering five more Magma bugs. Besides the evaluation with the three C/C++ benchmarks, we integrate DiPri into the Java fuzzer Zest and conduct experiments on a Java benchmark composed of five real-world programs for more than 8,000 CPU hours to empirically study the scalability of DiPri . The results with the Java benchmark demonstrate that DiPri is pretty scalable and can help the host fuzzer find bugs more consistently.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3654440},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {DiPri: Distance-based seed prioritization for greybox fuzzing},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On process discovery experimentation: Addressing the need
for research methodology in process discovery. <em>TOSEM</em>,
<em>34</em>(1), 1–29. (<a
href="https://doi.org/10.1145/3672447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining aims to derive insights into business processes from event logs recorded from information systems. Process discovery algorithms construct process models that describe the executed process. With the increasing availability of large-scale event logs, process discovery has shifted towards a data-oriented research discipline, aiming to design algorithms that are applicable and useful in practice. This shift has revealed a fundamental problem in process discovery research: Currently, contributions can only be considered in isolation. Researchers conduct experiments to show that they move the field forward, but due to a lack of reliability and validity, the individual contributions are hard to generalize. In this article, we argue that one reason for these problems is the lack of conventions or standards for experimental design in process discovery. Hence, we propose “process discovery engineering”: a research methodology for process discovery, consisting of a shared terminology and a checklist for conducting experiments. We demonstrate its applicability by means of an example experimental evaluation of process discovery algorithms and discuss the implications of the methodology on the field. This article is not meant to be prescriptive but to invite and encourage the community to contribute to this discussion to advance the field as a whole.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672447},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On process discovery experimentation: Addressing the need for research methodology in process discovery},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding test convention consistency as a dimension of
test quality. <em>TOSEM</em>, <em>34</em>(1), 1–39. (<a
href="https://doi.org/10.1145/3672448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unit tests must be readable to help developers understand and evolve production code. Most existing test quality metrics assess test code’s ability to detect bugs. Few metrics focus on test code’s readability. One standard approach to improve readability is the consistent application of conventions. We investigated test convention consistency as a dimension of test quality. We formalized test suite consistency as the extent to which alternatives are used within a code base and introduce two complementary metrics to capture this extent. We elaborated a catalog of over 30 test conventions for the Java language organized in 10 convention classes that group mutual alternatives. We developed tool support to detect occurrences of conventions, compute consistency metrics over a test suite, and view occurrences of conventions in the corresponding code. We applied our tools to study the consistency of the test suites of 20 large open source Java projects. The study validates the design of the test convention classes, provides descriptive statistics on the range of consistency values for 10 different convention classes, and enables us to link observed changes in consistency values to specific events in the change history of our target systems, thus providing evidence of the construct validity of the metrics. We conclude that analyzing test suite consistency via static analysis shows promise as a practical approach to help improve test suite quality.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672448},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Understanding test convention consistency as a dimension of test quality},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Studying the impact of TensorFlow and PyTorch bindings on
machine learning software quality. <em>TOSEM</em>, <em>34</em>(1), 1–31.
(<a href="https://doi.org/10.1145/3678168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bindings for machine learning frameworks (such as TensorFlow and PyTorch) allow developers to integrate a framework’s functionality using a programming language different from the framework’s default language (usually Python). In this article, we study the impact of using TensorFlow and PyTorch bindings in C#, Rust, Python and JavaScript on the software quality in terms of correctness (training and test accuracy) and time cost (training and inference time) when training and performing inference on five widely used deep learning models. Our experiments show that a model can be trained in one binding and used for inference in another binding for the same framework without losing accuracy. Our study is the first to show that using a non-default binding can help improve machine learning software quality from the time cost perspective compared to the default Python binding while still achieving the same level of correctness.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3678168},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Studying the impact of TensorFlow and PyTorch bindings on machine learning software quality},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MarMot: Metamorphic runtime monitoring of autonomous driving
systems. <em>TOSEM</em>, <em>34</em>(1), 1–35. (<a
href="https://doi.org/10.1145/3678171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving systems (ADSs) are complex cyber-physical systems (CPSs) that must ensure safety even in uncertain conditions. Modern ADSs often employ deep neural networks (DNNs), which may not produce correct results in every possible driving scenario. Thus, an approach to estimate the confidence of an ADS at runtime is necessary to prevent potentially dangerous situations. In this article we propose MarMot , an online monitoring approach for ADSs based on metamorphic relations (MRs), which are properties of a system that hold among multiple inputs and the corresponding outputs. Using domain-specific MRs, MarMot estimates the uncertainty of the ADS at runtime, allowing the identification of anomalous situations that are likely to cause a faulty behavior of the ADS, such as driving off the road. We perform an empirical assessment of MarMot with five different MRs, using two different subject ADSs, including a small-scale physical ADS and a simulated ADS. Our evaluation encompasses the identification of both external anomalies, e.g., fog, as well as internal anomalies, e.g., faulty DNNs due to mislabeled training data. Our results show that MarMot can identify up to 65% of the external anomalies and 100% of the internal anomalies in the physical ADS, and up to 54% of the external anomalies and 88% of the internal anomalies in the simulated ADS. With these results, MarMot outperforms or is comparable to other state-of-the-art approaches, including SelfOracle, Ensemble, and MC Dropout-based ADS monitors.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3678171},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {MarMot: Metamorphic runtime monitoring of autonomous driving systems},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An empirical study of testing machine learning in the wild.
<em>TOSEM</em>, <em>34</em>(1), 1–63. (<a
href="https://doi.org/10.1145/3680463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background : Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Traditionally, software systems were constructed deductively, by writing explicit rules that govern the behavior of the system as program code. However, ML/DL systems infer rules from training data i.e., they are generated inductively. Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies. Aims : To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow. Method : We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems. Results : Our findings reveal several key insights: (1) The most common testing strategies, accounting for less than 40%, are Grey-box and White-box methods, such as Negative Testing , Oracle Approximation , and Statistical Testing . (2) A wide range of \(17\) ML properties are tested, out of which only 20% to 30% are frequently tested, including Consistency , Correctness , and Efficiency . (3) Bias and Fairness is more tested in Recommendation (6%) and Computer Vision (CV) (3.9%) systems, while Security and Privacy is tested in CV (2%), Application Platforms (0.9%), and NLP (0.5%). (4) We identified 13 types of testing methods, such as Unit Testing , Input Testing , and Model Testing . Conclusions : This study sheds light on the current adoption of software testing techniques and highlights gaps and limitations in existing ML testing practices.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680463},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-63},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study of testing machine learning in the wild},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware fuzzing for robustness enhancement of deep
learning models. <em>TOSEM</em>, <em>34</em>(1), 1–68. (<a
href="https://doi.org/10.1145/3680464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the testing-retraining pipeline for enhancing the robustness property of deep learning (DL) models, many state-of-the-art robustness-oriented fuzzing techniques are metric-oriented. The pipeline generates adversarial examples as test cases via such a DL testing technique and retrains the DL model under test with test suites that contain these test cases. On the one hand, the strategies of these fuzzing techniques tightly integrate the key characteristics of their testing metrics. On the other hand, they are often unaware of whether their generated test cases are different from the samples surrounding these test cases and whether there are relevant test cases of other seeds when generating the current one. We propose a novel testing metric called Contextual Confidence (CC). CC measures a test case through the surrounding samples of a test case in terms of their mean probability predicted to the prediction label of the test case. Based on this metric, we further propose a novel fuzzing technique Clover as a DL testing technique for the pipeline. In each fuzzing round, Clover first finds a set of seeds whose labels are the same as the label of the seed under fuzzing. At the same time, it locates the corresponding test case that achieves the highest CC values among the existing test cases of each seed in this set of seeds and shares the same prediction label as the existing test case of the seed under fuzzing that achieves the highest CC value. Clover computes the piece of difference between each such pair of a seed and a test case. It incrementally applies these pieces of differences to perturb the current test case of the seed under fuzzing that achieves the highest CC value and to perturb the resulting samples along the gradient to generate new test cases for the seed under fuzzing. Clover finally selects test cases among the generated test cases of all seeds as much as possible and with a preference to select test cases with higher CC values for improving model robustness. The experiments show that Clover outperforms the state-of-the-art coverage-based technique Adapt and loss-based fuzzing technique RobOT by 67%–129% and 48%–100% in terms of robustness improvement ratio, respectively, delivered through the same testing-retraining pipeline. For test case generation, in terms of numbers of unique adversarial labels and unique categories for the constructed test suites, Clover outperforms Adapt by \(2.0\times\) and \(3.5\times\) and RobOT by \(1.6\times\) and \(1.7\times\) on fuzzing clean models, and also outperforms Adapt by \(3.4\times\) and \(4.5\times\) and RobOT by \(9.8\times\) and \(11.0\times\) on fuzzing adversarially trained models, respectively.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680464},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-68},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Context-aware fuzzing for robustness enhancement of deep learning models},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Test case minimization with quantum annealers.
<em>TOSEM</em>, <em>34</em>(1), 1–24. (<a
href="https://doi.org/10.1145/3680467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum annealers are specialized quantum computers for solving combinatorial optimization problems with special quantum computing characteristics, e.g., superposition and entanglement. Theoretically, quantum annealers can outperform classic computers. However, current quantum annealers are constrained by a limited number of qubits and cannot demonstrate quantum advantages. Nonetheless, research is needed to develop novel mechanisms to formulate combinatorial optimization problems for quantum annealing (QA). However, QA applications in software engineering remain unexplored. Thus, we propose BootQA , the very first effort at solving test case minimization (TCM) problems on classical software with QA. We provide a novel TCM formulation for QA and utilize bootstrap sampling to optimize the qubit usage. We also implemented our TCM formulation in three other optimization processes: simulated annealing (SA), QA without problem decomposition, and QA with an existing D-Wave problem decomposition strategy, and conducted an empirical evaluation with three real-world TCM datasets. Results show that BootQA outperforms QA without problem decomposition and QA with the existing decomposition strategy regarding effectiveness. Moreover, BootQA ’s effectiveness is similar to SA. Finally, BootQA has higher efficiency in terms of time when solving large TCM problems than the other three optimization processes.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680467},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Test case minimization with quantum annealers},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring and mining community evolution in developer social
networks with entropy-based indices. <em>TOSEM</em>, <em>34</em>(1),
1–43. (<a href="https://doi.org/10.1145/3688832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents four novel entropy-based indices for measuring the community evolution of developer social networks (DSNs) in open source software (OSS) projects. The proposed indices offer a quantitative measure of community split, shrink, merge, and expand events. The indices have proven properties like monotonicity, and they have defined maximum and minimum values that signify meaningful scenarios. These indices can be combined to describe complex community evolution events such as emergence and extinction. Expanding upon these indices, this research proposes a novel machine learning approach, leveraging shapelet mining, to unearth representative patterns of community evolution. The results from real-world OSS projects show that these indices effectively capture various community evolution behaviors with a 94.1% accuracy compared to existing work. They also predict OSS team productivity with a 0.718 accuracy. With the shapelet mining and learning framework, the indices can identify patterns of community evolution and predict the survival of OSS projects with 93% accuracy 3 months before the projects’ last observed commits. The findings highlight the potential of these entropy-based indices for understanding OSS project status and predicting future trends, which are valuable for supporting future research on DSNs and OSS communities.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3688832},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Measuring and mining community evolution in developer social networks with entropy-based indices},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MalSensor: Fast and robust windows malware classification.
<em>TOSEM</em>, <em>34</em>(1), 1–28. (<a
href="https://doi.org/10.1145/3688833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the substantial profits, the evolution of Portable Executable (PE) malware has posed persistent threats. PE malware classification has been an important research field, and numerous classification methods have been proposed. With the development of machine learning, learning-based static classification methods achieve excellent performance. However, most existing methods cannot meet the requirements of industrial applications due to the limited resource consumption and concept drift. In this article, we propose a fast, high-accuracy, and robust FCG-based PE malware classification method. We first extract precise function call relationships through code and data cross-referencing analysis. Then we normalize function names to construct a concise and accurate function call graph. Furthermore, we perform topological analysis of the function call graph using social network analysis techniques, thereby enhancing the program function call features. Finally, we use a series of machine learning algorithms for classification. We implement a prototype system named MalSensor and compare it with nine state-of-the-art static PE malware classification methods. The experimental results show that MalSensor is capable of classifying a malicious file in 0.7 seconds on average with up to 98.35% accuracy, which represents a significant advantage over existing methods.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3688833},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {MalSensor: Fast and robust windows malware classification},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking and categorizing the performance of neural
program repair systems for java. <em>TOSEM</em>, <em>34</em>(1), 1–35.
(<a href="https://doi.org/10.1145/3688834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen a rise in Neural Program Repair (NPR) systems in the software engineering community, which adopt advanced deep learning techniques to automatically fix bugs. Having a comprehensive understanding of existing systems can facilitate new improvements in this area and provide practical instructions for users. However, we observe two potential weaknesses in the current evaluation of NPR systems: ① published systems are trained with varying data, and ② NPR systems are roughly evaluated through the number of totally fixed bugs. Questions such as what types of bugs are repairable for current systems cannot be answered yet. Consequently, researchers cannot make target improvements in this area and users have no idea of the real affair of existing systems. In this article, we perform a systematic evaluation of the existing nine state-of-the-art NPR systems. To perform a fair and detailed comparison, we (1) build a new benchmark and framework that supports training and validating the nine systems with unified data and (2) evaluate re-trained systems with detailed performance analysis, especially on the effectiveness and the efficiency. We believe our benchmark tool and evaluation results could offer practitioners the real affairs of current NPR systems and the implications of further facilitating the improvements of NPR.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3688834},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Benchmarking and categorizing the performance of neural program repair systems for java},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuron semantic-guided test generation for deep neural
networks fuzzing. <em>TOSEM</em>, <em>34</em>(1), 1–38. (<a
href="https://doi.org/10.1145/3688835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant progress has been made in testing methods for deep neural networks (DNNs) to ensure their correctness and robustness. Coverage-guided criteria, such as neuron-wise, layer-wise, and path-/trace-wise, have been proposed for DNN fuzzing. However, existing coverage-based criteria encounter performance bottlenecks for several reasons: ❶ Testing Adequacy : Partial neural coverage criteria have been observed to achieve full coverage using only a small number of test inputs. In this case, increasing the number of test inputs does not consistently improve the quality of models. ❷ Interpretability : The current coverage criteria lack interpretability. Consequently, testers are unable to identify and understand which incorrect attributes or patterns of the model are triggered by the test inputs. This lack of interpretability hampers the subsequent debugging and fixing process. Therefore, there is an urgent need for a novel fuzzing criterion that offers improved testing adequacy, better interpretability, and more effective failure detection capabilities for DNNs. To alleviate these limitations, we propose NSGen, an approach for DNN fuzzing that utilizes neuron semantics as guidance during test generation. NSGen identifies critical neurons, translates their high-level semantic features into natural language descriptions, and then assembles them into human-readable DNN decision paths (representing the internal decision of the DNN). With these decision paths, we can generate more fault-revealing test inputs by quantifying the similarity between original test inputs and mutated test inputs for fuzzing. We evaluate NSGen on popular DNN models (VGG16_BN, ResNet50, and MobileNet_v2) using CIFAR10, CIFAR100, Oxford 102 Flower, and ImageNet datasets. Compared to 12 existing coverage-guided fuzzing criteria, NSGen outperforms all baselines, increasing the number of triggered faults by 21.4% to 61.2% compared to the state-of-the-art coverage-guided fuzzing criterion. This demonstrates NSGen&#39;s effectiveness in generating fault-revealing test inputs through guided input mutation, highlighting its potential to enhance DNN testing and interpretability.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3688835},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Neuron semantic-guided test generation for deep neural networks fuzzing},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving the t-wise coverage maximum problem via effective
and efficient local search-based sampling. <em>TOSEM</em>,
<em>34</em>(1), 1–64. (<a
href="https://doi.org/10.1145/3688836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the increasing demand for customized software, highly configurable systems become essential in practice. Such systems offer many options to configure, and ensuring the reliability of these systems is critical. A widely used evaluation metric for testing these systems is \(t\) -wise coverage, where \(t\) represents testing strength, and its value typically ranges from 2 to 6. It is crucial to design effective and efficient methods for generating test suites that achieve high \(t\) -wise coverage. However, current state-of-the-art methods need to generate large test suites for achieving high \(t\) -wise coverage. In this work, we propose a novel method called LS-Sampling-Plus that can efficiently generate test suites with high \(t\) -wise coverage for \(2\leq t\leq 6\) while being smaller in size compared to existing state-of-the-art methods. LS-Sampling-Plus incorporates many core algorithmic techniques, including two novel scoring functions, a dynamic mechanism for updating sampling probabilities, and a validity-guaranteed systematic search method. Our experiments on various practical benchmarks show that LS-Sampling-Plus can achieve higher \(t\) -wise coverage than current state-of-the-art methods, through building a test suite of the same size. Moreover, our evaluations indicate the effectiveness of all core algorithmic techniques of LS-Sampling-Plus . Furthermore, LS-Sampling-Plus exhibits better scalability and fault detection capability than existing state-of-the-art methods.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3688836},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-64},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Solving the t-wise coverage maximum problem via effective and efficient local search-based sampling},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CARL: Unsupervised code-based adversarial attacks for
programming language models via reinforcement learning. <em>TOSEM</em>,
<em>34</em>(1), 1–32. (<a
href="https://doi.org/10.1145/3688839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code based adversarial attacks play a crucial role in revealing vulnerabilities of software system. Recently, pre-trained programming language models (PLMs) have demonstrated remarkable success in various significant software engineering tasks, progressively transforming the paradigm of software development. Despite their impressive capabilities, these powerful models are vulnerable to adversarial attacks. Therefore, it is necessary to carefully investigate the robustness and vulnerabilities of the PLMs by means of adversarial attacks. Adversarial attacks entail imperceptible input modifications that cause target models to make incorrect predictions. Existing approaches for attacking PLMs often employ either identifier renaming or the greedy algorithm, which may yield sub-optimal performance or lead to high inference times. In response to these limitations, we propose CARL, an unsupervised black-box attack model that leverages reinforcement learning to generate imperceptible adversarial examples. Specifically, CARL comprises a programming language encoder and a perturbation prediction layer. In order to achieve more effective and efficient attack, we cast the task as a sequence decision-making process, optimizing through policy gradient with a suite of reward functions. We conduct extensive experiments to validate the effectiveness of CARL on code summarization, code translation, and code refinement tasks, covering various programming languages and PLMs. The experimental results demonstrate that CARL surpasses state-of-the-art code attack models, achieving the highest attack success rate across multiple tasks and PLMs while maintaining high attack efficiency, imperceptibility, consistency, and fluency.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3688839},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {CARL: Unsupervised code-based adversarial attacks for programming language models via reinforcement learning},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An exploratory study on machine learning model management.
<em>TOSEM</em>, <em>34</em>(1), 1–31. (<a
href="https://doi.org/10.1145/3688841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective model management is crucial for ensuring performance and reliability in Machine Learning (ML) systems, given the dynamic nature of data and operational environments. However, standard practices are lacking, often resulting in ad hoc approaches. To address this, our research provides a clear definition of ML model management activities, processes, and techniques. Analyzing 227 ML repositories, we propose a taxonomy of 16 model management activities and identify 12 unique challenges. We find that 57.9% of the identified activities belong to the maintenance category, with activities like refactoring (20.5%) and documentation (18.3%) dominating. Our findings also reveal significant challenges in documentation maintenance (15.3%) and bug management (14.9%), emphasizing the need for robust versioning tools and practices in the ML pipeline. Additionally, we conducted a survey that underscores a shift toward automation, particularly in data, model, and documentation versioning, as key to managing ML models effectively. Our contributions include a detailed taxonomy of model management activities, a mapping of challenges to these activities, practitioner-informed solutions for challenge mitigation, and a publicly available dataset of model management activities and challenges. This work aims to equip ML developers with knowledge and best practices essential for the robust management of ML models.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3688841},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An exploratory study on machine learning model management},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reputation gaming in crowd technical knowledge sharing.
<em>TOSEM</em>, <em>34</em>(1), 1–41. (<a
href="https://doi.org/10.1145/3691627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stack Overflow incentive system awards users with reputation scores to ensure quality. The decentralized nature of the forum may make the incentive system prone to manipulation. This article offers, for the first time, a comprehensive study of the reported types of reputation manipulation scenarios that might be exercised in Stack Overflow and the prevalence of such reputation gamers by a qualitative study of 1,697 posts from meta Stack Exchange sites. We found four different types of reputation fraud scenarios, such as voting rings where communities form to upvote each other repeatedly on similar posts. We developed algorithms that enable platform managers to automatically identify these suspicious reputation gaming scenarios for review. The first algorithm identifies isolated/semi-isolated communities where probable reputation frauds may occur mostly by collaborating with each other. The second algorithm looks for sudden unusual big jumps in the reputation scores of users. We evaluated the performance of our algorithms by examining the reputation history dashboard of Stack Overflow users from the Stack Overflow Web site. We observed that around 60–80% of users flagged as suspicious by our algorithms experienced reductions in their reputation scores by Stack Overflow.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3691627},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Reputation gaming in crowd technical knowledge sharing},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-tuning large language models to improve accuracy and
comprehensibility of automated code review. <em>TOSEM</em>,
<em>34</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3695993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As code review is a tedious and costly software quality practice, researchers have proposed several machine learning-based methods to automate the process. The primary focus has been on accuracy, that is, how accurately the algorithms are able to detect issues in the code under review. However, human intervention still remains inevitable since results produced by automated code review are not 100% correct. To assist human reviewers in making their final decisions on automatically generated review comments, the comprehensibility of the comments underpinned by accurate localization and relevant explanations for the detected issues with repair suggestions is paramount. However, this has largely been neglected in the existing research. Large language models (LLMs) have the potential to generate code review comments that are more readable and comprehensible by humans, thanks to their remarkable processing and reasoning capabilities. However, even mainstream LLMs perform poorly in detecting the presence of code issues because they have not been specifically trained for this binary classification task required in code review. In this article, we contribute Comprehensibility of Automated Code Review using Large Language Models ( Carllm ), a novel fine-tuned LLM that has the ability to improve not only the accuracy but, more importantly, the comprehensibility of automated code review, as compared to state-of-the-art pre-trained models and general LLMs.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3695993},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Fine-tuning large language models to improve accuracy and comprehensibility of automated code review},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diversity’s double-edged sword: Analyzing race’s effect on
remote pair programming interactions. <em>TOSEM</em>, <em>34</em>(1),
1–45. (<a href="https://doi.org/10.1145/3699601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote pair programming is widely used in software development, but no research has examined how race affects these interactions between developers. We embarked on this study due to the historical underrepresentation of Black developers in the tech industry, with White developers comprising the majority. Our study involved 24 experienced developers, forming 12 gender-balanced same- and mixed-race pairs. Pairs collaborated on a programming task using the think-aloud method, followed by individual retrospective interviews. Our findings revealed elevated productivity scores for mixed-race pairs, with no differences in code quality between same- and mixed-race pairs. Mixed-race pairs excelled in task distribution, shared decision-making, and role-exchange but encountered communication challenges, discomfort, and anxiety, shedding light on the complexity of diversity dynamics. Our study emphasizes race’s impact on remote pair programming and underscores the need for diverse tools and methods to address racial disparities for collaboration.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3699601},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-45},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Diversity’s double-edged sword: Analyzing race’s effect on remote pair programming interactions},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automating TODO-missed methods detection and patching.
<em>TOSEM</em>, <em>34</em>(1), 1–28. (<a
href="https://doi.org/10.1145/3700793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TODO comments are widely used by developers to remind themselves or others about incomplete tasks. In other words, TODO comments are usually associated with temporary or suboptimal solutions. In practice, all the equivalent suboptimal implementations should be updated (e.g., adding TODOs) simultaneously. However, due to various reasons (e.g., time constraints or carelessness), developers may forget or even are unaware of adding TODO comments to all necessary places, which results in the TODO-missed methods . These “hidden” suboptimal implementations in TODO-missed methods may hurt the software quality and maintainability in the long-term. Therefore, in this article, we propose the novel task of TODO-missed methods detection and patching and develop a novel model, namely T O D O-comment Patcher ( TDPatcher ), to automatically patch TODO comments to the TODO-missed methods in software projects. Our model has two main stages: offline learning and online inference. During the offline learning stage, TDPatcher employs the GraphCodeBERT and contrastive learning for encoding the TODO comment (natural language) and its suboptimal implementation (code fragment) into vector representations. For the online inference stage, we can identify the TODO-missed methods and further determine their patching position by leveraging the offline trained model. We built our dataset by collecting TODO-introduced methods from the top-10,000 Python GitHub repositories and evaluated TDPatcher on them. Extensive experimental results show the promising performance of our model over a set of benchmarks. We further conduct an in-the-wild evaluation that successfully detects 26 TODO-missed methods from 50 GitHub repositories.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3700793},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automating TODO-missed methods detection and patching},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep configuration performance learning: A systematic survey
and taxonomy. <em>TOSEM</em>, <em>34</em>(1), 1–62. (<a
href="https://doi.org/10.1145/3702986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance is arguably the most crucial attribute that reflects the quality of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. In this article, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 1,206 searched papers spanning six indexing services, based on which 99 primary papers were extracted and analyzed. Our results outline key statistics, taxonomy, strengths, weaknesses, and optimal usage scenarios for techniques related to the preparation of configuration data, the construction of deep learning performance models, the evaluation of these models, and their utilization in various software configuration-related tasks. We also identify the good practices and potentially problematic phenomena from the studies surveyed, together with a comprehensive summary of actionable suggestions and insights into future opportunities within the field. To promote open science, all the raw results of this survey can be accessed at our repository: https://github.com/ideas-labo/DCPL-SLR .},
  archive      = {J_TOSEM},
  doi          = {10.1145/3702986},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {1},
  pages        = {1-62},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Deep configuration performance learning: A systematic survey and taxonomy},
  volume       = {34},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GIST: Generated inputs sets transferability in deep
learning. <em>TOSEM</em>, <em>33</em>(8), 1–38. (<a
href="https://doi.org/10.1145/3672457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To foster the verifiability and testability of deep neural networks (DNN), an increasing number of methods for test case generation techniques are being developed. When confronted with testing DNN models, the user can apply any existing test generation technique. However, it needs to do so for each technique and each DNN model under test, which can be expensive. Therefore, a paradigm shift could benefit this testing process: rather than regenerating the test set independently for each DNN model under test, we could transfer from existing DNN models. This article introduces Generated Inputs Sets Transferability (GIST), a novel approach for the efficient transfer of test sets. Given a property selected by a user (e.g., neurons covered, faults), GIST enables the selection of good test sets from the point of view of this property among available test sets. This allows the user to recover similar properties on the transferred test sets as he would have obtained by generating the test set from scratch with a test cases generation technique. Experimental results show that GIST can select effective test sets for the given property to transfer. Moreover, GIST scales better than reapplying test case generation techniques from scratch on DNN models under test.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672457},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {GIST: Generated inputs sets transferability in deep learning},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cleaning up confounding: Accounting for endogeneity using
instrumental variables and two-stage models. <em>TOSEM</em>,
<em>33</em>(8), 1–31. (<a
href="https://doi.org/10.1145/3674730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies in empirical software engineering are often most useful if they make causal claims because this allows practitioners to identify how they can purposefully influence (rather than only predict) outcomes of interest. Unfortunately, many non-experimental studies suffer from potential endogeneity, for example, through omitted confounding variables, which precludes claims of causality. In this conceptual tutorial, we aim to transfer the proven solution of instrumental variables and two-stage models as a means to account for endogeneity from econometrics to the field of empirical software engineering. To this end, we discuss causality and causal inference, provide a definition of endogeneity, explain its causes, and lay out the conceptual idea behind instrumental variable approaches and two-stage models. We also provide an extensive illustration with simulated data and a brief illustration with real data to demonstrate the approach, offering Stata and R code to allow researchers to replicate our analyses and apply the techniques to their own research projects. We close with concrete recommendations and a guide for researchers on how to deal with endogeneity.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3674730},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Cleaning up confounding: Accounting for endogeneity using instrumental variables and two-stage models},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural solving uninterpreted predicates with abstract
gradient descent. <em>TOSEM</em>, <em>33</em>(8), 1–47. (<a
href="https://doi.org/10.1145/3675394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uninterpreted predicate solving is a fundamental problem in formal verification, including loop invariant and constrained horn clauses predicate solving. Existing approaches have been mostly in symbolic ways. While achieving sustainable progress, they still suffer from inefficiency and seem unable to leverage the ever-increasing computility, such as GPU. Recently, neural relaxation has been proposed to tackle this problem. They treat the uninterpreted predicate-solving task as an optimization problem by relaxing the discrete search process into a learning process of neural networks. However, two bottlenecks keep them from being valid. First, relaxed neural networks cannot match the original semantics of predicates rigorously; second, the neural networks are difficult to train to reach global optimization. Therefore, this article presents a novel discrete neural architecture with the Abstract Gradient Decent (AGD) algorithm to directly solve uninterpreted predicates in the discrete hypothesis space. The abstract gradient is for discrete neurons whose calculation rules are designed in an abstract domain. Our approach conforms to the original semantics of predicates, and the proposed AGD algorithm can achieve global optimization satisfactorily. We implement the tool Dasp in the Boxes abstract domain to solve uninterpreted predicates in the QF-NIA SMT theory. In the experiments, Dasp has outperformed seven state-of-the-art tools across three predicate synthesis tasks.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3675394},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-47},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Neural solving uninterpreted predicates with abstract gradient descent},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SURE: A visualized failure indexing approach using program
memory spectrum. <em>TOSEM</em>, <em>33</em>(8), 1–43. (<a
href="https://doi.org/10.1145/3676958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure indexing is a longstanding crux in software debugging, the goal of which is to automatically divide failures (e.g., failed test cases) into distinct groups according to the culprit root causes, as such multiple faults residing in a faulty program can be handled independently and simultaneously. The community of failure indexing has long been plagued by two challenges: (1) The effectiveness of division is still far from promising. Specifically, existing failure indexing techniques only employ a limited source of software runtime data, for example, code coverage, to be failure proximity and further divide them, which typically delivers unsatisfactory results. (2) The outcome can be hardly comprehensible. Specifically, a developer who receives the division result is just aware of how all failures are divided, without knowing why they should be divided the way they are. This leads to difficulties for developers to be convinced by the division result, which in turn affects the adoption of the results. To tackle these two problems, in this article, we propose SURE , a vi SU alized failu R e ind E xing approach using the program memory spectrum (PMS). We first collect the runtime memory information (i.e., variables’ names and values, as well as the depth of the stack frame) at several preset breakpoints during the execution of a failed test case, and transform the gathered memory information into a human-friendly image (called PMS). Then, any pair of PMS images that serve as proxies for two failures is fed to a trained Siamese convolutional neural network, to predict the likelihood of them being triggered by the same fault. Last, a clustering algorithm is adopted to divide all failures based on the mentioned likelihood. In the experiments, we use 30% of the simulated faults to train the neural network, and use 70% of the simulated faults as well as real-world faults to test. Results demonstrate the effectiveness of SURE: It achieves 101.20% and 41.38% improvements in faults number estimation, as well as 105.20% and 35.53% improvements in clustering, compared with the state-of-the-art technique in this field, in simulated and real-world environments, respectively. Moreover, we carry out a human study to quantitatively evaluate the comprehensibility of PMS, revealing that this novel type of representation can help developers better comprehend failure indexing results.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3676958},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {SURE: A visualized failure indexing approach using program memory spectrum},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning informed evolutionary search for
autonomous systems testing. <em>TOSEM</em>, <em>33</em>(8), 1–45. (<a
href="https://doi.org/10.1145/3680468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary search (ES)-based techniques are commonly used for testing autonomous robotic systems. However, these approaches often rely on computationally expensive simulator-based models for test scenario evaluation. To improve the computational efficiency of the search-based testing, we propose augmenting the ES with a reinforcement learning (RL) agent trained using surrogate rewards derived from domain knowledge. In our approach, known as RIGAA (Reinforcement learning Informed Genetic Algorithm for Autonomous systems testing), we first train an RL agent to learn useful constraints of the problem and then use it to produce a certain part of the initial population of the search algorithm. By incorporating an RL agent into the search process, we aim to guide the algorithm towards promising regions of the search space from the start, enabling more efficient exploration of the solution space. We evaluate RIGAA on two case studies: maze generation for an autonomous “Ant” robot and road topology generation for an autonomous vehicle lane-keeping assist system. In both case studies, RIGAA reveals more failures of a high level of diversity than the compared baselines. RIGAA also outperforms the state-of-the-art tools for vehicle lane-keeping assist system testing, such as AmbieGen, CRAG, WOGAN, and Frenetic in terms of the number of revealed failures in a two-hour budget.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680468},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-45},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Reinforcement learning informed evolutionary search for autonomous systems testing},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Let’s discover more API relations: A large language
model-based AI chain for unsupervised API relation inference.
<em>TOSEM</em>, <em>33</em>(8), 1–34. (<a
href="https://doi.org/10.1145/3680469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {APIs have intricate relations that can be described in text and represented as knowledge graphs to aid software engineering tasks. Existing relation extraction methods have limitations, such as limited API text corpus, and are affected by the characteristics of the input text. To address these limitations, we propose utilizing large language models (LLMs) (e.g., GPT-3.5) as a neural knowledge base for API relation inference. This approach leverages the entire Web used to pre-train LLMs as a knowledge base and is insensitive to the context and complexity of input texts. To ensure accurate inference, we design an AI chain consisting of three AI modules: API Fully Qualified Name (FQN) Parser, API Knowledge Extractor, and API Relation Decider. The accuracy of the API FQN Parser and API Relation Decider is 0.81 and 0.83, respectively. Using the generative capacity of the LLM and our approach’s inference capability, we achieve an average F1 value of 0.76 under the three datasets, significantly higher than the state-of-the-art method’s average F1 value of 0.40. Compared to the original CoT and modularized CoT methods, our AI chain design has improved the performance of API relation inference by 71% and 49%, respectively. Meanwhile, the prompt ensembling strategy enhances the performance of our approach by 32%. The API relations inferred by our method can be further organized into structured forms to provide support for other software engineering tasks.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680469},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Let’s discover more API relations: A large language model-based AI chain for unsupervised API relation inference},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing energy-awareness in deep learning through
fine-grained energy measurement. <em>TOSEM</em>, <em>33</em>(8), 1–34.
(<a href="https://doi.org/10.1145/3680470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing usage, scale, and complexity of Deep Learning ( dl ) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of dl systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at fine granularity (e.g., at the api level) hinders progress in this area. This paper introduces FECoM (Fine-grained Energy Consumption Meter) , a framework for fine-grained dl energy consumption measurement. FECoM enables researchers and developers to profile dl api s from energy perspective. FECoM addresses the challenges of fine-grained energy measurement using static instrumentation while considering factors such as computational load and temperature stability. We assess FECoM ’s capability for fine-grained energy measurement for one of the most popular open-source dl frameworks, namely TensorFlow . Using FECoM , we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow api s’ energy profiles. Furthermore, we elaborate on the considerations and challenges while designing and implementing a fine-grained energy measurement tool. This work will facilitate further advances in dl energy measurement and the development of energy-aware practices for dl systems.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680470},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Enhancing energy-awareness in deep learning through fine-grained energy measurement},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Navigating the complexity of generative AI adoption in
software engineering—RCR report. <em>TOSEM</em>, <em>33</em>(8), 1–5.
(<a href="https://doi.org/10.1145/3680471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This Replicated Computational Results (RCR) report complements the study “Navigating the Complexity of Generative AI Adoption in Software Engineering,” which examines the factors influencing the integration of AI tools in software engineering practices. Employing a mixed-methods approach grounded in the Technology Acceptance Model, Diffusion of Innovation Theory, and Social Cognitive Theory, the study introduces the Human-AI Collaboration and Adaptation Framework (HACAF), validated through PLS-SEM analysis. The replication package detailed herein includes survey instruments, raw data, and analysis scripts essential for reproducing the study&#39;s findings. By providing these artifacts, the RCR report aims to support transparency, enable replication, and encourage further research on effective AI tool adoption strategies in software engineering.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680471},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-5},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Navigating the complexity of generative AI adoption in software Engineering—RCR report},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MULTICR: Predicting merged and abandoned code changes in
modern code review using multi-objective search. <em>TOSEM</em>,
<em>33</em>(8), 1–44. (<a
href="https://doi.org/10.1145/3680472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern Code Review (MCR) is an essential process in software development to ensure high-quality code. However, developers often spend considerable time reviewing code changes before being merged into the main code base. Previous studies attempted to predict whether a code change was going to be merged or abandoned soon after it was submitted to improve the code review process. However, these approaches require complex cost-sensitive learning, which makes their adoption challenging since it is difficult for developers to understand the main factors behind the models’ predictions. To address this issue, we introduce in this article, MULTICR , a multi-objective search-based approach that uses Multi-Objective Genetic Programming (MOGP) to learn early code review prediction models as IF-THEN rules. MULTICR evolves predictive models while maximizing the accuracy of both merged and abandoned classes, eliminating the need for misclassification cost estimation. To evaluate MULTICR, we conducted an empirical study on 146,612 code reviews from Eclipse, LibreOffice, and Gerrithub. The obtained results show that MULTICR outperforms existing baselines in terms of Matthew Correlation Coefficient (MCC) and F1 scores while learning less complex models compared to decision trees. Our experiments also showed how MULTICR allows identifying the main factors related to abandoned code reviews as well as their associated thresholds, making it a promising approach for early code review prediction with notable performance and inter-operability. Additionally, we qualitatively evaluate MULTICR by conducting a user study through semi-structured interviews involving 10 practitioners from different organizations. The obtained results indicate that 90% of the participants find that MULTICR is useful and can help them to improve the code review process. Additionally, the learned IF-THEN rules of MULTICR are transparent.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680472},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {MULTICR: Predicting merged and abandoned code changes in modern code review using multi-objective search},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting attrition among software professionals:
Antecedents and consequences of burnout and engagement. <em>TOSEM</em>,
<em>33</em>(8), 1–45. (<a
href="https://doi.org/10.1145/3691629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study of burnout and engagement, we address three major themes. First, we offer a review of prior studies of burnout among IT professionals and link these studies to the Job Demands-Resources (JD-R) model. Informed by the JD-R model, we identify three factors that are organizational job resources and posit that these (a) increase engagement and (b) decrease burnout. Second, we extend the JD-R by considering software professionals’ intention to stay as a consequence of these two affective states, burnout and engagement. Third, we focus on the importance of factors for intention to stay, and actual retention behavior. We use a unique dataset of over 13,000 respondents at one global IT organization, enriched with employment status 90 days after the initial survey. Leveraging partial-least squares structural quation modeling and machine learning, we find that the data mostly support our theoretical model, with some variation across different subgroups of respondents. An importance-performance map analysis suggests that managers may wish to focus on interventions regarding burnout as a predictor of intention to leave. The Machine Learning model suggests that engagement and opportunities to learn are the top two most important factors that explain whether software professionals leave an organization.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3691629},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-45},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Predicting attrition among software professionals: Antecedents and consequences of burnout and engagement},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Termination and universal termination problems for
nondeterministic quantum programs. <em>TOSEM</em>, <em>33</em>(8), 1–41.
(<a href="https://doi.org/10.1145/3691632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifying quantum programs has attracted a lot of interest in recent years. In this article, we consider the following two categories of termination problems of quantum programs with nondeterminism, namely: (1) (termination) Is an input of a program terminating with probability one under all schedulers? If not, how can a scheduler be synthesized to evidence the nontermination? (2) (universal termination) Are all inputs terminating with probability one under their respective schedulers? If yes, a further question asks whether there is a scheduler that forces all inputs to be terminating with probability one together with how to synthesize it; otherwise, how can an input be provided to refute the universal termination? (termination) Is an input of a program terminating with probability one under all schedulers? If not, how can a scheduler be synthesized to evidence the nontermination? (universal termination) Are all inputs terminating with probability one under their respective schedulers? If yes, a further question asks whether there is a scheduler that forces all inputs to be terminating with probability one together with how to synthesize it; otherwise, how can an input be provided to refute the universal termination? For the effective verification of the first category, we over-approximate the reachable set of quantum program states by the reachable subspace, whose algebraic structure is a linear space. On the other hand, we study the set of divergent states from which the program terminates with probability zero under some scheduler. The divergent set also has an explicit algebraic structure. Exploiting these explicit algebraic structures, we address the decision problem by a necessary and sufficient condition, i.e., the disjointness of the reachable subspace and the divergent set. Furthermore, the scheduler synthesis is completed in exponential time, whose bottleneck lies in computing the divergent set reported for the first time. For the second category, we reduce the decision problem to the existence of an invariant subspace, from which the program terminates with probability zero under all schedulers. The invariant subspace is characterized by linear equations and thus can be efficiently computed. The states on that invariant subspace are evidence of the nontermination. Furthermore, the scheduler synthesis is completed by seeking a pattern of finite schedulers that forces all inputs to be terminating with positive probability. The repetition of that pattern yields the desired universal scheduler that forces all inputs to be terminating with probability one. All the problems in the second category are shown, also for the first time, to be solved in polynomial time. Finally, we demonstrate the aforementioned methods via a running example—the quantum Bernoulli factory protocol.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3691632},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Termination and universal termination problems for nondeterministic quantum programs},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language models for software engineering: A systematic
literature review. <em>TOSEM</em>, <em>33</em>(8), 1–79. (<a
href="https://doi.org/10.1145/3695988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at https://github.com/security-pride/LLM4SE_SLR .},
  archive      = {J_TOSEM},
  doi          = {10.1145/3695988},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {12},
  number       = {8},
  pages        = {1-79},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Large language models for software engineering: A systematic literature review},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MET-MAPF: A metamorphic testing approach for multi-agent
path finding algorithms. <em>TOSEM</em>, <em>33</em>(8), 1–37. (<a
href="https://doi.org/10.1145/3669663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multi-Agent Path Finding (MAPF) problem, i.e., the scheduling of multiple agents to reach their destinations, has been widely investigated. Testing MAPF systems is challenging, due to the complexity and variety of scenarios and the agents’ distribution and interaction. Moreover, MAPF testing suffers from the oracle problem, i.e., it is not always clear whether a test shows a failure or not. Indeed, only considering whether the agents reach their destinations without collision is not sufficient. Other properties related to the ‘quality’ of the generated paths should be assessed, e.g., an agent should not follow an unnecessarily long path. To tackle this issue, this article proposes MET-MAPF, a Metamorphic Testing approach for MAPF systems. We identified 10 Metamorphic Relations (MRs) that a MAPF system should guarantee, designed over the environment in which agents operate, the behaviour of the single agents and the interactions among agents. Starting from the different MRs, MET-MAPF automatically generates test cases addressing them, so possibly exposing different types of failures. Experimental results show that MET-MAPF can indeed find MR violations not exposed by approaches that only consider the completion of the mission as test oracle. Moreover, experiments show that different MRs expose different types of violations.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3669663},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {MET-MAPF: A metamorphic testing approach for multi-agent path finding algorithms},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient management of containers for software defined
vehicles. <em>TOSEM</em>, <em>33</em>(8), 1–36. (<a
href="https://doi.org/10.1145/3672461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containerization technology, such as Docker, is gaining in popularity in newly established software-defined vehicle architectures (SDVA). However, executing those containers can quickly become computationally expensive in constrained environments, given the limited CPU, memory, and energy resources in the Electric Control Units (ECU) of SDVA. Consequently, the efficient management of these containers is crucial for enabling the on-demand usage of the applications in the vehicle based on the available resources while considering several constraints and priorities, including failure tolerance, security, safety, and comfort. In this article, we propose a dynamic software container management approach for constrained environments such as embedded devices/ECUs in SDVA within smart cars. To address the conflicting objectives and constraints within the vehicle, we design a novel search-based approach based on multi-objective optimization. This approach facilitates the allocation, movement, or suspension of containers between ECUs in the cluster. Collaborating with our industry partner, Ford Motor Company, we evaluate our approach using different real-world software-defined scenarios. These scenarios involve using heterogeneous clusters of ECU devices in vehicles based on real-world software containers and use-case studies from the automotive industry. The experimental results demonstrate that our scheduler outperforms existing scheduling algorithms, including the default Docker scheduler -Spread- commonly used in automotive applications. Our proposed scheduler exhibits superior performance in terms of energy and resource cost efficiency. Specifically, it achieves a 35% reduction in energy consumption in power-saving mode compared to the scheduler employed by Ford Motor Company. Additionally, our scheduler effectively distributes workload among the ECUs in the cluster, minimizing resource usage, and dynamically adjusts to the real-time requirements and constraints of the car environment. This work will serve as a fundamental building block in the automotive industry to efficiently manage software containers in smart vehicles, considering constraints and priorities in the real world.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672461},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Efficient management of containers for software defined vehicles},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HeMiRCA: Fine-grained root cause analysis for microservices
with heterogeneous data sources. <em>TOSEM</em>, <em>33</em>(8), 1–25.
(<a href="https://doi.org/10.1145/3674726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA) , for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7% and 74% on average, respectively.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3674726},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {HeMiRCA: Fine-grained root cause analysis for microservices with heterogeneous data sources},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DSHGT: Dual-supervisors heterogeneous graph transformer—a
pioneer study of using heterogeneous graph learning for detecting
software vulnerabilities. <em>TOSEM</em>, <em>33</em>(8), 1–31. (<a
href="https://doi.org/10.1145/3674729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vulnerability detection is a critical problem in software security and attracts growing attention both from academia and industry. Traditionally, software security is safeguarded by designated rule-based detectors that heavily rely on empirical expertise, requiring tremendous effort from software experts to generate rule repositories for large code corpus. Recent advances in deep learning, especially Graph Neural Networks (GNN), have uncovered the feasibility of automatic detection of a wide range of software vulnerabilities. However, prior learning-based works only break programs down into a sequence of word tokens for extracting contextual features of codes, or apply GNN largely on homogeneous graph representation (e.g., AST) without discerning complex types of underlying program entities (e.g., methods, variables). In this work, we are one of the first to explore heterogeneous graph representation in the form of Code Property Graph and adapt a well-known heterogeneous graph network with a dual-supervisor structure for the corresponding graph learning task. Using the prototype built, we have conducted extensive experiments on both synthetic datasets and real-world projects. Compared with the state-of-the-art baselines, the results demonstrate superior performance in vulnerability detection (average F1 improvements over 10% in real-world projects) and language-agnostic transferability from C/C \({+}{+}\) to other programming languages (average F1 improvements over 11%).},
  archive      = {J_TOSEM},
  doi          = {10.1145/3674729},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {DSHGT: Dual-supervisors heterogeneous graph Transformer—A pioneer study of using heterogeneous graph learning for detecting software vulnerabilities},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated commit intelligence by pre-training.
<em>TOSEM</em>, <em>33</em>(8), 1–30. (<a
href="https://doi.org/10.1145/3674731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GitHub commits, which record the code changes with natural language messages for description, play a critical role in software developers’ comprehension of software evolution. Due to their importance in software development, several learning-based works are conducted for GitHub commits, such as commit message generation and security patch identification. However, most existing works focus on customizing specialized neural networks for different tasks. Inspired by the superiority of code pre-trained models, which has confirmed their effectiveness across different downstream tasks, to promote the development of open-source software community, we first collect a large-scale commit benchmark including over 7.99 million commits across 7 programming languages. Based on this benchmark, we present CommitBART, a pre-trained encoder–decoder Transformer model for GitHub commits. The model is pre-trained by three categories (i.e., denoizing objectives, cross-modal generation, and contrastive learning) for six pre-training tasks to learn commit fragment representations. Our model is evaluated on one understanding task and three generation tasks for commits. The comprehensive experiments on these tasks demonstrate that CommitBART significantly outperforms previous pre-trained works for code. Further analysis also reveals that each pre-training task enhances the model performance.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3674731},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automated commit intelligence by pre-training},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AceCoder: An effective prompting technique specialized in
code generation. <em>TOSEM</em>, <em>33</em>(8), 1–26. (<a
href="https://doi.org/10.1145/3675395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have shown great success in code generation. LLMs take as the input a prompt and output the code. How to make prompts (i.e., Prompting Techniques ) is a key question. Existing prompting techniques are designed for natural language generation and have low accuracy in code generation. In this article, we propose a new prompting technique named AceCoder . Our motivation is that code generation meets two unique challenges (i.e., requirement understanding and code implementation). AceCoder contains two novel mechanisms (i.e., guided code generation and example retrieval) to solve these challenges. ❶ Guided code generation asks LLMs first to analyze requirements and output an intermediate preliminary (e.g., test cases). The preliminary clarifies requirements and tells LLMs “what to write.” ❷ Example retrieval selects similar programs as examples in prompts, which provide lots of relevant content (e.g., algorithms, APIs) and teach LLMs “how to write.” We apply AceCoder to four LLMs (e.g., GPT-3.5, CodeGeeX) and evaluate it on three public benchmarks using the Pass@ \(k\) . Results show that AceCoder can significantly improve the performance of LLMs on code generation. In terms of Pass@1, AceCoder outperforms the SOTA baseline by up to 56.4% in MBPP, 70.7% in MBJP, and 88.4% in MBJSP . AceCoder is effective in LLMs with different sizes (i.e., 6B–13B) and different languages (i.e., Python, Java, and JavaScript). Human evaluation shows human developers prefer programs from AceCoder .},
  archive      = {J_TOSEM},
  doi          = {10.1145/3675395},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {AceCoder: An effective prompting technique specialized in code generation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Word closure-based metamorphic testing for machine
translation. <em>TOSEM</em>, <em>33</em>(8), 1–46. (<a
href="https://doi.org/10.1145/3675396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide application of machine translation, the testing of Machine Translation Systems (MTSs) has attracted much attention. Recent works apply Metamorphic Testing (MT) to address the oracle problem in MTS testing. Existing MT methods for MTS generally follow the workflow of input transformation and output relation comparison, which generates a follow-up input sentence by mutating the source input and compares the source and follow-up output translations to detect translation errors, respectively. These methods use various input transformations to generate the test case pairs and have successfully triggered numerous translation errors. However, they have limitations in performing fine-grained and rigorous output relation comparison and thus may report many false alarms and miss many true errors. In this article, we propose a word closure-based output comparison method to address the limitations of the existing MTS MT methods. We first propose word closure as a new comparison unit, where each closure includes a group of correlated input and output words in the test case pair. Word closures suggest the linkages between the appropriate fragment in the source output translation and its counterpart in the follow-up output for comparison. Next, we compare the semantics on the level of word closure to identify the translation errors. In this way, we perform a fine-grained and rigorous semantic comparison for the outputs and thus realize more effective violation identification. We evaluate our method with the test cases generated by five existing input transformations and the translation outputs from three popular MTSs. Results show that our method significantly outperforms the existing works in violation identification by improving the precision and recall and achieving an average increase of 29.9% in F1 score. It also helps to increase the F1 score of translation error localization by 35.9%.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3675396},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Word closure-based metamorphic testing for machine translation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shortening overlong method names with abbreviations.
<em>TOSEM</em>, <em>33</em>(8), 1–24. (<a
href="https://doi.org/10.1145/3676959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods should be named to summarize their responsibilities meaningfully. When a method has a non-trivial responsibility, it may require a naming using multiple words. However, overlong method names are susceptible to typos and reduced readability (e.g., displaying a statement partially in standard screen width or splitting it into multiple lines). Programming naming conventions commonly adopt a maximal length (in characters) for identifiers. In practice, developers may not necessarily find a meaningful name that follows such naming conventions when coding a non-trivial method. This article presents the first automated technique (called NameCompressor ) to shorten overlong method names. Our inspiration is that many lengthy words/phrases in an overlong method name have known and unambiguous abbreviations. The use of these abbreviations for method names is common. To shorten an overlong method name, NameCompressor employs three compression techniques, i.e., context-aware compression, probability-based compression, and machine learning-based compression, to find appropriate abbreviations for the words/phrases in the method name. We evaluate NameCompressor on a dataset of 700 overlong method names. It correctly generates 613 short names identical to those specified by the developers of these methods.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3676959},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Shortening overlong method names with abbreviations},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolution-aware constraint derivation approach for software
remodularization. <em>TOSEM</em>, <em>33</em>(8), 1–43. (<a
href="https://doi.org/10.1145/3676960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing software clustering techniques tend to ignore prior knowledge from domain experts, leading to results (suggested big-bang remodularization actions) that cannot be acceptable to developers. Incorporating domain experts knowledge or constraints during clustering ensures the obtained modularization aligns with developers’ perspectives, enhancing software quality. However, manual review by knowledgeable domain experts for constraint generation is time-consuming and labor-intensive. In this article, we propose an evolution-aware constraint derivation approach, Escort , which automatically derives clustering constraints based on the evolutionary history from the analyzed software. Specifically, Escort can serve as an alternative approach to derive implicit and explicit constraints in situations where domain experts are absent. In the subsequent constrained clustering process, Escort can be considered as a framework to help supplement and enhance various unconstrained clustering techniques to improve their accuracy and reliability. We evaluate Escort based on both quantitative and qualitative analysis. In quantitative validation, Escort , using generated clustering constraints, outperforms seven classic unconstrained clustering techniques. Qualitatively, a survey with developers from five IT companies indicates that 89% agree with Escort ’s clustering constraints. We also evaluate the utility of refactoring suggestions from our constrained clustering approach, with 54% acknowledged by project developers, either implemented or planned for future releases.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3676960},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Evolution-aware constraint derivation approach for software remodularization},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Timing side-channel mitigation via automated program repair.
<em>TOSEM</em>, <em>33</em>(8), 1–27. (<a
href="https://doi.org/10.1145/3678169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Side-channel vulnerability detection has gained prominence recently due to Spectre and Meltdown attacks. Techniques for side-channel detection range from fuzz testing to program analysis and program composition. Existing side-channel mitigation techniques repair the vulnerability at the IR/binary level or use runtime monitoring solutions. In both cases, the source code itself is not modified, can evolve while keeping the vulnerability, and the developer would get no feedback on how to develop secure applications in the first place. Thus, these solutions do not help the developer understand the side-channel risks in her code and do not provide guidance to avoid code patterns with side-channel risks. In this article, we present Pendulum , the first approach for automatically locating and repairing side-channel vulnerabilities in the source code, specifically for timing side channels. Our approach uses a quantitative estimation of found vulnerabilities to guide the fix localization, which goes hand-in-hand with a pattern-guided repair. Our evaluation shows that Pendulum can repair a large number of side-channel vulnerabilities in real-world applications. Overall, our approach integrates vulnerability detection, quantization, localization, and repair into one unified process. This also enhances the possibility of our side-channel mitigation approach being adopted into programmingenvironments.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3678169},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Timing side-channel mitigation via automated program repair},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can GitHub issues help in app review classifications?
<em>TOSEM</em>, <em>33</em>(8), 1–42. (<a
href="https://doi.org/10.1145/3678170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {App reviews reflect various user requirements that can aid in planning maintenance tasks. Recently, proposed approaches for automatically classifying user reviews rely on machine learning algorithms. A previous study demonstrated that models trained on existing labeled datasets exhibit poor performance when predicting new ones. Therefore, a comprehensive labeled dataset is essential to train a more precise model. In this paper, we propose a novel approach that assists in augmenting labeled datasets by utilizing information extracted from an additional source, GitHub issues, that contains valuable information about user requirements. First, we identify issues concerning review intentions (bug reports, feature requests, and others) by examining the issue labels. Then, we analyze issue bodies and define 19 language patterns for extracting targeted information. Finally, we augment the manually labeled review dataset with a subset of processed issues through the Within-App , Within-Context , and Between-App Analysis methods. We conducted several experiments to evaluate the proposed approach. Our results demonstrate that using labeled issues for data augmentation can improve the F1-score to 6.3 in bug reports and 7.2 in feature requests. Furthermore, we identify an effective range of 0.3 to 0.7 for the auxiliary volume, which provides better performance improvements.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3678170},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-42},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Can GitHub issues help in app review classifications?},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A disruptive research playbook for studying disruptive
innovations. <em>TOSEM</em>, <em>33</em>(8), 1–29. (<a
href="https://doi.org/10.1145/3678172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As researchers today, we are witnessing a fundamental change in our technologically-enabled world due to the advent and diffusion of highly disruptive technologies such as generative Artificial Intelligence (AI), Augmented Reality (AR) and Virtual Reality (VR). In particular, software engineering has been profoundly affected by the transformative power of disruptive innovations for decades, with a significant impact of technical advancements on social dynamics due to its socio-technical nature. In this article, we reflect on the importance of formulating and addressing research problems in software engineering through a socio-technical lens, thus ensuring a holistic understanding of the complex phenomena in this field. We propose a research playbook with the aim of providing a guide to formulate compelling and socially relevant research questions and to identify the appropriate research strategies for empirical investigations, with an eye on the long-term implications of technologies or their use. We showcase how to apply the research playbook. Firstly, we show how it can be used retrospectively to reflect on a prior disruptive technology, Stack Overflow, and its impact on software development. Secondly, we show how it can be used to question the impact of two current disruptive technologies: AI and AR/VR. Finally, we introduce a specialized GPT model to support the researcher in framing future investigations. We conclude by discussing the broader implications of adopting the playbook for both researchers and practitioners in software engineering and beyond.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3678172},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A disruptive research playbook for studying disruptive innovations},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reusing d-DNNFs for efficient feature-model counting.
<em>TOSEM</em>, <em>33</em>(8), 1–32. (<a
href="https://doi.org/10.1145/3680465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature models are commonly used to specify valid configurations of a product line. In industry, feature models are often complex due to numerous features and constraints. Thus, a multitude of automated analyses have been proposed. Many of those rely on computing the number of valid configurations, which typically depends on solving a # SAT problem, a computationally expensive operation. Even worse, most counting-based analyses require evaluation for multiple features or partial configurations resulting in numerous # SAT computations on the same feature model. Instead of repetitive computations on highly similar formulas, we aim to improve the performance by reusing knowledge between these computations. In this work, we are the first to propose reusing d-DNNFs for performing repetitive counting queries on features and partial configurations. In our experiments, reusing d-DNNFs saved up-to \(\sim\) 99.98% compared to repetitive invocations of # SAT solvers even when including compilation times. Overall, our tool ddnnife combined with the d-DNNF compiler d4 appears to be the most promising option when dealing with many repetitive feature-model counting queries.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680465},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Reusing d-DNNFs for efficient feature-model counting},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speeding up genetic improvement via regression test
selection. <em>TOSEM</em>, <em>33</em>(8), 1–31. (<a
href="https://doi.org/10.1145/3680466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic Improvement (GI) uses search-based optimisation algorithms to automatically improve software with respect to both functional and non-functional properties. Our previous work showed that Regression Test Selection (RTS) can help speed up the use of GI and enhance the overall results while not affecting the software system’s validity. This article expands upon our investigation by answering further questions about safety and applying a GI algorithm based on Local Search (LS) in addition to the previously explored Genetic Programming (GP) approach. Further, we extend the number of subjects to 12 by analysing five larger real-world open-source programs. We empirically compare two state-of-the-art RTS techniques combined with GP and LS for these 12 programs. The results show that both RTS techniques are safe to use and can reduce the cost of GI by up to 80% and by 31% on average across programs. We also observe that both search-based algorithms impact the effectiveness gains of GI differently, and that various RTS strategies achieve differing gains in terms of efficiency. These results serve as further evidence that RTS must be used as a core component of the GI search process to maximise its effectiveness and efficiency.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3680466},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {11},
  number       = {8},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Speeding up genetic improvement via regression test selection},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthesis and verification of mission plans for multiple
autonomous agents under complex road conditions. <em>TOSEM</em>,
<em>33</em>(7), 1–46. (<a
href="https://doi.org/10.1145/3672445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mission planning for multi-agent autonomous systems aims to generate feasible and optimal mission plans that satisfy given requirements. In this article, we propose a tool-supported mission-planning methodology that combines (i) a path-planning algorithm for synthesizing path plans that are safe in environments with complex road conditions, and (ii) a task-scheduling method for synthesizing task plans that schedule the tasks in the right and fastest order, taking into account the planned paths. The task-scheduling method is based on model checking, which provides means of automatically generating task execution orders that satisfy the requirements and ensure the correctness and efficiency of the plans by construction. We implement our approach in a tool named MALTA, which offers a user-friendly GUI for configuring mission requirements, a module for path planning, an integration with the model checker UPPAAL, and functions for automatic generation of formal models, and parsing of the execution traces of models. Experiments with the tool demonstrate its applicability and performance in various configurations of an industrial case study of an autonomous quarry. We also show the adaptability of our tool by employing it in a special case of an industrial case study.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672445},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Synthesis and verification of mission plans for multiple autonomous agents under complex road conditions},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can coverage criteria guide failure discovery for image
classifiers? An empirical study. <em>TOSEM</em>, <em>33</em>(7), 1–28.
(<a href="https://doi.org/10.1145/3672446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality assurance of deep neural networks (DNNs) is crucial for the deployment of DNN-based software, especially in mission- and safety-critical tasks. Inspired by structural white-box testing in traditional software, many test criteria have been proposed to test DNNs, i.e., to exhibit erroneous behaviors by activating new test units that have not been covered, such as new neurons, values, and decision paths. Many studies have been done to evaluate the effectiveness of DNN test coverage criteria. However, existing empirical studies mainly focused on measuring the effectiveness of DNN test criteria for improving the adversarial robustness of DNNs, while ignoring the correctness property when testing DNNs. To fill in this gap, we conduct a comprehensive study on 11 structural coverage criteria, 6 widely-used image datasets, and 9 popular DNNs. We investigate the effectiveness of DNN coverage criteria over natural inputs from four aspects: (1) the correlation between test coverage and test diversity; (2) the effects of criteria parameters and target DNNs; (3) the effectiveness to prioritize in-distribution natural inputs that lead to erroneous behaviors; and (4) the capability to detect out-of-distribution natural samples. Our findings include: (1) For measuring the diversity, coverage criteria considering the relationship between different neurons are more effective than coverage criteria that treat each neuron independently. For instance, the neuron-path criteria (i.e., SNPC and ANPC) show high correlation with test diversity, which is promising to measure test diversity for DNNs. (2) The hyper-parameters have a big influence on the effectiveness of criteria, especially those relevant to the granularity of test criteria. Meanwhile, the computational complexity is one of the important issues to be considered when designing deep learning test coverage criteria, especially for large-scale models. (3) Test criteria related to data distribution (i.e., LSA and DSA, SNAC, and NBC) can be used to prioritize both in-distribution natural faults and out-of-distribution inputs. Furthermore, for OOD detection, the boundary metrics (i.e., SNAC and NBC) are also effective indicators with lower computational costs and higher detection efficiency compared with LSA and DSA. These findings motivate follow-up research on scalable test coverage criteria that improve the correctness of DNNs.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672446},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Can coverage criteria guide failure discovery for image classifiers? an empirical study},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An empirical study on the characteristics of database access
bugs in java applications. <em>TOSEM</em>, <em>33</em>(7), 1–25. (<a
href="https://doi.org/10.1145/3672449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Database-backed applications rely on the database access code to interact with the underlying database management systems (DBMSs). Although many prior studies aim at database access issues like SQL anti-patterns or SQL code smells, there is a lack of study of database access bugs during the maintenance of database-backed applications. In this paper, we empirically investigate 423 database access bugs collected from seven large-scale Java open-source applications that use relational DBMSs (e.g., MySQL or PostgreSQL). We study the characteristics (e.g., occurrence and root causes) of the bugs by manually examining the bug reports and commit histories. We find that the number of reported database and non-database access bugs share a similar trend but their modified files in bug fixing commits are different. Additionally, we generalize categories of the root causes of database access bugs, containing five main categories (SQL queries, Schema, API, Configuration, and SQL query result) and 25 unique root causes. We find that the bugs pertaining to SQL queries, Schema, and API cover 84.2% of database access bugs across all studied applications. In particular, SQL queries bug (54%) and API bug (38.7%) are the most frequent issues when using JDBC and Hibernate, respectively. Finally, we provide a discussion on the implications of our findings for developers and researchers.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672449},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study on the characteristics of database access bugs in java applications},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When automated program repair meets regression testing—an
extensive study on two million patches. <em>TOSEM</em>, <em>33</em>(7),
1–23. (<a href="https://doi.org/10.1145/3672450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Automated Program Repair (APR) has been extensively studied in academia and even drawn wide attention from the industry. However, APR techniques can be extremely time consuming since (1) a large number of patches can be generated for a given bug, and (2) each patch needs to be executed on the original tests to ensure its correctness. In the literature, various techniques (e.g., based on learning, mining, and constraint solving) have been proposed/studied to reduce the number of patches. Intuitively, every patch can be treated as a software revision during regression testing; thus, traditional Regression Test Selection (RTS) techniques can be leveraged to only execute the tests affected by each patch (as the other tests would keep the same outcomes) to further reduce patch execution time. However, few APR systems actually adopt RTS and there is still a lack of systematic studies demonstrating the benefits of RTS and the impact of different RTS strategies on APR. To this end, this article presents the first extensive study of widely used RTS techniques at different levels (i.e., class/method/statement levels) for 12 state-of-the-art APR systems on over 2M patches. Our study reveals various practical guidelines for bridging the gap between APR and regression testing, including: (1) the number of patches widely used for measuring APR efficiency can incur skewed conclusions, and the use of inconsistent RTS configurations can further skew the conclusions; (2) all studied RTS techniques can substantially improve APR efficiency and should be considered in future APR work; (3) method- and statement-level RTS outperform class-level RTS substantially and should be preferred; (4) RTS techniques can substantially outperform state-of-the-art test prioritization techniques for APR, and combining them can further improve APR efficiency; and (5) traditional Regression Test Prioritization (RTP) widely studied in regression testing performs even better than APR-specific test prioritization when combined with most RTS techniques. Furthermore, we also present the detailed impact of different patch categories and patch validation strategies on our findings.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672450},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-23},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {When automated program repair meets regression Testing—An extensive study on two million patches},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Keeper: Automated testing and fixing of machine learning
software. <em>TOSEM</em>, <em>33</em>(7), 1–33. (<a
href="https://doi.org/10.1145/3672451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing number of software applications incorporating machine learning (ML) solutions has led to the need for testing techniques. However, testing ML software requires tremendous human effort to design realistic and relevant test inputs and to judge software output correctness according to human common sense. Even when misbehavior is exposed, it is often unclear whether the defect is inside ML API or the surrounding code and how to fix the implementation. This article tackles these challenges by proposing Keeper, an automated testing and fixing tool for ML software. The core idea of Keeper is designing pseudo-inverse functions that semantically reverse the corresponding ML task in an empirical way and proxy common human judgment of real-world data. It incorporates these functions into a symbolic execution engine to generate tests. Keeper also detects code smells that degrade software performance. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used to alleviate the misbehavior. Our evaluation on a variety of applications shows that Keeper greatly improves branch coverage, while identifying 74 previously unknown failures and 19 code smells from 56 out of 104 applications. Our user studies show that 78% of end-users and 95% of developers agree with Keeper’s detection and fixing results.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672451},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Keeper: Automated testing and fixing of machine learning software},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding vulnerability inducing commits of the linux
kernel. <em>TOSEM</em>, <em>33</em>(7), 1–28. (<a
href="https://doi.org/10.1145/3672452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Linux kernel is popular and well-maintained. Over the past decade, around 860 thousand commits were merged with hundreds of vulnerabilities (i.e., 223 on average) disclosed every year, taking the total lines of code to 35.1 million in 2022. Many algorithms have been proposed to detect the vulnerabilities, but few studied how they were induced. To fill this gap, we conduct the first empirical study on the Kernel Vulnerability Inducing Commits (KVIC), the commits that induced vulnerabilities in the Linux kernel. We utilized six different methods on identifying the Kernel Vulnerability Fixing Commits (KVFCs), the commits that fix vulnerabilities in the Linux kernel, and proposed the other four different methods for identifying KVICs by using the identified KVFCs as a bridge. In total, we constructed the first dataset of KVICs with 1,240 KVICs for 1,335 CVEs. We conducted a thorough analysis on the characteristics, purposes, and involved human factors of the KVICs and obtained many interesting findings and insights. For example, KVICs usually have limited reviewers and can still be induced by experienced authors or maintainers. Based on these insights, we proposed several suggestions to the Linux community to help mitigate the induction of KVICs.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672452},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Understanding vulnerability inducing commits of the linux kernel},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harmonising contributions: Exploring diversity in software
engineering through CQA mining on stack overflow. <em>TOSEM</em>,
<em>33</em>(7), 1–54. (<a
href="https://doi.org/10.1145/3672453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for collective intelligence in technology means that online Q&amp;A platforms, such as Stack Overflow and Reddit, have become invaluable in building the global knowledge ecosystem. Despite literature demonstrating a prevalence of inclusion and contribution disparities in online communities, studies investigating the underlying reasons behind such fluctuations remain scarce. The current study examines Stack Overflow users’ contribution profiles, both in isolation and relative to various diversity metrics, including GDP and access to electricity. This study also examines whether such profiles propagate to the city and state levels, supplemented by granular data such as per capita income and education, before validating quantitative findings using content analysis. We selected 143 countries and compared the profiles of their respective users to assess implicit diversity-related complications that impact how users contribute. Results show that countries with high GDP, prominent R&amp;D presence, less wealth inequality and sufficient access to infrastructure tend to have more users, regardless of their development status. Similarly, cities and states where technology is more prevalent (e.g., San Francisco and New York) have more users who tend to contribute more often. Qualitative analysis reveals distinct communication styles based on users’ locations. Urban users exhibited assertive, solution-oriented behaviour, actively sharing information. Conversely, rural users engaged through inquiries and discussions, incorporating personal anecdotes, gratitude and conciliatory language. Findings from this study may benefit scholars and practitioners, allowing them to develop sustainable mechanisms to bridge the inclusion and diversity gaps.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672453},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-54},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Harmonising contributions: Exploring diversity in software engineering through CQA mining on stack overflow},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuron sensitivity-guided test case selection.
<em>TOSEM</em>, <em>33</em>(7), 1–32. (<a
href="https://doi.org/10.1145/3672454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been widely deployed in software to address various tasks (e.g., autonomous driving, medical diagnosis). However, they can also produce incorrect behaviors that result in financial losses and even threaten human safety. To reveal and repair incorrect behaviors in DNNs, developers often collect rich, unlabeled datasets from the natural world and label them to test DNN models. However, properly labeling a large number of datasets is a highly expensive and time-consuming task. To address the above-mentioned problem, we propose neuron sensitivity-guided test case selection (NSS), which can reduce the labeling time by selecting valuable test cases from unlabeled datasets. NSS leverages the information of the internal neuron induced by the test cases to select valuable test cases, which have high confidence in causing the model to behave incorrectly. We evaluated NSS with four widely used datasets and four well-designed DNN models compared to the state-of-the-art (SOTA) baseline methods. The results show that NSS performs well in assessing the probability of failure triggering in test cases and in the improvement capabilities of the model. Specifically, compared to the baseline approaches, NSS achieves a higher fault detection rate (e.g., when selecting 5% of the test cases from the unlabeled dataset in the MNIST and LeNet1 experiment, NSS can obtain an 81.8% fault detection rate, which is a 20% increase compared with SOTA baseline strategies).},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672454},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Neuron sensitivity-guided test case selection},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated testing linguistic capabilities of NLP models.
<em>TOSEM</em>, <em>33</em>(7), 1–33. (<a
href="https://doi.org/10.1145/3672455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) has gained widespread adoption in the development of real-world applications. However, the black-box nature of neural networks in NLP applications poses a challenge when evaluating their performance, let alone ensuring it. Recent research has proposed testing techniques to enhance the trustworthiness of NLP-based applications. However, most existing works use a single, aggregated metric (i.e., accuracy) which is difficult for users to assess NLP model performance on fine-grained aspects, such as LCs. To address this limitation, we present ALiCT, an automated testing technique for validating NLP applications based on their LCs. ALiCT takes user-specified LCs as inputs and produces diverse test suite with test oracles for each of given LC. We evaluate ALiCT on two widely adopted NLP tasks, sentiment analysis and hate speech detection, in terms of diversity, effectiveness, and consistency. Using Self-BLEU and syntactic diversity metrics, our findings reveal that ALiCT generates test cases that are 190% and 2213% more diverse in semantics and syntax, respectively, compared to those produced by state-of-the-art techniques. In addition, ALiCT is capable of producing a larger number of NLP model failures in 22 out of 25 LCs over the two NLP applications.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672455},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automated testing linguistic capabilities of NLP models},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-planning code generation with large language models.
<em>TOSEM</em>, <em>33</em>(7), 1–30. (<a
href="https://doi.org/10.1145/3672456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4% in Pass@1 compared to direct code generation, and up to 11.9% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672456},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Self-planning code generation with large language models},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revealing the unseen: AI chain on LLMs for predicting
implicit dataflows to generate dataflow graphs in dynamically typed
code. <em>TOSEM</em>, <em>33</em>(7), 1–29. (<a
href="https://doi.org/10.1145/3672458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataflow graphs (DFGs) capture definitions (defs) and uses across program blocks, which is a fundamental program representation for program analysis, testing and maintenance. However, dynamically typed programming languages like Python present implicit dataflow issues that make it challenging to determine def-use flow information at compile time. Static analysis methods like Soot and WALA are inadequate for handling these issues, and manually enumerating comprehensive heuristic rules is impractical. Large pre-trained language models (LLMs) offer a potential solution, as they have powerful language understanding and pattern matching abilities, allowing them to predict implicit dataflow by analyzing code context and relationships between variables, functions, and statements in code. We propose leveraging LLMs’ in-context learning ability to learn implicit rules and patterns from code representation and contextual information to solve implicit dataflow problems. To further enhance the accuracy of LLMs, we design a five-step chain of thought (CoT) and break it down into an Artificial Intelligence (AI) chain, with each step corresponding to a separate AI unit to generate accurate DFGs for Python code. Our approach’s performance is thoroughly assessed, demonstrating the effectiveness of each AI unit in the AI Chain. Compared to static analysis, our method achieves 82% higher def coverage and 58% higher use coverage in DFG generation on implicit dataflow. We also prove the indispensability of each unit in the AI Chain. Overall, our approach offers a promising direction for building software engineering tools by utilizing foundation models, eliminating significant engineering and maintenance effort, but focusing on identifying problems for AI to solve.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672458},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Revealing the unseen: AI chain on LLMs for predicting implicit dataflows to generate dataflow graphs in dynamically typed code},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-collaboration code generation via ChatGPT.
<em>TOSEM</em>, <em>33</em>(7), 1–38. (<a
href="https://doi.org/10.1145/3672459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct “experts,” each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9–47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672459},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Self-collaboration code generation via ChatGPT},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bitmap-based security monitoring for deeply embedded
systems. <em>TOSEM</em>, <em>33</em>(7), 1–31. (<a
href="https://doi.org/10.1145/3672460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deeply embedded systems powered by microcontrollers are becoming popular with the emergence of Internet-of-Things (IoT) technology. However, these devices primarily run C/C \({+}{+}\) code and are susceptible to memory bugs, which can potentially lead to both control data attacks and non-control data attacks. Existing defense mechanisms (such as control-flow integrity (CFI), dataflow integrity (DFI) and write integrity testing (WIT), etc.) consume a massive amount of resources, making them less practical in real products. To make it lightweight, we design a bitmap-based allowlist mechanism to unify the storage of the runtime data for protecting both control data and non-control data. The memory requirements are constant and small, regardless of the number of deployed defense mechanisms. We store the allowlist in the TrustZone to ensure its integrity and confidentiality. Meanwhile, we perform an offline analysis to detect potential collisions and make corresponding adjustments when it happens. We have implemented our idea on an ARM Cortex-M-based development board. Our evaluation results show a substantial reduction in memory consumption when deploying the proposed CFI and DFI mechanisms, without compromising runtime performance. Specifically, our prototype enforces CFI and DFI at a cost of just 2.09% performance overhead and 32.56% memory overhead on average.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672460},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Bitmap-based security monitoring for deeply embedded systems},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Paving a path for a combined family of feature toggle and
configuration option research. <em>TOSEM</em>, <em>33</em>(7), 1–27. (<a
href="https://doi.org/10.1145/3672555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature toggles and configuration options are techniques to include or exclude functionality in software. The research contributions to these two techniques have most often been focused on either one of them. However, focusing on the similarities of these two techniques and the use of a common terminology may enable a combined family of research on software configuration (a term we use to encompass both techniques) and prevent duplication of effort. The goal of this study is to aid researchers in conducting a family of research on software configuration by extending an existing model of software configuration that provides a common terminology for feature toggles and configuration options in research studies. We started with Siegmund et al.’s Model of Software Configuration (MSC), which was developed based on configuration option-related resources. We extend the MSC by qualitative analysis of feature toggle-related resources. From our analysis, we proposed MSCv2 and evaluated it through its application on publications and an industrial system. Our results indicate researchers studying the same system may provide different definitions of software configuration in publications, similar research questions may be answered repeatedly because of a lack of a clear definition of software configuration, and having an MSC may enable generalized research on this family of research.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3672555},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Paving a path for a combined family of feature toggle and configuration option research},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FunFuzz: A function-oriented fuzzer for smart contract
vulnerability detection with high effectiveness and efficiency.
<em>TOSEM</em>, <em>33</em>(7), 1–20. (<a
href="https://doi.org/10.1145/3674725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of Decentralized Applications (DApps) in blockchain, securing smart contracts has been a long-term, high-priority subject in the domain. Among the various research directions for vulnerability detection, fuzzing has received extensive attention because of its high effectiveness. However, with the increasing complexity of smart contracts, existing fuzzers may waste substantial time exploring locations irrelevant to smart contract vulnerabilities. In this article, we present FunFuzz, a function-oriented fuzzer, which is dedicatedly tailored for detecting smart contract vulnerability with high effectiveness and efficiency. The key observation in our research is that most smart contract vulnerabilities exist in specific functions rather than randomly distributed in all program code like other traditional software. To this end, unlike traditional fuzzers which mainly target code coverage, FunFuzz identifies risky functions while pruning non-risky ones in smart contracts. In this way, it significantly narrows down the exploration scope during the fuzzing process. In addition, FunFuzz employs three unique strategies to direct itself toward effectively discovering vulnerabilities specific to smart contracts (e.g., reentrancy, block dependency, and gasless send). Extensive experiments on 170 real-world contracts demonstrate that FunFuzz outperforms state-of-the-art fuzzers in terms of effectiveness and efficiency.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3674725},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-20},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {FunFuzz: A function-oriented fuzzer for smart contract vulnerability detection with high effectiveness and efficiency},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive view on TD prevention practices and reasons
for not preventing it. <em>TOSEM</em>, <em>33</em>(7), 1–44. (<a
href="https://doi.org/10.1145/3674727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context . Technical debt (TD) prevention allows software practitioners to apply practices to avoid potential TD items in their projects. Aims . To uncover and prioritize, from the point of view of software practitioners, the practices that could be used to avoid TD items, the relations between these practices and the causes of TD, and the practice avoidance reasons (PARs) that could explain the failure to prevent TD. Method . We analyze data collected from six replications of a global industrial family of surveys on TD, totaling 653 answers. We also conducted a follow up survey to understand the importance level of analyzed data. Results . Most practitioners indicated that TD could be prevented, revealing 89 prevention practices and 23 PARs for explaining the failure to prevent TD. The article identifies statistically significant relationships between preventive practices and certain causes of TD. Further, it prioritizes the list of practices, PARs, and relationships regarding their level of importance for TD prevention based on the opinion of software practitioners. Conclusion . This work organizes TD prevention practices and PARs in a conceptual map and the relationships between practices and causes of TD in a Sankey diagram to help the visualization of the body of knowledge reported in this study.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3674727},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A comprehensive view on TD prevention practices and reasons for not preventing it},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective, platform-independent GUI testing via image
embedding and reinforcement learning. <em>TOSEM</em>, <em>33</em>(7),
1–27. (<a href="https://doi.org/10.1145/3674728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software applications (apps) have been playing an increasingly important role in various aspects of society. In particular, mobile apps and web apps are the most prevalent among all applications and are widely used in various industries as well as in people’s daily lives. To help ensure mobile and web app quality, many approaches have been introduced to improve app GUI testing via automated exploration, including random testing, model-based testing, learning-based testing, and so on. Despite the extensive effort, existing approaches are still limited in reaching high code coverage, constructing high-quality models, and being generally applicable. Reinforcement learning-based approaches, as a group of representative and advanced approaches for automated GUI exploration testing, are faced with difficult challenges, including effective app state abstraction, reward function design, and so on. Moreover, they heavily depend on the specific execution platforms (i.e., Android or Web), thus leading to poor generalizability and being unable to adapt to different platforms. This work specifically tackles these challenges based on the high-level observation that apps from distinct platforms share commonalities in GUI design. Indeed, we propose PIRLTest , an effective platform-independent approach for app testing. Specifically, PIRLTest utilizes computer vision and reinforcement learning techniques in a novel, synergistic manner for automated testing. It extracts the GUI widgets from GUI pages and characterizes the corresponding GUI layouts, embedding the GUI pages as states. The app GUI state combines the macroscopic perspective (app GUI layout) and the microscopic perspective (app GUI widget) and attaches the critical semantic information from GUI images. This enables PIRLTest to be platform-independent and makes the testing approach generally applicable on different platforms. PIRLTest explores apps with the guidance of a curiosity-driven strategy, which uses a Q-network to estimate the values of specific state-action pairs to encourage more exploration in uncovered pages without platform dependency. The exploration will be assigned with rewards for all actions, which are designed considering both the app GUI states and the concrete widgets, to help the framework explore more uncovered pages. We conduct an empirical study on 20 mobile apps and 5 web apps, and the results show that PIRLTest is zero-cost when being adapted to different platforms, and can perform better than the baselines, covering 6.3–41.4% more code on mobile apps and 1.5–51.1% more code on web apps. PIRLTest is capable of detecting 128 unique bugs on mobile and web apps, including 100 bugs that cannot be detected by the baselines.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3674728},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Effective, platform-independent GUI testing via image embedding and reinforcement learning},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDE4ICDS: A human-centric and model-driven proposal to
improve the digitization of clinical practice guideline. <em>TOSEM</em>,
<em>33</em>(7), 1–38. (<a
href="https://doi.org/10.1145/3674732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical practice guidelines (CPGs) are a formalization of specific clinical knowledge that states the best evidence-based clinical practices for treating pathologies. However, CPGs are limited because they are usually expressed as text. This gives rise to a certain level of ambiguity, subjective interpretation of the actions to be performed, and variability in clinical practice by different health professionals facing the same circumstances. The inherent complexity of CPGs is also a challenge for software engineers designing, developing, and maintaining software systems and clinical decision support system to manage and digitize them. This challenge stems from the need to evolve CPGs and design software systems capable of allowing their evolution. This paper proposes a model-driven, human-centric and tool-supported framework (called IDE 4 ICDS) for improving digitisation of CPG in practical environments. This framework is designed from a human-centric perspective to be used by mixed teams of clinicians and software engineers. It was also validated with the type 2 diabetes mellitus CPG in the Andalusian Public Health System (Spain) involving 89 patients and obtaining a kappa-based analysis. The recommendations were acceptable (0.61–0.80) with a total kappa index of 0.701, leading to the conclusion that the proposal provided appropriate recommendations for each patient.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3674732},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {IDE4ICDS: A human-centric and model-driven proposal to improve the digitization of clinical practice guideline},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic mapping study exploring quantification
approaches to code, design, and architecture technical debt.
<em>TOSEM</em>, <em>33</em>(7), 1–44. (<a
href="https://doi.org/10.1145/3675393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively manage Technical Debt (TD), we need reliable means to quantify it. We conducted a Systematic Mapping Study (SMS) where we identified 39 quantification approaches for Code, Design, and Architecture TD. We analyzed concepts and metrics discussed in these quantification approaches by classifying the quantification approaches based on a set of abstract TD Quantification (TDQ) concepts and their high-level themes, process/time, cost, benefit, probability, and priority, which we developed during our SMS. This helped identify gaps in the literature and to propose future research directions. Among the abstract TDQ concepts discussed in the different quantification approaches, TD item, TD remediation cost, TD interest, and Benefit of remediating TD were the most frequently discussed concepts. They were also supported by some form of measurement. However, some TDQ concepts were poorly examined, for example, the benefit of taking TD. It was evident that cost concepts were more frequently quantified among the approaches, while benefit concepts were not. Most of the approaches focused on remediating TD in retrospect rather than quantifying TD to strategically use it during software development. This raises the question of whether existing approaches reliably quantify TD and suggests the need to further explore TDQ.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3675393},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {9},
  number       = {7},
  pages        = {1-44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A systematic mapping study exploring quantification approaches to code, design, and architecture technical debt},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the model update strategies for supervised learning in
AIOps solutions. <em>TOSEM</em>, <em>33</em>(7), 1–38. (<a
href="https://doi.org/10.1145/3664599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AIOps (Artificial Intelligence for IT Operations) solutions leverage the massive data produced during the operation of large-scale systems and machine learning models to assist software engineers in their system operations. As operation data produced in the field are constantly evolving due to factors such as the changing operational environment and user base, the models in AIOps solutions need to be constantly maintained after deployment. While prior works focus on innovative modeling techniques to improve the performance of AIOps models before releasing them into the field, when and how to update AIOps models remain an under-investigated topic. In this work, we performed a case study on three large-scale public operation data: two trace datasets from the cloud computing platforms of Google and Alibaba and one disk stats dataset from the BackBlaze cloud storage data center. We empirically assessed five different types of model update strategies for supervised learning regarding their performance, updating cost, and stability. We observed that active model update strategies (e.g., periodical retraining, concept drift guided retraining, time-based model ensembles, and online learning) achieve better and more stable performance than a stationary model. Particularly, applying sophisticated model update strategies (e.g., concept drift detection, time-based ensembles, and online learning) could provide better performance, efficiency, and stability than simply retraining AIOps models periodically. In addition, we observed that, although some update strategies (e.g., time-based ensemble and online learning) can save model training time, they significantly sacrifice model testing time, which could hinder their applications in AIOps solutions where the operation data arrive at high pace and volume and where immediate inferences are required. Our findings highlight that practitioners should consider the evolution of operation data and actively maintain AIOps models over time. Our observations can also guide researchers and practitioners in investigating more efficient and effective model update strategies that fit in the context of AIOps.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664599},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On the model update strategies for supervised learning in AIOps solutions},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphuzz: Data-driven seed scheduling for coverage-guided
greybox fuzzing. <em>TOSEM</em>, <em>33</em>(7), 1–36. (<a
href="https://doi.org/10.1145/3664603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seed scheduling is a critical step of greybox fuzzing, which assigns different weights to seed test cases during seed selection, and significantly impacts the efficiency of fuzzing. Existing seed scheduling strategies rely on manually designed models to estimate the potentials of seeds and determine their weights, which fails to capture the rich information of a seed and its execution and thus the estimation of seeds’ potentials is not optimal. In this article, we introduce a new seed scheduling solution, Graphuzz, for coverage-guided greybox fuzzing, which utilizes deep learning models to estimate the potentials of seeds and works in a data-driven way. Specifically, we propose an extended control flow graph called e-CFG to represent the control-flow and data-flow features of a seed&#39;s execution, which is suitable for graph neural networks (GNN) to process and estimate seeds’ potential. We evaluate each seed&#39;s code coverage increment and use it as the label to train the GNN model. Further, we propose a self-attention mechanism to enhance the GNN model so that it can capture overlooked features. We have implemented a prototype of Graphuzz based on the baseline fuzzer AFLplusplus. The evaluation results show that our model can estimate the potential of seeds and has the robust capability to generalize to different targets. Furthermore, the evaluation using 12 benchmarks from FuzzBench shows that Graphuzz outperforms AFLplusplus and the state-of-the-art seed scheduling solution K-Scheduler and other coverage-guided fuzzers in terms of code coverage, and the evaluation using 8 benchmarks from Magma shows that Graphuzz outperforms the baseline fuzzer AFLplusplus and SOTA solutions in terms of bug detection.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664603},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Graphuzz: Data-driven seed scheduling for coverage-guided greybox fuzzing},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unveiling code pre-trained models: Investigating syntax and
semantics capacities. <em>TOSEM</em>, <em>33</em>(7), 1–29. (<a
href="https://doi.org/10.1145/3664606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code models have made significant advancements in code intelligence by encoding knowledge about programming languages. While previous studies have explored the capabilities of these models in learning code syntax, there has been limited investigation on their ability to understand code semantics. Additionally, existing analyses assume that the number of edges between nodes at the abstract syntax tree (AST) is related to syntax distance, and also often require transforming the high-dimensional space of deep learning models to a low-dimensional one, which may introduce inaccuracies. To study how code models represent code syntax and semantics, we conduct a comprehensive analysis of seven code models, including four representative code pre-trained models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) and three large language models (LLMs) (StarCoder, CodeLlama and CodeT5+). We design four probing tasks to assess the models’ capacities in learning both code syntax and semantics. These probing tasks reconstruct code syntax and semantics structures (AST, control dependence graph (CDG), data dependence graph (DDG), and control flow graph (CFG)) in the representation space. These structures are core concepts for code understanding. We also investigate the syntax token role in each token representation and the long dependency between the code tokens. Additionally, we analyze the distribution of attention weights related to code semantic structures. Through extensive analysis, our findings highlight the strengths and limitations of different code models in learning code syntax and semantics. The results demonstrate that these models excel in learning code syntax, successfully capturing the syntax relationships between tokens and the syntax roles of individual tokens. However, their performance in encoding code semantics varies. CodeT5 and CodeBERT demonstrate proficiency in capturing control and data dependencies, whereas UnixCoder shows weaker performance in this aspect. We do not observe LLMs generally performing much better than pre-trained models. The shallow layers of LLMs perform better than their deep layers. The investigation of attention weights reveals that different attention heads play distinct roles in encoding code semantics. Our research findings emphasize the need for further enhancements in code models to better learn code semantics. This study contributes to the understanding of code models’ abilities in syntax and semantics analysis. Our findings provide guidance for future improvements in code models, facilitating their effective application in various code-related tasks.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664606},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Unveiling code pre-trained models: Investigating syntax and semantics capacities},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical debt monitoring decision making with skin in the
game. <em>TOSEM</em>, <em>33</em>(7), 1–27. (<a
href="https://doi.org/10.1145/3664805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technical Debt Management (TDM) can suffer from unpredictability, communication gaps and the inaccessibility of relevant information, which hamper the effectiveness of its decision making. These issues can stem from division among decision-makers which takes root in unfair consequences of decisions among different decision-makers. One mitigation route is Skin in the Game thinking, which enforces transparency, fairness and shared responsibility during collective decision-making under uncertainty. This article illustrates characteristics which require Skin in the Game thinking in Technical Debt (TD) identification, measurement, prioritisation and monitoring. We point out crucial problems in TD monitoring rooted in asymmetric information and asymmetric payoff between different factions of decision-makers. A systematic TD monitoring method is presented to mitigate the said problems. The method leverages Replicator Dynamics and Behavioural Learning. The method supports decision-makers with automated TD monitoring decisions; it informs decision-makers when human interventions are required. Two publicly available industrial projects with a non-trivial number of TD and timestamps are utilised to evaluate the application of our method. Mann–Whitney U hypothesis tests are conducted on samples of decisions from our method and the baseline. The statistical evidence indicates that our method can produce cost-effective and contextual TD monitoring decisions.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664805},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Technical debt monitoring decision making with skin in the game},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-learning for multi-family android malware
classification. <em>TOSEM</em>, <em>33</em>(7), 1–27. (<a
href="https://doi.org/10.1145/3664806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of smartphones, Android has become a widely used mobile operating system. However, it is vulnerable when encountering various types of attacks. Every day, new malware threatens the security of users’ devices and private data. Many methods have been proposed to classify malicious applications, utilizing static or dynamic analysis for classification. However, previous methods still suffer from unsatisfactory performance due to two challenges. First, they are unable to address the imbalanced data distribution problem, leading to poor performance for malware families with few members. Second, they are unable to address the zero-day malware (zero-day malware refers to malicious applications that exploit unknown vulnerabilities) classification problem. In this article, we introduce an innovative meta -learning approach for m ulti-family A ndroid m alware c lassification named Meta-MAMC , which uses meta-learning technology to learn meta-knowledge (i.e., the similarities and differences among different malware families) of few-family samples and combines new sampling algorithms to solve the above challenges. Meta-MAMC integrates (i) the meta-knowledge contained within the dataset to guide models in learning to identify unknown malware; and (ii) more accurate and diverse tasks based on novel sampling strategies, as well as directly adapting meta-learning to a new few-sample and zero-sample task to classify families. We have evaluated Meta-MAMC on two popular datasets and a corpus of real-world Android applications. The results demonstrate its efficacy in accurately classifying malicious applications belonging to certain malware families, even achieving 100% classification in some families.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664806},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Meta-learning for multi-family android malware classification},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Code to qed, the project manager’s guide to proof
engineering. <em>TOSEM</em>, <em>33</em>(7), 1–50. (<a
href="https://doi.org/10.1145/3664807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite growing efforts and encouraging successes in recent decades, fully formally verified projects are still rare in the industrial landscape. The industry often lacks the tools and methodologies to efficiently scale the proof development process. In this work, we give a comprehensible overview of the proof development process for proof developers and project managers. The goal is to support proof developers by rationalizing the proof development process, which currently relies heavily on their intuition and expertise, and by facilitating communication with the management line. To this end, we concentrate on the aspect of proof manufacturing and highlight the most significant sources of proof effort. We propose means to mitigate the latter through proof practices (proof structuring, proof strategies, and proof planning), proof metrics, and tools. Our approach is project-agnostic, independent of specific proof expertise, and computed estimations do not assume prior similar developments. We evaluate our guidelines using a separation kernel undergoing formal verification, driving the proof process in an optimised way. Feedback from a project manager unfamiliar with proof development confirms the benefits of detailed planning of the proof development steps, clear progress communication to the hierarchy line, and alignment with established practices in the software industry.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664807},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-50},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Code to qed, the project manager&#39;s guide to proof engineering},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A tale of two comprehensions? Analyzing student programmer
attention during code summarization. <em>TOSEM</em>, <em>33</em>(7),
1–37. (<a href="https://doi.org/10.1145/3664808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code summarization is the task of creating short, natural language descriptions of source code. It is an important part of code comprehension and a powerful method of documentation. Previous work has made progress in identifying where programmers focus in code as they write their own summaries (i.e., Writing). However, there is currently a gap in studying programmers’ attention as they read code with pre-written summaries (i.e., Reading). As a result, it is currently unknown how these two forms of code comprehension compare: Reading and Writing. Also, there is a limited understanding of programmer attention with respect to program semantics. We address these shortcomings with a human eye-tracking study ( n = 27) comparing Reading and Writing. We examined programmers’ attention with respect to fine-grained program semantics, including their attention sequences (i.e., scan paths). We find distinctions in programmer attention across the comprehension tasks, similarities in reading patterns between them, and differences mediated by demographic factors. This can help guide code comprehension in both computer science education and automated code summarization. Furthermore, we mapped programmers’ gaze data onto the Abstract Syntax Tree to explore another representation of human attention. We find that visual behavior on this structure is not always consistent with that on source code.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664808},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A tale of two comprehensions? analyzing student programmer attention during code summarization},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A formal explainer for just-in-time defect predictions.
<em>TOSEM</em>, <em>33</em>(7), 1–31. (<a
href="https://doi.org/10.1145/3664809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just-in-Tim e (JIT) defect prediction has been proposed to help teams prioritize the limited resources on the most risky commits (or pull requests), yet it remains largely a black box, whose predictions are not explainable or actionable to practitioners. Thus, prior studies have applied various model-agnostic techniques to explain the predictions of JIT models. Yet, explanations generated from existing model-agnostic techniques are still not formally sound, robust, and actionable. In this article, we propose FoX , a Fo rmal e X plainer for JIT Defect Prediction, which builds on formal reasoning about the behavior of JIT defect prediction models and hence is able to provide provably correct explanations, which are additionally guaranteed to be minimal. Our experimental results show that FoX is able to efficiently generate provably correct, robust, and actionable explanations, while existing model-agnostic techniques cannot. Our survey study with 54 software practitioners provides valuable insights into the usefulness and trustworthiness of our FoX approach; 86% of participants agreed that our approach is useful, while 74% of participants found it trustworthy. Thus, this article serves as an important stepping stone towards trustable explanations for JIT models to help domain experts and practitioners better understand why a commit is predicted as defective and what to do to mitigate the risk.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664809},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A formal explainer for just-in-time defect predictions},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLMEffiChecker: Understanding and testing efficiency
degradation of large language models. <em>TOSEM</em>, <em>33</em>(7),
1–38. (<a href="https://doi.org/10.1145/3664812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker , which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker , we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs’ response latency and energy consumption by 325% to 3,244% and 344% to 3,616%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664812},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {8},
  number       = {7},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {LLMEffiChecker: Understanding and testing efficiency degradation of large language models},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepGD: A multi-objective black-box test selection approach
for deep neural networks. <em>TOSEM</em>, <em>33</em>(6), 1–29. (<a
href="https://doi.org/10.1145/3644388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. In particular, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this article, we propose DeepGD , a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault-revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by selecting diverse mispredicted inputs. The experimental results conducted on four widely used datasets and five DNN models show that in terms of fault-revealing ability, (1) white-box, coverage-based approaches fare poorly, (2) DeepGD outperforms existing black-box test selection approaches in terms of fault detection, and (3) DeepGD also leads to better guidance for DNN model retraining when using selected inputs to augment the training set.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3644388},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {DeepGD: A multi-objective black-box test selection approach for deep neural networks},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risky dynamic typing-related practices in python: An
empirical study. <em>TOSEM</em>, <em>33</em>(6), 1–35. (<a
href="https://doi.org/10.1145/3649593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Python’s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers’ high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models–based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3649593},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Risky dynamic typing-related practices in python: An empirical study},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data complexity: A new perspective for analyzing the
difficulty of defect prediction tasks. <em>TOSEM</em>, <em>33</em>(6),
1–45. (<a href="https://doi.org/10.1145/3649596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect prediction is crucial for software quality assurance and has been extensively researched over recent decades. However, prior studies rarely focus on data complexity in defect prediction tasks, and even less on understanding the difficulties of these tasks from the perspective of data complexity. In this article, we conduct an empirical study to estimate the hardness of over 33,000 instances, employing a set of measures to characterize the inherent difficulty of instances and the characteristics of defect datasets. Our findings indicate that: (1) instance hardness in both classes displays a right-skewed distribution, with the defective class exhibiting a more scattered distribution; (2) class overlap is the primary factor influencing instance hardness and can be characterized through feature, structural, and instance-level overlap; (3) no universal preprocessing technique is applicable to all datasets, and it may not consistently reduce data complexity, fortunately, dataset complexity measures can help identify suitable techniques for specific datasets; (4) integrating data complexity information into the learning process can enhance an algorithm’s learning capacity. In summary, this empirical study highlights the crucial role of data complexity in defect prediction tasks, and provides a novel perspective for advancing research in defect prediction techniques.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3649596},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-45},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Data complexity: A new perspective for analyzing the difficulty of defect prediction tasks},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing the impact of time evolution on source code
authorship attribution via domain adaptation. <em>TOSEM</em>,
<em>33</em>(6), 1–27. (<a
href="https://doi.org/10.1145/3652151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source code authorship attribution is an important problem in practical applications such as plagiarism detection, software forensics, and copyright disputes. Recent studies show that existing methods for source code authorship attribution can be significantly affected by time evolution, leading to a decrease in attribution accuracy year by year. To alleviate the problem of Deep Learning (DL)-based source code authorship attribution degrading in accuracy due to time evolution, we propose a new framework called Time D omain A daptation (TimeDA) by adding new feature extractors to the original DL-based code attribution framework that enhances the learning ability of the original model on source domain features without requiring new or more source data. Moreover, we employ a centroid-based pseudo-labeling strategy using neighborhood clustering entropy for adaptive learning to improve the robustness of DL-based code authorship attribution. Experimental results show that TimeDA can significantly enhance the robustness of DL-based source code authorship attribution to time evolution, with an average improvement of 8.7% on the Java dataset and 5.2% on the C++ dataset. In addition, our TimeDA benefits from employing the centroid-based pseudo-labeling strategy, which significantly reduced the model training time by 87.3% compared to traditional unsupervised domain adaptive methods.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3652151},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Reducing the impact of time evolution on source code authorship attribution via domain adaptation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do code summarization models process too much information?
Function signature may be all that is needed. <em>TOSEM</em>,
<em>33</em>(6), 1–35. (<a
href="https://doi.org/10.1145/3652156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast development of large software projects, automatic code summarization techniques, which summarize the main functionalities of a piece of code using natural languages as comments, play essential roles in helping developers understand and maintain large software projects. Many research efforts have been devoted to building automatic code summarization approaches. Typical code summarization approaches are based on deep learning models. They transform the task into a sequence-to-sequence task, which inputs source code and outputs summarizations in natural languages. All code summarization models impose different input size limits, such as 50 to 10,000, for the input source code. However, how the input size limit affects the performance of code summarization models still remains under-explored. In this article, we first conduct an empirical study to investigate the impacts of different input size limits on the quality of generated code comments. To our surprise, experiments on multiple models and datasets reveal that setting a low input size limit, such as 20, does not necessarily reduce the quality of generated comments. Based on this finding, we further propose to use function signatures instead of full source code to summarize the main functionalities first and then input the function signatures into code summarization models. Experiments and statistical results show that inputs with signatures are, on average, more than 2 percentage points better than inputs without signatures and thus demonstrate the effectiveness of involving function signatures in code summarization. We also invite programmers to do a questionnaire to evaluate the quality of code summaries generated by two inputs with different truncation levels. The results show that function signatures generate, on average, 9.2% more high-quality comments than full code.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3652156},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Do code summarization models process too much information? function signature may be all that is needed},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advanced white-box heuristics for search-based fuzzing of
REST APIs. <em>TOSEM</em>, <em>33</em>(6), 1–36. (<a
href="https://doi.org/10.1145/3652157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its importance and widespread use in industry, automated testing of REST APIs has attracted major interest from the research community in the last few years. However, most of the work in the literature has been focused on black-box fuzzing. Although existing fuzzers have been used to automatically find many faults in existing APIs, there are still several open research challenges that hinder the achievement of better results (e.g., in terms of code coverage and fault finding). For example, under-specified schemas are a major issue for black-box fuzzers. Currently, EvoMaster is the only existing tool that supports white-box fuzzing of REST APIs. In this paper, we provide a series of novel white-box heuristics, including for example how to deal with under-specified constrains in API schemas, as well as under-specified schemas in SQL databases. Our novel techniques are implemented as an extension to our open-source, search-based fuzzer EvoMaster . An empirical study on 14 APIs from the EMB corpus, plus one industrial API, shows clear improvements of the results in some of these APIs.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3652157},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Advanced white-box heuristics for search-based fuzzing of REST APIs},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Help them understand: Testing and improving voice user
interfaces. <em>TOSEM</em>, <em>33</em>(6), 1–33. (<a
href="https://doi.org/10.1145/3654438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers for building custom apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows the user to use natural language commands to perform actions. Testing such apps is not trivial: The same command can be expressed in different semantically equivalent ways. In this article, we introduce VUI-UPSET, an approach that adapts chatbot-testing approaches to VUI-testing. We conducted an empirical study to understand how VUI-UPSET compares to two state-of-the-art approaches (i.e., a chatbot testing technique and ChatGPT) in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. To this aim, we analyzed 14,898 generated paraphrases for 40 Alexa Skills. Our results show that VUI-UPSET generates more bug-revealing paraphrases than the two baselines with, however, ChatGPT being the approach generating the highest percentage of correct paraphrases. We also tried to use the generated paraphrases to improve the skills. We tried to include in the voice interaction models of the skills (i) only the bug-revealing paraphrases, (ii) all the valid paraphrases. We observed that including only bug-revealing paraphrases is sometimes not sufficient to make all the tests pass.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3654438},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Help them understand: Testing and improving voice user interfaces},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The IDEA of us: An identity-aware architecture for
autonomous systems. <em>TOSEM</em>, <em>33</em>(6), 1–38. (<a
href="https://doi.org/10.1145/3654439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous systems, such as drones and rescue robots, are increasingly used during emergencies. They deliver services and provide situational awareness that facilitate emergency management and response. To do so, they need to interact and cooperate with humans in their environment. Human behaviour is uncertain and complex, so it can be difficult to reason about it formally. In this article, we propose IDEA: an adaptive software architecture that enables cooperation between humans and autonomous systems, by leveraging the social identity approach. This approach establishes that group membership drives human behaviour. Identity and group membership are crucial during emergencies, as they influence cooperation among survivors. IDEA systems infer the social identity of surrounding humans, thereby establishing their group membership. By reasoning about groups, we limit the number of cooperation strategies the system needs to explore. IDEA systems select a strategy from the equilibrium analysis of game-theoretic models that represent interactions between group members and the IDEA system. We demonstrate our approach using a search-and-rescue scenario, in which an IDEA rescue robot optimises evacuation by collaborating with survivors. Using an empirically validated agent-based model, we show that the deployment of the IDEA system can reduce median evacuation time by 13.6%.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3654439},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {The IDEA of us: An identity-aware architecture for autonomous systems},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MTL-TRANSFER: Leveraging multi-task learning and transferred
knowledge for improving fault localization and program repair.
<em>TOSEM</em>, <em>33</em>(6), 1–31. (<a
href="https://doi.org/10.1145/3654441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault localization (FL) and automated program repair (APR) are two main tasks of automatic software debugging. Compared with traditional methods, deep learning-based approaches have been demonstrated to achieve better performance in FL and APR tasks. However, the existing deep learning-based FL methods ignore the deep semantic features or only consider simple code representations. And for APR tasks, existing template-based APR methods are weak in selecting the correct fix templates for more effective program repair, which are also not able to synthesize patches via the embedded end-to-end code modification knowledge obtained by training models on large-scale bug-fix code pairs. Moreover, in most of FL and APR methods, the model designs and training phases are performed separately, leading to ineffective sharing of updated parameters and extracted knowledge during the training process. This limitation hinders the further improvement in the performance of FL and APR tasks. To solve the above problems, we propose a novel approach called MTL-TRANSFER, which leverages a multi-task learning strategy to extract deep semantic features and transferred knowledge from different perspectives. First, we construct a large-scale open-source bug datasets and implement 11 multi-task learning models for bug detection and patch generation sub-tasks on 11 commonly used bug types, as well as one multi-classifier to learn the relevant semantics for the subsequent fix template selection task. Second, an MLP-based ranking model is leveraged to fuse spectrum-based, mutation-based and semantic-based features to generate a sorted list of suspicious statements. Third, we combine the patches generated by the neural patch generation sub-task from the multi-task learning strategy with the optimized fix template selecting order gained from the multi-classifier mentioned above. Finally, the more accurate FL results, the optimized fix template selecting order, and the expanded patch candidates are combined together to further enhance the overall performance of APR tasks. Our extensive experiments on widely-used benchmark Defects4J show that MTL-TRANSFER outperforms all baselines in FL and APR tasks, proving the effectiveness of our approach. Compared with our previously proposed FL method TRANSFER-FL (which is also the state-of-the-art statement-level FL method), MTL-TRANSFER increases the faults hit by 8/11/12 on Top-1/3/5 metrics (92/159/183 in total). And on APR tasks, the number of successfully repaired bugs of MTL-TRANSFER under the perfect localization setting reaches 75, which is 8 more than our previous APR method TRANSFER-PR. Furthermore, another experiment to simulate the actual repair scenarios shows that MTL-TRANSFER can successfully repair 15 and 9 more bugs (56 in total) compared with TBar and TRANSFER, which demonstrates the effectiveness of the combination of our optimized FL and APR components.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3654441},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {MTL-TRANSFER: Leveraging multi-task learning and transferred knowledge for improving fault localization and program repair},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the way to SBOMs: Investigating design issues and
solutions in practice. <em>TOSEM</em>, <em>33</em>(6), 1–25. (<a
href="https://doi.org/10.1145/3654442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase of software supply chain threats has underscored the necessity for robust security mechanisms, among which the Software Bill of Materials (SBOM) stands out as a promising solution. SBOMs, by providing a machine-readable inventory of software composition details, play a crucial role in enhancing transparency and traceability within software supply chains. This empirical study delves into the practical challenges and solutions associated with the adoption of SBOMs through an analysis of 4,786 GitHub discussions across 510 SBOM-related projects. Through repository mining and analysis, this research delineates key topics, challenges, and solutions intrinsic to the effective utilization of SBOMs. Furthermore, we shed light on commonly used tools and frameworks for SBOM generation, exploring their respective strengths and limitations. This study underscores a set of findings, for example, there are four phases of the SBOM life cycle, and each phase has a set of SBOM development activities and issues; in addition, this study emphasizes the role SBOM play in ensuring resilient software development practices and the imperative of their widespread adoption and integration to bolster supply chain security. The insights of our study provide vital input for future work and practical advancements in this topic.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3654442},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On the way to SBOMs: Investigating design issues and solutions in practice},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Early and realistic exploitability prediction of
just-disclosed software vulnerabilities: How reliable can it be?
<em>TOSEM</em>, <em>33</em>(6), 1–41. (<a
href="https://doi.org/10.1145/3654443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rate of discovered and disclosed vulnerabilities escalating, researchers have been experimenting with machine learning to predict whether a vulnerability will be exploited. Existing solutions leverage information unavailable when a CVE is created, making them unsuitable just after the disclosure. This paper experiments with early exploitability prediction models driven exclusively by the initial CVE record, i.e., the original description and the linked online discussions. Leveraging NVD and Exploit Database, we evaluate 72 prediction models trained using six traditional machine learning classifiers, four feature representation schemas, and three data balancing algorithms. We also experiment with five pre-trained large language models (LLMs). The models leverage seven different corpora made by combining three data sources, i.e., CVE description, Security Focus , and BugTraq . The models are evaluated in a realistic , time-aware fashion by removing the training and test instances that cannot be labeled “neutral” with sufficient confidence. The validation reveals that CVE descriptions and Security Focus discussions are the best data to train on. Pre-trained LLMs do not show the expected performance, requiring further pre-training in the security domain. We distill new research directions, identify possible room for improvement, and envision automated systems assisting security experts in assessing the exploitability.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3654443},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Early and realistic exploitability prediction of just-disclosed software vulnerabilities: How reliable can it be?},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On estimating the feasible solution space of multi-objective
testing resource allocation. <em>TOSEM</em>, <em>33</em>(6), 1–41. (<a
href="https://doi.org/10.1145/3654444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-objective testing resource allocation problem (MOTRAP) is concerned on how to reasonably plan the testing time of software testers to save the cost and improve the reliability as much as possible. The feasible solution space of a MOTRAP is determined by its variables (i.e., the time invested in each component) and constraints (e.g., the pre-specified reliability, cost, or time). Although a variety of state-of-the-art constrained multi-objective optimisers can be used to find individual solutions in this space, their search remains inefficient and expensive due to the fact that this space is very tiny compared to the large search space. The decision maker may often suffer a prolonged but unsuccessful search that fails to return a feasible solution. In this work, we first formulate a heavily constrained MOTRAP on the basis of an architecture-based model, in which reliability, cost, and time are optimised under the pre-specified multiple constraints on reliability, cost, and time. Then, to estimate the feasible solution space of this specific MOTRAP, we develop theoretical and algorithmic approaches to deduce new tighter lower and upper bounds on variables from constraints. Importantly, our approach can help the decision maker identify whether their constraint settings are practicable, and meanwhile, the derived bounds can just enclose the tiny feasible solution space and help off-the-shelf constrained multi-objective optimisers make the search within the feasible solution space as much as possible. Additionally, to further make good use of these bounds, we propose a generalised bound constraint handling method that can be readily employed by constrained multi-objective optimisers to pull infeasible solutions back into the estimated space with theoretical guarantee. Finally, we evaluate our approach on application and empirical cases. Experimental results reveal that our approach significantly enhances the efficiency, effectiveness, and robustness of off-the-shelf constrained multi-objective optimisers and state-of-the-art bound constraint handling methods at finding high-quality solutions for the decision maker. These improvements may help the decision maker take the stress out of setting constraints and selecting constrained multi-objective optimisers and facilitate the testing planning more efficiently and effectively.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3654444},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On estimating the feasible solution space of multi-objective testing resource allocation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the impact of lower recall and precision in defect
prediction for guiding search-based software testing. <em>TOSEM</em>,
<em>33</em>(6), 1–27. (<a
href="https://doi.org/10.1145/3655022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect predictors, static bug detectors, and humans inspecting the code can propose locations in the program that are more likely to be buggy before they are discovered through testing. Automated test generators such as search-based software testing (SBST) techniques can use this information to direct their search for test cases to likely buggy code, thus speeding up the process of detecting existing bugs in those locations. Often the predictions given by these tools or humans are imprecise, which can misguide the SBST technique and may deteriorate its performance. In this article, we study the impact of imprecision in defect prediction on the bug detection effectiveness of SBST. Our study finds that the recall of the defect predictor, i.e., the proportion of correctly identified buggy code, has a significant impact on bug detection effectiveness of SBST with a large effect size. More precisely, the SBST technique detects 7.5 fewer bugs on average (out of 420 bugs) for every 5% decrements of the recall. However, the effect of precision, a measure for false alarms, is not of meaningful practical significance, as indicated by a very small effect size. In the context of combining defect prediction and SBST, our recommendation is to increase the recall of defect predictors as a primary objective and precision as a secondary objective. In our experiments, we find that 75% precision is as good as 100% precision. To account for the imprecision of defect predictors, in particular low recall values, SBST techniques should be designed to search for test cases that also cover the predicted non-buggy parts of the program, while prioritising the parts that have been predicted as buggy.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3655022},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On the impact of lower recall and precision in defect prediction for guiding search-based software testing},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing multi-subroutine quantum programs: From unit testing
to integration testing. <em>TOSEM</em>, <em>33</em>(6), 1–61. (<a
href="https://doi.org/10.1145/3656339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing has emerged as a promising field with the potential to revolutionize various domains by harnessing the principles of quantum mechanics. As quantum hardware and algorithms continue to advance, developing high-quality quantum software has become crucial. However, testing quantum programs poses unique challenges due to the distinctive characteristics of quantum systems and the complexity of multi-subroutine programs. This article addresses the specific testing requirements of multi-subroutine quantum programs. We begin by investigating critical properties by surveying existing quantum libraries and providing insights into the challenges of testing these programs. Building upon this understanding, we focus on testing criteria and techniques based on the whole testing process perspective, spanning from unit testing to integration testing. We delve into various aspects, including IO analysis, quantum relation checking, structural testing, behavior testing, integration of subroutine pairs, and test case generation. We also introduce novel testing principles and criteria to guide the testing process. We conduct comprehensive testing on typical quantum subroutines, including diverse mutants and randomized inputs, to evaluate our proposed approach. The analysis of failures provides valuable insights into the effectiveness of our testing methodology. Additionally, we present case studies on representative multi-subroutine quantum programs, demonstrating the practical application and effectiveness of our proposed testing principles and criteria.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3656339},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-61},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Testing multi-subroutine quantum programs: From unit testing to integration testing},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MR-scout: Automated synthesis of metamorphic relations from
existing test cases. <em>TOSEM</em>, <em>33</em>(6), 1–28. (<a
href="https://doi.org/10.1145/3656340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metamorphic Testing (MT) alleviates the oracle problem by defining oracles based on metamorphic relations (MRs) that govern multiple related inputs and their outputs. However, designing MRs is challenging, as it requires domain-specific knowledge. This hinders the widespread adoption of MT. We observe that developer-written test cases can embed domain knowledge that encodes MRs. Such encoded MRs could be synthesized for testing not only their original programs but also other programs that share similar functionalities. In this article, we propose MR-Scout to automatically synthesize MRs from test cases in open-source software (OSS) projects. MR-Scout first discovers MR-encoded test cases (MTCs), and then synthesizes the encoded MRs into parameterized methods (called codified MRs ), and filters out MRs that demonstrate poor quality for new test case generation. MR-Scout discovered over 11,000 MTCs from 701 OSS projects. Experimental results show that over 97% of codified MRs are of high quality for automated test case generation, demonstrating the practical applicability of MR-Scout . Furthermore, codified-MRs-based tests effectively enhance the test adequacy of programs with developer-written tests, leading to 13.52% and 9.42% increases in line coverage and mutation score, respectively. Our qualitative study shows that 55.76% to 76.92% of codified MRs are easily comprehensible for developers.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3656340},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {MR-scout: Automated synthesis of metamorphic relations from existing test cases},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of source code search: A 3-dimensional perspective.
<em>TOSEM</em>, <em>33</em>(6), 1–51. (<a
href="https://doi.org/10.1145/3656341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {(Source) code search is widely concerned by software engineering researchers because it can improve the productivity and quality of software development. Given a functionality requirement usually described in a natural language sentence, a code search system can retrieve code snippets that satisfy the requirement from a large-scale code corpus, e.g., GitHub. To realize effective and efficient code search, many techniques have been proposed successively. These techniques improve code search performance mainly by optimizing three core components, including query understanding component, code understanding component, and query-code matching component. In this article, we provide a 3-dimensional perspective survey for code search. Specifically, we categorize existing code search studies into query-end optimization techniques, code-end optimization techniques, and match-end optimization techniques according to the specific components they optimize. These optimization techniques are proposed to enhance the performance of specific components, and thus the overall performance of code search. Considering that each end can be optimized independently and contributes to the code search performance, we treat each end as a dimension. Therefore, this survey is 3-dimensional in nature, and it provides a comprehensive summary of each dimension in detail. To understand the research trends of the three dimensions in existing code search studies, we systematically review 68 relevant literatures. Different from existing code search surveys that only focus on the query end or code end or introduce various aspects shallowly (including codebase, evaluation metrics, modeling technique, etc.), our survey provides a more nuanced analysis and review of the evolution and development of the underlying techniques used in the three ends. Based on a systematic review and summary of existing work, we outline several open challenges and opportunities at the three ends that remain to be addressed in future work.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3656341},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-51},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A survey of source code search: A 3-dimensional perspective},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BatFix: Repairing language model-based transpilation.
<em>TOSEM</em>, <em>33</em>(6), 1–29. (<a
href="https://doi.org/10.1145/3658668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To keep up with changes in requirements, frameworks, and coding practices, software organizations might need to migrate code from one language to another. Source-to-source migration, or transpilation, is often a complex, manual process. Transpilation requires expertise both in the source and target language, making it highly laborious and costly. Languages models for code generation and transpilation are becoming increasingly popular. However, despite capturing code-structure well, code generated by language models is often spurious and contains subtle problems. We propose BatFix , a novel approach that augments language models for transpilation by leveraging program repair and synthesis to fix the code generated by these models. BatFix takes as input both the original program, the target program generated by the machine translation model, and a set of test cases and outputs a repaired program that passes all test cases. Experimental results show that our approach is agnostic to language models and programming languages. BatFix can locate bugs spawning multiple lines and synthesize patches for syntax and semantic bugs for programs migrated from Java to C++ and Python to C++ from multiple language models, including, OpenAI’s Codex .},
  archive      = {J_TOSEM},
  doi          = {10.1145/3658668},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {BatFix: Repairing language model-based transpilation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Replication in requirements engineering: The NLP for RE
case. <em>TOSEM</em>, <em>33</em>(6), 1–33. (<a
href="https://doi.org/10.1145/3658669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Despite its empirical vocation, RE research has given limited attention to replication of NLP for RE studies. Replication is hampered by several factors, including the context specificity of the studies, the heterogeneity of the tasks involving NLP, the tasks’ inherent hairiness , and, in turn, the heterogeneous reporting structure. To address these issues, we propose a new artifact, referred to as ID-Card , whose goal is to provide a structured summary of research papers emphasizing replication-relevant information. We construct the ID-Card through a structured, iterative process based on design science. In this article: (i) we report on hands-on experiences of replication; (ii) we review the state-of-the-art and extract replication-relevant information: (iii) we identify, through focus groups, challenges across two typical dimensions of replication: data annotation and tool reconstruction; and (iv) we present the concept and structure of the ID-Card to mitigate the identified challenges. This study aims to create awareness of replication in NLP for RE. We propose an ID-Card that is intended to foster study replication but can also be used in other contexts, e.g., for educational purposes.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3658669},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Replication in requirements engineering: The NLP for RE case},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supporting emotional intelligence, productivity and team
goals while handling software requirements changes. <em>TOSEM</em>,
<em>33</em>(6), 1–38. (<a
href="https://doi.org/10.1145/3664600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Research shows that emotional intelligence (EI) should be used alongside cognitive intelligence during requirements change (RC) handling in Software Engineering (SE), especially in agile settings. Objective: We wanted to study the role of EI in-depth during RC handling. Method: We conducted a mixed-methods study (an interview study followed by a survey study) with 124 software practitioners. Findings: We found the causal condition, intervening condition and causes lead to key direct consequences of regulating own emotions, managing relationships, and extended consequences of sustaining productivity, setting and sustaining team goals. We found several strategies of supporting EI during RC handling. Further, we found strong correlations between six strategies and one being aware of own emotions, regulating own emotions, sustaining team productivity, and setting and sustaining team goals. Conclusion: Empathising with others and tracking commitments and decisions as a team are key strategies that have strong correlations between managing emotions, between sustaining team productivity, and between setting and sustaining team goals. To the best of our knowledge, the framework we present in this paper is the first theoretical framework on EI in SE research. We provide recommendations for software practitioners to consider during RC handling.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664600},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Supporting emotional intelligence, productivity and team goals while handling software requirements changes},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing updated apps by adapting learned models.
<em>TOSEM</em>, <em>33</em>(6), 1–40. (<a
href="https://doi.org/10.1145/3664601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although App updates are frequent and software engineers would like to verify updated features only, automated testing techniques verify entire Apps and are thus wasting resources. We present Continuous Adaptation of Learned Models (CALM) , an automated App testing approach that efficiently test App updates by adapting App models learned when automatically testing previous App versions. CALM focuses on functional testing. Since functional correctness can be mainly verified through the visual inspection of App screens, CALM minimizes the number of App screens to be visualized by software testers while maximizing the percentage of updated methods and instructions exercised. Our empirical evaluation shows that CALM exercises a significantly higher proportion of updated methods and instructions than six state-of-the-art approaches, for the same maximum number of App screens to be visually inspected. Further, in common update scenarios, where only a small fraction of methods are updated, CALM is even quicker to outperform all competing approaches in a more significant way.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664601},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-40},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Testing updated apps by adapting learned models},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep domain adaptation with max-margin principle for
cross-project imbalanced software vulnerability detection.
<em>TOSEM</em>, <em>33</em>(6), 1–34. (<a
href="https://doi.org/10.1145/3664602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software vulnerabilities (SVs) have become a common, serious, and crucial concern due to the ubiquity of computer software. Many AI-based approaches have been proposed to solve the software vulnerability detection (SVD) problem to ensure the security and integrity of software applications (in both the development and testing phases). However, there are still two open and significant issues for SVD in terms of (i) learning automatic representations to improve the predictive performance of SVD, and (ii) tackling the scarcity of labeled vulnerability datasets that conventionally need laborious labeling effort by experts. In this paper, we propose a novel approach to tackle these two crucial issues. We first exploit the automatic representation learning with deep domain adaptation for SVD. We then propose a novel cross-domain kernel classifier leveraging the max-margin principle to significantly improve the transfer learning process of SVs from imbalanced labeled into imbalanced unlabeled projects. Our approach is the first work that leverages solid body theories of the max-margin principle, kernel methods, and bridging the gap between source and target domains for imbalanced domain adaptation (DA) applied in cross-project SVD . The experimental results on real-world software datasets show the superiority of our proposed method over state-of-the-art baselines. In short, our method obtains a higher performance on F1-measure, one of the most important measures in SVD, from 1.83% to 6.25% compared to the second highest method in the used datasets.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664602},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Deep domain adaptation with max-margin principle for cross-project imbalanced software vulnerability detection},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic repair of quantum programs via unitary operation.
<em>TOSEM</em>, <em>33</em>(6), 1–43. (<a
href="https://doi.org/10.1145/3664604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of quantum computing (QC), the demand for high-quality quantum programs (QPs) is growing. To avoid program failure, in software engineering, the technology of automatic program repair (APR) employs appropriate patches to remove potential bugs without the intervention of a human. However, the method tailored for repairing defective QPs is still absent. This article proposes, to the best of our knowledge, a new APR method named UnitAR that can repair QPs via unitary operation automatically. Based on the characteristics of superposition and entanglement in QC, the article constructs an algebraic model and adopts a generate-and-validate approach for the repair procedure. Furthermore, the article presents two schemes that can respectively promote the efficiency of generating patches and guarantee the effectiveness of applying patches. For the purpose of evaluating the proposed method, the article selects 29 mutated versions as well as five real-world buggy programs as the objects and introduces two traditional APR approaches GenProg and TBar as baselines. According to the experiments, UnitAR can fix 23 buggy programs, and this method demonstrates the highest efficiency and effectiveness among three APR approaches. Besides, the experimental results further manifest the crucial roles of two constituents involved in the framework of UnitAR .},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664604},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automatic repair of quantum programs via unitary operation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Focused test generation for autonomous driving systems.
<em>TOSEM</em>, <em>33</em>(6), 1–32. (<a
href="https://doi.org/10.1145/3664605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing Autonomous Driving Systems (ADSs) is crucial to ensure their reliability when navigating complex environments. ADSs may exhibit unexpected behaviours when presented, during operation, with driving scenarios containing features inadequately represented in the training dataset. To address this shift from development to operation, developers must acquire new data with the newly observed features. This data can be then utilised to fine tune the ADS, so as to reach the desired level of reliability in performing driving tasks. However, the resource-intensive nature of testing ADSs requires efficient methodologies for generating targeted and diverse tests. In this work, we introduce a novel approach, DeepAtash-LR , that incorporates a surrogate model into the focused test generation process. This integration significantly improves focused testing effectiveness and applicability in resource-intensive scenarios. Experimental results show that the integration of the surrogate model is fundamental to the success of DeepAtash-LR . Our approach was able to generate an average of up to 60× more targeted, failure-inducing inputs compared to the baseline approach. Moreover, the inputs generated by DeepAtash-LR were useful to significantly improve the quality of the original ADS through fine tuning.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664605},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Focused test generation for autonomous driving systems},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobile application online cross-project just-in-time
software defect prediction framework. <em>TOSEM</em>, <em>33</em>(6),
1–31. (<a href="https://doi.org/10.1145/3664607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As mobile applications evolve rapidly, their fast iterative update nature leads to an increase in software defects. Just-In-Time Software Defect Prediction (JIT-SDP) offers immediate feedback on code changes. For new applications without historical data, researchers have proposed Cross-Project JIT-SDP (CP JIT-SDP). Existing CP JIT-SDP approaches are designed for offline scenarios where target data is available in advance. However, target data in real-world applications usually arrives online in a streaming manner, making online CP JIT-SDP face cross-project distribution differences and target project data concept drift challenges in online scenarios. These challenges often co-exist during application development, and their interactions cause model performance to degrade. To address these issues, we propose an online CP JIT-SDP framework called COTL. Specifically, COTL consists of two stages: offline and online. In the offline stage, the cross-domain structure preserving projection algorithm is used to reduce the cross-project distribution differences. In the online stage, target data arrives sequentially over time. By reducing the differences in marginal and conditional distributions between offline and online data for target project, concept drift is mitigated and classifier weights are updated online. Experimental results on 15 mobile application benchmark datasets show that COTL outperforms 13 benchmark methods on four performance metrics.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664607},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Mobile application online cross-project just-in-time software defect prediction framework},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness testing of machine translation systems.
<em>TOSEM</em>, <em>33</em>(6), 1–27. (<a
href="https://doi.org/10.1145/3664608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine translation is integral to international communication and extensively employed in diverse human-related applications. Despite remarkable progress, fairness issues persist within current machine translation systems. In this article, we propose FairMT, an automated fairness testing approach tailored for machine translation systems. FairMT operates on the assumption that translations of semantically similar sentences, containing protected attributes from distinct demographic groups, should maintain comparable meanings. It comprises three key steps: (1) test input generation, producing inputs covering various demographic groups; (2) test oracle generation, identifying potential unfair translations based on semantic similarity measurements; and (3) regression, discerning genuine fairness issues from those caused by low-quality translation. Leveraging FairMT, we conduct an empirical study on three leading machine translation systems–Google Translate, T5, and Transformer. Our investigation uncovers up to 832, 1,984, and 2,627 unfair translations across the three systems, respectively. Intriguingly, we observe that fair translations tend to exhibit superior translation performance, challenging the conventional wisdom of a fairness-performance tradeoff prevalent in the fairness literature.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664608},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Fairness testing of machine translation systems},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing GUI exploration coverage of android apps with deep
link-integrated monkey. <em>TOSEM</em>, <em>33</em>(6), 1–31. (<a
href="https://doi.org/10.1145/3664810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile apps are ubiquitous in our daily lives for supporting different tasks such as reading and chatting. Despite the availability of many GUI testing tools, app testers still struggle with low testing code coverage due to tools frequently getting stuck in loops or overlooking activities with concealed entries. This results in a significant amount of testing time being spent on redundant and repetitive exploration of a few GUI pages. To address this, we utilize Android’s deep links, which assist in triggering Android intents to lead users to specific pages and introduce a deep link-enhanced exploration method. This approach, integrated into the testing tool Monkey, gives rise to Delm (Deep Link-enhanced Monkey). Delm oversees the dynamic exploration process, guiding the tool out of meaningless testing loops to unexplored GUI pages. We provide a rigorous activity context mock-up approach for triggering existing Android intents to discover more activities with hidden entrances. We conduct experiments to evaluate Delm’s effectiveness on activity context mock-up, activity coverage, method coverage, and crash detection. The findings reveal that Delm can mock up more complex activity contexts and significantly outperform state-of-the-art baselines with 27.2% activity coverage, 21.13% method coverage, and 23.81% crash detection.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664810},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Enhancing GUI exploration coverage of android apps with deep link-integrated monkey},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What makes a good TODO comment? <em>TOSEM</em>,
<em>33</em>(6), 1–30. (<a
href="https://doi.org/10.1145/3664811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development is a collaborative process that involves various interactions among individuals and teams. TODO comments in source code play a critical role in managing and coordinating diverse tasks during this process. However, this study finds that a large proportion of open-source project TODO comments are left unresolved or take a long time to be resolved. About 46.7% of TODO comments in open-source repositories are of low-quality (e.g., TODOs that are ambiguous, lack information, or are useless to developers). This highlights the need for better TODO practices. In this study, we investigate four aspects regarding the quality of TODO comments in open-source projects: (1) the prevalence of low-quality TODO comments; (2) the key characteristics of high-quality TODO comments; (3) how are TODO comments of different quality managed in practice; and (4) the feasibility of automatically assessing TODO comment quality. Examining 2,863 TODO comments from Top100 GitHub Java repositories, we propose criteria to identify high-quality TODO comments and provide insights into their optimal composition. We discuss the lifecycle of TODO comments with varying quality. To assist developers, we construct deep learning-based methods that show promising performance in identifying the quality of TODO comments, potentially enhancing development efficiency and code quality.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3664811},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {6},
  pages        = {1-30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {What makes a good TODO comment?},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Fine-grained coverage-based fuzzing - RCR report.
<em>TOSEM</em>, <em>33</em>(5), 139:1–4. (<a
href="https://doi.org/10.1145/3649592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is the RCR report of the artifact for the article “Fine-grained Coverage-based Fuzzing.” This report contains scripts and pre-build binary programs to reproduce the results presented in the main article. The artifact is released on Zenodo with DOI: 10.5281/zenodo.7275184. We claim the artifact to be available, functional, and reusable. The technology skills needed to review the artifact are knowing how to use Linux/Unix terminal and a basic understanding of Docker.},
  archive      = {J_TOSEM},
  author       = {Wei-Cheng Wu and Bernard Nongpoh and Marwan Nour and Michaël Marcozzi and Sébastien Bardin and Christophe Hauser},
  doi          = {10.1145/3649592},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {139:1–4},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Fine-grained coverage-based fuzzing - RCR report},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Fine-grained coverage-based fuzzing. <em>TOSEM</em>,
<em>33</em>(5), 138:1–41. (<a
href="https://doi.org/10.1145/3587158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzing is a popular software testing method that discovers bugs by massively feeding target applications with automatically generated inputs. Many state-of-the-art fuzzers use branch coverage as a feedback metric to guide the fuzzing process. The fuzzer retains inputs for further mutation only if branch coverage is increased. However, branch coverage only provides a shallow sampling of program behaviors and hence may discard interesting inputs to mutate. This work aims to take advantage of the large body of research in defining finer-grained code coverage metrics (such as control-flow, data-flow, or mutation coverage) and to evaluate how fuzzing performance is impacted when using these metrics to select interesting inputs for mutation. We propose to make branch coverage-based fuzzers support most fine-grained coverage metrics out of the box (i.e., without changing fuzzer internals). We achieve this by making the test objectives defined by these metrics (such as conditions to activate or mutants to kill) explicit as new branches in the target program. Fuzzing such a modified target is then equivalent to fuzzing the original target, but the fuzzer will also retain inputs covering the additional metric objectives for mutation. In addition, all the fuzzer mechanisms to penetrate hard-to-cover branches will help in covering the additional metric objectives. We use this approach to evaluate the impact of supporting two fine-grained coverage metrics (multiple condition coverage and weak mutation) over the performance of two state-of-the-art fuzzers (AFL++ and QSYM) with the standard LAVA-M and MAGMA benchmarks. This evaluation suggests that our mechanism for runtime fuzzer guidance, where the fuzzed code is instrumented with additional branches, is effective and could be leveraged to encode guidance from human users or static analyzers. Our results also show that the impact of fine-grained metrics over fuzzing performance is hard to predict before fuzzing and most of the time either neutral or negative. As a consequence, we do not recommend using them to guide fuzzers, except maybe in some possibly favorable circumstances yet to be investigated, like for limited parts of the code or to complement classical fuzzing campaigns.},
  archive      = {J_TOSEM},
  author       = {Wei-Cheng Wu and Bernard Nongpoh and Marwan Nour and Michaël Marcozzi and Sébastien Bardin and Christophe Hauser},
  doi          = {10.1145/3587158},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {138:1–41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Fine-grained coverage-based fuzzing},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness testing: A comprehensive survey and analysis of
trends. <em>TOSEM</em>, <em>33</em>(5), 137:1–59. (<a
href="https://doi.org/10.1145/3652155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unfair behaviors of Machine Learning (ML) software have garnered increasing attention and concern among software engineers. To tackle this issue, extensive research has been dedicated to conducting fairness testing of ML software, and this article offers a comprehensive survey of existing studies in this field. We collect 100 papers and organize them based on the testing workflow (i.e., how to test) and testing components (i.e., what to test). Furthermore, we analyze the research focus, trends, and promising directions in the realm of fairness testing. We also identify widely adopted datasets and open-source tools for fairness testing.},
  archive      = {J_TOSEM},
  author       = {Zhenpeng Chen and Jie M. Zhang and Max Hort and Mark Harman and Federica Sarro},
  doi          = {10.1145/3652155},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {137:1–59},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Fairness testing: A comprehensive survey and analysis of trends},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lessons learned from developing a sustainability awareness
framework for software engineering using design science. <em>TOSEM</em>,
<em>33</em>(5), 136:1–39. (<a
href="https://doi.org/10.1145/3649597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To foster a sustainable society within a sustainable environment, we must dramatically reshape our work and consumption activities, most of which are facilitated through software. Yet, most software engineers hardly consider the effects on the sustainability of the IT products and services they deliver. This issue is exacerbated by a lack of methods and tools for this purpose. Despite the practical need for methods and tools that explicitly support consideration of the effects that IT products and services have on the sustainability of their intended environments, such methods and tools remain largely unavailable. Thus, urgent research is needed to understand how to design such tools for the IT community properly. In this article, we describe our experience using design science to create the Sustainability Awareness Framework (SusAF), which supports software engineers in anticipating and mitigating the potential sustainability effects during system development. More specifically, we identify and present the challenges faced during this process. The challenges that we have faced and addressed in the development of the SusAF are likely to be relevant to others who aim to create methods and tools to integrate sustainability analysis into their IT products and services development. Thus, the lessons learned in SusAF development are shared for the benefit of researchers and other professionals who design tools for that end.},
  archive      = {J_TOSEM},
  author       = {Stefanie Betz and Birgit Penzenstadler and Leticia Duboc and Ruzanna Chitchyan and Sedef Akinli Kocak and Ian Brooks and Shola Oyedeji and Jari Porras and Norbert Seyff and Colin C. Venters},
  doi          = {10.1145/3649597},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {136:1–39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Lessons learned from developing a sustainability awareness framework for software engineering using design science},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Navigating the complexity of generative AI adoption in
software engineering. <em>TOSEM</em>, <em>33</em>(5), 135:1–50. (<a
href="https://doi.org/10.1145/3652154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores the adoption of Generative Artificial Intelligence (AI) tools within the domain of software engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology Acceptance Model, the Diffusion of Innovation Theory, and the Social Cognitive Theory as guiding theoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares–Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies.},
  archive      = {J_TOSEM},
  author       = {Daniel Russo},
  doi          = {10.1145/3652154},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {135:1–50},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Navigating the complexity of generative AI adoption in software engineering},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated mapping of vulnerability advisories onto their fix
commits in open source repositories. <em>TOSEM</em>, <em>33</em>(5),
134:1–28. (<a href="https://doi.org/10.1145/3649590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of comprehensive sources of accurate vulnerability data represents a critical obstacle to studying and understanding software vulnerabilities (and their corrections). In this article, we present an approach that combines heuristics stemming from practical experience and machine-learning (ML)—specifically, natural language processing (NLP)—to address this problem. Our method consists of three phases. First, we construct an advisory record object containing key information about a vulnerability that is extracted from an advisory, such as those found in the National Vulnerability Database (NVD). These advisories are expressed in natural language. Second, using heuristics, a subset of candidate fix commits is obtained from the source code repository of the affected project, by filtering out commits that can be identified as unrelated to the vulnerability at hand. Finally, for each of the remaining candidate commits, our method builds a numerical feature vector reflecting the characteristics of the commit that are relevant to predicting its match with the advisory at hand. Based on the values of these feature vectors, our method produces a ranked list of candidate fixing commits. The score attributed by the ML model to each feature is kept visible to the users, allowing them to easily interpret the predictions. We implemented our approach and we evaluated it on an open data set, built by manual curation, that comprises 2,391 known fix commits corresponding to 1,248 public vulnerability advisories. When considering the top-10 commits in the ranked results, our implementation could successfully identify at least one fix commit for up to 84.03% of the vulnerabilities (with a fix commit on the first position for 65.06% of the vulnerabilities). Our evaluation shows that our method can reduce considerably the manual effort needed to search open-source software (OSS) repositories for the commits that fix known vulnerabilities.},
  archive      = {J_TOSEM},
  author       = {Daan Hommersom and Antonino Sabetta and Bonaventura Coppola and Dario Di Nucci and Damian A. Tamburri},
  doi          = {10.1145/3649590},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {134:1–28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automated mapping of vulnerability advisories onto their fix commits in open source repositories},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KADEL: Knowledge-aware denoising learning for commit message
generation. <em>TOSEM</em>, <em>33</em>(5), 133:1–32. (<a
href="https://doi.org/10.1145/3643675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance. However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not. On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation. Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL. Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits. To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits. This knowledge model enables supplementing more information for training samples that do not conform to good practice. However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method. This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process. Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods.},
  archive      = {J_TOSEM},
  author       = {Wei Tao and Yucheng Zhou and Yanlin Wang and Hongyu Zhang and Haofen Wang and Wenqiang Zhang},
  doi          = {10.1145/3643675},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {133:1–32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {KADEL: Knowledge-aware denoising learning for commit message generation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Test input prioritization for 3D point clouds.
<em>TOSEM</em>, <em>33</em>(5), 132:1–44. (<a
href="https://doi.org/10.1145/3643676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud applications have become increasingly prevalent in diverse domains, showcasing their efficacy in various software systems. However, testing such applications presents unique challenges due to the high-dimensional nature of 3D point cloud data and the vast number of possible test cases. Test input prioritization has emerged as a promising approach to enhance testing efficiency by prioritizing potentially misclassified test cases during the early stages of the testing process. Consequently, this enables the early labeling of critical inputs, leading to a reduction in the overall labeling cost. However, applying existing prioritization methods to 3D point cloud data is constrained by several factors: (1) inadequate consideration of crucial spatial information, and (2) susceptibility to noises inherent in 3D point cloud data. In this article, we propose PCPrior , the first test prioritization approach specifically designed for 3D point cloud test cases. The fundamental concept behind PCPrior is that test inputs closer to the decision boundary of the model are more likely to be predicted incorrectly. To capture the spatial relationship between a point cloud test and the decision boundary, we propose transforming each test (a point cloud) into a low-dimensional feature vector, toward indirectly revealing the underlying proximity between a test and the decision boundary. To achieve this, we carefully design a group of feature generation strategies, and for each test input, we generate four distinct types of features, namely spatial features, mutation features, prediction features, and uncertainty features. Through a concatenation of the four feature types, PCPrior assembles a final feature vector for each test. Subsequently, a ranking model is employed to estimate the probability of misclassification for each test based on its feature vector. Finally, PCPrior ranks all tests based on their misclassification probabilities. We conducted an extensive study based on 165 subjects to evaluate the performance of PCPrior, encompassing both natural and noisy datasets. The results demonstrate that PCPrior outperforms all of the compared test prioritization approaches, with an average improvement of 10.99% to 66.94% on natural datasets and 16.62% to 53% on noisy datasets.},
  archive      = {J_TOSEM},
  author       = {Yinghua Li and Xueqi Dang and Lei Ma and Jacques Klein and Yves Le Traon and Tegawendé F. Bissyandé},
  doi          = {10.1145/3643676},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {132:1–44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Test input prioritization for 3D point clouds},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing and detecting information types of developer live
chat threads. <em>TOSEM</em>, <em>33</em>(5), 131:1–32. (<a
href="https://doi.org/10.1145/3643677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online chatrooms serve as vital platforms for information exchange among software developers. With multiple developers engaged in rapid communication and diverse conversation topics, the resulting chat messages often manifest complexity and lack structure. To enhance the efficiency of extracting information from chat threads , automatic mining techniques are introduced for thread classification. However, previous approaches still grapple with unsatisfactory classification accuracy due to two primary challenges that they struggle to adequately capture long-distance dependencies within chat threads and address the issue of category imbalance in labeled datasets. To surmount these challenges, we present a topic classification approach for chat information types named EAEChat. Specifically, EAEChat comprises three core components: the text feature encoding component captures contextual text features using a multi-head self-attention mechanism-based text feature encoder, and a siamese network is employed to mitigate overfitting caused by limited data; the data augmentation component expands a small number of categories in the training dataset using a technique tailored to developer chat messages, effectively tackling the challenge of imbalanced category distribution; the non-text feature encoding component employs a feature fusion model to integrate deep text features with manually extracted non-text features. Evaluation across three real-world projects demonstrates that EAEChat, respectively, achieves an average precision, recall, and F1-score of 0.653, 0.651, and 0.644, and it marks a significant 7.60% improvement over the state-of-the-art approaches. These findings confirm the effectiveness of our method in proficiently classifying developer chat messages in online chatrooms.},
  archive      = {J_TOSEM},
  author       = {Xiuwei Shang and Shuai Zhang and Yitong Zhang and Shikai Guo and Yulong Li and Rong Chen and Hui Li and Xiaochen Li and He Jiang},
  doi          = {10.1145/3643677},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {131:1–32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Analyzing and detecting information types of developer live chat threads},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supporting safety analysis of image-processing DNNs through
clustering-based approaches. <em>TOSEM</em>, <em>33</em>(5), 130:1–48.
(<a href="https://doi.org/10.1145/3643671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work. In this article, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively capturing images of the same failure scenario, thus facilitating root cause analysis. Further, it generates distinct clusters for each root cause of failure, thus enabling engineers to detect all the unsafe scenarios. Interestingly, these results hold even for failure scenarios that are only observed in a small percentage of the failing images.},
  archive      = {J_TOSEM},
  author       = {Mohammed Oualid Attaoui and Hazem Fahmy and Fabrizio Pastore and Lionel Briand},
  doi          = {10.1145/3643671},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {130:1–48},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Supporting safety analysis of image-processing DNNs through clustering-based approaches},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Abstraction and refinement: Towards scalable and exact
verification of neural networks. <em>TOSEM</em>, <em>33</em>(5),
129:1–35. (<a href="https://doi.org/10.1145/3644387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new programming paradigm, deep neural networks (DNNs) have been increasingly deployed in practice, but the lack of robustness hinders their applications in safety-critical domains. While there are techniques for verifying DNNs with formal guarantees, they are limited in scalability and accuracy. In this article, we present a novel counterexample-guided abstraction refinement (CEGAR) approach for scalable and exact verification of DNNs. Specifically, we propose a novel abstraction to break down the size of DNNs by over-approximation. The result of verifying the abstract DNN is conclusive if no spurious counterexample is reported. To eliminate each spurious counterexample introduced by abstraction, we propose a novel counterexample-guided refinement that refines the abstract DNN to exclude the spurious counterexample while still over-approximating the original one, leading to a sound, complete yet efficient CEGAR approach. Our approach is orthogonal to and can be integrated with many existing verification techniques. For demonstration, we implement our approach using two promising tools, Marabou and Planet , as the underlying verification engines, and evaluate on widely used benchmarks for three datasets ACAS , Xu , MNIST , and CIFAR-10 . The results show that our approach can boost their performance by solving more problems in the same time limit, reducing on average 13.4%–86.3% verification time of Marabou on almost all the verification tasks, and reducing on average 8.3%–78.0% verification time of Planet on all the verification tasks. Compared to the most relevant CEGAR-based approach, our approach is 11.6–26.6 times faster.},
  archive      = {J_TOSEM},
  author       = {Jiaxiang Liu and Yunhan Xing and Xiaomu Shi and Fu Song and Zhiwu Xu and Zhong Ming},
  doi          = {10.1145/3644387},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {129:1–35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Abstraction and refinement: Towards scalable and exact verification of neural networks},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RAPID: Zero-shot domain adaptation for code search with
pre-trained models. <em>TOSEM</em>, <em>33</em>(5), 128:1–35. (<a
href="https://doi.org/10.1145/3641542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code search, which refers to the process of identifying the most relevant code snippets for a given natural language query, plays a crucial role in software maintenance. However, current approaches heavily rely on labeled data for training, which results in performance decreases when confronted with cross-domain scenarios including domain- or project-specific situations. This decline can be attributed to their limited ability to effectively capture the semantics associated with such scenarios. To tackle the aforementioned problem, we propose a ze R o-shot dom A in ada P tion with pre-tra I ned mo D els framework for code search named RAPID. The framework first generates synthetic data by pseudo labeling, then trains the CodeBERT with sampled synthetic data. To avoid the influence of noisy synthetic data and enhance the model performance, we propose a mixture sampling strategy to obtain hard negative samples during training. Specifically, the mixture sampling strategy considers both relevancy and diversity to select the data that are hard to be distinguished by the models. To validate the effectiveness of our approach in zero-shot settings, we conduct extensive experiments and find that RAPID outperforms the CoCoSoDa and UniXcoder model by an average of 15.7% and 10%, respectively, as measured by the MRR metric. When trained on full data, our approach results in an average improvement of 7.5% under the MRR metric using CodeBERT. We observe that as the model’s performance in zero-shot tasks improves, the impact of hard negatives diminishes. Our observation also indicates that fine-tuning CodeT5 for generating pseudo labels can enhance the performance of the code search model, and using only 100-shot samples can yield comparable results to the supervised baseline. Furthermore, we evaluate the effectiveness of RAPID in real-world code search tasks in three GitHub projects through both human and automated assessments. Our findings reveal RAPID exhibits superior performance, e.g., an average improvement of 18% under the MRR metric over the top-performing model.},
  archive      = {J_TOSEM},
  author       = {Guodong Fan and Shizhan Chen and Cuiyun Gao and Jianmao Xiao and Tao Zhang and Zhiyong Feng},
  doi          = {10.1145/3641542},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {128:1–35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {RAPID: Zero-shot domain adaptation for code search with pre-trained models},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond fidelity: Explaining vulnerability localization of
learning-based detectors. <em>TOSEM</em>, <em>33</em>(5), 127:1–33. (<a
href="https://doi.org/10.1145/3641543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vulnerability detectors based on deep learning (DL) models have proven their effectiveness in recent years. However, the shroud of opacity surrounding the decision-making process of these detectors makes it difficult for security analysts to comprehend. To address this, various explanation approaches have been proposed to explain the predictions by highlighting important features, which have been demonstrated effective in domains such as computer vision and natural language processing. Unfortunately, there is still a lack of in-depth evaluation of vulnerability-critical features, such as fine-grained vulnerability-related code lines, learned and understood by these explanation approaches. In this study, we first evaluate the performance of ten explanation approaches for vulnerability detectors based on graph and sequence representations, measured by two quantitative metrics including fidelity and vulnerability line coverage rate. Our results show that fidelity alone is insufficent for evaluating these approaches, as fidelity incurs significant fluctuations across different datasets and detectors. We subsequently check the precision of the vulnerability-related code lines reported by the explanation approaches, and find poor accuracy in this task among all of them. This can be attributed to the inefficiency of explainers in selecting important features and the presence of irrelevant artifacts learned by DL-based detectors.},
  archive      = {J_TOSEM},
  author       = {Baijun Cheng and Shengming Zhao and Kailong Wang and Meizhen Wang and Guangdong Bai and Ruitao Feng and Yao Guo and Lei Ma and Haoyu Wang},
  doi          = {10.1145/3641543},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {127:1–33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Beyond fidelity: Explaining vulnerability localization of learning-based detectors},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the reliability and explainability of language models for
program generation. <em>TOSEM</em>, <em>33</em>(5), 126:1–26. (<a
href="https://doi.org/10.1145/3641540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have adopted pre-trained language models, such as CodeT5 and CodeGPT, for automated program generation tasks like code generation, repair, and translation. Numerous language model based approaches have been proposed and evaluated on various benchmark datasets, demonstrating promising performance. However, there is still uncertainty about the reliability of these models, particularly their realistic ability to consistently transform code sequences. This raises a question: are these techniques sufficiently trustworthy for automated program generation? Consequently, further research is needed to understand model logic and assess reliability and explainability. To bridge these research gaps, we conduct a thorough empirical study of eight popular language models on five representative datasets to determine the capabilities and limitations of automated program generation approaches. We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code transformation. We discover that state-of-the-art approaches suffer from inappropriate performance evaluation stemming from severe data duplication, causing overoptimistic results. Our explainability analysis reveals that, in various experimental scenarios, language models can recognize code grammar and structural information, but they exhibit limited robustness to changes in input sequences. Overall, more rigorous evaluation approaches and benchmarks are critical to enhance the reliability and explainability of automated program generation moving forward. Our findings provide important guidelines for this goal.},
  archive      = {J_TOSEM},
  author       = {Yue Liu and Chakkrit Tantithamthavorn and Yonghui Liu and Li Li},
  doi          = {10.1145/3641540},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {126:1–26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On the reliability and explainability of language models for program generation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine translation testing via syntactic tree pruning.
<em>TOSEM</em>, <em>33</em>(5), 125:1–39. (<a
href="https://doi.org/10.1145/3640329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine translation systems have been widely adopted in our daily life, making life easier and more convenient. Unfortunately, erroneous translations may result in severe consequences, such as financial losses. This requires to improve the accuracy and the reliability of machine translation systems. However, it is challenging to test machine translation systems because of the complexity and intractability of the underlying neural models. To tackle these challenges, we propose a novel metamorphic testing approach by syntactic tree pruning (STP) to validate machine translation systems. Our key insight is that a pruned sentence should have similar crucial semantics compared with the original sentence. Specifically, STP (1) proposes a core semantics-preserving pruning strategy by basic sentence structures and dependency relations on the level of syntactic tree representation, (2) generates source sentence pairs based on the metamorphic relation, and (3) reports suspicious issues whose translations break the consistency property by a bag-of-words model. We further evaluate STP on two state-of-the-art machine translation systems (i.e., Google Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs. The results show that STP accurately finds 5,073 unique erroneous translations in Google Translate and 5,100 unique erroneous translations in Bing Microsoft Translator (400% more than state-of-the-art techniques), with 64.5% and 65.4% precision, respectively. The reported erroneous translations vary in types and more than 90% of them are not found by state-of-the-art techniques. There are 9,393 erroneous translations unique to STP, which is 711.9% more than state-of-the-art techniques. Moreover, STP is quite effective in detecting translation errors for the original sentences with a recall reaching 74.0%, improving state-of-the-art techniques by 55.1% on average.},
  archive      = {J_TOSEM},
  author       = {Quanjun Zhang and Juan Zhai and Chunrong Fang and Jiawei Liu and Weisong Sun and Haichuan Hu and Qingyu Wang},
  doi          = {10.1145/3640329},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {125:1–39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Machine translation testing via syntactic tree pruning},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompt sapper: A LLM-empowered production tool for building
AI chains. <em>TOSEM</em>, <em>33</em>(5), 124:1–24. (<a
href="https://doi.org/10.1145/3638247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, Prompt Sapper , which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper.},
  archive      = {J_TOSEM},
  author       = {Yu Cheng and Jieshan Chen and Qing Huang and Zhenchang Xing and Xiwei Xu and Qinghua Lu},
  doi          = {10.1145/3638247},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {124:1–24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Prompt sapper: A LLM-empowered production tool for building AI chains},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating python type annotations from type inference: How
far are we? <em>TOSEM</em>, <em>33</em>(5), 123:1–38. (<a
href="https://doi.org/10.1145/3652153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, dynamic languages such as Python have become popular due to their flexibility and productivity. The lack of static typing makes programs face the challenges of fixing type errors, early bug detection, and code understanding. To alleviate these issues, PEP 484 introduced optional type annotations for Python in 2014, but unfortunately, a large number of programs are still not annotated by developers. Annotation generation tools can utilize type inference techniques. However, several important aspects of type annotation generation are overlooked by existing works, such as in-depth effectiveness analysis, potential improvement exploration, and practicality evaluation. And it is unclear how far we have been and how far we can go. In this paper, we set out to comprehensively investigate the effectiveness of type inference tools for generating type annotations, applying three categories of state-of-the-art tools on a carefully-cleaned dataset. First, we use a comprehensive set of metrics and categories, finding that existing tools have different effectiveness and cannot achieve both high accuracy and high coverage. Then, we summarize six patterns to present the limitations in type annotation generation. Next, we implement a simple but effective tool to demonstrate that existing tools can be improved in practice. Finally, we conduct a controlled experiment showing that existing tools can reduce the time spent annotating types and determine more precise types, but cannot reduce subjective difficulty. Our findings point out the limitations and improvement directions in type annotation generation, which can inspire future work.},
  archive      = {J_TOSEM},
  author       = {Yimeng Guo and Zhifei Chen and Lin Chen and Wenjie Xu and Yanhui Li and Yuming Zhou and Baowen Xu},
  doi          = {10.1145/3652153},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {123:1–38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Generating python type annotations from type inference: How far are we?},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DinoDroid: Testing android apps using deep q-networks.
<em>TOSEM</em>, <em>33</em>(5), 122:1–24. (<a
href="https://doi.org/10.1145/3652150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers need to guarantee the quality of mobile apps before it is released to the market. There have been many approaches using different strategies to test the GUI of mobile apps. However, they still need improvement due to their limited effectiveness. In this article, we propose DinoDroid, an approach based on deep Q-networks to automate testing of Android apps. DinoDroid learns a behavior model from a set of existing apps and the learned model can be used to explore and generate tests for new apps. DinoDroid is able to capture the fine-grained details of GUI events (e.g., the content of GUI widgets) and use them as features that are fed into deep neural network, which acts as the agent to guide app exploration. DinoDroid automatically adapts the learned model during the exploration without the need of any modeling strategies or pre-defined rules. We conduct experiments on 64 open-source Android apps. The results showed that DinoDroid outperforms existing Android testing tools in terms of code coverage and bug detection.},
  archive      = {J_TOSEM},
  author       = {Yu Zhao and Brent Harrison and Tingting Yu},
  doi          = {10.1145/3652150},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {122:1–24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {DinoDroid: Testing android apps using deep Q-networks},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Precisely extracting complex variable values from android
apps. <em>TOSEM</em>, <em>33</em>(5), 121:1–56. (<a
href="https://doi.org/10.1145/3649591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millions of users nowadays rely on their smartphones to process sensitive data through apps from various vendors and sources. Therefore, it is vital to assess these apps for security vulnerabilities and privacy violations. Information such as to which server an app connects through which protocol, and which algorithm it applies for encryption, are usually encoded as variable values and arguments of API calls. However, extracting these values from an app is not trivial. The source code of an app is usually not available, and manual reverse engineering is cumbersome with binary sizes in the tens of megabytes. Current automated tools, however, cannot retrieve values that are computed at runtime through complex transformations. In this article, we present ValDroid , a novel static analysis tool for automatically extracting the set of possible values for a given variable at a given statement in the Dalvik byte code of an Android app. We evaluate ValDroid against existing approaches (JSA, Violist, DroidRA, Harvester, BlueSeal, StringHound, IC3, and COAL) on benchmarks and 794 real-world apps. ValDroid greatly outperforms existing tools. It provides an average F 1 score of more than 90%, while only requiring 0.1 s per value on average. For many data types including Network Connections and Dynamic Code Loading, its recall is more than twice the recall of the best existing approaches.},
  archive      = {J_TOSEM},
  author       = {Marc Miltenberger and Steven Arzt},
  doi          = {10.1145/3649591},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {121:1–56},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Precisely extracting complex variable values from android apps},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-autoregressive line-level code completion.
<em>TOSEM</em>, <em>33</em>(5), 120:1–34. (<a
href="https://doi.org/10.1145/3649594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software developers frequently use code completion tools to accelerate software development by suggesting the following code elements. Researchers usually employ AutoRegressive (AR) decoders to complete code sequences in a left-to-right, token-by-token fashion. To improve the accuracy and efficiency of code completion, we argue that tokens within a code statement have the potential to be predicted concurrently. In this article, we first conduct an empirical study to analyze the dependency among the target tokens in line-level code completion. The results suggest that it is potentially practical to generate all statement tokens in parallel. To this end, we introduce SANAR, a simple and effective syntax-aware non-autoregressive model for line-level code completion. To further improve the quality of the generated code, we propose an adaptive and syntax-aware sampling strategy to boost the model’s performance. The experimental results obtained from two widely used datasets indicate that our model outperforms state-of-the-art code completion approaches of similar model size by a considerable margin, and is faster than these models with up to 9× speed-up. Moreover, the extensive results additionally demonstrate that the enhancements achieved by SANAR become even more pronounced with larger model sizes, highlighting their significance.},
  archive      = {J_TOSEM},
  author       = {Fang Liu and Zhiyi Fu and Ge Li and Zhi Jin and Hui Liu and Yiyang Hao and Li Zhang},
  doi          = {10.1145/3649594},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {120:1–34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Non-autoregressive line-level code completion},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic transitive closure-based static analysis through the
lens of quantum search. <em>TOSEM</em>, <em>33</em>(5), 119:1–29. (<a
href="https://doi.org/10.1145/3644389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing static analysis algorithms suffer from cubic bottlenecks because of the need to compute a dynamic transitive closure (DTC). For the first time, this article studies the quantum speedups on searching subtasks in DTC-based static analysis algorithms using quantum search (e.g., Grover’s algorithm). We first introduce our oracle implementation in Grover’s algorithm for DTC-based static analysis and illustrate our quantum search subroutine. Then, we take two typical DTC-based analysis algorithms: context-free-language reachability and set constraint-based analysis, and show that our quantum approach can reduce the time complexity of these two algorithms to truly subcubic ( \(O(N^2\sqrt {N}{\it polylog}(N))\)), yielding better results than the upper bound ( O ( N 3 /log N )) of existing classical algorithms. Finally, we conducted a classical simulation of Grover’s search to validate our theoretical approach, due to the current quantum hardware limitation of lacking a practical, large-scale, noise-free quantum machine. We evaluated the correctness and efficiency of our approach using IBM Qiskit on nine open-source projects and randomly generated edge-labeled graphs/constraints. The results demonstrate the effectiveness of our approach and shed light on the promising direction of applying quantum algorithms to address the general challenges in static analysis.},
  archive      = {J_TOSEM},
  author       = {Jiawei Ren and Yulei Sui and Xiao Cheng and Yuan Feng and Jianjun Zhao},
  doi          = {10.1145/3644389},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {119:1–29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Dynamic transitive closure-based static analysis through the lens of quantum search},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enumerating valid non-alpha-equivalent programs for
interpreter testing. <em>TOSEM</em>, <em>33</em>(5), 118:1–31. (<a
href="https://doi.org/10.1145/3647994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeletal program enumeration (SPE) can generate a great number of test programs for validating the correctness of compilers or interpreters. The classic SPE generates programs by exhaustively enumerating all possible variable usage patterns into a given syntactic structure. Even though it is capable of producing many test programs, the exhaustive enumeration strategy generates a large number of invalid programs, which may waste plenty of testing time and resources. To address the problem, this article proposes a tree-based SPE technique. Compared to the state-of-the-art, the key merit of the tree-based approach is that it allows us to take the dependency information into consideration when producing test programs and, thus, make it possible to (1) directly generate non-equivalent programs and (2) apply dominance relations to eliminate invalid test programs that have undefined variables. Hence, our approach significantly saves the cost of the naïve SPE approach. We have implemented our approach into an automated testing tool, IFuzzer , and applied it to test eight different implementations of Python interpreters, including CPython, PyPy, IronPython, Jython, RustPython, GPython, Pyston, and Codon. In three months of fuzzing, IFuzzer detected 142 bugs, of which 87 have been confirmed to be previously unknown bugs, of which 34 have been fixed. Compared to the state-of-the-art SPE techniques, IFuzzer takes only 61.0% of the time cost given the same number of testing seeds and improves 5.3% source code function coverage in the same time budget of testing.},
  archive      = {J_TOSEM},
  author       = {Xinmeng Xia and Yang Feng and Qingkai Shi and James A. Jones and Xiangyu Zhang and Baowen Xu},
  doi          = {10.1145/3647994},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {118:1–31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Enumerating valid non-alpha-equivalent programs for interpreter testing},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An empirical analysis of issue templates usage in
large-scale projects on GitHub. <em>TOSEM</em>, <em>33</em>(5),
117:1–28. (<a href="https://doi.org/10.1145/3643673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GitHub Issues is a widely used issue tracking tool in open-source software projects. Originally designed with broad flexibility, its lack of standardization led to incomplete issue reports, impeding software development and maintenance efficiency. To counteract this, GitHub introduced issue templates in 2016, which rapidly became popular. Our study assesses the current use and evolution of these templates in large-scale open-source projects and their impact on issue tracking metrics, including resolution time, number of reopens, and number of issue comments. Employing a comprehensive analysis of 350 templates from 100 projects, we also evaluated over 1.9 million issues for template conformity and impact. Additionally, we solicited insights from open-source software maintainers through a survey. Our findings highlight issue templates’ extensive usage in 99 of the 100 surveyed projects, with a growing preference for YAML-based templates, a more structured template variant. Projects with a template exhibited markedly reduced resolution time (381.02 days to 103.18 days) and reduced issue comment count (4.95 to 4.32) compared to those without. The use of YAML-based templates further significantly decreased resolution time, the number of reopenings, and the discussion extent. Thus, our research underscores issue templates’ positive impact on large-scale open-source projects, offering recommendations for improved effectiveness.},
  archive      = {J_TOSEM},
  author       = {Emre Sülün and Metehan Saçakçı and Eray Tüzün},
  doi          = {10.1145/3643673},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {117:1–28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical analysis of issue templates usage in large-scale projects on GitHub},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Refining ChatGPT-generated code: Characterizing and
mitigating code quality issues. <em>TOSEM</em>, <em>33</em>(5),
116:1–26. (<a href="https://doi.org/10.1145/3643674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its introduction in November 2022, ChatGPT has rapidly gained popularity due to its remarkable ability in language understanding and human-like responses. ChatGPT, based on GPT-3.5 architecture, has shown great promise for revolutionizing various research fields, including code generation. However, the reliability and quality of code generated by ChatGPT remain unexplored, raising concerns about potential risks associated with the widespread use of ChatGPT-driven code generation. In this article, we systematically study the quality of 4,066 ChatGPT-generated programs of code implemented in two popular programming languages, i.e., Java and Python, for 2,033 programming tasks. The goal of this work is threefold. First, we analyze the correctness of ChatGPT on code generation tasks and uncover the factors that influence its effectiveness, including task difficulty, programming language, time that tasks are introduced, and program size. Second, we identify and characterize potential issues with the quality of ChatGPT-generated code. Last, we provide insights into how these issues can be mitigated. Experiments highlight that out of 4,066 programs generated by ChatGPT, 2,756 programs are deemed correct, 1,082 programs provide wrong outputs, and 177 programs contain compilation or runtime errors. Additionally, we further analyze other characteristics of the generated code through static analysis tools, such as code style and maintainability, and find that 1,930 ChatGPT-generated code snippets suffer from maintainability issues. Subsequently, we investigate ChatGPT’s self-repairing ability and its interaction with static analysis tools to fix the errors uncovered in the previous step. Experiments suggest that ChatGPT can partially address these challenges, improving code quality by more than 20%, but there are still limitations and opportunities for improvement. Overall, our study provides valuable insights into the current limitations of ChatGPT and offers a roadmap for future research and development efforts to enhance the code generation capabilities of artificial intelligence models such as ChatGPT.},
  archive      = {J_TOSEM},
  author       = {Yue Liu and Thanh Le-Cong and Ratnadira Widyasari and Chakkrit Tantithamthavorn and Li Li and Xuan-Bach D. Le and David Lo},
  doi          = {10.1145/3643674},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {116:1–26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Refining ChatGPT-generated code: Characterizing and mitigating code quality issues},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Try with simpler - an evaluation of improved principal
component analysis in log-based anomaly detection. <em>TOSEM</em>,
<em>33</em>(5), 115:1–27. (<a
href="https://doi.org/10.1145/3644386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of deep learning (DL), the recent trend of log-based anomaly detection focuses on extracting semantic information from log events (i.e., templates of log messages) and designing more advanced DL models for anomaly detection. Indeed, the effectiveness of log-based anomaly detection can be improved, but these DL-based techniques further suffer from the limitations of more heavy dependency on training data (such as data quality or data labels) and higher costs in time and resources due to the complexity and scale of DL models, which hinder their practical use. On the contrary, the techniques based on traditional machine learning or data mining algorithms are less dependent on training data and more efficient but produce worse effectiveness than DL-based techniques, which is mainly caused by the problem of unseen log events (some log events in incoming log messages are unseen in training data) confirmed by our motivating study. Intuitively, if we can improve the effectiveness of traditional techniques to be comparable with advanced DL-based techniques, then log-based anomaly detection can be more practical. Indeed, an existing study in the other area (i.e., linking questions posted on Stack Overflow) has pointed out that traditional techniques with some optimizations can indeed achieve comparable effectiveness with the state-of-the-art DL-based technique, indicating the feasibility of enhancing traditional log-based anomaly detection techniques to some degree. Inspired by the idea of “try-with-simpler,” we conducted the first empirical study to explore the potential of improving traditional techniques for more practical log-based anomaly detection. In this work, we optimized the traditional unsupervised PCA (Principal Component Analysis) technique by incorporating a lightweight semantic-based log representation in it, called SemPCA , and conducted an extensive study to investigate the potential of SemPCA for more practical log-based anomaly detection. By comparing seven log-based anomaly detection techniques (including four DL-based techniques, two traditional techniques, and SemPCA ) on both public and industrial datasets, our results show that SemPCA achieves comparable effectiveness as advanced supervised/semi-supervised DL-based techniques while being much more stable under insufficient training data and more efficient, demonstrating that the traditional technique can still excel after small but useful adaptation.},
  archive      = {J_TOSEM},
  author       = {Lin Yang and Junjie Chen and Shutao Gao and Zhihao Gong and Hongyu Zhang and Yue Kang and Huaan Li},
  doi          = {10.1145/3644386},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {115:1–27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Try with simpler - An evaluation of improved principal component analysis in log-based anomaly detection},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SGuard+: Machine learning guided rule-based automated
vulnerability repair on smart contracts. <em>TOSEM</em>, <em>33</em>(5),
114:1–55. (<a href="https://doi.org/10.1145/3641846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contracts are becoming appealing targets for hackers because of the vast amount of cryptocurrencies under their control. Asset loss due to the exploitation of smart contract codes has increased significantly in recent years. To guarantee that smart contracts are vulnerability-free, there are many works to detect the vulnerabilities of smart contracts, but only a few vulnerability repair works have been proposed. Repairing smart contract vulnerabilities at the source code level is attractive as it is transparent to users, whereas existing repair tools, such as SCRepair and sGuard , suffer from many limitations: (1) ignoring the code of vulnerability prevention; (2) possibly applying the repair to the wrong statements and changing the original business logic of smart contracts; and (3) showing poor performance in terms of time and gas overhead. In this work, we propose machine learning guided rule-based automated vulnerability repair on smart contracts to improve the effectiveness and efficiency of sGuard . To address the limitations mentioned above, we design the features that characterize both the symptoms of vulnerabilities and the methods of vulnerability prevention to learn various vulnerability patterns and reduce false positives. Additionally, a fine-grained localization algorithm is designed by traversing the nodes of the abstract syntax tree, and we refine and extend the repair rules of sGuard to preserve the original business logic of smart contracts and support new vulnerability types. Our tool, named sGuard+ , reduces time overhead based on machine learning models, and reduces gas overhead by fewer code changes and precise patching. In our experiment, we collect a publicly available vulnerability dataset from CVE, SWC, and SmartBugs Curated as a ground truth for evaluations. Overall, sGuard+ repairs more vulnerabilities with less time and gas overhead than state-of-the-art tools. Furthermore, we reproduce about 9,000 historical transactions for regression testing. It is shown that sGuard+ has no impact on the original business logic of smart contracts.},
  archive      = {J_TOSEM},
  author       = {Cuifeng Gao and Wenzhang Yang and Jiaming Ye and Yinxing Xue and Jun Sun},
  doi          = {10.1145/3641846},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {114:1–55},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {SGuard+: Machine learning guided rule-based automated vulnerability repair on smart contracts},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning failure-inducing models for testing
software-defined networks. <em>TOSEM</em>, <em>33</em>(5), 113:1–25. (<a
href="https://doi.org/10.1145/3641541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1) generating effective test data leading to failures in SDN-based systems and (2) learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, no existing work simultaneously addresses these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Furthermore, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines for learning failure-inducing models. Our results show that (1) compared to the state-of-the-art methods, FuzzSDN generates at least 12 times more failures, within the same time budget, with a controller that is fairly robust to fuzzing and (2) our failure-inducing models have, on average, a precision of 98% and a recall of 86%, significantly outperforming the baselines.},
  archive      = {J_TOSEM},
  author       = {Raphaël Ollando and Seung Yeob Shin and Lionel C. Briand},
  doi          = {10.1145/3641541},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {113:1–25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Learning failure-inducing models for testing software-defined networks},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communicating study design trade-offs in software
engineering. <em>TOSEM</em>, <em>33</em>(5), 112:1–10. (<a
href="https://doi.org/10.1145/3649598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reflecting on the limitations of a study is a crucial part of the research process. In software engineering studies, this reflection is typically conveyed through discussions of study limitations or threats to validity. In current practice, such discussions seldom provide sufficient insight to understand the rationale for decisions taken before and during the study, and their implications. We revisit the practice of discussing study limitations and threats to validity and identify its weaknesses. We propose to refocus this practice of self-reflection to a discussion centered on the notion of trade-offs . We argue that documenting trade-offs allows researchers to clarify how the benefits of their study design decisions outweigh the costs of possible alternatives. We present guidelines for reporting trade-offs in a way that promotes a fair and dispassionate assessment of researchers’ work.},
  archive      = {J_TOSEM},
  author       = {Martin P. Robillard and Deeksha M. Arya and Neil A. Ernst and Jin L. C. Guo and Maxime Lamothe and Mathieu Nassif and Nicole Novielli and Alexander Serebrenik and Igor Steinmacher and Klaas-Jan Stol},
  doi          = {10.1145/3649598},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {6},
  number       = {5},
  pages        = {112:1–10},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Communicating study design trade-offs in software engineering},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Test optimization in DNN testing: A survey. <em>TOSEM</em>,
<em>33</em>(4), 111:1–42. (<a
href="https://doi.org/10.1145/3643678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive survey on test optimization in deep neural network (DNN) testing. Here, test optimization refers to testing with low data labeling effort. We analyzed 90 papers, including 43 from the software engineering (SE) community, 32 from the machine learning (ML) community, and 15 from other communities. Our study: (i) unifies the problems as well as terminologies associated with low-labeling cost testing, (ii) compares the distinct focal points of SE and ML communities, and (iii) reveals the pitfalls in existing literature. Furthermore, we highlight the research opportunities in this domain.},
  archive      = {J_TOSEM},
  author       = {Qiang Hu and Yuejun Guo and Xiaofei Xie and Maxime Cordy and Lei Ma and Mike Papadakis and Yves Le Traon},
  doi          = {10.1145/3643678},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {111:1–42},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Test optimization in DNN testing: A survey},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding real-time collaborative programming: A study
of visual studio live share. <em>TOSEM</em>, <em>33</em>(4), 110:1–28.
(<a href="https://doi.org/10.1145/3643672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time collaborative programming (RCP) entails developers working simultaneously, regardless of their geographic locations. RCP differs from traditional asynchronous online programming methods, such as Git or SVN, where developers work independently and update the codebase at separate times. Although various real-time code collaboration tools (e.g., Visual Studio Live Share , Code with Me , and Replit ) have kept emerging in recent years, none of the existing studies explicitly focus on a deep understanding of the processes or experiences associated with RCP. To this end, we combine interviews and an e-mail survey with the users of Visual Studio Live Share , aiming to understand (i) the scenarios, (ii) the requirements, and (iii) the challenges when developers participate in RCP. We find that developers participate in RCP in 18 different scenarios belonging to six categories, e.g., pair programming , group debugging , and code review . However, existing users’ attitudes toward the usefulness of the current RCP tools in these scenarios were significantly more negative than the expectations of potential users. As for the requirements, the most critical category is live editing , followed by the need for sharing terminals to enable hosts and guests to run commands and see the results, as well as focusing and following , which involves “following” the host’s edit location and “focusing” the guests’ attention on the host with a notification. Under these categories, we identify 17 requirements, but most of them are not well supported by current tools. In terms of challenges, we identify 19 challenges belonging to seven categories. The most severe category of challenges is lagging followed by permissions and conflicts . The above findings indicate that the current RCP tools and even collaborative environment need to be improved greatly and urgently. Based on these findings, we discuss the recommendations for different stakeholders, including practitioners, tool designers, and researchers.},
  archive      = {J_TOSEM},
  author       = {Xin Tan and Xinyue Lv and Jing Jiang and Li Zhang},
  doi          = {10.1145/3643672},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {110:1–28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Understanding real-time collaborative programming: A study of visual studio live share},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enablers and barriers of empathy in software developer and
user interactions: A mixed methods case study. <em>TOSEM</em>,
<em>33</em>(4), 109:1–41. (<a
href="https://doi.org/10.1145/3641849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software engineering (SE) requires developers to collaborate with stakeholders, and understanding their emotions and perspectives is often vital. Empathy is a concept characterising a person’s ability to understand and share the feelings of another. However, empathy continues to be an under-researched human aspect in SE. We studied how empathy is practised between developers and end users using a mixed methods case study. We used an empathy test, observations, and interviews to collect data and socio-technical grounded theory and descriptive statistics to analyse data. We identified the nature of awareness required to trigger empathy and enablers of empathy. We discovered barriers to empathy and a set of potential strategies to overcome these barriers. We report insights on emerging relationships and present a set of recommendations and potential future works on empathy and SE for software practitioners and SE researchers.},
  archive      = {J_TOSEM},
  author       = {Hashini Gunatilake and John Grundy and Rashina Hoda and Ingo Mueller},
  doi          = {10.1145/3641849},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {109:1–41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Enablers and barriers of empathy in software developer and user interactions: A mixed methods case study},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Battling against protocol fuzzing: Protecting networked
embedded devices from dynamic fuzzers. <em>TOSEM</em>, <em>33</em>(4),
108:1–26. (<a href="https://doi.org/10.1145/3641847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {N etworked E mbedded D evices (NEDs) are increasingly targeted by cyberattacks, mainly due to their widespread use in our daily lives. Vulnerabilities in NEDs are the root causes of these cyberattacks. Although deployed NEDs go through thorough code audits, there can still be considerable exploitable vulnerabilities. Existing mitigation measures like code encryption and obfuscation adopted by vendors can resist static analysis on deployed NEDs, but are ineffective against protocol fuzzing. Attackers can easily apply protocol fuzzing to discover vulnerabilities and compromise deployed NEDs. Unfortunately, prior anti-fuzzing techniques are impractical as they significantly slow down NEDs, hampering NED availability. To address this issue, we propose Armor—the first anti-fuzzing technique specifically designed for NEDs. First, we design three adversarial primitives–delay, fake coverage, and forged exception–to break the fundamental mechanisms on which fuzzing relies to effectively find vulnerabilities. Second, based on our observation that inputs from normal users consistent with the protocol specification and certain program paths are rarely executed with normal inputs, we design static and dynamic strategies to decide whether to activate the adversarial primitives. Extensive evaluations show that Armor incurs negligible time overhead and effectively reduces the code coverage (e.g., line coverage by 22%-61%) for fuzzing, significantly outperforming the state of the art.},
  archive      = {J_TOSEM},
  author       = {Puzhuo Liu and Yaowen Zheng and Chengnian Sun and Hong Li and Zhi Li and Limin Sun},
  doi          = {10.1145/3641847},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {108:1–26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Battling against protocol fuzzing: Protecting networked embedded devices from dynamic fuzzers},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating debugger-based attacks to java applications with
self-debugging. <em>TOSEM</em>, <em>33</em>(4), 107:1–38. (<a
href="https://doi.org/10.1145/3631971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Java bytecode is a quite high-level language and, as such, it is fairly easy to analyze and decompile with malicious intents, e.g., to tamper with code and skip license checks. Code obfuscation was a first attempt to mitigate malicious reverse-engineering based on static analysis. However, obfuscated code can still be dynamically analyzed with standard debuggers to perform step-wise execution and to inspect (or change) memory content at important execution points, e.g., to alter the verdict of license validity checks. Although some approaches have been proposed to mitigate debugger-based attacks, they are only applicable to binary compiled code and none address the challenge of protecting Java bytecode. In this article, we propose a novel approach to protect Java bytecode from malicious debugging. Our approach is based on automated program transformation to manipulate Java bytecode and split it into two binary processes that debug each other (i.e., a self-debugging solution). In fact, when the debugging interface is already engaged, an additional malicious debugger cannot attach. To be resilient against typical attacks, our approach adopts a series of technical solutions, e.g., an encoded channel is shared by the two processes to avoid leaking information, an authentication protocol is established to avoid Man-in-the-middle attacks, and the computation is spread between the two processes to prevent the attacker to replace or terminate either of them. We test our solution on 18 real-world Java applications, showing that our approach can effectively block the most common debugging tasks (either with the Java debugger or the GNU debugger) while preserving the functional correctness of the protected programs. While the final decision on when to activate this protection is still up to the developers, the observed performance overhead was acceptable for common desktop application domains.},
  archive      = {J_TOSEM},
  author       = {Davide Pizzolotto and Stefano Berlato and Mariano Ceccato},
  doi          = {10.1145/3631971},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {107:1–38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Mitigating debugger-based attacks to java applications with self-debugging},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smart contract code repair recommendation based on
reinforcement learning and multi-metric optimization. <em>TOSEM</em>,
<em>33</em>(4), 106:1–31. (<a
href="https://doi.org/10.1145/3637229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A smart contract is a kind of code deployed on the blockchain that executes automatically once an event triggers a clause in the contract. Since smart contracts involve businesses such as asset transfer, they are more vulnerable to attacks, so it is crucial to ensure the security of smart contracts. Because a smart contract cannot be tampered with once deployed on the blockchain, for smart contract developers, it is necessary to fix vulnerabilities before deployment. Compared with many vulnerability detection tools for smart contracts, the amount of automatic fix approaches for smart contracts is relatively limited. These approaches mainly use defined pattern-based methods or heuristic search algorithms for vulnerability repairs. In this article, we propose RLRep , a reinforcement learning-based approach to provide smart contract repair recommendations for smart contract developers automatically. This approach adopts an agent to provide repair action suggestions based on the vulnerable smart contract without any supervision, which can solve the problem of missing labeled data in machine learning-based repair methods. We evaluate our approach on a dataset containing 853 smart contract programs (programming language: Solidity) with different kinds of vulnerabilities. We split them into training and test sets. The result shows that our approach can provide 54.97% correct repair recommendations for smart contracts.},
  archive      = {J_TOSEM},
  author       = {Hanyang Guo and Yingye Chen and Xiangping Chen and Yuan Huang and Zibin Zheng},
  doi          = {10.1145/3637229},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {106:1–31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Smart contract code repair recommendation based on reinforcement learning and multi-metric optimization},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating uncertainty in labeled changes by SZZ tools on
just-in-time defect prediction. <em>TOSEM</em>, <em>33</em>(4),
105:1–25. (<a href="https://doi.org/10.1145/3637226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of Just-In-Time (JIT) defect prediction is to predict software changes that are prone to defects in a project in a timely manner, thereby improving the efficiency of software development and ensuring software quality. Identifying changes that introduce bugs is a critical task in just-in-time defect prediction, and researchers have introduced the SZZ approach and its variants to label these changes. However, it has been shown that different SZZ algorithms introduce noise to the dataset to a certain extent, which may reduce the predictive performance of the model. To address this limitation, we propose the Confident Learning Imbalance (CLI) model. The model identifies and excludes samples whose labels may be corrupted by estimating the joint distribution of noisy labels and true labels, and mitigates the impact of noisy data on the performance of the prediction model. The CLI consists of two components: identifying noisy data (Confident Learning Component) and generating a predicted probability matrix for imbalanced data (Imbalanced Data Probabilistic Prediction Component). The IDPP component generates precise predicted probabilities for each instance in the training set, while the CL component uses the generated predicted probability matrix and noise labels to clean up the noise and build a classification model. We evaluate the performance of our model through extensive experiments on a total of 126,526 changes from ten Apache open source projects, and the results show that our model outperforms the baseline methods.},
  archive      = {J_TOSEM},
  author       = {Shikai Guo and Dongmin Li and Lin Huang and Sijia Lv and Rong Chen and Hui Li and Xiaochen Li and He Jiang},
  doi          = {10.1145/3637226},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {105:1–25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Estimating uncertainty in labeled changes by SZZ tools on just-in-time defect prediction},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond accuracy: An empirical study on unit testing in
open-source deep learning projects. <em>TOSEM</em>, <em>33</em>(4),
104:1–22. (<a href="https://doi.org/10.1145/3638245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning (DL) models have rapidly advanced, focusing on achieving high performance through testing model accuracy and robustness. However, it is unclear whether DL projects, as software systems, are tested thoroughly or functionally correct when there is a need to treat and test them like other software systems. Therefore, we empirically study the unit tests in open-source DL projects, analyzing 9,129 projects from GitHub. We find that: (1) unit tested DL projects have positive correlation with the open-source project metrics and have a higher acceptance rate of pull requests; (2) 68% of the sampled DL projects are not unit tested at all; (3) the layer and utilities (utils) of DL models have the most unit tests. Based on these findings and previous research outcomes, we built a mapping taxonomy between unit tests and faults in DL projects. We discuss the implications of our findings for developers and researchers and highlight the need for unit testing in open-source DL projects to ensure their reliability and stability. The study contributes to this community by raising awareness of the importance of unit testing in DL projects and encouraging further research in this area.},
  archive      = {J_TOSEM},
  author       = {Han Wang and Sijia Yu and Chunyang Chen and Burak Turhan and Xiaodong Zhu},
  doi          = {10.1145/3638245},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {104:1–22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Beyond accuracy: An empirical study on unit testing in open-source deep learning projects},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deceiving humans and machines alike: Search-based test input
generation for DNNs using variational autoencoders. <em>TOSEM</em>,
<em>33</em>(4), 103:1–24. (<a
href="https://doi.org/10.1145/3635706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid adoption of Deep Neural Networks (DNNs) into larger software systems, testing of DNN-based systems has received much attention recently. While many different test adequacy criteria have been suggested, we lack effective test input generation techniques. Inputs such as images of real-world objects and scenes are not only expensive to collect but also difficult to randomly sample. Consequently, current testing techniques for DNNs tend to apply small local perturbations to existing inputs to generate new inputs. We propose SINVAD (Search-based Input space Navigation using Variational AutoencoDers), a way to sample from, and navigate over, a space of realistic inputs that resembles the true distribution in the training data. Our input space is constructed using Variational Autoencoders (VAEs), and navigated through their latent vector space. Our analysis shows that the VAE-based input space is well-aligned with human perception of what constitutes realistic inputs. Further, we show that this space can be effectively searched to achieve various testing scenarios, such as boundary testing of two different DNNs or analyzing class labels that are difficult for the given DNN to distinguish. Guidelines on how to design VAE architectures are presented as well. Our results have the potential to open the field to meaningful exploration through the space of highly structured images.},
  archive      = {J_TOSEM},
  author       = {Sungmin Kang and Robert Feldt and Shin Yoo},
  doi          = {10.1145/3635706},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {103:1–24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Deceiving humans and machines alike: Search-based test input generation for DNNs using variational autoencoders},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mapping APIs in dynamic-typed programs by leveraging
transfer learning. <em>TOSEM</em>, <em>33</em>(4), 102:1–29. (<a
href="https://doi.org/10.1145/3641848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application Programming Interface (API) migration is a common task for adapting software across different programming languages and platforms, where manually constructing the mapping relations between APIs is indeed time-consuming and error-prone. To facilitate this process, many automated API mapping approaches have been proposed. However, existing approaches were mainly designed and evaluated for mapping APIs of statically-typed languages, while their performance on dynamically-typed languages remains unexplored. In this article, we conduct the first extensive study to explore existing API mapping approaches’ performance for mapping APIs in dynamically-typed languages, for which we have manually constructed a high-quality dataset. According to the empirical results, we have summarized several insights. In particular, the source code implementations of APIs can significantly improve the effectiveness of API mapping. However, due to the confidentiality policy, they may not be available in practice. To overcome this, we propose a novel API mapping approach, named Matl , which leverages the transfer learning technique to learn the semantic embeddings of source code implementations from large-scale open-source repositories and then transfers the learned model to facilitate the mapping of APIs. In this way, Matl can produce more accurate API embedding of its functionality for more effective mapping without knowing the source code of the APIs. To evaluate the performance of Matl , we have conducted an extensive study by comparing Matl with state-of-the-art approaches. The results demonstrate that Matl is indeed effective as it improves the state-of-the-art approach by at least 18.36% for mapping APIs of dynamically-typed language and by 30.77% for mapping APIs of the statically-typed language.},
  archive      = {J_TOSEM},
  author       = {Zhenfei Huang and Junjie Chen and Jiajun Jiang and Yihua Liang and Hanmo You and Fengjie Li},
  doi          = {10.1145/3641848},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {102:1–29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Mapping APIs in dynamic-typed programs by leveraging transfer learning},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bug analysis in jupyter notebook projects: An empirical
study. <em>TOSEM</em>, <em>33</em>(4), 101:1–34. (<a
href="https://doi.org/10.1145/3641539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational notebooks, such as Jupyter, have been widely adopted by data scientists to write code for analyzing and visualizing data. Despite their growing adoption and popularity, few studies have been found to understand Jupyter development challenges from the practitioners’ point of view. This article presents a systematic study of bugs and challenges that Jupyter practitioners face through a large-scale empirical investigation. We mined 14,740 commits from 105 GitHub open source projects with Jupyter Notebook code. Next, we analyzed 30,416 StackOverflow posts, which gave us insights into bugs that practitioners face when developing Jupyter Notebook projects. Next, we conducted 19 interviews with data scientists to uncover more details about Jupyter bugs and to gain insight into Jupyter developers’ challenges. Finally, to validate the study results and proposed taxonomy, we conducted a survey with 91 data scientists. We highlight bug categories, their root causes, and the challenges that Jupyter practitioners face.},
  archive      = {J_TOSEM},
  author       = {Taijara Loiola De Santana and Paulo Anselmo Da Mota Silveira Neto and Eduardo Santana De Almeida and Iftekhar Ahmed},
  doi          = {10.1145/3641539},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {101:1–34},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Bug analysis in jupyter notebook projects: An empirical study},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compiler autotuning through multiple-phase learning.
<em>TOSEM</em>, <em>33</em>(4), 100:1–38. (<a
href="https://doi.org/10.1145/3640330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Widely used compilers like GCC and LLVM usually have hundreds of optimizations controlled by optimization flags, which are enabled or disabled during compilation to improve the runtime performance (e.g., small execution time) of the compiler program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to manually tune compiler optimization flags. In the literature, a number of autotuning techniques have been proposed, which tune optimization flags for a compiled program by comparing its actual runtime performance with different optimization flag combinations. Due to the huge search space and heavy actual runtime cost, these techniques suffer from the widely recognized efficiency problem. To reduce the heavy runtime cost, in this article we propose a lightweight learning approach that uses a small number of actual runtime performance data to predict the runtime performance of a compiled program with various optimization flag combinations. Furthermore, to reduce the search space, we design a novel particle swarm algorithm that tunes compiler optimization flags with the prediction model. To evaluate the performance of the proposed approach, CompTuner, we conduct an extensive experimental study on two popular C compilers, GCC and LLVM, with two widely used benchmarks, cBench and PolyBench. The experimental results show that CompTuner significantly outperforms the six compared techniques, including the state-of-the-art technique BOCA.},
  archive      = {J_TOSEM},
  author       = {Mingxuan Zhu and Dan Hao and Junjie Chen},
  doi          = {10.1145/3640330},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {100:1–38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Compiler autotuning through multiple-phase learning},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Industry practices for challenging autonomous driving
systems with critical scenarios. <em>TOSEM</em>, <em>33</em>(4),
99:1–35. (<a href="https://doi.org/10.1145/3640334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing autonomous driving systems for safety and reliability is essential, yet complex. A primary challenge is identifying relevant test scenarios, especially the critical ones that may expose hazards or harm to autonomous vehicles and other road users. Although numerous approaches and tools for critical scenario identification are proposed, the industry practices for selection, implementation, and evaluation of approaches, are not well understood. Therefore, we aim at exploring practical aspects of how autonomous driving systems are tested, particularly the identification and use of critical scenarios. We interviewed 13 practitioners from 7 companies in autonomous driving in Sweden. We used thematic modeling to analyse and synthesize the interview data. As a result, we present 9 themes of practices and 4 themes of challenges related to critical scenarios. Our analysis indicates there is little joint effort in the industry, despite every approach has its own limitations, and tools and platforms are lacking. To that end, we recommend the industry and academia combine different approaches, collaborate among different stakeholders, and continuously learn the field. The contributions of our study are exploration and synthesis of industry practices and related challenges for critical scenario identification and testing, and potential increase of industry relevance for future studies.},
  archive      = {J_TOSEM},
  author       = {Qunying Song and Emelie Engström and Per Runeson},
  doi          = {10.1145/3640334},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {99:1–35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Industry practices for challenging autonomous driving systems with critical scenarios},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Method-level bug prediction: Problems and promises.
<em>TOSEM</em>, <em>33</em>(4), 98:1–31. (<a
href="https://doi.org/10.1145/3640331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fixing software bugs can be colossally expensive, especially if they are discovered in the later phases of the software development life cycle. As such, bug prediction has been a classic problem for the research community. As of now, the Google Scholar site generates ∼113,000 hits if searched with the “bug prediction” phrase. Despite this staggering effort by the research community, bug prediction research is criticized for not being decisively adopted in practice. A significant problem of the existing research is the granularity level (i.e., class/file level) at which bug prediction is historically studied. Practitioners find it difficult and time-consuming to locate bugs at the class/file level granularity. Consequently, method-level bug prediction has become popular in the past decade. We ask, are these method-level bug prediction models ready for industry use? Unfortunately, the answer is no . The reported high accuracies of these models dwindle significantly if we evaluate them in different realistic time-sensitive contexts. It may seem hopeless at first, but, encouragingly, we show that future method-level bug prediction can be improved significantly. In general, we show how to reliably evaluate future method-level bug prediction models and how to improve them by focusing on four different improvement avenues: building noise-free bug data, addressing concept drift, selecting similar training projects, and developing a mixture of models. Our findings are based on three publicly available method-level bug datasets and a newly built bug dataset of 774,051 Java methods originating from 49 open-source software projects.},
  archive      = {J_TOSEM},
  author       = {Shaiful Chowdhury and Gias Uddin and Hadi Hemmati and Reid Holmes},
  doi          = {10.1145/3640331},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {98:1–31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Method-level bug prediction: Problems and promises},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characterizing deep learning package supply chains in PyPI:
Domains, clusters, and disengagement. <em>TOSEM</em>, <em>33</em>(4),
97:1–27. (<a href="https://doi.org/10.1145/3640336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) frameworks have become the cornerstone of the rapidly developing DL field. Through installation dependencies specified in the distribution metadata, numerous packages directly or transitively depend on DL frameworks, layer after layer, forming DL package supply chains (SCs), which are critical for DL frameworks to remain competitive. However, vital knowledge on how to nurture and sustain DL package SCs is still lacking. Achieving this knowledge may help DL frameworks formulate effective measures to strengthen their SCs to remain competitive and shed light on dependency issues and practices in the DL SC for researchers and practitioners. In this paper, we explore the domains, clusters, and disengagement of packages in two representative PyPI DL package SCs to bridge this knowledge gap. We analyze the metadata of nearly six million PyPI package distributions and construct version-sensitive SCs for two popular DL frameworks: TensorFlow and PyTorch. We find that popular packages (measured by the number of monthly downloads) in the two SCs cover 34 domains belonging to eight categories. Applications , Infrastructure , and Sciences categories account for over 85% of popular packages in either SC and TensorFlow and PyTorch SC have developed specializations on Infrastructure and Applications packages, respectively. We employ the Leiden community detection algorithm and detect 131 and 100 clusters in the two SCs. The clusters mainly exhibit four shapes: Arrow, Star, Tree, and Forest with increasing dependency complexity. Most clusters are Arrow or Star, while Tree and Forest clusters account for most packages (Tensorflow SC: 70.7%, PyTorch SC: 92.9%). We identify three groups of reasons why packages disengage from the SC (i.e., remove the DL framework and its dependents from their installation dependencies): dependency issues, functional improvements, and ease of installation. The most common reason in TensorFlow SC is dependency incompatibility and in PyTorch SC is to simplify functionalities and reduce installation size. Our study provides rich implications for DL framework vendors, researchers, and practitioners on the maintenance and dependency management practices of PyPI DL SCs.},
  archive      = {J_TOSEM},
  author       = {Kai Gao and Runzhi He and Bing Xie and Minghui Zhou},
  doi          = {10.1145/3640336},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {97:1–27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Characterizing deep learning package supply chains in PyPI: Domains, clusters, and disengagement},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARCTURUS: Full coverage binary similarity analysis with
reachability-guided emulation. <em>TOSEM</em>, <em>33</em>(4), 96:1–31.
(<a href="https://doi.org/10.1145/3640337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary code similarity analysis is extremely useful, since it provides rich information about an unknown binary, such as revealing its functionality and identifying reused libraries. Robust binary similarity analysis is challenging, as heavy compiler optimizations can make semantically similar binaries have gigantic syntactic differences. Unfortunately, existing semantic-based methods still suffer from either incomplete coverage or low accuracy. In this article, we propose ARCTURUS , a new technique that can achieve high code coverage and high accuracy simultaneously by manipulating program execution under the guidance of code reachability. Our key insight is that the compiler must preserve program semantics (e.g., dependences between code fragments) during compilation; therefore, the code reachability, which implies the interdependence between code, is invariant across code transformations. Based on the above insight, our key idea is to leverage the stability of code reachability to manipulate the program execution such that deep code logic can also be covered in a consistent way. Experimental results show that ARCTURUS achieves an average precision of 87.8% with 100% block coverage, outperforming compared methods by 38.4%, on average. ARCTURUS takes only 0.15 second to process one function, on average, indicating that it is efficient for practical use.},
  archive      = {J_TOSEM},
  author       = {Anshunkang Zhou and Yikun Hu and Xiangzhe Xu and Charles Zhang},
  doi          = {10.1145/3640337},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {96:1–31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {ARCTURUS: Full coverage binary similarity analysis with reachability-guided emulation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rigorous assessment of model inference accuracy using
language cardinality. <em>TOSEM</em>, <em>33</em>(4), 95:1–39. (<a
href="https://doi.org/10.1145/3640332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models such as finite state automata are widely used to abstract the behavior of software systems by capturing the sequences of events observable during their execution. Nevertheless, models rarely exist in practice and, when they do, get easily outdated; moreover, manually building and maintaining models is costly and error-prone. As a result, a variety of model inference methods that automatically construct models from execution traces have been proposed to address these issues. However, performing a systematic and reliable accuracy assessment of inferred models remains an open problem. Even when a reference model is given, most existing model accuracy assessment methods may return misleading and biased results. This is mainly due to their reliance on statistical estimators over a finite number of randomly generated traces, introducing avoidable uncertainty about the estimation and being sensitive to the parameters of the random trace generative process. This article addresses this problem by developing a systematic approach based on analytic combinatorics that minimizes bias and uncertainty in model accuracy assessment by replacing statistical estimation with deterministic accuracy measures. We experimentally demonstrate the consistency and applicability of our approach by assessing the accuracy of models inferred by state-of-the-art inference tools against reference models from established specification mining benchmarks.},
  archive      = {J_TOSEM},
  author       = {Donato Clun and Donghwan Shin and Antonio Filieri and Domenico Bianculli},
  doi          = {10.1145/3640332},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {95:1–39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Rigorous assessment of model inference accuracy using language cardinality},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying and explaining safety-critical scenarios for
autonomous vehicles via key features. <em>TOSEM</em>, <em>33</em>(4),
94:1–32. (<a href="https://doi.org/10.1145/3640335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the safety of autonomous vehicles (AVs) is of utmost importance, and testing them in simulated environments is a safer option than conducting in-field operational tests. However, generating an exhaustive test suite to identify critical test scenarios is computationally expensive, as the representation of each test is complex and contains various dynamic and static features, such as the AV under test, road participants (vehicles, pedestrians, and static obstacles), environmental factors (weather and light), and the road’s structural features (lanes, turns, road speed, etc.). In this article, we present a systematic technique that uses Instance Space Analysis (ISA) to identify the significant features of test scenarios that affect their ability to reveal the unsafe behaviour of AVs. ISA identifies the features that best differentiate safety-critical scenarios from normal driving and visualises the impact of these features on test scenario outcomes (safe/unsafe) in two dimensions. This visualisation helps to identify untested regions of the instance space and provides an indicator of the quality of the test suite in terms of the percentage of feature space covered by testing. To test the predictive ability of the identified features, we train five Machine Learning classifiers to classify test scenarios as safe or unsafe. The high precision, recall, and F1 scores indicate that our proposed approach is effective in predicting the outcome of a test scenario without executing it and can be used for test generation, selection, and prioritisation.},
  archive      = {J_TOSEM},
  author       = {Neelofar Neelofar and Aldeida Aleti},
  doi          = {10.1145/3640335},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {94:1–32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Identifying and explaining safety-critical scenarios for autonomous vehicles via key features},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Test generation strategies for building failure models and
explaining spurious failures. <em>TOSEM</em>, <em>33</em>(4), 93:1–32.
(<a href="https://doi.org/10.1145/3638246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test inputs fail not only when the system under test is faulty but also when the inputs are invalid or unrealistic. Failures resulting from invalid or unrealistic test inputs are spurious. Avoiding spurious failures improves the effectiveness of testing in exercising the main functions of a system, particularly for compute-intensive (CI) systems where a single test execution takes significant time. In this article, we propose to build failure models for inferring interpretable rules on test inputs that cause spurious failures. We examine two alternative strategies for building failure models: (1) machine learning (ML)-guided test generation and (2) surrogate-assisted test generation. ML-guided test generation infers boundary regions that separate passing and failing test inputs and samples test inputs from those regions. Surrogate-assisted test generation relies on surrogate models to predict labels for test inputs instead of exercising all the inputs. We propose a novel surrogate-assisted algorithm that uses multiple surrogate models simultaneously, and dynamically selects the prediction from the most accurate model. We empirically evaluate the accuracy of failure models inferred based on surrogate-assisted and ML-guided test generation algorithms. Using case studies from the domains of cyber-physical systems and networks, we show that our proposed surrogate-assisted approach generates failure models with an average accuracy of 83%, significantly outperforming ML-guided test generation and two baselines. Further, our approach learns failure-inducing rules that identify genuine spurious failures as validated against domain knowledge.},
  archive      = {J_TOSEM},
  author       = {Baharin A. Jodat and Abhishek Chandar and Shiva Nejati and Mehrdad Sabetzadeh},
  doi          = {10.1145/3638246},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {93:1–32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Test generation strategies for building failure models and explaining spurious failures},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring semantic redundancy using backdoor triggers: A
complementary insight into the challenges facing DNN-based software
vulnerability detection. <em>TOSEM</em>, <em>33</em>(4), 92:1–28. (<a
href="https://doi.org/10.1145/3640333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To detect software vulnerabilities with better performance, deep neural networks (DNNs) have received extensive attention recently. However, these vulnerability detection DNN models trained with code representations are vulnerable to specific perturbations on code representations. This motivates us to rethink the bane of software vulnerability detection and find function-agnostic features during code representation which we name as semantic redundant features. This paper first identifies a tight correlation between function-agnostic triggers and semantic redundant feature space (where the redundant features reside) in these DNN models. For correlation identification, we propose a novel Backdoor-based Semantic Redundancy Exploration (BSemRE) framework. In BSemRE, the sensitivity of the trained models to function-agnostic triggers is observed to verify the existence of semantic redundancy in various code representations. Specifically, acting as the typical manifestations of semantic redundancy, naming conventions, ternary operators and identically-true conditions are exploited to generate function-agnostic triggers. Extensive comparative experiments on 1,613,823 samples of eight representative vulnerability datasets and state-of-the-art code representation techniques and vulnerability detection models demonstrate that the existence of semantic redundancy determines the upper trustworthiness limit of DNN-based software vulnerability detection. To the best of our knowledge, this is the first work exploring the bane of software vulnerability detection using backdoor triggers.},
  archive      = {J_TOSEM},
  author       = {Changjie Shao and Gaolei Li and Jun Wu and Xi Zheng},
  doi          = {10.1145/3640333},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {92:1–28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Exploring semantic redundancy using backdoor triggers: A complementary insight into the challenges facing DNN-based software vulnerability detection},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building domain-specific machine learning workflows: A
conceptual framework for the state of the practice. <em>TOSEM</em>,
<em>33</em>(4), 91:1–50. (<a
href="https://doi.org/10.1145/3638243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain experts are increasingly employing machine learning to solve their domain-specific problems. This article presents to software engineering researchers the six key challenges that a domain expert faces in addressing their problem with a computational workflow, and the underlying executable implementation. These challenges arise out of our conceptual framework which presents the “route” of transformations that a domain expert may choose to take while developing their solution. To ground our conceptual framework in the state of the practice, this article discusses a selection of available textual and graphical workflow systems and their support for the transformations described in our framework. Example studies from the literature in various domains are also examined to highlight the tools used by the domain experts as well as a classification of the domain specificity and machine learning usage of their problem, workflow, and implementation. The state of the practice informs our discussion of the six key challenges, where we identify which challenges and transformations are not sufficiently addressed by available tools. We also suggest possible research directions for software engineering researchers to increase the automation of these tools and disseminate best-practice techniques between software engineering and various scientific domains.},
  archive      = {J_TOSEM},
  author       = {Bentley James Oakes and Michalis Famelis and Houari Sahraoui},
  doi          = {10.1145/3638243},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {91:1–50},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Building domain-specific machine learning workflows: A conceptual framework for the state of the practice},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring and clustering heterogeneous chatbot designs.
<em>TOSEM</em>, <em>33</em>(4), 90:1–43. (<a
href="https://doi.org/10.1145/3637228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational agents, or chatbots, have become popular to access all kind of software services. They provide an intuitive natural language interface for interaction, available from a wide range of channels including social networks, web pages, intelligent speakers or cars. In response to this demand, many chatbot development platforms and tools have emerged. However, they typically lack support to statically measure properties of the chatbots being built, as indicators of their size, complexity, quality or usability. Similarly, there are hardly any mechanisms to compare and cluster chatbots developed with heterogeneous technologies. To overcome this limitation, we propose a suite of 21 metrics for chatbot designs, as well as two clustering methods that help in grouping chatbots along their conversation topics and design features. Both the metrics and the clustering methods are defined on a neutral chatbot design language, becoming independent of the implementation platform. We provide automatic translations of chatbots defined on some major platforms into this neutral notation to perform the measurement and clustering. The approach is supported by our tool Asymob , which we have used to evaluate the metrics and the clustering methods over a set of 259 Dialogflow and Rasa chatbots from open-source repositories. The results open the door to incorporating the metrics within chatbot development processes for the early detection of quality issues, and to exploit clustering to organise large collections of chatbots into significant groups to ease chatbot comprehension, search and comparison.},
  archive      = {J_TOSEM},
  author       = {Pablo C. Cañizares and Jose María López-Morales and Sara Pérez-Soler and Esther Guerra and Juan de Lara},
  doi          = {10.1145/3637228},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {90:1–43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Measuring and clustering heterogeneous chatbot designs},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A smart status based monitoring algorithm for the dynamic
analysis of memory safety. <em>TOSEM</em>, <em>33</em>(4), 89:1–47. (<a
href="https://doi.org/10.1145/3637227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {C is a dominant programming language for implementing system and low-level embedded software. Unfortunately, the unsafe nature of its low-level control of memory often leads to memory errors. Dynamic analysis has been widely used to detect memory errors at runtime. However, existing monitoring algorithms for dynamic analysis are not yet satisfactory, as they cannot deterministically and completely detect some types of errors, such as segment confusion errors, sub-object overflows, use-after-frees and memory leaks. We propose a new monitoring algorithm, namely Smatus , short for smart status , that improves memory safety by performing comprehensive dynamic analysis. The key innovation is to maintain at runtime a small status node for each memory object. A status node records the status value and reference count of an object, where the status value denotes the liveness and segment type of this object, and the reference count tracks the number of pointer variables pointing to this object. Smatus maintains at runtime a pointer metadata for each pointer variable, to record not only the base and bound of a pointer’s referent but also the address of the referent’s status node. All the pointers pointing to the same referent share the same status node in their pointer metadata. A status node is smart in the sense that it is automatically deleted when it becomes useless (indicated by its reference count reaching zero). To the best of our knowledge, Smatus represents the most comprehensive approach of its kind. We have evaluated Smatus by using a large set of programs including the NIST Software Assurance Reference Dataset, MSBench, MiBench, SPEC and stress testing benchmarks. In terms of effectiveness (detecting different types of memory errors), Smatus outperforms state-of-the-art tools, Google’s AddressSanitizer, SoftBoundCETS and Valgrind, as it is capable of detecting more errors. In terms of performance (the time and memory overheads), Smatus outperforms SoftBoundCETS and Valgrind in terms of both lower time and memory overheads incurred, and is on par with AddressSanitizer in terms of the time and memory overhead tradeoff made (with much lower memory overheads incurred).},
  archive      = {J_TOSEM},
  author       = {Zhe Chen and Rui Yan and Yingzi Ma and Yulei Sui and Jingling Xue},
  doi          = {10.1145/3637227},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {89:1–47},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A smart status based monitoring algorithm for the dynamic analysis of memory safety},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RE methods for virtual reality software product development:
A mapping study. <em>TOSEM</em>, <em>33</em>(4), 88:1–31. (<a
href="https://doi.org/10.1145/3649595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software practitioners use various methods in Requirements Engineering (RE) to elicit, analyze, and specify the requirements of enterprise products. The methods impact the final product characteristics and influence product delivery. Ad-hoc usage of the methods by software practitioners can lead to inconsistency and ambiguity in the product. With the notable rise in enterprise products, games, and so forth across various domains, Virtual Reality (VR) has become an essential technology for the future. The methods adopted for RE for developing VR products requires a detailed study. This article presents a mapping study on RE methods prescribed and used for developing VR applications including requirements elicitation, requirements analysis, and requirements specification. Our study provides insights into the use of such methods in the VR community and suggests using specific RE methods in various fields of interest. We also discuss future directions in RE for VR products.},
  archive      = {J_TOSEM},
  author       = {Sai Anirudh Karre and Y. Raghu Reddy and Raghav Mittal},
  doi          = {10.1145/3649595},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {88:1–31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {RE methods for virtual reality software product development: A mapping study},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using voice and biofeedback to predict user engagement
during product feedback interviews. <em>TOSEM</em>, <em>33</em>(4),
87:1–36. (<a href="https://doi.org/10.1145/3635712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing users’ engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collecting and analyzing users’ feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in contexts where online feedback is limited, as for the majority of apps, and software in general. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this article, we propose to utilize biometric data, in terms of physiological and voice features, to complement product feedback interviews with information about the engagement of the user on product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback ) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users’ engagement by training supervised machine learning algorithms on biofeedback and voice data, and that voice features alone can be sufficiently effective. The best configurations evaluated achieve an average F1 ∼ 70% in terms of classification performance, and use voice features only. This work is one of the first studies in requirements engineering in which biometrics are used to identify emotions. Furthermore, this is one of the first studies in software engineering that considers voice analysis. The usage of voice features can be particularly helpful for emotion-aware feedback collection in remote communication, either performed by human analysts or voice-based chatbots, and can also be exploited to support the analysis of meetings in software engineering research.},
  archive      = {J_TOSEM},
  author       = {Alessio Ferrari and Thaide Huichapa and Paola Spoletini and Nicole Novielli and Davide Fucci and Daniela Girardi},
  doi          = {10.1145/3635712},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {87:1–36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Using voice and biofeedback to predict user engagement during product feedback interviews},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing effectiveness of test suites: What do we know and
what should we do? <em>TOSEM</em>, <em>33</em>(4), 86:1–32. (<a
href="https://doi.org/10.1145/3635713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background. Software testing is a critical activity for ensuring the quality and reliability of software systems. To evaluate the effectiveness of different test suites, researchers have developed a variety of metrics. Problem. However, comparing these metrics is challenging due to the lack of a standardized evaluation framework including comprehensive factors. As a result, researchers often focus on single factors (e.g., size), which finally leads to different or even contradictory conclusions. After comparing dozens of pieces of work in detail, we have found two main problems most troubling to our community: (1) researchers tend to oversimplify the description of the ground truth they use, and (2) data involving real defects is not suitable for analysis using traditional statistical indicators. Objective. We aim at scrutinizing the whole process of comparing test suites for our community. Method. To hit this aim, we propose a framework ASSENT (ev A luating te S t S uite E ffective N ess me T rics) to guide the follow-up research for evaluating a test suite effectiveness metric. ASSENT consists of three fundamental components: ground truth, benchmark test suites, and agreement indicator. Its functioning is as follows: first, users clarify the ground truth for determining the real order in effectiveness among test suites. Second, users generate a set of benchmark test suites and derive their ground truth order in effectiveness. Third, users use the metric to derive the order in effectiveness for the same test suites. Finally, users calculate the agreement indicator between the two orders derived by two metrics. Result. With ASSENT, we are able to compare the accuracy of different test suite effectiveness metrics. We apply ASSENT to evaluate representative test suite effectiveness metrics, including mutation score and code coverage metrics. Our results show that, based on the real faults, mutation score, and subsuming mutation score are the best metrics to quantify test suite effectiveness. Meanwhile, by using mutants instead of real faults, test effectiveness will be overestimated by more than 20% in values. Conclusion. We recommend that the standardized evaluation framework ASSENT should be used for evaluating and comparing test effectiveness metrics in the future work.},
  archive      = {J_TOSEM},
  author       = {Peng Zhang and Yang Wang and Xutong Liu and Zeyu Lu and Yibiao Yang and Yanhui Li and Lin Chen and Ziyuan Wang and Chang-Ai Sun and Xiao Yu and Yuming Zhou},
  doi          = {10.1145/3635713},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {86:1–32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Assessing effectiveness of test suites: What do we know and what should we do?},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PACE: A program analysis framework for continuous
performance prediction. <em>TOSEM</em>, <em>33</em>(4), 85:1–23. (<a
href="https://doi.org/10.1145/3637230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development teams establish elaborate continuous integration pipelines containing automated test cases to accelerate the development process of software. Automated tests help to verify the correctness of code modifications decreasing the response time to changing requirements. However, when the software teams do not track the performance impact of pending modifications, they may need to spend considerable time refactoring existing code. This article presents PACE , a program analysis framework that provides continuous feedback on the performance impact of pending code updates. We design performance microbenchmarks by mapping the execution time of functional test cases given a code update. We map microbenchmarks to code stylometry features and feed them to predictors for performance predictions. Our experiments achieved significant performance in predicting code performance, outperforming current state-of-the-art by 75% on neural-represented code stylometry features.},
  archive      = {J_TOSEM},
  author       = {Chidera Biringa and Gökhan Kul},
  doi          = {10.1145/3637230},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {85:1–23},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {PACE: A program analysis framework for continuous performance prediction},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EASE: An effort-aware extension of unsupervised key class
identification approaches. <em>TOSEM</em>, <em>33</em>(4), 84:1–43. (<a
href="https://doi.org/10.1145/3635714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Key class identification approaches aim at identifying the most important classes to help developers, especially newcomers, start the software comprehension process. So far, many supervised and unsupervised approaches have been proposed; however, they have not considered the effort to comprehend classes. In this article, we identify the challenge of “ effort-aware key class identification ”; to partially tackle it, we propose an approach, EASE , which is implemented through a modification to existing unsupervised key class identification approaches to take into consideration the effort to comprehend classes. First, EASE chooses a set of network metrics that has a wide range of applications in the existing unsupervised approaches and also possesses good discriminatory power . Second, EASE normalizes the network metric values of classes to quantify the probability of any class to be a key class and utilizes Cognitive Complexity to estimate the effort required to comprehend classes. Third, EASE proposes a metric, RKCP , to measure the relative key-class proneness of classes and further uses it to sort classes in descending order. Finally, an effort threshold is utilized, and the top-ranked classes within the threshold are identified as the cost-effective key classes. Empirical results on a set of 18 software systems show that (i) the proposed effort-aware variants perform significantly better in almost all (≈98.33%) the cases, (ii) they are superior to most of the baseline approaches with only several exceptions, and (iii) they are scalable to large-scale software systems. Based on these findings, we suggest that (i) we should resort to effort-aware key class identification techniques in budget-limited scenarios; and (ii) when using different techniques, we should carefully choose the weighting mechanism to obtain the best performance.},
  archive      = {J_TOSEM},
  author       = {Weifeng Pan and Marouane Kessentini and Hua Ming and Zijiang Yang},
  doi          = {10.1145/3635714},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {4},
  number       = {4},
  pages        = {84:1–43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {EASE: An effort-aware extension of unsupervised key class identification approaches},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Understanding developers well-being and productivity: A
2-year longitudinal analysis during the COVID-19 pandemic—RCR report.
<em>TOSEM</em>, <em>33</em>(3), 82:1–4. (<a
href="https://doi.org/10.1145/3640338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The artifact accompanying the paper “Understanding Developers Well-Being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic” provides a comprehensive set of tools, data, and scripts that were utilized in the longitudinal study. Spanning 24 months, from April 2020 to April 2022, the study delves into the shifts in well-being, productivity, social contacts, needs, and several other variables of software engineers during the COVID-19 pandemic. The artifact facilitates the reproduction of the study’s findings, offering a deeper insight into the systematic changes observed in various variables, such as well-being, quality of social contacts, and emotional loneliness. By providing access to the evidence-generating mechanisms and the generated data, the artifact ensures transparency and reproducibility and allows researchers to use our rich dataset to test their own research question. This Replicated Computational Results report aims to detail the contents of the artifact, its relevance to the main paper, and guidelines for its effective utilization.},
  archive      = {J_TOSEM},
  author       = {Daniel Russo and Paul H. P. Hanel and Niels van Berkel},
  doi          = {10.1145/3640338},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {82:1–4},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Understanding developers well-being and productivity: A 2-year longitudinal analysis during the COVID-19 Pandemic—RCR report},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Early validation and verification of system behaviour in
model-based systems engineering: A systematic literature review.
<em>TOSEM</em>, <em>33</em>(3), 81:1–67. (<a
href="https://doi.org/10.1145/3631976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Systems Engineering (SE) domain there has been a paradigm shift from document-based to model-based system development artefacts; in fact, new methodologies are emerging to meet the increasing complexity of current systems and the corresponding growing need of digital workflows. In this regard, Model-Based Systems Engineering (MBSE) is considered as a key enabler by many central players of the SE community. MBSE has reached an adequate level of maturity, and there exist documented success stories in its adoption in industry. In particular, one significant benefit of utilising MBSE when compared to the traditional manual and document-centric workflows is that models are available from early phases of systems development; these enable a multitude of analyses prior any implementation effort together with other relevant capabilities, like the automation of development tasks. Nonetheless, it is noticeable there is a lack of a common understanding for how formal analyses for the verification and validation (V&amp;V) of systems behaviour, specifically in the early phases of development, could be placed in an MBSE setting. In this article, we report on the planning, execution, and results of a systematic literature review regarding the early V&amp;V of systems behaviour in the context of model-based systems engineering. The review aims to provide a structured representation of the state of the art with respect to motivations, proposed solutions, and limitations. From an initial set of potentially relevant 701 peer-reviewed publications we selected 149 primary studies, which we analysed according to a rigorous data extraction, analysis, and synthesis process. Based on our results, early V&amp;V has usually the goal of checking the quality of a system design to avoid discovering flaws when parts are being concretely realised; SysML is a de facto standard for describing the system under study, while the solutions for the analyses tend to be varied; also V&amp;V analyses tend to target varied properties with a slight predominance of functional concerns, and following the variation mentioned so far the proposed solutions are largely context specific; the proposed approaches are usually presented without explicit limitations, while when limitations are discussed, readiness of the solutions, handling of analyses simplifications/assumptions, and languages/tools integration are among the most frequently mentioned issues. Based on the survey results and the standard SE practices, we discuss how the current state-of-the-art MBSE supports early V&amp;V of systems behaviour with a special focus on industrial adoption and identify relevant challenges to be researched further.},
  archive      = {J_TOSEM},
  author       = {Johan Cederbladh and Antonio Cicchetti and Jagadish Suryadevara},
  doi          = {10.1145/3631976},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {81:1–67},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Early validation and verification of system behaviour in model-based systems engineering: A systematic literature review},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ethics in the age of AI: An analysis of AI practitioners’
awareness and challenges. <em>TOSEM</em>, <em>33</em>(3), 80:1–35. (<a
href="https://doi.org/10.1145/3635715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI—AI practitioners—have to say about their understanding of AI ethics and the challenges associated with incorporating it into the AI-based systems they develop? Understanding AI practitioners’ views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners’ awareness of AI ethics and their challenges in incorporating ethics. Based on 100 AI practitioners’ responses, our findings indicate that the majority of AI practitioners had a reasonable familiarity with the concept of AI ethics, primarily due to workplace rules and policies . Privacy protection and security was the ethical principle that the majority of them were aware of. Formal education/training was considered somewhat helpful in preparing practitioners to incorporate AI ethics. The challenges that AI practitioners faced in the development of ethical AI-based systems included (i) general challenges, (ii) technology-related challenges, and (iii) human-related challenges. We also identified areas needing further investigation and provided recommendations to assist AI practitioners and companies in incorporating ethics into AI development.},
  archive      = {J_TOSEM},
  author       = {Aastha Pant and Rashina Hoda and Simone V. Spiegler and Chakkrit Tantithamthavorn and Burak Turhan},
  doi          = {10.1145/3635715},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {80:1–35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Ethics in the age of AI: An analysis of AI practitioners’ awareness and challenges},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compositional verification of first-order masking
countermeasures against power side-channel attacks. <em>TOSEM</em>,
<em>33</em>(3), 79:1–38. (<a
href="https://doi.org/10.1145/3635707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power side-channel attacks allow an adversary to efficiently and effectively steal secret information (e.g., keys) by exploiting the correlation between secret data and runtime power consumption, hence posing a serious threat to software security, particularly cryptographic implementations. Masking is a commonly used countermeasure against such attacks, which breaks the statistical dependence between secret data and side-channel leaks via randomization. In a nutshell, a variable is represented by a vector of shares armed with random variables, called masking encoding , on which cryptographic computations are performed. While compositional verification for the security of masked cryptographic implementations has received much attention because of its high efficiency, existing compositional approaches either use implicitly fixed pre-conditions that may not be fulfilled by state-of-the-art efficient implementations, or require user-provided hard-coded pre-conditions that are time consuming and highly non-trivial, even for an expert. In this article, we tackle the compositional verification problem of first-order masking countermeasures, where first-order means that the adversary is allowed to access only one intermediate computation result. Following the literature, we consider countermeasures given as gadgets, which are special procedures whose inputs are masking encodings of variables. We introduce a new security notion parameterized by an explicit pre-condition for each gadget, as well as composition rules for reasoning about masking countermeasures against power side-channel attacks. We propose accompanying efficient algorithms to automatically infer proper pre-conditions, based on which our new compositional approach can efficiently and automatically prove security for masked implementations. We implement our approaches as a tool MaskCV and conduct experiments on publicly available masked cryptographic implementations including 10 different full AES implementations. The experimental results confirm the effectiveness and efficiency of our approach.},
  archive      = {J_TOSEM},
  author       = {Pengfei Gao and Fu Song and Taolue Chen},
  doi          = {10.1145/3635707},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {79:1–38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Compositional verification of first-order masking countermeasures against power side-channel attacks},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision transformer inspired automated vulnerability repair.
<em>TOSEM</em>, <em>33</em>(3), 78:1–29. (<a
href="https://doi.org/10.1145/3632746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, automated vulnerability repair approaches have been widely adopted to combat increasing software security issues. In particular, transformer-based encoder-decoder models achieve competitive results. Whereas vulnerable programs may only consist of a few vulnerable code areas that need repair, existing AVR approaches lack a mechanism guiding their model to pay more attention to vulnerable code areas during repair generation. In this article, we propose a novel vulnerability repair framework inspired by the Vision Transformer based approaches for object detection in the computer vision domain. Similar to the object queries used to locate objects in object detection in computer vision, we introduce and leverage vulnerability queries (VQs) to locate vulnerable code areas and then suggest their repairs. In particular, we leverage the cross-attention mechanism to achieve the cross-match between VQs and their corresponding vulnerable code areas. To strengthen our cross-match and generate more accurate vulnerability repairs, we propose to learn a novel vulnerability mask (VM) and integrate it into decoders’ cross-attention, which makes our VQs pay more attention to vulnerable code areas during repair generation. In addition, we incorporate our VM into encoders’ self-attention to learn embeddings that emphasize the vulnerable areas of a program. Through an extensive evaluation using the real-world 5,417 vulnerabilities, our approach outperforms all of the automated vulnerability repair baseline methods by 2.68% to 32.33%. Additionally, our analysis of the cross-attention map of our approach confirms the design rationale of our VM and its effectiveness. Finally, our survey study with 71 software practitioners highlights the significance and usefulness of AI-generated vulnerability repairs in the realm of software security. The training code and pre-trained models are available at https://github.com/awsm-research/VQM.},
  archive      = {J_TOSEM},
  author       = {Michael Fu and Van Nguyen and Chakkrit Tantithamthavorn and Dinh Phung and Trung Le},
  doi          = {10.1145/3632746},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {78:1–29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Vision transformer inspired automated vulnerability repair},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based relaxation of completeness requirements for
data entry forms. <em>TOSEM</em>, <em>33</em>(3), 77:1–32. (<a
href="https://doi.org/10.1145/3635708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data entry forms use completeness requirements to specify the fields that are required or optional to fill for collecting necessary information from different types of users. However, because of the evolving nature of software, some required fields may not be applicable for certain types of users anymore. Nevertheless, they may still be incorrectly marked as required in the form; we call such fields obsolete required fields. Since obsolete required fields usually have “not-null” validation checks before submitting the form, users have to enter meaningless values in such fields to complete the form submission. These meaningless values threaten the quality of the filled data and could negatively affect stakeholders or learning-based tools that use the data. To avoid users filling meaningless values, existing techniques usually rely on manually written rules to identify the obsolete required fields and relax their completeness requirements. However, these techniques are ineffective and costly. In this article, we propose LACQUER, a learning-based automated approach for relaxing the completeness requirements of data entry forms. LACQUER builds Bayesian Network models to automatically learn conditions under which users had to fill meaningless values. To improve its learning ability, LACQUER identifies the cases where a required field is only applicable for a small group of users and uses SMOTE, an oversampling technique, to generate more instances on such fields for effectively mining dependencies on them. During the data entry session, LACQUER predicts the completeness requirement of a target based on the already filled fields and their conditional dependencies in the trained model. Our experimental results show that LACQUER can accurately relax the completeness requirements of required fields in data entry forms with precision values ranging between 0.76 and 0.90 on different datasets. LACQUER can prevent users from filling 20% to 64% of meaningless values, with negative predictive values (i.e., the ability to correctly predict a field as “optional”) between 0.72 and 0.91. Furthermore, LACQUER is efficient; it takes at most 839 ms to predict the completeness requirement of an instance.},
  archive      = {J_TOSEM},
  author       = {Hichem Belgacem and Xiaochen Li and Domenico Bianculli and Lionel Briand},
  doi          = {10.1145/3635708},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {77:1–32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Learning-based relaxation of completeness requirements for data entry forms},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Algorithm selection for software verification using graph
neural networks. <em>TOSEM</em>, <em>33</em>(3), 76:1–36. (<a
href="https://doi.org/10.1145/3637225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of software verification has produced a wide array of algorithmic techniques that can prove a variety of properties of a given program. It has been demonstrated that the performance of these techniques can vary up to 4 orders of magnitude on the same verification problem. Even for verification experts, it is difficult to decide which tool will perform best on a given problem. For general users, deciding the best tool for their verification problem is effectively impossible. In this work, we present Graves , a selection strategy based on graph neural networks (GNNs). Graves generates a graph representation of a program from which a GNN predicts a score for a verifier that indicates its performance on the program. We evaluate Graves on a set of 10 verification tools and over 8,000 verification problems and find that it improves the state-of-the-art in verification algorithm selection by 12%, or 8 percentage points. Further, it is able to verify 9% more problems than any existing verifier on our test set. Through a qualitative study on model interpretability, we find strong evidence that the Graves model learns to base its predictions on factors that relate to the unique features of the algorithmic techniques.},
  archive      = {J_TOSEM},
  author       = {Will Leeson and Matthew B. Dwyer},
  doi          = {10.1145/3637225},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {76:1–36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Algorithm selection for software verification using graph neural networks},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An extractive-and-abstractive framework for source code
summarization. <em>TOSEM</em>, <em>33</em>(3), 75:1–39. (<a
href="https://doi.org/10.1145/3632742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {(Source) Code summarization aims to automatically generate summaries/comments for given code snippets in the form of natural language. Such summaries play a key role in helping developers understand and maintain source code. Existing code summarization techniques can be categorized into extractive methods and abstractive methods . The extractive methods extract a subset of important statements and keywords from the code snippet using retrieval techniques and generate a summary that preserves factual details in important statements and keywords. However, such a subset may miss identifier or entity naming, and consequently, the naturalness of the generated summary is usually poor. The abstractive methods can generate human-written-like summaries leveraging encoder-decoder models. However, the generated summaries often miss important factual details. To generate human-written-like summaries with preserved factual details, we propose a novel extractive-and-abstractive framework. The extractive module in the framework performs the task of extractive code summarization, which takes in the code snippet and predicts important statements containing key factual details. The abstractive module in the framework performs the task of abstractive code summarization, which takes in the code snippet and important statements in parallel and generates a succinct and human-written-like natural language summary. We evaluate the effectiveness of our technique, called EACS, by conducting extensive experiments on three datasets involving six programming languages. Experimental results show that EACS significantly outperforms state-of-the-art techniques for all three widely used metrics, including BLEU, METEOR, and ROUGH-L. In addition, the human evaluation demonstrates that the summaries generated by EACS have higher naturalness and informativeness and are more relevant to given code snippets.},
  archive      = {J_TOSEM},
  author       = {Weisong Sun and Chunrong Fang and Yuchen Chen and Quanjun Zhang and Guanhong Tao and Yudu You and Tingxu Han and Yifei Ge and Yuling Hu and Bin Luo and Zhenyu Chen},
  doi          = {10.1145/3632742},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {75:1–39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An extractive-and-abstractive framework for source code summarization},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causality-driven testing of autonomous driving systems.
<em>TOSEM</em>, <em>33</em>(3), 74:1–35. (<a
href="https://doi.org/10.1145/3635709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing Autonomous Driving Systems (ADS) is essential for safe development of self-driving cars. For thorough and realistic testing, ADS are usually embedded in a simulator and tested in interaction with the simulated environment. However, their high complexity and the multiple safety requirements lead to costly and ineffective testing. Recent techniques exploit many-objective strategies and ML to efficiently search the huge input space. Despite the indubitable advances, the need for smartening the search keep being pressing. This article presents CART ( CAusal-Reasoning-driven Testing ), a new technique that formulates testing as a causal reasoning task. Learning causation, unlike correlation, allows assessing the effect of actively changing an input on the output, net of possible confounding variables. CART first infers the causal relations between test inputs and outputs, then looks for promising tests by querying the learnt model. Only tests suggested by the model are run on the simulator. An extensive empirical evaluation, using Pylot as ADS and CARLA as simulator, compares CART with state-of-the-art algorithms used recently on ADS. CART shows a significant gain in exposing more safety violations and does so more efficiently. More broadly, the work opens to a wider exploitation of causal learning beside (or on top of) ML for testing-related tasks.},
  archive      = {J_TOSEM},
  author       = {Luca Giamattei and Antonio Guerriero and Roberto Pietrantuono and Stefano Russo},
  doi          = {10.1145/3635709},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {74:1–35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Causality-driven testing of autonomous driving systems},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing of deep reinforcement learning agents with surrogate
models. <em>TOSEM</em>, <em>33</em>(3), 73:1–33. (<a
href="https://doi.org/10.1145/3631970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (DRL) has received a lot of attention from the research community in recent years. As the technology moves away from game playing to practical contexts, such as autonomous vehicles and robotics, it is crucial to evaluate the quality of DRL agents. In this article, we propose a search-based approach to test such agents. Our approach, implemented in a tool called Indago , trains a classifier on failure and non-failure environment (i.e., pass) configurations resulting from the DRL training process. The classifier is used at testing time as a surrogate model for the DRL agent execution in the environment, predicting the extent to which a given environment configuration induces a failure of the DRL agent under test. The failure prediction acts as a fitness function, guiding the generation towards failure environment configurations, while saving computation time by deferring the execution of the DRL agent in the environment to those configurations that are more likely to expose failures. Experimental results show that our search-based approach finds 50% more failures of the DRL agent than state-of-the-art techniques. Moreover, such failures are, on average, 78% more diverse; similarly, the behaviors of the DRL agent induced by failure configurations are 74% more diverse.},
  archive      = {J_TOSEM},
  author       = {Matteo Biagiola and Paolo Tonella},
  doi          = {10.1145/3631970},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {73:1–33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Testing of deep reinforcement learning agents with surrogate models},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PTM-APIRec: Leveraging pre-trained models of source code in
API recommendation. <em>TOSEM</em>, <em>33</em>(3), 72:1–30. (<a
href="https://doi.org/10.1145/3632745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommending APIs is a practical and essential feature of IDEs. Improving the accuracy of API recommendations is an effective way to improve coding efficiency. With the success of deep learning in software engineering, the state-of-the-art (SOTA) performance of API recommendation is also achieved by deep-learning-based approaches. However, existing SOTAs either only consider the API sequences in the code snippets or rely on complex operations for extracting hand-crafted features, all of which have potential risks in under-encoding the input code snippets and further resulting in sub-optimal recommendation performance. To this end, this article proposes to utilize the code understanding ability of existing general code P re- T raining M odels to fully encode the input code snippet to improve the accuracy of API Rec ommendation, namely, PTM-APIRec . To ensure that the code semantics of the input are fully understood and the API recommended actually exists, we use separate vocabularies for the input code snippet and the APIs to be predicted. The experimental results on the JDK and Android datasets show that PTM-APIRec surpasses existing approaches. Besides, an effective way to improve the performance of PTM-APIRec is to enhance the pre-trained model with more pre-training data (which is easier to obtain than API recommendation datasets).},
  archive      = {J_TOSEM},
  author       = {Zhihao Li and Chuanyi Li and Ze Tang and Wanhong Huang and Jidong Ge and Bin Luo and Vincent Ng and Ting Wang and Yucheng Hu and Xiaopeng Zhang},
  doi          = {10.1145/3632745},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {72:1–30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {PTM-APIRec: Leveraging pre-trained models of source code in API recommendation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SourcererJBF: A java build framework for large-scale
compilation. <em>TOSEM</em>, <em>33</em>(3), 71:1–35. (<a
href="https://doi.org/10.1145/3635710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers and tool developers working on dynamic analysis, software testing, automated program repair, verification, and validation, need large compiled, compilable, and executable code corpora to test their ideas. The publicly available corpora are relatively small, and/or non-compilable, and/or non-executable. Developing a compiled code corpus is a laborious activity demanding significant manual effort and human intervention. To facilitate large-scale program analysis research, we develop SourcererJBF , a J ava B uild F ramework that can automatically build a large Java code corpus without project-specific instructions and human intervention. To generate a compiled code corpus, SourcererJBF creates an offline knowledge base by collecting external dependencies from the project directories and existing build scripts (if available). It constructs indices of those collected external dependencies that enable a fast search for resolving dependencies during the project compilation. As the output of the large-scale compilation, it produces JAigantic, a compilable Java corpus containing compiled projects, their bytecode, dependencies, normalized build script, and build command. We evaluated SourcererJBF’s effectiveness, correctness, performance, and scalability in a large collection of Java projects. Our experimental results demonstrate that SourcererJBF is significantly effective and scalable in building large Java code corpus. Besides, it substantiates reasonable performance and correctness similar to projects’ existing build systems.},
  archive      = {J_TOSEM},
  author       = {Md Rakib Hossain Misu and Rohan Achar and Cristina V. Lopes},
  doi          = {10.1145/3635710},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {71:1–35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {SourcererJBF: A java build framework for large-scale compilation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reusing convolutional neural network models through
modularization and composition. <em>TOSEM</em>, <em>33</em>(3), 70:1–39.
(<a href="https://doi.org/10.1145/3632744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread success of deep learning technologies, many trained deep neural network (DNN) models are now publicly available. However, directly reusing the public DNN models for new tasks often fails due to mismatching functionality or performance. Inspired by the notion of modularization and composition in software reuse, we investigate the possibility of improving the reusability of DNN models in a more fine-grained manner. Specifically, we propose two modularization approaches named CNNSplitter and GradSplitter, which can decompose a trained convolutional neural network (CNN) model for N -class classification into N small reusable modules. Each module recognizes one of the N classes and contains a part of the convolution kernels of the trained CNN model. Then, the resulting modules can be reused to patch existing CNN models or build new CNN models through composition. The main difference between CNNSplitter and GradSplitter lies in their search methods: the former relies on a genetic algorithm to explore search space, while the latter utilizes a gradient-based search method. Our experiments with three representative CNNs on three widely used public datasets demonstrate the effectiveness of the proposed approaches. Compared with CNNSplitter, GradSplitter incurs less accuracy loss, produces much smaller modules (19.88% fewer kernels), and achieves better results on patching weak models. In particular, experiments on GradSplitter show that (1) by patching weak models, the average improvement in terms of precision, recall, and F1-score is 17.13%, 4.95%, and 11.47%, respectively, and (2) for a new task, compared with the models trained from scratch, reusing modules achieves similar accuracy (the average loss of accuracy is only 2.46%) without a costly training process. Our approaches provide a viable solution to the rapid development and improvement of CNN models.},
  archive      = {J_TOSEM},
  author       = {Binhang Qi and Hailong Sun and Hongyu Zhang and Xiang Gao},
  doi          = {10.1145/3632744},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {70:1–39},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Reusing convolutional neural network models through modularization and composition},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representation learning for stack overflow posts: How far
are we? <em>TOSEM</em>, <em>33</em>(3), 69:1–24. (<a
href="https://doi.org/10.1145/3635711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content. The performance of such solutions hinges significantly on the selection of representation models for Stack Overflow posts. As the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers’ interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts. The state-of-the-art (SOTA) Stack Overflow post representation models are Post2Vec and BERTOverflow, which are built upon neural networks such as convolutional neural network and transformer architecture (e.g., BERT). Despite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and BERTOverflow) in a wide range of related tasks (i.e., tag recommendation, relatedness prediction, and API recommendation). The results show that Post2Vec cannot further improve the SOTA techniques of the considered downstream tasks, and BERTOverflow shows surprisingly poor performance. To find more suitable representation models for the posts, we further explore a diverse set of transformer-based models, including (1) general domain language models (RoBERTa, Longformer, and GPT2) and (2) language models built with software engineering related textual artifacts (CodeBERT, GraphCodeBERT, seBERT, CodeT5, PLBart, and CodeGen). This exploration shows that models like CodeBERT and RoBERTa are suitable for representing Stack Overflow posts. However, it also illustrates the “No Silver Bullet” concept, as none of the models consistently wins against all the others. Inspired by the findings, we propose SOBERT, which employs a simple yet effective strategy to improve the representation models of Stack Overflow posts by continuing the pre-training phase with the textual artifact from Stack Overflow. The overall experimental results demonstrate that SOBERT can consistently outperform the considered models and increase the SOTA performance significantly for all the downstream tasks.},
  archive      = {J_TOSEM},
  author       = {Junda He and Xin Zhou and Bowen Xu and Ting Zhang and Kisub Kim and Zhou Yang and Ferdian Thung and Ivana Clairine Irsan and David Lo},
  doi          = {10.1145/3635711},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {69:1–24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Representation learning for stack overflow posts: How far are we?},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attack as detection: Using adversarial attack methods to
detect abnormal examples. <em>TOSEM</em>, <em>33</em>(3), 68:1–45. (<a
href="https://doi.org/10.1145/3631977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new programming paradigm, deep learning (DL) has achieved impressive performance in areas such as image processing and speech recognition, and has expanded its application to solve many real-world problems. However, neural networks and DL are normally black-box systems; even worse, DL-based software are vulnerable to threats from abnormal examples, such as adversarial and backdoored examples constructed by attackers with malicious intentions as well as unintentionally mislabeled samples. Therefore, it is important and urgent to detect such abnormal examples. Although various detection approaches have been proposed respectively addressing some specific types of abnormal examples, they suffer from some limitations; until today, this problem is still of considerable interest. In this work, we first propose a novel characterization to distinguish abnormal examples from normal ones based on the observation that abnormal examples have significantly different (adversarial) robustness from normal ones. We systemically analyze those three different types of abnormal samples in terms of robustness and find that they have different characteristics from normal ones. As robustness measurement is computationally expensive and hence can be challenging to scale to large networks, we then propose to effectively and efficiently measure robustness of an input sample using the cost of adversarially attacking the input, which was originally proposed to test robustness of neural networks against adversarial examples. Next, we propose a novel detection method, named attack as detection (A 2 D for short), which uses the cost of adversarially attacking an input instead of robustness to check if it is abnormal. Our detection method is generic, and various adversarial attack methods could be leveraged. Extensive experiments show that A 2 D is more effective than recent promising approaches that were proposed to detect only one specific type of abnormal examples. We also thoroughly discuss possible adaptive attack methods to our adversarial example detection method and show that A 2 D is still effective in defending carefully designed adaptive adversarial attack methods—for example, the attack success rate drops to 0% on CIFAR10.},
  archive      = {J_TOSEM},
  author       = {Zhe Zhao and Guangke Chen and Tong Liu and Taishan Li and Fu Song and Jingyi Wang and Jun Sun},
  doi          = {10.1145/3631977},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {68:1–45},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Attack as detection: Using adversarial attack methods to detect abnormal examples},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep is better? An empirical comparison of information
retrieval and deep learning approaches to code summarization.
<em>TOSEM</em>, <em>33</em>(3), 67:1–37. (<a
href="https://doi.org/10.1145/3631975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code summarization aims to generate short functional descriptions for source code to facilitate code comprehension. While Information Retrieval (IR) approaches that leverage similar code snippets and corresponding summaries have led the early research, Deep Learning (DL) approaches that use neural models to capture statistical properties between code and summaries are now mainstream. Although some preliminary studies suggest that IR approaches are more effective in some cases, it is currently unclear how effective the existing approaches can be in general, where and why IR/DL approaches perform better, and whether the integration of IR and DL can achieve better performance. Consequently, there is an urgent need for a comprehensive study of the IR and DL code summarization approaches to provide guidance for future development in this area. This article presents the first large-scale empirical study of 18 IR, DL, and hybrid code summarization approaches on five benchmark datasets. We extensively compare different types of approaches using automatic metrics, we conduct quantitative and qualitative analyses of where and why IR and DL approaches perform better, respectively, and we also study hybrid approaches for assessing the effectiveness of integrating IR and DL. The study shows that the performance of IR approaches should not be underestimated, that while DL models perform better in predicting tokens from method signatures and capturing structural similarities in code, simple IR approaches tend to perform better in the presence of code with high similarity or long reference summaries, and that existing hybrid approaches do not perform as well as individual approaches in their respective areas of strength. Based on our findings, we discuss future research directions for better code summarization.},
  archive      = {J_TOSEM},
  author       = {Tingwei Zhu and Zhong Li and Minxue Pan and Chaoxuan Shi and Tian Zhang and Yu Pei and Xuandong Li},
  doi          = {10.1145/3631975},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {67:1–37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Deep is better? an empirical comparison of information retrieval and deep learning approaches to code summarization},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Octopus: Scaling value-flow analysis via parallel collection
of realizable path conditions. <em>TOSEM</em>, <em>33</em>(3), 66:1–33.
(<a href="https://doi.org/10.1145/3632743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Value-flow analysis is a fundamental technique in program analysis, benefiting various clients, such as memory corruption detection and taint analysis. However, existing efforts suffer from the low potential speedup that leads to a deficiency in scalability. In this work, we present a parallel algorithm Octopus to collect path conditions for realizable paths efficiently. Octopus builds on the realizability decomposition to collect the intraprocedural path conditions of different functions simultaneously on-demand and obtain realizable path conditions by concatenation, which achieves a high potential speedup in parallelization. We implement Octopus as a tool and evaluate it over 15 real-world programs. The experiment shows that Octopus significantly outperforms the state-of-the-art algorithms. Particularly, it detects NULL-pointer-dereference bugs for the project llvm with 6.3 MLoC within 6.9 minutes under the 40-thread setting. We also state and prove several theorems to demonstrate the soundness, completeness, and high potential speedup of Octopus . Our empirical and theoretical results demonstrate the great potential of Octopus in supporting various program analysis clients. The implementation has officially deployed at Ant Group, scaling the nightly code scan for massive FinTech applications.},
  archive      = {J_TOSEM},
  author       = {Wensheng Tang and Dejun Dong and Shijie Li and Chengpeng Wang and Peisen Yao and Jinguo Zhou and Charles Zhang},
  doi          = {10.1145/3632743},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {66:1–33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Octopus: Scaling value-flow analysis via parallel collection of realizable path conditions},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving automated program repair with domain adaptation.
<em>TOSEM</em>, <em>33</em>(3), 65:1–43. (<a
href="https://doi.org/10.1145/3631972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Program Repair (APR) is defined as the process of fixing a bug/defect in the source code, by an automated tool. APR tools have recently experienced promising results by leveraging state-of-the-art Neural Language Processing (NLP) techniques. APR tools such as TFix and CodeXGLUE that combine text-to-text transformers with software-specific techniques are outperforming alternatives, these days. However, in most APR studies, the train and test sets are chosen from the same set of projects (i.e., when APR fixes a bug in the test set from project A, the model has already seen example fixed bugs from project A in the training set). In the real world, however, APR models are meant to be generalizable to new and different projects. Therefore, there is a potential threat that reported APR models with high effectiveness perform poorly when the characteristics of the new project or its bugs are different than the training set’s (“Domain Shift”). In this study, we first define the problem of domain shift in automated program repair. Next, we measure the potential damage of domain shift on two recent APR models (TFix and CodeXGLUE). Based on this observation, we then propose a domain adaptation framework that can adapt an APR model for a given target project. We conduct an empirical study with three domain adaptation methods FullFineTuning , TuningWithLightWeightAdapterLayers , and CurriculumLearning and two APR models on 2,672 bugs from 12 projects. The results show that our proposed framework on average can improve the effectiveness of TFix by 13.05% and CodeXGLUE by 48.78%, in terms of “Exact Match”. Through experiments, we also show that the framework provides high efficiency and reliability (in terms of “Exposure Bias”). Using synthetic data to domain adapt TFix and CodeXGLUE on the projects with no data (Zero-shot learning), also results in an average improvement of 5.76% and 17.62% for TFix and CodeXGLUE, respectively.},
  archive      = {J_TOSEM},
  author       = {Armin Zirak and Hadi Hemmati},
  doi          = {10.1145/3631972},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {65:1–43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Improving automated program repair with domain adaptation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety of perception systems for automated driving: A case
study on apollo. <em>TOSEM</em>, <em>33</em>(3), 64:1–28. (<a
href="https://doi.org/10.1145/3631969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automotive industry is now known for its software-intensive and safety-critical nature. The industry is on a path to the holy grail of completely automating driving, starting from relatively simple operational areas like highways. One of the most challenging, evolving, and essential parts of automated driving is the software that enables understanding of surroundings and the vehicle’s own as well as surrounding objects’ relative position, otherwise known as the perception system. Current generation perception systems are formed by a combination of traditional software and machine learning-related software. With automated driving systems transitioning from research to production, it is imperative to assess their safety. We assess the safety of Apollo, the most popular open-source automotive software, at the design level for its use on a Dutch highway. We identified 58 safety requirements, 38 of which are found to be fulfilled at the design level. We observe that all requirements relating to traditional software are fulfilled, while most requirements specific to machine learning systems are not. This study unveils issues that need immediate attention; and directions for future research to make automated driving safe.},
  archive      = {J_TOSEM},
  author       = {Sangeeth Kochanthara and Tajinder Singh and Alexandru Forrai and Loek Cleophas},
  doi          = {10.1145/3631969},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {64:1–28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Safety of perception systems for automated driving: A case study on apollo},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How are multilingual systems constructed: Characterizing
language use and selection in open-source multilingual software.
<em>TOSEM</em>, <em>33</em>(3), 63:1–46. (<a
href="https://doi.org/10.1145/3631967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many years now, modern software is known to be developed in multiple languages (hence termed as multilingual or multi-language software). Yet, to date, we still only have very limited knowledge about how multilingual software systems are constructed. For instance, it is not yet really clear how different languages are used, selected together, and why they have been so in multilingual software development. Given the fact that using multiple languages in a single software project has become a norm, understanding language use and selection (i.e., language profile ) as a basic element of the multilingual construction in contemporary software engineering is an essential first step. In this article, we set out to fill this gap with a large-scale characterization study on language use and selection in open-source multilingual software. We start with presenting an updated overview of language use in 7,113 GitHub projects spanning the 5 past years by characterizing overall statistics of language profiles, followed by a deeper look into the functionality relevance/justification of language selection in these projects through association rule mining. We proceed with an evolutionary characterization of 1,000 GitHub projects for each of the 10 past years to provide a longitudinal view of how language use and selection have changed over the years, as well as how the association between functionality and language selection has been evolving. Among many other findings, our study revealed a growing trend of using three to five languages in one multilingual software project and the noticeable stableness of top language selections. We found a non-trivial association between language selection and certain functionality domains, which was less stable than that with individual languages over time. In a historical context, we also have observed major shifts in these characteristics of multilingual systems both in contrast to earlier peer studies and along the evolutionary timeline. Our findings offer essential knowledge on the multilingual construction in modern software development. Based on our results, we also provide insights and actionable suggestions for both researchers and developers of multilingual systems.},
  archive      = {J_TOSEM},
  author       = {Wen Li and Austin Marino and Haoran Yang and Na Meng and Li Li and Haipeng Cai},
  doi          = {10.1145/3631967},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {63:1–46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {How are multilingual systems constructed: Characterizing language use and selection in open-source multilingual software},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Poison attack and poison detection on deep source code
processing models. <em>TOSEM</em>, <em>33</em>(3), 62:1–31. (<a
href="https://doi.org/10.1145/3630008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the software engineering (SE) community, deep learning (DL) has recently been applied to many source code processing tasks, achieving state-of-the-art results. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat to DL models, namely, poison attacks . The attackers aim to inject insidious backdoors into DL models by poisoning the training data with poison samples. The backdoors mean that poisoned models work normally with clean inputs but produce targeted erroneous results with inputs embedded with specific triggers. By using triggers to activate backdoors, attackers can manipulate poisoned models in security-related scenarios (e.g., defect detection) and lead to severe consequences. To verify the vulnerability of deep source code processing models to poison attacks, we present a poison attack approach for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable and functionality-preserving poison samples and effectively attack deep source code processing models by poisoning the training data with poison samples. To defend against poison attacks, we further propose an effective poison detection approach named CodeDetector . CodeDetector can automatically identify poison samples in the training data. We apply CodePoisoner and CodeDetector to six deep source code processing models, including defect detection, clone detection, and code repair models. The results show that ❶ CodePoisoner conducts successful poison attacks with a high attack success rate (average: 98.3%, maximum: 100%). It validates that existing deep source code processing models have a strong vulnerability to poison attacks. ❷ CodeDetector effectively defends against multiple poison attack approaches by detecting (maximum: 100%) poison samples in the training data. We hope this work can help SE researchers and practitioners notice poison attacks and inspire the design of more advanced defense techniques.},
  archive      = {J_TOSEM},
  author       = {Jia Li ♂ and Zhuo Li and Huangzhao Zhang and Ge Li and Zhi Jin and Xing Hu and Xin Xia},
  doi          = {10.1145/3630008},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {62:1–31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Poison attack and poison detection on deep source code processing models},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A post-training framework for improving the performance of
deep learning models via model transformation. <em>TOSEM</em>,
<em>33</em>(3), 61:1–41. (<a
href="https://doi.org/10.1145/3630011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) techniques have attracted much attention in recent years and have been applied to many application scenarios. To improve the performance of DL models regarding different properties, many approaches have been proposed in the past decades, such as improving the robustness and fairness of DL models to meet the requirements for practical use. Among existing approaches, post-training is an effective method that has been widely adopted in practice due to its high efficiency and good performance. Nevertheless, its performance is still limited due to the incompleteness of training data. Additionally, existing approaches are always specifically designed for certain tasks, such as improving model robustness, which cannot be used for other purposes. In this article, we aim to fill this gap and propose an effective and general post-training framework, which can be adapted to improve the model performance from different aspects. Specifically, it incorporates a novel model transformation technique that transforms a classification model into an isomorphic regression model for fine-tuning, which can effectively overcome the problem of incomplete training data by forcing the model to strengthen the memory of crucial input features and thus improve the model performance eventually. To evaluate the performance of our framework, we have adapted it to two emerging tasks for improving DL models, i.e., robustness and fairness improvement, and conducted extensive studies by comparing it with state-of-the-art approaches. The experimental results demonstrate that our framework is indeed general, as it is effective in both tasks. Specifically, in the task of robustness improvement, our approach Dare has achieved the best results on 61.1% cases (vs. 11.1% cases achieved by baselines). In the task of fairness improvement, our approach FMT can effectively improve the fairness without sacrificing the accuracy of the models.},
  archive      = {J_TOSEM},
  author       = {Jiajun Jiang and Junjie Yang and Yingyi Zhang and Zan Wang and Hanmo You and Junjie Chen},
  doi          = {10.1145/3630011},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {61:1–41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A post-training framework for improving the performance of deep learning models via model transformation},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How important are good method names in neural code
generation? A model robustness perspective. <em>TOSEM</em>,
<em>33</em>(3), 60:1–35. (<a
href="https://doi.org/10.1145/3630010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neu RA l co D e gener A tor R obustifier (RADAR). RADAR consists of two components: RADAR -Attack and RADAR -Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR -Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR -Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR -Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.},
  archive      = {J_TOSEM},
  author       = {Guang Yang and Yu Zhou and Wenhua Yang and Tao Yue and Xiang Chen and Taolue Chen},
  doi          = {10.1145/3630010},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {60:1–35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {How important are good method names in neural code generation? a model robustness perspective},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The lost world: Characterizing and detecting undiscovered
test smells. <em>TOSEM</em>, <em>33</em>(3), 59:1–32. (<a
href="https://doi.org/10.1145/3631973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test smell refers to poor programming and design practices in testing and widely spreads throughout software projects. Considering test smells have negative impacts on the comprehension and maintenance of test code and even make code-under-test more defect-prone, it thus has great importance in mining, detecting, and refactoring them. Since Deursen et al. introduced the definition of “test smell”, several studies worked on discovering new test smells from test specifications and software practitioners’ experience. Indeed, many bad testing practices are “observed” by software developers during creating test scripts rather than through academic research and are widely discussed in the software engineering community (e.g., Stack Overflow) [ 70 , 94 ]. However, no prior studies explored new bad testing practices from software practitioners’ discussions, formally defined them as new test smell types, and analyzed their characteristics, which plays a bad role for developers in knowing these bad practices and avoiding using them during test code development. Therefore, we pick up those challenges and act by working on systematic methods to explore new test smell types from one of the most mainstream developers’ Q&amp;A platforms, i.e., Stack Overflow. We further investigate the harmfulness of new test smells and analyze possible solutions for eliminating them. We find that some test smells make it hard for developers to fix failed test cases and trace their failing reasons. To exacerbate matters, we have identified two types of test smells that pose a risk to the accuracy of test cases. Next, we develop a detector to detect test smells from software. The detector is composed of six detection methods for different smell types. These detection methods are both wrapped with a set of syntactic rules based on the code patterns extracted from different test smells and developers’ code styles. We manually construct a test smell dataset from seven popular Java projects and evaluate the effectiveness of our detector on it. The experimental results show that our detector achieves high performance in precision, recall, and F1 score. Then, we utilize our detector to detect smells from 919 real-world Java projects to explore whether the six test smells are prevalent in practice. We observe that these test smells are widely spread in 722 out of 919 Java projects, which demonstrates that they are prevalent in real-world projects. Finally, to validate the usefulness of test smells in practice, we submit 56 issue reports to 53 real-world projects with different smells. Our issue reports achieve 76.4% acceptance by conducting sentiment analysis on developers’ replies. These evaluations confirm the effectiveness of our detector and the prevalence and practicality of new test smell types on real-world projects.},
  archive      = {J_TOSEM},
  author       = {Yanming Yang and Xing Hu and Xin Xia and Xiaohu Yang},
  doi          = {10.1145/3631973},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {59:1–32},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {The lost world: Characterizing and detecting undiscovered test smells},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning from very little data: On the value of landscape
analysis for predicting software project health. <em>TOSEM</em>,
<em>33</em>(3), 58:1–22. (<a
href="https://doi.org/10.1145/3630252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g., the number of closed pull requests in 12 months time). The training data for this task may be very small (e.g., 5 years of data, collected every month means just 60 rows of training data). The models generated from such tiny datasets can make many prediction errors. Those errors can be tamed by a landscape analysis that selects better learner control parameters. Our niSNEAK tool (a) clusters the data to find the general landscape of the hyperparameters, then (b) explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g., FLASH, HYPEROPT, OPTUNA). The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as C = number of commits, I = number of closed issues, and R = number of closed pull requests, niSNEAK ’s 12-month prediction errors are {I=0%, R=33% C=47%}, whereas other methods have far larger errors of {I=61%,R=119% C=149%}. We conjecture that niSNEAK works so well since it finds the most informative regions of the hyperparameters, then jumps to those regions. Other methods (that do not reflect over the landscape) can waste time exploring less informative options. Based on the preceding, we recommend landscape analytics (e.g., niSNEAK ) especially when learning from very small datasets. This article only explores the application of niSNEAK to project health. That said, we see nothing in principle that prevents the application of this technique to a wider range of problems. To assist other researchers in repeating, improving, or even refuting our results, all our scripts and data are available on GitHub at https://github.com/zxcv123456qwe/niSneak.},
  archive      = {J_TOSEM},
  author       = {Andre Lustosa and Tim Menzies},
  doi          = {10.1145/3630252},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {58:1–22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Learning from very little data: On the value of landscape analysis for predicting software project health},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Understanding developers well-being and productivity: A
2-year longitudinal analysis during the COVID-19 pandemic.
<em>TOSEM</em>, <em>33</em>(3), 57:1–44. (<a
href="https://doi.org/10.1145/3638244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has brought significant and enduring shifts in various aspects of life, including increased flexibility in work arrangements. In a longitudinal study, spanning 24 months with six measurement points from April 2020 to April 2022, we explore changes in well-being, productivity, social contacts, and needs of software engineers during this time. Our findings indicate systematic changes in various variables. For example, well-being and quality of social contacts increased while emotional loneliness decreased as lockdown measures were relaxed. Conversely, people’s boredom and productivity remained stable. Furthermore, a preliminary investigation into the future of work at the end of the pandemic revealed a consensus among developers for a preference of hybrid work arrangements. We also discovered that prior job changes and low job satisfaction were consistently linked to intentions to change jobs if current work conditions do not meet developers’ needs. This highlights the need for software organizations to adapt to various work arrangements to remain competitive employers. Building upon our findings and the existing literature, we introduce the Integrated Job Demands-Resources and Self-Determination (IJARS) Model as a comprehensive framework to explain the well-being and productivity of software engineers during the COVID-19 pandemic.},
  archive      = {J_TOSEM},
  author       = {Daniel Russo and Paul H. P. Hanel and Niels Van Berkel},
  doi          = {10.1145/3638244},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {57:1–44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Understanding developers well-being and productivity: A 2-year longitudinal analysis during the COVID-19 pandemic},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measurement of embedding choices on cryptographic API
completion tasks. <em>TOSEM</em>, <em>33</em>(3), 56:1–30. (<a
href="https://doi.org/10.1145/3625291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we conduct a measurement study to comprehensively compare the accuracy impacts of multiple embedding options in cryptographic API completion tasks. Embedding is the process of automatically learning vector representations of program elements. Our measurement focuses on design choices of three important aspects, program analysis preprocessing , token-level embedding , and sequence-level embedding . Our findings show that program analysis is necessary even under advanced embedding. The results show 36.20% accuracy improvement, on average, when program analysis preprocessing is applied to transfer bytecode sequences into API dependence paths. With program analysis and the token-level embedding training, the embedding dep2vec improves the task accuracy from 55.80% to 92.04%. Moreover, only a slight accuracy advantage (0.55%, on average) is observed by training the expensive sequence-level embedding compared with the token-level embedding. Our experiments also suggest the differences made by the data. In the cross-app learning setup and a data scarcity scenario, sequence-level embedding is more necessary and results in a more obvious accuracy improvement (5.10%).},
  archive      = {J_TOSEM},
  author       = {Ya Xiao and Wenjia Song and Salman Ahmed and Xinyang Ge and Bimal Viswanath and Na Meng and Danfeng (Daphne) Yao},
  doi          = {10.1145/3625291},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {3},
  number       = {3},
  pages        = {56:1–30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Measurement of embedding choices on cryptographic API completion tasks},
  volume       = {33},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
