<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TACO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taco---87">TACO - 87</h2>
<ul>
<li><details>
<summary>
(2024). Agile c-states: A core c-state architecture for latency
critical applications optimizing both transition and cold-start latency.
<em>TACO</em>, <em>21</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3674734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70% with limited performance degradation (at most 2%).},
  archive      = {J_TACO},
  doi          = {10.1145/3674734},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Agile C-states: A core C-state architecture for latency critical applications optimizing both transition and cold-start latency},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of general-purpose polyhedral compilers.
<em>TACO</em>, <em>21</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3674735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the 1990s, many implementations of polyhedral compilers have been written and distributed, either as source-to-source translating compilers or integrated into wider-purpose compilers. This article provides a survey on those various available implementations as of today, 2024. First, we list and describe most commonly available polyhedral schedulers and compiler implementations. Then, we compare the general-purpose polyhedral compilers using two main criteria—robustness and performance—on the PolyBench/C set of benchmarks.},
  archive      = {J_TACO},
  doi          = {10.1145/3674735},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A survey of general-purpose polyhedral compilers},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AG-SpTRSV: An automatic framework to optimize sparse
triangular solve on GPUs. <em>TACO</em>, <em>21</em>(4), 1–25. (<a
href="https://doi.org/10.1145/3674911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Triangular Solve (SpTRSV) has long been an essential kernel in the field of scientific computing. Due to its low computational intensity and internal data dependencies, SpTRSV is hard to implement and optimize on graphics processing units (GPUs). Based on our experimental observations, existing implementations on GPUs fail to achieve the optimal performance due to their suboptimal parallelism setups and code implementations plus lack of consideration of the irregular data distribution. Moreover, their algorithm design lacks the adaptability to different input matrices, which may involve substantial manual efforts of algorithm redesigning and parameter tuning for performance consistency. In this work, we propose AG-SpTRSV, an automatic framework to optimize SpTRSV on GPUs, which provides high performance on various matrices while eliminating the costs of manual design. AG-SpTRSV abstracts the procedures of optimizing an SpTRSV kernel as a scheme and constructs a comprehensive optimization space based on it. By defining a unified code template and preparing code variants, AG-SpTRSV enables fine-grained dynamic parallelism and adaptive code optimizations to handle various tasks. Through computation graph transformation and multi-hierarchy heuristic scheduling, AG-SpTRSV generates schemes for task partitioning and mapping, which effectively address the issues of irregular data distribution and internal data dependencies. AG-SpTRSV searches for the best scheme to optimize the target kernel for the specific matrix. A learned lightweight performance model is also introduced to reduce search costs and provide an efficient end-to-end solution. Experimental results with SuiteSparse Matrix Collection on NVIDIA Tesla A100 and RTX 3080 Ti show that AG-SpTRSV outperforms state-of-the-art implementations with geometric average speedups of 2.12x ∼ 3.99x. With the performance model enabled, AG-SpTRSV can provide an efficient end-to-end solution, with preprocessing times ranging from 3.4 to 245 times of the execution time.},
  archive      = {J_TACO},
  doi          = {10.1145/3674911},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {AG-SpTRSV: An automatic framework to optimize sparse triangular solve on GPUs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperion: A highly effective page and PC based delta
prefetcher. <em>TACO</em>, <em>21</em>(4), 1–27. (<a
href="https://doi.org/10.1145/3675398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware prefetching plays an important role in modern processors for hiding memory access latency. Delta prefetchers show great potential at the L1D cache level, as they can impose small storage overhead by recording deltas. Furthermore, local delta prefetchers, such as Berti, have been shown to achieve high L1D accuracy. However, there is still room for improving the L1D coverage of existing delta prefetchers. Our goal is to develop a delta prefetcher capable of achieving both high L1D coverage and accuracy. We explore delta prefetchers trained on various types of contextual information, ranging from coarse-grained to fine-grained, and analyze their L1D coverage and accuracy. Our findings indicate that training deltas based on the access histories of both PCs and memory pages for individual PCs and memory pages can lead to increased L1D coverage alongside high accuracy. Therefore, we introduce Hyperion, a highly efficient Page and PC-based delta prefetcher. In terms of the vital component of recording access histories, we implement three different structures and engage in a detailed discussion about them. Furthermore, Hyperion utilizes micro-architecture information (e.g., L1D hits or misses, PQ occupancy) and real-time L1D accuracy to dynamically adjust its issuing mechanism, further enhancing performance and L1D accuracy. Our results show that Hyperion achieves an L1D accuracy of 92.4% and an L1D coverage of 51.9%, along with an L2C coverage of 63.0% and an LLC coverage of 67.5% across a diverse range of applications, including SPEC CPU2006, SPEC CPU2017, GAP, and PARSEC, with a baseline of no prefetching. Regarding performance, Hyperion achieves a 50.1% performance gain, outperforming the state-of-the-art delta prefetcher Berti by 5.0% over baseline across all memory-intensive traces from the four benchmark suites.},
  archive      = {J_TACO},
  doi          = {10.1145/3675398},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Hyperion: A highly effective page and PC based delta prefetcher},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MST: Topology-aware message aggregation for exascale graph
processing of traversal-centric algorithms. <em>TACO</em>,
<em>21</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3676846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents MST, a communication-efficient message library for fast graph traversal on exascale clusters. The key idea is to follow the multi-level network topology to perform topology-aware message aggregation, where small messages are gathered and scattered at each level of domain. To facilitate message aggregation, we equip MST with flexible buffer management including active buffer switching and dynamic buffer expansion. We implement MST on the newest-generation Tianhe supercomputer and evaluated its performance using various traversal-centric algorithms on both synthetic trillion-scale graphs and real-world big graphs. The results show that MST-based graph traversal is orders of magnitude faster than that based on Active Messages Library (AML). For the Graph500-BFS benchmark, MST-based Tianhe (with 77.2 K nodes) outperforms the Fugaku supercomputer (with 148.5 K nodes) by 18.53%, while Fugaku is ranked No. 1 in the latest Graph500-BFS ranking (June 2023). MST also greatly improves graph processing performance on other commercial large-scale computing systems at the National Supercomputing Center in Changsha (NSCC) and WuzhenLight.},
  archive      = {J_TACO},
  doi          = {10.1145/3676846},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {MST: Topology-aware message aggregation for exascale graph processing of traversal-centric algorithms},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of large-scale sparse matrix-vector
multiplication on multi-GPU systems. <em>TACO</em>, <em>21</em>(4),
1–24. (<a href="https://doi.org/10.1145/3676847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix-vector multiplication (SpMV) is one of the important kernels of many iterative algorithms for solving sparse linear systems. The limited storage and computational resources of individual GPUs restrict both the scale and speed of SpMV computing in problem-solving. As real-world engineering problems continue to increase in complexity, the imperative for collaborative execution of iterative solving algorithms across multiple GPUs is increasingly apparent. Although the multi-GPU-based SpMV takes less kernel execution time, it also introduces additional data transmission overhead, which diminishes the performance gains derived from parallelization across multi-GPUs. Based on the non-zero elements distribution characteristics of sparse matrices and the tradeoff between redundant computations and data transfer overhead, this article introduces a series of SpMV optimization techniques tailored for multi-GPU environments and effectively enhances the execution efficiency of iterative algorithms on multiple GPUs. First, we propose a two-level non-zero elements-based matrix partitioning method to increase the overlap of kernel execution and data transmission. Then, considering the irregular non-zero elements distribution in sparse matrices, a long-row-aware matrix partitioning method is proposed to hide more data transmissions. Finally, an optimization using redundant and inexpensive short-row execution to exchange costly data transmission is proposed. Our experimental evaluation demonstrates that, compared with the SpMV on a single GPU, the proposed method achieves an average speedup of 2.00× and 1.85× on platforms equipped with two RTX 3090 and two Tesla V100-SXM2, respectively. The average speedup of 2.65× is achieved on a platform equipped with four Tesla V100-SXM2.},
  archive      = {J_TACO},
  doi          = {10.1145/3676847},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Optimization of large-scale sparse matrix-vector multiplication on multi-GPU systems},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Environmental condition aware super-resolution acceleration
framework in server-client hierarchies. <em>TACO</em>, <em>21</em>(4),
1–26. (<a href="https://doi.org/10.1145/3678008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current landscape, high-resolution (HR) videos have gained immense popularity, promising an elevated viewing experience. Recent research has demonstrated that the video super-resolution (SR) algorithm, empowered by deep neural networks (DNNs), can substantially enhance the quality of HR videos by processing low-resolution (LR) frames. However, the existing DNN models demand significant computational resources, posing challenges for the deployment of SR algorithms on client devices. While numerous accelerators have proposed solutions, their primary focus remains on client-side optimization. In contrast, our research recognizes that the HR video is originally stored in the cloud server and presents an untapped opportunity for achieving both high accuracy and performance improvements. Building on this insight, this article introduces an end-to-end video CODEC-assisted super-resolution (E 2 SR+) algorithm, which tightly integrates the cloud server with the client device to deliver a seamless and real-time video viewing experience. We propose the motion vector search algorithm executed in a cloud server, which can search the motion vectors and residuals for a part of the HR video frames and then pack them as add-ons. We also design an auto-encoder algorithm to down-sample the residuals to save the bitstream cost while guaranteeing the quality of the residuals. Lastly, we propose a reconstruction algorithm performed in the client to quickly reconstruct the corresponding HR frames using the add-ons to skip part of the DNN computations. To implement the E 2 SR+ algorithm, we design corresponding E 2 SR+ architecture in the client, which achieves significant speedup with minimal hardware overhead. Given that the environmental condition varies in the server–client hierarchies, we believe that simply applying E 2 SR+ to all frames is irrational. Accordingly, we offer an environmental condition–aware system to chase the best performance while adapting to the diverse environment. In the system, we design a linear programming (LP) model to simulate the environment and allocate frames to three existing mechanisms. Our experimental results demonstrate that the E 2 SR+ algorithm enhances the peak signal-to-noise ratio by 1.2, 2.5, and 2.3 compared with the state-of-the-art (SOTA) methods EDVR, BasicVSR, and BasicVSR++, respectively. In terms of performance, the E 2 SR+ architecture offers significant improvements over existing SOTA methods. For instance, while BasicVSR++ requires 98 ms on an NVIDIA V100 graphics processing unit (GPU) to generate a 1,280 × 720 HR frame, the E 2 SR+ architecture reduces the execution time to just 39 ms, highlighting the efficiency and effectiveness of our proposed method. Overall, the E 2 SR+ architecture respectively achieves 1.4×, 2.2×, 4.6×, and 442.0× performance improvement compared with ADAS, ISRAcc, the NVIDIA V100 GPU, and a central processing unit. Lastly, the proposed system showcases its superiority and surpasses all the existing mechanisms in terms of execution time when varying environmental conditions.},
  archive      = {J_TACO},
  doi          = {10.1145/3678008},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Environmental condition aware super-resolution acceleration framework in server-client hierarchies},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EA4RCA: Efficient AIE accelerator design framework for
regular communication-avoiding algorithm. <em>TACO</em>, <em>21</em>(4),
1–24. (<a href="https://doi.org/10.1145/3678010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the introduction of the Adaptive Intelligence Engine (AIE), the Versal Adaptive Compute Acceleration Platform (Versal ACAP) has garnered great attention. However, the current focus of Vitis Libraries and limited research has mainly been on how to invoke AIE modules, without delving into a thorough discussion on effectively utilizing AIE in its typical use cases. As a result, the widespread adoption of Versal ACAP has been restricted. The Communication Avoidance (CA) algorithm is considered a typical application within the AIE architecture. Nevertheless, the effective utilization of AIE in CA applications remains an area that requires further exploration. We propose a top-down customized design framework, EA4RCA (Efficient AIE accelerator design framework for regular Communication-Avoiding Algorithm), specifically tailored for CA algorithms with regular communication patterns, and equipped with AIE Graph Code Generator software to accelerate the AIE design process. The primary objective of this framework is to maximize the performance of AIE while incorporating high-speed data streaming services. Experiments show that for the RCA algorithm Filter2D and Matrix Multiple (MM) with lower communication requirements and the RCA algorithm FFT with higher communication requirements, the accelerators implemented by the RA4RCA framework achieve the highest throughput improvements of 22.19×, 1.05×, and 3.88× compared with the current highest performance acceleration scheme (SOTA), and the highest energy efficiency improvements of 6.11×, 1.30× and 7.00×.},
  archive      = {J_TACO},
  doi          = {10.1145/3678010},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {EA4RCA: Efficient AIE accelerator design framework for regular communication-avoiding algorithm},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data-loader tunable knob to shorten GPU idleness for
distributed deep learning. <em>TACO</em>, <em>21</em>(4), 1–25. (<a
href="https://doi.org/10.1145/3680546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have been applied as an effective machine learning algorithm to tackle problems in different domains. However, the endeavor to train sophisticated DNN models can stretch from days into weeks, presenting substantial obstacles in the realm of research focused on large-scale DNN architectures. Distributed Deep Learning (DDL) contributes to accelerating DNN training by distributing training workloads across multiple computation accelerators, for example, graphics processing units (GPUs). Despite the considerable amount of research directed toward enhancing DDL training, the influence of data loading on GPU utilization and overall training efficacy remains relatively overlooked. It is non-trivial to optimize data-loading in DDL applications that need intensive central processing unit (CPU) and input/output (I/O) resources to process enormous training data. When multiple DDL applications are deployed on a system (e.g., Cloud and High-Performance Computing (HPC) system), the lack of a practical and efficient technique for data-loader allocation incurs GPU idleness and degrades the training throughput. Therefore, our work first focuses on investigating the impact of data-loading on the global training throughput. We then propose a throughput prediction model to predict the maximum throughput for an individual DDL training application. By leveraging the predicted results, A-Dloader is designed to dynamically allocate CPU and I/O resources to concurrently running DDL applications and use the data-loader allocation as a knob to reduce GPU idle intervals and thus improve the overall training throughput. We implement and evaluate A-Dloader in a DDL framework for a series of DDL applications arriving and completing across the runtime. Our experimental results show that A-Dloader can achieve a 28.9% throughput improvement and a 10% makespan improvement compared with allocating resources evenly across applications.},
  archive      = {J_TACO},
  doi          = {10.1145/3680546},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A data-loader tunable knob to shorten GPU idleness for distributed deep learning},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data deduplication based on content locality of transactions
to enhance blockchain scalability. <em>TACO</em>, <em>21</em>(4), 1–24.
(<a href="https://doi.org/10.1145/3680547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain is a promising infrastructure for the internet and digital economy, but it has serious scalability problems, that is, long block synchronization time and high storage cost. Conventional coarse-grained data deduplication schemes (block or file level) are proved to be ineffective on improving the scalability of blockchains. Based on comprehensive analysis on typical blockchain workloads, we propose two new locality concepts (economic and argument locality) and a novel fine-grained data deduplication scheme (transaction level) named Alias-Chain. Specifically, Alias-Chain replaces frequently used data, for example, smart contract arguments, with much shorter aliases to reduce the block sizes, which results in both shorter synchronization time and lower storage cost. Furthermore, to solve the potential consistency issue in Alias-Chain, we propose two complementary techniques: one is generating aliases from history blocks with high consistency, and the other is speeding up the generation of aliases via a specific algorithm. Our simulation results show: (1) the average transfer and SC-call transaction (a transaction used to call the smart contracts in the blockchain) sizes can be significantly reduced by up to 11.03% and 79.44% in native Ethereum, and up to 39.29% and 81.84% in Ethereum optimized by state-of-the-art techniques; and (2) the two complementary techniques well address the inconsistency risk with very limited impact on the benefit of Alias-Chain. Prototyping-based experiments are further conducted on a testbed consisting of up to 3200 miners. The results demonstrate the effectiveness and efficiency of Alias-Chain on reducing block synchronization time and storage cost under typical real-world workloads.},
  archive      = {J_TACO},
  doi          = {10.1145/3680547},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Data deduplication based on content locality of transactions to enhance blockchain scalability},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pac-sim: Simulation of multi-threaded workloads using
intelligent, live sampling. <em>TACO</em>, <em>21</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3680548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance, multi-core processors are the key to accelerating workloads in several application domains. To continue to scale performance at the limit of Moore’s Law and Dennard scaling, software and hardware designers have turned to dynamic solutions that adapt to the needs of applications in a transparent, automatic way. For example, modern hardware improves its performance and power efficiency by changing the hardware configuration, like the frequency and voltage of cores, according to a number of parameters, such as the technology used or the workload running at the time. With this level of dynamism, it is essential to simulate next-generation multi-core processors in a way that can both respond to system changes and accurately determine system performance metrics. Currently, no sampled simulation platform can achieve these goals of dynamic, fast, and accurate simulation of multi-threaded workloads. In this work, we propose a solution that allows for fast, accurate simulation in the presence of both hardware and software dynamism. To accomplish this goal, we present Pac-Sim, a novel sampled simulation methodology for fast, accurate sampled simulation that requires no upfront analysis of the workload. With our proposed methodology, it is now possible to simulate long-running dynamically scheduled multi-threaded programs with significant simulation speedups, even in the presence of dynamic hardware events. We evaluate Pac-Sim using the SPEC CPU2017, NPB, and PARSEC multi-threaded benchmarks with both static and dynamic thread scheduling. The experimental results show that Pac-Sim achieves a very low sampling error of 1.63% and 3.81% on average for statically and dynamically scheduled benchmarks, respectively. Pac-Sim also demonstrates significant simulation speedups as high as 523.5× (210.3× on average) for the training input set of SPEC CPU2017 running eight threads.},
  archive      = {J_TACO},
  doi          = {10.1145/3680548},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Pac-sim: Simulation of multi-threaded workloads using intelligent, live sampling},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A NUMA-aware version of an adaptive self-scheduling loop
scheduler. <em>TACO</em>, <em>21</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3680549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallelizing code in a shared-memory environment is commonly done utilizing loop scheduling (LS) in a fork-join manner as in OpenMP. This manner of parallelization is popular due to its ease to code, but the choice of the LS method is important when the workload per iteration is highly variable. Currently, the shared-memory environment is evolving in high-performance computing as larger chiplet-based processors with high core counts and segmented L3 cache are introduced. These processors have a stronger non-uniform memory access (NUMA) effect than the previous generation of x86-64 processors. This work attempts to modify the adaptive self-scheduling loop scheduler known as iCh ( i rregular Ch unk) for these NUMA environments while analyzing the impact of these systems on default OpenMP LS methods. In particular, iCh is as a default LS method for irregular applications (i.e., applications where the workload per iteration is highly variable) that guarantees “good” performance without tuning. The modified version, named NiCh , is demonstrated over multiple irregular applications to show the variation in performance. The work demonstrates that NiCh is able to better handle architectures with stronger NUMA effects, and particularly is better than iCh when the number of threads is greater than the number of cores. However, NiCh also comes with being less universally “good” than iCh and a set of parameters that are hardware dependent.},
  archive      = {J_TACO},
  doi          = {10.1145/3680549},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A NUMA-aware version of an adaptive self-scheduling loop scheduler},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LO-SpMM: Low-cost search for high-performance SpMM kernels
on GPUs. <em>TACO</em>, <em>21</em>(4), 1–25. (<a
href="https://doi.org/10.1145/3685277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep neural networks (DNNs) become increasingly large and complicated, pruning techniques are proposed for lower memory footprint and more efficient inference. The most critical kernel to execute pruned sparse DNNs on GPUs is Sparse-dense Matrix Multiplication (SpMM). To maximize the performance of SpMM, despite the high-performance implementation generated from advanced tensor compilers, they often take a long time to iteratively search tuning configurations. Such a long time slows down the cycle of exploring better DNN architectures or pruning algorithms. In this article, we propose LO-SpMM to efficiently generate high-performance SpMM implementations for sparse DNN inference. Based on the analysis of nonzero elements’ layout, the characterization of the GPU architecture, and a rank-based cost model, LO-SpMM can effectively reduce the search space and eliminate possibly low-performance candidates. Besides, rather than generating complete SpMM implementations for evaluation, LO-SpMM constructs simplified proxies to quickly estimate performance, thereby substantially reducing compilation and execution costs. Experimental results show that LO-SpMM can reduce the search time by 281× at most, while the performance of generated SpMM implementations is comparable to or better than the state-of-the-art sparse tensor compiling solutions.},
  archive      = {J_TACO},
  doi          = {10.1145/3685277},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {LO-SpMM: Low-cost search for high-performance SpMM kernels on GPUs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A2: Towards accelerator level parallelism for autonomous
micromobility systems. <em>TACO</em>, <em>21</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3688611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous micromobility systems (AMS) such as low-speed minicabs and robots are thriving. In AMS, multiple Deep Neural Networks execute in parallel on heterogeneous AI accelerators. An emerging paradigm called Accelerator Level Parallelism (ALP) suggests managing accelerators holistically. However, there lacks a specialized and practical solution populating ALP for an AMS, where the varying real-time requirements under different working scenarios bring an opportunity to dynamically tradeoff between latency and efficiency. Furthermore, accelerator heterogeneity introduces enormous configuration space, and the shared-memory architecture results in dynamic bandwidth interference. In this article, we propose A 2 , a novel AMS resource manager optimizing energy and memory space efficiency under variable latency constraints. We gain insight from prior Learn&amp;Control scheme to design an Analyze&amp;Adapt scheme specialized for heterogeneous AI accelerators under shared-memory architecture. It features analyzing the system thoroughly offline to support two-step adaptation online. We build a prototype of A 2 and evaluate it on a commercial edge platform. We show that A 2 achieves 32.8% improvements in power and 13.8% in memory compared with control-based methods. As for timeliness enhancement, A 2 reduces the deadline violation rate by 9.2 percentage points (12.8% → 3.6%) on average compared to directly porting Learn&amp;Control methods.},
  archive      = {J_TACO},
  doi          = {10.1145/3688611},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A2: Towards accelerator level parallelism for autonomous micromobility systems},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mentor: A memory-efficient sparse-dense matrix
multiplication accelerator based on column-wise product. <em>TACO</em>,
<em>21</em>(4), 1–25. (<a
href="https://doi.org/10.1145/3688612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse-dense matrix multiplication (SpMM) is the performance bottleneck of many high-performance and deep-learning applications, making it attractive to design specialized SpMM hardware accelerators. Unfortunately, existing hardware solutions do not take full advantage of data reuse opportunities of the input and output matrices or suffer from irregular memory access patterns. Their strategies increase the off-chip memory traffic and bandwidth pressure, leaving much room for improvement. We present Mentor , a new approach to designing SpMM accelerators. Our key insight is that column-wise dataflow, while rarely exploited in prior works, can address these issues in SpMM computations. Mentor is a software-hardware co-design approach for leveraging column-wise dataflow to improve data reuse and regular memory accesses of SpMM. On the software level, Mentor incorporates a novel streaming construction scheme to preprocess the input matrix for enabling a streaming access pattern. On the hardware level, it employs a fully pipelined design to unlock the potential of column-wise dataflow further. The design of Mentor is underpinned by a carefully designed analytical model to find the tradeoff between performance and hardware resources. We have implemented an FPGA prototype of Mentor . Experimental results show that Mentor achieves speedup by geomean 2.05× (up to 3.98×), reduces the memory traffic by geomean 2.92× (up to 4.93×), and improves bandwidth utilization by geomean 1.38× (up to 2.89×), compared with the state-of-the-art hardware solutions.},
  archive      = {J_TACO},
  doi          = {10.1145/3688612},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Mentor: A memory-efficient sparse-dense matrix multiplication accelerator based on column-wise product},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing garbage collection for ZNS SSDs via in-storage
data migration and address remapping. <em>TACO</em>, <em>21</em>(4),
1–25. (<a href="https://doi.org/10.1145/3689336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The NVMe Zoned Namespace (ZNS) is a high-performance interface for flash-based solid-state drives (SSDs), which divides the logical address space into fixed-size and sequential-write zones. Meanwhile, ZNS SSDs eliminate in-device garbage collection (GC) by shifting the responsibility of GC to the host. However, the host-side GC of ZNS SSDs is not efficient. On the one hand, data migration during GC first moves data to the host buffer and then writes back the transferred data to the new location in the SSD, resulting in an unnecessary end-to-end transfer overhead. On the other hand, due to the pre-configured mapping between zones and blocks, GC incurs a large block-to-block rewrite overhead, i.e., even if most of the data in a block of the victim zone is valid, the valid data will still be rewritten to another block in the target zone. To address these issues, this article proposes a novel ZNS SSD design that features dynamic zone mapping, termed Brick-ZNS . Brick-ZNS implements two key functionalities: in-storage data migration and address remapping. New ZNS commands are first designed to realize in-storage data migration to avoid the end-to-end transfer overhead of GC while ensuring performance predictability. Then, a remapping strategy exploiting parallel physical blocks is proposed to reduce the large block-to-block rewrite overhead while ensuring zone-level access parallelism. The basic idea of the strategy is to directly remap the parallel physical blocks with a sufficient amount of valid data in the victim zone to the target zone, hence avoiding the large block-to-block rewrite overhead. Based on a full-stack SSD emulator, the evaluation results show that Brick-ZNS improves write throughput by 25% and SSD lifetime by 1.41×.},
  archive      = {J_TACO},
  doi          = {10.1145/3689336},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Optimizing garbage collection for ZNS SSDs via in-storage data migration and address remapping},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PMGraph: Accelerating concurrent graph queries over
streaming graphs. <em>TACO</em>, <em>21</em>(4), 1–25. (<a
href="https://doi.org/10.1145/3689337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are usually a large number of concurrent graph queries (CGQs) requirements in streaming graphs. However, existing graph processing systems mainly optimize a single graph query in streaming graphs or CGQs in static graphs. They have a large number of redundant computations and expensive memory access overhead, and cannot process CGQs in streaming graphs efficiently. To address these issues, we propose PMGraph , a software-hardware collaborative accelerator for efficient processing of CGQs in streaming graphs. First, PMGraph centers on fine-grained data, selects graph queries that meet the requirements through vertex data, and utilizes the similarity between different graph queries to merge the same vertices they need to process to address the problem of a large amount of repeated access to the same data by different graph queries in CGQs, thereby reducing memory access overhead. Furthermore, it adopts the update strategy that regularizes the processing order of vertices in each graph query according to the order of the vertex dependence chain, consequently effectively reducing redundant computations. Second, we propose a CGQs-oriented scheduling strategy to increase the data overlap when different graph queries are processed, thereby further improving the performance. Finally, PMGraph prefetches the vertex information according to the global active vertex set Frontier of all graph queries, hiding the memory access latency. It also provides prefetching for the same vertices that need to be processed by different graph queries, reducing the memory access overhead. Compared with the state-of-the-art concurrent graph query software systems Kickstarter-C and Tripoline, PMGraph achieves average speedups of 5.57× and 4.58×, respectively. Compared with the state-of-the-art hardware accelerators Minnow, HATS, LCCG, and JetStream, PMGraph achieves the speedup of 3.65×, 3.41×, 1.73×, and 1.38× on average, respectively. Experimental results show that our proposed PMGraph outperforms the state-of-the-art concurrent graph processing systems and hardware accelerators.},
  archive      = {J_TACO},
  doi          = {10.1145/3689337},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {PMGraph: Accelerating concurrent graph queries over streaming graphs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DELTA: Memory-efficient training via dynamic fine-grained
recomputation and swapping. <em>TACO</em>, <em>21</em>(4), 1–25. (<a
href="https://doi.org/10.1145/3689338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accommodate the increasingly large-scale models within limited-capacity GPU memory, various coarse-grained techniques, such as recomputation and swapping, have been proposed to optimize memory usage. However, these methods have encountered limitations, either in terms of inefficient memory reduction or diminished training performance. In response to this, our article introduces dynamic tensor offloading and recomputation (DELTA), an innovative approach for memory-efficient large-scale model training that combines fine-grained memory optimization and prefetching technology to reduce memory usage while maintaining high training throughput concurrently. Initially, we formulate the problem of memory-throughput joint optimization as an easy-solving 0/1 Knapsack problem. Leveraging this formalization, we use an improving polynomial complexity heuristic algorithm to address the problem effectively. Furthermore, we introduce, to the best of our knowledge, a novel bidirectional prefetching technology into dynamic memory management that significantly accelerates the model training when compared to relying solely on recomputation or swapping. Finally, DELTA offers users an automated training execution library, eliminating the need for manual configuration or specialized expertise. Experimental results demonstrate the effectiveness of DELTA in reducing GPU memory consumption. Compared to state-of-the-art methods, DELTA achieves substantial memory savings ranging from 40% to 72%, while maintaining comparable convergence performance for various models, including ResNet-50, ResNet-101, and BERT-Large. Notably, DELTA enables the training of GPT2-Large and GPT2-XL with batch sizes increased by 5.5× and 6×, respectively, showcasing its versatility and practicality in enabling large-scale model training on GPU hardware.},
  archive      = {J_TACO},
  doi          = {10.1145/3689338},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {DELTA: Memory-efficient training via dynamic fine-grained recomputation and swapping},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimized GPU implementation for GIST descriptor.
<em>TACO</em>, <em>21</em>(4), 1–24. (<a
href="https://doi.org/10.1145/3689339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The GIST descriptor is a classic feature descriptor primarily used for scene categorization and recognition tasks. It drives a bank of Gabor filters, which respond to edges and textures at various scales and orientations to capture the spatial structures in an image. Compared to other scene recognition algorithms that rely on detailed object detection, GIST has lower computational complexity, allowing it to be widely applied. However, its internal multi-scale and multi-orientation Gabor filters also mean that systems based on it cannot be executed fast enough. This article proposes an optimized GPU kernel for the GIST descriptor. It fully takes advantage of the symmetry of Gabor filters and proposes different optimization strategies for both oblique and orthogonal orientations. Extensive experiments demonstrate that the proposed kernel is adaptable to images of various scales and different GPUs. Compared to the cuFFT library, our kernel achieves 12.09× and 3.86× acceleration on an RTX 3080 GPU and a Jetson AGX Xavier GPU, respectively.},
  archive      = {J_TACO},
  doi          = {10.1145/3689339},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {An optimized GPU implementation for GIST descriptor},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Potamoi: Accelerating neural rendering via a unified
streaming architecture. <em>TACO</em>, <em>21</em>(4), 1–25. (<a
href="https://doi.org/10.1145/3689340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Field (NeRF) has emerged as a promising alternative for photorealistic rendering. Despite recent algorithmic advancements, achieving real-time performance on today’s resource-constrained devices remains challenging. In this article, we identify the primary bottlenecks in current NeRF algorithms and introduce a unified algorithm-architecture co-design, Potamoi , designed to accommodate various NeRF algorithms. Specifically, we introduce a runtime system featuring a plug-and-play algorithm, SpaRW , which significantly reduces the per-frame computational workload and alleviates compute inefficiencies. Furthermore, our unified streaming pipeline coupled with customized hardware support effectively tames both SRAM and DRAM inefficiencies by minimizing repetitive DRAM access and completely eliminating SRAM bank conflicts. When evaluated against a baseline utilizing a dedicated DNN accelerator, our framework demonstrates a speedup and energy reduction of 53.1× and 67.7×, respectively, all while maintaining high visual quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.},
  archive      = {J_TACO},
  doi          = {10.1145/3689340},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Potamoi: Accelerating neural rendering via a unified streaming architecture},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoNST: Code generator for sparse tensor networks.
<em>TACO</em>, <em>21</em>(4), 1–24. (<a
href="https://doi.org/10.1145/3689342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse tensor networks represent contractions over multiple sparse tensors. Tensor contractions are higher-order analogs of matrix multiplication. Tensor networks arise commonly in many domains of scientific computing and data science. Such networks are typically computed using a tree of binary contractions. Several critical inter-dependent aspects must be considered in the generation of efficient code for a contraction tree, including sparse tensor layout mode order, loop fusion to reduce intermediate tensors, and the mutual dependence of loop order, mode order, and contraction order. We propose CoNST, a novel approach that considers these factors in an integrated manner using a single formulation. Our approach creates a constraint system that encodes these decisions and their interdependence, while aiming to produce reduced-order intermediate tensors via fusion. The constraint system is solved by the Z3 SMT solver and the result is used to create the desired fused loop structure and tensor mode layouts for the entire contraction tree. This structure is lowered to the IR of the TACO compiler, which is then used to generate executable code. Our experimental evaluation demonstrates significant performance improvements over current state-of-the-art sparse tensor compiler/library alternatives.},
  archive      = {J_TACO},
  doi          = {10.1145/3689342},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {CoNST: Code generator for sparse tensor networks},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PIMSAB: A processing-in-memory system with spatially-aware
communication and bit-serial-aware computation. <em>TACO</em>,
<em>21</em>(4), 1–27. (<a
href="https://doi.org/10.1145/3690824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bit-serial Processing-In-Memory (PIM) is an attractive paradigm for accelerator architectures, for parallel workloads such as Deep Learning (DL), because of its capability to achieve massive data parallelism at a low area overhead and provide orders-of-magnitude data movement savings by moving computational resources closer to the data. While many PIM architectures have been proposed, improvements are needed in communicating intermediate results to consumer kernels, for communication between tiles at scale, for reduction operations, and for efficiently performing bit-serial operations with constants. We present PIMSAB, a scalable architecture that provides a spatially aware communication network for efficient intra-tile and inter-tile data movement and provides efficient computation support for generally inefficient bit-serial compute patterns. Our architecture consists of a massive hierarchical array of compute-enabled SRAMs (CRAMs), which is codesigned with a compiler to achieve high utilization. The key novelties of our architecture are (1) in providing efficient support for spatially aware communication by providing local H-tree network for reductions, by adding explicit hardware for shuffling operands, and by deploying systolic broadcasting, as well as (2) by taking advantage of the divisible nature of bit-serial computations through adaptive precision and efficient handling of constant operations. These innovations are integrated into a tensor expressions-based programming framework (including a compiler for easy programmability) that enables simple programmer control of optimizations for mapping programs into massively parallel binaries for millions of PIM processing elements. When compared against a similarly provisioned modern Tensor Core GPU (NVIDIA A100), across common DL kernels and end-to-end DL networks (Resnet18 and BERT), PIMSAB outperforms the GPU by 4.80×, and reduces energy by 3.76×. We compare PIMSAB with similarly provisioned state-of-the-art SRAM PIM (Duality Cache) and DRAM PIM (SIMDRAM), and observe a speedup of 3.7× and 3.88×, respectively.},
  archive      = {J_TACO},
  doi          = {10.1145/3690824},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {PIMSAB: A processing-in-memory system with spatially-aware communication and bit-serial-aware computation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding silent data corruption in processors for
mitigating its effects. <em>TACO</em>, <em>21</em>(4), 1–27. (<a
href="https://doi.org/10.1145/3690825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Silent Data Corruption (SDC) in processors can lead to various application-level issues, such as incorrect calculations and even data loss. Since traditional techniques are not effective in detecting these errors, it is very hard to address problems caused by SDCs in processors. For the same reason, knowledge about these SDCs in the wild is limited. In this article, we conduct an extensive study on CPU SDCs in a large production CPU population, encompassing over one million processors. In addition to collecting overall statistics, we perform a detailed study to understand (1) whether certain processor features are particularly vulnerable and their potential impacts on applications; (2) the reproducibility of CPU SDCs and the triggering conditions (e.g., temperature) of those less reproducible SDCs; and (3) the challenges to mitigate and handle CPU SDCs. We further investigate the implications that our observations obtained from the above researches have on the SDC fault models, SDC mitigation strategies, and the future research fields. In addition, we design an efficient SDC mitigation approach called Farron, which uses prioritized testing to detect highly reproducible SDCs and temperature control to mitigate less-reproducible SDCs. Our experimental results indicate that Farron can achieve better coverage of CPU SDCs with lower overall overhead, compared to the baseline used in Alibaba Cloud. This demonstrates that our observations are able to assist in SDC mitigation.},
  archive      = {J_TACO},
  doi          = {10.1145/3690825},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Understanding silent data corruption in processors for mitigating its effects},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Access characteristic-guided remote swapping across mobile
devices. <em>TACO</em>, <em>21</em>(4), 1–25. (<a
href="https://doi.org/10.1145/3695870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory swapping ensures smooth application switching for mobile systems by caching applications in the background. To further play the role of memory swapping, remote swapping across mobile devices has been widely studied, which caches applications to nearby remote devices by remote paging. However, due to the massive remote I/Os and unguaranteed swap throughput, the current remote swapping is limited with an unsatisfactory user experience, especially under variable network conditions. This paper first studies the access characteristics of applications and clarifies the impact of various network traffic on remote swapping. Motivated by these, an efficient access characteristic-guided remote swapping framework (ACR-Swap + ) is proposed to optimize remote swapping across mobile devices with resilient remote paging. ACR-Swap + first performs selective remote paging based on the swap-in frequency of different processes and then prefetches data across devices based on the process running states. Finally, it conducts hierarchical remote paging to avoid the impact of network traffic on remote swapping. Evaluations on Google Pixel 6 show that ACR-Swap + reduces the application switching latency by 21.6% and achieves a negligible performance fluctuation under various network traffic compared to the state of the art.},
  archive      = {J_TACO},
  doi          = {10.1145/3695870},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Access characteristic-guided remote swapping across mobile devices},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stable idle time detection platform for real i/o
workloads. <em>TACO</em>, <em>21</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3695871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important to utilize the idle time of a workload to improve the system performance. In the article, we will explore multiple idle time detection methods to predict the idle time of the real I/O workloads. The objective is to build a stable idle time detection platform by investigating the impact of multiple representative methods to pursue a more stable prediction accuracy. The experimental results show that the prediction accuracy of the proposed platform can be stable between 60% and 80%.},
  archive      = {J_TACO},
  doi          = {10.1145/3695871},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A stable idle time detection platform for real I/O workloads},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards high performance QNNs via distribution-based CNOT
gate reduction. <em>TACO</em>, <em>21</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3695872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum Neural Networks (QNNs) are one of the most promising applications that can be implemented on NISQ-era quantum computers. In this study, we observe that QNNs often suffer from gate redundancy, which hugely declines the performance and accuracy of the network. Even state-of-the-art architecture search techniques like QuantumNAS do not completely alleviate this problem. Especially, we find that CNOT gates are major contributors to the execution delay and noise in quantum circuits, and there are many redundant CNOT gates in the QNN post-training. This motivates us to propose a novel distribution-based greedy-search circuit optimization technique that can be employed after the completion of the training process. Our technique significantly reduces the number of CNOT gates in QNNs without affecting the accuracy of the network. With this technique, we have achieved an average of 3× improvement in execution time while reaching a maximum of 12.4× improvement.},
  archive      = {J_TACO},
  doi          = {10.1145/3695872},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Towards high performance QNNs via distribution-based CNOT gate reduction},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SuccinctKV: A CPU-efficient LSM-tree based KV store with
scan-based compaction. <em>TACO</em>, <em>21</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3695873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CPU overhead of the LSM-tree becomes increasingly significant when high-speed storage devices are utilized. In this article, we propose SuccinctKV , a key-value store based on LSM-tree that is optimized to improve CPU efficiency in mixed workload scenarios. To achieve this, SuccinctKV reduces the CPU overhead of compaction by writing scan-sorted data directly to the storage device. SuccinctKV also redesigns the merge-sort operation of the LSM-tree, enhancing CPU locality and reducing the unnecessary CPU overhead of cache accesses and I/O system calls. Additionally, SuccinctKV introduces a scheduler to resolve potential bursty I/O contention by autonomously initiating I/O requests at appropriate times and quickly relieving I/O pressure by terminating background I/O requests. We implement SuccinctKV on RocksDB and conduct extensive experiments to evaluate our proposed methods. The experimental results demonstrate that, compared with RocksDB, SuccinctKV achieves a maximum improvement of 2.6x in scan performance and reduces CPU overhead of compaction by up to 89% under mixed workloads.},
  archive      = {J_TACO},
  doi          = {10.1145/3695873},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {11},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SuccinctKV: A CPU-efficient LSM-tree based KV store with scan-based compaction},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-core data sharing for energy-efficient GPUs.
<em>TACO</em>, <em>21</em>(3), 1–32. (<a
href="https://doi.org/10.1145/3653019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics Processing Units (GPUs) are the accelerator of choice in a variety of application domains, because they can accelerate massively parallel workloads and can be easily programmed using general-purpose programming frameworks such as CUDA and OpenCL. Each Streaming Multiprocessor (SM) contains an L1 data cache (L1D) to exploit the locality in data accesses. L1D misses are costly for GPUs for two reasons. First, L1D misses consume a lot of energy as they need to access the L2 cache (L2) via an on-chip network and the off-chip DRAM in case of L2 misses. Second, L1D misses impose performance overhead if the GPU does not have enough active warps to hide the long memory access latency. We observe that threads running on different SMs share 55% of the data they read from the memory. Unfortunately, as the L1Ds are in the non-coherent memory domain, each SM independently fetches data from the L2 or the off-chip memory into its L1D, even though the data may be currently available in the L1D of another SM. Our goal is to service L1D read misses via other SMs, as much as possible, to cut down costly accesses to the L2 or the off-chip DRAM. To this end, we propose a new data-sharing mechanism, called Cross-Core Data Sharing (CCDS) . CCDS employs a predictor to estimate whether the required cache block exists in another SM. If the block is predicted to exist in another SM’s L1D, then CCDS fetches the data from the L1D that contain the block. Our experiments on a suite of 26 workloads show that CCDS improves average energy and performance by 1.30× and 1.20×, respectively, compared to the baseline GPU. Compared to the state-of-the-art data-sharing mechanism, CCDS improves average energy and performance by 1.37× and 1.11×, respectively.},
  archive      = {J_TACO},
  doi          = {10.1145/3653019},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-32},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Cross-core data sharing for energy-efficient GPUs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ISwap: A new memory page swap mechanism for reducing
ineffective i/o operations in cloud environments. <em>TACO</em>,
<em>21</em>(3), 1–24. (<a
href="https://doi.org/10.1145/3653302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes iSwap , a new memory page swap mechanism that reduces the ineffective I/O swap operations and improves the QoS for applications with a high priority in cloud environments. iSwap works in the OS kernel. iSwap accurately learns the reuse patterns for memory pages and makes the swap decisions accordingly to avoid ineffective operations. In the cases where memory pressure is high, iSwap compresses pages that belong to the latency-critical (LC) applications (or high-priority applications) and keeps them in main memory, avoiding I/O operations for these LC applications to ensure QoS, and iSwap evicts low-priority applications’ pages out of main memory. iSwap has a low overhead and works well for cloud applications with large memory footprints. We evaluate iSwap on Intel x86 and ARM platforms. The experimental results show that iSwap can significantly reduce ineffective swap operations (8.0%–19.2%) and improve the QoS for LC applications (36.8%–91.3%) in cases where memory pressure is high, compared with the latest LRU-based approach widely used in modern OSes.},
  archive      = {J_TACO},
  doi          = {10.1145/3653302},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ISwap: A new memory page swap mechanism for reducing ineffective I/O operations in cloud environments},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReSA: Reconfigurable systolic array for multiple tiny DNN
tensors. <em>TACO</em>, <em>21</em>(3), 1–24. (<a
href="https://doi.org/10.1145/3653363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systolic array architecture has significantly accelerated deep neural networks (DNNs). A systolic array comprises multiple processing elements (PEs) that can perform multiply-accumulate (MAC). Traditionally, the systolic array can execute a certain amount of tensor data that matches the size of the systolic array simultaneously at each cycle. However, hyper-parameters of DNN models differ across each layer and result in various tensor sizes in each layer. Mapping these irregular tensors to the systolic array while fully utilizing the entire PEs in a systolic array is challenging. Furthermore, modern DNN systolic accelerators typically employ a single dataflow. However, such a dataflow is not optimal for every DNN model. This work proposes ReSA, a reconfigurable dataflow architecture that aims to minimize the execution time of a DNN model by mapping tiny tensors on the spatially partitioned systolic array. Unlike conventional systolic array architectures, the ReSA data path controller enables the execution of the input, weight, and output-stationary dataflow on PEs. ReSA also decomposes the coarse-grain systolic array into multiple small ones to reduce the fragmentation issue on the tensor mapping. Each small systolic sub-array unit relies on our data arbiter to dispatch tensors to each other through the simple interconnected network. Furthermore, ReSA reorders the memory access to overlap the memory load and execution stages to hide the memory latency when tackling tiny tensors. Finally, ReSA splits tensors of each layer into multiple small ones and searches for the best dataflow for each tensor on the host side. Then, ReSA encodes the predefined dataflow in our proposed instruction to notify the systolic array to switch the dataflow correctly. As a result, our optimization on the systolic array architecture achieves a geometric mean speedup of 1.87× over the weight-stationary systolic array architecture across nine different DNN models.},
  archive      = {J_TACO},
  doi          = {10.1145/3653363},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ReSA: Reconfigurable systolic array for multiple tiny DNN tensors},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D2Comp: Efficient offload of LSM-tree compaction with data
processing units on disaggregated storage. <em>TACO</em>,
<em>21</em>(3), 1–22. (<a
href="https://doi.org/10.1145/3656584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LSM-based key-value stores suffer from sub-optimal performance due to their slow and heavy background compactions. The compaction brings severe CPU and network overhead on high-speed disaggregated storage. This article further reveals that data-intensive compression in compaction consumes a significant portion of CPU power. Moreover, the multi-threaded compactions cause substantial CPU contention and network traffic during high-load periods. Based on the above observations, we propose fine-grained dynamical compaction offloading by leveraging the modern Data Processing Unit (DPU) to alleviate the CPU and network overhead. To achieve this, we first customized a file system to enable efficient data access for DPU. We then leverage the Arm cores on the DPU to meet the burst CPU and network requirements to reduce resource contention and data movement. We further employ dedicated hardware-based accelerators on the DPU to speed up the compression in compactions. We integrate our DPU-offloaded compaction with RocksDB and evaluate it with NVIDIA’s latest Bluefield-2 DPU on a real system. The evaluation shows that the DPU is an effective solution to solve the CPU bottleneck and reduce data traffic of compaction. The results show that compaction performance is accelerated by 2.86 to 4.03 times, system write and read throughput is improved by up to 3.2 times and 1.4 times respectively, and host CPU contention and network traffic are effectively reduced compared to the fine-tuned CPU-only baseline.},
  archive      = {J_TACO},
  doi          = {10.1145/3656584},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {D2Comp: Efficient offload of LSM-tree compaction with data processing units on disaggregated storage},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intermediate address space: Virtual memory optimization of
heterogeneous architectures for cache-resident workloads. <em>TACO</em>,
<em>21</em>(3), 1–23. (<a
href="https://doi.org/10.1145/3659207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for computing power and the emergence of heterogeneous computing architectures have driven the exploration of innovative techniques to address current limitations in both the compute and memory subsystems. One such solution is the use of Accelerated Processing Units (APUs), processors that incorporate both a central processing unit (CPU) and an integrated graphics processing unit (iGPU). However, the performance of both APU and CPU systems can be significantly hampered by address translation overhead, leading to a decline in overall performance, especially for cache-resident workloads. To address this issue, we propose the introduction of a new intermediate address space (IAS) in both APU and CPU systems. IAS serves as a bridge between virtual address (VA) spaces and physical address (PA) spaces, optimizing the address translation process. In the case of APU systems, our research indicates that the iGPU suffers from significant translation look-aside buffer (TLB) misses in certain workload situations. Using an IAS, we can divide the initial address translation into front- and back-end phases, effectively shifting the bottleneck in address translation from the cache side to the memory controller side, a technique that proves to be effective for cache-resident workloads. Our simulations demonstrate that implementing IAS in the CPU system can boost performance by up to 40% compared to conventional CPU systems. Furthermore, we evaluate the effectiveness of APU systems, comparing the performance of IAS-based systems with traditional systems, showing up to a 185% improvement in APU system performance with our proposed IAS implementation. Furthermore, our analysis indicates that over 90% of TLB misses can be filtered by the cache, and employing a larger cache within the system could potentially result in even greater improvements. The proposed IAS offers a promising and practical solution to enhance the performance of both APU and CPU systems, contributing to state-of-the-art research in the field of computer architecture.},
  archive      = {J_TACO},
  doi          = {10.1145/3659207},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Intermediate address space: Virtual memory optimization of heterogeneous architectures for cache-resident workloads},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReHarvest: An ADC resource-harvesting crossbar architecture
for ReRAM-based DNN accelerators. <em>TACO</em>, <em>21</em>(3), 1–26.
(<a href="https://doi.org/10.1145/3659208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ReRAM-based Processing-In-Memory (PIM) architectures have been increasingly explored to accelerate various Deep Neural Network (DNN) applications because they can achieve extremely high performance and energy-efficiency for in-situ analog Matrix-Vector Multiplication (MVM) operations. However, since ReRAM crossbar arrays’ peripheral circuits– analog-to-digital converters (ADCs) often feature high latency and low area efficiency, AD conversion has become a performance bottleneck of in-situ analog MVMs. Moreover, since each crossbar array is tightly coupled with very limited ADCs in current ReRAM-based PIM architectures, the scarce ADC resource is often underutilized. In this article, we propose ReHarvest, an ADC-crossbar decoupled architecture to improve the utilization of ADC resource. Particularly, we design a many-to-many mapping structure between crossbars and ADCs to share all ADCs in a tile as a resource pool, and thus one crossbar array can harvest much more ADCs to parallelize the AD conversion for each MVM operation. Moreover, we propose a multi-tile matrix mapping (MTMM) scheme to further improve the ADC utilization across multiple tiles by enhancing data parallelism. To support fine-grained data dispatching for the MTMM, we also design a bus-based interconnection network to multicast input vectors among multiple tiles, and thus eliminate data redundancy and potential network congestion during multicasting. Extensive experimental results show that ReHarvest can improve the ADC utilization by 3.2×, and achieve 3.5× performance speedup while reducing the ReRAM resource consumption by 3.1× on average compared with the state-of-the-art PIM architecture–FORMS.},
  archive      = {J_TACO},
  doi          = {10.1145/3659208},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ReHarvest: An ADC resource-harvesting crossbar architecture for ReRAM-based DNN accelerators},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An example of parallel merkle tree traversal: Post-quantum
leighton-micali signature on the GPU. <em>TACO</em>, <em>21</em>(3),
1–25. (<a href="https://doi.org/10.1145/3659209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hash-based signature (HBS) is the most conservative and time-consuming among many post-quantum cryptography (PQC) algorithms. Two HBSs, LMS and XMSS, are the only PQC algorithms standardised by the National Institute of Standards and Technology (NIST) now. Existing HBSs are designed based on serial Merkle tree traversal, which is not conducive to taking full advantage of the computing power of parallel architectures such as CPUs and GPUs. We propose a parallel Merkle tree traversal (PMTT), which is tested by implementing LMS on the GPU. This is the first work accelerating LMS on the GPU, which performs well even with over 10,000 cores. Considering different scenarios of algorithmic parallelism and data parallelism, we implement corresponding variants for PMTT. The design of PMTT for algorithmic parallelism mainly considers the execution efficiency of a single task, while that for data parallelism starts with the full utilisation of GPU performance. In addition, we are the first to design a CPU-GPU collaborative processing solution for traversal algorithms to reduce the communication overhead between CPU and GPU. For algorithmic parallelism, our implementation is still 4.48× faster than the ideal time of the state-of-the-art traversal algorithm. For data parallelism, when the number of cores increases from 1 to 8,192, the parallel efficiency is 78.39%. In comparison, our LMS implementation outperforms most existing LMS and XMSS implementations.},
  archive      = {J_TACO},
  doi          = {10.1145/3659209},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {An example of parallel merkle tree traversal: Post-quantum leighton-micali signature on the GPU},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COER: A network interface offloading architecture for RDMA
and congestion control protocol codesign. <em>TACO</em>, <em>21</em>(3),
1–26. (<a href="https://doi.org/10.1145/3660525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDMA (Remote Direct Memory Access) networks require efficient congestion control to maintain their high throughput and low latency characteristics. However, congestion control protocols deployed at the software layer suffer from slow response times due to the communication overhead between host hardware and software. This limitation has hindered their ability to meet the demands of high-speed networks and applications. Harnessing the capabilities of rapidly advancing Network Interface Cards (NICs) can drive progress in congestion control. Some simple congestion control protocols have been offloaded to RDMA NICs to enable faster detection and processing of congestion. However, offloading congestion control to the RDMA NIC faces a significant challenge in integrating the RDMA transport protocol with advanced congestion control protocols that involve complex mechanisms. We have observed that reservation-based proactive congestion control protocols share strong similarities with RDMA transport protocols, allowing them to integrate seamlessly and combine the functionalities of the transport layer and network layer. In this article, we present COER, an RDMA NIC architecture that leverages the functional components of RDMA to perform reservations and completes the scheduling of congestion control during the scheduling process of the RDMA protocol. COER facilitates the streamlined development of offload strategies for congestion control techniques —specifically, proactive congestion control —on RDMA NICs. We use COER to design offloading schemes for 11 congestion control protocols, which we implement and evaluate using a network emulator with a cycle-accurate RDMA NIC model that can load Message Passing Interface (MPI) programs. The evaluation results demonstrate that the architecture of COER does not compromise the original characteristics of the congestion control protocols. Compared with a layered protocol stack approach, COER enables the performance of RDMA networks to reach new heights.},
  archive      = {J_TACO},
  doi          = {10.1145/3660525},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {COER: A network interface offloading architecture for RDMA and congestion control protocol codesign},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge-augmented mutation-based bug localization for
hardware design code. <em>TACO</em>, <em>21</em>(3), 1–26. (<a
href="https://doi.org/10.1145/3660526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verification of hardware design code is crucial for the quality assurance of hardware products. Being an indispensable part of verification, localizing bugs in the hardware design code is significant for hardware development but is often regarded as a notoriously difficult and time-consuming task. Thus, automated bug localization techniques that could assist manual debugging have attracted much attention in the hardware community. However, existing approaches are hampered by the challenge of achieving both demanding bug localization accuracy and facile automation in a single method. Simulation-based methods are fully automated but have limited localization accuracy, slice-based techniques can only give an approximate range of the presence of bugs, and spectrum-based techniques can also only yield a reference value for the likelihood that a statement is buggy. Furthermore, formula-based bug localization techniques suffer from the complexity of combinatorial explosion for automated application in industrial large-scale hardware designs. In this work, we propose Kummel, a K nowledge-a u g m ented m utation-bas e d bug loca l ization for hardware design code to address these limitations. Kummel achieves the unity of precise bug localization and full automation by utilizing the knowledge augmentation through mutation analysis. To evaluate the effectiveness of Kummel, we conduct large-scale experiments on 76 versions of 17 hardware projects by seven state-of-the-art bug localization techniques. The experimental results clearly show that Kummel is statistically more effective than baselines, e.g., our approach can improve the seven original methods by 64.48% on average under the RImp metric. It brings fresh insights of hardware bug localization to the community.},
  archive      = {J_TACO},
  doi          = {10.1145/3660526},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Knowledge-augmented mutation-based bug localization for hardware design code},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GraphSER: Distance-aware stream-based edge repartition for
many-core systems. <em>TACO</em>, <em>21</em>(3), 1–25. (<a
href="https://doi.org/10.1145/3661998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of graph data, distributed graph processing has become popular, and many graph hardware accelerators use distributed frameworks. Graph partitioning is foundation in distributed graph processing. However, dynamic changes in graph make existing partitioning shifted from its optimized points and cause system performance degraded. Therefore, more efficient dynamic graph partition methods are needed. In this work, we propose GraphSER, a dynamic graph partition method for many-core systems. To improve the cross-node spatial locality and reduce the overhead of repartition, we propose a stream-based edge repartition, in which each computing node sequentially traverses its local edge list in parallel, then migrating edges based on distance and replica degree. GraphSER does not need costly searching and prioritizes nodes so it can avoid poor cross-node spatial locality. Our evaluation shows that compared to state-of-the-art edge repartition software methods, GraphSER has an average speedup of 1.52×, with the maximum up to 2×. Compared to the previous many-core hardware repartition method, GraphSER performance has an average of 40% improvement, with the maximum to 117%.},
  archive      = {J_TACO},
  doi          = {10.1145/3661998},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GraphSER: Distance-aware stream-based edge repartition for many-core systems},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characterizing and optimizing LDPC performance on 3D NAND
flash memories. <em>TACO</em>, <em>21</em>(3), 1–26. (<a
href="https://doi.org/10.1145/3663478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of NAND flash memories’ bit density and stacking technologies, while storage capacity keeps increasing, the issue of reliability becomes increasingly prominent. Low-density parity check (LDPC) code, as a robust error-correcting code, is extensively employed in flash memory. However, when the RBER is prohibitively high, LDPC decoding would introduce long latency. To study how LDPC performs on the latest 3D NAND flash memory, we conduct a comprehensive analysis of LDPC decoding performance using both the theoretically derived threshold voltage distribution model obtained through modeling (Modeling-based method) and the actual voltage distribution collected from on-chip data through testing (Ideal case). Based on LDPC decoding results under various interference conditions, we summarize four findings that can help us gain a better understanding of the characteristics of LDPC decoding in 3D NAND flash memory. Following our characterization, we identify the differences in LDPC decoding performance between the Modeling-based method and the Ideal case. Due to the accuracy of initial probability information, the threshold voltage distribution derived through modeling deviates by certain degrees from the actual threshold voltage distribution. This leads to a performance gap between using the threshold voltage distribution derived from the Modeling-based method and the actual distribution. By observing the abnormal behaviors in the decoding with the Modeling-based method, we introduce an Offsetted Read Voltage (ΔRV) method for optimizing LDPC decoding performance by offsetting the reading voltage in each layer of a flash block. The evaluation results show that our ΔRV method enhances the decoding performance of LDPC on the Modeling-based method by reducing the total number of sensing levels needed for LDPC decoding by 0.67% to 18.92% for different interference conditions on average, under the P/E cycles from 3,000 to 7,000.},
  archive      = {J_TACO},
  doi          = {10.1145/3663478},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Characterizing and optimizing LDPC performance on 3D NAND flash memories},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asynchronous memory access unit: Exploiting massive
parallelism for far memory access. <em>TACO</em>, <em>21</em>(3), 1–28.
(<a href="https://doi.org/10.1145/3663479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing memory demands of modern applications have driven the adoption of far memory technologies in data centers to provide cost-effective, high-capacity memory solutions. However, far memory presents new performance challenges because its access latencies are significantly longer and more variable than local DRAM. For applications to achieve acceptable performance on far memory, a high degree of memory-level parallelism (MLP) is needed to tolerate the long access latency. While modern out-of-order processors are capable of exploiting a certain degree of MLP, they are constrained by resource limitations and hardware complexity. The key obstacle is the synchronous memory access semantics of traditional load/store instructions, which occupy critical hardware resources for a long time. The longer far memory latencies exacerbate this limitation. This article proposes a set of Asynchronous Memory Access Instructions (AMI) and its supporting function unit, Asynchronous Memory Access Unit (AMU), inside contemporary Out-of-Order Core. AMI separates memory request issuing from response handling to reduce resource occupation. Additionally, AMU architecture supports up to several hundreds of asynchronous memory requests through re-purposing a portion of L2 Cache as scratchpad memory (SPM) to provide sufficient temporal storage. Together with a coroutine-based programming framework, this scheme can achieve significantly higher MLP for hiding far memory latencies. Evaluation with a cycle-accurate simulation shows AMI achieves 2.42× speedup on average for memory-bound benchmarks with 1μs additional far memory latency. Over 130 outstanding requests are supported with 26.86× speedup for GUPS (random access) with 5 μs latency. These demonstrate how the techniques tackle far memory performance impacts through explicit MLP expression and latency adaptation.},
  archive      = {J_TACO},
  doi          = {10.1145/3663479},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-28},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Asynchronous memory access unit: Exploiting massive parallelism for far memory access},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fixed-point encoding and architecture exploration for
residue number systems. <em>TACO</em>, <em>21</em>(3), 1–27. (<a
href="https://doi.org/10.1145/3664923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residue Number Systems (RNS) demonstrate the fascinating potential to serve integer addition/ multiplication-intensive applications. The complexity of Artificial Intelligence (AI) models has grown enormously in recent years. From a computer system’s perspective, ensuring the training of these large-scale AI models within an adequate time and energy consumption has become a big concern. Matrix multiplication is a dominant subroutine in many prevailing AI models, with an addition/multiplication-intensive attribute. However, the data type of matrix multiplication within machine learning training typically requires real numbers, which indicates that RNS benefits for integer applications cannot be directly gained by AI training. The state-of-the-art RNS real-number encodings, including floating-point and fixed-point, have defects and can be further enhanced. To transform default RNS benefits to the efficiency of large-scale AI training, we propose a low-cost and high-accuracy RNS fixed-point representation: Single RNS Logical Partition (S-RNS-Logic-P) representation with Scaling-down Postprocessing Multiplication (SD-Post-Mul) . Moreover, we extend the implementation details of the other two RNS fixed-point methods: Double RNS Concatenation and S-RNS-Logic-P representation with Scaling-down Preprocessing Multiplication . We also design the architectures of these three fixed-point multipliers. In empirical experiments, our S-RNS-Logic-P representation with SD-Post-Mul method achieves less latency and energy overhead while maintaining good accuracy. Furthermore, this method can easily extend to the Redundant Residue Number System to raise the efficiency of error-tolerant domains, such as improving the error correction efficiency of quantum computing.},
  archive      = {J_TACO},
  doi          = {10.1145/3664923},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Fixed-point encoding and architecture exploration for residue number systems},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of sparse matrix computation for algebraic
multigrid on GPUs. <em>TACO</em>, <em>21</em>(3), 1–27. (<a
href="https://doi.org/10.1145/3664924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMG is one of the most efficient and widely used methods for solving sparse linear systems. The computational process of AMG mainly consists of a series of iterative calculations of generalized sparse matrix-matrix multiplication (SpGEMM) and sparse matrix-vector multiplication (SpMV). Optimizing these sparse matrix calculations is crucial for accelerating solving linear systems. In this paper, we first focus on optimizing the SpGEMM algorithm in AmgX, a popular AMG library for GPUs. We propose a new algorithm called SpGEMM-upper, which achieves an average speedup of 2.02× on Tesla V100 and 1.96× on RTX 3090 against the original algorithm. Next, through experimental investigation, we conclude that no single SpGEMM library or algorithm performs optimally for most sparse matrices, and the same holds true for SpMV. Therefore, we build machine learning-based models to predict the optimal SpGEMM and SpMV used in the AMG calculation process. Finally, we integrate the prediction models, SpGEMM-upper, and other selected algorithms into a framework for adaptive sparse matrix computation in AMG. Our experimental results prove that the framework achieves promising performance improvements on the test set.},
  archive      = {J_TACO},
  doi          = {10.1145/3664924},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Optimization of sparse matrix computation for algebraic multigrid on GPUs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoolDC: A cost-effective immersion-cooled datacenter with
workload-aware temperature scaling. <em>TACO</em>, <em>21</em>(3), 1–27.
(<a href="https://doi.org/10.1145/3664925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For datacenter architects, it is the most important goal to minimize the datacenter’s total cost of ownership for the target performance (i.e., TCO/performance). As the major component of a datacenter is a server farm, the most effective way of reducing TCO/performance is to improve the server’s performance and power efficiency. To achieve the goal, we claim that it is highly promising to reduce each server’s temperature to its most cost-effective point (or temperature scaling). In this article, we propose CoolDC , a novel and immediately applicable low-temperature cooling method to minimize the datacenter’s TCO. The key idea is to find and apply the most cost-effective sub-freezing temperature to target servers and workloads. For that purpose, we first apply the immersion cooling method to the entire servers to maintain a stable low temperature with little extra cooling and maintenance costs. Second, we define the TCO-optimal temperature for datacenter operation (e.g., 248K~273K (-25℃~0℃)) by carefully estimating all the costs and benefits at low temperatures. Finally, we propose CoolDC, our immersion-cooling datacenter architecture to run every workload at its own TCO-optimal temperature. By incorporating our low-temperature workload-aware temperature scaling, CoolDC achieves 12.7% and 13.4% lower TCO/performance than the conventional air-cooled and immersion-cooled datacenters, respectively, without any modification to existing computers.},
  archive      = {J_TACO},
  doi          = {10.1145/3664925},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {CoolDC: A cost-effective immersion-cooled datacenter with workload-aware temperature scaling},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stripe-schedule aware repair in erasure-coded clusters with
heterogeneous star networks. <em>TACO</em>, <em>21</em>(3), 1–24. (<a
href="https://doi.org/10.1145/3664926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More and more storage systems use erasure code to tolerate faults. It takes pieces of data blocks as input and encodes a small number of parity blocks as output, where these blocks form a stripe. When reconsidering the recovery problem in the multi-stripe level and heterogeneous network clusters, quickly generating an efficient multi-stripe recovery solution that reduces recovery time remains a challenging and time-consuming task. Previous works either use a greedy algorithm that may fall into the local optimal and have low recovery performance or a meta-heuristic algorithm with a long running time and low solution generation efficiency. In this article, we propose a Stripe-schedule Aware Repair (SARepair) technique for multi-stripe recovery in heterogeneous erasure-coded clusters based on Reed–Solomon code. By carefully examining the metadata of blocks, SARepair intelligently adjusts the recovery solution for each stripe and obtains another multi-stripe solution with less recovery time in a computationally efficient manner. It then tolerates worse solutions to overcome the local optimal and uses a rollback mechanism to adjust search regions to reduce recovery time further. Moreover, instead of reading blocks sequentially from each node, SARepair also selectively schedules the reading order for each block to reduce the memory overhead. We extend SARepair to address the full-node recovery and adapt to the LRC code. We prototype SARepair and show via both simulations and Amazon EC2 experiments that the recovery performance can be improved by up to 59.97% over a state-of-the-art recovery approach while keeping running time and memory overhead low.},
  archive      = {J_TACO},
  doi          = {10.1145/3664926},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Stripe-schedule aware repair in erasure-coded clusters with heterogeneous star networks},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scythe: A low-latency RDMA-enabled distributed transaction
system for disaggregated memory. <em>TACO</em>, <em>21</em>(3), 1–26.
(<a href="https://doi.org/10.1145/3666004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disaggregated memory separates compute and memory resources into independent pools connected by RDMA (Remote Direct Memory Access) networks, which can improve memory utilization, reduce cost, and enable elastic scaling of compute and memory resources. However, existing RDMA-based distributed transactions on disaggregated memory suffer from severe long-tail latency under high-contention workloads. In this article, we propose Scythe, a novel low-latency RDMA-enabled distributed transaction system for disaggregated memory. Scythe optimizes the latency of high-contention transactions in three approaches: (1) Scythe proposes a hot-aware concurrency control policy that uses optimistic concurrency control (OCC) to improve transaction processing efficiency in low-conflict scenarios. Under high conflicts, Scythe designs a timestamp-ordered OCC (TOCC) strategy based on fair locking to reduce the number of retries and cross-node communication overhead. (2) Scythe presents an RDMA-friendly timestamp service for improved timestamp management. And, (3) Scythe designs an RDMA-optimized RPC framework to improve RDMA bandwidth utilization. The evaluation results show that, compared with state-of-the-art distributed transaction systems, Scythe achieves more than 2.5× lower latency with 1.8× higher throughput under high-contention workloads.},
  archive      = {J_TACO},
  doi          = {10.1145/3666004},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Scythe: A low-latency RDMA-enabled distributed transaction system for disaggregated memory},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Achieving tunable erasure coding with cluster-aware
redundancy transitioning. <em>TACO</em>, <em>21</em>(3), 1–24. (<a
href="https://doi.org/10.1145/3672077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure coding has been demonstrated as a storage-efficient means against failures, yet its tunability remains a challenging issue in data centers, which is prone to induce substantial cross-cluster traffic. In this article, we present ClusterRT , a cluster-aware redundancy transitioning approach that can dynamically tailor the redundancy degree of erasure coding in data centers. ClusterRT formulates the data relocation as the maximum flow problem to reduce cross-cluster data transfers. It then designs a parity-coordinated update algorithm, which gathers the parity chunks within the same cluster and leverages encoding dependency to further decrease the cross-cluster update traffic. ClusterRT finally rotates the parity chunks to balance the cross-cluster transitioning traffic across the data center. Large-scale simulation and Alibaba Cloud ECS experiments show that ClusterRT reduces 94.0% to 96.2% of transitioning traffic and reduces 70.4% to 88.4% of transitioning time.},
  archive      = {J_TACO},
  doi          = {10.1145/3672077},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Achieving tunable erasure coding with cluster-aware redundancy transitioning},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sectored DRAM: A practical energy-efficient and
high-performance fine-grained DRAM architecture. <em>TACO</em>,
<em>21</em>(3), 1–29. (<a
href="https://doi.org/10.1145/3673653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern computing systems access data in main memory at coarse granularity (e.g., at 512-bit cache block granularity). Coarse-grained access leads to wasted energy because the system does not use all individually accessed small portions (e.g., words , each of which typically is 64 bits) of a cache block. In modern DRAM-based computing systems, two key coarse-grained access mechanisms lead to wasted energy: large and fixed-size (i) data transfers between DRAM and the memory controller and (ii) DRAM row activations. We propose Sectored DRAM, a new, low-overhead DRAM substrate that reduces wasted energy by enabling fine-grained DRAM data transfer and DRAM row activation. To retrieve only useful data from DRAM, Sectored DRAM exploits the observation that many cache blocks are not fully utilized in many workloads due to poor spatial locality. Sectored DRAM predicts the words in a cache block that will likely be accessed during the cache block’s residency in cache and (i) transfers only the predicted words on the memory channel by dynamically tailoring the DRAM data transfer size for the workload and (ii) activates a smaller set of cells that contain the predicted words by carefully operating physically isolated portions of DRAM rows (i.e., mats). Activating a smaller set of cells on each access relaxes DRAM power delivery constraints and allows the memory controller to schedule DRAM accesses faster. We evaluate Sectored DRAM using 41 workloads from widely used benchmark suites. Compared to a system with coarse-grained DRAM, Sectored DRAM reduces the DRAM energy consumption of highly memory intensive workloads by up to (on average) 33% (20%) while improving their performance by up to (on average) 36% (17%). Sectored DRAM’s DRAM energy savings, combined with its system performance improvement, allows system-wide energy savings of up to 23%. Sectored DRAM’s DRAM chip area overhead is 1.7% of the area of a modern DDR4 chip. Compared to state-of-the-art fine-grained DRAM architectures, Sectored DRAM greatly reduces DRAM energy consumption, does not reduce DRAM bandwidth, and can be implemented with low hardware cost. Sectored DRAM provides 89% of the performance benefits of, consumes 12% less DRAM energy than, and takes up 34% less DRAM chip area than a high-performance state-of-the-art fine-grained DRAM architecture (Half-DRAM). It is our hope and belief that Sectored DRAM’s ideas and results will help to enable more efficient and high-performance memory systems. To this end, we open source Sectored DRAM at https://github.com/CMU-SAFARI/Sectored-DRAM.},
  archive      = {J_TACO},
  doi          = {10.1145/3673653},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-29},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Sectored DRAM: A practical energy-efficient and high-performance fine-grained DRAM architecture},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAL: Optimizing the dataflow of spin-based architectures for
lightweight neural networks. <em>TACO</em>, <em>21</em>(3), 1–27. (<a
href="https://doi.org/10.1145/3673654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Convolutional Neural Network (CNN) goes deeper and more complex, the network becomes memory-intensive and computation-intensive. To address this issue, the lightweight neural network reduces parameters and Multiplication-and-Accumulation (MAC) operations by using the Depthwise Separable Convolution (DSC) to improve speed and efficiency. Nonetheless, the energy efficiency of classical Von Neumann architectures for CNNs is limited due to the memory wall challenge. Spin-based architectures have the potential to address this challenge thanks to the integration of memory and computing with ultra-high energy efficiency. However, deploying the DSC on spin-based architectures with the traditional dataflow leads to huge activation movements and low hardware utilization. Moreover, the inter-layer data dependency of neural networks increases latency. These factors become the bottleneck of improving energy efficiency and performance. Inspired by these challenges, we propose a novel dataflow on Spin-based Architectures for Lightweight neural networks (SAL). The novel dataflow replaces convolution unrolling by selecting activations in the crossbar according to the convolution window and also realizes the inter-layer data reuse. Moreover, the novel dataflow also reduces the latency due to the data dependency between layers, realizing higher performance. To the best of our knowledge, this is the first design to use hybrid dataflow for the PIM architecture. We also optimize the structure of the spin-based crossbar and the pipeline based on the dataflow to achieve better data reuse and computational parallelism. For deploying the MobileNet V1, the novel dataflow improves the hardware utilization by 23×∼ 105× and reduces the data traffic by 1.09×∼ 18.6×. Compared with the NEBULA, a spin-based non-Von Neumann architecture, the SAL reduces the energy consumption by 4× and improves the performance by 7.3×, which are 0.32 mJ and 10.43 GOPs -1 , respectively. Moreover, the SAL improves power efficiency over 29 times more than the NEBULA. Compared with the Eyeriss, the SAL improves the energy efficiency by four orders of magnitude.},
  archive      = {J_TACO},
  doi          = {10.1145/3673654},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SAL: Optimizing the dataflow of spin-based architectures for lightweight neural networks},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lavender: An efficient resource partitioning framework for
large-scale job colocation. <em>TACO</em>, <em>21</em>(3), 1–23. (<a
href="https://doi.org/10.1145/3674736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workload consolidation is a widely used approach to enhance resource utilization in modern data centers. However, the concurrent execution of multiple jobs on a shared server introduces contention for essential shared resources such as CPU cores, Last Level Cache, and memory bandwidth. This contention negatively impacts job performance, leading to significant degradation in throughput. To mitigate resource contention, effective resource isolation techniques at the software or hardware level can be employed to partition the shared resources among colocated jobs. However, existing solutions for resource partitioning often assume a limited number of jobs that can be colocated, making them unsuitable for scenarios with a large-scale job colocation due to several critical challenges. In this study, we propose Lavender, a framework specifically designed for addressing large-scale resource partitioning problems. Lavender incorporates several key techniques to tackle the challenges associated with large-scale resource partitioning, ensuring efficiency, adaptivity, and optimality. We conducted comprehensive evaluations of Lavender to validate its performance and analyze the reasons for its advantages. The experimental results demonstrate that Lavender significantly outperforms state-of-the-art baselines. Lavender is publicly available at https://github.com/yanxiaoqi932/OpenSourceLavender.},
  archive      = {J_TACO},
  doi          = {10.1145/3674736},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Lavender: An efficient resource partitioning framework for large-scale job colocation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReIPE: Recycling idle PEs in CNN accelerator for vulnerable
filters soft-error detection. <em>TACO</em>, <em>21</em>(3), 1–26. (<a
href="https://doi.org/10.1145/3674909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To satisfy prohibitively massive computational requirements of current deep Convolutional Neural Networks (CNNs), CNN-specific accelerators are widely deployed in large-scale systems. Caused by high-energy neutrons and α-particle strikes, soft error may lead to catastrophic failures when CNN is deployed on high integration density accelerators. As CNNs become ubiquitous in mission-critical domains, ensuring the reliable execution of CNN accelerators in the presence of soft errors is increasingly essential. In this article, we propose to Re cycle I dle P rocessing E lements (PEs) in the CNN accelerator for vulnerable filters soft error detection (ReIPE). Considering the error-sensitivity of filters, ReIPE first carries out a filter-level gradient analysis process to replace fault injection for fast filter-wise error resilience estimation. Then, to achieve maximal reliability benefits, combining the hardware-level systolic array idleness and software-level CNN filter-wise error resilience profile, ReIPE preferentially duplicated loads the most vulnerable filters onto systolic array to recycle idle-column PEs for opportunistically redundant execution (error detection). Exploiting the data reuse properties of accelerators, ReIPE incorporates the error detection process into the original computation flow of accelerators to perform real-time error detection. Once the error is detected, ReIPE will trigger a correction round to rectify the erroneous output. Experimental results performed on LeNet-5, Cifar-10-CNN, AlexNet, ResNet-20, VGG-16, and ResNet-50 exhibit that ReIPE can cover 96.40% of errors while reducing 75.06% performance degradation and 67.79% energy consumption of baseline dual modular redundancy on average. Moreover, to satisfy the reliability requirements of various application scenarios, ReIPE is also applicable for pruned, quantized, and Transformer-based models, as well as portable to other accelerator architectures.},
  archive      = {J_TACO},
  doi          = {10.1145/3674909},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ReIPE: Recycling idle PEs in CNN accelerator for vulnerable filters soft-error detection},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-aware spectrum-based bug localization for hardware
design code with data purification. <em>TACO</em>, <em>21</em>(3), 1–25.
(<a href="https://doi.org/10.1145/3678009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The verification of hardware design code is a critical aspect in ensuring the quality and reliability of hardware products. Finding bugs in hardware design code is important for hardware development and is frequently considered as a notoriously challenging and time-consuming activity while being an essential aspect of verification. Thus, bug localization techniques that could assist manual debugging have attracted much attention in the hardware community. However, there exists an unpredictable time span between the precise origin of a bug and its detected manifestation in prior work without costly formal verification. Locating the bug responsible for the exposed discrepancy between expected and exhibited design behavior remains a major challenge. In this work, we propose Tartan, a T ime- a ware spect r um-based bug localiza t ion with d a ta purificatio n for hardware design code to address these limitations. Tartan integrates hardware-specific timing information with the spectrum and captures the changes of executed statements when the state of the circuit changes to effectively locate bugs. Further, Tartan purifies the spectrum data from the simulation and evaluates the suspiciousness of the statements in the design to indicate the likelihood of being buggy. To evaluate the effectiveness of Tartan, we conduct large-scale experiments on 69 versions of 15 hardware projects by the state-of-the-art bug localization techniques. The experimental results clearly show that Tartan is statistically more effective than the baselines. It provides a new perspective on hardware design code bug localization and brings fresh insights to the community.},
  archive      = {J_TACO},
  doi          = {10.1145/3678009},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Time-aware spectrum-based bug localization for hardware design code with data purification},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Architectural support for sharing, isolating and
virtualizing FPGA resources. <em>TACO</em>, <em>21</em>(2), 1–26. (<a
href="https://doi.org/10.1145/3648475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FPGAs are increasingly popular in cloud environments for their ability to offer on-demand acceleration and improved compute efficiency. Providers would like to increase utilization, by multiplexing customers on a single device, similar to how processing cores and memory are shared. Nonetheless, multi-tenancy still faces major architectural limitations including: (a) inefficient sharing of memory interfaces across hardware tasks (HT) exacerbated by technological limitations and peculiarities, (b) insufficient solutions for performance and data isolation and high quality of service, and (c) absent or simplistic allocation strategies to effectively distribute external FPGA memory across HT. This article presents a full-stack solution for enabling multi-tenancy on FPGAs. Specifically, our work proposes an intra-fpga virtualization layer to share FPGA interfaces and its resources across tenants. To achieve efficient inter-connectivity between virtual FPGAs (vFGPAs) and external interfaces, we employ a compact network-on-chip architecture to optimize resource utilization. Dedicated memory management units implement the concept of virtual memory in FPGAs, providing mechanisms to isolate the address space and enable memory protection. We also introduce a memory segmentation scheme to effectively allocate FPGA address space and enhance isolation through hardware-software support, while preserving the efficacy of memory transactions. We assess our solution on an Alveo U250 Data Center FPGA Card, employing 10 real-world benchmarks from the Rodinia and Rosetta suites. Our framework preserves the performance of HT from a non-virtualized environment, while enhancing the device aggregate throughput through resource sharing; up to 3.96x in isolated and up to 2.31x in highly congested settings, where an external interface is shared across four vFPGAs. Finally, our work ensures high-quality of service, with HT achieving up to 0.95x of their native performance, even when resource sharing introduces interference from other accelerators.},
  archive      = {J_TACO},
  doi          = {10.1145/3648475},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Architectural support for sharing, isolating and virtualizing FPGA resources},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FASA-DRAM: Reducing DRAM latency with destructive activation
and delayed restoration. <em>TACO</em>, <em>21</em>(2), 1–27. (<a
href="https://doi.org/10.1145/3649455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DRAM memory is a performance bottleneck for many applications, due to its high access latency. Previous work has mainly focused on data locality, introducing small but fast regions to cache frequently accessed data, thereby reducing the average latency. However, these locality-based designs have three challenges in modern multi-core systems: (1) inter-application interference leads to random memory access traffic, (2) fairness issues prevent the memory controller from over-prioritizing data locality, and (3) write-intensive applications have much lower locality and evict substantial dirty entries. With frequent data movement between the fast in-DRAM cache and slow regular arrays, the overhead induced by moving data may even offset the performance and energy benefits of in-DRAM caching. In this article, we decouple the data movement process into two distinct phases. The first phase is Load-Reduced Destructive Activation (LRDA), which destructively promotes data into the in-DRAM cache. The second phase is Delayed Cycle-Stealing Restoration (DCSR), which restores the original data when the DRAM bank is idle. LRDA decouples the most time-consuming restoration phase from activation, and DCSR hides the restoration latency through prevalent bank-level parallelism. We propose FASA-DRAM, incorporating destructive activation and delayed restoration techniques to enable both in-DRAM caching and proactive latency-hiding mechanisms. Our evaluation shows that FASA-DRAM improves the average performance by 19.9% and reduces average DRAM energy consumption by 18.1% over DDR4 DRAM for four-core workloads, with less than 3.4% extra area overhead. Furthermore, FASA-DRAM outperforms state-of-the-art designs in both performance and energy efficiency.},
  archive      = {J_TACO},
  doi          = {10.1145/3649455},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {FASA-DRAM: Reducing DRAM latency with destructive activation and delayed restoration},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The droplet search algorithm for kernel scheduling.
<em>TACO</em>, <em>21</em>(2), 1–28. (<a
href="https://doi.org/10.1145/3650109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel scheduling is the problem of finding the most efficient implementation for a computational kernel. Identifying this implementation involves experimenting with the parameters of compiler optimizations, such as the size of tiling windows and unrolling factors. This article shows that it is possible to organize these parameters as points in a coordinate space. The function that maps these points to the running time of kernels, in general, will not determine a convex surface. However, this article provides empirical evidence that the origin of this surface (an unoptimized kernel) and its global optimum (the fastest kernel) reside on a convex region. We call this hypothesis the “droplet expectation.” Consequently, a search method based on the Coordinate Descent algorithm tends to find the optimal kernel configuration quickly if the hypothesis holds. This approach—called Droplet Search—has been available in Apache TVM since April of 2023. Experimental results with six large deep learning models on various computing devices (ARM, Intel, AMD, and NVIDIA) indicate that Droplet Search is not only as effective as other AutoTVM search techniques but also 2 to 10 times faster. Moreover, models generated by Droplet Search are competitive with those produced by TVM’s AutoScheduler (Ansor), despite the latter using 4 to 5 times more code transformations than AutoTVM.},
  archive      = {J_TACO},
  doi          = {10.1145/3650109},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {The droplet search algorithm for kernel scheduling},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camouflage: Utility-aware obfuscation for accurate
simulation of sensitive program traces. <em>TACO</em>, <em>21</em>(2),
1–23. (<a href="https://doi.org/10.1145/3650110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trace-based simulation is a widely used methodology for system design exploration. It relies on realistic traces that represent a range of behaviors necessary to be evaluated, containing a lot of information about the application, its inputs and the underlying system on which it was generated. Consequently, generating traces from real-world executions risks leakage of sensitive information. To prevent this, traces can be obfuscated before release. However, this can undermine their ideal utility, i.e., how realistically a program behavior was captured. To address this, we propose Camouflage, a novel obfuscation framework, designed with awareness of the necessary architectural properties required to preserve trace utility , while ensuring secrecy of the inputs used to generate the trace. Focusing on memory access traces, our extensive evaluation on various benchmarks shows that camouflaged traces preserve the performance measurements of the original execution, with an average τ correlation of 0.66. We model input secrecy as an input indistinguishability problem and show that the average security loss is 7.8%, which is better than traces generated from the state-of-the-art.},
  archive      = {J_TACO},
  doi          = {10.1145/3650110},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Camouflage: Utility-aware obfuscation for accurate simulation of sensitive program traces},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TEA+: A novel temporal graph random walk engine with hybrid
storage architecture. <em>TACO</em>, <em>21</em>(2), 1–26. (<a
href="https://doi.org/10.1145/3652604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world networks are characterized by being temporal and dynamic, wherein the temporal information signifies the changes in connections, such as the addition or removal of links between nodes. Employing random walks on these temporal networks is a crucial technique for understanding the structural evolution of such graphs over time. However, existing state-of-the-art sampling methods are designed for traditional static graphs, and as such, they struggle to efficiently handle the dynamic aspects of temporal networks. This deficiency can be attributed to several challenges, including increased sampling complexity, extensive index space, limited programmability, and a lack of scalability. In this article, we introduce TEA+ , a robust, fast, and scalable engine for conducting random walks on temporal graphs. Central to TEA+ is an innovative hybrid sampling method that amalgamates two Monte Carlo sampling techniques. This fusion significantly diminishes space complexity while maintaining a fast sampling speed. Additionally, TEA+ integrates a range of optimizations that significantly enhance sampling efficiency. This is further supported by an effective graph updating strategy, skilled in managing dynamic graph modifications and adeptly handling the insertion and deletion of both edges and vertices. For ease of implementation, we propose a temporal-centric programming model, designed to simplify the development of various random walk algorithms on temporal graphs. To ensure optimal performance across storage constraints, TEA+ features a degree-aware hybrid storage architecture, capable of adeptly scaling in different memory environments. Experimental results showcase the prowess of TEA+ , as it attains up to three orders of magnitude speedups compared to current random walk engines on extensive temporal graphs.},
  archive      = {J_TACO},
  doi          = {10.1145/3652604},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {TEA+: A novel temporal graph random walk engine with hybrid storage architecture},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Orchard: Heterogeneous parallelism and fine-grained fusion
for complex tree traversals. <em>TACO</em>, <em>21</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3652605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications are designed to perform traversals on tree-like data structures. Fusing and parallelizing these traversals enhance the performance of applications. Fusing multiple traversals improves the locality of the application. The runtime of an application can be significantly reduced by extracting parallelism and utilizing multi-threading. Prior frameworks have tried to fuse and parallelize tree traversals using coarse-grained approaches, leading to missed fine-grained opportunities for improving performance. Other frameworks have successfully supported fine-grained fusion on heterogeneous tree types but fall short regarding parallelization. We introduce a new framework Orchard built on top of Grafter . Orchard ’s novelty lies in allowing the programmer to transform tree traversal applications by automatically applying fine-grained fusion and extracting heterogeneous parallelism. Orchard allows the programmer to write general tree traversal applications in a simple and elegant embedded Domain-Specific Language (eDSL). We show that the combination of fine-grained fusion and heterogeneous parallelism performs better than each alone when the conditions are met.},
  archive      = {J_TACO},
  doi          = {10.1145/3652605},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Orchard: Heterogeneous parallelism and fine-grained fusion for complex tree traversals},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). XMeta: SSD-HDD-hybrid optimization for metadata maintenance
of cloud-scale object storage. <em>TACO</em>, <em>21</em>(2), 1–20. (<a
href="https://doi.org/10.1145/3652606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object storage has been widely used in the cloud. Traditionally, the size of object metadata is much smaller than that of object data, and thus existing object storage systems (such as Ceph and Oasis) can place object data and metadata, respectively, on hard disk drives (HDDs) and solid-state drives (SSDs) to achieve high I/O performance at a low monetary cost. Currently, however, a wide range of cloud applications organize their data as large numbers of small objects of which the data size is close to (or even smaller than) the metadata size, thus greatly increasing the cost if placing all metadata on expensive SSDs. This article presents x Meta , an SSD-HDD-hybrid optimization for metadata maintenance of cloud-scale object storage. We observed that a substantial portion of the metadata of small objects is rarely accessed and thus can be stored on HDDs with little performance penalty. Therefore, x Meta first classifies the hot and cold metadata based on the frequency of metadata accesses of upper-layer applications and then adaptively stores the hot metadata on SSDs and the cold metadata on HDDs. We also propose a merging mechanism for hot metadata to further improve the efficiency of SSD storage and optimize range key query and insertion for hot metadata by designing composite keys. We have integrated the x Meta metadata service with Ceph to realize a high-performance, low-cost object store (called xCeph). The extensive evaluation shows that xCeph outperforms the original Ceph by an order of magnitude in the space requirement of SSD storage, while improving the throughput by up to 2.7×.},
  archive      = {J_TACO},
  doi          = {10.1145/3652606},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-20},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {XMeta: SSD-HDD-hybrid optimization for metadata maintenance of cloud-scale object storage},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NEM-GNN: DAC/ADC-less, scalable, reconfigurable, graph and
sparsity-aware near-memory accelerator for graph neural networks.
<em>TACO</em>, <em>21</em>(2), 1–26. (<a
href="https://doi.org/10.1145/3652607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are of great interest in real-life applications such as citation networks and drug discovery owing to GNN’s ability to apply machine learning techniques on graphs. GNNs utilize a two-step approach to classify the nodes in a graph into pre-defined categories. The first step uses a combination kernel to perform data-intensive convolution operations with regular memory access patterns. The second step uses an aggregation kernel that operates on sparse data having irregular access patterns. These mixed data patterns render CPU/GPU-based compute energy-inefficient. Von Neumann based accelerators like AWB-GCN [ 7 ] suffer from increased data movement, as the data-intensive combination requires large data movement to/from memory to perform computations. ReFLIP [ 8 ] performs resistive random access memory based in-memory (PIM) compute to overcome data movement costs. However, ReFLIP suffers from increased area requirement due to dedicated accelerator arrangement, and reduced performance due to limited parallelism and energy due to fundamental issues in ReRAM-based compute. This article presents a scalable (non-exponential storage requirement), DAC/ADC-less PIM-based combination, with (i) early compute termination and (ii) pre-compute by reconfiguring SOC components. Graph and sparsity-aware near-memory aggregation using the proposed compute-as-soon-as-ready (CAR) broadcast approach improves performance and energy further. NEM-GNN achieves ∼80–230x, ∼80–300x, ∼850–1,134x, and ∼7–8x improvement over ReFLIP, in terms of performance, throughput, energy efficiency, and compute density.},
  archive      = {J_TACO},
  doi          = {10.1145/3652607},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {NEM-GNN: DAC/ADC-less, scalable, reconfigurable, graph and sparsity-aware near-memory accelerator for graph neural networks},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cerberus: Triple mode acceleration of sparse matrix and
vector multiplication. <em>TACO</em>, <em>21</em>(2), 1–24. (<a
href="https://doi.org/10.1145/3653020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiplication of sparse matrix and vector (SpMV) is one of the most widely used kernels in high-performance computing as well as machine learning acceleration for sparse neural networks. The design space of SpMV accelerators has two axes: algorithm and matrix representation. There have been two widely used algorithms and data representations. Two algorithms, scalar multiplication and dot product, can be combined with two sparse data representations, compressed sparse and bitmap formats for the matrix and vector. Although the prior accelerators adopted one of the possible designs, it is yet to be investigated which design is the best one across different hardware resources and workload characteristics. This paper first investigates the impact of design choices with respect to the algorithm and data representation. Our evaluation shows that no single design always outperforms the others across different workloads, but the two best designs (i.e., compressed sparse format and bitmap format with dot product) have complementary performance with trade-offs incurred by the matrix characteristics. Based on the analysis, this study proposes Cerberus, a triple-mode accelerator supporting two sparse operation modes in addition to the base dense mode. To allow such multi-mode operation, it proposes a prediction model based on matrix characteristics under a given hardware configuration, which statically selects the best mode for a given sparse matrix with its dimension and density information. Our experimental results show that Cerberus provides 12.1× performance improvements from a dense-only accelerator, and 1.5× improvements from a fixed best SpMV design.},
  archive      = {J_TACO},
  doi          = {10.1145/3653020},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {5},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Cerberus: Triple mode acceleration of sparse matrix and vector multiplication},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An instruction inflation analyzing framework for dynamic
binary translators. <em>TACO</em>, <em>21</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3640813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic binary translators (DBTs) are widely used to migrate applications between different instruction set architectures (ISAs). Despite extensive research to improve DBT performance, noticeable overhead remains, preventing near-native performance, especially when translating from complex instruction set computer (CISC) to reduced instruction set computer (RISC). For computational workloads, the main overhead stems from translated code quality. Experimental data show that state-of-the-art DBT products have dynamic code inflation of at least 1.46. This indicates that on average, more than 1.46 host instructions are needed to emulate one guest instruction. Worse, inflation closely correlates with translated code quality. However, the detailed sources of instruction inflation remain unclear. To understand the sources of inflation, we present Deflater , an instruction inflation analysis framework comprising a mathematical model, a collection of black-box unit tests called BenchMIAOes , and a trace-based simulator called InflatSim . The mathematical model calculates overall inflation based on the inflation of individual instructions and translation block optimizations. BenchMIAOes extract model parameters from DBTs without accessing DBT source code. InflatSim implements the model and uses the extracted parameters from BenchMIAOes to simulate a given DBT’s behavior. Deflater is a valuable tool to guide DBT analysis and improvement. Using Deflater, we simulated inflation for three state-of-the-art CISC-to-RISC DBTs: ExaGear, Rosetta2, and LATX, with inflation errors of 5.63%, 5.15%, and 3.44%, respectively for SPEC CPU 2017, gaining insights into these commercial DBTs. Deflater also efficiently models inflation for the open source DBT QEMU and suggests optimizations that can substantially reduce inflation. Implementing the suggested optimizations confirms Deflater’s effective guidance, with 4.65% inflation error, and gains 5.47x performance improvement.},
  archive      = {J_TACO},
  doi          = {10.1145/3640813},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {An instruction inflation analyzing framework for dynamic binary translators},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-aware service placement and scheduling in the
edge-cloud continuum. <em>TACO</em>, <em>21</em>(2), 1–24. (<a
href="https://doi.org/10.1145/3640823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edge to data center computing continuum is the aggregation of computing resources located anywhere between the network edge (e.g., close to 5G antennas), and servers in traditional data centers. Kubernetes is the de facto standard for the orchestration of services in data center environments, where it is very efficient. It, however, fails to give the same performance when including edge resources. At the edge, resources are more limited, and networking conditions are changing over time. In this article, we present a methodology that lowers the costs of running applications in the edge-to-cloud computing continuum. This methodology can adapt to changing environments, e.g., moving end-users. We are also monitoring some Key Performance Indicators of the applications to ensure that cost optimizations do not negatively impact their Quality of Service. In addition, to ensure that performances are optimal even when users are moving, we introduce a background process that periodically checks if a better location is available for the service and, if so, moves the service. To demonstrate the performance of our scheduling approach, we evaluate it using a vehicle cooperative perception use case, a representative 5G application. With this use case, we can demonstrate that our scheduling approach can robustly lower the cost in different scenarios, while other approaches that are already available fail in either being adaptive to changing environments or will have poor cost-effectiveness in some scenarios.},
  archive      = {J_TACO},
  doi          = {10.1145/3640823},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Cost-aware service placement and scheduling in the edge-cloud continuum},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tyche: An efficient and general prefetcher for indirect
memory accesses. <em>TACO</em>, <em>21</em>(2), 1–26. (<a
href="https://doi.org/10.1145/3641853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indirect memory accesses (IMAs, i.e., A [ f ( B [ i ])]) are typical memory access patterns in applications such as graph analysis, machine learning, and database. IMAs are composed of producer-consumer pairs, where the consumers’ memory addresses are derived from the producers’ memory data. Due to the built-in value-dependent feature, IMAs exhibit poor locality, making prefetching ineffective. Hindered by the challenges of recording the potentially complex graphs of instruction dependencies among IMA producers and consumers, current state-of-the-art hardware prefetchers either (a) exhibit inadequate IMA identification abilities or (b) rely on the run-ahead mechanism to prefetch IMAs intermittently and insufficiently. To solve this problem, we propose Tyche, 1 an efficient and general hardware prefetcher to enhance IMA performance. Tyche adopts a bilateral propagation mechanism to precisely excavate the instruction dependencies in simple chains with moderate length (rather than complex graphs). Based on the exact instruction dependencies, Tyche can accurately identify various IMA patterns, including nonlinear ones, and generate accurate prefetching requests continuously. Evaluated on broad benchmarks, Tyche achieves an average performance speedup of 16.2% over the state-of-the-art spatial prefetcher Berti. More importantly, Tyche outperforms the state-of-the-art IMA prefetchers IMP, Gretch, and Vector Runahead, by 15.9%, 12.8%, and 10.7%, respectively, with a lower storage overhead of only 0.57 KB.},
  archive      = {J_TACO},
  doi          = {10.1145/3641853},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Tyche: An efficient and general prefetcher for indirect memory accesses},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Winols: A large-tiling sparse winograd CNN accelerator on
FPGAs. <em>TACO</em>, <em>21</em>(2), 1–24. (<a
href="https://doi.org/10.1145/3643682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) can benefit from the computational reductions provided by the Winograd minimal filtering algorithm and weight pruning. However, harnessing the potential of both methods simultaneously introduces complexity in designing pruning algorithms and accelerators. Prior studies aimed to establish regular sparsity patterns in the Winograd domain, but they were primarily suited for small tiles, with domain transformation dictating the sparsity ratio. The irregularities in data access and domain transformation pose challenges in accelerator design, especially for larger Winograd tiles. This paper introduces “Winols,” an innovative algorithm-hardware co-design strategy that emphasizes the strengths of the large-tiling Winograd algorithm. Through a spatial-to-Winograd relevance degree evaluation, we extensively explore domain transformation and propose a cross-domain pruning technique that retains sparsity across both spatial and Winograd domains. To compress pruned weight matrices, we invent a relative column encoding scheme. We further design an FPGA-based accelerator for CNN models with large Winograd tiles and sparse matrix-vector operations. Evaluations indicate our pruning method achieves up to 80% weight tile sparsity in the Winograd domain without compromising accuracy. Our Winols accelerator outperforms dense accelerator by a factor of 31.7× in inference latency. When compared with prevailing sparse Winograd accelerators, Winols reduces latency by an average of 10.9×, and improves DSP and energy efficiencies by over 5.6× and 5.7×, respectively. When compared with the CPU and GPU platform, Winols accelerator with tile size 8× 8 achieves 24.6× and 2.84× energy efficiency improvements, respectively.},
  archive      = {J_TACO},
  doi          = {10.1145/3643682},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Winols: A large-tiling sparse winograd CNN accelerator on FPGAs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SLAP: Segmented reuse-time-label based admission policy for
content delivery network caching. <em>TACO</em>, <em>21</em>(2), 1–24.
(<a href="https://doi.org/10.1145/3646550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {‘‘Learned” admission policies have shown promise in improving Content Delivery Network (CDN) cache performance and lowering operational costs. Unfortunately, existing learned policies are optimized with a few fixed cache sizes while in reality, cache sizes often vary over time in an unpredictable manner. As a result, existing solutions cannot provide consistent benefits in production settings. We present SLAP , a learned CDN cache admission approach based on segmented object reuse time prediction. SLAP predicts an object’s reuse time range using the Long-Short-Term-Memory model and admits objects that will be reused (before eviction) given the current cache size. SLAP decouples model training from cache size, allowing it to adapt to arbitrary sizes. The key to our solution is a novel segmented labeling scheme that makes SLAP without requiring precise prediction on object reuse time. To further make SLAP a practical and efficient solution, we propose aggressive reusing of computation and training on sampled traces to optimize model training, and a specialized predictor architecture that overlaps prediction computation with miss object fetching to optimize model inference. Our experiments using production CDN traces show that SLAP achieves significantly lower write traffic (38%-59%), longer SSDs lifetime (104%-178%), a consistently higher hit rate (3.2%-11.7%), and requires no effort to adapt to changing cache sizes, outperforming existing policies.},
  archive      = {J_TACO},
  doi          = {10.1145/3646550},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SLAP: Segmented reuse-time-label based admission policy for content delivery network caching},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Highly efficient self-checking matrix multiplication on
tiled AMX accelerators. <em>TACO</em>, <em>21</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3633332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General Matrix Multiplication (GEMM) is a computationally expensive operation that is used in many applications such as machine learning. Hardware accelerators are increasingly popular for speeding up GEMM computation, with Tiled Matrix Multiplication (TMUL) in recent Intel processors being an example. Unfortunately, the TMUL hardware is susceptible to errors, necessitating online error detection. The Algorithm-based Error Detection (ABED) technique is a powerful technique to detect errors in matrix multiplications. In this article, we consider implementation of an ABED technique that integrates seamlessly with the TMUL hardware to minimize performance overhead. Unfortunately, rounding errors introduced by floating-point operations do not allow a straightforward implementation of ABED in TMUL. Previously an error bound was considered for addressing rounding errors in ABED. If the error detection threshold is set too low, it will a trigger false alarm, while a loose bound will allow errors to escape detection. In this article, we propose an adaptive error threshold that takes into account the TMUL input values to address the problem of false triggers and error escapes and provide a taxonomy of various error classes. This threshold is obtained from theoretical error analysis but is not easy to implement in hardware. Consequently, we relax the threshold such that it can be easily computed in hardware. While ABED ensures error-free computation, it does not guarantee full coverage of all hardware faults. To address this problem, we propose an algorithmic pattern generation technique to ensure full coverage for all hardware faults. To evaluate the benefits of our proposed solution, we conducted fault injection experiments and show that our approach does not produce any false alarms or detection escapes for observable errors. We conducted additional fault injection experiments on a Deep Neural Network (DNN) model and find that if a fault is not detected, it does not cause any misclassification.},
  archive      = {J_TACO},
  doi          = {10.1145/3633332},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Highly efficient self-checking matrix multiplication on tiled AMX accelerators},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coherence attacks and countermeasures in interposer-based
chiplet systems. <em>TACO</em>, <em>21</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3633461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industry is moving towards large-scale hardware systems that bundle processor cores, memories, accelerators, and so on. via 2.5D integration. These components are fabricated separately as chiplets and then integrated using an interposer as an interconnect carrier. This new design style is beneficial in terms of yield and economies of scale, as chiplets may come from various vendors and are relatively easy to integrate into one larger sophisticated system. However, the benefits of this approach come at the cost of new security challenges, especially when integrating chiplets that come from untrusted or not fully trusted, third- party vendors. In this work, we explore these challenges for modern interposer-based systems of cache-coherent, multi-core chiplets. First, we present basic coherence-oriented hardware Trojan attacks that pose a significant threat to chiplet-based designs and demonstrate how these basic attacks can be orchestrated to pose a significant threat to interposer-based systems. Second, we propose a novel scheme using an active interposer as a generic, secure-by-construction platform that forms a physical root of trust for modern 2.5D systems. The implementation of our scheme is confined to the interposer, resulting in little cost and leaving the chiplets and coherence system untouched. We show that our scheme prevents a range of coherence attacks with low overheads on system performance, ∼4%. Further, we demonstrate that our scheme scales efficiently as system size and memory capacities increase, resulting in reduced performance overheads.},
  archive      = {J_TACO},
  doi          = {10.1145/3633461},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Coherence attacks and countermeasures in interposer-based chiplet systems},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WIPE: A write-optimized learned index for persistent memory.
<em>TACO</em>, <em>21</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3634915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned Index, which utilizes effective machine learning models to accelerate locating sorted data positions, has gained increasing attention in many big data scenarios. Using efficient learned models, the learned indexes build large nodes and flat structures, thereby greatly improving the performance. However, most of the state-of-the-art learned indexes are designed for DRAM, and there is hence an urgent need to enable high-performance learned indexes for emerging Non-Volatile Memory (NVM). In this article, we first evaluate and analyze the performance of the existing learned indexes on NVM. We discover that these learned indexes encounter severe write amplification and write performance degradation due to the requirements of maintaining large sorted/semi-sorted data nodes. To tackle the problems, we propose a novel three-tiered architecture of write-optimized persistent learned index, which is named WIPE , by adopting unsorted fine-granularity data nodes to achieve high write performance on NVM. Thereinto, we devise a new root node construction algorithm to accelerate searching numerous small data nodes. The algorithm ensures stable flat structure and high read performance in large-size datasets by introducing an intermediate layer (i.e., index nodes) and achieving accurate prediction of index node positions from the root node. Our extensive experiments on Intel DCPMM show that WIPE can improve write throughput and read throughput by up to 3.9× and 7×, respectively, compared to the state-of-the-art learned indexes. Also, WIPE can recover from a system crash in ∼ 18 ms. WIPE is free as an open-source software package. 1},
  archive      = {J_TACO},
  doi          = {10.1145/3634915},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {WIPE: A write-optimized learned index for persistent memory},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing the impact of compiler optimizations on GPUs
reliability. <em>TACO</em>, <em>21</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3638249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics Processing Units (GPUs) compilers have evolved in order to support general-purpose programming languages for multiple architectures. NVIDIA CUDA Compiler (NVCC) has many compilation levels before generating the machine code and applies complex optimizations to improve performance. These optimizations modify how the software is mapped in the underlying hardware; thus, as we show in this article, they can also affect GPU reliability. We evaluate the effects on the GPU error rate of the optimization flags applied at the NVCC Parallel Thread Execution (PTX) compiling phase by analyzing two NVIDIA GPU architectures (Kepler and Volta) and two compiler versions (NVCC 10.2 and 11.3). We compare and combine fault propagation analysis based on software fault injection, hardware utilization distribution obtained with application-level profiling, and machine instructions radiation-induced error rate measured with beam experiments. We consider eight different workloads and 144 combinations of compilation flags, and we show that optimizations can impact the GPUs’ error rate of up to an order of magnitude. Additionally, through accelerated neutron beam experiments on a NVIDIA Kepler GPU, we show that the error rate of the unoptimized GEMM (-O0 flag) is lower than the optimized GEMM’s (-O3 flag) error rate. When the performance is evaluated together with the error rate, we show that the most optimized versions (-O1 and -O3) always produce a higher amount of correct data than the unoptimized code (-O0).},
  archive      = {J_TACO},
  doi          = {10.1145/3638249},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Assessing the impact of compiler optimizations on GPUs reliability},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A concise concurrent b+-tree for persistent memory.
<em>TACO</em>, <em>21</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3638717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent memory (PM) presents a unique opportunity for designing data management systems that offer improved performance, scalability, and instant restart capability. As a widely used data structure for managing data in such systems, B + -Tree must address the challenges presented by PM in both data consistency and device performance. However, existing studies suffer from significant performance degradation when maintaining data consistency on PM. To settle this problem, we propose a new concurrent B + -Tree, CC-Tree, optimized for PM. CC-Tree ensures data consistency while providing high concurrent performance, thanks to several technologies, including partitioned metadata, log-free split, and lock-free read. We conducted experiments using state-of-the-art indices, and the results demonstrate significant performance improvements, including approximately 1.2–1.6x search, 1.5–1.7x insertion, 1.5–2.8x update, 1.9–4x deletion, 0.9–10x range scan, and up to 1.55–1.82x in hybrid workloads.},
  archive      = {J_TACO},
  doi          = {10.1145/3638717},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A concise concurrent b+-tree for persistent memory},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient hybrid deep learning accelerator for compact
and heterogeneous CNNs. <em>TACO</em>, <em>21</em>(2), 1–26. (<a
href="https://doi.org/10.1145/3639823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource-efficient Convolutional Neural Networks (CNNs) are gaining more attention. These CNNs have relatively low computational and memory requirements. A common denominator among such CNNs is having more heterogeneity than traditional CNNs. This heterogeneity is present at two levels: intra-layer type and inter-layer type. Generic accelerators do not capture these levels of heterogeneity, which harms their efficiency. Consequently, researchers have proposed model-specific accelerators with dedicated engines. When designing an accelerator with dedicated engines, one option is to dedicate one engine per CNN layer. We refer to accelerators designed with this approach as single-engine single-layer (SESL). This approach enables optimizing each engine for its specific layer. However, such accelerators are resource-demanding and unscalable. Another option is to design a minimal number of dedicated engines such that each engine handles all layers of one type. We refer to these accelerators as single-engine multiple-layer (SEML). SEML accelerators capture the inter-layer-type but not the intra-layer-type heterogeneity. We propose  the Fixed Budget Hybrid CNN Accelerator (FiBHA), a hybrid accelerator composed of an SESL part and an SEML part, each processing a subset of CNN layers. FiBHA captures more heterogeneity than SEML while being more resource-aware and scalable than SESL. Moreover, we propose a novel module, Fused Inverted Residual Bottleneck (FIRB), a fine-grained and memory-light SESL architecture building block. The proposed architecture is implemented and evaluated using high-level synthesis (HLS) on different Field Programmable Gate Arrays representing various resource budgets. Our evaluation shows that FiBHA improves the throughput by up to 4 x and 2.5 x compared to state-of-the-art SESL and SEML accelerators, respectively. Moreover, FiBHA reduces memory and energy consumption compared to an SEML accelerator. The evaluation also shows that FIRB reduces the required memory by up to 54%, and energy requirements by up to 35% compared to traditional pipelining.},
  archive      = {J_TACO},
  doi          = {10.1145/3639823},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {An efficient hybrid deep learning accelerator for compact and heterogeneous CNNs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dedicated hardware accelerators for processing of sparse
matrices and vectors: A survey. <em>TACO</em>, <em>21</em>(2), 1–26. (<a
href="https://doi.org/10.1145/3640542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance in scientific and engineering applications such as computational physics, algebraic graph problems or Convolutional Neural Networks (CNN), is dominated by the manipulation of large sparse matrices—matrices with a large number of zero elements. Specialized software using data formats for sparse matrices has been optimized for the main kernels of interest: SpMV and SpMSpM matrix multiplications, but due to the indirect memory accesses, the performance is still limited by the memory hierarchy of conventional computers. Recent work shows that specific hardware accelerators can reduce memory traffic and improve the execution time of sparse matrix multiplication, compared to the best software implementations. The performance of these sparse hardware accelerators depends on the choice of the sparse format, COO , CSR , etc, the algorithm, inner-product , outer-product , Gustavson , and many hardware design choices. In this article, we propose a systematic survey which identifies the design choices of state-of-the-art accelerators for sparse matrix multiplication kernels. We introduce the necessary concepts and then present, compare, and classify the main sparse accelerators in the literature, using consistent notations. Finally, we propose a taxonomy for these accelerators to help future designers make the best choices depending on their objectives.},
  archive      = {J_TACO},
  doi          = {10.1145/3640542},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Dedicated hardware accelerators for processing of sparse matrices and vectors: A survey},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring data layout for sparse tensor times dense matrix
on GPUs. <em>TACO</em>, <em>21</em>(1), 20:1–20. (<a
href="https://doi.org/10.1145/3633462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important sparse tensor computation is sparse-tensor-dense-matrix multiplication (SpTM), which is used in tensor decomposition and applications. SpTM is a multi-dimensional analog to sparse-matrix-dense-matrix multiplication (SpMM). In this article, we employ a hierarchical tensor data layout that can unfold a multidimensional tensor to derive a 2D matrix, making it possible to compute SpTM using SpMM kernel implementations for GPUs. We compare two SpMM implementations to the state-of-the-art PASTA sparse tensor contraction implementation using: (1) SpMM with hierarchical tensor data layout; and, (2) unfolding followed by an invocation of cuSPARSE’s SpMM. Results show that SpMM can outperform PASTA 70.9% of the time, but none of the three approaches is best overall. Therefore, we use a decision tree classifier to identify the best performing sparse tensor contraction kernel based on precomputed properties of the sparse tensor.},
  archive      = {J_TACO},
  author       = {Khalid Ahmad and Cris Cecka and Michael Garland and Mary Hall},
  doi          = {10.1145/3633462},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {1},
  pages        = {20:1–20},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Exploring data layout for sparse tensor times dense matrix on GPUs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ApHMM: Accelerating profile hidden markov models for fast
and energy-efficient genome analysis. <em>TACO</em>, <em>21</em>(1),
19:1–29. (<a href="https://doi.org/10.1145/3632950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Profile hidden Markov models (pHMMs) are widely employed in various bioinformatics applications to identify similarities between biological sequences, such as DNA or protein sequences. In pHMMs, sequences are represented as graph structures, where states and edges capture modifications (i.e., insertions, deletions, and substitutions) by assigning probabilities to them. These probabilities are subsequently used to compute the similarity score between a sequence and a pHMM graph. The Baum-Welch algorithm, a prevalent and highly accurate method, utilizes these probabilities to optimize and compute similarity scores. Accurate computation of these probabilities is essential for the correct identification of sequence similarities. However, the Baum-Welch algorithm is computationally intensive, and existing solutions offer either software-only or hardware-only approaches with fixed pHMM designs. When we analyze state-of-the-art works, we identify an urgent need for a flexible, high-performance, and energy-efficient hardware-software co-design to address the major inefficiencies in the Baum-Welch algorithm for pHMMs. We introduce ApHMM , the first flexible acceleration framework designed to significantly reduce both computational and energy overheads associated with the Baum-Welch algorithm for pHMMs. ApHMM employs hardware-software co-design to tackle the major inefficiencies in the Baum-Welch algorithm by (1) designing flexible hardware to accommodate various pHMM designs, (2) exploiting predictable data dependency patterns through on-chip memory with memoization techniques, (3) rapidly filtering out unnecessary computations using a hardware-based filter, and (4) minimizing redundant computations. ApHMM achieves substantial speedups of 15.55×–260.03×, 1.83×–5.34×, and 27.97× when compared to CPU, GPU, and FPGA implementations of the Baum-Welch algorithm, respectively. ApHMM outperforms state-of-the-art CPU implementations in three key bioinformatics applications: (1) error correction, (2) protein family search, and (3) multiple sequence alignment, by 1.29×–59.94×, 1.03×–1.75×, and 1.03×–1.95×, respectively, while improving their energy efficiency by 64.24×–115.46×, 1.75×, and 1.96×.},
  archive      = {J_TACO},
  author       = {Can Firtina and Kamlesh Pillai and Gurpreet S. Kalsi and Bharathwaj Suresh and Damla Senol Cali and Jeremie S. Kim and Taha Shahroodi and Meryem Banu Cavlak and Joël Lindegger and Mohammed Alser and Juan Gómez Luna and Sreenivas Subramoney and Onur Mutlu},
  doi          = {10.1145/3632950},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {1},
  pages        = {19:1–29},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ApHMM: Accelerating profile hidden markov models for fast and energy-efficient genome analysis},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extension VM: Interleaved data layout in vector memory.
<em>TACO</em>, <em>21</em>(1), 18:1–23. (<a
href="https://doi.org/10.1145/3631528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While vector architecture is widely employed in processors for neural networks, signal processing, and high-performance computing; however, its performance is limited by inefficient column-major memory access. The column-major access limitation originates from the unsuitable mapping of multidimensional data structures to two-dimensional vector memory spaces. In addition, the traditional data layout mapping method creates an irreconcilable conflict between row- and column-major accesses. Ideally, both row- and column-major accesses can take advantage of the bank parallelism of vector memory. To this end, we propose the Interleaved Data Layout (IDL) method in vector memory, which can distribute vector elements into different banks regardless of whether they are in the row- or column-major category, so that any vector memory access can benefit from bank parallelism. Additionally, we propose an Extension Vector Memory (EVM) architecture to achieve IDL in vector memory. EVM can support two data layout methods and vector memory access modes simultaneously. The key idea is to continuously distribute the data that needs to be accessed from the main memory to different banks during the loading period. Thus, EVM can provide a larger spatial locality level through careful programming and the extension ISA support. The experimental results showed a 1.43-fold improvement of state-of-the-art vector processors by the proposed architecture, with an area cost of only 1.73%. Furthermore, the energy consumption was reduced by 50.1%.},
  archive      = {J_TACO},
  author       = {Dunbo Zhang and Qingjie Lang and Ruoxi Wang and Li Shen},
  doi          = {10.1145/3631528},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {1},
  pages        = {18:1–23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Extension VM: Interleaved data layout in vector memory},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving utilization of dataflow unit for multi-batch
processing. <em>TACO</em>, <em>21</em>(1), 17:1–26. (<a
href="https://doi.org/10.1145/3637906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataflow architectures can achieve much better performance and higher efficiency than general-purpose core, approaching the performance of a specialized design while retaining programmability. However, advanced application scenarios place higher demands on the hardware in terms of cross-domain and multi-batch processing. In this article, we propose a unified scale-vector architecture that can work in multiple modes and adapt to diverse algorithms and requirements efficiently. First, a novel reconfigurable interconnection structure is proposed, which can organize execution units into different cluster typologies as a way to accommodate different data-level parallelism. Second, we decouple threads within each DFG node into consecutive pipeline stages and provide architectural support. By time-multiplexing during these stages, dataflow hardware can achieve much higher utilization and performance. In addition, the task-based program model can also exploit multi-level parallelism and deploy applications efficiently. Evaluated in a wide range of benchmarks, including digital signal processing algorithms, CNNs, and scientific computing algorithms, our design attains up to 11.95× energy efficiency (performance-per-watt) improvement over GPU (V100), and 2.01× energy efficiency improvement over state-of-the-art dataflow architectures.},
  archive      = {J_TACO},
  author       = {Zhihua Fan and Wenming Li and Zhen Wang and Yu Yang and Xiaochun Ye and Dongrui Fan and Ninghui Sun and Xuejun An},
  doi          = {10.1145/3637906},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {2},
  number       = {1},
  pages        = {17:1–26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Improving utilization of dataflow unit for multi-batch processing},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WA-zone: Wear-aware zone management optimization for
LSM-tree on ZNS SSDs. <em>TACO</em>, <em>21</em>(1), 16:1–23. (<a
href="https://doi.org/10.1145/3637488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ZNS SSDs divide the storage space into sequential-write zones, reducing costs of DRAM utilization, garbage collection, and over-provisioning. The sequential-write feature of zones is well-suited for LSM-based databases, where random writes are organized into sequential writes to improve performance. However, the current compaction mechanism of LSM-tree results in widely varying access frequencies (i.e., hotness) of data and thus incurs an extreme imbalance in the distribution of erasure counts across zones. The imbalance significantly limits the lifetime of SSDs. Moreover, the current zone-reset method involves a large number of unnecessary erase operations on unused blocks, further shortening the SSD lifetime. Considering the access pattern of LSM-tree, this article proposes a wear-aware zone-management technique, termed WA-Zone , to effectively balance inter- and intra-zone wear in ZNS SSDs. In WA-Zone, a wear-aware zone allocator is first proposed to dynamically allocate data with different hotness to zones with corresponding lifetimes, enabling an even distribution of the erasure counts across zones. Then, a partial-erase-based zone-reset method is presented to avoid unnecessary erase operations. Furthermore, because the novel zone-reset method might lead to an unbalanced distribution of erasure counts across blocks in a zone, a wear-aware block allocator is proposed. Experimental results based on the FEMU emulator demonstrate the proposed WA-Zone enhances the ZNS-SSD lifetime by 5.23×, compared with the baseline scheme.},
  archive      = {J_TACO},
  author       = {Linbo Long and Shuiyong He and Jingcheng Shen and Renping Liu and Zhenhua Tan and Congming Gao and Duo Liu and Kan Zhong and Yi Jiang},
  doi          = {10.1145/3637488},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {16:1–23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {WA-zone: Wear-aware zone management optimization for LSM-tree on ZNS SSDs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rcmp: Reconstructing RDMA-based memory disaggregation via
CXL. <em>TACO</em>, <em>21</em>(1), 15:1–26. (<a
href="https://doi.org/10.1145/3634916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory disaggregation is a promising architecture for modern datacenters that separates compute and memory resources into independent pools connected by ultra-fast networks, which can improve memory utilization, reduce cost, and enable elastic scaling of compute and memory resources. However, existing memory disaggregation solutions based on remote direct memory access (RDMA) suffer from high latency and additional overheads including page faults and code refactoring. Emerging cache-coherent interconnects such as CXL offer opportunities to reconstruct high-performance memory disaggregation. However, existing CXL-based approaches have physical distance limitation and cannot be deployed across racks. In this article, we propose Rcmp, a novel low-latency and highly scalable memory disaggregation system based on RDMA and CXL. The significant feature is that Rcmp improves the performance of RDMA-based systems via CXL, and leverages RDMA to overcome CXL’s distance limitation. To address the challenges of the mismatch between RDMA and CXL in terms of granularity, communication, and performance, Rcmp (1) provides a global page-based memory space management and enables fine-grained data access, (2) designs an efficient communication mechanism to avoid communication blocking issues, (3) proposes a hot-page identification and swapping strategy to reduce RDMA communications, and (4) designs an RDMA-optimized RPC framework to accelerate RDMA transfers. We implement a prototype of Rcmp and evaluate its performance by using micro-benchmarks and running a key-value store with YCSB benchmarks. The results show that Rcmp can achieve 5.2× lower latency and 3.8× higher throughput than RDMA-based systems. We also demonstrate that Rcmp can scale well with the increasing number of nodes without compromising performance.},
  archive      = {J_TACO},
  author       = {Zhonghua Wang and Yixing Guo and Kai Lu and Jiguang Wan and Daohui Wang and Ting Yao and Huatao Wu},
  doi          = {10.1145/3634916},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {15:1–26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Rcmp: Reconstructing RDMA-based memory disaggregation via CXL},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-grain quantitative analysis of demand paging in unified
virtual memory. <em>TACO</em>, <em>21</em>(1), 14:1–24. (<a
href="https://doi.org/10.1145/3632953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The abstraction of a shared memory space over separate CPU and GPU memory domains has eased the burden of portability for many HPC codebases. However, users pay for ease of use provided by system-managed memory with a moderate-to-high performance overhead. NVIDIA Unified Virtual Memory (UVM) is currently the primary real-world implementation of such abstraction and offers a functionally equivalent testbed for in-depth performance study for both UVM and future Linux Heterogeneous Memory Management (HMM) compatible systems. The continued advocacy for UVM and HMM motivates improvement of the underlying system. We focus on UVM-based systems and investigate the root causes of UVM overhead, a non-trivial task due to complex interactions of multiple hardware and software constituents and the desired cost granularity. In our prior work, we delved deeply into UVM system architecture and showed internal behaviors of page fault servicing in batches. We provided quantitative evaluation of batch handling for various applications under different scenarios, including prefetching and oversubscription. We revealed that the driver workload depends on the interactions among application access patterns, GPU hardware constraints, and host OS components. Host OS components have significant overhead present across implementations, warranting close attention. This extension furthers our prior study in three aspects: fine-grain cost analysis and breakdown, extension to multiple GPUs, and investigation of platforms with different GPU-GPU interconnects. We take a top-down approach to quantitative batch analysis and uncover how constituent component costs accumulate and overlap, governed by synchronous and asynchronous operations. Our multi-GPU analysis shows reduced cost of GPU-GPU batch workloads compared to CPU-GPU workloads. We further demonstrate that while specialized interconnects, NVLink, can improve batch cost, their benefits are limited by host OS software overhead and GPU oversubscription. This study serves as a proxy for future shared memory systems, such as those that interface with HMM, and the development of interconnects.},
  archive      = {J_TACO},
  author       = {Tyler Allen and Bennett Cooper and Rong Ge},
  doi          = {10.1145/3632953},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {14:1–24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Fine-grain quantitative analysis of demand paging in unified virtual memory},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware-hardened sandbox enclaves for trusted serverless
computing. <em>TACO</em>, <em>21</em>(1), 13:1–25. (<a
href="https://doi.org/10.1145/3632954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud-based serverless computing, an application consists of multiple functions provided by mutually distrusting parties. For secure serverless computing, the hardware-based trusted execution environment (TEE) can provide strong isolation among functions. However, not only protecting each function from the host OS and other functions, but also protecting the host system from the functions, is critical for the security of the cloud servers. Such an emerging trusted serverless computing poses new challenges: Each TEE must be isolated from the host system bi-directionally, and the system calls from it must be validated. In addition, the resource utilization of each TEE must be accountable in a mutually trusted way. However, the current TEE model cannot efficiently represent such trusted serverless applications. To overcome the lack of such hardware support, this article proposes an extended TEE model called Cloister , designed for trusted serverless computing. Cloister proposes four new key techniques. First, it extends the hardware-based memory isolation in SGX to confine a deployed function only within its TEE (enclave). Second, it proposes a trusted monitor enclave that filters and validates system calls from enclaves. Third, it provides a trusted resource accounting mechanism for enclaves that is agreeable to both service developers and cloud providers. Finally, Cloister accelerates enclave loading by redesigning its memory verification for fast function deployment. Using an emulated Intel SGX platform with the proposed extensions, this article shows that trusted serverless applications can be effectively supported with small changes in the SGX hardware.},
  archive      = {J_TACO},
  author       = {Joongun Park and Seunghyo Kang and Sanghyeon Lee and Taehoon Kim and Jongse Park and Youngjin Kwon and Jaehyuk Huh},
  doi          = {10.1145/3632954},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {13:1–25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Hardware-hardened sandbox enclaves for trusted serverless computing},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COWS for high performance: Cost aware work stealing for
irregular parallel loop. <em>TACO</em>, <em>21</em>(1), 12:1–26. (<a
href="https://doi.org/10.1145/3633331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel libraries such as OpenMP distribute the iterations of parallel-for-loops among the threads, using a programmer-specified scheduling policy. While the existing scheduling policies perform reasonably well in the context of balanced workloads, in computations that involve highly imbalanced workloads it is extremely non-trivial to obtain an efficient distribution of work (even using non-static scheduling methods like dynamic and guided). In this paper, we present a scheme called COst aware Work Stealing (COWS) to efficiently extend the idea of work-stealing to OpenMP. In contrast to the traditional work-stealing schedulers, COWS takes into consideration that (i) not all iterations of a parallel-for-loops may take the same amount of time. (ii) identifying a suitable victim for stealing is important for load-balancing, and (iii) queues lead to significant overheads in traditional work-stealing and should be avoided. We present two variations of COWS: WSRI (a naive work-stealing scheme based on the number of remaining iterations) and WSRW (work-stealing scheme based on the amount of remaining workload). Since in irregular loops like those found in graph analytics it is not possible to statically compute the cost of the iterations of the parallel-for-loops, we use a combined compile-time + runtime approach, where the remaining workload of a loop is computed efficiently at runtime by utilizing the code generated by our compile-time component. We have performed an evaluation over seven different benchmark programs, using five different input datasets, on two different hardware across a varying number of threads; leading to a total number of 275 configurations. We show that in 225 out of 275 configurations, compared to the best OpenMP scheduling scheme for that configuration, our approach achieves clear performance gains.},
  archive      = {J_TACO},
  author       = {Prasoon Mishra and V. Krishna Nandivada},
  doi          = {10.1145/3633331},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {12:1–26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {COWS for high performance: Cost aware work stealing for irregular parallel loop},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ISP agent: A generalized in-storage-processing workload
offloading framework by providing multiple optimization opportunities.
<em>TACO</em>, <em>21</em>(1), 11:1–24. (<a
href="https://doi.org/10.1145/3632951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As solid-state drives (SSDs) with sufficient computing power have recently become the dominant devices in modern computer systems, in-storage processing (ISP), which processes data within the storage without transferring it to the host memory, is being utilized in various emerging applications. The main challenge of ISP is to deliver storage data to the offloaded workload. This is difficult because of the information gap between the host and storage, the data consistency problem between the host and offloaded workloads, and SSD-specific hardware limitations. Moreover, because the offloaded workloads use internal SSD resources, host I/O performance might be degraded due to resource conflicts. Although several ISP frameworks have been proposed, existing ISP approaches that do not deeply consider the internal SSD behavior are often insufficient to support efficient ISP workload offloading with high programmability. In this article, we propose an ISP agent, a lightweight ISP workload offloading framework for SSD devices. The ISP agent provides I/O and memory interfaces that allow users to run existing function codes on SSDs without major code modifications, and separates the resources for the offloaded workloads from the existing SSD firmware to minimize interference with host I/O processing. The ISP agent also provides further optimization opportunities for the offloaded workload by considering SSD architectures. We have implemented the ISP agent on the OpenSSD Cosmos+ board and evaluated its performance using synthetic benchmarks and a real-world ISP-assisted database checkpointing application. The experimental results demonstrate that the ISP agent enhances host application performance while increasing ISP programmability, and that the optimization opportunities provided by the ISP agent can significantly improve ISP-side performance without compromising host I/O processing.},
  archive      = {J_TACO},
  author       = {Seokwon Kang and Jongbin Kim and Gyeongyong Lee and Jeongmyung Lee and Jiwon Seo and Hyungsoo Jung and Yong Ho Song and Yongjun Park},
  doi          = {10.1145/3632951},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {11:1–24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ISP agent: A generalized in-storage-processing workload offloading framework by providing multiple optimization opportunities},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Abakus: Accelerating k-mer counting with storage technology.
<em>TACO</em>, <em>21</em>(1), 10:1–26. (<a
href="https://doi.org/10.1145/3632952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work seeks to leverage Processing-with-storage-technology (PWST) to accelerate a key bioinformatics kernel called k -mer counting, which involves processing large files of sequence data on the disk to build a histogram of fixed-size genome sequence substrings and thereby entails prohibitively high I/O overhead. In particular, this work proposes a set of accelerator designs called Abakus that offer varying degrees of tradeoffs in terms of performance, efficiency, and hardware implementation complexity. The key to these designs is a set of domain-specific hardware extensions to accelerate the key operations for k -mer counting at various levels of the SSD hierarchy, with the goal of enhancing the limited computing capabilities of conventional SSDs, while exploiting the parallelism of the multi-channel, multi-way SSDs. Our evaluation suggests that Abakus can achieve 8.42×, 6.91×, and 2.32× speedup over the CPU-, GPU-, and near-data processing solutions.},
  archive      = {J_TACO},
  author       = {Lingxi Wu and Minxuan Zhou and Weihong Xu and Ashish Venkat and Tajana Rosing and Kevin Skadron},
  doi          = {10.1145/3632952},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {10:1–26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Abakus: Accelerating k-mer counting with storage technology},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QuCloud+: A holistic qubit mapping scheme for
single/multi-programming on 2D/3D NISQ quantum computers. <em>TACO</em>,
<em>21</em>(1), 9:1–27. (<a
href="https://doi.org/10.1145/3631525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Qubit mapping for NISQ superconducting quantum computers is essential to fidelity and resource utilization. The existing qubit mapping schemes meet challenges, e.g., crosstalk, SWAP overheads, diverse device topologies, etc., leading to qubit resource underutilization and low fidelity in computing results. This article introduces QuCloud+, a new qubit mapping scheme that tackles these challenges. QuCloud+ has several new designs. (1) QuCloud+ supports single/multi-programming quantum computing on quantum chips with 2D/3D topology. (2) QuCloud+ partitions physical qubits for concurrent quantum programs with the crosstalk-aware community detection technique and further allocates qubits according to qubit degree, improving fidelity, and resource utilization. (3) QuCloud+ includes an X-SWAP mechanism that avoids SWAPs with high crosstalk errors and enables inter-program SWAPs to reduce the SWAP overheads. (4) QuCloud+ schedules concurrent quantum programs to be mapped and executed based on estimated fidelity for the best practice. Experimental results show that, compared with the existing typical multi-programming study [ 12 ], QuCloud+ achieves up to 9.03% higher fidelity and saves on the required SWAPs during mapping, reducing the number of CNOT gates inserted by 40.92%. Compared with a recent study [ 30 ] that enables post-mapping gate optimizations to further reduce gates, QuCloud+ reduces the post-mapping circuit depth by 21.91% while using a similar number of gates.},
  archive      = {J_TACO},
  author       = {Lei Liu and Xinglei Dou},
  doi          = {10.1145/3631525},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {9:1–27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {QuCloud+: A holistic qubit mapping scheme for Single/Multi-programming on 2D/3D NISQ quantum computers},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient cross-platform multiplexing of hardware
performance counters via adaptive grouping. <em>TACO</em>,
<em>21</em>(1), 8:1–26. (<a
href="https://doi.org/10.1145/3629525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collecting sufficient microarchitecture performance data is essential for performance evaluation and workload characterization. There are many events to be monitored in a modern processor while only a few hardware performance monitoring counters (PMCs) can be used, so multiplexing is commonly adopted. However, inefficiency commonly exists in state-of-the-art profiling tools when grouping events for multiplexing PMCs. It has the risk of inaccurate measurement and misleading analysis. Commercial tools can leverage PMCs, but they are closed source and only support their specified platforms. To this end, we propose an approach for efficient cross-platform microarchitecture performance measurement via adaptive grouping, aiming to improve the metrics’ sampling ratios. The approach generates event groups based on the number of available PMCs detected on arbitrary machines while avoiding the scheduling pitfall of Linux perf_event subsystem. We evaluate our approach with SPEC CPU 2017 on four mainstream x86-64 and AArch64 processors and conduct comparative analyses of efficiency with two other state-of-the-art tools, LIKWID and ARM Top-down Tool. The experimental results indicate that our approach gains around 50% improvement in the average sampling ratio of metrics without compromising the correctness and reliability.},
  archive      = {J_TACO},
  author       = {Tong-Yu Liu and Jianmei Guo and Bo Huang},
  doi          = {10.1145/3629525},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {8:1–26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Efficient cross-platform multiplexing of hardware performance counters via adaptive grouping},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAC: An ultra-efficient spin-based architecture for
compressed DNNs. <em>TACO</em>, <em>21</em>(1), 7:1–26. (<a
href="https://doi.org/10.1145/3632957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have achieved great progress in academia and industry. But they have become computational and memory intensive with the increase of network depth. Previous designs seek breakthroughs in software and hardware levels to mitigate these challenges. At the software level, neural network compression techniques have effectively reduced network scale and energy consumption. However, the conventional compression algorithm is complex and energy intensive. At the hardware level, the improvements in the semiconductor process have effectively reduced power and energy consumption. However, it is difficult for the traditional Von-Neumann architecture to further reduce the power consumption, due to the memory wall and the end of Moore’s law. To overcome these challenges, the spintronic device based DNN machines have emerged for their non-volatility, ultra low power, and high energy efficiency. However, there is no spin-based design that has achieved innovation at both the software and hardware level. Specifically, there is no systematic study of spin-based DNN architecture to deploy compressed networks. In our study, we present an ultra-efficient Spin-based Architecture for Compressed DNNs (SAC), to substantially reduce power consumption and energy consumption. Specifically, we propose a One-Step Compression algorithm (OSC) to reduce the computational complexity with minimum accuracy loss. We also propose a spin-based architecture to realize better performance for the compressed network. Furthermore, we introduce a novel computation flow that enables the reuse of activations and weights. Experimental results show that our study can reduce the computational complexity of compression algorithm from 𝒪( Tk 3 to 𝒪( k 2 log k ), and achieve 14× ∼ 40× compression ratio. Furthermore, our design can attain a 2× enhancement in power efficiency and a 5× improvement in computational efficiency compared to the Eyeriss. Our models are available at an anonymous link https://bit.ly/39cdtTa .},
  archive      = {J_TACO},
  author       = {Yunping Zhao and Sheng Ma and Heng Liu and Libo Huang and Yi Dai},
  doi          = {10.1145/3632957},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {7:1–26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SAC: An ultra-efficient spin-based architecture for compressed DNNs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QoS-pro: A QoS-enhanced transaction processing framework for
shared SSDs. <em>TACO</em>, <em>21</em>(1), 6:1–25. (<a
href="https://doi.org/10.1145/3632955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solid State Drives (SSDs) are widely used in data-intensive scenarios due to their high performance and decreasing cost. However, in shared environments, concurrent workloads can interfere with each other, leading to a violation of Quality of Service (QoS). While QoS mechanisms like fairness guarantees and latency constraints have been integrated into SSDs, existing transaction processing frameworks offer limited QoS guarantees and can significantly degrade overall performance in a shared environment. The reason is that the internal components of an SSD, originally designed to exploit parallelism, struggle to coordinate effectively when QoS mechanisms are applied to them. This article proposes a novel QoS -enhanced transaction pro cessing framework, called QoS-pro, which enhances QoS guarantees for concurrent workloads while maintaining high parallelism for SSDs. QoS-pro achieves this by redesigning transaction processing procedures to fully exploit the parallelism of shared SSDs and enhancing QoS-oriented transaction translation and scheduling with parallelism features in mind. In terms of fairness guarantees, QoS-pro outperforms state-of-the-art methods by achieving 96% fairness improvement and 64% maximum latency reduction. QoS-pro also shows almost no loss in throughput when compared with parallelism-oriented methods. Additionally, QoS-pro triggers the fewest Garbage Collection (GC) operations and minimally affects concurrently running workloads during GC operations.},
  archive      = {J_TACO},
  author       = {Hao Fan and Yiliang Ye and Shadi Ibrahim and Zhuo Huang and Xingru Li and Weibin Xue and Song Wu and Chen Yu and Xuanhua Shi and Hai Jin},
  doi          = {10.1145/3632955},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {6:1–25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {QoS-pro: A QoS-enhanced transaction processing framework for shared SSDs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast convolution meets low precision: Exploring efficient
quantized winograd convolution on modern CPUs. <em>TACO</em>,
<em>21</em>(1), 5:1–26. (<a
href="https://doi.org/10.1145/3632956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-precision computation has emerged as one of the most effective techniques for accelerating convolutional neural networks and has garnered widespread support on modern hardware. Despite its effectiveness in accelerating convolutional neural networks, low-precision computation has not been commonly applied to fast convolutions, such as the Winograd algorithm, due to numerical issues. In this article, we propose an effective quantized Winograd convolution, named LoWino, which employs an in-side quantization method in the Winograd domain to reduce the precision loss caused by transformations. Meanwhile, we present an efficient implementation that integrates well-designed optimization techniques, allowing us to fully exploit the capabilities of low-precision computation on modern CPUs. We evaluate LoWino on two Intel Xeon Scalable Processor platforms with representative convolutional layers and neural network models. The experimental results demonstrate that our approach can achieve an average of 1.84× and 1.91× operator speedups over state-of-the-art implementations in the vendor library while preserving accuracy loss at a reasonable level.},
  archive      = {J_TACO},
  author       = {Xueying Wang and Guangli Li and Zhen Jia and Xiaobing Feng and Yida Wang},
  doi          = {10.1145/3632956},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {1},
  number       = {1},
  pages        = {5:1–26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Fast convolution meets low precision: Exploring efficient quantized winograd convolution on modern CPUs},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
