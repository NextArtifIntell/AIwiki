<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TKDD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tkdd---199">TKDD - 199</h2>
<ul>
<li><details>
<summary>
(2024). GeoGail: A model-based imitation learning framework for
human trajectory synthesizing. <em>TKDD</em>, <em>19</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3699961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesized human trajectories are crucial for a large number of applications. Existing solutions are mainly based on the generative adversarial network (GAN), which is limited due to the lack of modeling the human decision-making process. In this article, we propose a novel imitation learning-based method to synthesize human trajectories. This model utilizes a novel semantics-based interaction mechanism between the decision-making strategy and visitations to diverse geographical locations to model them in the semantic domain in a uniform manner. To augment the modeling ability to the real-world human decision-making policy, we propose a feature extraction model to extract the internal latent factors of variation of different individuals and then propose a novel self-attention-based policy net to capture the long-term correlation of mobility and decision-making patterns. Then, to better reward users’ mobility behavior, we propose a novel multi-scale reward net combined with mutual information to model the instant reward, long-term reward, and individual characteristics in a cohesive manner. Extensive experimental results on two real-world trajectory datasets show that our proposed model can synthesize the most high-quality trajectory data compared with six state-of-the-art baselines in terms of a number of key usability metrics and can well support practical applications based on trajectory data, demonstrating its effectiveness. Furthermore, our proposed method can learn explainable knowledge automatically from data, including explainable statistical features of trajectories and statistical relation between decision-making policy and features.},
  archive      = {J_TKDD},
  doi          = {10.1145/3699961},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {GeoGail: A model-based imitation learning framework for human trajectory synthesizing},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial multi-label learning via exploiting instance and
label correlations. <em>TKDD</em>, <em>19</em>(1), 1–22. (<a
href="https://doi.org/10.1145/3700879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of partial multi-label learning is to induce a multi-label classifier from partial multi-label data where each instance is annotated with a number of candidate labels but only a subset of them are valid. Many of the existing studies either fail to fully utilize instance and label correlations to eliminate noisy labels or build an over-simplified multi-label classifier, both of which are unfavorable for the improvement of generalization performance. In this article, we put forward a novel model named P ml-ilc to learn a multi-label classifier from partial multi-label data. Specifically, P ml-ilc first encodes instances and labels into a compact semantic space and takes full advantage of instance and label correlations to eliminate noisy labels. Then, it induces a linear mapping from the feature space to the label space while exploiting label-specific features and instance correlations to facilitate the multi-label classifier learning process. Finally, the above two steps are combined into a joint optimization problem and an efficient alternating optimization procedure is developed to find a satisfactory solution. Extensive experiments show that P ml-ilc achieves superior performance on both real-world and synthetic partial multi-label datasets in terms of different evaluation metrics.},
  archive      = {J_TKDD},
  doi          = {10.1145/3700879},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Partial multi-label learning via exploiting instance and label correlations},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scale-aware neural architecture search for multivariate time
series forecasting. <em>TKDD</em>, <em>19</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3701038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series (MTS) forecasting has attracted much attention in many intelligent applications. It is not a trivial task, as we need to consider both intra-variable dependencies and inter-variable dependencies. However, existing works are designed for specific scenarios and require much domain knowledge and expert efforts, which is difficult to transfer between different scenarios. In this article, we propose a scale-aware neural architecture search framework for MTS forecasting (SNAS4MTF). A multi-scale decomposition module transforms raw time series into multi-scale sub-series, which can preserve multi-scale temporal patterns. An adaptive graph learning module infers the different inter-variable dependencies under different time scales without any prior knowledge. For MTS forecasting, a search space is designed to capture both intra-variable dependencies and inter-variable dependencies at each time scale. The multi-scale decomposition, adaptive graph learning, and neural architecture search modules are jointly learned in an end-to-end framework. Extensive experiments on two real-world datasets demonstrate that SNAS4MTF achieves a promising performance compared with the state-of-the-art methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3701038},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Scale-aware neural architecture search for multivariate time series forecasting},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learnable transform-assisted tensor decomposition for
spatio-irregular multidimensional data recovery. <em>TKDD</em>,
<em>19</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3701235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor decompositions have been successfully applied to multidimensional data recovery. However, classical tensor decompositions are not suitable for emerging spatio-irregular multidimensional data (i.e., spatio-irregular tensor), whose spatial domain is non-rectangular, e.g., spatial transcriptomics data from bioinformatics and semantic units from computer vision. By using preprocessing (e.g., zero-padding or element-wise 0-1 weighting), the spatio-irregular tensor can be converted to a spatio-regular tensor and then classical tensor decompositions can be applied, but this strategy inevitably introduces bias information, leading to artifacts. How to design a tensor-based method suitable for emerging spatio-irregular tensors is an imperative challenge. To address this challenge, we propose a learnable transform-assisted tensor singular value decomposition (LTA-TSVD) for spatio-irregular tensor recovery, which allows us to leverage the intrinsic structure behind the spatio-irregular tensor. Specifically, we design a learnable transform to project the original spatio-irregular tensor into its latent spatio-regular tensor, and then the latent low-rank structure is captured by classical TSVD on the resulting regular tensor. Empowered by LTA-TSVD, we develop spatio-irregular low-rank tensor completion (SIR-LRTC) and spatio-irregular tensor robust principal component analysis (SIR-TRPCA) models for the spatio-irregular tensor imputation and denoising respectively, and we design corresponding solving algorithms with theoretical convergence. Extensive experiments including the spatial transcriptomics data imputation and hyperspectral image denoising show SIR-LRTC and SIR-TRPCA are superior performance to competing approaches and benefit downstream applications.},
  archive      = {J_TKDD},
  doi          = {10.1145/3701235},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learnable transform-assisted tensor decomposition for spatio-irregular multidimensional data recovery},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JobFormer: Skill-aware job recommendation with
semantic-enhanced transformer. <em>TKDD</em>, <em>19</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3701735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Job recommendation aims to provide potential talents with suitable job descriptions (JDs) consistent with their career trajectory, which plays an essential role in proactive talent recruitment. In real-world management scenarios, the available JD-user records always consist of JDs, user profiles, and click data, in which the user profiles are typically summarized as the user&#39;s skill distribution for privacy reasons. Although existing sophisticated recommendation methods can be directly employed, effective recommendation still has challenges considering the information deficit of JD itself and the natural heterogeneous gap between JD and user profile. To address these challenges, we proposed a novel skill-aware recommendation model based on the designed semantic-enhanced Transformer to parse JDs and complete personalized job recommendation. Specifically, we first model the relative items of each JD and then adopt an encoder with the local-global attention mechanism to better mine the intra-job and inter-job dependencies from JD tuples. Moreover, we adopt a two-stage learning strategy for skill-aware recommendation, in which we utilize the skill distribution to guide JD representation learning in the recall stage and then combine the user profiles for final prediction in the ranking stage. Consequently, we can embed rich contextual semantic representations for learning JDs, while skill-aware recommendation provides effective JD-user joint representation for click-through rate (CTR) prediction. To validate the superior performance of our method for job recommendation, we present a thorough empirical analysis of large-scale real-world and public datasets to demonstrate its effectiveness and interpretability.},
  archive      = {J_TKDD},
  doi          = {10.1145/3701735},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {JobFormer: Skill-aware job recommendation with semantic-enhanced transformer},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controllable human trajectory generation using
profile-guided latent diffusion. <em>TKDD</em>, <em>19</em>(1), 1–25.
(<a href="https://doi.org/10.1145/3701736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory generation is a vital element in AI applications. Firstly, it enables simulation such as traffic simulation and epidemic spreading modeling. Secondly, it can provide synthetic privacy-preserving data for training AI models. Notably, trajectory generation featuring controllable user profiles holds substantial value in generating customized mobility trajectories tailored to diverse requirements. However, relevant work is still lacking. On the one hand, traditional deep generative models fall short in guiding controllable trajectory generation due to the statistical nature of human mobility patterns and the corresponding insufficient control mechanisms. On the other hand, though the diffusion model has demonstrated strong generative capabilities in many fields, to achieve controllable generation on discrete trajectory data, we still need to redesign the structure of the continuous diffusion model. In this article, we introduce a controllable trajectory generation framework that leverages a continuous diffusion model and classifier guidance for more robust condition control. Our proposed framework comprises two modules: a latent trajectory diffusion model and a trajectory classifier for profile guidance. Experiments on two real-world mobility datasets consistently demonstrate its capability of generating trajectories matching given user profiles and conforming to human mobility patterns. Our source code and trained models are released at https://github.com/tsinghua-fib-lab/User-Profile-Guided-Latent-Diffusion .},
  archive      = {J_TKDD},
  doi          = {10.1145/3701736},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Controllable human trajectory generation using profile-guided latent diffusion},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general concave fairness framework for influence
maximization based on poverty reward. <em>TKDD</em>, <em>19</em>(1),
1–23. (<a href="https://doi.org/10.1145/3701737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) aims to find a group of influential nodes as initial spreaders to maximize the influence spread over a network. Yet, traditional IM algorithms have not been designed with fairness in mind, resulting in discrimination against some groups, like LGBTQ communities and racial minorities. This issue has spurred research on Fair Influence Maximization (FIM). However, existing FIM studies come with some drawbacks. First, most proposed notions of fairness for FIM cannot adjust the tradeoff between fairness level and influence spread. Second, though a few specific notions of fairness allow such balancing, they are limited to a few specific concave functions, which may not be suitable for various real-world scenarios. Furthermore, none of them have studied the deep relations between the features of concave functions and the level of fairness. Third, existing fairness metrics are limited to their corresponding concepts of fairness. Comparing the level of fairness across different algorithms using existing metrics can be challenging. To tackle the above problems, this article first proposes a novel fairness notion named Poverty Reward (PR), which achieves fairness by rewarding the enrichment of groups with low utility. Based on PR, we further propose an algorithmic framework called Concave Fairness Framework (CFF) that allows any concave function that satisfies specific requirements. We also systematically clarify how fairness is improved by applying concave functions and provide an in-depth quantitative analysis of how to select appropriate concave functions for different utility distributions. Moreover, we propose the Reward of Fairness (RoF) metric that evaluates the disparity between groups. Based on RoF, an evaluation system is built to uniformly compare FIM algorithms from different fairness notions. Experiments in real-world datasets have demonstrated the validity of the CFF, as well as the proposed fairness notion.},
  archive      = {J_TKDD},
  doi          = {10.1145/3701737},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A general concave fairness framework for influence maximization based on poverty reward},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital twin enhanced multi-agent reinforcement learning for
large-scale mobile network coverage optimization. <em>TKDD</em>,
<em>19</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3702644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of communication technology and the exponential growth of mobile users, improving network coverage quality and throughput has become increasingly important. In particular, large-scale Base Station (BS) cooperative optimization has become a highly significant topic. BSs can adjust various parameters for high-quality communication, but automating this optimization remains challenging due to environmental sensitivity and interdependencies. Traditional methods for network optimization are constrained by the intricate nature of real-world environments. Further, Reinforcement Learning (RL) techniques, which are effective for configuration policies, encounter difficulties in intricate, high-dimensional wireless communication networks, especially in multi-agent cooperative optimization. To overcome these challenges, this article proposes the Enhanced Multi-Agent Proximal Policy Optimization (EMAPPO), which utilizes the capabilities of the UNet network to extract multi-spatial relationships among a massive number of network elements and employs the DiffPool network to efficiently depict the impact of large-scale action coordination among massive agents on coverage performance. To facilitate evaluation in communication optimization, we further introduce a high-fidelity digital twin-driven mobile network. Extensive experiments validate the effectiveness and superior performance of EMAPPO by utilizing the network digital twin. The results demonstrate significant improvements in signal coverage rate and network throughput compared to the competing methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3702644},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Digital twin enhanced multi-agent reinforcement learning for large-scale mobile network coverage optimization},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SOHUPDS+: An efficient one-phase algorithm for mining high
utility patterns over a data stream. <em>TKDD</em>, <em>19</em>(1),
1–32. (<a href="https://doi.org/10.1145/3702645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing algorithms for mining high utility patterns over a data stream are two-phase algorithms that are not scalable due to the large number of candidates generation in the first phase, particularly when the minimum utility threshold is low. Moreover, in the second phase, the algorithm needs to scan the database again to find out actual utility for candidates. In this article, we propose one-phase algorithm SOHUPDS \(+\) to mine high utility itemsets in the current sliding window of the data stream with respect to absolute or relative minimum utility threshold. To facilitate SOHUPDS \(+\) , we propose a data structure IUDataListSW \(+\) , which stores and maintains utility and upper-bound values of the items in the current sliding window when sliding window advances. In addition, we propose a transaction merging strategy, called BitmapTransactionMerging , which saves execution time for utility and upper-bound values computations in denser datasets. Moreover, we propose update strategies to utilize mined high utility patterns from the previous sliding window to update high utility patterns in the current sliding window. The results of experiments illustrate that SOHUPDS \(+\) is more efficient than the state-of-the-art algorithms in terms of execution time as well as memory usage in most of the experiments on various datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3702645},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {SOHUPDS+: An efficient one-phase algorithm for mining high utility patterns over a data stream},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Question embedding on weighted heterogeneous information
network for knowledge tracing. <em>TKDD</em>, <em>19</em>(1), 1–28. (<a
href="https://doi.org/10.1145/3703158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Tracing (KT) aims to predict students’ future performance on answering questions based on their historical exercise sequences. To alleviate the problem of data sparsity in KT, recent works have introduced auxiliary information to mine question similarity, resulting in the enhancement of question embeddings. Nonetheless, there remains a gap in developing an approach that effectively incorporates various forms of auxiliary information, including relational information (e.g., question–student , question–skill relation), relationship attributes (e.g., correctness indicating a student&#39;s performance on a question), and node attributes (e.g., student ability ). To tackle this challenge, the Similarity-enhanced Question Embedding (SimQE) method for KT is proposed, with its central feature being the utilization of weighted and attributed meta-paths for extracting question similarity. To capture multi-dimensional question similarity semantics by integrating multiple relations, various meta-paths are constructed for learning question embeddings separately. These embeddings, each encoding different similarity semantics, are then fused to serve the task of KT. To capture finer-grained similarity by leveraging the relationship attributes and node attributes on the meta-paths, the biased random walk algorithm is designed. In addition, the auxiliary node generation method is proposed to capture high-order question similarity. Finally, extensive experiments conducted on six datasets demonstrate that SimQE performs the best among 10 representative question embedding methods. Furthermore, SimQE proves to be more effective in alleviating the problem of data sparsity.},
  archive      = {J_TKDD},
  doi          = {10.1145/3703158},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Question embedding on weighted heterogeneous information network for knowledge tracing},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MagNet: Multilevel dynamic wavelet graph neural network for
multivariate time series classification. <em>TKDD</em>, <em>19</em>(1),
1–22. (<a href="https://doi.org/10.1145/3703915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate Time Series Classification (MTSC) is a fundamental data mining task, which is widely applied in the fields like health care and energy management. However, the existing MTSC methods are mostly adapted from univariate versions and model the static patterns among series in the time domain. We argue they fail to capture the inter-dependencies across variables and rarely consider the unique dynamic features in multilevel frequencies, which are susceptible to signal noise and lack sufficient feature extraction capability to achieve satisfactory classification accuracy. To address these issues, we propose a novel framework called Multilevel Dynamic Wavelet Graph Neural Network (MagNet) , which effectively captures inherent temporal-frequency dependencies in multivariate time series data in a global view, facilitating the information flow among inter-related variables and leveraging learnable Graph Neural Networks (GNNs) to uncover dynamic frequency dependencies. We propose an orthogonal temporal convolution layer that utilizes soft orthogonal losses to constrain features learned at different frequency components to reduce feature redundancy. Additionally, we introduce a hierarchical graph coarsening operator to address the flat learning challenges in traditional GNNs. Our dynamic wavelet GNN and hierarchical coarsening enable deep model stacking and end-to-end learning. Extensive experiments on 30 UEA benchmarks demonstrate that our method outperforms the state-of-the-art baselines in the MTSC tasks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3703915},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {MagNet: Multilevel dynamic wavelet graph neural network for multivariate time series classification},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual perspective of label-specific feature learning for
multi-label classification. <em>TKDD</em>, <em>19</em>(1), 1–30. (<a
href="https://doi.org/10.1145/3705006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label-specific features work as an effective supervised feature manipulation strategy to account for distinct discriminative properties of each class label in multi-label classification. Existing approaches implement this strategy in its primal form, i.e., finding the most pertinent features specific to each class label and directly inducing classifiers on these features. Instead of such a straightforward implementation, a dual perspective for label-specific feature learning is investigated in this article. As a dual problem of existing primal one, we consider label-specific discriminative properties by identifying non-informative features for each class label and making the discrimination process immutable to variations of identified features. Accordingly, a perturbation-based approach Dela is presented, which endows classifiers with immutability on simultaneously identified non-informative features by solving a probabilistically relaxed expected risk minimization problem. Furthermore, we touch the realistic issue of label-specific feature learning in a weakly supervised scenario via extending Dela to accommodate to multi-label data with missing labels. Comprehensive experiments show that our approach outperforms the state-of-the-art counterparts.},
  archive      = {J_TKDD},
  doi          = {10.1145/3705006},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Dual perspective of label-specific feature learning for multi-label classification},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal discovery using weight-based conditional independence
test. <em>TKDD</em>, <em>19</em>(1), 1–24. (<a
href="https://doi.org/10.1145/3687467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional Independence (CI) tests play an essential role in causal discovery from observational data, enabling the measurement of independence between two nodes. However, traditional CI tests ignore the imbalanced occurrence probabilities of node values, which may affect the accuracy of determining independence between nodes. To address this problem, we first introduce a new concept of the Node-imbalance phenomenon to describe the imbalance of node values in the Bayesian network data and analyze the influence of the Node-imbalance phenomenon on the traditional CI tests, then we propose a Weight-Based Conditional Independence (WCI) test to improve the accuracy of CI tests in the presence of Node-imbalance. In the experiments, we verify that WCI effectively measures the dependency between nodes in the Node-imbalance phenomenon compared with the traditional independence tests, and the state-of-the-art causal discovery algorithms reduce the number of false causal orientations through WCI.},
  archive      = {J_TKDD},
  doi          = {10.1145/3687467},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Causal discovery using weight-based conditional independence test},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting pre-trained language models for black-box attack
against knowledge graph embeddings. <em>TKDD</em>, <em>19</em>(1), 1–14.
(<a href="https://doi.org/10.1145/3688850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the emerging research on adversarial attacks against knowledge graph embedding (KGE) models, most of them focus on white-box attack settings. However, white-box attacks are difficult to apply in practice compared to black-box attacks since they require access to model parameters that are unlikely to be provided. In this article, we propose a novel black-box attack method that only requires access to knowledge graph data, making it more realistic in real-world attack scenarios. Specifically, we utilize pre-trained language models (PLMs) to encode text features of the knowledge graphs, an aspect neglected by previous research. We then employ these encoded text features to identify the most influential triples for constructing corrupted triples for the attack. To improve the transferability of the attack, we further propose to fine-tune the PLM model by enriching triple embeddings with structure information. Extensive experiments conducted on two knowledge graph datasets illustrate the effectiveness of our proposed method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3688850},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {1},
  pages        = {1-14},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Exploiting pre-trained language models for black-box attack against knowledge graph embeddings},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hubness-enabled clustering and recovery for large-scale
incomplete multi-view data. <em>TKDD</em>, <em>19</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3694689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering has gained considerable attention in recent years due to the prevalence of incomplete multi-view data in real-world applications. However, existing methods often struggle to effectively deal with large-scale datasets, particularly those with a significant number of missing instances. To address these issues, we propose a novel method called Hubness-Enabled Clustering and Recovery for Large-Scale Incomplete Multi-View Data (HENRI). HENRI utilizes the consensus hubs of all views to identify informative anchors to handle large-scale incomplete datasets. Furthermore, it incorporates a novel sample-level fusion strategy that effectively integrates information from all views, leading to remarkable outcomes in both cluster formation and missing data reconstruction. HENRI demonstrates exceptional capability in capturing the underlying structures of the data and recovering missing information, even when faced with a significant number of instances with incomplete data in partial views. To validate its effectiveness, we conducted experiments on 6 complete datasets and 31 incomplete datasets, comparing against 11 baseline methods. The results are impressive, demonstrating the superior performance of HENRI over the state-of-the-art methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3694689},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Hubness-enabled clustering and recovery for large-scale incomplete multi-view data},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Label distribution guided hashing for cross-modal retrieval.
<em>TKDD</em>, <em>19</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3697353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing methods have recently attracted extensive attention in cross-modal retrieval. Most supervised hashing methods attempt to preserve the semantic information into hash codes by leveraging the original logical label matrix. However, they generally treat all labels equally, and ignore the relative significance of different labels due to the variety of data features. In this article, we argue that exploring the relative importance of labels benefits the enhancement of semantic information, and we propose a novel LAbel Distribution Guided Hashing (LADH) method for cross-modal retrieval. In particular, LADH first learns a feature-induced label distribution for each sample to weigh different labels, which leverages the multi-modal feature information to enrich the semantic label information. By jointly using the learned label distributions and multi-modal features, the latent representation and hash codes are obtained with multi-modal feature selection and enhanced semantic similarities embedded. An efficient algorithm is designed to solve the proposed method whose time complexity is linear to the number of the training instances. Experimental results on several public benchmark datasets verify the effectiveness and efficiency of our method compared with the state-of-the-art methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3697353},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Label distribution guided hashing for cross-modal retrieval},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing the spatial-temporal causal impact of
COVID-19-related policies on epidemic spread. <em>TKDD</em>,
<em>19</em>(1), 1–19. (<a
href="https://doi.org/10.1145/3697841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the causal impact of various government-related policies on the epidemic spread is of critical importance. This article aims to investigate the problem of assessing the causal effects of different COVID-19-related policies on the USA epidemic spread in different counties at any given time period, while eliminating biased interference from unobserved confounders (e.g., the vigilance of residents). However, the infection outcome of each region is influenced not only by its own confounding factors but also by policy interventions implemented in neighboring regions. Furthermore, the government policy index may exhibit a time-delay influence on outbreak dynamics. To this end, we implement observational data about different COVID-19-related policies (treatment) and outbreak dynamics (outcome) across different U.S. counties over time and develop a causal framework that learns the representations of time-varying confounders to tackle the aforementioned issues. More specifically, we employ one recurrent structure to capture the accumulative effects stemming from the policy history and then utilize hypergraph neural network to model the interactions among spatial regions. Our experimental results demonstrate the effectiveness of the proposed framework in quantifying the causal impact of different policy types on epidemics. Compared with baseline methods, our assessment provides valuable insights for future policy-making endeavors.},
  archive      = {J_TKDD},
  doi          = {10.1145/3697841},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {1},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Assessing the spatial-temporal causal impact of COVID-19-related policies on epidemic spread},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative time series imputation by maintaining dependency
consistency. <em>TKDD</em>, <em>19</em>(1), 1–24. (<a
href="https://doi.org/10.1145/3698107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data imputation is crucial in the analysis of incomplete time series, such as forecasting and classification, which involves learning dependencies among the observed values to infer missing ones. As there are no ground truths for missing values, the challenge of time series imputation lies in preventing the model from overfitting to spurious correlations. In this article, we believe that ensuring dependency consistency between observed and imputed values in a sequence is paramount for data imputation. Based on this idea, we propose a model called IR 2 -Net , 1 which combines an incomplete representation mechanism (IRM) with an iterative reconstruction framework (IRF) to establish a closed-loop learning-validation imputation paradigm. Firstly, IRM facilitates the representation of dependencies in incomplete sequences while preserving their distributions and semantics, effectively preventing the model from capturing spurious correlations. Secondly, IRF enables the model to reconstruct identical complete sequences separately based on imputed and observed values, ensuring that the dependencies of imputed values remain consistent with those of the observed ones. We conduct experiments on four datasets and compare IR 2 -Net with seven state-of-the-art imputation models. The experiment results show that IR 2 -Net outperforms all the baselines by 4.1%–23.4% in terms of accuracy. Moreover, IRF and IRM are two general modules that can be easily integrated into two existing models, significantly enhancing their performance by 18.3%–42.0%.},
  archive      = {J_TKDD},
  doi          = {10.1145/3698107},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Iterative time series imputation by maintaining dependency consistency},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FGTL: Federated graph transfer learning for node
classification. <em>TKDD</em>, <em>19</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3699962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised multi-source domain transfer in federated scenario has become an emerging research direction, which can help unlabeled target domain to obtain the adapted model through source domains under privacy-preserving. However, when local data are graph, the difference of domains (or data heterogeneity) mainly originates from the difference in node attributes and sub-graph structures, leading to serious model drift, which is not considered by the existing related algorithms. Currently, there are two challenges in this scenario: (1) The node representations extracted directly through conventional GNNs lack inter-domain generalized and consistent information, making it difficult to apply existing federated learning algorithms. (2) The knowledge of source domains has quality differences, which may lead to negative transfer. To address these issues, we propose a novel two-phase Federated Graph Transfer Learning (FGTL) framework. In the generalization phase, FGTL utilizes local contrastive learning and global context embedding to force node representations to capture the inter-domain generalized and consistent information, lightly alleviating model drift. In the transfer phase, FGTL utilizes consensus knowledge to force the decision bound of classifier to adapt to the target client. In addition, FGTL+ exploits model grouping to make consensus knowledge generation more efficient, further enhancing the scalability of FGTL. Extensive experiments show that FGTL significantly outperforms state-of-the-art related methods, while FGTL+ further enhances privacy protection and reduces both communication and computation overhead.},
  archive      = {J_TKDD},
  doi          = {10.1145/3699962},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {FGTL: Federated graph transfer learning for node classification},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Promoting machine abilities of discovering and utilizing
knowledge in a unified zero-shot learning paradigm. <em>TKDD</em>,
<em>19</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3700444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge discovery and utilization are two essential cognitive processes that enable humans to understand the world and extract new insights from their surroundings. These processes have motivated machine learning studies, particularly zero-shot (ZS) learning, which seeks to identify unseen concepts through the use of side information. Previous ZS studies primarily focused on utilizing existing knowledge to infer unseen events, yet they overlook the crucial process of knowledge discovery and the integrated modeling of these knowledge-aware processes. In this study, we present a comprehensive ZS learning approach that explores and evaluates the machine’s abilities of discovering and utilizing knowledge. More specifically, to emulate human-like knowledge discovery and utilization processes, we propose a novel visual-aware ZS knowledge graph completion task for evaluation, incorporating a traditional ZS image classification task. Technically, we develop a unified ZS learning paradigm named Cognitive Learner (CoLa) to foster the two knowledge-aware abilities. Including a knowledge representation learning (KRL) module and a knowledge adaptation (KA) module, CoLa adapts well to the two specified tasks with the corresponding data. Extensive experiments on large-scale datasets demonstrate CoLa models’ outstanding performance over compared methods in the two ZS tasks, illustrating their superior ability of discovering and utilizing knowledge.},
  archive      = {J_TKDD},
  doi          = {10.1145/3700444},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Promoting machine abilities of discovering and utilizing knowledge in a unified zero-shot learning paradigm},
  volume       = {19},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on knowledge graph related research in smart city
domain. <em>TKDD</em>, <em>18</em>(9), 1–31. (<a
href="https://doi.org/10.1145/3672615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph employs the specific graph structure to store knowledge in the form of entities, relations, attributes, and so forth, which can effectively represent correlations among data and has been applied in many fields, including search engine optimization, intelligent question answering, and recommendation systems. In this article, we mainly focus on the research and application of the domain-specific knowledge graph in the field of the smart city, which has not been fully paid attention to. Currently, the major problem faced by the smart city lies in data mining and proper application. On the one hand, data are usually stored by government management departments, which creates challenges such as high data storing overhead and inefficient data usage. On the other hand, data cannot be coordinated and collaborated between different city management systems, because data silos exist. By constructing the corresponding knowledge graph, the data of urban traffic, services, and public resources are integrated to provide help for city builders and managers to make important decisions. Therefore, we will review the related literature on the knowledge graph existing in the smart city domain to expore reasearch scopes. Specifically, we will analyze and summarize knowledge graph construction research in the field of smart cities from four perspectives, i.e., smart city ontology, urban data processing, urban knowledge graph construction, and their application. Finally, the research limitations and prospects of the urban knowledge graph are provided.},
  archive      = {J_TKDD},
  doi          = {10.1145/3672615},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-31},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A survey on knowledge graph related research in smart city domain},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale k-clustering. <em>TKDD</em>, <em>18</em>(9),
1–23. (<a href="https://doi.org/10.1145/3674508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale learning algorithms are essential for modern data collections that may have billions of data points. Here, we study the design of parallel \(k\) -clustering algorithms, which include the \(k\) -median, \(k\) -medoids, and \(k\) -means clustering problems. We design efficient parallel algorithms for these problems and prove that they still compute constant-factor approximations to the optimal solution for stable clustering instances. In addition to our theoretic results, we present computational experiments that show that our \(k\) -median and \(k\) -means algorithms work well in practice—we are able to find better clusterings than state-of-the-art coreset constructions using samples of the same size.},
  archive      = {J_TKDD},
  doi          = {10.1145/3674508},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Large-scale K-clustering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobility prediction via rule-enhanced knowledge graph.
<em>TKDD</em>, <em>18</em>(9), 1–21. (<a
href="https://doi.org/10.1145/3677019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of location acquisition technologies, massive mobile trajectories have been collected and made available to us, which support a fantastic way of understanding and modeling individuals’ mobility. However, existing data-driven methods either fail to capture the long-range dependency or suffer from a high computational cost. To overcome these issues, we propose a knowledge-driven framework for mobility prediction, which leverages knowledge graphs (KG) to formulate the mobility prediction task into the KG completion problem through integrating the structured “knowledge” from the mobility data. However, most related mobility prediction works only focus on the structured information encoded in existing triples, which ignores the rich semantic information of relation paths composed of multiple relation triples. In this article, we apply a dedicated module to extract the supplementary semantic structure of paths in KG, which contributes to the interpretability and accuracy of our model. Specifically, the extracted rules are applied to capture the dependencies between relational facts. Moreover, by incorporating user information in the entity-relation space with the corresponding hyperplane, our method could capture diverse user mobility patterns and model the personal characteristics of users to improve the accuracy of mobility prediction. Extensive evaluations illustrate that our proposed model beats state-of-the-art mobility prediction algorithms, which verifies the superiority of utilizing logical rules and user hyperplanes. Our implementation code is available at https://github.com/tsinghua-fib-lab/RulekG-MobiPre.git},
  archive      = {J_TKDD},
  doi          = {10.1145/3677019},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Mobility prediction via rule-enhanced knowledge graph},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal parallel transformer based model for traffic
prediction. <em>TKDD</em>, <em>18</em>(9), 1–25. (<a
href="https://doi.org/10.1145/3679017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic forecasting problems involve jointly modeling the non-linear spatio-temporal dependencies at different scales. While graph neural network models have been effectively used to capture the non-linear spatial dependencies, capturing the dynamic spatial dependencies between the locations remains a major challenge. The errors in capturing such dependencies propagate in modeling the temporal dependencies between the locations, thereby severely affecting the performance of long-term predictions. While transformer-based mechanisms have been recently proposed for capturing the dynamic spatial dependencies, these methods are susceptible to fluctuations in data brought on by unforeseen events like traffic congestion and accidents. To mitigate these issues we propose an improvised spatio-temporal parallel transformer (STPT) based model for traffic prediction that uses multiple adjacency graphs passed through a pair of coupled graph transformer-convolution network units, operating in parallel, to generate more noise-resilient embeddings. We conduct extensive experiments on 4 real-world traffic datasets and compare the performance of STPT with several state-of-the-art baselines, in terms of measures like RMSE, MAE, and MAPE. We find that using STPT improves the performance by around \(10-34\%\) as compared to the baselines. We also investigate the applicability of the model on other spatio-temporal data in other domains. We use a Covid-19 dataset to predict the number of future occurrences in different regions from a given set of historical occurrences. The results demonstrate the superiority of our model for such datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3679017},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Spatio-temporal parallel transformer based model for traffic prediction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-GPS++: Enhancing graph meta-learning with contrastive
learning and self-training. <em>TKDD</em>, <em>18</em>(9), 1–30. (<a
href="https://doi.org/10.1145/3679018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node classification is an essential problem in graph learning. However, many models typically obtain unsatisfactory performance when applied to few-shot scenarios. Some studies have attempted to combine meta-learning with graph neural networks to solve few-shot node classification on graphs. Despite their promising performance, some limitations remain. First, they employ the node encoding mechanism of homophilic graphs to learn node embeddings, even in heterophilic graphs. Second, existing models based on meta-learning ignore the interference of randomness in the learning process. Third, they are trained using only limited labeled nodes within the specific task, without explicitly utilizing numerous unlabeled nodes. Finally, they treat almost all sampled tasks equally without customizing them for their uniqueness. To address these issues, we propose a novel framework for few-shot node classification called Meta-GPS \(++\) . Specifically, we first adopt an efficient method to learn discriminative node representations on homophilic and heterophilic graphs. Then, we leverage a prototype-based approach to initialize parameters and contrastive learning for regularizing the distribution of node embeddings. Moreover, we apply self-training to extract valuable information from unlabeled nodes. Additionally, we adopt S \({}^{2}\) (scaling and shifting) transformation to learn transferable knowledge from diverse tasks. The results on real-world datasets show the superiority of Meta-GPS \(++\) . Our code is available here .},
  archive      = {J_TKDD},
  doi          = {10.1145/3679018},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Meta-GPS++: Enhancing graph meta-learning with contrastive learning and self-training},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A segment augmentation and prediction consistency framework
for multi-label unknown intent detection. <em>TKDD</em>, <em>18</em>(9),
1–18. (<a href="https://doi.org/10.1145/3680286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label unknown intent detection is a challenging task where each utterance may contain not only multiple known but also unknown intents. To tackle this challenge, pioneers proposed to predict the intent number of the utterance first, then compare it with the results of known intent matching to decide whether the utterence contains unknown intent(s). Though they have made remarkable progress on this task, their methods still suffer from two important issues: (1) It is inadequate to extract multiple intents using only utterance encoding; (2) Optimizing two sub-tasks (intent number prediction and known intent matching) independently leads to inconsistent predictions. In this article, we propose to incorporate segment augmentation rather than only use utterance encoding to better detect multiple intents. We also design a prediction consistency module to bridge the gap between the two sub-tasks. Empirical results on MultiWOZ2.3 and MixSNIPS datasets show that our method achieves state-of-the-art performance and significantly improves the best baseline.},
  archive      = {J_TKDD},
  doi          = {10.1145/3680286},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A segment augmentation and prediction consistency framework for multi-label unknown intent detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VAE*: A novel variational autoencoder via revisiting
positive and negative samples for top-n recommendation. <em>TKDD</em>,
<em>18</em>(9), 1–24. (<a
href="https://doi.org/10.1145/3680552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the easy access, implicit feedback is often used for recommender systems. Compared with point-wise learning and pair-wise learning methods, list-wise rank learning methods have superior performance for top- \(N\) recommendation. Recent solutions, especially the list-wise methods, simply treat all interacted items of a user as equally important positives and annotate all no-interaction items of a user as negatives. For the list-wise approaches, we argue that this annotation scheme of implicit feedback is over-simplified due to the sparsity and missing fine-grained labels of the feedback data. To overcome this issue, we revisit the so-called positive and negative samples. First, considering the loss function of list-wise ranking, we analyze the impact of false positives and negatives theoretically. Second, based on the observation, we propose a self-adjusting credibility weight mechanism to re-weigh the positive samples and exploit the higher-order relation based on item–item matrix to sample the critical negative samples. In order to prevent the introduction of noise, we design a pruning strategy for critical negatives. Besides, to combine the reconstruction loss function for the positive samples and critical negative samples, we develop a simple yet effective VAEs framework with linear structure, which abandons the complex non-linear structure. Extensive experiments are conducted on six public real-world datasets. The results demonstrate that, our VAE* outperforms other VAE-based models by a large margin. Besides, we also verify the effect of denoising positives and exploring critical negatives by ablation study.},
  archive      = {J_TKDD},
  doi          = {10.1145/3680552},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {VAE*: A novel variational autoencoder via revisiting positive and negative samples for top-N recommendation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of co-clustering. <em>TKDD</em>, <em>18</em>(9),
1–28. (<a href="https://doi.org/10.1145/3681793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-clustering is to cluster samples and features simultaneously, which can also reveal the relationship between row clusters and column clusters. Therefore, lots of scientists have drawn much attention to conduct extensive research on it, and co-clustering is widely used in recommendation systems, gene analysis, medical data analysis, natural language processing, image analysis, and social network analysis. In this article, we survey the entire research aspect of co-clustering, especially the latest advances in co-clustering, and discover the current research challenges and future directions. First, due to different views from researchers on the definition of co-clustering, this article summarizes the definition of co-clustering and its extended definitions, as well as related issues, based on the perspectives of various scientists. Second, existing co-clustering techniques are approximately categorized into four classes: information-theory-based, graph-theory-based, matrix-factorization-based, and other theories-based. Third, co-clustering is applied in various aspects such as recommendation systems, medical data analysis, natural language processing, image analysis, and social network analysis. Furthermore, 10 popular co-clustering algorithms are empirically studied on 10 benchmark datasets with 4 metrics—accuracy, purity, block discriminant index, and running time, and their results are objectively reported. Finally, future work is provided to get insights into the research challenges of co-clustering.},
  archive      = {J_TKDD},
  doi          = {10.1145/3681793},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A survey of co-clustering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VITR: Augmenting vision transformers with relation-focused
learning for cross-modal information retrieval. <em>TKDD</em>,
<em>18</em>(9), 1–21. (<a
href="https://doi.org/10.1145/3686805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relations expressed in user queries are vital for cross-modal information retrieval. Relation-focused cross-modal retrieval aims to retrieve information that corresponds to these relations, enabling effective retrieval across different modalities. Pre-trained networks, such as Contrastive Language-Image Pre-training networks, have gained significant attention and acclaim for their exceptional performance in various cross-modal learning tasks. However, the Vision Transformer (ViT) used in these networks is limited in its ability to focus on image region relations. Specifically, ViT is trained to match images with relevant descriptions at the global level, without considering the alignment between image regions and descriptions. This article introduces VITR, a novel network that enhances ViT by extracting and reasoning about image region relations based on a local encoder. VITR is comprised of two key components. Firstly, it extends the capabilities of ViT-based cross-modal networks by enabling them to extract and reason with region relations present in images. Secondly, VITR incorporates a fusion module that combines the reasoned results with global knowledge to predict similarity scores between images and descriptions. The proposed VITR network was evaluated through experiments on the tasks of relation-focused cross-modal information retrieval. The results derived from the analysis of the Flickr30K, MS-COCO, RefCOCOg, and CLEVR datasets demonstrated that the proposed VITR network consistently outperforms state-of-the-art networks in image-to-text and text-to-image retrieval.},
  archive      = {J_TKDD},
  doi          = {10.1145/3686805},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {VITR: Augmenting vision transformers with relation-focused learning for cross-modal information retrieval},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural-symbolic methods for knowledge graph reasoning: A
survey. <em>TKDD</em>, <em>18</em>(9), 1–44. (<a
href="https://doi.org/10.1145/3686806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural symbolic knowledge graph (KG) reasoning offers a promising approach that combines the expressive power of symbolic reasoning with the learning capabilities inherent in neural networks. This survey provides a comprehensive overview of advancements, techniques, and challenges in the field of neural symbolic KG reasoning. The survey introduces the fundamental concepts of KGs and symbolic logic, followed by an exploration of three significant KG reasoning tasks: KG completion, complex query answering, and logical rule learning. For each task, we thoroughly discuss three distinct categories of methods: pure symbolic methods, pure neural approaches, and the integration of neural networks and symbolic reasoning methods known as neural-symbolic. We carefully analyze and compare the strengths and limitations of each category of methods to provide a comprehensive understanding. By synthesizing recent research contributions and identifying open research directions, this survey aims to equip researchers and practitioners with a comprehensive understanding of the state-of-the-art in neural symbolic KG reasoning, fostering future advancements in this interdisciplinary domain.},
  archive      = {J_TKDD},
  doi          = {10.1145/3686806},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-44},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Neural-symbolic methods for knowledge graph reasoning: A survey},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProcessGAN: Generating privacy-preserving time-aware process
data with conditional generative adversarial nets. <em>TKDD</em>,
<em>18</em>(9), 1–31. (<a
href="https://doi.org/10.1145/3687464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process data constructed from event logs provides valuable insights into procedural dynamics over time. The confidential information in process data, together with the data’s intricate nature, makes the datasets not sharable and challenging to collect. Consequently, research is limited using process data and analytics in the process mining domain. In this study, we introduced a synthetic process data generation task to address the limitation of sharable process data. We introduced a generative adversarial network, called ProcessGAN, to generate process data with activity sequences and corresponding timestamps. ProcessGAN consists of a transformer-based network as the generator, and a time-aware self-attention network as the discriminator. It can generate privacy-preserving process data from random noise. ProcessGAN considers the duration of the process and time intervals between activities to generate realistic activity sequences with timestamps. We evaluated ProcessGAN on five real-world datasets, two that are public and three collected in medical domains that are private. To evaluate the synthetic data, in addition to statistical metrics, we trained a supervised model to score the synthetic processes. We also used process mining to discover workflows for synthetic medical processes and had domain experts evaluate the clinical applicability of the synthetic workflows. ProcessGAN outperformed the existing generative models in generating complex processes with valid parallel pathways. The synthetic process data generated by ProcessGAN better represented the long-range dependencies between activities, a feature relevant to complicated medical and other processes. The timestamps generated by the ProcessGAN model showed similar distributions with the authentic timestamps. In addition, we trained a transformer-based network to generate synthetic contexts (e.g., patient demographics) that were associated with the synthetic processes. The synthetic contexts generated by our model outperformed the baseline models, with the distributions similar to the authentic contexts. We conclude that ProcessGAN can generate sharable synthetic process data indistinguishable from authentic data. Our source code is available in https://github.com/raaachli/ProcessGAN .},
  archive      = {J_TKDD},
  doi          = {10.1145/3687464},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-31},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {ProcessGAN: Generating privacy-preserving time-aware process data with conditional generative adversarial nets},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous network motif coding, counting, and profiling.
<em>TKDD</em>, <em>18</em>(9), 1–21. (<a
href="https://doi.org/10.1145/3687465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network motifs, as a fundamental higher-order structure in large-scale networks, have received significant attention over recent years. Particularly in heterogeneous networks, motifs offer a higher capacity to uncover diverse information compared to homogeneous networks. However, the structural complexity and heterogeneity pose challenges in coding, counting, and profiling heterogeneous motifs. This work addresses these challenges by first introducing a novel heterogeneous motif coding method, adaptable to homogeneous motifs as well. Building upon this coding framework, we then propose GIFT, a heterogeneous network motif counting algorithm. GIFT effectively leverages combined structures of heterogeneous motifs through three key procedures: neighborhood searching, motif combination, and redundant motif filtering. We apply GIFT to count three-order and four-order motifs across eight distinct heterogeneous networks. Subsequently, we profile these detected motifs using four classical motif-based indicators. Experimental results demonstrate that by appropriately selecting motifs tailored to specific networks, heterogeneous motifs emerge as significant features in characterizing the underlying network structure.},
  archive      = {J_TKDD},
  doi          = {10.1145/3687465},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Heterogeneous network motif coding, counting, and profiling},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPORT: A subgraph perspective on graph classification with
label noise. <em>TKDD</em>, <em>18</em>(9), 1–20. (<a
href="https://doi.org/10.1145/3687468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have achieved great success recently on graph classification tasks using supervised end-to-end training. Unfortunately, extensive noisy graph labels could exist in the real world because of the complicated processes of manual graph data annotations, which may significantly degrade the performance of GNNs. Therefore, we investigate the problem of graph classification with label noise, which is demanding because of the complex graph representation learning issue and serious memorization of noisy samples. In this work, we present a novel approach called S ubgra p h Set Netw or k with Sample Selection and Consis t ency Learning (SPORT) for this problem. To release the overfitting of GNNs, SPORT proposes to characterize each graph as a set of subgraphs generated by certain predefined stratagems, which can be viewed as samples from its underlying semantic distribution in graph space. Then we develop an equivariant network to encode the subgraph set with the consideration of the symmetry group. To further release the influences of noisy examples, we leverage the predictions of subgraphs to measure the likelihood of a sample being clean or noisy, followed by effective label updating. In addition, we propose a joint loss to advance the model generalizability by introducing consistency regularization. Comprehensive experiments on a wide range of graph classification datasets demonstrate the effectiveness of our SPORT. Specifically, SPORT outperforms the most competing baseline by up to 6.4%.},
  archive      = {J_TKDD},
  doi          = {10.1145/3687468},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {SPORT: A subgraph perspective on graph classification with label noise},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A meta-learning approach to mitigating the estimation bias
of q-learning. <em>TKDD</em>, <em>18</em>(9), 1–23. (<a
href="https://doi.org/10.1145/3688849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a longstanding problem that Q-learning suffers from the overestimation bias. This issue originates from the fact that Q-learning uses the expectation of maximum Q-value to approximate the maximum expected Q-value. A number of algorithms, such as Double Q-learning, were proposed to address this problem by reducing the estimation of maximum Q-value, but this may lead to an underestimation bias. Note that this underestimation bias may have a larger performance penalty than the overestimation bias. Different from previous algorithms, this article studies this issue from a fresh perspective, i.e., meta-learning view, which leads to our Meta-Debias Q-learning. The main idea is to extract the maximum expected Q-value with meta-learning over multiple tasks to remove the estimation bias of maximum Q-value and help the agent choose the optimal action more accurately. However, there are two challenges: (1) How to automatically select suitable training tasks? (2) How to positively transfer the meta-knowledge from selected tasks to remove the estimation bias of maximum Q-value? To address the two challenges mentioned above, we quantify the similarity between the training tasks and the test task. This similarity enables us to select appropriate “partial” training tasks and helps the agent extract the maximum expected Q-value to remove the estimation bias. Extensive experiment results show that our Meta-Debias Q-learning outperforms SOTA baselines drastically in three evaluation indicators, i.e., maximum Q-value, policy, and reward. More specifically, our Meta-Debias Q-learning only underestimates \(1.2*10^{-3}\) than the maximum expected Q-value in the multi-armed bandit environment and only differs \(5.04\%-5\%=0.04\%\) than the optimal policy in the two states MDP environment. In addition, we compare the uniform weight and our similarity weight. Experiment results reveal fundamental insights into why our proposed algorithm outperforms in the maximum Q-value, policy, and reward.},
  archive      = {J_TKDD},
  doi          = {10.1145/3688849},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A meta-learning approach to mitigating the estimation bias of Q-learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph representation learning enhanced semi-supervised
feature selection. <em>TKDD</em>, <em>18</em>(9), 1–20. (<a
href="https://doi.org/10.1145/3689428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a key step in machine learning by eliminating features that are not related to the modeling target to create reliable and interpretable models. By exploring the potential complex correlations among features of unlabeled data, recently introduced self-supervision-enhanced feature selection greatly reduces the reliance on the labeled samples. However, they are generally based on the autoencoder with sample-wise self-supervision, which can hardly exploit the relations among samples. To address this limitation, this article proposes graph representation learning enhanced semi-supervised feature selection (G-FS) which performs feature selection based on the discovery and exploitation of the non-Euclidean relations among features and samples by translating unlabeled “plain” tabular data into a bipartite graph. A self-supervised edge prediction task is designed to distill rich information on the graph into low-dimensional embeddings, which remove redundant features and noise. Guided by the condensed graph representation, we propose a batch attention feature weight generation mechanism that generates more robust weights according to batch-based selection patterns rather than individual samples. The results show that G-FS achieves significant performance edges in 14 datasets compared to twelve state-of-the-art baselines, including two recent self-supervised baselines. The source code is public available at https://github.com/Icannotnamemyselff/G-FS_Graph_enhacned_feature_selection .},
  archive      = {J_TKDD},
  doi          = {10.1145/3689428},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Graph representation learning enhanced semi-supervised feature selection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward cross-lingual social event detection with hybrid
knowledge distillation. <em>TKDD</em>, <em>18</em>(9), 1–36. (<a
href="https://doi.org/10.1145/3689948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently published graph neural networks (GNNs) show promising performance at social event detection tasks. However, most studies are oriented toward monolingual data in languages with abundant training samples. This has left the common lesser-spoken languages relatively unexplored. Thus, in this work, we present a GNN-based framework that integrates cross-lingual word embeddings into the process of graph knowledge distillation for detecting events in low-resource language data streams. To achieve this, a novel cross-lingual knowledge distillation framework, called CLKD, exploits prior knowledge learned from similar threads in English to make up for the paucity of annotated data. Specifically, to extract sufficient useful knowledge, we propose a hybrid distillation method that consists of both feature-wise and relation-wise information. To transfer both kinds of knowledge in an effective way, we add a cross-lingual module in the feature-wise distillation to eliminate the language gap and selectively choose beneficial relations in the relation-wise distillation to avoid distraction caused by teachers’ misjudgments. Our proposed CLKD framework also adopts different configurations to suit both offline and online situations. Experiments on real-world datasets show that the framework is highly effective at detection in languages where training samples are scarce.},
  archive      = {J_TKDD},
  doi          = {10.1145/3689948},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-36},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Toward cross-lingual social event detection with hybrid knowledge distillation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatial-temporal aggregated graph neural network for
docked bike-sharing demand forecasting. <em>TKDD</em>, <em>18</em>(9),
1–27. (<a href="https://doi.org/10.1145/3690388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the number of rented and returned bikes at each station is crucial for operators to proactively manage shared bike relocation. Although existing research has proposed spatial-temporal prediction models that significantly advance traffic prediction, these models often neglect the unique characteristics of shared bike systems (BSS). Spatially, the entire bike-sharing system (BSS) experiences peak activity during morning and evening rush hours, whereas, during other periods, activity is localized to local stations, with some recording no rides, highlighting the need to distinguish between global and local spatial information across different times. Temporally, the historical riding records for each station exhibit non-stationary patterns, necessitating the analysis of both global trends and local fluctuations. Existing Graph Neural Network (GNN) approaches to predicting shared bike demand primarily capture static spatial-temporal data and fail to account for the dynamic nature of bike flows. Moreover, these studies focus on global spatial-temporal information without considering local nuances, making it challenging to capture spatiotemporal dynamics in fluctuating BSS. To address these challenges, we introduce the Spatial-Temporal Aggregated Graph Neural Network (STAGNN). Our model first constructs a dynamic adjacent matrix to describe the evolving connections between stations, followed by local and global information layers to capture spatial-temporal information from large-scale shared bike networks accurately. Our methodology has been validated through experiments on four real-world datasets, comparing it against benchmark models to demonstrate superior prediction accuracy. Additionally, we conduct extended experiments on four datasets during the morning and evening rush hours, and the results also affirm the efficacy of the STAGNN in enhancing prediction performance.},
  archive      = {J_TKDD},
  doi          = {10.1145/3690388},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A spatial-temporal aggregated graph neural network for docked bike-sharing demand forecasting},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSTGCN: Inductive spatial temporal imputation using long
short-term dependencies. <em>TKDD</em>, <em>18</em>(9), 1–25. (<a
href="https://doi.org/10.1145/3690645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial temporal forecasting of urban sensors is essentially important for many urban systems, such as intelligent transportation and smart cities. However, due to the problem of hardware failure or network failure, there are some missing values or missing monitoring sensors that need to be interpolated. Recent research on deep learning has made substantial progress on imputation problem, especially temporal aspect (i.e., time series imputation), while little attention has been paid to spatial aspect (both dynamic and static) and long-term temporal dependencies. In this article, we proposed a spatial temporal imputation model, named Long Short-Term Graph Convolution Networks (LSTGCN), which includes gated temporal extraction (GTE) module, multi-head attention-based temporal capture (MHAT) module, long-term periodic temporal encoding (LPTE) module, and bidirectional spatial graph convolution (BSGC) module. The GTE adopts a gated mechanism to filter short-term temporal information, while the MHAT utilizes position encoding to enhance the difference of each timestamps, then use multi-head attention to capture short-term temporal dependency. The BSGC is adopted to handle with spatial relationships between sensor nodes. And we design a periodic encoding technique to process long-term temporal dependencies. The BSGC handles spatial relationships between sensor nodes, and a periodic encoding technique is used to process long-term temporal dependencies. Our experimental analysis includes completion and forecasting tasks, as well as transfer and ablation analyses. The results show that our proposed model outperforms state-of-the-art baselines on real-world datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3690645},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {LSTGCN: Inductive spatial temporal imputation using long short-term dependencies},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair single index model. <em>TKDD</em>, <em>18</em>(9),
1–33. (<a href="https://doi.org/10.1145/3690646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single index models (SIMs) have been widely used in various applications due to their simplicity and interpretability. However, despite the potential for SIMs to result in discriminatory outcomes based on sensitive attributes like gender, race, or ethnicity, the issue of fairness has not been thoroughly examined in recent studies on the topic. This article aims to address these fairness concerns by proposing methods for building fair SIMs. Specifically, based on the definition of equal opportunity, we first provide a fairness definition for SIM. Next, we develop a unified fair SIM model and propose an efficient method to solve the fair SIM. Theoretically, we also show that our output is consistent in fairness. Finally, we conduct comprehensive experimental studies over eleven benchmark datasets and demonstrate that our fair SIM outperforms the other eight baseline methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3690646},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-33},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Fair single index model},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient generation of hidden outliers for improved outlier
detection. <em>TKDD</em>, <em>18</em>(9), 1–21. (<a
href="https://doi.org/10.1145/3690827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier generation is a popular technique used to solve important outlier detection tasks. Generating outliers with realistic behavior is challenging. Popular existing methods tend to disregard the “multiple views” property of outliers in high-dimensional spaces. The only existing method accounting for this property falls short in efficiency and effectiveness. We propose Bisect , a new outlier generation method that creates realistic outliers mimicking said property. To do so, Bisect employs a novel proposition introduced in this article stating how to efficiently generate said realistic outliers. Our method has better guarantees and complexity than the current method for recreating “multiple views.” We use the synthetic outliers generated by Bisect to effectively enhance outlier detection in diverse datasets for multiple use cases. For instance, oversampling with Bisect reduced the error by up to 3 times when compared with the baselines.},
  archive      = {J_TKDD},
  doi          = {10.1145/3690827},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Efficient generation of hidden outliers for improved outlier detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FINEST: Stabilizing recommendations by rank-preserving
fine-tuning. <em>TKDD</em>, <em>18</em>(9), 1–22. (<a
href="https://doi.org/10.1145/3695256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern recommender systems may output considerably different recommendations due to small perturbations in the training data. Changes in the data from a single user will alter the recommendations as well as the recommendations of other users. In applications like healthcare, housing, and finance, this sensitivity can have adverse effects on user experience. We propose a method to stabilize a given recommender system against such perturbations. This is a challenging task due to (1) the lack of a “reference” rank list that can be used to anchor the outputs; and (2) the computational challenges in ensuring the stability of rank lists with respect to all possible perturbations of training data. Our method, FINEST , overcomes these challenges by obtaining reference rank lists from a given recommendation model and then fine-tuning the model under simulated perturbation scenarios with rank-preserving regularization on sampled items. Our experiments on real-world datasets demonstrate that FINEST can ensure that recommender models output stable recommendations under a wide range of different perturbations without compromising next-item prediction accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3695256},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {FINEST: Stabilizing recommendations by rank-preserving fine-tuning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel tree-based method for interpretable reinforcement
learning. <em>TKDD</em>, <em>18</em>(9), 1–22. (<a
href="https://doi.org/10.1145/3695464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) has garnered remarkable success across various domains, propelled by advancements in deep learning (DL) technologies. However, the opacity of DL presents significant challenges, limiting the application of DRL in critical systems. In response, decision tree (DT)-based methods, known for their transparent decision-making mechanisms, have shown promise in making interpretable policies for decision-making problems. Existing methods often employ differential DTs to model RL policies and discretize them to conventional DTs for higher interpretability. Yet, this method leads to discrepancies between the trained differential DTs and the discretized DTs. To address this issue, we introduce Generative Consistent Trees (GCTs), a novel solution that circumvents the information loss typically associated with the argmax operation in prior research. By implementing a reparameterization technique to approximate the categorical distribution, GCTs ensure the consistencies between trained GCTs and discretized counterparts. Moreover, we have developed an imitation learning-based framework for interpretable reinforcement learning. This framework is designed to train GCTs by efficiently mimicking expert policies. Our extensive experiments across multiple environments have validated the effectiveness of this approach, highlighting the potential of GCTs in enhancing the interpretability and applicability of DRL.},
  archive      = {J_TKDD},
  doi          = {10.1145/3695464},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A novel tree-based method for interpretable reinforcement learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label and evolvable dataset preparation for web-based
object detection. <em>TKDD</em>, <em>18</em>(9), 1–21. (<a
href="https://doi.org/10.1145/3695465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we focus on the emerging field of web-based object detection, which has gained considerable attention due to its ability to utilize large amounts of web data for training, thus eliminating the need for labor-intensive manual annotations. However, the noisy and ever-evolving nature of web data poses challenges in preparing high-quality datasets for web-based object detection. To address these challenges, we propose a fully automatic dataset preparation method in this article. Our proposed method incorporates a hierarchical clustering module that assigns multiple precise labels to each image. This module is based on our observation that web image data exhibits different distributions at varying granularities. Furthermore, an evolutionary relabeling module ensures the adaptability of both the prepared dataset and trained detection models to the ever-evolving web data. Extensive experiments demonstrate that our method outperforms other web-based methods, and achieves a comparable performance to those manually labeled benchmark datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3695465},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {9},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-label and evolvable dataset preparation for web-based object detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subspace-contrastive multi-view clustering. <em>TKDD</em>,
<em>18</em>(9), 1–35. (<a
href="https://doi.org/10.1145/3674839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most multi-view clustering methods based on shallow models are limited in sound nonlinear information perception capability, or fail to effectively exploit complementary information hidden in different views. To tackle these issues, we propose a novel Subspace-Contrastive Multi-View Clustering (SCMC) approach. Specifically, SCMC utilizes a set of view-specific auto-encoders to map the original multi-view data into compact features capturing its nonlinear structures. Considering the large semantic gap of data from different modalities, we project multiple heterogeneous features into a joint semantic space, namely the embedded compact features are passed through the self-expression layers to learn the subspace representations, respectively. In order to enhance the discriminability and efficiently excavate the complementarity of various subspace representations, we use the contrastive strategy to maximize the similarity between positive pairs while differentiate negative pairs. Thus, the graph regularization is employed to encode the local geometric structure within varying subspaces for optimizing the consistent affinity matrix. Furthermore, to endow the proposed SCMC with the ability of handling the multi-view out-of-samples, we develop a consistent sparse representation (CSR) learning mechanism over the in-samples. To demonstrate the effectiveness of the proposed model, we conduct a large number of comparative experiments on ten challenging datasets, and the experimental results show that SCMC outperforms existing shallow and deep multi-view clustering methods. In addition, the experimental results on out-of-samples illustrate the effectiveness of the proposed CSR.},
  archive      = {J_TKDD},
  doi          = {10.1145/3674839},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {9},
  pages        = {1-35},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Subspace-contrastive multi-view clustering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trustworthiness-driven graph convolutional networks for
signed network embedding. <em>TKDD</em>, <em>18</em>(9), 1–26. (<a
href="https://doi.org/10.1145/3685279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of representing nodes in a signed network as low-dimensional vectors, known as signed network embedding (SNE), has garnered considerable attention in recent years. While several SNE methods based on graph convolutional networks (GCNs) have been proposed for this problem, we point out that they significantly rely on the assumption that the decades-old balance theory always holds in the real-world. To address this limitation, we propose a novel GCN-based SNE approach, named as TrustSGCN, which corrects for incorrect embedding propagation in GCN by utilizing the trustworthiness on edge signs for high-order relationships inferred by the balance theory. The proposed approach consists of three modules: (M1) generation of each node’s extended ego-network; (M2) measurement of trustworthiness on edge signs; and (M3) trustworthiness-aware propagation of embeddings. Specifically, TrustSGCN leverages topological information to measure trustworthiness on edge sign for high-order relationships inferred by balance theory. It then considers structural properties inherent to an input network, such as the ratio of triads, to correct for incorrect embedding propagation. Furthermore, TrustSGCN learns the node embeddings by leveraging two well-known social theories, i.e., balance and status, to jointly preserve the edge sign and direction between nodes connected by existing edges in the embedding space. The experiments on six real-world signed network datasets demonstrate that TrustSGCN consistently outperforms six state-of-the-art GCN-based SNE methods. The code is available at https://github.com/kmj0792/TrustSGCN .},
  archive      = {J_TKDD},
  doi          = {10.1145/3685279},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {9},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Trustworthiness-driven graph convolutional networks for signed network embedding},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Triangle centrality. <em>TKDD</em>, <em>18</em>(9), 1–34.
(<a href="https://doi.org/10.1145/3685677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle centrality is introduced for finding important vertices in a graph based on the concentration of triangles surrounding each vertex. It has the distinct feature of allowing a vertex to be central if it is in many triangles or none at all. Given a simple, undirected graph \(G=(V,E)\) with \(n=|V|\) vertices and \(m=|E|\) edges, let \(\triangle(v)\) and \(\triangle(G)\) denote the respective triangle counts of \(v\) and \(G\) . Let \(N(v)\) be the neighborhood set of \(v\) . Respectively, \(N_{\triangle}(v)\) and \(N_{\triangle}[v]=\{v\}\cup N_{\triangle}(v)\) denote the set of neighbors that are in triangles with \(v\) and the closed set including \(v\) . Then the triangle centrality for a vertex \(v\) is \begin{align*}TC(v)=\frac{\frac{1}{3}\sum_{u\in N_{\triangle}[v]}\triangle(u)+\sum_{w\in\{N(v)\setminus N_{\triangle}(v)\}}\triangle(w)}{\triangle(G)}.\end{align*} We show experimentally that triangle centrality is broadly applicable to many different types of networks. Our empirical results demonstrate that 30% of the time triangle centrality identified central vertices that differed with those found by five well-known centrality measures, which suggests novelty without being overly specialized. It is also asymptotically faster to compute on sparse graphs than all but the most trivial of these other measures. We introduce optimal algorithms that compute triangle centrality in \(O(m\overline{\delta})\) time and \(O(m+n)\) space, where \(\overline{\delta}\leq O(\sqrt{m})\) is the average degeneracy introduced by Burkhardt, Faber, and Harris (2020). In practical applications, \(\overline{\delta}\) is much smaller than \(\sqrt{m}\) so triangle centrality can be computed in nearly linear time. On a Concurrent Read Exclusive Write (CREW) Parallel Random Access Machine (PRAM), we give a near work-optimal parallel algorithm that takes \(O(\log n)\) time using \(O(m\sqrt{m})\) CREW PRAM processors. In MapReduce, we show it takes four rounds using \(O(m\sqrt{m})\) communication bits and is therefore optimal. We also derive a linear algebraic formulation of triangle centrality which can be computed in \(O(m\overline{\delta})\) time on sparse graphs.},
  archive      = {J_TKDD},
  doi          = {10.1145/3685677},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {9},
  pages        = {1-34},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Triangle centrality},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Billiards sports analytics: Datasets and tasks.
<em>TKDD</em>, <em>18</em>(9), 1–27. (<a
href="https://doi.org/10.1145/3686804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, it becomes a common practice to capture some data of sports games with devices such as GPS sensors and cameras and then use the data to perform various analyses on sports games, including tactics discovery, similar game retrieval, performance study, and so forth. While this practice has been conducted to many sports such as basketball and soccer, it remains largely unexplored on the billiards sports, which is mainly due to the lack of publicly available datasets. Motivated by this, we collect a dataset of billiards sports, which includes the layouts (i.e., locations) of billiards balls after performing break shots, called break shot layouts, the traces of the balls as a result of strikes (in the form of trajectories), and detailed statistics and performance indicators. We then study and develop techniques for three tasks on the collected dataset, including (1) prediction and (2) generation on the layouts data, and (3) similar billiards layout retrieval on the layouts data, which can serve different users such as coaches, players and fans. We conduct extensive experiments on the collected dataset and the results show that our methods perform effectively and efficiently.},
  archive      = {J_TKDD},
  doi          = {10.1145/3686804},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {9},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Billiards sports analytics: Datasets and tasks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection as deep sequential generative learning.
<em>TKDD</em>, <em>18</em>(9), 1–21. (<a
href="https://doi.org/10.1145/3687485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection aims to identify the most pattern-discriminative feature subset. In prior literature, filter (e.g., backward elimination) and embedded (e.g., LASSO) methods have hyperparameters (e.g., top- k , score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly. To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences. Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses. Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences into embedding vectors associated with utility scores. (2) We leverage the trained feature subset utility evaluator as a gradient provider to guide the identification of the optimal feature subset embedding; (3) We decode the optimal feature subset embedding to autoregressively generate the best feature selection decision sequence with autostop. Extensive experimental results show this generative perspective is effective and generic, without large discrete search space and expert-specific hyperparameters. The code is available at http://tinyurl.com/FSDSGL .},
  archive      = {J_TKDD},
  doi          = {10.1145/3687485},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {9},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Feature selection as deep sequential generative learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting process duration drift using gamma mixture models
in a left-truncated and right-censored environment. <em>TKDD</em>,
<em>18</em>(8), 1–24. (<a
href="https://doi.org/10.1145/3669942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the realm of business context, process duration signifies time spent by customers between successive activities. This temporal perspective offers important insight to customer behavior, highlighting potential bottlenecks, and influencing business management decisions. The distribution of these process duration often changes over time due to factors such as seasonality, emerging legislation, changes to supply chains, and customer demand. Referred to as concept drift, these variations pose challenges for robust process modeling, understanding, and refinement. Subsequently, gamma mixture models are widely employed to model durations. These source data can, however, become left-truncated and right-censored within any specific observation window thereby necessitating a (well-known) modification to the likelihood function. The approach reported in this article leveraged this adapted likelihood across a series of observation windows, applying the likelihood ratio test to identify duration changes/concept drift. Due to its flexibility in modelling any duration distribution, the gamma mixture model was used with Nelder–Mead optimized likelihood for the left-truncated and right-censored data. The number of gamma components was determined by the Bayesian information criterion. The proposed framework underwent validation through simulated exponential samples, leading to recommendations for its practical application. Subsequently, we applied the methodology to three real-life event logs exhibiting diverse characteristics. Experimental results showcase the effectiveness of our approach in terms of data fitting, as compared to Kaplan–Meier curves, and in detecting instances of drift. This comprehensive validation underscores the practical utility and reliability of our framework for dynamic business scenarios.},
  archive      = {J_TKDD},
  doi          = {10.1145/3669942},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Detecting process duration drift using gamma mixture models in a left-truncated and right-censored environment},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-information-based reasoning over the knowledge
graph: A survey of methods and applications. <em>TKDD</em>,
<em>18</em>(8), 1–42. (<a
href="https://doi.org/10.1145/3671148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knowledge graph (KG) is an efficient form of knowledge organization and expression, providing prior knowledge support for various downstream tasks, and has received extensive attention in natural language processing. However, existing large-scale KGs have many hidden facts that need to be discovered. How to effectively use the structure information of KG is an important research direction of knowledge reasoning. Structure-Information-based reasoning over the KG is a technique used to find the missing facts by the structure information of KG. This survey summarizes the methods and applications of Structure-Information-based reasoning and hopes to be helpful to the research in this field. First, we introduced the definition of knowledge reasoning and the conceptual description of related tasks. Then, we reviewed the methods of Structure-Information-based reasoning. Specifically, we categorized them into four representative classes: PRA-based reasoning, Path-Embedding-based reasoning, RL-based reasoning, and GNN-based reasoning. We compared the motivations and details between practices in the same category. After that, we described the application of Structure-Information-based knowledge reasoning in the KG Completion, Question Answering System, Recommendation System, and other fields. Finally, we discussed the future research directions of Structure-Information-based reasoning.},
  archive      = {J_TKDD},
  doi          = {10.1145/3671148},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-42},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Structure-information-based reasoning over the knowledge graph: A survey of methods and applications},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MICCF: A mutual information constrained clustering framework
for learning clustering-oriented feature representations. <em>TKDD</em>,
<em>18</em>(8), 1–22. (<a
href="https://doi.org/10.1145/3672402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering is a crucial task in machine learning and data mining that focuses on acquiring feature representations conducive to clustering. Previous research relies on self-supervised representation learning for general feature representations, such features may not be optimally suited for downstream clustering tasks. In this article, we introduce MICCF, a framework designed to bridge this gap and enhance clustering performance. MICCF enhances feature representations by combining mutual information constraints at different levels and employs an auxiliary alignment mutual information module for learning clustering-oriented features. To be specific, we propose a dual mutual information constraints module, incorporating minimal mutual information constraints at the feature level and maximal mutual information constraints at the instance level. This reduction in feature redundancy encourages the neural network to extract more discriminative features, while maximization ensures more unbiased and robust representations. To obtain clustering-oriented representations, the auxiliary alignment mutual information module utilizes pseudo-labels to maximize mutual information through a multi-classifier network, aligning features with the clustering task. The main network and the auxiliary module work in synergy to jointly optimize feature representations that are well-suited for the clustering task. We validate the effectiveness of our method through extensive experiments on six benchmark datasets. The results indicate that our method performs well in most scenarios, particularly on fine-grained datasets, where our approach effectively distinguishes subtle differences between closely related categories. Notably, our approach achieved a remarkable accuracy of 96.4% on the ImageNet-10 dataset, surpassing other comparison methods. The code is available at https://github.com/Li-Hyn/MICCF.git.},
  archive      = {J_TKDD},
  doi          = {10.1145/3672402},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {MICCF: A mutual information constrained clustering framework for learning clustering-oriented feature representations},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning individual treatment effects under heterogeneous
interference in networks. <em>TKDD</em>, <em>18</em>(8), 1–21. (<a
href="https://doi.org/10.1145/3673761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating individual treatment effects in networked observational data is a crucial and increasingly recognized problem. One major challenge of this problem is violating the stable unit treatment value assumption (SUTVA), which posits that a unit’s outcome is independent of others’ treatment assignments. However, in network data, a unit’s outcome is influenced not only by its treatment (i.e., direct effect) but also by the treatments of others (i.e., spillover effect) since the presence of interference. Moreover, the interference from other units is always heterogeneous (e.g., friends with similar interests have a different influence than those with different interests). In this article, we focus on the problem of estimating individual treatment effects (including direct effect and spillover effect) under heterogeneous interference in networks. To address this problem, we propose a novel dual weighting regression (DWR) algorithm by simultaneously learning attention weights to capture the heterogeneous interference from neighbors and sample weights to eliminate the complex confounding bias in networks. We formulate the learning process as a bi-level optimization problem. Theoretically, we give a generalization error bound for the expected estimation error of the individual treatment effects. Extensive experiments on four benchmark datasets demonstrate that the proposed DWR algorithm outperforms the state-of-the-art methods in estimating individual treatment effects under heterogeneous network interference.},
  archive      = {J_TKDD},
  doi          = {10.1145/3673761},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learning individual treatment effects under heterogeneous interference in networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deconfounding user preference in recommendation systems
through implicit and explicit feedback. <em>TKDD</em>, <em>18</em>(8),
1–18. (<a href="https://doi.org/10.1145/3673762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are influenced by many confounding factors (i.e., confounders) which result in various biases (e.g., popularity biases) and inaccurate user preference. Existing approaches try to eliminate these biases by inference with causal graphs. However, they assume all confounding factors can be observed and no hidden confounders exist. We argue that many confounding factors (e.g., season) may not be observable from user–item interaction data, resulting inaccurate user preference. In this article, we propose a deconfounded recommender considering unobservable confounders. Specifically, we propose a new causal graph with explicit and implicit feedback, which can better model user preference. Then, we realize a deconfounded estimator by the front-door adjustment, which is able to eliminate the effect of unobserved confounders. Finally, we conduct a series of experiments on two real-world datasets, and the results show that our approach performs better than other counterparts in terms of recommendation accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3673762},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Deconfounding user preference in recommendation systems through implicit and explicit feedback},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards faster deep graph clustering via efficient graph
auto-encoder. <em>TKDD</em>, <em>18</em>(8), 1–23. (<a
href="https://doi.org/10.1145/3674983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep graph clustering (DGC) has been a promising method for clustering graph data in recent years. However, existing research primarily focuses on optimizing clustering outcomes by improving the quality of embedded representations, resulting in slow-speed complex models. Additionally, these methods do not consider changes in node similarity and corresponding adjustments in the original structure during the iterative optimization process after updating node embeddings, which easily falls into the representation collapse issue. We introduce an Efficient Graph Auto-Encoder (EGAE) and a dynamic graph weight updating strategy to address these issues, forming the basis for our proposed Fast DGC (FastDGC) network. Specifically, we significantly reduce feature dimensions using a linear transformation that preserves the original node similarity. We then employ a single-layer graph convolutional filtering approximation to replace multiple layers of graph convolutional neural network, reducing computational complexity and parameter count. During iteration, we calculate the similarity between nodes using the linearly transformed features and periodically update the original graph structure to reduce edges with low similarity, thereby enhancing the learning of discriminative and cohesive representations. Theoretical analysis confirms that EGAE has lower computational complexity. Extensive experiments on standard datasets demonstrate that our proposed method improves clustering performance and achieves a speedup of 2–3 orders of magnitude compared to state-of-the-art methods, showcasing outstanding performance. The code for our model is available at https://github.com/Marigoldwu/FastDGC . Furthermore, we have organized a portion of the DGC code into a unified framework, available at https://github.com/Marigoldwu/A-Unified-Framework-for-Deep-Attribute-Graph-Clustering .},
  archive      = {J_TKDD},
  doi          = {10.1145/3674983},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Towards faster deep graph clustering via efficient graph auto-encoder},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards domain-aware stable meta learning for
out-of-distribution generalization. <em>TKDD</em>, <em>18</em>(8), 1–24.
(<a href="https://doi.org/10.1145/3676558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models are often trained on datasets that are limited in size and distribution, which may not fully represent the entire range of data encountered in practice. Thus, making deep learning models generalize to out-of-distribution data has received a significant amount of attention in recent studies due to the critical importance of this ability in real-world applications. Meta learning as an effective knowledge transfer paradigm, which learns a base model with high generalization ability to adapt to new data distributions by minimizing domain shifts across tasks during meta-training. However, most existing meta learning methods assume that the base model can access the labels of different domains, and this assumption is demanding in many real application scenarios. In addition, these methods focus on narrowing data-level domain shifts, while ignoring task-level domain shifts, which may lead to inadequate or even negative transfer. Inspired by human learners who use induction to learn and master new tasks, we propose a novel domain-aware meta learning framework for out-of-distribution generalization, termed SMLG. This framework enables the base model to generalize effectively to unseen domains without relying on domain-specific labels. Specifically, we develop a domain-aware transformation module to obtain meta representation and pseudo domain labels. As a result, the base model can be trained robustly without the need for direct domain label input. Furthermore, to investigate the impact of domain shifts at different levels, we introduce a joint loss function that combines cross-entropy with a domain alignment constraint. Extensive experiments on benchmark datasets demonstrate the efficacy of our framework.},
  archive      = {J_TKDD},
  doi          = {10.1145/3676558},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Towards domain-aware stable meta learning for out-of-distribution generalization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neighbor-enhanced representation learning for link
prediction in dynamic heterogeneous attributed networks. <em>TKDD</em>,
<em>18</em>(8), 1–25. (<a
href="https://doi.org/10.1145/3676559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic link prediction aims to predict future connections among unconnected nodes in a network. It can be applied for friend recommendations, link completion, and other tasks. Network representation learning algorithms have demonstrated considerable effectiveness in various prediction tasks. However, most network representation learning algorithms are based on homogeneous networks and static networks for link prediction that do not consider rich semantic and dynamic information. Additionally, existing dynamic network representation learning methods neglect the neighborhood interaction structure of the node. In this work, we design a neighbor-enhanced dynamic heterogeneous attributed network embedding method (NeiDyHNE) for link prediction. In light of the impressive achievements of the heuristic methods, we learn the information of common neighbors and neighbors’ interaction in heterogeneous networks to preserve the neighbors proximity and common neighbors proximity. NeiDyHNE encodes the attributes and neighborhood structure of nodes as well as the evolutionary features of the dynamic network. More specifically, NeiDyHNE consists of the hierarchical structure attention module and the convolutional temporal attention module. The hierarchical structure attention module captures the rich features and semantic structure of nodes. The convolutional temporal attention module captures the evolutionary features of the network over time in dynamic heterogeneous networks. We evaluate our method and various baseline methods on the dynamic link prediction task. Experimental results demonstrate that our method is superior to baseline methods in terms of accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3676559},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Neighbor-enhanced representation learning for link prediction in dynamic heterogeneous attributed networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling sequences as star graphs to address over-smoothing
in self-attentive sequential recommendation. <em>TKDD</em>,
<em>18</em>(8), 1–24. (<a
href="https://doi.org/10.1145/3676560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention (SA) mechanisms have been widely used in developing sequential recommendation (SR) methods, and demonstrated state-of-the-art performance. However, in this article, we show that self-attentive SR methods substantially suffer from the over-smoothing issue that item embeddings within a sequence become increasingly similar across attention blocks. As widely demonstrated in the literature, this issue could lead to a loss of information in individual items, and significantly degrade models’ scalability and performance. To address the over-smoothing issue, in this article, we view items within a sequence constituting a star graph and develop a method, denoted as \(\mathop{\mathtt{MSSG}}\limits\) , for SR. Different from existing self-attentive methods, \(\mathop{\mathtt{MSSG}}\limits\) introduces an additional internal node to specifically capture the global information within the sequence, and does not require information propagation among items. This design fundamentally addresses the over-smoothing issue and enables \(\mathop{\mathtt{MSSG}}\limits\) a linear time complexity with respect to the sequence length. We compare \(\mathop{\mathtt{MSSG}}\limits\) with eleven state-of-the-art baseline methods on six public benchmark datasets. Our experimental results demonstrate that \(\mathop{\mathtt{MSSG}}\limits\) significantly outperforms the baseline methods, with an improvement of as much as 10.10%. Our analysis shows the superior scalability of \(\mathop{\mathtt{MSSG}}\limits\) over the state-of-the-art self-attentive methods. Our complexity analysis and runtime performance comparison together show that \(\mathop{\mathtt{MSSG}}\limits\) is both theoretically and practically more efficient than self-attentive methods. Our analysis of the attention weights learned in SA-based methods indicates that on sparse recommendation data, modeling dependencies in all item pairs using the SA mechanism yields limited information gain, and thus, might not benefit the recommendation performance. Our source code and data are publicly accessible through GitHub .},
  archive      = {J_TKDD},
  doi          = {10.1145/3676560},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Modeling sequences as star graphs to address over-smoothing in self-attentive sequential recommendation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair federated learning with multi-objective hyperparameter
optimization. <em>TKDD</em>, <em>18</em>(8), 1–13. (<a
href="https://doi.org/10.1145/3676968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an attractive paradigm for privacy-aware distributed machine learning, which enables clients to collaboratively learn a global model without sharing clients’ data. Recently, many strategies have been proposed to improve the generality of the global model and thus improve FL effect. However, existing strategies either ignore the fairness among clients or sacrifice performance for fairness. They cannot ensure that the gap among clients is as small as possible without sacrificing federated performance. To address this issue, we propose ParetoFed , a new local information aggregation method dedicated to obtaining better federated performance with smaller gap among clients. Specifically, we propose to use multi-objective hyperparameter optimization (HPO) algorithm to gain global models that are both fair and effective. Then, we send Pareto Optimal global models to each client, allowing them to choose the most suitable one as the base to optimize their local model. ParetoFed not only make the global models more fair but also make the selection of local models more personalized, which can further improve the federated performance. Extensive experiments show that ParetoFed outperforms existing FL methods in terms of fairness, and even achieves better federated performance, which demonstrates the significance of our method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3676968},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-13},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Fair federated learning with multi-objective hyperparameter optimization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AGENDA: Predicting trip purposes with a new graph embedding
network and active domain adaptation. <em>TKDD</em>, <em>18</em>(8),
1–25. (<a href="https://doi.org/10.1145/3677020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trip purpose is a meaningful aspect of travel behaviour for the understanding of urban mobility. However, it is non-trivial to automatically obtain trip purposes. On one hand, trip purposes are naturally diverse and complicated, but the available predictive data sources are limited in real-world scenarios. On the other hand, since trip purpose labeling is costly and the development levels of cities are unbalanced, it is infeasible to access large-scale labeled data in less developed cities to train advanced prediction models. To narrow the gaps, this article presents A new Graph Embedding Network and active Domain Adaptation based framework (AGENDA) that only requires open data sources and is capable of predicting in both label-rich cities and label-scarce cities. Specifically, in label-rich source cities, we first use the vehicle’s GPS trajectory and open POI check-ins to augment trip contexts. Then we establish a supervised graph embedding network with two attention mechanisms to extract the passenger’s latent activity semantics and a classifier to predict trip purpose. To enable the prediction in label-scarce target cities, we further devise an active domain adaptation framework, in which adversarial domain adaptation is used to transfer the source-learned knowledge, and active learning is used to integrate human intelligence in the model training. A group of experiments are conducted with real-world datasets in Beijing and Shanghai. Evaluation results demonstrate that the proposed framework significantly outperforms existing trip purpose prediction algorithms, and could make accurate trip purpose prediction in label-scarce cities with much fewer labeling efforts.},
  archive      = {J_TKDD},
  doi          = {10.1145/3677020},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {AGENDA: Predicting trip purposes with a new graph embedding network and active domain adaptation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Concept evolution detecting over feature streams.
<em>TKDD</em>, <em>18</em>(8), 1–32. (<a
href="https://doi.org/10.1145/3678012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosion of data volume has gradually transformed big data processing from the static batch mode to the online streaming model. Streaming data can be divided into instance streams (feature space remains fixed while instances increase over time), feature streams (instance space is fixed while features arrive over time), or both. Generally, online streaming data learning has two main challenges: infinite length and concept changing. Recently, feature stream learning has received much attention. However, existing feature stream learning methods focus on feature selection or classification but ignore the concept changing over time. To the best of our knowledge, this is the first work that studies concept evolution detection over feature streams. Specifically, we first give the formal definition of concept evolution over feature streams, which include three different types: concept emerging, concept drift, and concept forgetting. Then, we design a novel framework to detect the concept evolution over feature streams that consists of a sliding window, an improved density peak-based clustering algorithm, and a weighted bipartite graph-based concept detecting method. Extensive experiments have been conducted on several synthetic and high-dimensional datasets to indicate our new method’s ability to cluster and detect concept evolution over feature streams.},
  archive      = {J_TKDD},
  doi          = {10.1145/3678012},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {8},
  pages        = {1-32},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Concept evolution detecting over feature streams},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EML: Emotion-aware meta learning for cross-event false
information detection. <em>TKDD</em>, <em>18</em>(8), 1–25. (<a
href="https://doi.org/10.1145/3661485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern social media’s development has dramatically changed how people obtain information. However, the wide dissemination of various false information has severe detrimental effects. Accordingly, many deep learning-based methods have been proposed to detect false information and achieve promising results. However, these methods are unsuitable for new events due to the extremely limited labeled data and their discrepant data distribution to existing events. Domain adaptation methods have been proposed to mitigate these problems. However, their performance is suboptimal because they are not sensitive to new events due to they aim to align the domain information between existing events, and they hardly capture the fine-grained difference between real and fake claims by only using semantic information. Therefore, we propose a novel Emotion-aware Meta Learning (EML) approach for cross-event false information early detection, which deeply integrates emotions in meta learning to find event-sensitive initialization parameters that quickly adapt to new events. EML is non-trivial and faces three challenges: (1) How to effectively model semantic and emotional features to capture fine-grained differences? (2) How to reduce the impact of noise in meta learning based on semantic and emotional features? (3) How to detect the false information in a zero-shot detection scenario, i.e., no labeled data for new events? To tackle these challenges, firstly, we construct the emotion-aware meta tasks by selecting claims with similar and opposite emotions to the target claim other than usually used random sampling. Secondly, we propose a task weighting method and event-adaptation meta tasks to further improve the model’s robustness and generalization ability for detecting new events. Finally, we propose a weak label annotation method to extend EML to zero-shot detection according to the calculated labels’ confidence. Extensive experiments on real-world datasets show that the EML achieves superior performances on false information detection for new events.},
  archive      = {J_TKDD},
  doi          = {10.1145/3661485},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {EML: Emotion-aware meta learning for cross-event false information detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning with asynchronous labels. <em>TKDD</em>,
<em>18</em>(8), 1–27. (<a
href="https://doi.org/10.1145/3662186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with data streams has attracted much attention in recent decades. Conventional approaches typically assume that the feature and label of a data item can be timely observed at each round. In many real-world tasks, however, it often occurs that either the feature or the label is observed firstly while the other arrives with delay. For instance, in distributed learning systems, a central processor collects training data from different sub-processors to train a learning model, whereas the feature and label of certain data items can arrive asynchronously due to network latency. The problem of learning with asynchronous feature or label in streams encompasses many applications but still lacks sound solutions. In this article, we formulate the problem and propose a new approach to alleviate the negative effect of asynchronicity and mining asynchronous data streams. Our approach carefully exploits the timely arrived information and builds an online ensemble structure to adaptively reuse historical models and instances. We provide the theoretical guarantees of our approach and conduct extensive experiments to validate its effectiveness.},
  archive      = {J_TKDD},
  doi          = {10.1145/3662186},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learning with asynchronous labels},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variate associated domain adaptation for unsupervised
multivariate time series anomaly detection. <em>TKDD</em>,
<em>18</em>(8), 1–24. (<a
href="https://doi.org/10.1145/3663573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate Time Series Anomaly Detection (MTS-AD) is crucial for the effective management and maintenance of devices in complex systems, such as server clusters, spacecrafts, and financial systems, and so on. However, upgrade or cross-platform deployment of these devices will introduce the issue of cross-domain distribution shift, which leads to the prototypical problem of domain adaptation for MTS-AD. Compared with general domain adaptation problems, MTS-AD domain adaptation presents two peculiar challenges: (1) the dimensions of data from the source domain and the target domain are usually different, so alignment without losing any information is necessary; and (2) the association between different variates plays a vital role in the MTS-AD task, which is overlooked by traditional domain adaptation approaches. Aiming at addressing the above issues, we propose a Variate Associated Domain Adaptation Method Combined with a Graph Deviation Network (VANDA) for MTS-AD, which includes two major contributions. First, we characterize the intra-domain variate associations of the source domain by a graph deviation network (GDN), which can share parameters across domains without dimension alignment. Second, we propose a sliding similarity to measure the inter-domain variate associations and perform joint training by minimizing the optimal transport distance between source and target data for transferring variate associations across domains. VANDA achieves domain adaptation by transferring both variate associations and GDN parameters from the source domain to the target domain. We construct two pairs of MTS-AD datasets from existing MTS-AD data and combine three domain adaptation strategies with six MTS-AD backbones as the benchmark methods for experimental evaluation and comparison. Extensive experiments demonstrate the effectiveness of our approach, which outperforms the benchmark methods, and significantly improves the AD performance of the target domain by effectively utilizing the source domain knowledge.},
  archive      = {J_TKDD},
  doi          = {10.1145/3663573},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Variate associated domain adaptation for unsupervised multivariate time series anomaly detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving graph collaborative filtering with directional
behavior enhanced contrastive learning. <em>TKDD</em>, <em>18</em>(8),
1–20. (<a href="https://doi.org/10.1145/3663574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Collaborative Filtering is a widely adopted approach for recommendation, which captures similar behavior features through Graph Neural Network (GNN). Recently, Contrastive Learning (CL) has been demonstrated as an effective method to enhance the performance of graph collaborative filtering. Typically, CL-based methods first perturb users’ history behavior data (e.g., drop clicked items), then construct a self-discriminating task for behavior representations under different random perturbations. However, for widely existing inactive users, random perturbation makes their sparse behavior information more incomplete, thereby harming the behavior feature extraction. To tackle the above issue, we design a novel directional perturbation-based CL method to improve the graph collaborative filtering performance. The idea is to perturb node representations through directionally enhancing behavior features. To do so, we propose a simple yet effective feedback mechanism, which fuses the representations of nodes based on behavior similarity. Then, to avoid irrelevant behavior preferences introduced by the feedback mechanism, we construct a behavior self-contrast task before and after feedback, to align the node representations between the final output and the first layer of GNN. Different from the widely adopted self-discriminating task, the behavior self-contrast task avoids complex message propagation on different perturbed graphs, which is more efficient than previous methods. Extensive experiments on three public datasets demonstrate that the proposed method has distinct advantages over other CL methods on recommendation accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3663574},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Improving graph collaborative filtering with directional behavior enhanced contrastive learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imbalance-robust multi-label self-adjusting kNN.
<em>TKDD</em>, <em>18</em>(8), 1–30. (<a
href="https://doi.org/10.1145/3663575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of multi-label classification in data streams, instances arriving in real-time need to be associated with multiple labels simultaneously. Various methods based on the k Nearest Neighbors algorithm have been proposed to address this task. However, these methods face limitations when dealing with imbalanced data streams, a problem that has received limited attention in existing works. To approach this gap, this article introduces the Imbalance-Robust Multi-Label Self-Adjusting kNN (IRMLSAkNN), designed to tackle multi-label imbalanced data streams. IRMLSAkNN’s strength relies on maintaining relevant instances with imbalance labels by using a discarding mechanism that considers the imbalance ratio per label. On the other hand, it evaluates subwindows with an imbalance-aware measure to discard older instances that are lacking performance. We conducted statistical experiments on 32 benchmark data streams, evaluating IRMLSAkNN against eight multi-label classification algorithms using common accuracy-aware and imbalance-aware measures. The obtained results demonstrate that IRMLSAkNN consistently outperforms these algorithms in terms of predictive capacity and time cost across various levels of imbalance.},
  archive      = {J_TKDD},
  doi          = {10.1145/3663575},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Imbalance-robust multi-label self-adjusting kNN},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FastHGNN: A new sampling technique for learning with
hypergraph neural networks. <em>TKDD</em>, <em>18</em>(8), 1–26. (<a
href="https://doi.org/10.1145/3663670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraphs can represent higher-order relations among objects. Traditional hypergraph neural networks involve node-edge-node transform, leading to high computational cost and timing. The main aim of this article is to propose a new sampling technique for learning with hypergraph neural networks. The core idea is to design a layer-wise sampling scheme for nodes and hyperedges to approximate the original hypergraph convolution. We rewrite hypergraph convolution in the form of double integral and leverage Monte Carlo to achieve a discrete and consistent estimator. In addition, we use importance sampling and finally derive feasible probability mass functions for both nodes and hyperedges in consideration of variance reduction, based on some assumptions. Notably, the proposed sampling technique allows us to handle large-scale hypergraph learning, which is not feasible with traditional hypergraph neural networks. Experiment results demonstrate that our proposed model keeps a good balance between running time and prediction accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3663670},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {FastHGNN: A new sampling technique for learning with hypergraph neural networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data completion-guided unified graph learning for incomplete
multi-view clustering. <em>TKDD</em>, <em>18</em>(8), 1–23. (<a
href="https://doi.org/10.1145/3664290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its heterogeneous property, multi-view data has been widely concerned over single-view data for performance improvement. Unfortunately, some instances may be with partially available information because of some uncontrollable factors, for which the incomplete multi-view clustering (IMVC) problem is raised. IMVC aims to partition unlabeled incomplete multi-view data into their clusters by exploiting the heterogeneity of multi-view data and overcoming the difficulty of data loss. However, most existing IMVC methods like BSV, MIC, OMVC, and IVC tend to conduct basic completion processing on the input data, without taking advantage of the correlation between samples and information redundancy. To overcome the above issue, we propose one novel IMVC method named data completion-guided unified graph learning (DCUGL), which could complete the data of missing views and fuse multiple learned view-specific similarity matrices into one unified graph. Specifically, we first reduce the dimension of the input data to learn multiple view-specific similarity matrices. By stacking all view-specific similarity matrices, DCUGL constructs a third-order tensor with the low-rank constraint, such that sample correlation within and between views can be well explored. Finally, by dividing the original data into observed data and unobserved data, DCUGL can infer and complete the missing data according to the view-specific similarity matrices, and obtain a unified graph, which can be directly used for clustering. To solve the proposed model, we design an iterative algorithm, which is based on the alternating direction method of multipliers framework. The proposed model proves to be superior by benchmarking on six challenging datasets compared with state-of-the-art IMVC methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3664290},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Data completion-guided unified graph learning for incomplete multi-view clustering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobile user traffic generation via multi-scale hierarchical
GAN. <em>TKDD</em>, <em>18</em>(8), 1–19. (<a
href="https://doi.org/10.1145/3664655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile user traffic facilitates diverse applications, including network planning and optimization, whereas large-scale mobile user traffic is hardly available due to privacy concerns. One alternative solution is to generate mobile user traffic data for downstream applications. However, existing generation models cannot simulate the multi-scale temporal dynamics in mobile user traffic on individual and aggregate levels. In this work, we propose a multi-scale hierarchical generative adversarial network (MSH-GAN) containing multiple generators and a multi-class discriminator. Specifically, the mobile traffic usage behavior exhibits a mixture of multiple behavior patterns, which are called micro-scale behavior patterns and are modeled by different pattern generators in our model. Moreover, the traffic usage behavior of different users exhibits strong clustering characteristics, with the co-existence of users with similar and different traffic usage behaviors. Thus, we model each cluster of users as a class in the discriminator’s output, referred to as macro-scale user clusters. Then, the gap between micro-scale behavior patterns and macro-scale user clusters is bridged by introducing the switch mode generators, which describe the traffic usage behavior in switching between different patterns. All users share the pattern generators. In contrast, the switch mode generators are only shared by a specific cluster of users, which models the multi-scale hierarchical structure of the traffic usage behavior of massive users. Finally, we urge MSH-GAN to learn the multi-scale temporal dynamics via a combined loss function, including adversarial loss, clustering loss, aggregated loss, and regularity terms. Extensive experiment results demonstrate that MSH-GAN outperforms state-of-art baselines by at least 118.17% in critical data fidelity and usability metrics. Moreover, observations show that MSH-GAN can simulate traffic patterns and pattern switch behaviors.},
  archive      = {J_TKDD},
  doi          = {10.1145/3664655},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Mobile user traffic generation via multi-scale hierarchical GAN},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On mean-optimal robust linear discriminant analysis.
<em>TKDD</em>, <em>18</em>(8), 1–27. (<a
href="https://doi.org/10.1145/3665500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) is widely used for dimensionality reduction under supervised learning settings. Traditional LDA objective aims to minimize the ratio of the squared Euclidean distances that may not perform optimally on noisy datasets. Multiple robust LDA objectives have been proposed to address this problem, but their implementations have two major limitations. One is that their mean calculations use the squared \(\ell_{2}\) -norm distance to center the data, which is not valid when the objective depends on other distance functions. The second problem is that there is no generalized optimization algorithm to solve different robust LDA objectives. In addition, most existing algorithms can only guarantee that the solution is locally optimal rather than globally optimal. In this article, we review multiple robust loss functions and propose a new and generalized robust objective for LDA. Besides, to remove the mean value within data better, our objective uses an optimal way to center the data through learning. As one important algorithmic contribution, we derive an efficient iterative algorithm to optimize the resulting non-smooth and non-convex objective function. We theoretically prove that our solution algorithm guarantees that both the objective and the solution sequences converge to globally optimal solutions at a sub-linear convergence rate. The results of comprehensive experimental evaluations demonstrate the effectiveness of our new method, achieving significant improvements compared to the other competing methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3665500},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {On mean-optimal robust linear discriminant analysis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection in dynamic graphs: A comprehensive survey.
<em>TKDD</em>, <em>18</em>(8), 1–44. (<a
href="https://doi.org/10.1145/3669906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey article presents a comprehensive and conceptual overview of anomaly detection (AD) using dynamic graphs. We focus on existing graph-based AD techniques and their applications to dynamic networks. The contributions of this survey article include the following: (i) a comparative study of existing surveys on AD; (ii) a Dynamic Graph-based anomaly detection (DGAD) review framework in which approaches for detecting anomalies in dynamic graphs are grouped based on traditional machine learning models, matrix transformations, probabilistic approaches, and deep learning approaches; (iii) a discussion of graphically representing both discrete and dynamic networks; and (iv) a discussion of the advantages of graph-based techniques for capturing the relational structure and complex interactions in dynamic graph data. Finally, this work identifies the potential challenges and future directions for detecting anomalies in dynamic networks. This DGAD survey approach aims to provide a valuable resource for researchers and practitioners by summarizing the strengths and limitations of each approach, highlighting current research trends, and identifying open challenges. In doing so, it can guide future research efforts and promote advancements in AD in dynamic graphs.},
  archive      = {J_TKDD},
  doi          = {10.1145/3669906},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-44},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Anomaly detection in dynamic graphs: A comprehensive survey},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utility-oriented reranking with counterfactual context.
<em>TKDD</em>, <em>18</em>(8), 1–22. (<a
href="https://doi.org/10.1145/3671004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a critical task for large-scale commercial recommender systems, reranking rearranges items in the initial ranking lists from the previous ranking stage to better meet users’ demands. Foundational work in reranking has shown the potential of improving recommendation results by uncovering mutual influence among items. However, rather than considering the context of initial lists as most existing methods do, an ideal reranking algorithm should consider the counterfactual context— the position and the alignment of the items in the reranked lists . In this work, we propose a novel pairwise reranking framework, Utility-oriented Reranking with Counterfactual Context (URCC), which maximizes the overall utility after reranking efficiently. Specifically, we first design a utility-oriented evaluator, which applies Bi-LSTM and graph attention mechanism to estimate the listwise utility via the counterfactual context modeling. Then, under the guidance of the evaluator, we propose a pairwise reranker model to find the most suitable position for each item by swapping misplaced item pairs. Extensive experiments on two benchmark datasets and a proprietary real-world dataset demonstrate that URCC significantly outperforms the state-of-the-art models in terms of both relevance-based metrics and utility-based metrics.},
  archive      = {J_TKDD},
  doi          = {10.1145/3671004},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Utility-oriented reranking with counterfactual context},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A compact vulnerability knowledge graph for risk assessment.
<em>TKDD</em>, <em>18</em>(8), 1–17. (<a
href="https://doi.org/10.1145/3671005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software vulnerabilities, also known as flaws, bugs or weaknesses, are common in modern information systems, putting critical data of organizations and individuals at cyber risk. Due to the scarcity of resources, initial risk assessment is becoming a necessary step to prioritize vulnerabilities and make better decisions on remediation, mitigation, and patching. Datasets containing historical vulnerability information are crucial digital assets to enable AI-based risk assessments. However, existing datasets focus on collecting information on individual vulnerabilities while simply storing them in relational databases, disregarding their structural connections. This article constructs a compact vulnerability knowledge graph, VulKG, containing over 276 K nodes and 1 M relationships to represent the connections between vulnerabilities, exploits, affected products, vendors, referred domain names, and more. We provide a detailed analysis of VulKG modeling and construction, demonstrating VulKG-based query and reasoning, and providing a use case of applying VulKG to a vulnerability risk assessment task, i.e., co-exploitation behavior discovery. Experimental results demonstrate the value of graph connections in vulnerability risk assessment tasks. VulKG offers exciting opportunities for more novel and significant research in areas related to vulnerability risk assessment. The data and codes of this article are available at https://github.com/happyResearcher/VulKG.git .},
  archive      = {J_TKDD},
  doi          = {10.1145/3671005},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-17},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A compact vulnerability knowledge graph for risk assessment},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attribute diversity aware community detection on attributed
graphs using three-view graph attention neural networks. <em>TKDD</em>,
<em>18</em>(8), 1–24. (<a
href="https://doi.org/10.1145/3672081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is a fundamental yet important task for characterizing and understanding the structure of attributed graphs. Existing methods mainly focus on the structural tightness and attribute similarity among nodes in a community. However, grouping numerous semantically homogeneous nodes will result in information cocoons and thus reduce the robustness of community structure and the efficiency of node collaboration in real-world applications, such as recommendation systems and collaboration networks. Since nodes with closer connections tend to be more similar, finding communities with dense structures and diverse attributes poses great challenges to mining latent relationships between the graph structure and attribute distribution. To our best knowledge, very little research has been conducted to address this challenge. In this article, we propose a novel three-view graph attention neural networks (TvGANN) model to formally address the attribute diversity aware community detection problem. TvGANN reveals correlations between the graph structure and attributes distribution from the perspective of node organization, attribute co-occurrence, and the node-attribute interaction. It effectively captures structural features and attributes distribution by feeding a structural network and an attribute co-occurrence network into graph attention modules through the encoder–decoder framework. It also learns heterogeneous information by feeding a network into a meta-node attention module. Then, it fuzes the three modules and clusters the embedding representations through a Student&#39;s t -distribution approach, which iteratively refines the clustering results. The experiments show that our method not only improves the quality in dense community detection but also performs efficiently for attributed graphs.},
  archive      = {J_TKDD},
  doi          = {10.1145/3672081},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Attribute diversity aware community detection on attributed graphs using three-view graph attention neural networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous meta-path graph learning for higher-order
social recommendation. <em>TKDD</em>, <em>18</em>(8), 1–25. (<a
href="https://doi.org/10.1145/3673658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems have become an indispensable part of daily life. Social recommendation systems, which utilize social relationships and past behaviors to infer users’ preferences, have gained popularity in recent years. Exploring the inherent characteristics implied by higher-order relationships offers a new approach to social recommendation. However, it is challenging due to sparse social networks, influence heterogeneity, and noisy feedback. In this article, we propose a Heterogeneous Meta-path Graph Learning model for Higher-order Social Recommendation (HEAL). Within HEAL, we introduce a heterogeneous graph in social recommendation and utilize a meta-path-guided random walk to generate higher-order relationships. By encoding higher-order structures and semantics along different meta-graphs, HEAL can mitigate the limitation of data sparsity. Moreover, HEAL exploits aspect-aware and semantic-aware attentions to adaptively propagate and aggregate useful features from different meta-neighbors and higher-order relations. These attention-based aggregation layers allow HEAL to suppress the heterogeneity of social influences. Furthermore, HEAL adopts contrastive learning as a supplemental task to the recommendation task by maximizing the consistency between the self-discriminating objectives. This auxiliary task enables the model to learn more differentiated representations, further reducing its sensitivity to noisy feedback. We evaluate the performance of HEAL through extensive experiments on public datasets. The results demonstrate that leveraging higher-order relations can enhance the quality of social recommendations by better capturing the complexity and diversity of users’ preferences and interactions.},
  archive      = {J_TKDD},
  doi          = {10.1145/3673658},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Heterogeneous meta-path graph learning for higher-order social recommendation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structural properties on scale-free tree network with an
ultra-large diameter. <em>TKDD</em>, <em>18</em>(8), 1–26. (<a
href="https://doi.org/10.1145/3674146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scale-free networks are prevalently observed in a great variety of complex systems, which triggers various researches relevant to networked models of such type. In this work, we propose a family of growth tree networks \(\mathcal{T}_{t}\) , which turn out to be scale-free, in an iterative manner. As opposed to most of published tree models with scale-free feature, our tree networks have the power-law exponent \(\gamma=1{ + }\ln 5/\ln 2\) that is obviously larger than \(3\) . At the same time, “small-world” property can not be found particularly because models \(\mathcal{T}_{t}\) have an ultra-large diameter \(D_{t}\) (i.e., \(D_{t}\sim|\mathcal{T}_{t}|^{\ln 3/\ln 5}\) ) and a greater average shortest path length \(\langle\mathcal{W}_{t}\rangle\) (namely, \(\langle\mathcal{W}_{t}\rangle\sim|\mathcal{T}_{t}|^{\ln 3/\ln 5}\) ) where \(|\mathcal{T}_{t}|\) represents vertex number. Next, we determine Pearson correlation coefficient and verify that networks \(\mathcal{T}_{t}\) display disassortative mixing structure. In addition, we study random walks on tree networks \(\mathcal{T}_{t}\) and derive exact solution to mean hitting time \(\langle\mathcal{H}_{t}\rangle\) . The results suggest that the analytic formula for quantity \(\langle\mathcal{H}_{t}\rangle\) as a function of vertex number \(|\mathcal{T}_{t}|\) shows a power-law form, i.e., \(\langle\mathcal{H}_{t}\rangle\sim|\mathcal{T}_{t}|^{1+\ln 3/\ln 5}\) . Accordingly, we execute extensive experimental simulations, and demonstrate that empirical analysis is in strong agreement with theoretical results. Lastly, we provide a guide to extend the proposed iterative manner in order to generate more general scale-free tree networks with large diameter.},
  archive      = {J_TKDD},
  doi          = {10.1145/3674146},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Structural properties on scale-free tree network with an ultra-large diameter},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum nearest neighbor collaborative filtering algorithm
for recommendation system. <em>TKDD</em>, <em>18</em>(8), 1–28. (<a
href="https://doi.org/10.1145/3674982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation has become especially crucial during the COVID-19 pandemic as a significant number of people rely on online shopping from home. Existing recommendation algorithms, designed to address issues like cold start and data sparsity, often overlook the time constraints of users. Specifically, users expect to receive recommendations for products of interest in the shortest possible time. To address this challenge, we propose a novel collaborative filtering recommendation algorithm that leverages the advantages of quantum computing circuits based on data reconstruction. This approach allows for the rapid identification of users similar to the target user, thereby improving recommendation speed. In our method, we utilize the information of known users to linearly reconstruct that of the target users, forming a relational matrix. Subsequently, we employ \(l_{2,1}-\) norm and \(l_{1}-\) norm to sparsely constrain the relationship matrix, deducing the weight of each known user. The final step involves providing similar recommendations to target users based on these weights. Furthermore, we implement the proposed algorithm using a quantum circuit, enabling exponential acceleration. The final weight matrix is derived from the quantum state outputted by the circuit. The speed of this process is theoretically demonstrated in detail. Experimental results indicate that our algorithm outperforms state-of-the-art methods in terms of root mean squared error (RMSE), mean absolute error (MAE) and normalized discounted cumulative gain (NDCG). Compared to state-of-the-art comparison algorithms, the proposed algorithm achieves the fastest recommendation speed across eight public datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3674982},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {8},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Quantum nearest neighbor collaborative filtering algorithm for recommendation system},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ID-SR: Privacy-preserving social recommendation based on
infinite divisibility for trustworthy AI. <em>TKDD</em>, <em>18</em>(7),
1–25. (<a href="https://doi.org/10.1145/3639412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems powered by artificial intelligence (AI) are widely used to improve user experience. However, AI inevitably raises privacy leakage and other security issues due to the utilization of extensive user data. Addressing these challenges can protect users’ personal information, benefit service providers, and foster service ecosystems. Presently, numerous techniques based on differential privacy have been proposed to solve this problem. However, existing solutions encounter issues such as inadequate data utilization and a tenuous trade-off between privacy protection and recommendation effectiveness. To enhance recommendation accuracy and protect users’ private data, we propose ID-SR, a novel privacy-preserving social recommendation scheme for trustworthy AI based on the infinite divisibility of Laplace distribution. We first introduce a novel recommendation method adopted in ID-SR, which is established based on matrix factorization with a newly designed social regularization term for improving recommendation effectiveness. We then propose a differential privacy-preserving scheme tailored to the above method that leverages the Laplace distribution’s characteristics to safeguard user data. Theoretical analysis and experimentation evaluation on two publicly available datasets demonstrate that our scheme achieves a superior balance between privacy protection and recommendation effectiveness, ultimately delivering an enhanced user experience.},
  archive      = {J_TKDD},
  doi          = {10.1145/3639412},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {ID-SR: Privacy-preserving social recommendation based on infinite divisibility for trustworthy AI},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed graph contrastive network for semi-supervised node
classification. <em>TKDD</em>, <em>18</em>(7), 1–19. (<a
href="https://doi.org/10.1145/3641549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved promising performance in semi-supervised node classification in recent years. However, the problem of insufficient supervision, together with representation collapse, largely limits the performance of the GNNs in this field. To alleviate the collapse of node representations in semi-supervised scenario, we propose a novel graph contrastive learning method, termed M ixed G raph C ontrastive N etwork (MGCN). In our method, we improve the discriminative capability of the latent embeddings by an interpolation-based augmentation strategy and a correlation reduction mechanism. Specifically, we first conduct the interpolation-based augmentation in the latent space and then force the prediction model to change linearly between samples. Second, we enable the learned network to tell apart samples across two interpolation-perturbed views through forcing the correlation matrix across views to approximate an identity matrix. By combining the two settings, we extract rich supervision information from both the abundant unlabeled nodes and the rare yet valuable labeled nodes for discriminative representation learning. Extensive experimental results on six datasets demonstrate the effectiveness and the generality of MGCN compared to the existing state-of-the-art methods. The code of MGCN is available at https://github.com/xihongyang1999/MGCN on Github.},
  archive      = {J_TKDD},
  doi          = {10.1145/3641549},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Mixed graph contrastive network for semi-supervised node classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Voxel-wise medical image generalization for eliminating
distribution shift. <em>TKDD</em>, <em>18</em>(7), 1–16. (<a
href="https://doi.org/10.1145/3643034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the medical field is witnessing an increase in the use of machine learning techniques. Supervised learning methods adopted in classification, prediction, and segmentation tasks for medical images always experience decreased performance when the training and testing datasets do not follow the independent and identically distributed assumption. These distribution shift situations seriously influence machine learning applications’ robustness, fairness, and trustworthiness in the medical domain. Hence, in this article, we adopt the CycleGAN (generative adversarial network) method to cycle train the computed tomography data from different scanners/manufacturers. It aims to eliminate the distribution shift from diverse data terminals based on our previous work [ 14 ]. However, due to the model collapse problem and generative mechanisms of the GAN-based model, the images we generated contained serious artifacts. To remove the boundary marks and artifacts, we adopt score-based diffusion generative models to refine the images voxel-wisely. This innovative combination of two generative models enhances the quality of data providers while maintaining significant features. Meanwhile, we use five paired patients’ medical images to deal with the evaluation experiments with structural similarity index measure metrics and the segmentation model’s performance comparison. We conclude that CycleGAN can be utilized as an efficient data augmentation technique rather than a distribution-shift-eliminating method. In contrast, the denoising diffusion the denoising diffusion model is more suitable for dealing with the distribution shift problem aroused by the different terminal modules. The limitation of generative methods applied in medical images is the difficulty in obtaining large and diverse datasets that accurately capture the complexity of biological structure and variability. In our following research, we plan to assess the initial and generated datasets to explore more possibilities to overcome the above limitation. We will also incorporate the generative methods into the federated learning architecture, which can maintain their advantages and resolve the distribution shift issue on a larger scale.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643034},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-16},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Voxel-wise medical image generalization for eliminating distribution shift},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair feature selection: A causal perspective. <em>TKDD</em>,
<em>18</em>(7), 1–23. (<a
href="https://doi.org/10.1145/3643890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fair feature selection for classification decision tasks has recently garnered significant attention from researchers. However, existing fair feature selection algorithms fall short of providing a full explanation of the causal relationship between features and sensitive attributes, potentially impacting the accuracy of fair feature identification. To address this issue, we propose a fair causal feature selection algorithm, called FairCFS . Specifically, FairCFS constructs a localized causal graph that identifies the Markov blankets of class and sensitive variables, to block the transmission of sensitive information for selecting fair causal features. Extensive experiments on seven public real-world datasets validate that FairCFS has accuracy comparable to eight state-of-the-art feature selection algorithms while presenting more superior fairness.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643890},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Fair feature selection: A causal perspective},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FairGAT: Fairness-aware graph attention networks.
<em>TKDD</em>, <em>18</em>(7), 1–20. (<a
href="https://doi.org/10.1145/3645096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs can facilitate modeling various complex systems such as gene networks and power grids as well as analyzing the underlying relations within them. Learning over graphs has recently attracted increasing attention, particularly graph neural network (GNN)–based solutions, among which graph attention networks (GATs) have become one of the most widely utilized neural network structures for graph-based tasks. Although it is shown that the use of graph structures in learning results in the amplification of algorithmic bias, the influence of the attention design in GATs on algorithmic bias has not been investigated. Motivated by this, the present study first carries out a theoretical analysis in order to demonstrate the sources of algorithmic bias in GAT-based learning for node classification. Then, a novel algorithm, FairGAT, which leverages a fairness-aware attention design, is developed based on the theoretical findings. Experimental results on real-world networks demonstrate that FairGAT improves group fairness measures while also providing comparable utility to the fairness-aware baselines for node classification and link prediction.},
  archive      = {J_TKDD},
  doi          = {10.1145/3645096},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {FairGAT: Fairness-aware graph attention networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-side adversarial learning based fair recommendation for
sensitive attribute filtering. <em>TKDD</em>, <em>18</em>(7), 1–20. (<a
href="https://doi.org/10.1145/3648683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of recommendation algorithms, researchers are paying increasing attention to fairness issues such as user discrimination in recommendations. To address these issues, existing works often filter users’ sensitive information that may cause discrimination during the process of learning user representations. However, these approaches overlook the latent relationship between items’ content attributes and users’ sensitive information. In this article, we propose DALFRec, a fairness-aware recommendation algorithm based on user-side and item-side adversarial learning to mitigate the effects of sensitive information on both sides of the recommendation process. First, we conduct a statistical analysis to demonstrate the latent relationship between items’ information and users’ sensitive attributes. Then, we design a dual-side adversarial learning network that simultaneously filters out users’ sensitive information on the user and item side. Additionally, we propose a new evaluation strategy that leverages the latent relationship between items’ content attributes and users’ sensitive attributes to better assess the algorithm’s ability to reduce discrimination. Our experiments on three real datasets demonstrate the superiority of our proposed algorithm over state-of-the-art methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3648683},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Dual-side adversarial learning based fair recommendation for sensitive attribute filtering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BapFL: You can backdoor personalized federated learning.
<em>TKDD</em>, <em>18</em>(7), 1–17. (<a
href="https://doi.org/10.1145/3649316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In federated learning (FL), malicious clients could manipulate the predictions of the trained model through backdoor attacks, posing a significant threat to the security of FL systems. Existing research primarily focuses on backdoor attacks and defenses within the generic federated learning scenario, where all clients collaborate to train a single global model. A recent study conducted by Qin et al. [ 24 ] marks the initial exploration of backdoor attacks within the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with parameter decoupling can significantly enhance robustness against backdoor attacks. However, in this article, we whistleblow that pFL methods with parameter decoupling are still vulnerable to backdoor attacks. The resistance of pFL methods with parameter decoupling is attributed to the heterogeneous classifiers between malicious clients and benign counterparts. We analyze two direct causes of the heterogeneous classifiers: (1) data heterogeneity inherently exists among clients and (2) poisoning by malicious clients further exacerbates the data heterogeneity. To address these issues, we propose a two-pronged attack method, BapFL, which comprises two simple yet effective strategies: (1) poisoning only the feature encoder while keeping the classifier fixed and (2) diversifying the classifier through noise introduction to simulate that of the benign clients. Extensive experiments on three benchmark datasets under varying conditions demonstrate the effectiveness of our proposed attack. Additionally, we evaluate the effectiveness of six widely used defense methods and find that BapFL still poses a significant threat even in the presence of the best defense, Multi-Krum. We hope to inspire further research on attack and defense strategies in pFL scenarios. The code is available at: https://github.com/BapFL/code},
  archive      = {J_TKDD},
  doi          = {10.1145/3649316},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-17},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {BapFL: You can backdoor personalized federated learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fully test-time training framework for semi-supervised
node classification on out-of-distribution graphs. <em>TKDD</em>,
<em>18</em>(7), 1–19. (<a
href="https://doi.org/10.1145/3649507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have shown great potential in representation learning for various graph tasks. However, the distribution shift between the training and test sets poses a challenge to the efficiency of GNNs. To address this challenge, HomoTTT proposes a fully test-time training framework for GNNs to enhance the model’s generalization capabilities for node classification tasks. Specifically, our proposed HomoTTT designs a homophily-based and parameter-free graph contrastive learning task with adaptive augmentation to guide the model’s adaptation during the test-time training, allowing the model to adapt for specific target data. In the inference stage, HomoTTT proposes to integrate the original GNN model and the adapted model after TTT using a homophily-based model selection method, which prevents potential performance degradation caused by unconstrained model adaptation. Extensive experimental results on six benchmark datasets demonstrate the effectiveness of our proposed framework. Additionally, the exploratory study further validates the rationality of the homophily-based graph contrastive learning task with adaptive augmentation and the homophily-based model selection designed in HomoTTT .},
  archive      = {J_TKDD},
  doi          = {10.1145/3649507},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A fully test-time training framework for semi-supervised node classification on out-of-distribution graphs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust rumor detection with graph contrastive and
curriculum learning. <em>TKDD</em>, <em>18</em>(7), 1–21. (<a
href="https://doi.org/10.1145/3653023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Establishing a robust rumor detection model is vital in safeguarding the veracity of information on social media platforms. However, existing approaches to stopping rumor from spreading rely on abundant and clean training data, which is rarely available in real-world scenarios. In this work, we aim to develop a trustworthy rumor detection model that can handle inadequate and noisy labeled data. Our work addresses robust rumor detection, including classic and early detection, as well as five types of robustness issues: noisy and incomplete propagation, label scarcity and noise, and user disappearance. We propose a novel method, Robustness-Enhanced Rumor Detection (RERD), which mainly leverages the information propagation graphs of source tweets, along with user profiles and retweeting knowledge, for model learning. The novelty of RERD is four-fold. First, we jointly exploit the propagation structures of non-text and text retweets to learn the representation of a source tweet. Second, we simultaneously utilize the top-down and bottom-up information flows with relational propagations for graph representation learning. Third, to have effective early and robust detection, we implement contrastive learning on graphs with early and complete views of information propagation so that small snapshots can foresee their future shapes. Last, we use curriculum pseudo-labeling to mitigate the impact of label scarcity and noisy labels, and to correct representations learned from corrupted data. Experimental results on three benchmark datasets demonstrate that RERD consistently outperforms competitors in classic, early, and robust rumor detection scenarios. To the best of our knowledge, we are the first to simultaneously cope with early and five robust detections of rumors.},
  archive      = {J_TKDD},
  doi          = {10.1145/3653023},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Towards robust rumor detection with graph contrastive and curriculum learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SA2E-AD: A stacked attention autoencoder for anomaly
detection in multivariate time series. <em>TKDD</em>, <em>18</em>(7),
1–15. (<a href="https://doi.org/10.1145/3653677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection for multivariate time series is an essential task in the modern industrial field. Although several methods have been developed for anomaly detection, they usually fail to effectively exploit the metrical-temporal correlation and the other dependencies among multiple variables. To address this problem, we propose a stacked attention autoencoder for anomaly detection in multivariate time series (SA2E-AD); it focuses on fully utilizing the metrical and temporal relationships among multivariate time series. We design a multiattention block, alternately containing the temporal attention and metrical attention components in a hierarchical structure to better reconstruct normal time series, which is helpful in distinguishing the anomalies from the normal time series. Meanwhile, a two-stage training strategy is designed to further separate the anomalies from the normal data. Experiments on three publicly available datasets show that SA2E-AD outperforms the advanced baseline methods in detection performance and demonstrate the effectiveness of each part of the process in our method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3653677},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-15},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {SA2E-AD: A stacked attention autoencoder for anomaly detection in multivariate time series},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatically inspecting thousands of static bug warnings
with large language model: How far are we? <em>TKDD</em>,
<em>18</em>(7), 1–34. (<a
href="https://doi.org/10.1145/3653718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static analysis tools for capturing bugs and vulnerabilities in software programs are widely employed in practice, as they have the unique advantages of high coverage and independence from the execution environment. However, existing tools for analyzing large codebases often produce a great deal of false warnings over genuine bug reports. As a result, developers are required to manually inspect and confirm each warning, a challenging, time-consuming, and automation-essential task. This article advocates a fast, general, and easily extensible approach called Llm4sa that automatically inspects a sheer volume of static warnings by harnessing (some of) the powers of Large Language Models (LLMs). Our key insight is that LLMs have advanced program understanding capabilities, enabling them to effectively act as human experts in conducting manual inspections on bug warnings with their relevant code snippets. In this spirit, we propose a static analysis to effectively extract the relevant code snippets via program dependence traversal guided by the bug warning reports themselves. Then, by formulating customized questions that are enriched with domain knowledge and representative cases to query LLMs, Llm4sa can remove a great deal of false warnings and facilitate bug discovery significantly. Our experiments demonstrate that Llm4sa is practical in automatically inspecting thousands of static warnings from Juliet benchmark programs and 11 real-world C/C++ projects, showcasing a high precision (81.13%) and a recall rate (94.64%) for a total of 9,547 bug warnings. Our research introduces new opportunities and methodologies for using the LLMs to reduce human labor costs, improve the precision of static analyzers, and ensure software trustworthiness},
  archive      = {J_TKDD},
  doi          = {10.1145/3653718},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-34},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Automatically inspecting thousands of static bug warnings with large language model: How far are we?},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing unsupervised outlier model selection: A study on
IREOS algorithms. <em>TKDD</em>, <em>18</em>(7), 1–25. (<a
href="https://doi.org/10.1145/3653719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection stands as a critical cornerstone in the field of data mining, with a wide range of applications spanning from fraud detection to network security. However, real-world scenarios often lack labeled data for training, necessitating unsupervised outlier detection methods. This study centers on Unsupervised Outlier Model Selection (UOMS), with a specific focus on the family of Internal, Relative Evaluation of Outlier Solutions (IREOS) algorithms. IREOS measures outlier candidate separability by evaluating multiple maximum-margin classifiers and, while effective, it is constrained by its high computational demands. We investigate the impact of several different separation methods in UOMS in terms of ranking quality and runtime. Surprisingly, our findings indicate that different separability measures have minimal impact on IREOS’ effectiveness. However, using linear separation methods within IREOS significantly reduces its computation time. These insights hold significance for real-world applications where efficient outlier detection is critical. In the context of this work, we provide the code for the IREOS algorithm and our separability techniques.},
  archive      = {J_TKDD},
  doi          = {10.1145/3653719},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Enhancing unsupervised outlier model selection: A study on IREOS algorithms},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Properties of fairness measures in the context of varying
class imbalance and protected group ratios. <em>TKDD</em>,
<em>18</em>(7), 1–18. (<a
href="https://doi.org/10.1145/3654659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, and hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this article, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this work to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.},
  archive      = {J_TKDD},
  doi          = {10.1145/3654659},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Properties of fairness measures in the context of varying class imbalance and protected group ratios},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attacking social media via behavior poisoning.
<em>TKDD</em>, <em>18</em>(7), 1–27. (<a
href="https://doi.org/10.1145/3654673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since social media such as Facebook and X (formerly known as Twitter) have permeated various aspects of daily life, people have strong incentives to influence information dissemination on these platforms and differentiate their content from the fierce competition. Existing dissemination strategies typically employ marketing techniques, such as seeking publicity through renowned actors or targeted advertising placements. Despite their various forms, most simply spread information to strengthen user impressions without conducting formal analyses of specific influence enhancement. And coupled with high costs, most fall short of expectations. To this end, we ingeniously formulate the task of social media dissemination as poisoning attacks, which influence specified content’s dissemination among target users by intervening in some users’ social media behaviors (including retweeting, following, and profile modifying). Correspondingly, we propose a novel poisoning attack, Influence-based Social Media Attack (ISMA) to generate discrete poisoning behaviors, which is difficult to achieve with existing attacks. In ISMA, we first contribute an efficient influence evaluator to quantify the spread influence of poisoning behaviors. Based on the estimated influence, we then present an imperceptible hierarchical selector and a profile modification method ProMix to select influential behaviors to poison. Notably, our attack is driven by custom attack objectives, which allows one to flexibly design different optimization goals to change the information flow, which could solve the blindness of existing influence maximization methods. Besides, behaviors such as retweeting are gentle and simple to implement. These properties make our attack more cost-effective and practical. Extensive experiments on two large-scale real-world datasets demonstrate the superiority of our method as it significantly outperforms baselines, and additionally, the proposed evaluator’s analysis of user influence provides new insights for influence maximization on social media.},
  archive      = {J_TKDD},
  doi          = {10.1145/3654673},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Attacking social media via behavior poisoning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TOMGPT: Reliable text-only training approach for
cost-effective multi-modal large language model. <em>TKDD</em>,
<em>18</em>(7), 1–19. (<a
href="https://doi.org/10.1145/3654674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal large language models (MLLMs), such as GPT-4, exhibit great comprehension capabilities on human instruction, as well as zero-shot ability on new downstream multi-modal tasks. To integrate the different modalities within a unified embedding space, previous MLLMs attempted to conduct visual instruction tuning with massive and high-quality image-text pair data, which requires substantial costs in data collection and training resources. In this article, we propose TOMGPT (Text-Only training Multi-modal GPT), a cost-effective MLLM tuned solely on easily accessible text data with much fewer resources. Along with pre-trained visual-linguistic coupled modality space (e.g., CLIP and ALIGN model), a text-only training strategy is devised to further project the aligned multi-modal latent space to that of LLM, endowing the LLM with visual comprehension capabilities in an efficient manner. Instead of enormous image-text training data required by previous MLLMs, we find that TOMGPT can be well-tuned with fewer yet diverse GPT-generated free-form text data, as we establish the semantic connection between LLM and pre-trained vision-language model. A quantitative evaluation is conducted on both MME and LVLM, which are recently released and extensively utilized MLLM benchmarks. The experiments reveal that TOMGPT achieved reliable performance compared to numerous models trained on a large amount of image-text pair data. Case studies are also presented, demonstrating TOMGPT’s broad understanding and dialogue capabilities across diverse image categories.},
  archive      = {J_TKDD},
  doi          = {10.1145/3654674},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {TOMGPT: Reliable text-only training approach for cost-effective multi-modal large language model},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward few-label vertical federated learning. <em>TKDD</em>,
<em>18</em>(7), 1–21. (<a
href="https://doi.org/10.1145/3656344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) provides a novel paradigm for privacy-preserving machine learning, enabling multiple clients to collaborate on model training without sharing private data. To handle multi-source heterogeneous data, Vertical Federated Learning (VFL) has been extensively investigated. However, in the context of VFL, the label information tends to be kept in one authoritative client and is very limited. This poses two challenges for model training in the VFL scenario. On the one hand, a small number of labels cannot guarantee to train a well VFL model with informative network parameters, resulting in unclear boundaries for classification decisions. On the other hand, the large amount of unlabeled data is dominant and should not be discounted, and it is worthwhile to focus on how to leverage them to improve representation modeling capabilities. To address the preceding two challenges, we first introduce supervised contrastive loss to enhance the intra-class aggregation and inter-class estrangement, which is to deeply explore label information and improve the effectiveness of downstream classification tasks. Then, for unlabeled data, we introduce a pseudo-label-guided consistency mechanism to induce the classification results coherent across clients, which allows the representations learned by local networks to absorb the knowledge from other clients, and alleviates the disagreement between different clients for classification tasks. We conduct sufficient experiments on four commonly used datasets, and the experimental results demonstrate that our method is superior to the state-of-the-art methods, especially in the low-label rate scenario, and the improvement becomes more significant.},
  archive      = {J_TKDD},
  doi          = {10.1145/3656344},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Toward few-label vertical federated learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing random forest-distances in the presence of missing
data. <em>TKDD</em>, <em>18</em>(7), 1–18. (<a
href="https://doi.org/10.1145/3656345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the problem of computing Random Forest-distances in the presence of missing data. We present a general framework which avoids pre-imputation and uses in an agnostic way the information contained in the input points. We centre our investigation on RatioRF, an RF-based distance recently introduced in the context of clustering and shown to outperform most known RF-based distance measures. We also show that the same framework can be applied to several other state-of-the-art RF-based measures and provide their extensions to the missing data case. We provide significant empirical evidence of the effectiveness of the proposed framework, showing extensive experiments with RatioRF on 15 datasets. Finally, we also positively compare our method with many alternative literature distances, which can be computed with missing values.},
  archive      = {J_TKDD},
  doi          = {10.1145/3656345},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Computing random forest-distances in the presence of missing data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FETILDA: Evaluation framework for effective representations
of long financial documents. <em>TKDD</em>, <em>18</em>(7), 1–27. (<a
href="https://doi.org/10.1145/3657299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the financial sphere, there is a wealth of accumulated unstructured financial data, such as the textual disclosure documents that companies submit on a regular basis to regulatory agencies, such as the Securities and Exchange Commission. These documents are typically very long and tend to contain valuable soft information about a company’s performance that is not present in quantitative predictors. It is therefore of great interest to learn predictive models from these long textual documents, especially for forecasting numerical key performance indicators. In recent years, there has been great progress in natural language processing via pre-trained language models (LMs) learned from large corpora of textual data. This prompts the important question of whether they can be used effectively to produce representations for long documents, as well as how we can evaluate the effectiveness of representations produced by various LMs. Our work focuses on answering this critical question, namely, the evaluation of the efficacy of various LMs in extracting useful soft information from long textual documents for prediction tasks. In this article, we propose and implement a deep learning evaluation framework that utilizes a sequential chunking approach combined with an attention mechanism. We perform an extensive set of experiments on a collection of 10-K reports submitted annually by U.S. banks, and another dataset of reports submitted by U.S. companies, to investigate thoroughly the performance of different types of language models. Overall, our framework using LMs outperforms strong baseline methods for textual modeling as well as for numerical regression. Our work provides better insights into how utilizing pre-trained domain-specific and fine-tuned long-input LMs for representing long documents can improve the quality of representation of textual data and, therefore, help in improving predictive analyses.},
  archive      = {J_TKDD},
  doi          = {10.1145/3657299},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {FETILDA: Evaluation framework for effective representations of long financial documents},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed pseudo-likelihood method for community detection
in large-scale networks. <em>TKDD</em>, <em>18</em>(7), 1–25. (<a
href="https://doi.org/10.1145/3657300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a distributed pseudo-likelihood method (DPL) to conveniently identify the community structure of large-scale networks. Specifically, we first propose a block-wise splitting method to divide large-scale network data into several subnetworks and distribute them among multiple workers. For simplicity, we assume the classical stochastic block model. Then, the DPL algorithm is iteratively implemented for the distributed optimization of the sum of the local pseudo-likelihood functions. At each iteration, the worker updates its local community labels and communicates with the master. The master then broadcasts the combined estimator to each worker for the new iterative steps. Based on the distributed system, DPL significantly reduces the computational complexity of the traditional pseudo-likelihood method using a single machine. Furthermore, to ensure statistical accuracy, we theoretically discuss the requirements of the worker sample size. Moreover, we extend the DPL method to estimate degree-corrected stochastic block models. The superior performance of the proposed distributed algorithm is demonstrated through extensive numerical studies and real data analysis.},
  archive      = {J_TKDD},
  doi          = {10.1145/3657300},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Distributed pseudo-likelihood method for community detection in large-scale networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of trustworthy representation learning across
domains. <em>TKDD</em>, <em>18</em>(7), 1–53. (<a
href="https://doi.org/10.1145/3657301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As AI systems have obtained significant performance to be deployed widely in our daily lives and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts of AI systems, and representation learning is the fundamental technology in machine learning. How to make representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.},
  archive      = {J_TKDD},
  doi          = {10.1145/3657301},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-53},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A survey of trustworthy representation learning across domains},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LMACL: Improving graph collaborative filtering with
learnable model augmentation contrastive learning. <em>TKDD</em>,
<em>18</em>(7), 1–24. (<a
href="https://doi.org/10.1145/3657302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph collaborative filtering (GCF) has achieved exciting recommendation performance with its ability to aggregate high-order graph structure information. Recently, contrastive learning (CL) has been incorporated into GCF to alleviate data sparsity and noise issues. However, most of the existing methods employ random or manual augmentation to produce contrastive views that may destroy the original topology and amplify the noisy effects. We argue that such augmentation is insufficient to produce the optimal contrastive view, leading to suboptimal recommendation results. In this article, we proposed a L earnable M odel A ugmentation C ontrastive L earning (LMACL) framework for recommendation, which effectively combines graph-level and node-level collaborative relations to enhance the expressiveness of collaborative filtering (CF) paradigm. Specifically, we first use the graph convolution network (GCN) as a backbone encoder to incorporate multi-hop neighbors into graph-level original node representations by leveraging the high-order connectivity in user-item interaction graphs. At the same time, we treat the multi-head graph attention network (GAT) as an augmentation view generator to adaptively generate high-quality node-level augmented views. Finally, joint learning endows the end-to-end training fashion. In this case, the mutual supervision and collaborative cooperation of GCN and GAT achieves learnable model augmentation. Extensive experiments on several benchmark datasets demonstrate that LMACL provides a significant improvement over the strongest baseline in terms of Recall and NDCG by 2.5%–3.8% and 1.6%–4.0%, respectively. Our model implementation code is available at https://github.com/LiuHsinx/LMACL .},
  archive      = {J_TKDD},
  doi          = {10.1145/3657302},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {LMACL: Improving graph collaborative filtering with learnable model augmentation contrastive learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Congestion-aware spatio-temporal graph convolutional
network-based a* search algorithm for fastest route search.
<em>TKDD</em>, <em>18</em>(7), 1–19. (<a
href="https://doi.org/10.1145/3657640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fastest route search, which is to find a path with the shortest travel time when the user initiates a query, has become one of the most important services in many map applications. To enhance the user experience of travel, it is necessary to achieve accurate and real-time route search. However, traffic conditions are changing dynamically, and the frequent occurrence of traffic congestion may greatly increase travel time. Thus, it is challenging to achieve the above goal. To deal with it, we present a congestion-aware spatio-temporal graph convolutional network-based A* search algorithm for the task of fastest route search. We first identify a sequence of consecutive congested traffic conditions as a traffic congestion event. Then, we propose a spatio-temporal graph convolutional network that jointly models the congestion events and changing travel time to capture their complex spatio-temporal correlations, which can predict the future travel-time information of each road segment as the basis of route planning. Further, we design a path-aided neural network to achieve effective origin-destination (OD) shortest travel-time estimation by encoding the complex relationships between OD pairs and their corresponding fastest paths. Finally, the cost function in the A* algorithm is set by fusing the output results of the two components, which is used to guide the route search. Our experimental results on the two real-world datasets show the superior performance of the proposed method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3657640},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {7},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Congestion-aware spatio-temporal graph convolutional network-based a* search algorithm for fastest route search},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do we really need imputation in AutoML predictive modeling?
<em>TKDD</em>, <em>18</em>(6), 1–64. (<a
href="https://doi.org/10.1145/3643643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous real-world data contain missing values, while in contrast, most Machine Learning (ML) algorithms assume complete datasets. For this reason, several imputation algorithms have been proposed to predict and fill in the missing values. Given the advances in predictive modeling algorithms tuned in an Automated Machine Learning context (AutoML) setting, a question that naturally arises is to what extent sophisticated imputation algorithms (e.g., Neural Network based) are really needed, or we can obtain a descent performance using simple methods like Mean/Mode (MM). In this article, we experimentally compare six state-of-the-art representatives of different imputation algorithmic families from an AutoML predictive modeling perspective, including a feature selection step and combined algorithm and hyper-parameter selection. We used a commercial AutoML tool for our experiments, in which we included the selected imputation methods. Experiments ran on 25 binary classification real-world incomplete datasets with missing values and 10 binary classification complete datasets in which synthetic missing values are introduced according to different missingness mechanisms, at varying missing frequencies. The main conclusion drawn from our experiments is that the best method on average is the Denoise AutoEncoder on real-world datasets and the MissForest in simulated datasets, followed closely by MM. In addition, binary indicator variables encoding missingness patterns actually improve predictive performance, on average. Last, although there are cases where Neural-Network-based imputation significantly improves predictive performance, this comes at a great computational cost and requires measuring all feature values to impute new samples.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643643},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-64},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Do we really need imputation in AutoML predictive modeling?},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On breaking truss-based and core-based communities.
<em>TKDD</em>, <em>18</em>(6), 1–43. (<a
href="https://doi.org/10.1145/3644077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the general problem of identifying a smallest edge subset of a given graph whose deletion makes the graph community-free. We consider this problem under two community notions that have attracted significant attention: k -truss and k -core. We also introduce a problem variant where the identified subset contains edges incident to a given set of nodes and ensures that these nodes are not contained in any community: k -truss or k -core, in our case. These problems are directly applicable in social networks: The identified edges can be hidden by users or sanitized from the output graph; or in communication networks: the identified edges correspond to vital network connections. We present a series of theoretical and practical results. On the theoretical side, we show through non-trivial reductions that the problems we introduce are NP-hard and, in fact, hard to approximate. For the k -truss-based problems, we also show exact exponential-time algorithms, as well as a non-trivial lower bound on the size of an optimal solution. On the practical side, we develop a series of heuristics that are sped up by efficient data structures that we propose for updating the truss or core decomposition under edge deletions. In addition, we develop an algorithm to compute the lower bound. Extensive experiments on 11 real-world and synthetic graphs show that our heuristics are effective, outperforming natural baselines, and also efficient (up to two orders of magnitude faster than a natural baseline), thanks to our data structures. Furthermore, we present a case study on a co-authorship network and experiments showing that the removal of edges identified by our heuristics does not substantially affect the clustering structure of the input graph. This work extends a KDD 2021 paper, providing new theoretical results as well as introducing core-based problems and algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3644077},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-43},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {On breaking truss-based and core-based communities},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Social behavior analysis in exclusive enterprise social
networks by FastHAND. <em>TKDD</em>, <em>18</em>(6), 1–32. (<a
href="https://doi.org/10.1145/3646552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an emerging trend in the Chinese automobile industries that automakers are introducing exclusive enterprise social networks (EESNs) to expand sales and provide after-sale services. The traditional online social networks (OSNs) and enterprise social networks (ESNs), such as X (formerly known as Twitter) and Yammer, are ingeniously designed to facilitate unregulated communications among equal individuals. However, users in EESNs are naturally social stratified, consisting of both enterprise staffs and customers. In addition, the motivation to operate EESNs can be quite complicated, including providing customer services and facilitating communication among enterprise staffs. As a result, the social behaviors in EESNs can be quite different from those in OSNs and ESNs. In this work, we aim to analyze the social behaviors in EESNs. We consider the Chinese car manufacturer NIO as a typical example of EESNs and provide the following contributions. First, we formulate the social behavior analysis in EESNs as a link prediction problem in heterogeneous social networks. Second, to analyze this link prediction problem, we derive plentiful user features and build multiple meta-path graphs for EESNs. Third, we develop a novel Fast (H)eterogeneous graph (A)ttention (N)etwork algorithm for (D)irected graphs (FastHAND) to predict directed social links among users in EESNs. This algorithm introduces feature group attention at the node-level and uses an edge sampling algorithm over directed meta-path graphs to reduce the computation cost. By conducting various experiments on the NIO community data, we demonstrate the predictive power of our proposed FastHAND method. The experimental results also verify our intuitions about social affinity propagation in EESNs.},
  archive      = {J_TKDD},
  doi          = {10.1145/3646552},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-32},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Social behavior analysis in exclusive enterprise social networks by FastHAND},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NSimplex zen: A novel dimensionality reduction for euclidean
and hilbert spaces. <em>TKDD</em>, <em>18</em>(6), 1–44. (<a
href="https://doi.org/10.1145/3647642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction techniques map values from a high dimensional space to one with a lower dimension. The result is a space which requires less physical memory and has a faster distance calculation. These techniques are widely used where required properties of the reduced-dimension space give an acceptable accuracy with respect to the original space. Many such transforms have been described. They have been classified in two main groups: linear and topological . Linear methods such as Principal Component Analysis (PCA) and Random Projection (RP) define matrix-based transforms into a lower dimension of Euclidean space. Topological methods such as Multidimensional Scaling (MDS) attempt to preserve higher-level aspects such as the nearest-neighbour relation, and some may be applied to non-Euclidean spaces. Here, we introduce nSimplex Zen , a novel topological method of reducing dimensionality. Like MDS, it relies only upon pairwise distances measured in the original space. The use of distances, rather than coordinates, allows the technique to be applied to both Euclidean and other Hilbert spaces, including those governed by Cosine, Jensen–Shannon and Quadratic Form distances. We show that in almost all cases, due to geometric properties of high-dimensional spaces, our new technique gives better properties than others, especially with reduction to very low dimensions.},
  archive      = {J_TKDD},
  doi          = {10.1145/3647642},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-44},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {NSimplex zen: A novel dimensionality reduction for euclidean and hilbert spaces},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intricate spatiotemporal dependency learning for temporal
knowledge graph reasoning. <em>TKDD</em>, <em>18</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3648366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph (KG) reasoning has been an interesting topic in recent decades. Most current researches focus on predicting the missing facts for incomplete KG. Nevertheless, Temporal KG (TKG) reasoning, which is to forecast future facts, still faces with a dilemma due to the complex interactions between entities over time. This article proposes a novel intricate Spatiotemporal Dependency learning Network (STDN) based on Graph Convolutional Network (GCN) to capture the underlying correlations of an entity at different timestamps. Specifically, we first learn an adaptive adjacency matrix to depict the direct dependencies from the temporally adjacent facts of an entity, obtaining its previous context embedding. Then, a Spatiotemporal feature Encoding GCN (STE-GCN) is proposed to capture the latent spatiotemporal dependencies of the entity, getting the spatiotemporal embedding. Finally, a time gate unit is used to integrate the previous context embedding and the spatiotemporal embedding at the current timestamp to update the entity evolutional embedding for predicting future facts. STDN could generate the more expressive embeddings for capturing the intricate spatiotemporal dependencies in TKG. Extensive experiments on WIKI, ICEWS14, and ICEWS18 datasets prove our STDN has the advantage over state-of-the-art baselines for the temporal reasoning task.},
  archive      = {J_TKDD},
  doi          = {10.1145/3648366},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Intricate spatiotemporal dependency learning for temporal knowledge graph reasoning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic environment responsive online meta-learning with
fairness awareness. <em>TKDD</em>, <em>18</em>(6), 1–23. (<a
href="https://doi.org/10.1145/3648684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner’s objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it’s crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this article, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model’s primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints. Our experimental evaluation of various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3648684},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Dynamic environment responsive online meta-learning with fairness awareness},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Citation forecasting with multi-context attention-aided
dependency modeling. <em>TKDD</em>, <em>18</em>(6), 1–23. (<a
href="https://doi.org/10.1145/3649140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting citations of scientific patents and publications is a crucial task for understanding the evolution and development of technological domains and for foresight into emerging technologies. By construing citations as a time series, the task can be cast into the domain of temporal point processes. Most existing work on forecasting with temporal point processes, both conventional and neural network-based, only performs single-step forecasting. In citation forecasting, however, the more salient goal is n -step forecasting: predicting the arrival of the next n citations. In this article, we propose Dynamic Multi-Context Attention Networks (DMA-Nets), a novel deep learning sequence-to-sequence (Seq2Seq) model with a novel hierarchical dynamic attention mechanism for long-term citation forecasting. Extensive experiments on two real-world datasets demonstrate that the proposed model learns better representations of conditional dependencies over historical sequences compared to state-of-the-art counterparts and thus achieves significant performance for citation predictions.},
  archive      = {J_TKDD},
  doi          = {10.1145/3649140},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Citation forecasting with multi-context attention-aided dependency modeling},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to generate temporal origin-destination flow
based-on urban regional features and traffic information. <em>TKDD</em>,
<em>18</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3649141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Origin-destination (OD) flow contains population mobility information between every two regions in the city, which is of great value in urban planning and transportation management. Nevertheless, the collection of OD flow data is extremely difficult due to the hindrance of privacy issues and collection costs. Significant efforts have been made to generate OD flow based on urban regional features, e.g., demographics, land use, and so on, since spatial heterogeneity of urban function is the primary cause that drives people to move from one place to another. On the other hand, people travel through various routes between OD, which will have effects on urban traffic, e.g., road travel speed and time. These effects of OD flows reveal the fine-grained spatiotemporal patterns of population mobility. Few works have explored the effectiveness of incorporating urban traffic information into OD generation. To bridge this gap, we propose to generate real-world daily temporal OD flows enhanced by urban traffic information in this paper. Our model consists of two modules: Urban2OD and OD2Traffic . In the Urban2OD module, we devise a spatiotemporal graph neural network to model the complex dependencies between daily temporal OD flows and regional features. In the OD2Traffic module, we introduce an attention-based neural network to predict urban traffic based on OD flow from the Urban2OD module. Then, by utilizing gradient backpropagation, these two modules are able to enhance each other to generate high-quality OD flow data. Extensive experiments conducted on real-world datasets demonstrate the superiority of our proposed model over the state of the art.},
  archive      = {J_TKDD},
  doi          = {10.1145/3649141},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learning to generate temporal origin-destination flow based-on urban regional features and traffic information},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness-aware graph neural networks: A survey.
<em>TKDD</em>, <em>18</em>(6), 1–23. (<a
href="https://doi.org/10.1145/3649142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. We categorize these techniques by whether they focus on improving fairness in the pre-processing, in-processing (during training), or post-processing phases. We discuss how such techniques can be used together whenever appropriate and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics, including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fairness metrics. In addition, graph datasets that are useful for benchmarking the fairness of GNN models are summarized succinctly. Finally, we highlight key open problems and challenges that remain to be addressed.},
  archive      = {J_TKDD},
  doi          = {10.1145/3649142},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Fairness-aware graph neural networks: A survey},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProtoMGAE: Prototype-aware masked graph auto-encoder for
graph representation learning. <em>TKDD</em>, <em>18</em>(6), 1–22. (<a
href="https://doi.org/10.1145/3649143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph self-supervised representation learning has gained considerable attention and demonstrated remarkable efficacy in extracting meaningful representations from graphs, particularly in the absence of labeled data. Two representative methods in this domain are graph auto-encoding and graph contrastive learning. However, the former methods primarily focus on global structures, potentially overlooking some fine-grained information during reconstruction. The latter methods emphasize node similarity across correlated views in the embedding space, potentially neglecting the inherent global graph information in the original input space. Moreover, handling incomplete graphs in real-world scenarios, where original features are unavailable for certain nodes, poses challenges for both types of methods. To alleviate these limitations, we integrate masked graph auto-encoding and prototype-aware graph contrastive learning into a unified model to learn node representations in graphs. In our method, we begin by masking a portion of node features and utilize a specific decoding strategy to reconstruct the masked information. This process facilitates the recovery of graphs from a global or macro level and enables handling incomplete graphs easily. Moreover, we treat the masked graph and the original one as a pair of contrasting views, enforcing the alignment and uniformity between their corresponding node representations at a local or micro level. Last, to capture cluster structures from a meso level and learn more discriminative representations, we introduce a prototype-aware clustering consistency loss that is jointly optimized with the preceding two complementary objectives. Extensive experiments conducted on several datasets demonstrate that the proposed method achieves significantly better or competitive performance on downstream tasks, especially for graph clustering, compared with the state-of-the-art methods, showcasing its superiority in enhancing graph representation learning.},
  archive      = {J_TKDD},
  doi          = {10.1145/3649143},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {ProtoMGAE: Prototype-aware masked graph auto-encoder for graph representation learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DP-GCN: Node classification by connectivity and local
topology structure on real-world network. <em>TKDD</em>, <em>18</em>(6),
1–20. (<a href="https://doi.org/10.1145/3649460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node classification is to predict the class label of a node by analyzing its properties and interactions in a network. We note that many existing solutions for graph-based node classification only consider node connectivity but not the node’s local topology structure. However, nodes residing in different parts of a real-world network may share similar local topology structures. For example, local topology structures in a payment network may reveal sellers’ business roles (e.g., supplier or retailer). To model both connectivity and local topology structure for better node classification performance, we present DP-GCN, a dual-path graph convolution network. DP-GCN consists of three main modules: (i) a C-GCN module to capture the connectivity relationships between nodes, (ii) a T-GCN module to capture the topology structure similarity among nodes, and (iii) a multi-head self-attention module to align both properties. We evaluate DP-GCN on seven benchmark datasets against diverse baselines to demonstrate its effectiveness. We also provide a case study of running DP-GCN on three large-scale payment networks from PayPal, a leading payment service provider, for risky seller detection. Experimental results show DP-GCN’s effectiveness and practicability in large-scale settings. PayPal’s internal testing also shows DP-GCN’s effectiveness in defending against real risks from transaction networks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3649460},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DP-GCN: Node classification by connectivity and local topology structure on real-world network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoMENt: Marked point processes with memory-enhanced neural
networks for user activity modeling. <em>TKDD</em>, <em>18</em>(6),
1–32. (<a href="https://doi.org/10.1145/3649504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marked temporal point process models (MTPPs) aim to model event sequences and event markers (associated features) in continuous time. These models have been applied to various application domains where capturing event dynamics in continuous time is beneficial, such as education systems, social networks, and recommender systems. However, current MTPPs suffer from two major limitations, i.e., inefficient representation of event dynamic’s influence on marker distribution and losing fine-grained representation of historical marker distributions in the modeling. Motivated by these limitations, we propose a novel model called M arked P o int Processes with M emory- E nhanced N eural Ne t works (MoMENt) that can capture the bidirectional interrelations between markers and event dynamics while providing fine-grained marker representations. Specifically, MoMENt is constructed of two concurrent networks: Recurrent Activity Updater (RAU) to capture model event dynamics and Memory-Enhanced Marker Updater (MEMU) to represent markers. Both RAU and MEMU components are designed to update each other at every step to model the bidirectional influence of markers and event dynamics. To obtain a fine-grained representation of maker distributions, MEMU is devised with external memories that model detailed marker-level features with latent component vectors. Our extensive experiments on six real-world user interaction datasets demonstrate that MoMENt can accurately represent users’ activity dynamics, boosting time, type, and marker predictions, as well as recommendation performance up to 76.5%, 65.6%, 77.2%, and 57.7%, respectively, compared to baseline approaches. Furthermore, our case studies show the effectiveness of MoMENt in providing meaningful and fine-grained interpretations of user-system relations over time, e.g., how user choices influence their future preferences in the recommendation domain.},
  archive      = {J_TKDD},
  doi          = {10.1145/3649504},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-32},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {MoMENt: Marked point processes with memory-enhanced neural networks for user activity modeling},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Node embedding preserving graph summarization.
<em>TKDD</em>, <em>18</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3649505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph summarization is a useful tool for analyzing large-scale graphs. Some works tried to preserve original node embeddings encoding rich structural information of nodes on the summary graph. However, their algorithms are designed heuristically and not theoretically guaranteed. In this article, we theoretically study the problem of preserving node embeddings on summary graph. We prove that three matrix-factorization-based node embedding methods of the original graph can be approximated by that of the summary graph, and we propose a novel graph summarization method, named HCSumm , based on this analysis. Extensive experiments are performed on real-world datasets to evaluate the effectiveness of our proposed method. The experimental results show that our method outperforms the state-of-the-art methods in preserving node embeddings.},
  archive      = {J_TKDD},
  doi          = {10.1145/3649505},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Node embedding preserving graph summarization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harnessing the power of LLMs in practice: A survey on
ChatGPT and beyond. <em>TKDD</em>, <em>18</em>(6), 1–32. (<a
href="https://doi.org/10.1145/3649506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide . An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai.},
  archive      = {J_TKDD},
  doi          = {10.1145/3649506},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-32},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FulBM: Fast fully batch maintenance for landmark-based 3-hop
cover labeling. <em>TKDD</em>, <em>18</em>(6), 1–26. (<a
href="https://doi.org/10.1145/3650035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landmark-based 3-hop cover labeling is a category of approaches for shortest distance/path queries on large-scale complex networks. It pre-computes an index offline to accelerate the online distance/path query. Most real-world graphs undergo rapid changes in topology, which makes index maintenance on dynamic graphs necessary. So far, the majority of index maintenance methods can handle only one edge update (either an addition or deletion) each time. To keep up with frequently changing graphs, we research the ful ly b atch m aintenance problem for the 3-hop cover labeling, and proposed the method called FulBM . FulBM is composed of two algorithms: InsBM and DelBM, which are designed to handle batch edge insertions and deletions, respectively. This separation is motivated by the insight that batch maintenance for edge insertions are much more time-efficient and the fact that most edge updates in the real world are incremental. Both InsBM and DelBM are equipped with well-designed pruning strategies to minimize the number of vertex accesses. We have conducted comprehensive experiments on both synthetic and real-world graphs to verify the efficiency of FulBM and its variants for weighted graphs. The results show that our methods achieve 5.5× to 228× speedup compared with the state-of-the-art method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3650035},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {FulBM: Fast fully batch maintenance for landmark-based 3-hop cover labeling},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building shortcuts between distant nodes with biaffine
mapping for graph convolutional networks. <em>TKDD</em>, <em>18</em>(6),
1–21. (<a href="https://doi.org/10.1145/3650113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple recent studies show a paradox in graph convolutional networks (GCNs)—that is, shallow architectures limit the capability of learning information from high-order neighbors, whereas deep architectures suffer from over-smoothing or over-squashing. To enjoy the simplicity of shallow architectures and overcome their limits of neighborhood extension, in this work we introduce a biaffine technique to improve the expressiveness of GCNs with a shallow architecture. The core design of our method is to learn direct dependency on long-distance neighbors for nodes, with which only 1-hop message passing is capable of capturing rich information for node representation. Besides, we propose a multi-view contrastive learning method to exploit the representations learned from long-distance dependencies. Extensive experiments on nine graph benchmark datasets suggest that the shallow biaffine graph convolutional networks (BAGCN) significantly outperform state-of-the-art GCNs (with deep or shallow architectures) on semi-supervised node classification. We further verify the effectiveness of biaffine design in node representation learning and the performance consistency on different sizes of training data.},
  archive      = {J_TKDD},
  doi          = {10.1145/3650113},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Building shortcuts between distant nodes with biaffine mapping for graph convolutional networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scenario and multi-task aware feature interaction for
recommendation system. <em>TKDD</em>, <em>18</em>(6), 1–20. (<a
href="https://doi.org/10.1145/3651312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scenario and multi-task recommendation can use various feedback behaviors of users in different scenarios to learn users’ preferences and then make recommendations, which has attracted attention. However, the existing work ignores feature interactions and the fact that a pair of feature interactions will have differing levels of importance under different scenario-task pairs, leading to sub-optimal user preference learning. In this article, we propose a M ulti-scenario and M ulti-task aware F eature I nteraction model, dubbed MMFI , to explicitly model feature interactions and learn the importance of feature interaction pairs in different scenarios and tasks. Specifically, MMFI first incorporates a pairwise feature interaction unit and a scenario-task interaction unit to effectively capture the interaction of feature pairs and scenario-task pairs. Then MMFI designs a scenario-task aware attention layer for learning the importance of feature interactions from coarse-grained to fine-grained, improving the model’s performance on various scenario-task pairs. More specifically, this attention layer consists of three modules: a fully shared bottom module, a partially shared middle module, and a specific output module. Finally, MMFI adapts two sparsity-aware functions to remove some useless feature interactions. Extensive experiments on two public datasets demonstrate the superiority of the proposed method over the existing multi-task recommendation, multi-scenario recommendation, and multi-scenario &amp; multi-task recommendation models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3651312},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-scenario and multi-task aware feature interaction for recommendation system},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SsAG: Summarization and sparsification of attributed graphs.
<em>TKDD</em>, <em>18</em>(6), 1–22. (<a
href="https://doi.org/10.1145/3651619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph summarization has become integral for managing and analyzing large-scale graphs in diverse real-world applications, including social networks, biological networks, and communication networks. Existing methods for graph summarization often face challenges, being either computationally expensive, limiting their applicability to large graphs, or lacking the incorporation of node attributes. In response, we introduce SsAG , an efficient and scalable lossy graph summarization method designed to preserve the essential structure of the original graph. SsAG computes a sparse representation (summary) of the input graph, accommodating graphs with node attributes. The summary is structured as a graph on supernodes (subsets of vertices of G ), where weighted superedges connect pairs of supernodes. The methodology focuses on constructing a summary graph with k supernodes, aiming to minimize the reconstruction error (the difference between the original graph and the graph reconstructed from the summary) while maximizing homogeneity with respect to the node attributes. The construction process involves iteratively merging pairs of nodes. To enhance computational efficiency, we derive a closed-form expression for efficiently computing the reconstruction error (RE) after merging a pair, enabling constant-time approximation of this score. We assign a weight to each supernode, quantifying their contribution to the score of pairs, and utilize a weighted sampling strategy to select the best pair for merging. Notably, a logarithmic-sized sample achieves a summary comparable in quality based on various measures. Additionally, we propose a sparsification step for the constructed summary, aiming to reduce storage costs to a specified target size with a marginal increase in RE. Empirical evaluations across diverse real-world graphs demonstrate that SsAG exhibits superior speed, being up to 17 × faster, while generating summaries of comparable quality. This work represents a significant advancement in the field, addressing computational challenges and showcasing the effectiveness of SsAG in graph summarization.},
  archive      = {J_TKDD},
  doi          = {10.1145/3651619},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {SsAG: Summarization and sparsification of attributed graphs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive content-aware influence maximization via online
learning to rank. <em>TKDD</em>, <em>18</em>(6), 1–35. (<a
href="https://doi.org/10.1145/3651987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we adapt the composition of a post over a series of rounds to make it more appealing in a social network? Techniques that progressively learn how to make a fixed post more influential over rounds have been studied in the context of the Influence Maximization (IM) problem, which seeks a set of seed users that maximize a post’s influence. However, there is no work on progressively learning how a post’s features affect its influence. In this article, we propose and study the problem of Adaptive Content-Aware Influence Maximization (ACAIM), which calls to find k features to form a post in each round so as to maximize the cumulative influence of those posts over all rounds. We solve ACAIM by applying, for the first time, an Online Learning to Rank (OLR) framework for IM purposes. We introduce the CATRID propagation model , which expresses how posts disseminate in a social network using click probabilities and post visibility criteria and develop a simulator that runs CATRID via a training-testing scheme based on real posts of the VK social network, so as to realistically represent the learning environment. We deploy three learners that solve ACAIM in an online (real-time) manner. We experimentally prove the practical suitability of our solutions via exhaustive experiments on multiple brands (operating as different case studies ) and several VK datasets; the best learner is evaluated on 45 separate case studies yielding convincing results.},
  archive      = {J_TKDD},
  doi          = {10.1145/3651987},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-35},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Adaptive content-aware influence maximization via online learning to rank},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual perspective framework of knowledge-correlation for
cross-domain recommendation. <em>TKDD</em>, <em>18</em>(6), 1–28. (<a
href="https://doi.org/10.1145/3652520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender System provides users with online services in a personalized way. The performance of traditional recommender systems may deteriorate because of problems such as cold-start and data sparsity. Cross-domain Recommendation System utilizes the richer information from auxiliary domains to guide the task in the target domain. However, direct knowledge transfer may lead to a negative impact due to data heterogeneity and feature mismatch between domains. In this article, we innovatively explore the cross-domain correlation from the perspectives of content semanticity and structural connectivity to fully exploit the information of Knowledge Graph. First, we adopt domain adaptation that automatically extracts transferable features to capture cross-domain semantic relations. Second, we devise a knowledge-aware graph neural network to explicitly model the high-order connectivity across domains. Third, we develop feature fusion strategies to combine the advantages of semantic and structural information. By simulating the cold-start scenario on two real-world datasets, the experimental results show that our proposed method has superior performance in accuracy and diversity compared with the SOTA methods. It demonstrates that our method can accurately predict users’ expressed preferences while exploring their potential diverse interests.},
  archive      = {J_TKDD},
  doi          = {10.1145/3652520},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A dual perspective framework of knowledge-correlation for cross-domain recommendation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepMeshCity: A deep learning model for urban grid
prediction. <em>TKDD</em>, <em>18</em>(6), 1–26. (<a
href="https://doi.org/10.1145/3652859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban grid prediction can be applied to many classic spatial-temporal prediction tasks such as air quality prediction, crowd density prediction, and traffic flow prediction, which is of great importance to smart city building. In light of its practical values, many methods have been developed for it and have achieved promising results. Despite their successes, two main challenges remain open: (a) how to well capture the global dependencies and (b) how to effectively model the multi-scale spatial-temporal correlations? To address these two challenges, we propose a novel method— DeepMeshCity , with a carefully-designed Self-Attention Citywide Grid Learner ( SA-CGL ) block comprising of a self-attention unit and a Citywide Grid Learner ( CGL ) unit. The self-attention block aims to capture the global spatial dependencies, and the CGL unit is responsible for learning the spatial-temporal correlations. In particular, a multi-scale memory unit is proposed to traverse all stacked SA-CGL blocks along a zigzag path to capture the multi-scale spatial-temporal correlations. In addition, we propose to initialize the single-scale memory units and the multi-scale memory units by using the corresponding ones in the previous fragment stack, so as to speed up the model training. We evaluate the performance of our proposed model by comparing with several state-of-the-art methods on four real-world datasets for two urban grid prediction applications. The experimental results verify the superiority of DeepMeshCity over the existing ones. The code is available at https://github.com/ILoveStudying/DeepMeshCity.},
  archive      = {J_TKDD},
  doi          = {10.1145/3652859},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DeepMeshCity: A deep learning model for urban grid prediction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised multi-view clustering based on NMF with
fusion regularization. <em>TKDD</em>, <em>18</em>(6), 1–26. (<a
href="https://doi.org/10.1145/3653022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering has attracted significant attention and application. Nonnegative matrix factorization is one popular feature of learning technology in pattern recognition. In recent years, many semi-supervised nonnegative matrix factorization algorithms were proposed by considering label information, which has achieved outstanding performance for multi-view clustering. However, most of these existing methods have either failed to consider discriminative information effectively or included too much hyper-parameters. Addressing these issues, a semi-supervised multi-view nonnegative matrix factorization with a novel fusion regularization (FRSMNMF) is developed in this article. In this work, we uniformly constrain alignment of multiple views and discriminative information among clusters with designed fusion regularization. Meanwhile, to align the multiple views effectively, two kinds of compensating matrices are used to normalize the feature scales of different views. Additionally, we preserve the geometry structure information of labeled and unlabeled samples by introducing the graph regularization simultaneously. Due to the proposed methods, two effective optimization strategies based on multiplicative update rules are designed. Experiments implemented on six real-world datasets have demonstrated the effectiveness of our FRSMNMF comparing with several state-of-the-art unsupervised and semi-supervised approaches.},
  archive      = {J_TKDD},
  doi          = {10.1145/3653022},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Semi-supervised multi-view clustering based on NMF with fusion regularization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NOODLE: Joint cross-view discrepancy discovery and
high-order correlation detection for multi-view subspace clustering.
<em>TKDD</em>, <em>18</em>(6), 1–23. (<a
href="https://doi.org/10.1145/3653305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the effective exploration of the valuable topological pair-wise relationship of data points across multiple views, multi-view subspace clustering (MVSC) has received increasing attention in recent years. However, we observe that existing MVSC approaches still suffer from two limitations that need to be further improved to enhance the clustering effectiveness. Firstly, previous MVSC approaches mainly prioritize extracting multi-view consistency, often neglecting the cross-view discrepancy that may arise from noise, outliers, and view-inherent properties. Secondly, existing techniques are constrained by their reliance on pair-wise sample correlation and pair-wise view correlation, failing to capture the high-order correlations that are enclosed within multiple views. To address these issues, we propose a novel MVSC framework via joi N t cr O ss-view discrepancy disc O very an D high-order corre L ation d E tection ( NOODLE ), seeking an informative target subspace representation compatible across multiple features to facilitate the downstream clustering task. Specifically, we first exploit the self-representation mechanism to learn multiple view-specific affinity matrices, which are further decomposed into cohesive factors and incongruous factors to fit the multi-view consistency and discrepancy, respectively. Additionally, an explicit cross-view sparse regularization is applied to incoherent parts, ensuring the consistency and discrepancy to be precisely separated from the initial subspace representations. Meanwhile, the multiple cohesive parts are stacked into a three-dimensional tensor associated with a tensor-Singular Value Decomposition (t-SVD) based weighted tensor nuclear norm constraint, enabling effective detection of the high-order correlations implicit in multi-view data. Our proposed method outperforms state-of-the-art methods for multi-view clustering on six benchmark datasets, demonstrating its effectiveness.},
  archive      = {J_TKDD},
  doi          = {10.1145/3653305},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {NOODLE: Joint cross-view discrepancy discovery and high-order correlation detection for multi-view subspace clustering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representative and back-in-time sampling from real-world
hypergraphs. <em>TKDD</em>, <em>18</em>(6), 1–48. (<a
href="https://doi.org/10.1145/3653306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are widely used for representing pairwise interactions in complex systems. Since such real-world graphs are large and often evergrowing, sampling subgraphs is useful for various purposes, including simulation, visualization, stream processing, representation learning, and crawling. However, many complex systems consist of group interactions (e.g., collaborations of researchers and discussions on online Q&amp;A platforms) and thus are represented more naturally and accurately by hypergraphs than by ordinary graphs. Motivated by the prevalence of large-scale hypergraphs, we study the problem of sampling from real-world hypergraphs, aiming at answering (Q1) how can we measure the goodness of sub-hypergraphs, and (Q2) how can we efficiently find a “good” sub-hypergraph. Regarding Q1, we distinguish between two goals: (a) representative sampling , which aims at capturing the characteristics of the input hypergraph, and (b) back-in-time sampling , which aims at closely approximating a past snapshot of the input time-evolving hypergraph. To evaluate the similarity of the sampled sub-hypergraph to the target (i.e., the input hypergraph or its past snapshot), we consider 10 graph-level, hyperedge-level, and node-level statistics. Regarding Q2, we first conduct a thorough analysis of various intuitive approaches using 11 real-world hypergraphs. Then, based on this analysis, we propose MiDaS and MiDaS-B , designed for representative sampling and back-in-time sampling, respectively. Regarding representative sampling, we demonstrate through extensive experiments that MiDaS , which employs a sampling bias toward high-degree nodes in hyperedge selection, is (a) Representative : finding overall the most representative samples among 15 considered approaches, (b) Fast : several orders of magnitude faster than the strongest competitors, and (c) Automatic : automatically tuning the degree of sampling bias. Regarding back-in-time sampling, we demonstrate that MiDaS-B inherits the strengths of MiDaS despite an additional challenge—the unavailability of the target (i.e., past snapshot). It effectively handles this challenge by focusing on replicating universal evolutionary patterns, rather than directly replicating the target.},
  archive      = {J_TKDD},
  doi          = {10.1145/3653306},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-48},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Representative and back-in-time sampling from real-world hypergraphs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical convolutional neural network with knowledge
complementation for long-tailed classification. <em>TKDD</em>,
<em>18</em>(6), 1–22. (<a
href="https://doi.org/10.1145/3653717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods based on transfer learning leverage auxiliary information to help tail generalization and improve the performance of the tail classes. However, they cannot fully exploit the relationships between auxiliary information and tail classes and bring irrelevant knowledge to the tail classes. To solve this problem, we propose a hierarchical CNN with knowledge complementation, which regards hierarchical relationships as auxiliary information and transfers relevant knowledge to tail classes. First, we integrate semantics and clustering relationships as hierarchical knowledge into the CNN to guide feature learning. Then, we design a complementary strategy to jointly exploit the two types of knowledge, where semantic knowledge acts as a prior dependence and clustering knowledge reduces the negative information caused by excessive semantic dependence (i.e., semantic gaps). In this way, the CNN facilitates the utilization of the two complementary hierarchical relationships and transfers useful knowledge to tail data to improve long-tailed classification accuracy. Experimental results on public benchmarks show that the proposed model outperforms existing methods. In particular, our model improves accuracy by 3.46% compared with the second-best method on the long-tailed tieredImageNet dataset.},
  archive      = {J_TKDD},
  doi          = {10.1145/3653717},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Hierarchical convolutional neural network with knowledge complementation for long-tailed classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual homogeneity hypergraph motifs with cross-view
contrastive learning for multiple social recommendations. <em>TKDD</em>,
<em>18</em>(6), 1–24. (<a
href="https://doi.org/10.1145/3653976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social relations are often used as auxiliary information to address data sparsity and cold-start issues in social recommendations. In the real world, social relations among users are complex and diverse. Widely used graph neural networks (GNNs) can only model pairwise node relationships and are not conducive to exploring higher-order connectivity, while hypergraph provides a natural way to model high-order relations between nodes. However, recent studies show that social recommendations still face the following challenges: 1) a majority of social recommendations ignore the impact of multifaceted social relationships on user preferences; 2) the item homogeneity is often neglected, mainly referring to items with similar static attributes have similar attractiveness when exposed to users that indicating hidden links between items; and 3) directly combining the representations learned from different independent views cannot fully exploit the potential connections between different views. To address these challenges, in this article, we propose a novel method DH-HGCN++ for multiple social recommendations. Specifically, dual homogeneity (i.e., social homogeneity and item homogeneity) is introduced to mine the impact of diverse social relations on user preferences and enrich item representations. Hypergraph convolution networks with motifs are further exploited to model the high-order relations between nodes. Finally, cross-view contrastive learning is proposed as an auxiliary task to jointly optimize the DH-HGCN++. Real-world datasets are used to validate the effectiveness of the proposed model, where we use sentiment analysis to extract comment relations and employ the k-means clustering algorithm to construct the item-item correlation graph. Experiment results demonstrate that our proposed method consistently outperforms the state-of-the-art baselines on Top-N recommendations.},
  archive      = {J_TKDD},
  doi          = {10.1145/3653976},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Dual homogeneity hypergraph motifs with cross-view contrastive learning for multiple social recommendations},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Math word problem generation via disentangled memory
retrieval. <em>TKDD</em>, <em>18</em>(5), 1–21. (<a
href="https://doi.org/10.1145/3639569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of math word problem (MWP) generation, which generates an MWP given an equation and relevant topic words, has increasingly attracted researchers’ attention. In this work, we introduce a simple memory retrieval module to search related training MWPs, which are used to augment the generation. To retrieve more relevant training data, we also propose a disentangled memory retrieval module based on the simple memory retrieval module. To this end, we first disentangle the training MWPs into logical description and scenario description and then record them in respective memory modules. Later, we use the given equation and topic words as queries to retrieve relevant logical descriptions and scenario descriptions from the corresponding memory modules, respectively. The retrieved results are then used to complement the process of the MWP generation. Extensive experiments and ablation studies verify the superior performance of our method and the effectiveness of each proposed module. The code is available at https://github.com/mwp-g/MWPG-DMR .},
  archive      = {J_TKDD},
  doi          = {10.1145/3639569},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Math word problem generation via disentangled memory retrieval},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable and inductive semi-supervised classifier with
sample weighting based on graph topology. <em>TKDD</em>, <em>18</em>(5),
1–18. (<a href="https://doi.org/10.1145/3643645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph-based semi-supervised learning (GSSL) has garnered significant interest in the realms of machine learning and pattern recognition. Although some of the proposed methods have made some progress, there are still some shortcomings that need to be overcome. There are three main limitations. First, the graphs used in these approaches are usually predefined regardless of the task at hand. Second, due to the use of graphs, almost all approaches are unable to process and consider data with a very large number of unlabeled samples. Thirdly, the imbalance of the topology of the samples is very often not taken into account. In particular, processing large datasets with GSSL might pose challenges in terms of computational resource feasibility. In this article, we present a scalable and inductive GSSL method. We broaden the scope of the graph topology imbalance paradigm to extensive databases. Second, we employ the calculated weights of the labeled sample for the label-matching term in the global objective function. This leads to a unified, scalable, semi-supervised learning model that allows simultaneous labeling of unlabeled data, projection of the feature space onto the labeling space, along with the graph matrix of anchors. In the proposed scheme, the integration of labels and features from anchors is applied for the adaptive construction of the anchor graph. Experimental results were performed on four large databases: NORB, RCV1, Covtype, and MNIST. These experiments demonstrate that the proposed method exhibits superior performance when compared to existing scalable semi-supervised learning models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643645},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Scalable and inductive semi-supervised classifier with sample weighting based on graph topology},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-FSPMiner: A novel algorithm for frequent similar pattern
mining. <em>TKDD</em>, <em>18</em>(5), 1–26. (<a
href="https://doi.org/10.1145/3643820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent similar pattern mining (FSP mining) allows for finding frequent patterns hidden from the classical approach. However, the use of similarity functions implies more computational effort, necessitating the development of more efficient algorithms for FSP mining. This work aims to improve the efficiency of mining all FSPs when using Boolean and non-increasing monotonic similarity functions. A data structure to condense an object description collection, named FV-Tree , and an algorithm for mining all FSPs from the FV-Tree , named X-FSPMiner , are proposed. The experimental results reveal that the novel algorithm X-FSPMiner vastly outperforms the state-of-the-art algorithms for mining all FSPs using Boolean and non-increasing monotonic similarity functions.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643820},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {X-FSPMiner: A novel algorithm for frequent similar pattern mining},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards differential privacy in sequential recommendation: A
noisy graph neural network approach. <em>TKDD</em>, <em>18</em>(5),
1–21. (<a href="https://doi.org/10.1145/3643821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, we are the first to achieve differential privacy in sequential recommendation with dependent interactions. Specifically, in DIPSGNN, we first leverage piecewise mechanism to protect sensitive user features. Then, we innovatively add calibrated noise into aggregation step of graph neural network based on aggregation perturbation mechanism. And, this noisy graph neural network can protect sequentially dependent interactions and capture user preferences simultaneously. Extensive experiments demonstrate the superiority of our method over state-of-the-art differentially private recommender systems in terms of better balance between privacy and accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643821},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Towards differential privacy in sequential recommendation: A noisy graph neural network approach},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the value of head labels in multi-label text
classification. <em>TKDD</em>, <em>18</em>(5), 1–21. (<a
href="https://doi.org/10.1145/3643853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A formidable challenge in the multi-label text classification (MLTC) context is that the labels often exhibit a long-tailed distribution, which typically prevents deep MLTC models from obtaining satisfactory performance. To alleviate this problem, most existing solutions attempt to improve tail performance by means of sampling or introducing extra knowledge. Data-rich labels, though more trustworthy, have not received the attention they deserve. In this work, we propose a multiple-stage training framework to exploit both model- and feature-level knowledge from the head labels, to improve both the representation and generalization ability of MLTC models. Moreover, we theoretically prove the superiority of our framework design over other alternatives. Comprehensive experiments on widely used MLTC datasets clearly demonstrate that the proposed framework achieves highly superior results to state-of-the-art methods, highlighting the value of head labels in MLTC.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643853},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {On the value of head labels in multi-label text classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-instance learning with one side label noise.
<em>TKDD</em>, <em>18</em>(5), 1–24. (<a
href="https://doi.org/10.1145/3644076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-instance Learning (MIL) is a popular learning paradigm arising from many real applications. It assigns a label to a set of instances, which is called a bag, and the bag’s label is determined by the instances within it. A bag is positive if and only if it has at least one positive instance. Since labeling bags is more complicated than labeling each instance, we will often face the mislabeling problem in MIL. Furthermore, it is more common that a negative bag has been mislabeled to a positive one, since one mislabeled instance will lead to the change of the whole bag label. This is an important problem that originated from real applications, e.g., web mining and image classification, but little research has concentrated on it as far as we know. In this article, we focus on this MIL problem with one side label noise that the negative bags are mislabeled as positive ones. To address this challenging problem, we propose, to the best our our knowledge, a novel multi-instance learning method with one side label noise. We design a new double weighting approach under traditional framework to characterize the “faithfulness” of each instance and each bag in learning the classifier. Briefly, on the instance level, we employ a sparse weighting method to select the key instances, and the MIL problem with one size label noise is converted to a mislabeled supervised learning scenario. On the bag level, the weights of bags, together with the selected key instances, will be utilized to identify the real positive bags. In addition, we have solved our proposed model by an alternative iteration method with proved convergence behavior. Empirical studies on various datasets have validated the effectiveness of our method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3644076},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-instance learning with one side label noise},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local community detection in multiple private networks.
<em>TKDD</em>, <em>18</em>(5), 1–21. (<a
href="https://doi.org/10.1145/3644078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals are often involved in multiple online social networks. Considering that owners of these networks are unwilling to share their networks, some global algorithms combine information from multiple networks to detect all communities in multiple networks without sharing their edges. When data owners are only interested in the community containing a given node, it is unnecessary and computationally expensive for multiple networks to interact with each other to mine all communities. Moreover, data owners who are specifically looking for a community typically prefer to provide less data than the global algorithms require. Therefore, we propose the Local Collaborative Community Detection problem (LCCD). It exploits information from multiple networks to jointly detect the local community containing a given node without directly sharing edges between networks. To address the LCCD problem, we present a method developed from M method, called colM, to detect the local community in multiple networks. This method adopts secure multiparty computation protocols to protect each network’s private information. Our experiments were conducted on real-world and synthetic datasets. Experimental results show that colM method could effectively identify community structures and outperform comparison algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3644078},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Local community detection in multiple private networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A taxonomy for learning with perturbation and algorithms.
<em>TKDD</em>, <em>18</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3644391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighting strategy prevails in machine learning. For example, a common approach in robust machine learning is to exert low weights on samples which are likely to be noisy or quite hard. This study summarizes another less-explored strategy, namely, perturbation. Various incarnations of perturbation have been utilized but it has not been explicitly revealed. Learning with perturbation is called perturbation learning and a systematic taxonomy is constructed for it in this study. In our taxonomy, learning with perturbation is divided on the basis of the perturbation targets, directions, inference manners, and granularity levels. Many existing learning algorithms including some classical ones can be understood with the constructed taxonomy. Alternatively, these algorithms share the same component, namely, perturbation in their procedures. Furthermore, a family of new learning algorithms can be obtained by varying existing learning algorithms with our taxonomy. Specifically, three concrete new learning algorithms are proposed for robust machine learning. Extensive experiments on image classification and text sentiment analysis verify the effectiveness of the three new algorithms. Learning with perturbation can also be used in other various learning scenarios, such as imbalanced learning, clustering, regression, and so on.},
  archive      = {J_TKDD},
  doi          = {10.1145/3644391},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A taxonomy for learning with perturbation and algorithms},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing out-of-distribution generalization on graphs via
causal attention learning. <em>TKDD</em>, <em>18</em>(5), 1–24. (<a
href="https://doi.org/10.1145/3644392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In graph classification, attention- and pooling-based graph neural networks (GNNs) predominate to extract salient features from the input graph and support the prediction. They mostly follow the paradigm of “learning to attend,” which maximizes the mutual information between the attended graph and the ground-truth label. However, this paradigm causes GNN classifiers to indiscriminately absorb all statistical correlations between input features and labels in the training data without distinguishing the causal and noncausal effects of features. Rather than emphasizing causal features, the attended graphs tend to rely on noncausal features as shortcuts to predictions. These shortcut features may easily change outside the training distribution, thereby leading to poor generalization for GNN classifiers. In this article, we take a causal view on GNN modeling. Under our causal assumption, the shortcut feature serves as a confounder between the causal feature and prediction. It misleads the classifier into learning spurious correlations that facilitate prediction in in-distribution (ID) test evaluation while causing significant performance drop in out-of-distribution (OOD) test data. To address this issue, we employ the backdoor adjustment from causal theory—combining each causal feature with various shortcut features, to identify causal patterns and mitigate the confounding effect. Specifically, we employ attention modules to estimate the causal and shortcut features of the input graph. Then, a memory bank collects the estimated shortcut features, enhancing the diversity of shortcut features for combination. Simultaneously, we apply the prototype strategy to improve the consistency of intra-class causal features. We term our method as CAL+, which can promote stable relationships between causal estimation and prediction, regardless of distribution changes. Extensive experiments on synthetic and real-world OOD benchmarks demonstrate our method’s effectiveness in improving OOD generalization. Our codes are released at https://github.com/shuyao-wang/CAL-plus .},
  archive      = {J_TKDD},
  doi          = {10.1145/3644392},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Enhancing out-of-distribution generalization on graphs via causal attention learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional generative adversarial network for early
classification of longitudinal datasets using an imputation approach.
<em>TKDD</em>, <em>18</em>(5), 1–25. (<a
href="https://doi.org/10.1145/3644821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early classification of longitudinal data remains an active area of research today. The complexity of these datasets and the high rates of missing data caused by irregular sampling present data-level challenges for the Early Longitudinal Data Classification (ELDC) problem. Coupled with the algorithmic challenge of optimising the opposing objectives of early classification (i.e., earliness and accuracy), ELDC becomes a non-trivial task. Inspired by the generative power and utility of the Generative Adversarial Network (GAN), we propose a novel context-conditional, longitudinal early classifier GAN (LEC-GAN). This model utilises informative missingness, static features and earlier observations to improve the ELDC objective. It achieves this by incorporating ELDC as an auxiliary task within an imputation optimization process. Our experiments on several datasets demonstrate that LEC-GAN outperforms all relevant baselines in terms of F1 scores while increasing the earliness of prediction.},
  archive      = {J_TKDD},
  doi          = {10.1145/3644821},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Conditional generative adversarial network for early classification of longitudinal datasets using an imputation approach},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating multi-level sampling with adaptive aggregation
for inductive knowledge graph completion. <em>TKDD</em>, <em>18</em>(5),
1–16. (<a href="https://doi.org/10.1145/3644822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Graph Neural Networks (GNNs) have achieved unprecedented success in handling graph-structured data, thereby driving the development of numerous GNN-oriented techniques for inductive knowledge graph completion (KGC). A key limitation of existing methods, however, is their dependence on pre-defined aggregation functions, which lack the adaptability to diverse data, resulting in suboptimal performance on established benchmarks. Another challenge arises from the exponential increase in irrelated entities as the reasoning path lengthens, introducing unwarranted noise and consequently diminishing the model’s generalization capabilities. To surmount these obstacles, we design an innovative framework that synergizes M ulti- L evel S ampling with an A daptive A ggregation mechanism (MLSAA). Distinctively, our model couples GNNs with enhanced set transformers, enabling dynamic selection of the most appropriate aggregation function tailored to specific datasets and tasks. This adaptability significantly boosts both the model’s flexibility and its expressive capacity. Additionally, we unveil a unique sampling strategy designed to selectively filter irrelevant entities, while retaining potentially beneficial targets throughout the reasoning process. We undertake an exhaustive evaluation of our novel inductive KGC method across three pivotal benchmark datasets and the experimental results corroborate the efficacy of MLSAA.},
  archive      = {J_TKDD},
  doi          = {10.1145/3644822},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-16},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Incorporating multi-level sampling with adaptive aggregation for inductive knowledge graph completion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generation-based multi-view contrast for self-supervised
graph representation learning. <em>TKDD</em>, <em>18</em>(5), 1–17. (<a
href="https://doi.org/10.1145/3645095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning has made remarkable achievements in the self-supervised representation learning of graph-structured data. By employing perturbation function (i.e., perturbation on the nodes or edges of graph), most graph contrastive learning methods construct contrastive samples on the original graph. However, the perturbation-based data augmentation methods randomly change the inherent information (e.g., attributes or structures) of the graph. Therefore, after nodes embedding on the perturbed graph, we cannot guarantee the validity of the contrastive samples as well as the learned performance of graph contrastive learning. To this end, in this article, we propose a novel generation-based multi-view contrastive learning framework (GMVC) for self-supervised graph representation learning, which generates the contrastive samples based on our generator rather than perturbation function. Specifically, after nodes embedding on the original graph we first employ random walk in the neighborhood to develop multiple relevant node sequences for each anchor node. We then utilize the transformer to generate the representations of relevant contrastive samples of anchor node based on the features and structures of the sampled node sequences. Finally, by maximizing the consistency between the anchor view and the generated views, we force the model to effectively encode graph information into nodes embeddings. We perform extensive experiments of node classification and link prediction tasks on eight benchmark datasets, which verify the effectiveness of our generation-based multi-view graph contrastive learning method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3645095},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-17},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Generation-based multi-view contrast for self-supervised graph representation learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining top-k high on-shelf utility itemsets using novel
threshold raising strategies. <em>TKDD</em>, <em>18</em>(5), 1–23. (<a
href="https://doi.org/10.1145/3645115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High utility itemsets (HUIs) mining is an emerging area of data mining which discovers sets of items generating a high profit from transactional datasets. In recent years, several algorithms have been proposed for this task. However, most of them do not consider the on-shelf time period of items and negative utility of items. High on-shelf utility itemset (HOUIs) mining is more difficult than traditional HUIs mining because it deals with on-shelf-based time period and negative utility of items. Moreover, most algorithms need minimum utility threshold ( min_util ) to find rules. However, specifying the appropriate min_util threshold is a difficult problem for users. A smaller min_util threshold may generate too many rules and a higher one may generate a few rules, which can degrade performance. To address these issues, a novel top-k HOUIs mining algorithm named TKOS ( T op- K high O n- S helf utility itemsets miner) is proposed which considers on-shelf time period and negative utility. TKOS presents a novel branch and bound-based strategy to raise the internal min_util threshold efficiently. It also presents two pruning strategies to speed up the mining process. In order to reduce the dataset scanning cost, we utilize transaction merging and dataset projection techniques. Extensive experiments have been conducted on real and synthetic datasets having various characteristics. Experimental results show that the proposed algorithm outperforms the state-of-the-art algorithms. The proposed algorithm is up to 42 times faster and uses up-to 19 times less memory compared to the state-of-the-art KOSHU. Moreover, the proposed algorithm has excellent scalability in terms of time periods and the number of transactions.},
  archive      = {J_TKDD},
  doi          = {10.1145/3645115},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Mining top-k high on-shelf utility itemsets using novel threshold raising strategies},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correlation-aware graph data augmentation with implicit and
explicit neighbors. <em>TKDD</em>, <em>18</em>(5), 1–23. (<a
href="https://doi.org/10.1145/3638057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a significant surge in commercial demand for citation graph-based tasks, such as patent analysis, social network analysis, and recommendation systems. Graph Neural Networks (GNNs) are widely used for these tasks due to their remarkable performance in capturing topological graph information. However, GNNs’ output results are highly dependent on the composition of local neighbors within the topological structure. To address this issue, we identify two types of neighbors in a citation graph: explicit neighbors based on the topological structure and implicit neighbors based on node features. Our primary motivation is to clearly define and visualize these neighbors, emphasizing their importance in enhancing graph neural network performance. We propose a Correlation-aware Network (CNet) to re-organize the citation graph and learn more valuable informative representations by leveraging these implicit and explicit neighbors. Our approach aims to improve graph data augmentation and classification performance, with the majority of our focus on stating the importance of using these neighbors, while also introducing a new graph data augmentation method. We compare CNet with state-of-the-art (SOTA) GNNs and other graph data augmentation approaches acting on GNNs. Extensive experiments demonstrate that CNet effectively extracts more valuable informative representations from the citation graph, significantly outperforming baselines. The code is available on public GitHub. 1},
  archive      = {J_TKDD},
  doi          = {10.1145/3638057},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Correlation-aware graph data augmentation with implicit and explicit neighbors},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised clustering of persian handwritten images using
regularization and dimension reduction methods. <em>TKDD</em>,
<em>18</em>(5), 1–19. (<a
href="https://doi.org/10.1145/3638060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering, as a fundamental exploratory data technique, not only is used to discover patterns and structures in complex datasets but also is utilized to group variables in high-dimensional data analysis. Dimension reduction through clustering helps identify important variables and reduce data dimensions without losing significant information. High-dimensional image datasets, such as Persian handwritten images, have numerous pixels, making statistical inference difficult. Such high-dimensionality property pose challenges for analysis and processing, requiring specialized techniques like clustering to extract information. Incorporating response variable information enhances clustering analysis, transforming it into a supervised method. This article evaluates a supervised clustering approach using Ridge and Lasso penalties, comparing them in analyzing a real dataset while identifying important variables. We demonstrate that despite choosing a small number of variables as important variables, Lasso penalty performs relatively well in predicting the labels of new observations for this multi-class dataset.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638060},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Supervised clustering of persian handwritten images using regularization and dimension reduction methods},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph time-series modeling in deep learning: A survey.
<em>TKDD</em>, <em>18</em>(5), 1–35. (<a
href="https://doi.org/10.1145/3638534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series and graphs have been extensively studied for their ubiquitous existence in numerous domains. Both topics have been separately explored in the field of deep learning. For time-series modeling, recurrent neural networks or convolutional neural networks model the relations between values across timesteps, while for graph modeling, graph neural networks model the inter-relations between nodes. Recent research in deep learning requires simultaneous modeling for time-series and graphs when both representations are present. For example, both types of modeling are necessary for time-series classification, regression, and anomaly detection in graphs. This article aims to provide a comprehensive summary of these models, which we call graph time-series models. To the best of our knowledge, this is the first survey article that provides a picture of related models from the perspective of deep graph time-series modeling to address a range of time-series tasks, including regression, classification, and anomaly detection. Graph time-series models are split into two categories: (a) graph recurrent/convolutional neural networks and (b) graph attention neural networks. Under each category, we further categorize models based on their properties. Additionally, we compare representative models and discuss how distinctive model characteristics are utilized with respect to various model components and data challenges. Pointers to commonly used datasets and code are included to facilitate access for further research. In the end, we discuss potential directions for future research.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638534},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Graph time-series modeling in deep learning: A survey},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-distribution: Retraceable power-law exponent of complex
networks. <em>TKDD</em>, <em>18</em>(5), 1–12. (<a
href="https://doi.org/10.1145/3639413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network modeling has been explored extensively by means of theoretical analysis as well as numerical simulations for Network Reconstruction (NR). The network reconstruction problem requires the estimation of the power-law exponent (γ) of a given input network. Thus, the effectiveness of the NR solution depends on the accuracy of the calculation of γ. In this article, we re-examine the degree distribution-based estimation of γ, which is not very accurate due to approximations. We propose X -distribution, which is more accurate than degree distribution. Various state-of-the-art network models, including CPM, NRM, RefOrCite2, BA, CDPAM, and DMS, are considered for simulation purposes, and simulated results support the proposed claim. Further, we apply X -distribution over several real-world networks to calculate their power-law exponents, which differ from those calculated using respective degree distributions. It is observed that X -distributions exhibit more linearity (straight line) on the log-log scale than degree distributions. Thus, X -distribution is more suitable for the evaluation of power-law exponent using linear fitting (on the log-log scale). The MATLAB implementation of power-law exponent (γ) calculation using X -distribution for different network models and the real-world datasets used in our experiments are available at https://github.com/Aikta-Arya/X-distribution-Retraceable-Power-Law-Exponent-of-Complex-Networks.git .},
  archive      = {J_TKDD},
  doi          = {10.1145/3639413},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-12},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {X-distribution: Retraceable power-law exponent of complex networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TaSPM: Targeted sequential pattern mining. <em>TKDD</em>,
<em>18</em>(5), 1–18. (<a
href="https://doi.org/10.1145/3639827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential pattern mining (SPM) is an important technique in the field of pattern mining, which has many applications in reality. Although many efficient SPM algorithms have been proposed, there are few studies that can focus on targeted tasks. Targeted querying of the concerned sequential patterns can not only reduce the number of patterns generated, but also increase the efficiency of users in performing related analysis. The current algorithms available for targeted sequence querying are based on specific scenarios and can not be extended to other applications. In this article, we formulate the problem of targeted sequential pattern mining and propose a generic algorithm, namely TaSPM. What is more, to improve the efficiency of TaSPM on large-scale datasets and multiple-item-based sequence datasets, we propose several pruning strategies to reduce meaningless operations in the mining process. Totally four pruning strategies are designed in TaSPM, and hence TaSPM can terminate unnecessary pattern extensions quickly and achieve better performance. Finally, we conducted extensive experiments on different datasets to compare the baseline SPM algorithm with TaSPM. Experiments show that the novel targeted mining algorithm TaSPM can achieve faster running time and less memory consumption.},
  archive      = {J_TKDD},
  doi          = {10.1145/3639827},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {TaSPM: Targeted sequential pattern mining},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymmetric learning for graph neural network based link
prediction. <em>TKDD</em>, <em>18</em>(5), 1–18. (<a
href="https://doi.org/10.1145/3640347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction is a fundamental problem in many graph-based applications, such as protein-protein interaction prediction. Recently, graph neural network (GNN) has been widely used for link prediction. However, existing GNN-based link prediction (GNN-LP) methods suffer from scalability problem during training for large-scale graphs, which has received little attention from researchers. In this paper, we first analyze the computation complexity of existing GNN-LP methods, revealing that one reason for the scalability problem stems from their symmetric learning strategy in applying the same class of GNN models to learn representation for both head nodes and tail nodes. We then propose a novel method, called a sym m etric l earning (AML), for GNN-LP. More specifically, AML applies a GNN model to learn head node representation while applying a multi-layer perceptron (MLP) model to learn tail node representation. To the best of our knowledge, AML is the first GNN-LP method to adopt an asymmetric learning strategy for node representation learning. Furthermore, we design a novel model architecture and apply a row-wise mini-batch sampling strategy to ensure promising model accuracy and training efficiency for AML. Experiments on three real large-scale datasets show that AML is 1.7×∼7.3× faster in training than baselines with a symmetric learning strategy while having almost no accuracy loss.},
  archive      = {J_TKDD},
  doi          = {10.1145/3640347},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Asymmetric learning for graph neural network based link prediction},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task learning with sequential dependence toward
industrial applications: A systematic formulation. <em>TKDD</em>,
<em>18</em>(5), 1–29. (<a
href="https://doi.org/10.1145/3640468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning (MTL) is widely used in the online recommendation and financial services for multi-step conversion estimation, but current works often overlook the sequential dependence among tasks. In particular, sequential dependence multi-task learning (SDMTL) faces challenges in dealing with complex task correlations and extracting valuable information in real-world scenarios, leading to negative transfer and a deterioration in the performance. Herein, a systematic learning paradigm of the SDMTL problem is established for the first time, which applies to more general multi-step conversion scenarios with longer conversion paths or various task dependence relationships. Meanwhile, an SDMTL architecture, named Task-Aware Feature Extraction (TAFE), is designed to enable the dynamic task representation learning from a sample-wise view. TAFE selectively reconstructs the implicit shared information corresponding to each sample case and performs the explicit task-specific extraction under dependence constraints, which can avoid the negative transfer, resulting in more effective information sharing and joint representation learning. Extensive experiment results demonstrate the effectiveness and applicability of the proposed theoretical and implementation frameworks. Furthermore, the online evaluations at MYbank showed that TAFE had an average increase of 9.22% and 3.76% in various scenarios on the post-view click-through &amp; conversion rate (CTCVR) estimation task. Currently, TAFE is deployed in an online platform to provide various traffic services.},
  archive      = {J_TKDD},
  doi          = {10.1145/3640468},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-29},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-task learning with sequential dependence toward industrial applications: A systematic formulation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EffCause: Discover dynamic causal relationships efficiently
from time-series. <em>TKDD</em>, <em>18</em>(5), 1–21. (<a
href="https://doi.org/10.1145/3640818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the proposal of Granger causality, many researchers have followed the idea and developed extensions to the original algorithm. The classic Granger causality test aims to detect the existence of the static causal relationship. Notably, a fundamental assumption underlying most previous studies is the stationarity of causality, which requires the causality between variables to keep stable. However, this study argues that it is easy to break in real-world scenarios. Fortunately, our paper presents an essential observation: if we consider a sufficiently short window when discovering the rapidly changing causalities, they will keep approximately static and thus can be detected using the static way correctly. In light of this, we develop EffCause, bringing dynamics into classic Granger causality. Specifically, to efficiently examine the causalities on different sliding window lengths, we design two optimization schemes in EffCause and demonstrate the advantage of EffCause through extensive experiments on both simulated and real-world datasets. The results validate that EffCause achieves state-of-the-art accuracy in continuous causal discovery tasks while achieving faster computation. Case studies from cloud system failure analysis and traffic flow monitoring show that EffCause effectively helps us understand real-world time-series data and solve practical problems.},
  archive      = {J_TKDD},
  doi          = {10.1145/3640818},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {EffCause: Discover dynamic causal relationships efficiently from time-series},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FiFrauD: Unsupervised financial fraud detection in dynamic
graph streams. <em>TKDD</em>, <em>18</em>(5), 1–29. (<a
href="https://doi.org/10.1145/3641857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a stream of financial transactions between traders in an e-market, how can we accurately detect fraudulent traders and suspicious behaviors in real time? Despite the efforts made in detecting these fraudsters, this field still faces serious challenges, including the ineffectiveness of existing methods for the complex and streaming environment of e-markets. As a result, it is still difficult to quickly and accurately detect suspected traders and behavior patterns in real-time transactions, and it is still considered an open problem. To solve this problem and alleviate the existing challenges, in this article, we propose FiFrauD, which is an unsupervised, scalable approach that depicts the behavior of manipulators in a transaction stream. In this approach, real-time transactions between traders are converted into a stream of graphs and, instead of using supervised and semi-supervised learning methods, fraudulent traders are detected precisely by exploiting density signals in graphs. Specifically, we reveal the traits of fraudulent traders in the market and propose a novel metric from this perspective, i.e., graph topology, time, and behavior. Then, we search for suspicious blocks by greedily optimizing the proposed metric. Theoretical analysis demonstrates upper bounds for FiFrauD&#39;s effectiveness in catching suspicious trades. Extensive experiments on five real-world datasets with both actual and synthetic labels demonstrate that FiFrauD achieves significant accuracy improvements compared with state-of-the-art fraud detection methods. Also, it can find various suspicious behavior patterns in a linear runtime and provide interpretable results. Furthermore, FiFrauD is resistant to the camouflage tactics used by fraudulent traders.},
  archive      = {J_TKDD},
  doi          = {10.1145/3641857},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-29},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {FiFrauD: Unsupervised financial fraud detection in dynamic graph streams},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Package arrival time prediction via knowledge distillation
graph neural network. <em>TKDD</em>, <em>18</em>(5), 1–19. (<a
href="https://doi.org/10.1145/3643033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately estimating packages’ arrival time in e-commerce can enhance users’ shopping experience and improve the placement rate of products. This problem is often formalized as an Origin-Destination (OD)-based ETA (i.e., estimated time of arrival) prediction task, where the delivery time is estimated mainly based on sender and receiver addresses and other context information. One inherent challenge of the OD-based ETA problem is that the delivery time highly depends on the actual delivery trajectory which is unknown at the time of prediction. In this article, we tackle this challenge by effectively exploiting historical delivery trajectories. We propose a novel Knowledge Distillation Graph neural network-based package ETA prediction (KDG-ETA) model, which uses knowledge distillation in the training phase to distill the knowledge of historical trajectories into OD pair embeddings. In KDG-ETA, a multi-level trajectory graph representation model is proposed to fully exploit trajectory information at the node-level, edge-level, and path-level. Then, the OD representations embedded with trajectory knowledge are combined with context embeddings from feature extraction module for delivery time prediction using an adaptive attention module. KDG-ETA consistently outperforms existing state-of-the-art OD-based ETA prediction methods on three real-world Alibaba datasets, reducing the Mean Absolute Error (MAE) by 3.0%–39.1% as demonstrated in our extensive empirical evaluation.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643033},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Package arrival time prediction via knowledge distillation graph neural network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain generalization in time series forecasting.
<em>TKDD</em>, <em>18</em>(5), 1–24. (<a
href="https://doi.org/10.1145/3643035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization aims to design models that can effectively generalize to unseen target domains by learning from observed source domains. Domain generalization poses a significant challenge for time series data, due to varying data distributions and temporal dependencies. Existing approaches to domain generalization are not designed for time series data, which often results in suboptimal or unstable performance when confronted with diverse temporal patterns and complex data characteristics. We propose a novel approach to tackle the problem of domain generalization in time series forecasting. We focus on a scenario where time series domains share certain common attributes and exhibit no abrupt distribution shifts. Our method revolves around the incorporation of a key regularization term into an existing time series forecasting model: domain discrepancy regularization . In this way, we aim to enforce consistent performance across different domains that exhibit distinct patterns. We calibrate the regularization term by investigating the performance within individual domains and propose the domain discrepancy regularization with domain difficulty awareness . We demonstrate the effectiveness of our method on multiple datasets, including synthetic and real-world time series datasets from diverse domains such as retail, transportation, and finance. Our method is compared against traditional methods, deep learning models, and domain generalization approaches to provide comprehensive insights into its performance. In these experiments, our method showcases superior performance, surpassing both the base model and competing domain generalization models across all datasets. Furthermore, our method is highly general and can be applied to various time series models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643035},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Domain generalization in time series forecasting},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on AutoML methods and systems for clustering.
<em>TKDD</em>, <em>18</em>(5), 1–30. (<a
href="https://doi.org/10.1145/3643564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Machine Learning (AutoML) aims to identify the best-performing machine learning algorithm along with its input parameters for a given dataset and a specific machine learning task. This is a challenging problem, as the process of finding the best model and tuning it for a particular problem at hand is both time-consuming for a data scientist and computationally expensive. In this survey, we focus on unsupervised learning, and we turn our attention on AutoML methods for clustering. We present a systematic review that includes many recent research works for automated clustering. Furthermore, we provide a taxonomy for the classification of existing works, and we perform a qualitative comparison. As a result, this survey provides a comprehensive overview of the field of AutoML for clustering. Moreover, we identify open challenges for future research in this field.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643564},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A survey on AutoML methods and systems for clustering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoBjeason: Reasoning covered object in image by multi-agent
collaboration based on informed knowledge graph. <em>TKDD</em>,
<em>18</em>(5), 1–56. (<a
href="https://doi.org/10.1145/3643565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a widely studied problem in existing works. However, in this paper, we turn to a more challenging problem of “ Covered Object Reasoning ”, aimed at reasoning the category label of target object in the given image particularly when it has been totally covered (or invisible ). To resolve this problem, we propose CoBjeason to seize the opportunity when visual reasoning meets the knowledge graph, where “ empirical cognition ” on common visual contexts have been incorporated as knowledge graph to conduct reinforced multi-hop reasoning via two collaborative agents. Such two agents, for one thing, stand at the covered object (or unknown entity ) to observe the surrounding visual cues in the given image and gradually select entities and relations from the global gallery-level knowledge graph which contains entity-pairs frequently occurring across the entire image-collection, so as to infer the main structure of image-level knowledge graph forward expanded from the unknown entity . In turn, for another, based on the reasoned image-level knowledge graph, the semantic context among entities will be aggregated backward into unknown entity to select an appropriate entity from the global gallery-level knowledge graph as the reasoning result. Moreover, such two agents will collaborate with each other, securing that the above Forward &amp; Backward Reasoning will step towards the same destination of the higher performance on covered object reasoning. To our best knowledge, this is the first work on Covered Object Reasoning with Knowledge Graphs and reinforced Multi-Agent collaboration. Particularly, our study on Covered Object Reasoning and the proposed model CoBjeason could offer novel insights into more basic Computer Vision (CV) tasks, such as Semantic Segmentation with better understanding on the current scene when some objects are blurred or covered, Visual Question Answering with enhancement on the inference in more complicated visual context when some objects are covered or invisible, and Image Caption Generation with the augmentation on the richness of visual context for images containing partially visible objects. The improvement on the above basic CV tasks can further refine more complicated ones involved with nuanced visual interpretation like Autonomous Driving, where the recognition and reasoning on partially visible or covered object are critical. According to the experimental results, our proposed CoBjeason can achieve the best overall ranking performance on covered object reasoning compared with other models, meanwhile enjoying the advantage of lower “ exploration cost ”, with the insensitivity against the long-tail covered objects and the acceptable time complexity.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643565},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-56},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {CoBjeason: Reasoning covered object in image by multi-agent collaboration based on informed knowledge graph},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prerequisite-enhanced category-aware graph neural networks
for course recommendation. <em>TKDD</em>, <em>18</em>(5), 1–21. (<a
href="https://doi.org/10.1145/3643644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Massive Open Online Courses (MOOCs) platforms has created an urgent need for an efficient personalized course recommender system that can assist learners of all backgrounds and levels of knowledge in selecting appropriate courses. Currently, most existing methods utilize a sequential recommendation paradigm that captures the user’s learning interests from their learning history, typically through recurrent or graph neural networks. However, fewer studies have explored how to incorporate principles of human learning at both the course and category levels to enhance course recommendations. In this article, we aim at addressing this gap by introducing a novel model, named Prerequisite-Enhanced Catory-Aware Graph Neural Network (PCGNN), for course recommendation. Specifically, we first construct a course prerequisite graph that reflects the human learning principles and further pre-train the course prerequisite relationships as the base embeddings for courses and categories. Then, to capture the user’s complex learning patterns, we build an item graph and a category graph from the user’s historical learning records, respectively: (1) the item graph reflects the course-level local learning transition patterns and (2) the category graph provides insight into the user’s long-term learning interest. Correspondingly, we propose a user interest encoder that employs a gated graph neural network to learn the course-level user interest embedding and design a category transition pattern encoder that utilizes GRU to yield the category-level user interest embedding. Finally, the two fine-grained user interest embeddings are fused to achieve precise course prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of PCGNN compared with other state-of-the-art methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643644},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Prerequisite-enhanced category-aware graph neural networks for course recommendation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attacking click-through rate predictors via generating
realistic fake samples. <em>TKDD</em>, <em>18</em>(5), 1–24. (<a
href="https://doi.org/10.1145/3643685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to construct imperceptible (realistic) fake samples is critical in adversarial attacks. Due to the sample feature diversity of a recommender system (containing both discrete and continuous features), traditional gradient-based adversarial attack methods may fail to construct realistic fake samples. Meanwhile, most recommendation models adopt click-through rate (CTR) predictors, which usually utilize black-box deep models with discrete features as input. Thus, how to efficiently construct realistic fake samples for black-box recommender systems is still full of challenges. In this article, we propose a hierarchical adversarial attack method against black-box CTR models via generating realistic fake samples, named CTRAttack. To better train the generation network, the weights of its embedding layer are shared with those of the substitute model, with both the similarity loss and classification loss used to update the generation network. To ensure that the discrete features of the generated fake samples are all real, we first adopt the similarity loss to ensure that the distribution of the generated perturbed samples is sufficiently close to the distribution of the real features, and then the nearest neighbor algorithm is used to retrieve the most appropriate features for non-existent discrete features from the candidate instance set. Extensive experiments demonstrate that CTRAttack can not only effectively attack the black-box recommender systems but also improve the robustness of these models while maintaining prediction accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643685},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Attacking click-through rate predictors via generating realistic fake samples},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Networked time-series prediction with incomplete data via
generative adversarial network. <em>TKDD</em>, <em>18</em>(5), 1–25. (<a
href="https://doi.org/10.1145/3643822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A networked time series (NETS) is a family of time series on a given graph, one for each node. It has a wide range of applications from intelligent transportation to environment monitoring to smart grid management. An important task in such applications is to predict the future values of a NETS based on its historical values and the underlying graph. Most existing methods require complete data for training. However, in real-world scenarios, it is not uncommon to have missing data due to sensor malfunction, incomplete sensing coverage, and so on. In this article, we study the problem of NETS prediction with incomplete data . We propose networked time series Imputation Generative Adversarial Network (NETS-ImpGAN), a novel deep learning framework that can be trained on incomplete data with missing values in both history and future. Furthermore, we propose Graph Temporal Attention Networks , which incorporate the attention mechanism to capture both inter-time series and temporal correlations. We conduct extensive experiments on four real-world datasets under different missing patterns and missing rates. The experimental results show that NETS-ImpGAN outperforms existing methods, reducing the Mean Absolute Error by up to 25%.},
  archive      = {J_TKDD},
  doi          = {10.1145/3643822},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Networked time-series prediction with incomplete data via generative adversarial network},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Totally-ordered sequential rules for utility maximization.
<em>TKDD</em>, <em>18</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3628450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-utility sequential pattern mining (HUSPM) is a significant and valuable activity in knowledge discovery and data analytics with many real-world applications. In some cases, HUSPM can not provide an excellent measure to predict what will happen. High-utility sequential rule mining (HUSRM) discovers high utility and high confidence sequential rules, so it can solve the issue in HUSPM. However, all existing HUSRM algorithms aim to find high-utility partially-ordered sequential rules (HUSRs), which are not consistent with reality and may generate fake HUSRs. Therefore, in this article, we formulate the problem of high-utility totally-ordered sequential rule mining and propose a novel algorithm, called TotalSR, which aims to identify all high-utility totally-ordered sequential rules (HTSRs). TotalSR introduces a left-first expansion strategy that can utilize the anti-monotonic property to use a confidence pruning strategy. TotalSR also designs a new utility upper bound: RSPEU , which is tighter than the existing upper bounds. TotalSR can drastically reduce the search space with the help of utility upper bounds pruning strategies, avoiding much more meaningless computation. To effectively compute the information, TotalSR proposes an auxiliary antecedent record table that can efficiently calculate the antecedent’s support and a utility prefix sum list that can compute the upper bound in O (1) time for a sequence. Finally, there are numerous experimental results on both real and synthetic datasets demonstrating that TotalSR is more efficient than the existing algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3628450},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Totally-ordered sequential rules for utility maximization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring and mitigating gender bias in legal contextualized
language models. <em>TKDD</em>, <em>18</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3628602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based contextualized language models constitute the state-of-the-art in several natural language processing (NLP) tasks and applications. Despite their utility, contextualized models can contain human-like social biases, as their training corpora generally consist of human-generated text. Evaluating and removing social biases in NLP models has been a major research endeavor. In parallel, NLP approaches in the legal domain, namely, legal NLP or computational law, have also been increasing. Eliminating unwanted bias in legal NLP is crucial, since the law has the utmost importance and effect on people. In this work, we focus on the gender bias encoded in BERT-based models. We propose a new template-based bias measurement method with a new bias evaluation corpus using crime words from the FBI database. This method quantifies the gender bias present in BERT-based models for legal applications. Furthermore, we propose a new fine-tuning-based debiasing method using the European Court of Human Rights (ECtHR) corpus to debias legal pre-trained models. We test the debiased models’ language understanding performance on the LexGLUE benchmark to confirm that the underlying semantic vector space is not perturbed during the debiasing process. Finally, we propose a bias penalty for the performance scores to emphasize the effect of gender bias on model performance.},
  archive      = {J_TKDD},
  doi          = {10.1145/3628602},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Measuring and mitigating gender bias in legal contextualized language models},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight, effective, and efficient model for label
aggregation in crowdsourcing. <em>TKDD</em>, <em>18</em>(4), 1–27. (<a
href="https://doi.org/10.1145/3630102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the presence of noise in crowdsourced labels, label aggregation (LA) has become a standard procedure for post-processing these labels. LA methods estimate true labels from crowdsourced labels by modeling worker quality. However, most existing LA methods are iterative in nature. They require multiple passes through all crowdsourced labels, jointly and iteratively updating true labels and worker qualities until a termination condition is met. As a result, these methods are burdened with high space and time complexities, which restrict their applicability in scenarios where scalability and online aggregation are essential. Furthermore, defining a suitable termination condition for iterative algorithms can be challenging. In this article, we view LA as a dynamic system and represent it as a Dynamic Bayesian Network. From this dynamic model, we derive two lightweight and scalable algorithms: LA onepass and LA twopass . These algorithms can efficiently and effectively estimate worker qualities and true labels by traversing all labels at most twice, thereby eliminating the need for explicit termination conditions and multiple traversals over the crowdsourced labels. Due to their dynamic nature, the proposed algorithms are also capable of performing label aggregation online. We provide theoretical proof of the convergence property of the proposed algorithms and bound the error of the estimated worker qualities. Furthermore, we analyze the space and time complexities of our proposed algorithms, demonstrating their equivalence to those of majority voting. Through experiments conducted on 20 real-world datasets, we demonstrate that our proposed algorithms can effectively and efficiently aggregate labels in both offline and online settings, even though they traverse all labels at most twice. The code is on https://github.com/yyang318/LA_onepass .},
  archive      = {J_TKDD},
  doi          = {10.1145/3630102},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A lightweight, effective, and efficient model for label aggregation in crowdsourcing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust graph meta-learning for weakly supervised few-shot
node classification. <em>TKDD</em>, <em>18</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3630260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph machine learning (Graph ML) models typically require abundant labeled instances to provide sufficient supervision signals, which is commonly infeasible in real-world scenarios since labeled data for newly emerged concepts (e.g., new categorizations of nodes) on graphs is rather limited. To efficiently learn with a small amount of data on graphs, meta-learning has been investigated in Graph ML. By transferring the knowledge learned from previous experiences to new tasks, graph meta-learning approaches have demonstrated promising performance on few-shot graph learning problems. However, most existing efforts predominately assume that all the data from the seen classes is gold labeled, yet those methods may lose their efficacy when the seen data is weakly labeled with severe label noise. As such, we aim to investigate a novel problem of weakly supervised graph meta-learning for improving the model robustness in terms of knowledge transfer. To achieve this goal, we propose Meta-GIN (Meta Graph Interpolation Network), a new graph meta-learning framework. Based on a new robustness-enhanced episodic training paradigm, Meta-GIN is meta-learned to interpolate node representations from weakly labeled data and extracts highly transferable meta-knowledge, which enables the model to quickly adapt to unseen tasks with few labeled instances. Extensive experiments demonstrate the superiority of Meta-GIN over existing graph meta-learning studies on the task of weakly supervised few-shot node classification.},
  archive      = {J_TKDD},
  doi          = {10.1145/3630260},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Robust graph meta-learning for weakly supervised few-shot node classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal multiple granularity interactive fusion network
for long document classification. <em>TKDD</em>, <em>18</em>(4), 1–24.
(<a href="https://doi.org/10.1145/3631711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long Document Classification (LDC) has attracted great attention in Natural Language Processing and achieved considerable progress owing to the large-scale pre-trained language models. In spite of this, as a different problem from the traditional text classification, LDC is far from being settled. Long documents, such as news and articles, generally have more than thousands of words with complex structures. Moreover, compared with flat text, long documents usually contain multi-modal content of images, which provide rich information but not yet being utilized for classification. In this article, we propose a novel cross-modal method for long document classification, in which multiple granularity feature shifting networks are proposed to integrate the multi-scale text and visual features of long documents adaptively. Additionally, a multi-modal collaborative pooling block is proposed to eliminate redundant fine-grained text features and simultaneously reduce the computational complexity. To verify the effectiveness of the proposed model, we conduct experiments on the Food101 dataset and two constructed multi-modal long document datasets. The experimental results show that the proposed cross-modal method outperforms the single-modal text methods and defeats the state-of-the-art related multi-modal baselines.},
  archive      = {J_TKDD},
  doi          = {10.1145/3631711},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Cross-modal multiple granularity interactive fusion network for long document classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory-user linking via hierarchical spatio-temporal
attention networks. <em>TKDD</em>, <em>18</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3635718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory-User Linking (TUL) is crucial for human mobility modeling by linking different trajectories to users with the exploration of complex mobility patterns. Existing works mainly rely on the recurrent neural framework to encode the temporal dependencies in trajectories, have fall short in capturing spatial-temporal global context for TUL prediction. To fill this gap, this work presents a new hierarchical spatio-temporal attention neural network, called AttnTUL , to jointly encode the local trajectory transitional patterns and global spatial dependencies for TUL. Specifically, our first model component is built over the graph neural architecture to preserve the local and global context and enhance the representation paradigm of geographical regions and user trajectories. Additionally, a hierarchically structured attention network is designed to simultaneously encode the intra-trajectory and inter-trajectory dependencies, with the integration of the temporal attention mechanism and global elastic attentional encoder. Extensive experiments demonstrate the superiority of our AttnTUL method as compared to state-of-the-art baselines on various trajectory datasets. The source code of our model is available at https://github.com/Onedean/AttnTUL .},
  archive      = {J_TKDD},
  doi          = {10.1145/3635718},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Trajectory-user linking via hierarchical spatio-temporal attention networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hi-PART: Going beyond graph pooling with hierarchical
partition tree for graph-level representation learning. <em>TKDD</em>,
<em>18</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3636429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph pooling refers to the operation that maps a set of node representations into a compact form for graph-level representation learning. However, existing graph pooling methods are limited by the power of the Weisfeiler–Lehman (WL) test in the performance of graph discrimination. In addition, these methods often suffer from hard adaptability to hyper-parameters and training instability. To address these issues, we propose Hi-PART, a simple yet effective graph neural network (GNN) framework with Hi erarchical Par tition T ree (HPT). In HPT, each layer is a partition of the graph with different levels of granularities that are going toward a finer grain from top to bottom. Such an exquisite structure allows us to quantify the graph structure information contained in HPT with the aid of structural information theory. Algorithmically, by employing GNNs to summarize node features into the graph feature based on HPT’s hierarchical structure, Hi-PART is able to adequately leverage the graph structure information and provably goes beyond the power of the WL test. Due to the separation of HPT optimization from graph representation learning, Hi-PART involves the height of HPT as the only extra hyper-parameter and enjoys higher training stability. Empirical results on graph classification benchmarks validate the superior expressive power and generalization ability of Hi-PART compared with state-of-the-art graph pooling approaches.},
  archive      = {J_TKDD},
  doi          = {10.1145/3636429},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Hi-PART: Going beyond graph pooling with hierarchical partition tree for graph-level representation learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the learning difficulty of data: Theory and
measure. <em>TKDD</em>, <em>18</em>(4), 1–37. (<a
href="https://doi.org/10.1145/3636512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {‘‘Easy/hard sample” is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.},
  archive      = {J_TKDD},
  doi          = {10.1145/3636512},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-37},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Exploring the learning difficulty of data: Theory and measure},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image hash layer triggered CNN framework for wafer map
failure pattern retrieval and classification. <em>TKDD</em>,
<em>18</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3638053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning methods are often used in wafer map failure pattern classification. CNN requires less feature engineering but still needs preprocessing, e.g., denoising and resizing. Denoising is used to improve the quality of the input data, and resizing is used to transform the input into an identical size when the input data sizes are various. However, denoising and resizing may distort the original data information. Nevertheless, CNN-based applications are focusing on studying different feature map architectures and the input data manipulation is less attractive. In this study, we proposed an image hash layer triggered CNN framework for wafer map failure pattern retrieval and classification. The motivation and novelty are to design a CNN layer that can play as a resizing, information retrieval-preservation method in one step. The experiments proved that the proposed hash layer can retrieve the failure pattern information while maintaining the classification performance even though the input data size is decreased significantly. In the meantime, it can prevent overfitting, false negatives, and false positives, and save computing costs to a certain extent.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638053},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Image hash layer triggered CNN framework for wafer map failure pattern retrieval and classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ArieL: Adversarial graph contrastive learning.
<em>TKDD</em>, <em>18</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3638054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning is an effective unsupervised method in graph representation learning. The key component of contrastive learning lies in the construction of positive and negative samples. Previous methods usually utilize the proximity of nodes in the graph as the principle. Recently, the data-augmentation-based contrastive learning method has advanced to show great power in the visual domain, and some works have extended this method from images to graphs. However, unlike the data augmentation on images, the data augmentation on graphs is far less intuitive and it is much harder to provide high-quality contrastive samples, which leaves much space for improvement. In this work, by introducing an adversarial graph view for data augmentation, we propose a simple but effective method, Adversarial Graph Contrastive Learning ( ArieL ), to extract informative contrastive samples within reasonable constraints. We develop a new technique called information regularization for stable training and use subgraph sampling for scalability. We generalize our method from node-level contrastive learning to the graph level by treating each graph instance as a super-node. ArieL consistently outperforms the current graph contrastive learning methods for both node-level and graph-level classification tasks on real-world datasets. We further demonstrate that ArieL is more robust in the face of adversarial attacks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638054},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {ArieL: Adversarial graph contrastive learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-training question embeddings for improving knowledge
tracing with self-supervised bi-graph co-contrastive learning.
<em>TKDD</em>, <em>18</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3638055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning high-quality vector representations (aka. embeddings) of educational questions lies at the core of knowledge tracing (KT), which defines a task of estimating students’ knowledge states by predicting the probability that they correctly answer questions. Although existing KT efforts have leveraged question information to achieve remarkable improvements, most of them learn question embeddings by following the supervised learning paradigm. In this article, we propose a novel question embedding pre-training method for improving knowledge tracing with self-supervised Bi -graph Co -contrastive learning ( BiCo ). Technically, on the basis of self-supervised learning paradigm, we first select two similar but distinct views (i.e., representing objective and subjective semantic perspectives) as the semantic source of question embeddings. Then, we design a primary task (structure recovery) together with two auxiliary tasks (question difficulty recovery and contrastive learning) to further enhance the representativeness of questions. Finally, extensive experiments conducted on two real-world datasets show BiCo has a higher expressive power that enables KT methods to effectively predict students’ performances.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638055},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Pre-training question embeddings for improving knowledge tracing with self-supervised bi-graph co-contrastive learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributional learning for network alignment with global
constraints. <em>TKDD</em>, <em>18</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3638056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network alignment, pairing corresponding nodes across the source and target networks, plays an important role in many data mining tasks. Extensive studies focus on learning node embeddings across different networks in a unified space. However, these methods have not taken the large structural discrepancy between aligned nodes into account and, thus, are largely confined by the deterministic representations of nodes. In this work, we propose a novel network alignment framework highlighted by distributional learning and globally optimal alignment. By modeling the uncertainty of each node by Gaussian distribution, our framework builds similarity matrices on the Wasserstein distance between distributions and applies Sinkhorn operation, which learns the globally optimal mapping in an end-to-end fashion. We show that each integrated part of the framework contributes to the overall performance. Under a variety of experimental settings, our alignment framework shows superior accuracy and efficiency to the state-of-the-art.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638056},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Distributional learning for network alignment with global constraints},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Swarm self-supervised hypergraph embedding for
recommendation. <em>TKDD</em>, <em>18</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3638058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information era brings both opportunities and challenges to information services. Confronting information overload, recommendation technology is dedicated to filtering personalized content to meet users’ requirements. The extremely sparse interaction records and their imbalanced distribution become a big obstacle to building a high-quality recommendation model. In this article, we propose a swarm self-supervised hypergraph embedding (SHE) model to predict users’ interests by hypergraph convolution and self-supervised discrimination. SHE builds a hypergraph with multiple interest clues to alleviate the interaction sparsity issue and performs interest propagation to embed CF signals in hybrid learning on the hypergraph. It follows an auxiliary local view by similar hypergraph construction and interest propagation to restrain unnecessary propagation between user swarms. Besides, interest contrast further inserts self-discrimination to deal with long-tail bias issue and enhance interest modeling, which aid recommendation by a multi-task learning optimization. Experiments on public datasets show that the proposed SHE outperforms the state-of-the-art models demonstrating the effectiveness of hypergraph-based interest propagation and swarm-aware interest contrast to enhance embedding for recommendation.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638058},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Swarm self-supervised hypergraph embedding for recommendation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Traceable group-wise self-optimizing feature transformation
learning: A dual optimization perspective. <em>TKDD</em>,
<em>18</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3638059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature transformation aims to reconstruct an effective representation space by mathematically refining the existing features. It serves as a pivotal approach to combat the curse of dimensionality, enhance model generalization, mitigate data sparsity, and extend the applicability of classical models. Existing research predominantly focuses on domain knowledge-based feature engineering or learning latent representations. However, these methods, while insightful, lack full automation and fail to yield a traceable and optimal representation space. An indispensable question arises: Can we concurrently address these limitations when reconstructing a feature space for a machine learning task? Our initial work took a pioneering step towards this challenge by introducing a novel self-optimizing framework. This framework leverages the power of three cascading reinforced agents to automatically select candidate features and operations for generating improved feature transformation combinations. Despite the impressive strides made, there was room for enhancing its effectiveness and generalization capability. In this extended journal version, we advance our initial work from two distinct yet interconnected perspectives: 1) We propose a refinement of the original framework, which integrates a graph-based state representation method to capture the feature interactions more effectively and develop different Q-learning strategies to alleviate Q-value overestimation further. 2) We utilize a new optimization technique (actor-critic) to train the entire self-optimizing framework in order to accelerate the model convergence and improve the feature transformation performance. Finally, to validate the improved effectiveness and generalization capability of our framework, we perform extensive experiments and conduct comprehensive analyses. These provide empirical evidence of the strides made in this journal version over the initial work, solidifying our framework’s standing as a substantial contribution to the field of automated feature transformation. To improve the reproducibility, we have released the associated code and data by the Github link https://github.com/coco11563/TKDD2023_code.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638059},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Traceable group-wise self-optimizing feature transformation learning: A dual optimization perspective},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized federated learning with layer-wise feature
transformation via meta-learning. <em>TKDD</em>, <em>18</em>(4), 1–21.
(<a href="https://doi.org/10.1145/3638252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning enables multiple clients to collaboratively learn machine learning models in a privacy-preserving manner. However, in real-world scenarios, a key challenge encountered in federated learning is the statistical heterogeneity among clients. Existing work mainly focused on a single global model shared across the clients, making it hard to generalize well to all clients due to the large discrepancy in the data distributions. To address this challenge, we propose pFedLT , a novel approach that can adapt the single global model to different data distributions. Specifically, we propose to perform a pluggable layer-wise transformation during the local update phase based on scaling and shifting operations. In particular, these operations are learned with a meta-learning strategy. By doing so, pFedLT can capture the diversity of data distribution among clients, therefore, can generalize well even when the data distributions among clients exhibit high statistical heterogeneity. We conduct extensive experiments on synthetic and real-world datasets (MNIST, Fashion_MNIST, CIFAR-10, and Office+Caltech10) under different Non-IID settings. Experimental results demonstrate that pFedLT significantly improves the model accuracy by up to 11.67% and reduces the communication costs compared with state-of-the-art approaches.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638252},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Personalized federated learning with layer-wise feature transformation via meta-learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-based text classification by contrastive learning with
text-level graph augmentation. <em>TKDD</em>, <em>18</em>(4), 1–21. (<a
href="https://doi.org/10.1145/3638353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text Classification (TC) is a fundamental task in the information retrieval community. Nowadays, the mainstay TC methods are built on the deep neural networks, which can learn much more discriminative text features than the traditional shallow learning methods. Among existing deep TC methods, the ones based on Graph Neural Network (GNN) have attracted more attention due to the superior performance. Technically, the GNN-based TC methods mainly transform the full training dataset to a graph of texts; however, they often neglect the dependency between words, so as to miss potential semantic information of texts, which may be significant to exactly represent them. To solve the aforementioned problem, we generate graphs of words instead, so as to capture the dependency information of words. Specifically, each text is translated into a graph of words, where neighboring words are linked. We learn the node features of words by a GNN-like procedure and then aggregate them as the graph feature to represent the current text. To further improve the text representations, we suggest a contrastive learning regularization term. Specifically, we generate two augmented text graphs for each original text graph, we constrain the representations of the two augmented graphs from the same text close and the ones from different texts far away. We propose various techniques to generate the augmented graphs. Upon those ideas, we develop a novel deep TC model, namely Text-level Graph Networks with Contrastive Learning (TGN cl ). We conduct a number of experiments to evaluate the proposed TGN cl model. The empirical results demonstrate that TGN cl can outperform the existing state-of-the-art TC models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638353},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Graph-based text classification by contrastive learning with text-level graph augmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information-aware multi-view outlier detection.
<em>TKDD</em>, <em>18</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3638354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of multi-view learning, multi-view outlier detection has received increasing attention in recent years. However, the current research still faces two challenges: (1) The current research lacks theoretical analysis tools for multi-view outliers. (2) Most current multi-view outlier detection algorithms are based on shallow structural assumptions of the data, such as cluster assumptions and subspace assumptions, thus they are not suitable for more complex data distributions. In addressing these two issues, this article proposes three occurrence mechanisms of multi-view outlier, which serve as foundational theoretical analysis tools for multi-view outliers. Utilizing proposed mechanisms, we analyze the impact of multi-view outliers and the information structure of multi-view data and validate our findings through experiments. Finally, we propose a novel algorithm referred to as Information-Aware Multi-View Outlier Detection (IAMOD). In contrast to other methods, IAMOD focuses on the information structure of multi-view data without relying on shallow structural assumptions. By learning a compact representation of the sample that is semantically rich and non-redundant, IAMOD can accurately identify multi-view outliers by comparing the consistency of the representations’ neighbors and views. Extensive experimental results demonstrate that our approach outperforms several state-of-the-art multi-view outlier detection methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638354},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Information-aware multi-view outlier detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PU-detector: A PU learning-based framework for real money
trading detection in MMORPG. <em>TKDD</em>, <em>18</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3638561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive multiplayer online role-playing games (MMORPG) have been becoming one of the most popular and exciting online games. In recent years, a cheating phenomenon called real money trading (RMT) has arisen and damaged the fantasy world in many ways. RMT is the sale of in-game items, currency, or even characters to earn real money, breaking the balance of the game economy ecosystem and damaging the game experience. Therefore, some studies have emerged to address the problem of RMT detection. However, they cannot well handle the label uncertainty problem in practice, where there are only labeled RMT samples (positive samples) and unlabeled samples, which could either be RMT samples or normal transactions (negative samples). Meanwhile, the trading relationship between RMTers is modeled in a simple way, leading to some normal transactions being falsely classified as RMT. In this article, we propose PU-Detector, a novel framework based on PU learning (learning from positive and unlabeled data) for RMT detection, considering the fact that there are only labeled RMT samples and other unlabeled transactions. We first automatically estimate the likelihood of one transaction being RMT by developing an improved PU learning method and proposing an assessment rule. Sequentially, we use the estimated likelihood as edge weight to construct a trading graph to learn trader representation. Then, with the trader representations and basic trading features, we detect RMT samples by the improved PU learning method. PU-Detector is evaluated on a large-scale real world dataset consisting of 33,809,956 transaction logs generated by 43,217 unique players. Compared with other approaches, it achieves the state-of-the-art performance and demonstrates its advantages in detecting underlying RMT samples.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638561},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {PU-detector: A PU learning-based framework for real money trading detection in MMORPG},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning global and multi-granularity local representation
with MLP for sequential recommendation. <em>TKDD</em>, <em>18</em>(4),
1–15. (<a href="https://doi.org/10.1145/3638562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation aims to predict the next item of interest to users based on their historical behavior data. Usually, users’ global and local preferences jointly affect the final recommendation result in different ways. Most existing works use transformers to globally model sequences, which makes them face the dilemma of quadratic computational complexity when dealing with long sequences. Moreover, the scope setting of the user’s local preference is usually static and single, and cannot cover richer multi-level local semantics. To this end, we proposed a parallel architecture for capturing global representation and M ulti-granularity L ocal dependencies with M LP for sequential Rec ommendation ( MLM4Rec ). For global representation, we utilize modified MLP-Mixer to capture global information of user sequences due to its simplicity and efficiency. For local representation, we incorporate convolution into MLP and propose a multi-granularity local awareness mechanism for capturing richer local semantic information. Moreover, we introduced a weight pooling method to adaptively fuse local-global representations instead of directly concatenation. Our model has the advantages of low complexity and high efficiency thanks to its simple MLP structure. Experimental results on three public datasets demonstrate the effectiveness of our proposed model. Our code is available here 1 .},
  archive      = {J_TKDD},
  doi          = {10.1145/3638562},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learning global and multi-granularity local representation with MLP for sequential recommendation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple-instance learning from triplet comparison bags.
<em>TKDD</em>, <em>18</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3638776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-instance learning (MIL) solves the problem where training instances are grouped in bags, and a binary (positive or negative) label is provided for each bag. Most of the existing MIL studies need fully labeled bags for training an effective classifier, while it could be quite hard to collect such data in many real-world scenarios, due to the high cost of data labeling process. Fortunately, unlike fully labeled data, triplet comparison data can be collected in a more accurate and human-friendly way. Therefore, in this article, we for the first time investigate MIL from only triplet comparison bags , where a triplet (X a , X b , X c ) contains the weak supervision information that bag X a is more similar to X b than to X c . To solve this problem, we propose to train a bag-level classifier by the empirical risk minimization framework and theoretically provide a generalization error bound. We also show that a convex formulation can be obtained only when specific convex binary losses such as the square loss and the double hinge loss are used. Extensive experiments validate that our proposed method significantly outperforms other baselines.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638776},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multiple-instance learning from triplet comparison bags},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Concept drift adaptation by exploiting drift type.
<em>TKDD</em>, <em>18</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3638777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is a phenomenon where the distribution of data streams changes over time. When this happens, model predictions become less accurate. Hence, models built in the past need to be re-learned for the current data. Two design questions need to be addressed in designing a strategy to re-learn models: which type of concept drift has occurred, and how to utilize the drift type to improve re-learning performance. Existing drift detection methods are often good at determining when drift has occurred. However, few retrieve information about how the drift came to be present in the stream. Hence, determining the impact of the type of drift on adaptation is difficult. Filling this gap, we designed a framework based on a lazy strategy called Type-Driven Lazy Drift Adaptor (Type-LDA). Type-LDA first retrieves information about both how and when a drift has occurred, then it uses this information to re-learn the new model. To identify the type of drift, a drift type identifier is pre-trained on synthetic data of known drift types. Furthermore, a drift point locator locates the optimal point of drift via a sharing loss. Hence, Type-LDA can select the optimal point, according to the drift type, to re-learn the new model. Experiments validate Type-LDA on both synthetic data and real-world data, and the results show that accurately identifying drift type can improve adaptation accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638777},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Concept drift adaptation by exploiting drift type},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diverse structure-aware relation representation in
cross-lingual entity alignment. <em>TKDD</em>, <em>18</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3638778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-lingual entity alignment (CLEA) aims to find equivalent entity pairs between knowledge graphs (KGs) in different languages. It is an important way to connect heterogeneous KGs and facilitate knowledge completion. Existing methods have found that incorporating relations into entities can effectively improve KG representation and benefit entity alignment, and these methods learn relation representation depending on entities, which cannot capture the diverse structures of relations. However, multiple relations in KG form diverse structures, such as adjacency structure and ring structure. This diversity of relation structures makes the relation representation challenging. Therefore, we propose to construct the weighted line graphs to model the diverse structures of relations and learn relation representation independently from entities. Especially, owing to the diversity of adjacency structures and ring structures, we propose to construct adjacency line graph and ring line graph, respectively, to model the structures of relations and to further improve entity representation. In addition, to alleviate the hubness problem in alignment, we introduce the optimal transport into alignment and compute the distance matrix in a different way. From a global perspective, we calculate the optimal 1-to-1 alignment bi-directionally to improve the alignment accuracy. Experimental results on two benchmark datasets show that our proposed method significantly outperforms state-of-the-art CLEA methods in both supervised and unsupervised manners.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638778},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Diverse structure-aware relation representation in cross-lingual entity alignment},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HITS-based propagation paradigm for graph neural networks.
<em>TKDD</em>, <em>18</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3638779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a new propagation paradigm based on the principle of Hyperlink-Induced Topic Search (HITS) algorithm. The HITS algorithm utilizes the concept of a “self-reinforcing” relationship of authority-hub. Using HITS, the centrality of nodes is determined via repeated updates of authority-hub scores that converge to a stationary distribution. Unlike PageRank-based propagation methods, which rely solely on the idea of authorities (in-links), HITS considers the relevance of both authorities (in-links) and hubs (out-links), thereby allowing for a more informative graph learning process. To segregate node prediction and propagation, we use a Multilayer Perceptron in combination with a HITS-based propagation approach and propose two models: HITS-GNN and HITS-GNN+. We provided additional validation of our models’ efficacy by performing an ablation study to assess the performance of authority-hub in independent models. Moreover, the effect of the main hyper-parameters and normalization is also analyzed to uncover how these techniques influence the performance of our models. Extensive experimental results indicate that the proposed approach significantly improves baseline methods on the graph (citation network) benchmark datasets by a decent margin for semi-supervised node classification, which can aid in predicting the categories (labels) of scientific articles not exclusively based on their content but also based on the type of articles they cite.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638779},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {HITS-based propagation paradigm for graph neural networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NNC-GCN: Neighbours-to-neighbours contrastive graph
convolutional network for semi-supervised classification. <em>TKDD</em>,
<em>18</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3638780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning (CL) is a popular learning paradigm in deep learning, which uses contrastive principle to learn low-dimensional embeddings, and has been applied in Graph Neural Networks (GNNs) successfully. Existing works of contrastive multi-view GNNs usually focus on point-to-point contrastive learning strategies. However, they neglect the local information in neighbors, which brings isolated positive samples. The quality of selected positive samples is hard to evaluate, and these samples may lead to invalid contrastiveness. Therefore, we propose a simple and efficient neighbors-to-neighbors contrastive graph neural network (NNC-GCN), which constructs a consistent multi-view by using the topologies of original input graphs. Moreover, we raise a new learning problem of unlabeled data base on these constructed multi-view topologies and propose a loss function NNC-InfoNCE to guide its learning process. The NNC-InfoNCE is an improved version of InfoNCE, which can be adapted to neighborhood-level contrast learning. Specifically, the neighborhoods and the remaining nodes of the selected anchor are weighted and treated as positive and negative sample sets. The experimental results show that our method is effective on public benchmark datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638780},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {NNC-GCN: Neighbours-to-neighbours contrastive graph convolutional network for semi-supervised classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rationalizing graph neural networks with data augmentation.
<em>TKDD</em>, <em>18</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3638781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph rationales are representative subgraph structures that best explain and support the graph neural network (GNN) predictions. Graph rationalization involves the joint identification of these subgraphs during GNN training, resulting in improved interpretability and generalization. GNN is widely used for node-level tasks such as paper classification and graph-level tasks such as molecular property prediction. However, on both levels, little attention has been given to GNN rationalization and the lack of training examples makes it difficult to identify the optimal graph rationales. In this work, we address the problem by proposing a unified data augmentation framework with two novel operations on environment subgraphs to rationalize GNN prediction. We define the environment subgraph as the remaining subgraph after rationale identification and separation. The framework efficiently performs rationale–environment separation in the representation space for a node’s neighborhood graph or a graph’s complete structure to avoid the high complexity of explicit graph decoding and encoding. We conduct experiments on 17 datasets spanning node classification, graph classification, and graph regression. Results demonstrate that our framework is effective and efficient in rationalizing and enhancing GNNs for different levels of tasks on graphs.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638781},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Rationalizing graph neural networks with data augmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel neural ensemble architecture for on-the-fly
classification of evolving text streams. <em>TKDD</em>, <em>18</em>(4),
1–24. (<a href="https://doi.org/10.1145/3639054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study on-the-fly classification of evolving text streams in which the relation between the input data and target labels changes over time—i.e., “concept drift.” These variations decrease the model’s performance, as predictions become less accurate over time and they necessitate a more adaptable system. While most studies focus on concept drift detection and handling with ensemble approaches, the application of neural models in this area is relatively less studied. We introduce Adaptive Neural Ensemble Network ( AdaNEN ), a novel ensemble-based neural approach, capable of handling concept drift in data streams. With our novel architecture, we address some of the problems neural models face when exploited for online adaptive learning environments. Most current studies address concept drift detection and handling in numerical streams, and the evolving text stream classification remains relatively unexplored. We hypothesize that the lack of public and large-scale experimental data could be one reason. To this end, we propose a method based on an existing approach for generating evolving text streams by introducing various types of concept drifts to real-world text datasets. We provide an extensive evaluation of our proposed approach using 12 state-of-the-art baselines and 13 datasets. We first evaluate concept drift handling capability of AdaNEN and the baseline models on evolving numerical streams; this aims to demonstrate the concept drift handling capabilities of our method on a general spectrum and motivate its use in evolving text streams. The models are then evaluated in evolving text stream classification. Our experimental results show that AdaNEN consistently outperforms the existing approaches in terms of predictive performance with conservative efficiency.},
  archive      = {J_TKDD},
  doi          = {10.1145/3639054},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A novel neural ensemble architecture for on-the-fly classification of evolving text streams},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian graph local extrema convolution with long-tail
strategy for misinformation detection. <em>TKDD</em>, <em>18</em>(4),
1–21. (<a href="https://doi.org/10.1145/3639408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has become a cardinal task to identify fake information (misinformation) on social media, because it has significantly harmed the government and the public. There are many spam bots maliciously retweeting misinformation. This study proposes an efficient model for detecting misinformation with self-supervised contrastive learning. A B ayesian graph L ocal extrema C onvolution (BLC) is first proposed to aggregate node features in the graph structure. The BLC approach considers unreliable relationships and uncertainties in the propagation structure, and the differences between nodes and neighboring nodes are emphasized in the attributes. Then, a new long-tail strategy for matching long-tail users with the global social network is advocated to avoid over-concentration on high-degree nodes in graph neural networks. Finally, the proposed model is experimentally evaluated with two public Twitter datasets and demonstrates that the proposed long-tail strategy significantly improves the effectiveness of existing graph-based methods in terms of detecting misinformation. The robustness of BLC has also been examined on three graph datasets and demonstrates that it consistently outperforms traditional algorithms when perturbed by 15% of a dataset.},
  archive      = {J_TKDD},
  doi          = {10.1145/3639408},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Bayesian graph local extrema convolution with long-tail strategy for misinformation detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semantics-enhanced topic modelling technique:
Semantic-LDA. <em>TKDD</em>, <em>18</em>(4), 1–27. (<a
href="https://doi.org/10.1145/3639409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic modelling is a beneficial technique used to discover latent topics in text collections. But to correctly understand the text content and generate a meaningful topic list, semantics are important. By ignoring semantics, that is, not attempting to grasp the meaning of the words, most of the existing topic modelling approaches can generate some meaningless topic words. Even existing semantic-based approaches usually interpret the meanings of words without considering the context and related words. In this article, we introduce a semantic-based topic model called semantic-LDA that captures the semantics of words in a text collection using concepts from an external ontology. A new method is introduced to identify and quantify the concept–word relationships based on matching words from the input text collection with concepts from an ontology without using pre-calculated values from the ontology that quantify the relationships between the words and concepts. These pre-calculated values may not reflect the actual relationships between words and concepts for the input collection, because they are derived from datasets used to build the ontology rather than from the input collection itself. Instead, quantifying the relationship based on the word distribution in the input collection is more realistic and beneficial in the semantic capture process. Furthermore, an ambiguity handling mechanism is introduced to interpret the unmatched words, that is, words for which there are no matching concepts in the ontology. Thus, this article makes a significant contribution by introducing a semantic-based topic model that calculates the word–concept relationships directly from the input text collection. The proposed semantic-based topic model and an enhanced version with the disambiguation mechanism were evaluated against a set of state-of-the-art systems, and our approaches outperformed the baseline systems in both topic quality and information filtering evaluations.},
  archive      = {J_TKDD},
  doi          = {10.1145/3639409},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A semantics-enhanced topic modelling technique: Semantic-LDA},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding subgraphs with maximum total density and limited
overlap in weighted hypergraphs. <em>TKDD</em>, <em>18</em>(4), 1–21.
(<a href="https://doi.org/10.1145/3639410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding dense subgraphs in large (hyper)graphs is a key primitive in a variety of real-world application domains, encompassing social network analytics, event detection, biology, and finance. In most such applications, one typically aims at finding several (possibly overlapping) dense subgraphs, which might correspond to communities in social networks or interesting events. While a large amount of work is devoted to finding a single densest subgraph, perhaps surprisingly, the problem of finding several dense subgraphs in weighted hypergraphs with limited overlap has not been studied in a principled way, to the best of our knowledge. In this work, we define and study a natural generalization of the densest subgraph problem in weighted hypergraphs, where the main goal is to find at most k subgraphs with maximum total aggregate density, while satisfying an upper bound on the pairwise weighted Jaccard coefficient, i.e., the ratio of weights of intersection divided by weights of union on two nodes sets of the subgraphs. After showing that such a problem is NP-Hard, we devise an efficient algorithm that comes with provable guarantees in some cases of interest, as well as, an efficient practical heuristic. Our extensive evaluation on large real-world hypergraphs confirms the efficiency and effectiveness of our algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3639410},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Finding subgraphs with maximum total density and limited overlap in weighted hypergraphs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utility-aware privacy perturbation for training data.
<em>TKDD</em>, <em>18</em>(4), 1–21. (<a
href="https://doi.org/10.1145/3639411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data perturbation under differential privacy constraint is an important approach of protecting data privacy. However, as the data dimensions increase, the privacy budget allocated to each dimension decreases and thus the amount of noise added increases, which eventually leads to lower data utility in training tasks. To protect the privacy of training data while enhancing data utility, we propose a Utility-aware training data Privacy Perturbation scheme based on attribute Partition and budget Allocation (UPPPA). UPPPA includes three procedures: the quantification of attribute privacy and attribute importance, attribute partition, and budget allocation. The quantification of attribute privacy and attribute importance based on information entropy and attribute correlation provide an arithmetic basis for attribute partition and budget allocation. During the attribute partition, all attributes of training data are classified into high and low classes to achieve privacy amplification and utility enhancement. During the budget allocation, a γ-privacy model is proposed to balance data privacy and data utility so as to provide privacy constraint and guide budget allocation. Three comprehensive sets of real-world data are applied to evaluate the performance of UPPPA. Experiments and privacy analysis show that our scheme can achieve the tradeoff between privacy and utility.},
  archive      = {J_TKDD},
  doi          = {10.1145/3639411},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Utility-aware privacy perturbation for training data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing heterogeneous knowledge graph completion with a
novel GAT-based approach. <em>TKDD</em>, <em>18</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3639472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) play a vital role in enhancing search results and recommendation systems. With the rapid increase in the size of KGs, they are becoming inaccurate and incomplete. This problem can be solved by the KG completion methods, of which graph attention network (GAT)-based methods stand out because of their superior performance. However, existing GAT-based KG completion methods often suffer from overfitting issues when dealing with heterogeneous KGs, primarily due to the unbalanced number of samples. Additionally, these methods demonstrate poor performance in predicting the tail (head) entity that shares the same relation and head (tail) entity with others. To solve these problems, we propose GATH, a novel GAT -based method designed for H eterogeneous KGs. GATH incorporates two separate attention network modules that work synergistically to predict the missing entities. We also introduce novel encoding and feature transformation approaches, enabling the robust performance of GATH in scenarios with imbalanced samples. Comprehensive experiments are conducted to evaluate GATH’s performance. Compared with the existing state-of-the-art GAT-based model on Hits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the FB15K-237 dataset and by 4.5% and 14.6% on the WN18RR dataset, respectively.},
  archive      = {J_TKDD},
  doi          = {10.1145/3639472},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Enhancing heterogeneous knowledge graph completion with a novel GAT-based approach},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiGRASS: Directed graph spectral sparsification via
spectrum-preserving symmetrization. <em>TKDD</em>, <em>18</em>(4), 1–25.
(<a href="https://doi.org/10.1145/3639568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent spectral graph sparsification research aims to construct ultra-sparse subgraphs for preserving the original graph spectral (structural) properties, such as the first few Laplacian eigenvalues and eigenvectors, which has led to the development of a variety of nearly-linear time numerical and graph algorithms. However, there is very limited progress for spectral sparsification of directed graphs. In this work, we prove the existence of nearly-linear-sized spectral sparsifiers for directed graphs under certain conditions. Furthermore, we introduce a practically-efficient spectral algorithm (diGRASS) for sparsifying real-world, large-scale directed graphs leveraging spectral matrix perturbation analysis. The proposed method has been evaluated using a variety of directed graphs obtained from real-world applications, showing promising results for solving directed graph Laplacians, spectral partitioning of directed graphs, and approximately computing (personalized) PageRank vectors.},
  archive      = {J_TKDD},
  doi          = {10.1145/3639568},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DiGRASS: Directed graph spectral sparsification via spectrum-preserving symmetrization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Laplacian change point detection for single and multi-view
dynamic graphs. <em>TKDD</em>, <em>18</em>(3), 1–32. (<a
href="https://doi.org/10.1145/3631609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic graphs are rich data structures that are used to model complex relationships between entities over time. In particular, anomaly detection in temporal graphs is crucial for many real-world applications such as intrusion identification in network systems, detection of ecosystem disturbances, and detection of epidemic outbreaks. In this article, we focus on change point detection in dynamic graphs and address three main challenges associated with this problem: (i) how to compare graph snapshots across time, (ii) how to capture temporal dependencies, and (iii) how to combine different views of a temporal graph. To solve the above challenges, we first propose Laplacian Anomaly Detection (LAD) which uses the spectrum of graph Laplacian as the low dimensional embedding of the graph structure at each snapshot. LAD explicitly models short-term and long-term dependencies by applying two sliding windows. Next, we propose MultiLAD, a simple and effective generalization of LAD to multi-view graphs. MultiLAD provides the first change point detection method for multi-view dynamic graphs. It aggregates the singular values of the normalized graph Laplacian from different views through the scalar power mean operation. Through extensive synthetic experiments, we show that (i) LAD and MultiLAD are accurate and outperforms state-of-the-art baselines and their multi-view extensions by a large margin, (ii) MultiLAD’s advantage over contenders significantly increases when additional views are available, and (iii) MultiLAD is highly robust to noise from individual views. In five real-world dynamic graphs, we demonstrate that LAD and MultiLAD identify significant events as top anomalies such as the implementation of government COVID-19 interventions which impacted the population mobility in multi-view traffic networks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3631609},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-32},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Laplacian change point detection for single and multi-view dynamic graphs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph domain adaptation: A generative view. <em>TKDD</em>,
<em>18</em>(3), 1–24. (<a
href="https://doi.org/10.1145/3631712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed tremendous interest in deep learning on graph-structured data. Due to the high cost of collecting labeled graph-structured data, domain adaptation is important to supervised graph learning tasks with limited samples. However, current graph domain adaptation methods are generally adopted from traditional domain adaptation tasks, and the properties of graph-structured data are not well utilized. For example, the observed social networks on different platforms are controlled not only by the different crowds or communities but also by domain-specific policies and background noise. Based on these properties in graph-structured data, we first assume that the graph-structured data generation process is controlled by three independent types of latent variables, i.e., the semantic latent variables, the domain latent variables, and the random latent variables. Based on this assumption, we propose a disentanglement-based unsupervised domain adaptation method for the graph-structured data, which applies variational graph auto-encoders to recover these latent variables and disentangles them via three supervised learning modules. Extensive experimental results on two real-world datasets in the graph classification task reveal that our method not only significantly outperforms the traditional domain adaptation methods and the disentangled-based domain adaptation methods but also outperforms the state-of-the-art graph domain adaptation algorithms. The code is available at https://github.com/rynewu224/GraphDA .},
  archive      = {J_TKDD},
  doi          = {10.1145/3631712},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Graph domain adaptation: A generative view},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causality-based fair multiple decision by response
functions. <em>TKDD</em>, <em>18</em>(3), 1–23. (<a
href="https://doi.org/10.1145/3632529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent trend of fair machine learning is to build a decision model subjected to causality-based fairness requirements, which concern with the causality between sensitive attributes and decisions. Almost all (if not all) solutions focus on a single fair decision model and assume no hidden confounder to model causal effects in a too simplified way. However, multiple interdependent decision models are actually used and discrimination may transmit among them. The hidden confounder is another inescapable fact and causal effects cannot be computed from observational data in the unidentifiable situation. To address these problems, we propose a method called CMFL (Causality-based Multiple Fairness Learning). CMFL parameterizes the causal model by response-function variables, whose distributions capture the randomness of causal models. CMFL treats each classifier as a soft intervention to infer the post-intervention distribution, and combines the fairness constraints with the classification loss to train multiple decision classifiers. In this way, all classifiers can make approximately fair decisions. Experiments on synthetic and benchmark datasets confirm its effectiveness, the response-function variables can deal with the unidentifiable issue and hidden confounders.},
  archive      = {J_TKDD},
  doi          = {10.1145/3632529},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Causality-based fair multiple decision by response functions},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSAB: User behavioral pattern modeling in sequential
recommendation by learning self-attention bias. <em>TKDD</em>,
<em>18</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3632625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the weight of a self-attention model is not affected by the sequence interval, it can more accurately and completely describe the user interests, so it is widely used in processing sequential recommendation. However, the mainstream self-attention model focuses on the similarity between items when calculating the attention weight of user behavioral patterns but fails to reflect the impact of user sudden drift decisions on the model in time. In this article, we introduce a bias strategy in the self-attention module, referred to as Learning Self-Attention Bias (LSAB) to more accurately learn the fast-changing user behavioral patterns. The introduction of LSAB allows for the adjustment of bias resulting from self-attention weights, leading to enhanced prediction performance in sequential recommendation. In addition, this article designs four attention-weight bias types catering to diverse user behavior preferences. After testing on the benchmark datasets, each bias strategy in LSAB is useful for state-of-the-art and can improve the performance of the models by nearly 5% on average. The source code listing is publicly available at https://gitee.com/kyle-liao/lsab .},
  archive      = {J_TKDD},
  doi          = {10.1145/3632625},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {LSAB: User behavioral pattern modeling in sequential recommendation by learning self-attention bias},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving non-negative matrix factorization with
outliers. <em>TKDD</em>, <em>18</em>(3), 1–26. (<a
href="https://doi.org/10.1145/3632961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization is a popular unsupervised machine learning algorithm for extracting meaningful features from inherently non-negative data. Such data often contain privacy-sensitive user information. Additionally, the dataset can contain outliers, which may lead to extracting sub-optimal features from the data. It is, therefore, necessary to address these two issues while analyzing privacy-sensitive data that may contain outliers. In this work, we develop a non-negative matrix factorization algorithm in the privacy-preserving framework that (i) considers the presence of outliers in the data, and (ii) can achieve results comparable to those of the non-private algorithm. We design our method in such a way that one has the control to select the degree of privacy grantee based on the required utility gap. We show the effectiveness of our proposed algorithm’s performance on six real and diverse datasets. The experimental results show that our proposed method can achieve a performance that closely approximates the performance of the non-private algorithm under some parameter choices, while ensuring strict privacy guarantees.},
  archive      = {J_TKDD},
  doi          = {10.1145/3632961},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Privacy-preserving non-negative matrix factorization with outliers},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter-agnostic deep graph clustering. <em>TKDD</em>,
<em>18</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3633783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep graph clustering, efficiently dividing nodes into multiple disjoint clusters in an unsupervised manner, has become a crucial tool for analyzing ubiquitous graph data. Existing methods have acquired impressive clustering effects by optimizing the clustering network under the parametric condition—predefining the true number of clusters ( K tr ). However, K tr is inaccessible in pure unsupervised scenarios, in which existing methods are incapable of inferring the number of clusters ( K ), causing limited feasibility. This article proposes the first Parameter-Agnostic Deep Graph Clustering method (PADGC), which consists of two core modules: K -guidence clustering and topological-hierarchical inference, to infer K efficiently and gain impressive clustering predictions. Specifically, K -guidence clustering is employed to optimize the cluster assignments and discriminative embeddings in a mutual promotion manner under the latest updated K , even though K may deviate from K tr . In turn, such optimized cluster assignments are utilized to explore more accurate K in the topological-hierarchical inference, which can split the dispersive clusters and merge the coupled ones. In this way, these two modules are complementarily optimized until generating the final convergent K and discriminative cluster assignments. Extensive experiments on several benchmarks, including graphs and images, can demonstrate the superiority of our method. The mean values of our inferred K , in 11 out of 12 datasets, deviates from K tr by less than 1. Our method can also achieve competitive clustering effects with existing parametric deep graph clustering.},
  archive      = {J_TKDD},
  doi          = {10.1145/3633783},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Parameter-agnostic deep graph clustering},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local overlapping spatial-aware community detection.
<em>TKDD</em>, <em>18</em>(3), 1–21. (<a
href="https://doi.org/10.1145/3634707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local spatial-aware community detection refers to detecting a spatial-aware community for a given node using local information. A spatial-aware community means that nodes in the community are tightly connected in structure, and their locations are close to each other. Existing studies focus on detecting the local non-overlapping spatial-aware community, i.e., detecting a spatial-aware community containing the given node. However, many geosocial networks often contain overlapping spatial-aware communities. Therefore, we propose a local overlapping spatial-aware community detection (LOSCD) problem, which aims to detect all spatial-aware communities that contain a given node with local information. To address LOSCD problem, we design an algorithm based on Spatial Modularity and Edge Similarity, called SMES. SMES contains two processes: spatial expansion and structure detection. The spatial expansion process involves using spatial modularity to identify nodes that are spatially close, while the structural detection process employs edge similarity to identify nodes that are structurally close. Experimental results demonstrate that SMES outperforms comparison algorithms in terms of both structural and spatial cohesiveness.},
  archive      = {J_TKDD},
  doi          = {10.1145/3634707},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Local overlapping spatial-aware community detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse grid imputation using unpaired imprecise auxiliary
data: Theory and application to PM2.5 estimation. <em>TKDD</em>,
<em>18</em>(3), 1–26. (<a
href="https://doi.org/10.1145/3634751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse grid imputation (SGI) is a challenging problem, as its goal is to infer the values of the entire grid from a limited number of cells with values. Traditionally, the problem is solved using regression methods such as KNN and kriging, whereas in the real world, there is often extra information—usually imprecise—that can aid inference and yield better performance. In the SGI problem, in addition to the limited number of fixed grid cells with precise target domain values, there are contextual data and imprecise observations over the whole grid. To solve this problem, we propose a distribution estimation theory for the whole grid and realize the theory via the composition architecture of the Target-Embedding and the Contextual CycleGAN trained with contextual information and imprecise observations. Contextual CycleGAN is structured as two generator–discriminator pairs and uses different types of contextual loss to guide the training. We consider the real-world problem of fine-grained PM2.5 inference with realistic settings: a few (less than 1%) grid cells with precise PM2.5 data and all grid cells with contextual information concerning weather and imprecise observations from satellites and microsensors. The task is to infer reasonable values for all grid cells. As there is no ground truth for empty cells, out-of-sample mean squared error and Jensen–Shannon divergence measurements are used in the empirical study. The results show that Contextual CycleGAN supports the proposed theory and outperforms the methods used for comparison.},
  archive      = {J_TKDD},
  doi          = {10.1145/3634751},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Sparse grid imputation using unpaired imprecise auxiliary data: Theory and application to PM2.5 estimation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning hierarchical task structures for few-shot graph
classification. <em>TKDD</em>, <em>18</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3635473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of few-shot graph classification targets at assigning class labels for graph samples, where only limited labeled graphs are provided for each class. To solve the problem brought by label scarcity, recent studies have proposed to adopt the prevalent few-shot learning framework to achieve fast adaptations to graph classes with limited labeled graphs. In particular, these studies typically propose to accumulate meta-knowledge across a large number of meta-training tasks, and then generalize such meta-knowledge to meta-test tasks sampled from a disjoint class set. Nevertheless, existing studies generally ignore the crucial task correlations among meta-training tasks and treat them independently. In fact, such task correlations can help promote the model generalization to meta-test tasks and result in better classification performance. On the other hand, it remains challenging to capture and utilize task correlations due to the complex components and interactions in meta-training tasks. To deal with this, we propose a novel few-shot graph classification framework FAITH to capture task correlations via learning a hierarchical task structure at different granularities. We further propose a task-specific classifier to incorporate the learned task correlations into the few-shot graph classification process. Moreover, we derive FAITH+, a variant of FAITH that can improve the sampling process for the hierarchical task structure. The extensive experiments on four prevalent graph datasets further demonstrate the superiority of FAITH and FAITH+ over other state-of-the-art baselines.},
  archive      = {J_TKDD},
  doi          = {10.1145/3635473},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learning hierarchical task structures for few-shot graph classification},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StructCoder: Structure-aware transformer for code
generation. <em>TKDD</em>, <em>18</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3636430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a recent surge of interest in automating software engineering tasks using deep learning. This article addresses the problem of code generation, in which the goal is to generate target code given source code in a different language or a natural language description. Most state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model in which both the encoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively. We not only make the encoder structure aware by leveraging the source code’s syntax tree and dataflow graph, but we also support the decoder in preserving the syntax and dataflow of the target code by introducing two novel auxiliary tasks: Abstract Syntax Tree (AST) path prediction and dataflow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder that models both syntax and dataflow to enhance the quality of generated code. The proposed StructCoder model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark and improves over baselines of similar size on the APPS code generation benchmark. Our code is publicly available at https://github.com/reddy-lab-code-research/StructCoder/ .},
  archive      = {J_TKDD},
  doi          = {10.1145/3636430},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {StructCoder: Structure-aware transformer for code generation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-stage transferable and generative crowdsourced comment
integration framework based on zero- and few-shot learning with domain
distribution alignment. <em>TKDD</em>, <em>18</em>(3), 1–43. (<a
href="https://doi.org/10.1145/3636511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online shopping has become a crucial way to encourage daily consumption, where the User-generated, or crowdsourced product comments, can offer a broad range of feedback on e-commerce products. As a result, integrating critical opinions or major attitudes from the crowdsourced comments can provide valuable feedback for marketing strategy adjustment or product-quality monitoring. Unfortunately, the scarcity of annotated ground truth on the integrated comment, or the limited gold integration reference, has incurred the infeasibility of the regular supervised-learning-based comment integration. To resolve this problem, in this article, inspired by the principle of Transfer Learning, we propose a three-stage transferable and generative crowdsourced comment integration framework ( TTGCIF ) based on zero-and-few-shot learning with the support of domain distribution alignment. The proposed framework aims at generating abstractive integrated comment in target domain via the enhanced neural text generation model, by referring the available integration resource in related source domains, to avoid the exhausted effort on resource annotation devoted to the target domain. Specifically, at the first stage, to enhance the domain transferability, representations on the crowdsourced comments have been aligned up between the source and target domain, by minimizing the domain distribution discrepancy in the kernel space. At the second stage, Zero-shot comment integration mechanism has been adopted to deal with the dilemma that none of the gold integration reference may be available in target domain. In other words, taking the sample-level semantic prototype as input, the enhanced neural text generation model in TTGCIF is trained to learn data semantic association among different domains via semantic prototype transduction, so that the “ unlabeled ” crowdsourced comments in target domain can be associated with existing integration references in related source domains. At the third stage, based on the parameters trained at the second stage, fast domain adaptation mechanism in a Few-shot manner has also been adopted by seeking most potential parameters along the gradient direction constrained by instances across multiple source domains. In this way, parameters in TTGCIF can be sensitive to any alteration on training data, ensuring that even if only few annotated resource in target domain are available for “Fine-tune,” TTGCIF can still react promptly to achieve effective target domain adaptation. According to the experimental results, TTGCIF can achieve the best transferable product comment integration performance in target domain, with fast and stable domain adaption effect depending on no more than 10% annotated resource in target domain. More importantly, even if TTGCIF has not been fine-tuned on the target domain, yet by referring to the available integration resource in related source domains, the integrated comments generated by TTGCIF on the target domain are still superior to those generated by models already fine-tuned on the target domain.},
  archive      = {J_TKDD},
  doi          = {10.1145/3636511},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-43},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Three-stage transferable and generative crowdsourced comment integration framework based on zero- and few-shot learning with domain distribution alignment},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient version space algorithms for human-in-the-loop
model development. <em>TKDD</em>, <em>18</em>(3), 1–49. (<a
href="https://doi.org/10.1145/3637443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When active learning (AL) is applied to help users develop a model on a large dataset through interactively presenting data instances for labeling, existing AL techniques often suffer from two main drawbacks: First, to reach high accuracy they may require the user to label hundreds of data instances, which is an onerous task for the user. Second, retrieving the next instance to label from a large dataset can be time-consuming, making it incompatible with the interactive nature of the human exploration process. To address these issues, we introduce a novel version-space-based active learner for kernel classifiers, which possesses strong theoretical guarantees on performance and efficient implementation in time and space. In addition, by leveraging additional insights obtained in the user labeling process, we can factorize the version space to perform active learning in a set of subspaces, which further reduces the user labeling effort. Evaluation results show that our algorithms significantly outperform state-of-the-art version space strategies, as well as a recent factorization-aware algorithm, for model development over large datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3637443},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-49},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Efficient version space algorithms for human-in-the-loop model development},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DEWP: Deep expansion learning for wind power forecasting.
<em>TKDD</em>, <em>18</em>(3), 1–21. (<a
href="https://doi.org/10.1145/3637552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind is one kind of high-efficient, environmentally-friendly, and cost-effective energy source. Wind power, as one of the largest renewable energy in the world, has been playing a more and more important role in supplying electricity. Though growing dramatically in recent years, the amount of generated wind power can be directly or latently affected by multiple uncertain factors, such as wind speed, wind direction, temperatures, and so on. More importantly, there exist very complicated dependencies of the generated power on the latent composition of these multiple time-evolving variables, which are always ignored by existing works and thus largely hinder the prediction performances. To this end, we propose DEWP , a novel D eep E xpansion learning for W ind P ower forecasting framework to carefully model the complicated dependencies with adequate expressiveness. DEWP starts with a stack-by-stack architecture, where each stack is composed of (i) a variable expansion block that makes use of convolutional layers to capture dependencies among multiple variables; (ii) a time expansion block that applies Fourier series and backcast/forecast mechanism to learn temporal dependencies in sequential patterns. These two tailored blocks expand raw inputs into different latent feature spaces which can model different levels of dependencies of time-evolving sequential data. Moreover, we propose an inference block corresponding for each stack, which applies multi-head self-attentions to acquire attentive features and maps expanded latent representations into generated wind power. In addition, to make DEWP more expressive in handling deep neural architectures, we adapt doubly residue learning to process stack-by-stack outputs. Accurate wind power forecasting (WPF) is then better achieved through fine-grained outputs by continuously removing stack residues and accumulating useful stack forecasts. Finally, we present extensive experiments in the real-world WPF application on two datasets from two different turbines, in order to demonstrate the effectiveness of our approach.},
  archive      = {J_TKDD},
  doi          = {10.1145/3637552},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DEWP: Deep expansion learning for wind power forecasting},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced fuzzy clustering for incomplete instance with
evidence combination. <em>TKDD</em>, <em>18</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3638061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering incomplete instance is still a challenging task since missing values maybe make the cluster information ambiguous, leading to the uncertainty and imprecision in results. This article investigates an enhanced fuzzy clustering with evidence combination method based on Dempster-Shafer theory (DST) to address this problem. First, the dataset is divided into several subsets, and missing values are imputed by neighbors with different weights in each subset. It aims to model missing values locally to reduce the negative impact of the bad estimations. Second, an objective function of enhanced fuzzy clustering is designed and then optimized until the best membership and reliability matrices are found. Each subset has a membership matrix that contains all sub-instances’ membership to different clusters. The fuzzy reliability matrix is employed to characterize the reliability of each subset on different clusters. Third, an adaptive evidence combination rule based on the DST is developed to combine the discounted subresults (memberships) with different reliability to make the final decision for each instance. The proposed method can characterize uncertainty and imprecision by assigning instances to specific clusters or meta-clusters composed of several specific clusters. Once an instance is assigned to a meta-cluster, the cluster information of this instance is (locally) imprecise. The effectiveness of proposed method is demonstrated on several real-world datasets by comparing with existing techniques.},
  archive      = {J_TKDD},
  doi          = {10.1145/3638061},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Enhanced fuzzy clustering for incomplete instance with evidence combination},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
