<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tap---20">TAP - 20</h2>
<ul>
<li><details>
<summary>
(2024). How changes in the mean latency, jitter amplitude, and
jitter frequency impact target acquisition performance. <em>TAP</em>,
<em>22</em>(2), 1–18. (<a
href="https://doi.org/10.1145/3701984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive technologies require the user to frequently generate purposeful movements, where actions are guided towards targets. While delays between the movement onset and the corresponding update within the virtual space have been shown to impair target acquisition, the relative contribution of the individual latency parameters remains unclear. This article investigates how changes in latency, the moment-by-moment variability in the latency (jitter amplitude) and the rate of variability (jitter frequency), affect the speed and accuracy of target acquisition at different mean latencies. Experiment 1 explored changes in the mean latency and jitter amplitude. As the mean latency increased, we observed substantial impairments in completion time and accuracy, with increased variability in performance. There was also an interaction between the mean latency and jitter amplitude: although large jitter amplitudes caused a small completion time impairment, this effect decreased as the mean latency increased. Experiment 2 isolated the effect of the jitter by investigating changes in the jitter amplitude and jitter frequency at a fixed mean latency. Here, we observed completion time impairments from 67 ms amplitude and accuracy impairments from 134 ms amplitude. Like Experiment 1, the effect of the amplitude was small. Notably, we found no evidence that changes in the jitter frequency significantly influenced performance. Overall, increases in the mean latency contributed most to the impairment, disrupting acquisition speed and accuracy as it increased. Large jitter amplitudes also disrupted speed and accuracy, but this was a comparatively small effect that was mediated by the mean latency.},
  archive      = {J_TAP},
  doi          = {10.1145/3701984},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {2},
  pages        = {1-18},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {How changes in the mean latency, jitter amplitude, and jitter frequency impact target acquisition performance},
  volume       = {22},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can you tell real from fake face images? Perception of
computer-generated faces by humans. <em>TAP</em>, <em>22</em>(2), 1–23.
(<a href="https://doi.org/10.1145/3696667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent advances in machine learning and big data, it is now possible to create synthetic images that look real. Face generation is often of particular interest, as faces can be used for various purposes. However, improper use of such content can lead to the dissemination of false information, such as fake news, and thus pose a threat to society. This work studies whether people believe the truthfulness of faces using eye tracking and self-reports, including free-form textual explanations when participants encounter real and computer-generated faces. We used three different datasets for our evaluations, and our experimental results show that while people are relatively better at identifying the truthfulness of real faces and faces generated by earlier machine learning algorithms with different gazing behaviors in viewing and rating phases, they perform less accurately when deciding the truthfulness of synthetic face images that are generated by newer algorithms. Our findings provide important insights for society and policymakers.},
  archive      = {J_TAP},
  doi          = {10.1145/3696667},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Can you tell real from fake face images? perception of computer-generated faces by humans},
  volume       = {22},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Virtual reality audio game for entertainment and sound
localization training. <em>TAP</em>, <em>22</em>(1), 1–24. (<a
href="https://doi.org/10.1145/3676557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the gaming and electronics industry, there is a continuous evolution of alternative applications. Nevertheless, accessibility to video games remains a persistent hurdle for individuals with disabilities, especially those with visual impairments due to the inherent visual-oriented design of games. Audio games (AGs) are electronic games that rely primarily on auditory cues instead of visual interfaces. This study focuses on the creation of a virtual reality AG for cell phones that integrates natural head and torso movements involved in spatial hearing. Its assessment encompasses user experience, interface usability, and sound localization performance. The study engaged eighteen sighted participants in a pre-post test with a control group. The experimental group underwent 7 training sessions with the AG. Via interviews, facets of the gaming experience were explored, while horizontal plane sound source localization was also tested before and after the training. The results enabled the characterization of sensations related to the use of the game and the interaction with the interfaces. Sound localization tests demonstrated distinct enhancements in performance among trained participants, varying with assessed stimuli. These promising results show advances for future virtual AGs, presenting prospects for auditory training. These innovations hold potential for skill development, entertainment, and the integration of visually impaired individuals.},
  archive      = {J_TAP},
  doi          = {10.1145/3676557},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Virtual reality audio game for entertainment and sound localization training},
  volume       = {22},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the impact of visual and kinematic information
on the perception of physicality errors. <em>TAP</em>, <em>22</em>(1),
1–30. (<a href="https://doi.org/10.1145/3660636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Errors that arise due to a mismatch in the dynamics of a person’s motion and the visualized movements of their avatar in virtual reality are termed “physicality errors” to distinguish them from simple physical errors, such as footskate. Physicality errors involve plausible motions, but with dynamic inconsistencies. Even with perfect tracking and ideal virtual worlds, such errors are inevitable in virtual reality whenever a person adopts an avatar that does not match their own proportions or lifts a virtual object that appears heavier than the movement of their hand. This study investigates people’s sensitivity to physicality errors to understand when they are likely to be noticeable and need to be mitigated. It uses a simple, well-understood exercise of a dumbbell lift to explore the impact of motion kinematics and varied sources of visual information, including changing body size, changing the size of manipulated objects, and displaying muscular strain. Results suggest that kinematic (motion) information has a dominant impact on perception of effort, but visual information, particularly the visual size of the lifted object, has a strong impact on perceived weight. This can lead to perceptual mismatches that reduce perceived naturalness. Small errors may not be noticeable, but large errors reduce naturalness. Further results are discussed, which inform the requirements for animation algorithms.},
  archive      = {J_TAP},
  doi          = {10.1145/3660636},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {1},
  pages        = {1-30},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Understanding the impact of visual and kinematic information on the perception of physicality errors},
  volume       = {22},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The influence that the complexity of the three-dimensional
eye model used to generate simulated eye-tracking data has on the gaze
estimation errors achieved using the data. <em>TAP</em>, <em>22</em>(1),
1–16. (<a href="https://doi.org/10.1145/3660637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulated eye-tracking data are an integral tool in the development of eye-tracking methods. Most of the simulated data used in eye-tracking-related research has been generated using low-complexity eye models that include a single spherical corneal surface. This study investigated the influence of eye-model complexity on the ability of simulated eye-tracking data to predict real-world outcomes. The experimental procedures of two pertinent comparative eye-tracking studies were replicated in a simulated environment using various eye-model complexities. The simulated outcomes were then evaluated against the findings of the comparative studies that were derived from real-world outcomes. The simulated outcomes of both comparative studies were significantly influenced by the eye-model complexity. Eye models that included an aspheric corneal surface best replicated experimental eye-tracking outcomes, while including a posterior corneal surface did not improve the ability of simulated data to replicate real-world outcomes. Using a wide-angle eye model that accurately replicates the peripheral optics of the eye did not improve simulated outcomes relative to a paraxial eye model.},
  archive      = {J_TAP},
  doi          = {10.1145/3660637},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {1},
  pages        = {1-16},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {The influence that the complexity of the three-dimensional eye model used to generate simulated eye-tracking data has on the gaze estimation errors achieved using the data},
  volume       = {22},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The impact of nature realism on the restorative quality of
virtual reality forest bathing. <em>TAP</em>, <em>22</em>(1), 1–18. (<a
href="https://doi.org/10.1145/3670406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) forest bathing for stress relief and mental health has recently become a popular research topic. As people spend more of their lives indoors and have less access to the restorative benefit of nature, having a VR nature supplement has the potential to improve quality of life. However, the optimal design of VR nature environments is an active area of investigation with many research questions to be explored. One major issue with VR is the difficulty of rendering high-fidelity assets in real time without causing cybersickness, or VR motion sickness, within the headset. Due to this limitation, we investigate if the realism of VR nature is critical for the restorative effects by comparing a low-realism nature environment to a high-realism nature environment. We only found a significant difference in the perceived restorativeness of the two environments, but after observing trends in our data toward the stress reduction potential of the high-realism environment, we suggest exploring more varieties of high and low-realism environments in future work to investigate the full potential of VR and how people respond.},
  archive      = {J_TAP},
  doi          = {10.1145/3670406},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {1},
  pages        = {1-18},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {The impact of nature realism on the restorative quality of virtual reality forest bathing},
  volume       = {22},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the effects of user-agent and user-designer
similarity in virtual human design to promote mental health intentions
for college students. <em>TAP</em>, <em>22</em>(1), 1–41. (<a
href="https://doi.org/10.1145/3689822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual humans (i.e., embodied conversational agents) have the potential to support college students’ mental health, particularly in Science, Technology, Engineering, and Mathematics (STEM) fields where students are at a heightened risk of mental disorders such as anxiety and depression. A comprehensive understanding of students, considering their cultural characteristics, experiences, and expectations, is crucial for creating timely and effective virtual human interventions. To this end, we conducted a user study with 481 computer science students from a major university in North America, exploring how they co-designed virtual humans to support mental health conversations for students similar to them . Our findings suggest that computer science students who engage in co-design processes of virtual humans tend to create agents that closely resemble them demographically—agent-designer demographic similarity. Key factors influencing virtual human design included age, gender, ethnicity, and the matching between appearance and voice. We also observed that the demographic characteristics of virtual human designers, especially ethnicity and gender, tend to be associated with those of the virtual humans they designed. Finally, we provide insights concerning the impact of user-designer demographic similarity in virtual humans’ effectiveness in promoting mental health conversations when designers’ characteristics are shared explicitly or implicitly. Understanding how virtual humans’ characteristics serve users’ experiences in mental wellness conversations and the similarity-attraction effects between agents, users, and designers may help tailor virtual humans’ design to enhance their acceptance and increase their counseling effectiveness.},
  archive      = {J_TAP},
  doi          = {10.1145/3689822},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {1},
  pages        = {1-41},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Exploring the effects of user-agent and user-designer similarity in virtual human design to promote mental health intentions for college students},
  volume       = {22},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GazeFusion: Saliency-guided image generation. <em>TAP</em>,
<em>21</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3694969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models offer unprecedented image generation power given just a text prompt. While emerging approaches for controlling diffusion models have enabled users to specify the desired spatial layouts of the generated content, they cannot predict or control where viewers will pay more attention due to the complexity of human vision. Recognizing the significance of attention-controllable image generation in practical applications, we present a saliency-guided framework to incorporate the data priors of human visual attention mechanisms into the generation process. Given a user-specified viewer attention distribution, our control module conditions a diffusion model to generate images that attract viewers’ attention toward the desired regions. To assess the efficacy of our approach, we performed an eye-tracked user study and a large-scale model-based saliency analysis. The results evidence that both the cross-user eye gaze distributions and the saliency models’ predictions align with the desired attention distributions. Lastly, we outline several applications, including interactive design of saliency guidance, attention suppression in unwanted regions, and adaptive generation for varied display/viewing conditions.},
  archive      = {J_TAP},
  doi          = {10.1145/3694969},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {GazeFusion: Saliency-guided image generation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating the perception of facial anonymization
techniques in 360° videos. <em>TAP</em>, <em>21</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3695254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate facial anonymization techniques in 360° videos and assess their influence on the perceived realism, anonymization effect, and presence of participants. In comparison to traditional footage, 360° videos can convey engaging, immersive experiences that accurately represent the atmosphere of real-world locations. As the entire environment is captured simultaneously, it is necessary to anonymize the faces of bystanders in recordings of public spaces. Since this alters the video content, the perceived realism and immersion could be reduced. To understand these effects, we compare non-anonymized and anonymized 360° videos using blurring, black boxes, and face-swapping shown either on a regular screen or in a head-mounted display (HMD). Our results indicate significant differences in the perception of the anonymization techniques. We find that face-swapping is the most realistic and least disruptive; however, participants raised concerns regarding the effectiveness of the anonymization. Furthermore, we observe that presence is affected by facial anonymization in HMD condition. Overall, the results underscore the need for facial anonymization techniques that balance both photo-realism and a sense of privacy.},
  archive      = {J_TAP},
  doi          = {10.1145/3695254},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Investigating the perception of facial anonymization techniques in 360° videos},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the effects of self-overlapping spaces on distance
perception and action judgments. <em>TAP</em>, <em>21</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3695632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-overlapping spaces, also known as impossible spaces, are a design mechanic in virtual reality (VR) that allows a user to naturally walk through an environment that is larger than the physical space available to them. Prior work has focused on generating these spaces and evaluating when their self-overlapping nature is detectable. Comparatively, little work has evaluated how the self-overlapping nature of these spaces impacts users’ spatial understanding and whether any misperceptions carry over into altered action judgments. We present a study evaluating how self-overlapping spaces influence action judgments related to relative distances within the virtual environment. Participants were presented with a variety of self-overlapping spaces and, after exploring them, were asked to judge which of the two locations was closer to their current position in the environment. Participants’ were more likely to make correct decisions as the relative difference in distance between the two locations increased; however, this effect was affected by both the amount of overlap present in a particular environment and by the relative position from which they made their decision.},
  archive      = {J_TAP},
  doi          = {10.1145/3695632},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Exploring the effects of self-overlapping spaces on distance perception and action judgments},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). “Together with who?” Recognizing partners during
collaborative avatar manipulation. <em>TAP</em>, <em>21</em>(4), 1–16.
(<a href="https://doi.org/10.1145/3698237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of novel computer interfaces has led to the possibility of integrating inputs from multiple individuals into a single avatar, fostering collaboration by combining skills and sharing the cognitive load. However, the collaboration dynamic and its effectiveness may vary depending on the individuals involved. Particularly in scenarios where two individuals remotely control a robotic avatar without the possibility of direct communication, understanding each other’s characteristics can result in enhanced performance. To achieve this, it is essential to ascertain if individuals can discern their partner’s characteristics within the merged embodiment. This paper investigates the accuracy with which participants can distinguish between two different collaborating partners (one attempting to lead and one attempting to follow) when sharing control of a robot arm during a block pick-and-place task. The results suggested that participants who changed their roles according to the different roles of the two partners achieved the highest discrimination rates. Furthermore, participants changed their movements through the trials, adapting their actions to their preferred approach. This research provides insights into the factors determining individuals’ ability to understand partner characteristics during control of collaborative avatars.},
  archive      = {J_TAP},
  doi          = {10.1145/3698237},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {“Together with who?” recognizing partners during collaborative avatar manipulation},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introduction to the special issue on SAP 2024. <em>TAP</em>,
<em>21</em>(4), 1–2. (<a href="https://doi.org/10.1145/3702963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAP},
  doi          = {10.1145/3702963},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {4},
  pages        = {1-2},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Introduction to the special issue on SAP 2024},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoding functional brain data for emotion recognition: A
machine learning approach. <em>TAP</em>, <em>21</em>(3), 1–18. (<a
href="https://doi.org/10.1145/3657638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of emotions is an open research area and has a potential leading role in the improvement of socio-emotional skills such as empathy, sensitivity, and emotion recognition in humans. The current study aimed at using Event Related Potential (ERP) components (N100, N200, P200, P300, early Late Positive Potential (LPP), middle LPP, and late LPP) of EEG data for the classification of emotional states (positive, negative, neutral). EEG data were collected from 62 healthy individuals over 18 electrodes. An emotional paradigm with pictures from the International Affective Picture System (IAPS) was used to record the EEG data. A linear Support Vector Machine (C = 0.1) was used to classify emotions, and a forward feature selection approach was used to eliminate irrelevant features. The early LPP component, which was the most discriminative among all ERP components, had the highest classification accuracy (70.16%) for identifying negative and neutral stimuli. The classification of negative versus neutral stimuli had the best accuracy (79.84%) when all ERP components were used as a combined feature set, followed by positive versus negative stimuli (75.00%) and positive versus neutral stimuli (68.55%). Overall, the combined ERP component feature sets outperformed single ERP component feature sets for all stimulus pairings in terms of accuracy. These findings are promising for further research and development of EEG-based emotion recognition systems.},
  archive      = {J_TAP},
  doi          = {10.1145/3657638},
  journal      = {ACM Transactions on Applied Perception},
  month        = {7},
  number       = {3},
  pages        = {1-18},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Decoding functional brain data for emotion recognition: A machine learning approach},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Color theme evaluation through user preference modeling.
<em>TAP</em>, <em>21</em>(3), 1–35. (<a
href="https://doi.org/10.1145/3665329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color composition (or color theme) is a key factor to determine how well a piece of art work or graphical design is perceived by humans. Despite a few color harmony models have been proposed, their results are often less satisfactory since they mostly neglect the variations of aesthetic cognition among individuals and treat the influence of all ratings equally as if they were all rated by the same anonymous user. To overcome this issue, in this article we propose a new color theme evaluation model by combining a back propagation neural network and a kernel probabilistic model to infer both the color theme rating and the user aesthetic preference. Our experiment results show that our model can predict more accurate and personalized color theme ratings than state of the art methods. Our work is also the first-of-its-kind effort to quantitatively evaluate the correlation between user aesthetic preferences and color harmonies of five-color themes, and study such a relation for users with different aesthetic cognition.},
  archive      = {J_TAP},
  doi          = {10.1145/3665329},
  journal      = {ACM Transactions on Applied Perception},
  month        = {7},
  number       = {3},
  pages        = {1-35},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Color theme evaluation through user preference modeling},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Color hint-guided ink wash painting colorization with ink
style prediction mechanism. <em>TAP</em>, <em>21</em>(3), 1–21. (<a
href="https://doi.org/10.1145/3657637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an end-to-end generative adversarial network that allows for controllable ink wash painting generation from sketches by specifying the colors via color hints. To the best of our knowledge, this is the first study for interactive Chinese ink wash painting colorization from sketches. To help our network understand the ink style and artistic conception, we introduced an ink style prediction mechanism for our discriminator, which enables the discriminator to accurately predict the style with the help of a pre-trained style encoder. We also designed our generator to receive multi-scale feature information from the feature pyramid network for detail reconstruction of ink wash painting. Experimental results and user study show that ink wash paintings generated by our network have higher realism and richer artistic conception than existing image generation methods.},
  archive      = {J_TAP},
  doi          = {10.1145/3657637},
  journal      = {ACM Transactions on Applied Perception},
  month        = {6},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Color hint-guided ink wash painting colorization with ink style prediction mechanism},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing human reactions in a virtual crowd based on crowd
disposition, perceived agency, and user traits. <em>TAP</em>,
<em>21</em>(3), 1–21. (<a
href="https://doi.org/10.1145/3658670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive virtual environments populated by real and virtual humans provide valuable insights into human decision-making processes under controlled conditions. Existing literature indicates elevated comfort, higher presence, and a more positive user experience when virtual humans exhibit rich behaviors. Based on this knowledge, we conducted a web-based, interactive study, in which participants were embodied within a virtual crowd with complex behaviors driven by an underlying psychological model. While participants interacted with a group of autonomous humanoid agents in a shopping scenario similar to Black Friday, the platform recorded their non-verbal behaviors. In this independent-subjects study, we investigated behavioral and emotional variances across participants with diverse backgrounds focusing on two conditions: perceived agency and the crowd’s emotional disposition. For perceived agency, one group of participants was told that the other crowd members were avatars controlled by humans, whereas another group was told that they were artificial agents. For emotional disposition, the crowd behaved either in a docile or hostile manner. The results suggest that the crowd’s disposition and specific participant traits significantly affected certain emotions and behaviors. For instance, participants collected fewer items and reported a higher increase of negative emotions when placed in a hostile crowd. However, perceived agency did not yield any statistically significant effects.},
  archive      = {J_TAP},
  doi          = {10.1145/3658670},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Assessing human reactions in a virtual crowd based on crowd disposition, perceived agency, and user traits},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptation to simulated hypergravity in a virtual reality
throwing task. <em>TAP</em>, <em>21</em>(2), 1–23. (<a
href="https://doi.org/10.1145/3643849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to previous research, humans are generally poor at adapting to earth-discrepant gravity, especially in Virtual Reality (VR), which cannot simulate the effects of gravity on the physical body. Most of the previous VR research on gravity adaptation has used perceptual or interception tasks, although adaptation to these tasks seems to be especially challenging compared to tasks with a more pronounced motor component. This article describes the results of two between-subjects studies ( n = 60 and n = 42) that investigated adaptation to increased gravity simulated by an interactive VR experience. The experimental procedure was identical in both studies: In the adaptation phase, one group was trained to throw a ball at a target using Valve Index motion controllers in gravity that was simulated at five times of earth’s gravity (hypergravity group), whereas another group threw at a longer-distance target under normal gravity (normal gravity group) so both groups had to exert the same amount of force when throwing (approximated manually in Study 1 and mathematically in Study 2). Then, in the measurement phase, both groups repeatedly threw a virtual ball at targets in normal gravity. In this phase, the trajectory of the ball was hidden at the moment of release so that the participants had to rely on their internal model of gravity to hit the targets rather than on visual feedback. Target distances were placed within the same range for both groups in the measurement phase. According to our preregistered hypotheses, we predicted that the hypergravity group would display worse overall throwing accuracy and would specifically overshoot the target more often than the normal gravity group. Our experimental data supported both hypotheses in both studies. The findings indicate that training an interactive task in higher simulated gravity led participants in both studies to update their internal gravity models, and therefore, some adaptation to higher gravity did indeed occur. However, our exploratory analysis also indicates that the participants in the hypergravity group began to gradually regain their throwing accuracy throughout the course of the measurement phase.},
  archive      = {J_TAP},
  doi          = {10.1145/3643849},
  journal      = {ACM Transactions on Applied Perception},
  month        = {3},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Adaptation to simulated hypergravity in a virtual reality throwing task},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and validation of a virtual reality mental rotation
test. <em>TAP</em>, <em>21</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3626238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental rotation, a common measure of spatial ability, has traditionally been assessed through paper-based instruments like the Mental Rotation Test (MRT) or the Purdue Spatial Visualization Test: Rotations (PSVT:R). The fact that these instruments present 3D shapes in a 2D format devoid of natural cues like shading and perspective likely limits their ability to accurately assess the fundamental skill of mentally rotating 3D shapes. In this paper, we describe the Virtual Reality Mental Rotation Assessment (VRMRA), a virtual reality-based mental rotation assessment derived from the Revised PSVT:R and MRT. The VRMRA reimagines traditional mental rotation assessments in a room-scale virtual environment and uses hand-tracking and elements of gamification in attempts to create an intuitive, engaging experience for test-takers. To validate the instrument, we compared response patterns in the VRMRA with patterns observed on the MRT and Revised PSVT:R. For the PSVT:R-type questions, items requiring a rotation around two axes were significantly harder than items requiring rotations around a single axis in the VRMRA, which is not the case in the Revised PSVT:R. For the MRT-type questions in the VRMRA, a moderate negative correlation was found between the degree of rotation in the X direction and item difficulty. While the problem of occlusion was reduced, features of the shapes and distractors accounted for 50.6% of the variance in item difficulty. Results suggest that the VRMRA is likely a more accurate tool to assess mental rotation ability in comparison to traditional instruments which present the stimuli through 2D media. Our findings also point to potential problems with the fundamental designs of the Revised PSVT:R and MRT question formats.},
  archive      = {J_TAP},
  doi          = {10.1145/3626238},
  journal      = {ACM Transactions on Applied Perception},
  month        = {1},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Design and validation of a virtual reality mental rotation test},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-finger stiffness discrimination with the stochastic
resonance effect. <em>TAP</em>, <em>21</em>(2), 1–17. (<a
href="https://doi.org/10.1145/3630254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigated the ability of two fingers to discriminate stiffness with stochastic resonance. It is known that the haptic perception at the fingertip improves when vibrotactile noise propagates to the fingertip, which is a phenomenon called the stochastic resonance. The improvement in the haptic sensation of a fingertip depends on the intensity of the noise propagating to the fingertip. An improvement in the haptic sensation of multiple fingertips does not require multiple noise sources, such as vibrators, to be attached to multiple fingertips; i.e., even a single vibrator can propagate noise to multiple fingers. In this study, we focus on stiffness discrimination as a task using multiple fingers, in which the thumb and index finger are used to touch an object and perceive its stiffness. Subsequently, we demonstrate that the stiffness perception is improved by propagating sufficiently intense noise to the thumb and index finger using only a single vibrator. The findings indicate the possibility of improving the haptic sensation at multiple fingertips using one vibrator.},
  archive      = {J_TAP},
  doi          = {10.1145/3630254},
  journal      = {ACM Transactions on Applied Perception},
  month        = {1},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Two-finger stiffness discrimination with the stochastic resonance effect},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimates of temporal edge detection filters in human
vision. <em>TAP</em>, <em>21</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3639052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection is an important process in human visual processing. However, as far as we know, few attempts have been made to map the temporal edge detection filters in human vision. To that end, we devised a user study and collected data from which we derived estimates of human temporal edge detection filters based on three different models, including the derivative of the infinite symmetric exponential function and temporal contrast sensitivity function. We analyze our findings using several different methods, including extending the filter to higher frequencies than were shown during the experiment. In addition, we show a proof of concept that our filter may be used in spatiotemporal image quality metrics by incorporating it into a flicker detection pipeline.},
  archive      = {J_TAP},
  doi          = {10.1145/3639052},
  journal      = {ACM Transactions on Applied Perception},
  month        = {1},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Estimates of temporal edge detection filters in human vision},
  volume       = {21},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
