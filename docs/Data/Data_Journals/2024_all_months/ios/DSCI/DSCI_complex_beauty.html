<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DSCI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dsci---5">DSCI - 5</h2>
<ul>
<li><details>
<summary>
(2024). Estimating reaction barriers with deep reinforcement
learning 1. <em>DSCI</em>, <em>7</em>(2), 73–92. (<a
href="https://doi.org/10.3233/DS-240063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stable states in complex systems correspond to local minima on the associated potential energy surface. Transitions between these local minima govern the dynamics of such systems. Precisely determining the transition pathways in complex and high-dimensional systems is challenging because these transitions are rare events, and isolating the relevant species in experiments is difficult. Most of the time, the system remains near a local minimum, with rare, large fluctuations leading to transitions between minima. The probability of such transitions decreases exponentially with the height of the energy barrier, making the system’s dynamics highly sensitive to the calculated energy barriers. This work aims to formulate the problem of finding the minimum energy barrier between two stable states in the system’s state space as a cost-minimization problem. It is proposed to solve this problem using reinforcement learning algorithms. The exploratory nature of reinforcement learning agents enables efficient sampling and determination of the minimum energy barrier for transitions.},
  archive      = {J_DSCI},
  author       = {Pal, Adittya},
  doi          = {10.3233/DS-240063},
  journal      = {Data Science},
  month        = {11},
  number       = {2},
  pages        = {73-92},
  shortjournal = {Data Sci.},
  title        = {Estimating reaction barriers with deep reinforcement learning 1},
  volume       = {7},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting CSV file dialects by table uniformity measurement
and data type inference. <em>DSCI</em>, <em>7</em>(2), 55–72. (<a
href="https://doi.org/10.3233/DS-240062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human-readable simplicity with which the CSV format was devised, together with the absence of a standard that strictly defines this format, has allowed the proliferation of several variants in the dialects with which these files are written. The latter has meant that the exchange of information between data management systems, or between countries and regions, requires human intervention during the data mining and cleansing process. This has led to the development of various computational tools that aim to accurately determine the dialects of CSV files, in order to avoid data loss at data loading stage in a given system. However, the dialect detection is a complex problem and current systems have limitations or make assumptions that need to be improved and/or extended. This paper proposes a method for determining CSV file dialects through table uniformity, a statistical approach based on table consistency and records dispersion measurement along with the detection of data type over each field. The new method has a 93.38% average accuracy on a dataset with 548 CSV files composed of samples coming from a data load testing framework, the test suite provided by the CSV on the Web Working Group (CSVW), curated experimental data set from similar tool development and some others CSV files added as verification of the parsing routines. In tests, the proposed solution outperforms the state-of-the-art tool by achieving an average improvement of 16.45%, resulting in an net increment of about 10% in the accuracy with which dialects are detected on truly messy data for this research dataset. Furthermore, the proposed method is accurate enough to determine dialects by reading only ten records, requiring more data to disambiguate those cases where the first records do not contain the necessary information to conclude with a dialect determination.},
  archive      = {J_DSCI},
  author       = {García, Wilfredo},
  doi          = {10.3233/DS-240062},
  journal      = {Data Science},
  month        = {11},
  number       = {2},
  pages        = {55-72},
  shortjournal = {Data Sci.},
  title        = {Detecting CSV file dialects by table uniformity measurement and data type&amp;nbsp;inference},
  volume       = {7},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Electrocardiogram arrhythmia detection with novel signal
processing and persistent homology-derived predictors. <em>DSCI</em>,
<em>7</em>(1), 29–53. (<a
href="https://doi.org/10.3233/DS-240061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many approaches to computer-aided electrocardiogram (ECG) arrhythmia detection have been performed, several of which combine persistent homology and machine learning. We present a novel ECG signal processing pipeline and method of constructing predictor variables for use in statistical models. Specifically, we introduce an isoelectric baseline to yield non-trivial topological features corresponding to the P, Q, S, and T-waves (if they exist) and utilize the N -most persistent 1-dimensional homological features and their corresponding area-minimal cycle representatives to construct predictor variables derived from the persistent homology of the ECG signal for some choice of N . The binary classification of (1) Atrial Fibrillation vs. Non-Atrial Fibrillation, (2) Arrhythmia vs. Normal Sinus Rhythm, and (3) Arrhythmias with Morphological Changes vs. Sinus Rhythm with Bradycardia and Tachycardia Treated as Non-Arrhythmia was performed using Logistic Regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Naive Bayes, Random Forest, Gradient Boosted Decision Tree, K -Nearest Neighbors, and Support Vector Machine with a linear, radial, and polynomial kernel Models with stratified 5-fold cross validation. The Gradient Boosted Decision Tree Model attained the best results with a mean F1-score and mean Accuracy of ( 0.967 , 0.946 ) , ( 0.839 , 0.946 ) , and ( 0.943 , 0.921 ) across the five folds for binary classifications of (1), (2), and (3), respectively.},
  archive      = {J_DSCI},
  author       = {Dlugas, Hunter},
  doi          = {10.3233/DS-240061},
  journal      = {Data Science},
  month        = {6},
  number       = {1},
  pages        = {29-53},
  shortjournal = {Data Sci.},
  title        = {Electrocardiogram arrhythmia detection with novel signal processing and persistent homology-derived predictors},
  volume       = {7},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A benchmark dataset for the retail multiskilled personnel
planning under uncertain demand. <em>DSCI</em>, <em>7</em>(1), 13–27.
(<a href="https://doi.org/10.3233/DS-240060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this data article, we present and describe datasets designed to address multiskilled personnel assignment problems (MPAP) under uncertain demand. The data article introduces simulated datasets and a real dataset obtained from a retail store in Chile. The real dataset provides details on the structure of the store, including the number of departments and workers, the type of labor contract, the cost parameter values, and the average demand across all store departments. The simulated datasets, consisting of 18 categorized text files, were generated through Monte Carlo simulation to encapsulate information about the stochastic demand for store departments. These text files are classified based on: (i) type of sample (in-sample or out-of-sample), (ii) type of truncation method (zero-truncated or percentile-truncated), and (iii) demand coefficient of variation (5%, 10%, 20%, 30%, 40%, 50%). This categorization allows academics and practitioners to select the scenarios that meet with their specific research or application needs, increasing the flexibility and applicability of the datasets. In addition, researchers and practitioners can use these comprehensive real and simulated datasets to benchmark the performance of diverse optimization methods under uncertain demand, thereby ensuring robust multiskilling levels for similar MPAPs. Furthermore, we offer an Excel workbook with the capability to generate up to 10,000 demand scenarios for varying coefficients of variation in demand.},
  archive      = {J_DSCI},
  author       = {Henao, César Augusto and Porto, Andrés Felipe and González, Virginia I.},
  doi          = {10.3233/DS-240060},
  journal      = {Data Science},
  month        = {6},
  number       = {1},
  pages        = {13-27},
  shortjournal = {Data Sci.},
  title        = {A benchmark dataset for the retail multiskilled personnel planning under uncertain demand},
  volume       = {7},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring data drift with the unstable population indicator
1. <em>DSCI</em>, <em>7</em>(1), 1–12. (<a
href="https://doi.org/10.3233/DS-240059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring data drift is essential in machine learning applications where model scoring (evaluation) is done on data samples that differ from those used in training. The Kullback-Leibler divergence is a common measure of shifted probability distributions, for which discretized versions are invented to deal with binned or categorical data. We present the Unstable Population Indicator, a robust, flexible and numerically stable, discretized implementation of Jeffrey’s divergence, along with an implementation in a Python package that can deal with continuous, discrete, ordinal and nominal data in a variety of popular data types. We show the numerical and statistical properties in controlled experiments. It is not advised to employ a common cut-off to distinguish stable from unstable populations, but rather to let that cut-off depend on the use case.},
  archive      = {J_DSCI},
  author       = {Haas, Marcel R. and Sibbald, Lisette},
  doi          = {10.3233/DS-240059},
  journal      = {Data Science},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Data Sci.},
  title        = {Measuring data drift with the unstable population indicator 1},
  volume       = {7},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
