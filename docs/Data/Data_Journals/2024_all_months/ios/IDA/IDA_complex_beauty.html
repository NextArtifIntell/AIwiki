<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IDA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ida---90">IDA - 90</h2>
<ul>
<li><details>
<summary>
(2024). Atmospheric SO 2 pollutant prediction using mutual
information based TCNN-GRU model for flue gas desulfurization process.
<em>IDA</em>, <em>28</em>(6), 1723–1740. (<a
href="https://doi.org/10.3233/IDA-230890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past several years, sulfur dioxide (SO2) has raised growing concern in China owing to its adverse impact on atmosphere and human respiratory system. The major contributor to SO2 emissions is flue gas generated by fossil-fired electricity-generating plants, and as a consequence diverse flue gas desulphurization (FGD) techniques are installed to abate SO2 emissions. However, the FGD is a dynamic process with serious nonlinearity and large time delay, making the FGD process modeling problem a formidable one. In our research study, a novel hybrid deep learning model with temporal convolution neural network (TCNN), gated recurrent unit (GRU) and mutual information (MI) technique is proposed to predict SO2 emissions in an FGD process. Among those technique, MI is applied to select variables that are best suited for SO2 emission prediction, while TCNN and GRU are innovatively integrated to capture dynamics of SO2 emission in the FGD process. A real FGD system in a power plant with a coal-fired unit of 1000 MW is used as a study case for SO2 emission prediction. Experimental results show that the proposed approach offers satisfactory performance in predicting SO2 emissions for the FGD process, and outperforms other contrastive predictive methods in terms of different performance indicators.},
  archive      = {J_IDA},
  author       = {Liu, Quanbo and Li, Xiaoli and Wang, Kang},
  doi          = {10.3233/IDA-230890},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1723-1740},
  shortjournal = {Intell. Data Anal.},
  title        = {Atmospheric SO 2 pollutant prediction using mutual information based TCNN-GRU model for flue gas desulfurization process},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Processing and optimized learning for improved
classification of categorical plant disease datasets. <em>IDA</em>,
<em>28</em>(6), 1697–1721. (<a
href="https://doi.org/10.3233/IDA-230651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PURPOSE: Crop diseases can cause significant reductions in yield, subsequently impacting a country’s economy. The current research is concentrated on detecting diseases in three specific crops – tomatoes, soybeans, and mushrooms, using a real-time dataset collected for tomatoes and two publicly acc essible datasets for the other crops. The primary emphasis is on employing datasets with exclusively categorical attributes, which poses a notable challenge to the research community. METHODS: After applying label encoding to the attributes, the datasets undergo four distinct preprocessing techniques to address missing values. Following this, the SMOTE-N technique is employed to tackle class imbalance. Subsequently, the pre-processed datasets are subjected to classification using three ensemble methods: bagging, boosting, and voting. To further refine the classification process, the metaheuristic Ant Lion Optimizer (ALO) is utilized for hyper-parameter tuning. RESULTS: This comprehensive approach results in the evaluation of twelve distinct models. The top two performers are then subjected to further validation using ten standard categorical datasets. The findings demonstrate that the hybrid model II-SN-OXGB, surpasses all other models as well as the current state-of-the-art in terms of classification accuracy across all thirteen categorical datasets. II utilizes the Random Forest classifier to iteratively impute missing feature values, employing a nearest features strategy. Meanwhile, SMOTE-N (SN) serves as an oversampling technique particularly for categorical attributes, again utilizing nearest neighbors. Optimized (using ALO) Xtreme Gradient Boosting OXGB, sequentially trains multiple decision trees, with each tree correcting errors from its predecessor. CONCLUSION: Consequently, the model II-SN-OXGB emerges as the optimal choice for addressing classification challenges in categorical datasets. Applying the II-SN-OXGB model to crop datasets can significantly enhance disease detection which in turn, enables the farmers to take timely and appropriate measures to prevent yield losses and mitigate the economic impact of crop diseases.},
  archive      = {J_IDA},
  author       = {Gupta, Ayushi and Chug, Anuradha and Singh, Amit Prakash},
  doi          = {10.3233/IDA-230651},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1697-1721},
  shortjournal = {Intell. Data Anal.},
  title        = {Processing and optimized learning for improved classification of categorical plant disease datasets},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple-local feature and attention fused person
re-identification method. <em>IDA</em>, <em>28</em>(6), 1679–1695. (<a
href="https://doi.org/10.3233/IDA-230392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) is widely used in intelligent security, monitoring, criminal investigation and other fields. Aiming at the problems of local occlusion, scale misalignment and attitude change of pedestrian images in actual scenes, we propose a Multi-local Feature and Attention fused network (MFA) used for person re-identification task. Firstly, Channel Point Affinity Attention module (CPAA) is embedded in the backbone network to enhance the ability of the network for extracting local details. The feature map output from the backbone network is horizontally segmented into four local feature maps, and further four branch networks are concatenated to the feature map of the backbone network. The four local feature maps are used to guide the four branch networks to pay more attention on different areas of pedestrians through Global Local Aligned loss (GLA) function. Finally, the pedestrian feature vector containing multi-local features is obtained. The mAP of the network on Market-1501, DukeMTMC-reID,CUHK03 and MSMT17 datasets were 88.6%, 81.4%, 79.5% and 64.7%, and the Rank-1 was 95.8%, 90.1%, 81.2% and 84.1% respectively. In addition, the model also obtained 73.2% and 68.1% of Rank-1 on partial dataset Patial-REID and Patial-iLIDS, respectively. Recently, The MFA model parameter is 28.3M and the inference efficiency is approximately 32 fps to an image with a resulation of 256 × 128. Compared with other ReID methods, our proposed methods achieved a competitive performance for ReID task. The code was available at github: [email protected] :ISCLab-Bistu/MFA.git.},
  archive      = {J_IDA},
  author       = {Yu, Mingxin and Wang, Jun and You, Rui and Ji, Xinglong and Lu, Wenshuai},
  doi          = {10.3233/IDA-230392},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1679-1695},
  shortjournal = {Intell. Data Anal.},
  title        = {Multiple-local feature and attention fused person re-identification method},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ultrasound breast images denoising using generative
adversarial networks (GANs). <em>IDA</em>, <em>28</em>(6), 1661–1678.
(<a href="https://doi.org/10.3233/IDA-230631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  author       = {Jiménez-Gaona, Yuliana and Rodríguez-Alvarez, María José and Escudero, Líder and Sandoval, Carlos and Lakshminarayanan, Vasudevan},
  doi          = {10.3233/IDA-230631},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1661-1678},
  shortjournal = {Intell. Data Anal.},
  title        = {Ultrasound breast images denoising using generative adversarial networks (GANs)},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sampleformer: An efficient conformer-based neural network
for automatic speech recognition. <em>IDA</em>, <em>28</em>(6),
1647–1659. (<a href="https://doi.org/10.3233/IDA-230612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Convolution-augmented Transformer (Conformer) model, which was recently introduced, has attained state-of-the-art(SOTA) results in Automatic Speech Recognition (ASR). In this paper, a series of methodical investigations uncover that the Conformer’s design decisions may not represent the most ef ficient choices when operating within the constraints of a limited computational budget. After a thorough re-evaluation of the Conformer architecture’s design choices, we propose Sampleformer which reduces the Conformer architecture complexity and has more robust performance. We introduce downsampling to the Conformer Encoder, and to exploit the information in the speech features, we incorporate an additional downsampling module to enhance the efficiency and accuracy of our model. Additionally, we propose a novel and adaptable attention mechanism called multi-group attention, effectively reducing the attention complexity from O⁢(n2⁢d) to O⁢(n2⁢d⋅f/g). We performed experiments on the AISHELL-1 corpora, our 13.3 million-parameter CTC model demonstrates a 3.0%/2.6% relative reduction in character error rate (CER) on the dev/test sets, all without the utilization of a language model (LM). Additionally, the model exhibits a 30% improvement in inference compared to our CTC Conformer baseline and trains 27% faster.},
  archive      = {J_IDA},
  author       = {Fan, Zeping and Zhang, Xuejun and Huang, Min and Bu, Zhaohui},
  doi          = {10.3233/IDA-230612},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1647-1659},
  shortjournal = {Intell. Data Anal.},
  title        = {Sampleformer: An efficient conformer-based neural network for automatic speech recognition},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An evolutionary approach to extreme individual impact
opinions based on time sunk costs. <em>IDA</em>, <em>28</em>(6),
1627–1646. (<a href="https://doi.org/10.3233/IDA-230677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale studies indicate that the distinct approach to opinion fusion employed by extreme agents exerts a more potent influence on overall opinion evolution when compared to regular agents. The presence of extreme agents within the network tends to undermine the development of opinion neutralit y, which is harmful to the guidance of online public opinion. Notably, prior research often overlooks the existence of opinion extreme agents in social networks. However, existing researches seldom consider the time sunk cost in the evolution of opinions. Building upon this foundation, we introduce a temporal dimension to the opinion evolution, integrating the time sunk cost with the opinion evolution process. Furthermore, we devise an agent partitioning method that categorizes agents into four states based on their opinion values: watch state, subjective state, firm state, and extreme state, with extreme state agents generally expressing radical opinions. We constructed an agent network based on the phenomenon of time sunk costs and proposed a model for the evolution of extreme opinions in this network. Our study found that the information sharing among extreme agents significantly influences the extremization of opinions in various networks. After restricting the exchange of opinions on extreme agents, the number of extreme agents in the network decreased by 40% to 50% compared to the initial situation. Additionally, we also discovered that imposing restrictions on extreme agents in the early stages can help increase the possibility of network opinions moving towards neutral positions. When restriction of extreme agents(REA) was performed at the beginning of the experiment compared to REA in the midway of the experiment, the final number of extreme state agents decreased by 15.57%. The results show that extreme agents have a great influence on the spread and evolution of extreme opinions on platforms.},
  archive      = {J_IDA},
  author       = {Feng, Zhuo and Du, Yajun and Huang, Jiaming and Li, Xianyong and Chen, Xiaoliang and Xie, Chunzhi},
  doi          = {10.3233/IDA-230677},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1627-1646},
  shortjournal = {Intell. Data Anal.},
  title        = {An evolutionary approach to extreme individual impact opinions based on time sunk costs},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating emoji sentiment information into a pre-trained
language model for chinese and english sentiment analysis. <em>IDA</em>,
<em>28</em>(6), 1601–1625. (<a
href="https://doi.org/10.3233/IDA-230864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emojis in texts provide lots of additional information in sentiment analysis. Previous implicit sentiment analysis models have primarily treated emojis as unique tokens or deleted them directly, and thus have ignored the explicit sentiment information inside emojis. Considering the different relati onships between emoji descriptions and texts, we propose a pre-training Bidirectional Encoder Representations from Transformers (BERT) with emojis (BEMOJI) for Chinese and English sentiment analysis. At the pre-training stage, we pre-train BEMOJI by predicting the emoji descriptions from the corresponding texts via prompt learning. At the fine-tuning stage, we propose a fusion layer to fuse text representations and emoji descriptions into fused representations. These representations are used to predict text sentiment orientations. Experimental results show that BEMOJI gets the highest accuracy (91.41% and 93.36%), Macro-precision (91.30% and 92.85%), Macro-recall (90.66% and 93.65%) and Macro-F1-measure (90.95% and 93.15%) on the Chinese and English datasets. The performance of BEMOJI is 29.92% and 24.60% higher than emoji-based methods on average on Chinese and English datasets, respectively. Meanwhile, the performance of BEMOJI is 3.76% and 5.81% higher than transformer-based methods on average on Chinese and English datasets, respectively. The ablation study verifies that the emoji descriptions and fusion layer play a crucial role in BEMOJI. Besides, the robustness study illustrates that BEMOJI achieves comparable results with BERT on four sentiment analysis tasks without emojis, which means BEMOJI is a very robust model. Finally, the case study shows that BEMOJI can output more reasonable emojis than BERT.},
  archive      = {J_IDA},
  author       = {Huang, Jiaming and Li, Xianyong and Li, Qizhi and Du, Yajun and Fan, Yongquan and Chen, Xiaoliang and Huang, Dong and Wang, Shumin},
  doi          = {10.3233/IDA-230864},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1601-1625},
  shortjournal = {Intell. Data Anal.},
  title        = {Incorporating emoji sentiment information into a pre-trained language model for chinese and english sentiment analysis},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bitemporal RDF index based on skip list. <em>IDA</em>,
<em>28</em>(6), 1579–1599. (<a
href="https://doi.org/10.3233/IDA-230609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Resource Description Framework (RDF) is a framework for expressing information about resources in the form of triples (subject, predicate, object). The information represented by the standard RDF is static, i.e., that does not change over time. To better deal with a large amount of time-related information, temporal RDF is proposed. Consequently, how to explore index technology to efficiently query temporal information has become an important research issue, but the research on the index of temporal RDF is still short, especially the index of bitemporal RDF. BitemporalRDF can represent more complicated situations (e.g., RDF triples with both validtime and transactiontime). Indexes for bitemporal RDF can further expand the application scenarios and functions of temporal RDF. In this paper, we propose an efficient index for bitemporal RDF queries. The index innovatively introduces and re-designs skip list structure into the bitemporal RDF query. We also investigate how to cover almost all query patterns with as few indexes as possible. In addition, although the proposed index is conceived for temporal RDF, it also takes into account the performance of standard RDF queries when the time element is unknown. Finally, we run experiments with synthetic data sets of different sizes using the Lehigh University Benchmark (LUBM), and results prove that the proposed index is scalable and effective.},
  archive      = {J_IDA},
  author       = {Zhang, Fu and Zhang, Wei and Wang, Gang},
  doi          = {10.3233/IDA-230609},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1579-1599},
  shortjournal = {Intell. Data Anal.},
  title        = {A bitemporal RDF index based on skip list},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying longevity profiles from longitudinal data
through factor analysis and biclustering. <em>IDA</em>, <em>28</em>(6),
1555–1578. (<a href="https://doi.org/10.3233/IDA-230314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing longevity profiles from longitudinal studies is a task with many challenges. Firstly, the longitudinal databases usually have high dimensionality, and the similarities between long-lived and non-long-lived records are a highly burdening task for profile characterization. Addressing t hese issues, in this work, we use data from the English Longitudinal Study of Ageing (ELSA-UK) to characterize longevity profiles through data mining. We propose a method for feature engineering for reducing data dimensionality through merging techniques, factor analysis and biclustering. We apply biclustering to select relevant features discriminating both profiles. Two classification models, one based on a decision tree and the other on a random forest, are built from the preprocessed dataset. Experiments show that our methodology can successfully discriminate longevity profiles. We identify insights into features contributing to individuals being long-lived or non-long-lived. According to the results presented by both models, the main factor that impacts longevity is related to the correlations between the economic situation and the mobility of the elderly. We suggest that this methodology can be applied to identify longevity profiles from other longitudinal studies since that factor is deemed relevant for profile classification.},
  archive      = {J_IDA},
  author       = {Noronha, Marta D.M. and Zárate, Luis E.},
  doi          = {10.3233/IDA-230314},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1555-1578},
  shortjournal = {Intell. Data Anal.},
  title        = {Identifying longevity profiles from longitudinal data through factor analysis and biclustering},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying relevant features of CSE-CIC-IDS2018 dataset for
the development of an intrusion detection system. <em>IDA</em>,
<em>28</em>(6), 1527–1553. (<a
href="https://doi.org/10.3233/IDA-230264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection systems (IDSs) are essential elements of IT systems. Their key component is a classification module that continuously evaluates some features of the network traffic and identifies possible threats. Its efficiency is greatly affected by the right selection of the features to be m onitored. Therefore, the identification of a minimal set of features that are necessary to safely distinguish malicious traffic from benign traffic is indispensable in the course of the development of an IDS. This paper presents the preprocessing and feature selection workflow as well as its results in the case of the CSE-CIC-IDS2018 on AWS dataset, focusing on five attack types. To identify the relevant features, six feature selection methods were applied, and the final ranking of the features was elaborated based on their average score. Next, several subsets of the features were formed based on different ranking threshold values, and each subset was tried with five classification algorithms to determine the optimal feature set for each attack type. During the evaluation, four widely used metrics were taken into consideration.},
  archive      = {J_IDA},
  author       = {Göcs, László and Johanyák, Zsolt Csaba},
  doi          = {10.3233/IDA-230264},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1527-1553},
  shortjournal = {Intell. Data Anal.},
  title        = {Identifying relevant features of CSE-CIC-IDS2018 dataset for the development of an intrusion detection system},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CBDA: Chaos-based binary dragonfly algorithm for
evolutionary feature selection. <em>IDA</em>, <em>28</em>(6), 1491–1526.
(<a href="https://doi.org/10.3233/IDA-230540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of feature selection in machine learning is to simultaneously maintain more classification accuracy, while reducing lager amount of attributes. In this paper, we firstly design a fitness function that achieves both objectives jointly. Then we come up with a chaos-based binary dragonfly alg orithm (CBDA) that incorporates several improvements over the conventional dragonfly algorithm (DA) for developing a wrapper-based feature selection method to solve the fitness function. Specifically, the CBDA innovatively introduces three improved factors, namely the chaotic map, evolutionary population dynamics (EPD) mechanism, and binarization strategy on the basis of conventional DA to balance the exploitation and exploration capabilities of the algorithm and make it more suitable to handle the formulated problem. We conduct experiments on 24 well-known data sets from the UCI repository with three ablated versions of CBDA targeting different components of the algorithm in order to explain their contributions in CBDA and also with five established comparative algorithms in terms of fitness value, classification accuracy, CPU running time, and number of selected features. The results show that the proposed CBDA has remarkable advantages in most of the tested data sets.},
  archive      = {J_IDA},
  author       = {Liu, Zhao and Wang, Aimin and Bao, Haiming and Zhang, Kunpeng and Wu, Jing and Sun, Geng and Li, Jiahui},
  doi          = {10.3233/IDA-230540},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1491-1526},
  shortjournal = {Intell. Data Anal.},
  title        = {CBDA: Chaos-based binary dragonfly algorithm for evolutionary feature selection},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online course evaluation model based on graph auto-encoder.
<em>IDA</em>, <em>28</em>(6), 1467–1489. (<a
href="https://doi.org/10.3233/IDA-230557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the post-epidemic era, online learning has gained increasing attention due to the advancements in information and big data technology, leading to large-scale online course data with various student behaviors. Online data mining has become a popular and important way of extracting valuable insigh ts from large amounts of data. However, previous online course analysis methods often focused on individual aspects of the data and neglected the correlation among the large-scale learning behavior data, which can lead to an incomplete understanding of the overall learning behavior and patterns within the online course. To solve the problems, this paper proposes an online course evaluation model based on a graph auto-encoder. In our method, the features of collected online course data are used to construct K-Nearest Neighbor(KNN) graphs to represent the association among the courses. Then the variational graph auto-encoder(VGAE) is introduced to learn the useful implicit features. Finally, we feed the learned implicit features into unsupervised and semi-supervised downstream tasks for online course evaluation, respectively. We conduct experiments on two datasets. In the clustering task, our method showed a more than tenfold increase in the Calinski-Harabasz index compared to unoptimized features, demonstrating significant structural distinction and group coherence. In the classification task, compared to traditional methods, our model exhibited an overall performance improvement of about 10%, indicating its effectiveness in handling complex network data.},
  archive      = {J_IDA},
  author       = {Yuan, Wei and Zhao, Shiyu and Wang, Li and Cai, Lijia and Zhang, Yong},
  doi          = {10.3233/IDA-230557},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1467-1489},
  shortjournal = {Intell. Data Anal.},
  title        = {Online course evaluation model based on graph auto-encoder},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous graph community detection method based on
k-nearest neighbor graph neural network. <em>IDA</em>, <em>28</em>(6),
1445–1466. (<a href="https://doi.org/10.3233/IDA-230356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional community detection models either ignore the feature space information and require a large amount of domain knowledge to define the meta-paths manually, or fail to distinguish the importance of different meta-paths. To overcome these limitations, we propose a novel heterogeneous graph c ommunity detection method (called KGNN_HCD, heterogeneous graph Community Detection method based on K-nearest neighbor Graph Neural Network). Firstly, the similarity matrix is generated to construct the topological structure of K-nearest neighbor graph; secondly, the meta-path information matrix is generated using a meta-path transformation layer (Mp-Trans Layer) by adding weighted convolution; finally, a graph convolutional network (GCN) is used to learn high-quality node representation, and the k-means algorithm is adopted on node embeddings to detect the community structure. We perform extensive experiments and on three heterogeneous datasets, ACM, DBLP and IMDB, and we consider as competitors 11 community detection methods such as CP-GNN and GTN. The experimental results show that the proposed KGNN_HCD method improves 2.54% and 2.56% on the ACM dataset, 2.59% and 1.47% on the DBLP dataset, and 1.22% and 1.67% on the IMDB dataset for both NMI and ARI. Experiments findings suggest that the proposed KGNN_HCD method is reasonable and effective, and KGNN_HCD can be applied to complex network classification and clustering tasks.},
  archive      = {J_IDA},
  author       = {Liu, Xiaoyang and Wu, Yudie and Fiumara, Giacomo and De Meo, Pasquale},
  doi          = {10.3233/IDA-230356},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1445-1466},
  shortjournal = {Intell. Data Anal.},
  title        = {Heterogeneous graph community detection method based on K-nearest neighbor graph neural network},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cluster analysis via projection onto convex sets.
<em>IDA</em>, <em>28</em>(6), 1427–1444. (<a
href="https://doi.org/10.3233/IDA-230655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a data clustering algorithm that is inspired by the prominent convergence property of the Projection onto Convex Sets (POCS) method, termed the POCS-based clustering algorithm. For disjoint convex sets, the form of simultaneous projections of the POCS method can result in a mini mum mean square error solution. Relying on this important property, the proposed POCS-based clustering algorithm treats each data point as a convex set and simultaneously projects the cluster prototypes onto respective member data points, the projections are convexly combined via adaptive weight values in order to minimize a predefined objective function for data clustering purposes. The performance of the proposed POCS-based clustering algorithm has been verified through a large scale of experiments and data sets. The experimental results have shown that the proposed POCS-based algorithm is competitive in terms of both effectiveness and efficiency against some of the prevailing clustering approaches such as the K-Means/K-Means+⁣+ and Fuzzy C-Means (FCM) algorithms. Based on extensive comparisons and analyses, we can confirm the validity of the proposed POCS-based clustering algorithm for practical purposes.},
  archive      = {J_IDA},
  author       = {Tran, Le-Anh and Kwon, Daehyun and Deberneh, Henock Mamo and Park, Dong-Chul},
  doi          = {10.3233/IDA-230655},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1427-1444},
  shortjournal = {Intell. Data Anal.},
  title        = {Cluster analysis via projection onto convex sets},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised contaminated user profile identification
against shilling attack in recommender system. <em>IDA</em>,
<em>28</em>(6), 1411–1426. (<a
href="https://doi.org/10.3233/IDA-230575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recommender system is susceptible to manipulation through the injection of carefully crafted profiles. Some recent profile identification methods only perform well in specific attack scenarios. A general attack detection method is usually complicated or requires label samples. Such methods are pr one to overtraining easily, and the process of annotation incurs high expenses. This study proposes an unsupervised divide-and-conquer method aiming to identify attack profiles, utilizing a specifically designed model for each kind of shilling attack. Initially, our method categorizes the profile set into two attack types, namely Standard and Obfuscated Behavior Attacks. Subsequently, profiles are separated into clusters within the extracted feature space based on the identified attack type. The selection of attack profiles is then determined through target item analysis within the suspected cluster. Notably, our method offers the advantage of requiring no prior knowledge or annotation. Furthermore, the precision is heightened as the identification method is designed to a specific attack type, employing a less complicated model. The outstanding performance of our model, validated through experimental results on MovieLens-100K and Netflix under various attack settings, demonstrates superior accuracy and reduced running time compared to current detection methods in identifying Standard and Obfuscated Behavior Attacks.},
  archive      = {J_IDA},
  author       = {Zhang, Fei and Chan, Patrick P.K. and He, Zhi-Min and Yeung, Daniel S.},
  doi          = {10.3233/IDA-230575},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1411-1426},
  shortjournal = {Intell. Data Anal.},
  title        = {Unsupervised contaminated user profile identification against shilling attack in recommender system},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Editorial. <em>IDA</em>, <em>28</em>(6), 1407–1410. (<a
href="https://doi.org/10.3233/IDA-249006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-249006},
  journal      = {Intelligent Data Analysis},
  month        = {11},
  number       = {6},
  pages        = {1407-1410},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging spark-based machine learning algorithm for
audience sentiment analysis in youtube content. <em>IDA</em>,
<em>28</em>(5), 1395–1405. (<a
href="https://doi.org/10.3233/IDA-240198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s digital era, the generation and sharing of information are rapidly expanding. The increased volume of complex data is big data. YouTube is the primary source of big data. The proliferation of the internet and smart devices has led to a significant increase in content creators on social m edia platforms, with YouTube being a prominent example. There has been a substantial increase in content creators across various social media platforms, with YouTube emerging as one of the foremost platforms for content generation and sharing. YouTubers face challenges in enhancing content strategies due to the growing number of comments, such as big data on shared videos. Reading and finding viewers’ opinions of such a large amount of data through manual methods is time-consuming and challenging and makes it hard to understand people’s sentiments. To address this, spark-based machine learning algorithms have emerged as a transformative tool for content creators to understand the audience. The Improved Novel Ensemble Method (INEM) algorithm is designed to predict viewers’ sentiments and emotional responses based on the content they interact through the comments. The proposed results provide valuable insights for content creators, helping them refine the strategies to optimize the channel’s revenue and performance. Fit Tuber Channel is analyzed to perform the sentiment of user comments.},
  archive      = {J_IDA},
  author       = {K, Subha and N, Bharathi},
  doi          = {10.3233/IDA-240198},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1395-1405},
  shortjournal = {Intell. Data Anal.},
  title        = {Leveraging spark-based machine learning algorithm for audience sentiment analysis in youtube content},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Floating-point histograms for exploratory analysis of large
scale real-world data sets. <em>IDA</em>, <em>28</em>(5), 1347–1394. (<a
href="https://doi.org/10.3233/IDA-230638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Histograms are among the most popular methods used in exploratory analysis to summarize univariate distributions. In particular, irregular histograms are good non-parametric density estimators that require very few parameters: the number of bins with their lengths and frequencies. Although many app roaches have been proposed in the literature to infer these parameters, most existing histogram methods are difficult to exploit for exploratory analysis in the case of real-world data sets, with scalability issues, truncated data, outliers or heavy-tailed distributions. In this paper, we focus on the G-Enum histogram method, which exploits the Minimum Description Length (MDL) principle to build histograms without any user parameter. We then propose to extend this method by exploiting a new modeling space based on floating-point representation, with the objective of building histograms resistant to outliers or heavy-tailed distributions. We also suggest several heuristics and a methodology suitable for the exploratory analysis of large scale real-world data sets, whose underlying patterns are difficult to recover for digitization reasons. Extensive experiments show the benefits of the approach, evaluated with a dual objective: the accuracy of density estimation in the case of outliers or heavy-tailed distributions, and the effectiveness of the approach for exploratory data analysis.},
  archive      = {J_IDA},
  author       = {Boullé, Marc},
  doi          = {10.3233/IDA-230638},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1347-1394},
  shortjournal = {Intell. Data Anal.},
  title        = {Floating-point histograms for exploratory analysis of large scale real-world data sets},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel power load abnormalities detection using fast
density peak clustering with a hybrid canopy-k-means algorithm.
<em>IDA</em>, <em>28</em>(5), 1321–1346. (<a
href="https://doi.org/10.3233/IDA-230573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel power loads anomalies are processed by a fast-density peak clustering technique that capitalizes on the hybrid strengths of Canopy and K-means algorithms all within Apache Mahout’s distributed machine-learning environment. The study taps into Apache Hadoop’s robust tools for data storage a nd processing, including HDFS and MapReduce, to effectively manage and analyze big data challenges. The preprocessing phase utilizes Canopy clustering to expedite the initial partitioning of data points, which are subsequently refined by K-means to enhance clustering performance. Experimental results confirm that incorporating the Canopy as an initial step markedly reduces the computational effort to process the vast quantity of parallel power load abnormalities. The Canopy clustering approach, enabled by distributed machine learning through Apache Mahout, is utilized as a preprocessing step within the K-means clustering technique. The hybrid algorithm was implemented to minimise the length of time needed to address the massive scale of the detected parallel power load abnormalities. Data vectors are generated based on the time needed, sequential and parallel candidate feature data are obtained, and the data rate is combined. After classifying the time set using the canopy with the K-means algorithm and the vector representation weighted by factors, the clustering impact is assessed using purity, precision, recall, and F value. The results showed that using canopy as a preprocessing step cut the time it proceeds to deal with the significant number of power load abnormalities found in parallel using a fast density peak dataset and the time it proceeds for the k-means algorithm to run. Additionally, tests demonstrate that combining canopy and the K-means algorithm to analyze data performs consistently and dependably on the Hadoop platform and has a clustering result that offers a scalable and effective solution for power system monitoring.},
  archive      = {J_IDA},
  author       = {Al-Jumaili, Ahmed Hadi Ali and Muniyandi, Ravie Chandren and Hasan, Mohammad Kamrul and Singh, Mandeep Jit and Paw, Johnny Koh Siaw and Al-Jumaily, Abdulmajeed},
  doi          = {10.3233/IDA-230573},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1321-1346},
  shortjournal = {Intell. Data Anal.},
  title        = {Parallel power load abnormalities detection using fast density peak clustering with a hybrid canopy-K-means algorithm},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight blockchain approach to reduce double-spend and
51. <em>IDA</em>, <em>28</em>(5), 1309–1319. (<a
href="https://doi.org/10.3233/IDA-230153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain has attracted tremendous attention in recent years due to its significant features including anonymity, security, immutability, and audibility. Blockchain technology has been used in several nonmonetary applications, including Internet-of-Things. Though blockchain has limited resources, and scalability is computationally expensive, resulting in delays and large bandwidth overhead that are unsuitable for many IoT devices. In this paper, we work on a lightweight blockchain approach that is suited for IoT needs and provides end-to-end security. Decentralization is achieved in our lightweight blockchain implementation by building a network with a lot of high-resource devices collaborate to maintain the blockchain. The nodes in the network is arranged in sorted order w.r.t execution time and count to reduce the mining overheads and is accountable for handling the public blockchain. We propose a distributed execution time-based consensus algorithm that decreases the delay and overhead of the mining process. We also propose a randomized node-selection algorithm for the selection of nodes to verify the mined blocks to eliminate the double-spend and 51% attack. The results are encouraging and significantly reduce the mining overhead and keep a check on the double-spending problem and 51% attack.},
  archive      = {J_IDA},
  author       = {Nayancy, and Dutta, Sandip and Chakraborty, Soubhik},
  doi          = {10.3233/IDA-230153},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1309-1319},
  shortjournal = {Intell. Data Anal.},
  title        = {Lightweight blockchain approach to reduce double-spend and 51% attacks on proof-of-work},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cross-model hierarchical interactive fusion network for
end-to-end multimodal aspect-based sentiment analysis. <em>IDA</em>,
<em>28</em>(5), 1293–1308. (<a
href="https://doi.org/10.3233/IDA-230305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the aspect-based sentiment analysis task, traditional works are only for text modality. However, in social media scenarios, texts often contain abbreviations, clerical errors, or grammatical errors, which invalidate traditional methods. In this study, the cross-model hierarchical interactive fu sion network incorporating an end-to-end approach is proposed to address this challenge. In the network, a feature attention module and a feature fusion module are proposed to obtain the multimodal interaction feature between the image modality and the text modality. Through the attention mechanism and gated fusion mechanism, these two modules realize the auxiliary function of image in the text-based aspect-based sentiment analysis task. Meanwhile, a boundary auxiliary module is used to explore the dependencies between two core subtasks of the aspect-based sentiment analysis. Experimental results on two publicly available multi-modal aspect-based sentiment datasets validate the effectiveness of the proposed approach.},
  archive      = {J_IDA},
  author       = {Zhong, Qing and Shao, Xinhui},
  doi          = {10.3233/IDA-230305},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1293-1308},
  shortjournal = {Intell. Data Anal.},
  title        = {A cross-model hierarchical interactive fusion network for end-to-end multimodal aspect-based sentiment analysis},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning traffic as videos: A spatio-temporal VAE approach
to periodic traffic raster data imputation. <em>IDA</em>,
<em>28</em>(5), 1271–1292. (<a
href="https://doi.org/10.3233/IDA-230091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For modern Intelligent Transportation System (ITS), data missing during traffic raster acquisition can be inevitable because of the loop detector malfunction or signal interference. Nevertheless, missing data imputation is meaningful due to the periodic spatio-temporal characteristics and individua l randomness of traffic raster data. In this paper, traffic raster data collected from all spatial regions at each time interval are considered as a multiple channel image. Accordingly, the traffic raster data over a period of time can be regarded as video, on which an unsupervised generative neural network called MSST-VAE (Multiple Streams Spatial Temporal-VAE) is proposed for traffic raster data imputation, and this model can even robustly performs at varied missing rates while many other approaches fail to conduct. Two major innovations can be summarized in MSSTVAE: Firstly, it uses multiple periodic streams of Variational Auto-Encoders (VAEs) with Sylvester Normalizing Flows (SNFs), which shows strong generalization ability. Secondly, after the traffic raster data are transferred into videos, an ECB (Extraction-and-Calibration Block) consisting of dilated P3D gated convolution and multi-horizon attention mechanism is employed to learn global-local-granularity spatial features and long-short-term temporal features. Extensive experiments on three real traffic flow datasets validate that MSST-VAE outperforms other classical traffic imputation models with the least imputation error.},
  archive      = {J_IDA},
  author       = {Zhang, Shuo and Hu, Xingbang and Zhang, Wenbo and Chen, Jinyi and Huang, Hejiao},
  doi          = {10.3233/IDA-230091},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1271-1292},
  shortjournal = {Intell. Data Anal.},
  title        = {Learning traffic as videos: A spatio-temporal VAE approach to periodic traffic raster data imputation},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining jumping knowledge into traffic forecasting: An
attention-based spatial-temporal adaptive integration gated network.
<em>IDA</em>, <em>28</em>(5), 1245–1269. (<a
href="https://doi.org/10.3233/IDA-230101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic forecasting has become a core component of Intelligent Transportation Systems. However, accurate traffic forecasting is very challenging, caused by the complex traffic road networks. Most existing forecasting methods do not fully consider the topological structure information of road networ ks, making it difficult to extract accurate spatial features. In addition, spatial and temporal features have different impacts on traffic conditions, but the existing studies ignore the distribution of spatial-temporal features in traffic regions. To address these limitations, we propose a novel graph neural network architecture named Attention-based Spatial-Temporal Adaptive Integration Gated Network (AST-AIGN). The originality of AST-AIGN is to obtain a spatial feature that more accurately reflects the topological structure of the road networks by embedding Graph Attention Network (GAT) into Jumping Knowledge Net (JK-Net). We propose a data-dependent function called spatial-temporal adaptive integration gate to process the diversity of feature distribution and highlight features in road networks that significantly affects traffic conditions. We evaluate our model on two real-world traffic datasets from the Caltrans Performance Measurement System (PEMS04 and PEMS08), and the extensive experimental results demonstrate the proposed AST-AIGN architecture outperforms other baselines.},
  archive      = {J_IDA},
  author       = {Zhou, Rucheng and Zhang, Dongmei and Zhu, Jiabao and Min, Geyong},
  doi          = {10.3233/IDA-230101},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1245-1269},
  shortjournal = {Intell. Data Anal.},
  title        = {Combining jumping knowledge into traffic forecasting: An attention-based spatial-temporal adaptive integration gated network},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analytical learning classifier based on predefined
evenly-distributed class centroids. <em>IDA</em>, <em>28</em>(5),
1229–1244. (<a href="https://doi.org/10.3233/IDA-230044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the pattern recognition, most classification models are solved iteratively, except for Linear LDA, KLDA and ELM etc. In this paper, a nonlinear classification network model based on predefined evenly-distributed class centroids (PEDCC) is proposed. Its analytical solution can be obtained and ha s good interpretability. Using the characteristics of maximizing the inter-class distance of PEDCC and derivative weighted minimum mean square error loss function to minimize the intra-class distance, we can not only realize the effective nonlinearity of the network, but also obtain the analytical solution of the network weight. Then, the sample is classified based on GDA. In order to further improve the performance of classification, PCA is used to reduces the dimensionality of the original sample, meanwhile, the CReLU activation function are adopted to enhances the expression ability of the features. The network transforms the samples into the higher dimensional feature space through the weighted minimum mean square error, so as to find a better separation hyperplane. In experiments, the feasibility of the network structure is verified from pure linear 𝑾, 𝑾+Tanh, and PCA+𝑾+Tanh respectively on many small data sets and large data sets, and compared with SVM and ELM in terms of training speed and recognition rate. The results show that, in general, this model has advantages on small data set both in recognition accuracy and training speed, while it has advantages in training speed on large data sets. Finally, by introducing a multi-stage network structure based on the latent feature norm, the classifier network can further significantly improve the classification performance, the recognition rate of small data sets is effectively improved and much higher than that of existing methods, while the recognition rate of large data sets is similar to that of SVM.},
  archive      = {J_IDA},
  author       = {Hu, Haiping and Huo, Wei and Yan, Yingying and Zhu, Qiuyu},
  doi          = {10.3233/IDA-230044},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1229-1244},
  shortjournal = {Intell. Data Anal.},
  title        = {Analytical learning classifier based on predefined evenly-distributed class centroids},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel dual-granularity lightweight transformer for vision
tasks. <em>IDA</em>, <em>28</em>(5), 1213–1228. (<a
href="https://doi.org/10.3233/IDA-230799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based networks have revolutionized visual tasks with their continuous innovation, leading to significant progress. However, the widespread adoption of Vision Transformers (ViT) is limited due to their high computational and parameter requirements, making them less feasible for resource- constrained mobile and edge computing devices. Moreover, existing lightweight ViTs exhibit limitations in capturing different granular features, extracting local features efficiently, and incorporating the inductive bias inherent in convolutional neural networks. These limitations somewhat impact the overall performance. To address these limitations, we propose an efficient ViT called Dual-Granularity Former (DGFormer). DGFormer mitigates these limitations by introducing two innovative modules: Dual-Granularity Attention (DG Attention) and Efficient Feed-Forward Network (Efficient FFN). In our experiments, on the image recognition task of ImageNet, DGFormer surpasses lightweight models such as PVTv2-B0 and Swin Transformer by 2.3% in terms of Top1 accuracy. On the object detection task of COCO, under RetinaNet detection framework, DGFormer outperforms PVTv2-B0 and Swin Transformer with increase of 0.5% and 2.4% in average precision (AP), respectively. Similarly, under Mask R-CNN detection framework, DGFormer exhibits improvement of 0.4% and 1.8% in AP compared to PVTv2-B0 and Swin Transformer, respectively. On the semantic segmentation task on the ADE20K, DGFormer achieves a substantial improvement of 2.0% and 2.5% in mean Intersection over Union (mIoU) over PVTv2-B0 and Swin Transformer, respectively. The code is open-source and available at: https://github.com/ISCLab-Bistu/DGFormer.git.},
  archive      = {J_IDA},
  author       = {Zhang, Ji and Yu, Mingxin and Lu, Wenshuai and Dai, Yuxiang and Shi, Huiyu and You, Rui},
  doi          = {10.3233/IDA-230799},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1213-1228},
  shortjournal = {Intell. Data Anal.},
  title        = {A novel dual-granularity lightweight transformer for vision tasks},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual-ways feature fusion mechanism enhancing active
learning based on TextCNN. <em>IDA</em>, <em>28</em>(5), 1189–1211. (<a
href="https://doi.org/10.3233/IDA-230332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active Learning (AL) is a technique being widely employed to minimize the time and labor costs in the task of annotating data. By querying and extracting the specific instances to train the model, the relevant task’s performance is improved maximally within limited iterations. However, rare work wa s conducted to fully fuse features from different hierarchies to enhance the effectiveness of active learning. Inspired by the thought of information compensation in many famous deep learning models (such as ResNet, etc.), this work proposes a novel TextCNN-based Two ways Active Learning model (TCTWAL) to extract task-relevant texts. TextCNN takes the advantage of little hyper-parameter tuning and static vectors and achieves excellent results on various natural language processing (NLP) tasks, which are also beneficial to human-computer interaction (HCI) and the AL relevant tasks. In the process of the proposed AL model, the candidate texts are measured from both global and local features by the proposed AL framework TCTWAL depending on the modified TextCNN. Besides, the query strategy is strongly enhanced by maximum normalized log-probability (MNLP), which is sensitive to detecting the longer sentences. Additionally, the selected instances are characterized by general global information and abundant local features simultaneously. To validate the effectiveness of the proposed model, extensive experiments are conducted on three widely used text corpus, and the results are compared with with eight manual designed instance query strategies. The results show that our method outperforms the planned baselines in terms of accuracy, macro precision, macro recall, and macro F1 score. Especially, to the classification results on AG’s News corpus, the improvements of the four indicators after 39 iterations are 40.50%, 45.25%, 48.91%, and 45.25%, respectively.},
  archive      = {J_IDA},
  author       = {Shi, Xuefeng and Hu, Min and Ren, Fuji and Shi, Piao},
  doi          = {10.3233/IDA-230332},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1189-1211},
  shortjournal = {Intell. Data Anal.},
  title        = {A dual-ways feature fusion mechanism enhancing active learning based on TextCNN},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving process discovery by filtering noises based on
event dependency. <em>IDA</em>, <em>28</em>(5), 1171–1188. (<a
href="https://doi.org/10.3233/IDA-230118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process discovery techniques analyze process logs to extract models that characterize the behavior of business processes. In real-life logs, however, noises exist and adversely affect the extraction and thus decrease the understandability of discovered models. In this paper, we propose a novel doub le granularity filtering method, executed on both the event and trace levels, to detect noises by analyzing the directly-following and parallel relations between events. Based on the probability of an event occurring in a sequence, the infrequent behaviors and redundant events in the logs can be filtered out. In addition, the missing events in parallel blocks are detected to further improve the performance of filtering. Experiments on synthetic logs and five real-life datasets demonstrate that our method significantly outperforms other state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {Yu, Dongjin and Ni, Ke and Li, Zhongyang and Zhang, Shengyi and Sun, Xiaoxiao and Hou, Wenjie and Ying, Yuke},
  doi          = {10.3233/IDA-230118},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1171-1188},
  shortjournal = {Intell. Data Anal.},
  title        = {Improving process discovery by filtering noises based on event dependency},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CCTSDB dataset enhancement based on a cross-augmentation
method for image datasets. <em>IDA</em>, <em>28</em>(5), 1151–1169. (<a
href="https://doi.org/10.3233/IDA-230075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital era, the rapid advancement of artificial intelligence has put a spotlight on target detection, especially in traffic settings. This area of study is pivotal for crucial projects like autonomous vehicles, road monitoring, and traffic sign recognition. However, existing Chinese traffic datasets lack comprehensive benchmarks for traffic signs and signals, and foreign datasets do not match Chinese traffic conditions. Manually annotating a large-scale dataset tailored for Chinese traffic conditions presents a significant challenge. This study addresses this gap by proposing a cross-augmentation method for image datasets. We utilized YOLOX for target detection and trained models on the BDD100K dataset, achieving an impressive mAP of 60.25%, surpassing most algorithms. Leveraging transfer learning, we enhanced the CCTSDB dataset, creating the ACCTSDB dataset, which includes annotations for common traffic objects and Chinese traffic signs. Using YOLOX, we trained a traffic detector tailored for Chinese traffic scenarios, achieving an mAP of 75.79%. To further validate our approach, we conducted experiments on the TT100K dataset and successfully introduced the ATT100K dataset. Our methodology is poised to alleviate the limitations of manually annotating image datasets. The proposed ACCTSDB dataset and ATT100K dataset are expected to compensate for the lack of large-scale, multi-class traffic datasets in China.},
  archive      = {J_IDA},
  author       = {Lin, Xinrui and Wang, Wei and Zhu, Xiaohui and Yue, Yong},
  doi          = {10.3233/IDA-230075},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1151-1169},
  shortjournal = {Intell. Data Anal.},
  title        = {CCTSDB dataset enhancement based on a cross-augmentation method for image datasets},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of hyperparameter tuning techniques for
software quality prediction models. <em>IDA</em>, <em>28</em>(5),
1131–1149. (<a href="https://doi.org/10.3233/IDA-230653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BACKGROUND: Software quality prediction models play a crucial role in identifying vulnerable software components during early stages of development, and thereby optimizing the resource allocation and enhancing the overall software quality. While various classification algorithms have been employed for developing these prediction models, most studies have relied on default hyperparameter settings, leading to significant variability in model performance. Tuning the hyperparameters of classification algorithms can enhance the predictive capability of quality models by identifying optimal settings for improved accuracy and effectiveness. METHOD: This systematic review examines studies that have utilized hyperparameter tuning techniques to develop prediction models in software quality domain. The review focused on diverse areas such as defect prediction, maintenance estimation, change impact prediction, reliability prediction, and effort estimation, as these domains demonstrate the wide applicability of common learning algorithms. RESULTS: This review identified 31 primary studies on hyperparameter tuning for software quality prediction models. The results demonstrate that tuning the parameters of classification algorithms enhances the performance of prediction models. Additionally, the study found that certain classification algorithms exhibit high sensitivity to their parameter settings, achieving optimal performance when tuned appropriately. Conversely, certain classification algorithms exhibit low sensitivity to their parameter settings, making tuning unnecessary in such instances. CONCLUSION: Based on the findings of this review, the study conclude that the predictive capability of software quality prediction models can be significantly improved by tuning their hyperparameters. To facilitate effective hyperparameter tuning, we provide practical guidelines derived from the insights obtained through this study.},
  archive      = {J_IDA},
  author       = {Malhotra, Ruchika and Cherukuri, Madhukar},
  doi          = {10.3233/IDA-230653},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1131-1149},
  shortjournal = {Intell. Data Anal.},
  title        = {A systematic review of hyperparameter tuning techniques for software quality prediction models},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Editorial. <em>IDA</em>, <em>28</em>(5), 1127–1129. (<a
href="https://doi.org/10.3233/IDA-249005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-249005},
  journal      = {Intelligent Data Analysis},
  month        = {9},
  number       = {5},
  pages        = {1127-1129},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text classification method based on dependency parsing and
hybrid neural network. <em>IDA</em>, <em>28</em>(4), 1115–1126. (<a
href="https://doi.org/10.3233/IDA-230061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the vigorous development of big data, news topic text classification has received extensive attention, and the accuracy of news topic text classification and the semantic analysis of text are worth us to explore. The semantic information contained in news topic text has an important impact o n the classification results. Traditional text classification methods tend to default the text structure to the sequential linear structure, then classify by giving weight to words or according to the frequency value of words, while ignoring the semantic information in the text, which eventually leads to poor classification results. In order to solve the above problems, this paper proposes a BiLSTM-GCN (Bidirectional Long Short-Term Memory and Graph Convolutional Network) hybrid neural network text classification model based on dependency parsing. Firstly, we use BiLSTM to complete the extraction of feature vectors in the text; Then, we employ dependency parsing to strengthen the influence of words with semantic relationship, and obtain the global information of the text through GCN; Finally, aim to prevent the overfitting problem of the hybrid neural network which may be caused by too many network layers, we add a global average pooling layer. Our experimental results show that this method has a good performance on the THUCNews and SogouCS datasets, and the F-score reaches 91.37% and 91.76% respectively.},
  archive      = {J_IDA},
  author       = {He, Xinyu and Liu, Siyu and Yan, Ge and Zhang, Xueyan},
  doi          = {10.3233/IDA-230061},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1115-1126},
  shortjournal = {Intell. Data Anal.},
  title        = {Text classification method based on dependency parsing and hybrid neural network},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust log parsing using self-supervised learning
for system security analysis. <em>IDA</em>, <em>28</em>(4), 1093–1113.
(<a href="https://doi.org/10.3233/IDA-230133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logs play an important role in anomaly detection, fault diagnosis, and trace checking of software and network systems. Log parsing, which converts each raw log line to a constant template and a variable parameter list, is a prerequisite for system security analysis. Traditional parsing methods util izing specific rules can only parse logs of specific formats, and most parsing methods based on deep learning require labels. However, the existing parsing methods are not applicable to logs of inconsistent formats and insufficient labels. To address these issues, we propose a robust Log parsing method based on Self-supervised Learning (LogSL), which can extract templates from logs of different formats. The essential idea of LogSL is modeling log parsing as a multi-token prediction task, which makes the multi-token prediction model learn the distribution of tokens belonging to the template in raw log lines by self-supervision mode. Furthermore, to accurately predict the tokens of the template without labeled data, we construct a Multi-token Prediction Model (MPM) combining the pre-trained XLNet module, the n-layer stacked Long Short-Term Memory Net module, and the Self-attention module. We validate LogSL on 12 benchmark log datasets, resulting in the average parsing accuracy of our parser being 3.9% higher than that of the best baseline method. Experimental results show that LogSL has superiority in terms of robustness and accuracy. In addition, a case study of anomaly detection is conducted to demonstrate the support of the proposed MPM to system security tasks based on logs.},
  archive      = {J_IDA},
  author       = {Cao, Jinhui and Di, Xiaoqiang and Liu, Xu and Xu, Rui and Li, Jinqing and Ren, Weiwu and Qi, Hui and Hu, Pengfei and Zhang, Kehan and Li, Bo},
  doi          = {10.3233/IDA-230133},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1093-1113},
  shortjournal = {Intell. Data Anal.},
  title        = {Towards robust log parsing using self-supervised learning for system security analysis},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application research of credit fraud detection based on
distributed rotation deep forest. <em>IDA</em>, <em>28</em>(4),
1067–1091. (<a href="https://doi.org/10.3233/IDA-230193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit fraud is a common financial crime that causes significant economic losses to financial institutions. To address this issue, researchers have proposed various fraud detection methods. Recently, research on deep forests has opened up a new path for exploring deep models beyond neural networks. It combines the features of neural networks and ensemble learning, and has achieved good results in various fields. This paper mainly studies the application of deep forests to the field of fraud detection and proposes a distributed dense rotation deep forest algorithm (DRDF-spark) based on the improved RotBoost. The model has three main characteristics: firstly, it solves the problem of multi-granularity scanning due to the lack of spatial correlation in the data by introducing RotBoost. Secondly, Spark is used for parallel construction to improve the processing speed and efficiency of data. Thirdly, a pre-aggregation mechanism is added to the distributed algorithm to locally aggregate the statistical results of sub-forests in the same node in advance to improve communication efficiency. The experiments show that DRDF-spark performs better than deep forests and some mainstream ensemble learning algorithms on the fraud dataset in this paper, and the training speed is up to 3.53 times faster. Furthermore, if the number of nodes is further increased, the speedup ratio will continue to increase.},
  archive      = {J_IDA},
  author       = {Chen, Hongwei and Shi, Dewei and Zhou, Xun and Zhang, Man and Liu, Luanxuan},
  doi          = {10.3233/IDA-230193},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1067-1091},
  shortjournal = {Intell. Data Anal.},
  title        = {Application research of credit fraud detection based on distributed rotation deep forest},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effectiveness of ELMo embeddings, and semantic models in
predicting review helpfulness. <em>IDA</em>, <em>28</em>(4), 1045–1065.
(<a href="https://doi.org/10.3233/IDA-230349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online product reviews (OPR) are a commonly used medium for consumers to communicate their experiences with products during online shopping. Previous studies have investigated the helpfulness of OPRs using frequency-based, linguistic, meta-data, readability, and reviewer attributes. In this study, we explored the impact of robust contextual word embeddings, topic, and language models in predicting the helpfulness of OPRs. In addition, the wrapper-based feature selection technique is employed to select effective subsets from each type of features. Five feature generation techniques including word2vec, FastText, Global Vectors for Word Representation (GloVe), Latent Dirichlet Allocation (LDA), and Embeddings from Language Models (ELMo), were employed. The proposed framework is evaluated on two Amazon datasets (Video games and Health &amp; personal care). The results showed that the ELMo model outperformed the six standard baselines, including the fine-tuned Bidirectional Encoder Representations from Transformers (BERT) model. In addition, ELMo achieved Mean Square Error (MSE) of 0.0887 and 0.0786 respectively on two datasets and MSE of 0.0791 and 0.0708 with the wrapper method. This results in the reduction of 1.43% and 1.63% in MSE as compared to the fine-tuned BERT model on respective datasets. However, the LDA model has a comparable performance with the fine-tuned BERT model but outperforms the other five baselines. The proposed framework demonstrated good generalization abilities by uncovering important factors of product reviews and can be evaluated on other voting platforms.},
  archive      = {J_IDA},
  author       = {Malik, Muhammad Shahid Iqbal and Nawaz, Aftab and Jamjoom, Mona Mamdouh and Ignatov, Dmitry I.},
  doi          = {10.3233/IDA-230349},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1045-1065},
  shortjournal = {Intell. Data Anal.},
  title        = {Effectiveness of ELMo embeddings, and semantic models in predicting review helpfulness},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MusicNeXt: Addressing category bias in fused music using
musical features and genre-sensitive adjustment layer. <em>IDA</em>,
<em>28</em>(4), 1029–1043. (<a
href="https://doi.org/10.3233/IDA-230428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been successfully applied to music genre classification tasks. With the development of diverse music, genre fusion has become common. Fused music exhibits multiple similar musical features such as rhythm, timbre, and structure, which typically arise from th e temporal information in the spectrum. However, traditional CNNs cannot effectively capture temporal information, leading to difficulties in distinguishing fused music. To address this issue, this study proposes a CNN model called MusicNeXt for music genre classification. Its goal is to enhance the feature extraction method to increase focus on musical features, and increase the distinctiveness between different genres, thereby reducing classification result bias. Specifically, we construct the feature extraction module which can fully utilize temporal information, thereby enhancing its focus on music features. It exhibits an improved understanding of the complexity of fused music. Additionally, we introduce a genre-sensitive adjustment layer that strengthens the learning of differences between different genres through within-class angle constraints. This leads to increased distinctiveness between genres and provides interpretability for the classification results. Experimental results demonstrate that our proposed MusicNeXt model outperforms baseline networks and other state-of-the-art methods in music genre classification tasks, without generating category bias in the classification results.},
  archive      = {J_IDA},
  author       = {Meng, Shiting and Hao, Qingbo and Xiao, Yingyuan and Zheng, Wenguang},
  doi          = {10.3233/IDA-230428},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1029-1043},
  shortjournal = {Intell. Data Anal.},
  title        = {MusicNeXt: Addressing category bias in fused music using musical features and genre-sensitive adjustment layer},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ATIN: Attention-embedded time-aware imputation networks for
production data anomaly detection. <em>IDA</em>, <em>28</em>(4),
1007–1027. (<a href="https://doi.org/10.3233/IDA-230301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective identification of anomalous data from production time series in the oilfield affects future analysis and forecasting. Such time series is often characterized by irregular time intervals due to uneven manual sampling, and missing values caused by incomplete measurements. Therefore, the ide ntification task becomes more challenging. In this paper, an Attention-Embedded Time-Aware Imputation Network (ATIN) with two sub-networks is proposed for this task. First, Time-Aware Imputation LSTM (TI-LSTM) is designed for modeling irregular time intervals and incomplete measurements. It decays the long-term memory component as the producing well conditions may be varied during the water cut stage. Second, Attention-Embedding LSTM (ATEM) is designed to improve the effectiveness of anomaly detection. It focuses on the correlation between the last and historical measurements in a given sequence. Comparison experiments with several state-of-the-art methods, including mTAN, GRU-D, T-LSTM, ATTAIN, and BRITS are conducted. Results show that the proposed ATIN performs better in accuracy, F1-score, and area under curve (AUC).},
  archive      = {J_IDA},
  author       = {Zhang, Xi and Chen, Hu and Li, Rui and Fei, Zhaolei and Min, Fan},
  doi          = {10.3233/IDA-230301},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {1007-1027},
  shortjournal = {Intell. Data Anal.},
  title        = {ATIN: Attention-embedded time-aware imputation networks for production data anomaly detection},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HFD: Hierarchical feature decoupling for SQL generation from
text. <em>IDA</em>, <em>28</em>(4), 991–1005. (<a
href="https://doi.org/10.3233/IDA-230390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-SQL, a computational linguistics task, seeks to facilitate the conversion of natural language queries into SQL queries. Recent methodologies have leveraged the concept of slot-filling in conjunction with predetermined SQL templates to effectively bridge the semantic gap between natural lang uage questions and structured database queries, achieving commendable performance by harnessing the power of multi-task learning. However, employing identical features across diverse tasks is an ill-suited practice, fraught with inherent drawbacks. Firstly, based on our observation, there are clear boundaries in the natural language corresponding to SELECT and WHERE clauses. Secondly, the exclusive features integral to each subtask are inadequately emphasized and underutilized, thereby hampering the acquisition of discriminative features for each specific subtask. In an endeavor to rectify these issues, the present work introduces an innovative approach: the hierarchical feature decoupling model for SQL query generation from natural language. This novel approach involves the deliberate separation of features pertaining to subtasks within both SELECT and WHERE clauses, further dissociating these features at the subtask level to foster better model performance. Empirical results derived from experiments conducted on the WikiSQL benchmark dataset reveal the superiority of the proposed approach over several state-of-the-art baseline methods in the context of text-to-SQL query generation.},
  archive      = {J_IDA},
  author       = {Zhang, Xu and Hu, Xiaoyu and Liu, Zejie and Xiang, Yanzheng and Zhou, Deyu},
  doi          = {10.3233/IDA-230390},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {991-1005},
  shortjournal = {Intell. Data Anal.},
  title        = {HFD: Hierarchical feature decoupling for SQL generation from text},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ELCA: Enhanced boundary location for chinese named entity
recognition via contextual association. <em>IDA</em>, <em>28</em>(4),
973–990. (<a href="https://doi.org/10.3233/IDA-230383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Entity Recognition (NER) is a fundamental task that aids in the completion of other tasks such as text understanding, information retrieval and question answering in Natural Language Processing (NLP). In recent years, the use of a mix of character-word structure and dictionary information for Chinese NER has been demonstrated to be effective. As a representative of hybrid models, Lattice-LSTM has obtained better benchmarking results in several publicly available Chinese NER datasets. However, Lattice-LSTM does not address the issue of long-distance entities or the detection of several entities with the same character. At the same time, the ambiguity of entity boundary information also leads to a decrease in the accuracy of embedding NER. This paper proposes ELCA: Enhanced Boundary Location for Chinese Named Entity Recognition Via Contextual Association, a method that solves the problem of long-distance dependent entities by using sentence-level position information. At the same time, it uses adaptive word convolution to overcome the problem of several entities sharing the same character. ELCA achieves the state-of-the-art outcomes in Chinese Word Segmentation and Chinese NER.},
  archive      = {J_IDA},
  author       = {Wang, Yizhao and Mao, Shun and Jiang, Yuncheng},
  doi          = {10.3233/IDA-230383},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {973-990},
  shortjournal = {Intell. Data Anal.},
  title        = {ELCA: Enhanced boundary location for chinese named entity recognition via contextual association},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight method of pose estimation for indoor object.
<em>IDA</em>, <em>28</em>(4), 961–972. (<a
href="https://doi.org/10.3233/IDA-230278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the multiple types of objects and the uncertainty of their geometric structures and scales in indoor scenes, the position and pose estimation of point clouds of indoor objects by mobile robots has the problems of domain gap, high learning cost, and high computing cost. In this paper, a light weight 6D pose estimation method is proposed, which decomposes the pose estimation into a viewpoint and the in-plane rotation around the optical axis of the viewpoint, and the improved PointNet+⁣+ network structure and two lightweight modules are used to construct a codebook, and the 6d pose estimation of the point cloud of the indoor objects is completed by building and querying the codebook. The model was trained on the ShapeNetV2 dataset, and reports the ADD-S metric validation on the YCB-Video and LineMOD datasets, reaching 97.0% and 94.6% respectively. The experiment shows that the model can be trained to estimate the 6d position and pose of the unknown object point cloud with lower computation and storage cost, and the model with fewer parameters and better real-time performance is superior to other high-recision methods.},
  archive      = {J_IDA},
  author       = {Wang, Sijie and Li, Yifei and Chen, Diansheng and Li, Jiting and Zhang, Xiaochuan},
  doi          = {10.3233/IDA-230278},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {961-972},
  shortjournal = {Intell. Data Anal.},
  title        = {A lightweight method of pose estimation for indoor object},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised multi-source domain adaptation for person
re-identification via sample weighting. <em>IDA</em>, <em>28</em>(4),
943–960. (<a href="https://doi.org/10.3233/IDA-230178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of unsupervised domain adaptation (UDA) in person re-identification (re-ID) is to develop a model that can identify the same individual across different cameras in the target domain, using labeled data from the source domain and unlabeled data from the target domain. However, existing UDA p erson re-ID methods typically assume a single source domain and a single target domain, and seldom consider the scenario of multiple source domains and a single target domain. In the latter scenario, differences in sample size between domains can lead to biased training of the model. To address this, we propose an unsupervised multi-source domain adaptation person re-ID method via sample weighting. Our approach utilizes multiple source domains to leverage valuable label information and balances the inter-domain sample imbalance through sample weighting. We also employ an adversarial learning method to align the domains. The experimental results, conducted on four datasets, demonstrate the effectiveness of our proposed method.},
  archive      = {J_IDA},
  author       = {Tian, Qing and Cheng, Yao},
  doi          = {10.3233/IDA-230178},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {943-960},
  shortjournal = {Intell. Data Anal.},
  title        = {Unsupervised multi-source domain adaptation for person re-identification via sample weighting},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple distilling-based spatial-temporal attention
networks for unsupervised human action recognition. <em>IDA</em>,
<em>28</em>(4), 921–941. (<a
href="https://doi.org/10.3233/IDA-230399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised action recognition based on spatiotemporal fusion feature extraction has attracted much attention in recent years. However, existing methods still have several limitations: (1) The long-term dependence relationship is not effectively extracted at the time level. (2) The high-order moti on relationship between non-adjacent nodes is not effectively captured at the spatial level. (3) The model complexity is too high when the cascade layer input sequence is long, or there are many key points. To solve these problems, a Multiple Distilling-based spatial-temporal attention (MD-STA) networks is proposed in this paper. This model can extract temporal and spatial features respectively and fuse them. Specifically, we first propose a Screening Self-attention (SSA) module; this module can find long-term dependencies in distant frames and high-order motion patterns between non-adjacent nodes in a single frame through a sparse metric on dot product pairs. Then, we propose the Frames and Keypoint-Distilling (FKD) module, which uses extraction operations to halve the input of the cascade layer to eliminate invalid key points and time frame features, thus reducing time and memory complexity. Finally, the Dim-reduction Fusion (DRF) module is proposed to reduce the dimension of existing features to further eliminate redundancy. Numerous experiments were conducted on three distinct datasets: NTU-60, NTU-120, and UWA3D, showing that MD-STA achieves state-of-the-art standards in skeleton-based unsupervised action recognition.},
  archive      = {J_IDA},
  author       = {Zhang, Cheng and Zhong, Jianqi and Cao, Wenming and Ji, Jianhua},
  doi          = {10.3233/IDA-230399},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {921-941},
  shortjournal = {Intell. Data Anal.},
  title        = {Multiple distilling-based spatial-temporal attention networks for unsupervised human action recognition},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved learning in human evolutionary systems with dynamic
contrastive learning. <em>IDA</em>, <em>28</em>(4), 909–919. (<a
href="https://doi.org/10.3233/IDA-230555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new inductive bias for learning in dynamic event-based human systems. This is intended to partially address the issue of deep learning in chaotic systems. Instead of fitting the data to polynomial expansions that are expressive enough to approximate the generative functions or of ind ucing a universal approximator to learn the patterns and inductive bias, we only assume that the relationship between the input features and output classes changes over time, and embed this assumption through a form of dynamic contrastive learning in pre-training, where pre-training labels contain information about the class labels and time periods. We do this by extending and integrating two separate forms of contrastive learning. We note that this approach is not equivalent to inserting an extra feature into the input data that contains time period, because the input data cannot contain the label. We illustrate the approach on a recently designed learning algorithm for event-based graph time-series classification, and demonstrate its value on real-world data.},
  archive      = {J_IDA},
  author       = {Johnson, Joseph and Giraud-Carrier, Christophe and Hatch, Bradley},
  doi          = {10.3233/IDA-230555},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {909-919},
  shortjournal = {Intell. Data Anal.},
  title        = {Improved learning in human evolutionary systems with dynamic contrastive learning},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ordination-based verification of feature selection in
pattern evolution research. <em>IDA</em>, <em>28</em>(4), 891–907. (<a
href="https://doi.org/10.3233/IDA-230326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explains the idea of pattern systems that develop gradually. These systems involve symbolic communication that includes symbols, syntax, and layout rules. Some pattern systems change over time, like historical scripts. The scientific study of pattern systems is called pattern evolution research, and scriptinformatics is concerned with the modelling of the evolution of scripts. The symbol series consists of symbols from a pattern system, while the graph sequence is a symbol sequence applied with a specific technology. This article describes a method for examining tested pattern systems to confirm their classification, which focuses on more ancient features. The method’s effectiveness was tested on Rovash scripts and graph sequences. Multivariate analysis was carried out by using PAST4 software, employing principal coordinates analysis ordination and k-means clustering algorithms.},
  archive      = {J_IDA},
  author       = {Hosszú, Gábor},
  doi          = {10.3233/IDA-230326},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {891-907},
  shortjournal = {Intell. Data Anal.},
  title        = {Ordination-based verification of feature selection in pattern evolution research},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using eigenvalues of distance matrices for outlier
detection. <em>IDA</em>, <em>28</em>(4), 871–889. (<a
href="https://doi.org/10.3233/IDA-230048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance or dissimilarity matrices are widely used in applications. We study the relationships between the eigenvalues of the distance matrices and outliers and show that outliers affect the pairwise distances and inflate the eigenvalues. We obtain the eigenvalues of a distance matrix that is affec ted by k outliers and compare them to the eigenvalues of a distance matrix with a constant structure. We show a discrepancy in the sizes of the eigenvalues of a distance matrix that is contaminated with outliers, present an algorithm and offer a new outlier detection method based on the eigenvalues of the distance matrix. We compare the new distance-based outlier technique with several existing methods under five distributions. The methods are applied to a study of public utility companies and gene expression data.},
  archive      = {J_IDA},
  author       = {Modarres, Reza},
  doi          = {10.3233/IDA-230048},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {871-889},
  shortjournal = {Intell. Data Anal.},
  title        = {Using eigenvalues of distance matrices for outlier detection},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Editorial. <em>IDA</em>, <em>28</em>(4), 867–869. (<a
href="https://doi.org/10.3233/IDA-249004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-249004},
  journal      = {Intelligent Data Analysis},
  month        = {7},
  number       = {4},
  pages        = {867-869},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SocialNER2.0: A comprehensive dataset for enhancing named
entity recognition in short human-produced text. <em>IDA</em>,
<em>28</em>(3), 841–865. (<a
href="https://doi.org/10.3233/IDA-230588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Entity Recognition (NER) is an essential task in Natural Language Processing (NLP), and deep learning-based models have shown outstanding performance. However, the effectiveness of deep learning models in NER relies heavily on the quality and quantity of labeled training datasets available. A novel and comprehensive training dataset called SocialNER2.0 is proposed to address this challenge. Based on selected datasets dedicated to different tasks related to NER, the SocialNER2.0 construction process involves data selection, extraction, enrichment, conversion, and balancing steps. The pre-trained BERT (Bidirectional Encoder Representations from Transformers) model is fine-tuned using the proposed dataset. Experimental results highlight the superior performance of the fine-tuned BERT in accurately identifying named entities, demonstrating the SocialNER2.0 dataset’s capacity to provide valuable training data for performing NER in human-produced texts.},
  archive      = {J_IDA},
  author       = {Belbekri, Adel and Benchikha, Fouzia and Slimani, Yahya and Marir, Naila},
  doi          = {10.3233/IDA-230588},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {841-865},
  shortjournal = {Intell. Data Anal.},
  title        = {SocialNER2.0: A comprehensive dataset for enhancing named entity recognition in short human-produced text},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generate custom travel magazine layouts. <em>IDA</em>,
<em>28</em>(3), 825–840. (<a
href="https://doi.org/10.3233/IDA-230063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the problems of specifying the style and number of elements of a travel magazine, the problem of generating magazine layout by constraining text, and constraining graph layout remains a complex and unsolved problem. In this paper, we generate layouts of text satisfying constraints via GAN. Du e to the complexity and variety of graph designs, we enhance the performance of the discriminator and the generator so that the layouts generated by the generator are more constrained. Add non-corresponding constraint text and real layout pairs to the discriminator to enhance the performance of the discriminator; then add a spatial attention mechanism to the layout encoder to extract the features of the layout and generate high-quality layouts. We demonstrate that the proposed method can generate high-quality layouts of text satisfying the constraints, and we validate the effectiveness of this method through user ratings.},
  archive      = {J_IDA},
  author       = {Wu, Xiangping and Yao, Shuaiwei and Zhang, Zheng and Hu, Jun},
  doi          = {10.3233/IDA-230063},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {825-840},
  shortjournal = {Intell. Data Anal.},
  title        = {Generate custom travel magazine layouts},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I 2R: Intra and inter-modal representation learning for code
search. <em>IDA</em>, <em>28</em>(3), 807–823. (<a
href="https://doi.org/10.3233/IDA-230082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code search, which locates code snippets in large code repositories based on natural language queries entered by developers, has become increasingly popular in the software development process. It has the potential to improve the efficiency of software developers. Recent studies have demonstrated t he effectiveness of using deep learning techniques to represent queries and codes accurately for code search. In specific, pre-trained models of programming languages have recently achieved significant progress in code searching. However, we argue that aligning programming and natural languages are crucial as there are two different modalities. Existing pre-train models based approaches for code search do not effectively consider implicit alignments of representations across modalities (inter-modal representation). Moreover, the existing methods do not take into account the consistency constraint of intra-modal representations, making the model ineffective. As a result, we propose a novel code search method that optimizes both intra-modal and inter-modal representation learning. The alignment of the representation between the two modalities is achieved by introducing contrastive learning. Furthermore, the consistency of intra-modal feature representation is constrained by KL-divergence. Our experimental results confirm the model’s effectiveness on seven different test datasets. This paper proposes a code search method that significantly improves existing methods. Our source code is publicly available on GitHub.1},
  archive      = {J_IDA},
  author       = {Zhang, Xu and Xiang, Yanzheng and Liu, Zejie and Hu, Xiaoyu and Zhou, Deyu},
  doi          = {10.3233/IDA-230082},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {807-823},
  shortjournal = {Intell. Data Anal.},
  title        = {I 2R: Intra and inter-modal representation learning for code search},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ACEANet: Ambiguous context enhanced attention network for
skin lesion segmentation. <em>IDA</em>, <em>28</em>(3), 791–805. (<a
href="https://doi.org/10.3233/IDA-230298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin lesion segmentation from dermatoscopic images is essential for the diagnosis of skin cancer. However, it is still a challenging task due to the ambiguity of the skin lesions, the irregular shape of the lesions and the presence of various interfering factors. In this paper, we propose a novel A mbiguous Context Enhanced Attention Network (ACEANet) based on the classical encoder-decoder architecture, which is able to accurately and reliably segment a variety of lesions with efficiency. Specifically, a novel Ambiguous Context Enhanced Attention module is embedded in the skip connection to augment the ambiguous boundary information. A Dilated Gated Fusion block is employed in the end of the encoding phase, which effectively reduces the loss of spatial location information due to continuous downsampling. In addition, we propose a novel Cascading Global Context Attention to fuse feature information generated by the encoder with features generated by the decoder of the corresponding layer. In order to verify the effectiveness and advantages of the proposed network, we have performed comparative experiments on ISIC2018 dataset and PH2 dataset. Experiments results demonstrate that the proposed model has superior segmentation performance for skin lesions.},
  archive      = {J_IDA},
  author       = {Jiang, Yun and Qiao, Hao},
  doi          = {10.3233/IDA-230298},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {791-805},
  shortjournal = {Intell. Data Anal.},
  title        = {ACEANet: Ambiguous context enhanced attention network for skin lesion segmentation},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pure large kernel convolutional neural network transformer
for medical image registration. <em>IDA</em>, <em>28</em>(3), 769–790.
(<a href="https://doi.org/10.3233/IDA-230197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deformable medical image registration is a fundamental and critical task in medical image analysis. Recently, deep learning-based methods have rapidly developed and have shown impressive results in deformable image registration. However, existing approaches still suffer from limitations in registra tion accuracy or generalization performance. To address these challenges, in this paper, we propose a pure convolutional neural network module (CVTF) to implement hierarchical transformers and enhance the registration performance of medical images. CVTF has a larger convolutional kernel, providing a larger global effective receptive field, which can improve the network’s ability to capture long-range dependencies. In addition, we introduce the spatial interaction attention (SIA) module to compute the interrelationship between the target feature pixel points and all other points in the feature map. This helps to improve the semantic understanding of the model by emphasizing important features and suppressing irrelevant ones. Based on the proposed CVTF and SIA, we construct a novel registration framework named PCTNet. We applied PCTNet to generate displacement fields and register medical images, and we conducted extensive experiments and validation on two public datasets, OASIS and LPBA40. The experimental results demonstrate the effectiveness and generality of our method, showing significant improvements in registration accuracy and generalization performance compared to existing methods. Our code has been available at https://github.com/fz852/PCTNet.},
  archive      = {J_IDA},
  author       = {Fang, Zhao and Cao, Wenming},
  doi          = {10.3233/IDA-230197},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {769-790},
  shortjournal = {Intell. Data Anal.},
  title        = {Pure large kernel convolutional neural network transformer for medical image registration},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal attention-aware evidential recurrent network for
trustworthy prediction of alzheimer’s disease progression. <em>IDA</em>,
<em>28</em>(3), 751–768. (<a
href="https://doi.org/10.3233/IDA-230220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and reliable prediction of Alzheimer’s disease (AD) progression is crucial for effective interventions and treatment to delay its onset. Recently, deep learning models for AD progression achieve excellent predictive accuracy. However, their predictions lack reliability due to the non-calib ration defects, that affects their recognition and acceptance. To address this issue, this paper proposes a temporal attention-aware evidential recurrent network for trustworthy prediction of AD progression. Specifically, evidential recurrent network explicitly models uncertainty of the output and converts it into a reliability measure for trustworthy AD progression prediction. Furthermore, considering that the actual scenario of AD progression prediction frequently relies on historical longitudinal data, we introduce temporal attention into evidential recurrent network, which improves predictive performance. We demonstrate the proposed model on the TADPOLE dataset. For predictive performance, the proposed model achieves mAUC of 0.943 and BCA of 0.881, which is comparable to the SOTA model MinimalRNN. More importantly, the proposed model provides reliability measures of the predicted results through uncertainty estimation and the ECE of the method on the TADPOLE dataset is 0.101, which is much lower than the SOTA model at 0.147, indicating that the proposed model can provide important decision-making support for risk-sensitive prediction of AD progression.},
  archive      = {J_IDA},
  author       = {Zhang, Chenran and Bao, Qingsen and Zhang, Feng and Li, Ping and Chen, Lei},
  doi          = {10.3233/IDA-230220},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {751-768},
  shortjournal = {Intell. Data Anal.},
  title        = {Temporal attention-aware evidential recurrent network for trustworthy prediction of alzheimer’s disease progression},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feature-aware long-short interest evolution network for
sequential recommendation. <em>IDA</em>, <em>28</em>(3), 733–750. (<a
href="https://doi.org/10.3233/IDA-230288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems are an effective solution to deal with information overload, particularly in the e-commerce sector, in which sequential recommendation is extensively utilized. Sequential recommendations aim to acquire users’ interests and provide accurate recommendations by analyzing users’ historical interaction sequences. To improve recommendation performance, it is vital to take into account the long- and short-term interests of users. Despite significant advancements in this domain, some issues need to be addressed. Conventional sequential recommendation models typically express each item with a uniform embedding, ignoring evolutionary patterns among item attributes, such as category, brand, and price. Moreover, these models often model users’ long- and short-term interests independently, failing to adequately address the issues of interest drift and short-term interest evolution. This study proposes a new model, the Feature-aware Long-Short Interest Evolution Network (FLSIE), to address the above-mentioned issues. Specifically, the model uses explicit feature embedding to represent item attribute information and employs a two-dimensional (2D) attention mechanism to distinguish the significance of individual features in a specific item and the relevance of each item in the interaction sequence. Furthermore, to avoid the issue of interest drift, the model employs a long-term interest guidance mechanism to enhance the representation of short-term interest and adopts a gated recurrent unit with attentional update gate to model the dynamic evolution of users’ short-term interest. Experimental results indicate that our presented model outperforms existing methods on three real-world datasets.},
  archive      = {J_IDA},
  author       = {Tang, Jing and Fan, Yongquan and Du, Yajun and Li, Xianyong and Chen, Xiaoliang},
  doi          = {10.3233/IDA-230288},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {733-750},
  shortjournal = {Intell. Data Anal.},
  title        = {A feature-aware long-short interest evolution network for sequential recommendation},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CMCEE: A joint learning framework for cascade decoding with
multi-feature fusion and conditional enhancement for overlapping event
extraction. <em>IDA</em>, <em>28</em>(3), 717–732. (<a
href="https://doi.org/10.3233/IDA-230284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event extraction (EE) is an important natural language processing task. With the passage of time, many powerful and effective models for event extraction tasks have been developed. However, there has been limited research on complex overlapping event extraction. Therefore, we propose a new cascade decoding model: A Joint Learning Framework for Cascade Decoding with Multi-Feature Fusion and Conditional Enhancement for Overlapping Event Extraction. 1) In this model, we introduce a cascade decoding mechanism with multi-feature fusion to better capture the interaction between decoding layers. 2) Additionally, we introduce an enhanced conditional layer normalization (ECLN) mechanism to enhance the interaction between subtasks. Simultaneously, the use of a cascade decoding model effectively addresses the problem of overlapping events. The model successively performs three subtasks, type detection, trigger word extraction and argument extraction. All three subtasks learned together in a framework, and a new conditional normalization mechanism is used to capture dependencies among these subtasks. The experiments are conducted using the overlapping event benchmark, FewFC dataset. The experimental evaluation demonstrates that our model achieves a higher F1 score on the overlapping event extraction task compared to the original overlapping event extraction model.},
  archive      = {J_IDA},
  author       = {Dai, Zerui and Tian, Shengwei and Yu, Long and Yang, Qimeng},
  doi          = {10.3233/IDA-230284},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {717-732},
  shortjournal = {Intell. Data Anal.},
  title        = {CMCEE: A joint learning framework for cascade decoding with multi-feature fusion and conditional enhancement for overlapping event extraction},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GT-CHES: Graph transformation for classification in human
evolutionary systems. <em>IDA</em>, <em>28</em>(3), 699–715. (<a
href="https://doi.org/10.3233/IDA-230194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While increasingly complex algorithms are being developed for graph classification in highly-structured domains, such as image processing and climate forecasting, they often lead to over-fitting and inefficiency when applied to human interaction networks where the confluence of cooperation, conflic t, and evolutionary pressures produces chaotic environments. We propose a graph transformation approach for efficient classification in chaotic human systems that is based on game theoretic, network theoretic, and chaos theoretic principles. Graph structural properties are compiled into time-series that are then transposed into the frequency domain to offer a dynamic view of the system for classification. We propose a set of benchmark data sets and show through experiments that the approach is efficient and appropriate for many dynamic networks in which agents both compete and cooperate, such as social media networks, stock markets, political campaigns, legislation, and geopolitical events.},
  archive      = {J_IDA},
  author       = {Johnson, J. and Giraud-Carrier, C.},
  doi          = {10.3233/IDA-230194},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {699-715},
  shortjournal = {Intell. Data Anal.},
  title        = {GT-CHES: Graph transformation for classification in human evolutionary systems},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-domain adaptive neural machine translation method
based on domain data balancer. <em>IDA</em>, <em>28</em>(3), 685–698.
(<a href="https://doi.org/10.3233/IDA-230155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most methods for multi-domain adaptive neural machine translation (NMT) currently rely on mixing data from multiple domains in a single model to achieve multi-domain translation. However, this mixing can lead to imbalanced training data, causing the model to focus on training for the large-scale ge neral domain while ignoring the scarce resources of specific domains, resulting in a decrease in translation performance. In this paper, we propose a multi-domain adaptive NMT method based on Domain Data Balancer (DDB) to address the problems of imbalanced data caused by simple fine-tuning. By adding DDB to the Transformer model, we adaptively learn the sampling distribution of each group of training data, replace the maximum likelihood estimation criterion with empirical risk minimization training, and introduce a reward-based iterative update of the bilevel optimizer based on reinforcement learning. Experimental results show that the proposed method improves the baseline model by an average of 1.55 and 0.14 BLEU (Bilingual Evaluation Understudy) scores respectively in English-German and Chinese-English multi-domain NMT.},
  archive      = {J_IDA},
  author       = {Xu, Jinlei and Wen, Yonghua and Huang, Shuanghong and Yu, Zhengtao},
  doi          = {10.3233/IDA-230155},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {685-698},
  shortjournal = {Intell. Data Anal.},
  title        = {A multi-domain adaptive neural machine translation method based on domain data balancer},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting active domain adaptation with exploration of
samples. <em>IDA</em>, <em>28</em>(3), 667–683. (<a
href="https://doi.org/10.3233/IDA-230150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the idea of active learning is gradually adopted to assist domain adaptation. However, due to the existence of domain shift, the traditional active learning methods originating from semi-supervised scenarios can not be directly applied to domain adaptation. To solve the problem, active do main adaptation is proposed as a new domain adaptation paradigm, which aims to improve the performance of the model by annotating a small amount of target domain samples. In this regard, we propose an active domain adaptation method named Boosting Active Domain Adaptation with Exploration of Samples (BADA), dividing Active DA into two related issues: sample selection and sample utilization. We design the instability selection criterion based on predictive consistency and the diversity selection criterion. For the remaining unlabeled samples, we design a self-training framework, which screens out reliable samples and unreliable samples through the sample screening mechanism similar to selection criteria. And we adopt respective loss functions for reliable samples and unreliable samples. Experiments show that BADA remarkably outperforms previous active learning methods and Active DA methods on several domain adaptation datasets.},
  archive      = {J_IDA},
  author       = {Tian, Qing and Zhang, Heng},
  doi          = {10.3233/IDA-230150},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {667-683},
  shortjournal = {Intell. Data Anal.},
  title        = {Boosting active domain adaptation with exploration of samples},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning hierarchical embedding space for image-text
matching. <em>IDA</em>, <em>28</em>(3), 647–665. (<a
href="https://doi.org/10.3233/IDA-230214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two mainstream strategies for image-text matching at present. The one, termed as joint embedding learning, aims to model the semantic information of both image and sentence in a shared feature subspace, which facilitates the measurement of semantic similarity but only focuses on global al ignment relationship. To explore the local semantic relationship more fully, the other one, termed as metric learning, aims to learn a complex similarity function to directly output score of each image-text pair. However, it significantly suffers from more computation burden at retrieval stage. In this paper, we propose a hierarchically joint embedding model to incorporate the local semantic relationship into a joint embedding learning framework. The proposed method learns the shared local and global embedding spaces simultaneously, and models the joint local embedding space with respect to specific local similarity labels which are easy to access from the lexical information of corpus. Unlike the methods based on metric learning, we can prepare the fixed representations of both images and sentences by concatenating the normalized local and global representations, which makes it feasible to perform the efficient retrieval. And experiments show that the proposed model can achieve competitive performance when compared to the existing joint embedding learning models on two publicly available datasets Flickr30k and MS-COCO.},
  archive      = {J_IDA},
  author       = {Sun, Hao and Qin, Xiaolin and Liu, Xiaojing},
  doi          = {10.3233/IDA-230214},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {647-665},
  shortjournal = {Intell. Data Anal.},
  title        = {Learning hierarchical embedding space for image-text matching},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modality semantic guidance for multi-label image
classification. <em>IDA</em>, <em>28</em>(3), 633–646. (<a
href="https://doi.org/10.3233/IDA-230239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image classification aims to predict a set of labels that are present in an image. The key challenge of multi-label image classification lies in two aspects: modeling label correlations and utilizing spatial information. However, the existing approaches mainly calculate the correlation between labels according to co-occurrence among them. While the result is easily affected by the label noise and occasional co-occurrences. In addition, some works try to model the correlation between labels and spatial features, but the correlation among labels is not fully considered to model the spatial relationships among features. To address the above issues, we propose a novel cross-modality semantic guidance-based framework for multi-label image classification, namely CMSG. First, we design a semantic-guided attention (SGA) module, which applies the label correlation matrix to guide the learning of class-specific features, which implicitly models semantic correlations among labels. Second, we design a spatial-aware attention (SAA) module to extract high-level semantic-aware spatial features based on class-specific features obtained from the SGA module. The experiments carried out on three benchmark datasets demonstrate that our proposed method outperforms existing state-of-the-art algorithms on multi-label image classification.},
  archive      = {J_IDA},
  author       = {Huang, Jun and Wang, Dian and Hong, Xudong and Qu, Xiwen and Xue, Wei},
  doi          = {10.3233/IDA-230239},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {633-646},
  shortjournal = {Intell. Data Anal.},
  title        = {Cross-modality semantic guidance for multi-label image classification},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Editorial. <em>IDA</em>, <em>28</em>(3), 629–631. (<a
href="https://doi.org/10.3233/IDA-249003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-249003},
  journal      = {Intelligent Data Analysis},
  month        = {5},
  number       = {3},
  pages        = {629-631},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Management optimization strategy of the closed-loop supply
chain by deep learning technology. <em>IDA</em>, <em>28</em>(2),
613–627. (<a href="https://doi.org/10.3233/IDA-227449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous progress of science and the development of society, there are also problems of resource shortage and environmental pollution. People are eager to change this situation. How to achieve sustainable development has become an important problem to be solved. The development of societ y is inseparable from the supply chain, but the traditional supply chain cannot meet the requirements of sustainable development. The research manifests that the Closed-Loop supply chain (CLSC) can protect the environment while economic development, and achieve harmonious coexistence between man and nature. CLSC is studied initially, and the connotation and characteristics of CLSC are introduced. A management optimization of CLSC based on deep learning technology is proposed. Meanwhile, the genetic simulated annealing algorithm is used for simulation, and the obtained solution is the optimal solution. And this algorithm has good convergence, which provides new ideas for the future development of Chinese enterprises.},
  archive      = {J_IDA},
  author       = {Gao, Chunjuan},
  doi          = {10.3233/IDA-227449},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {613-627},
  shortjournal = {Intell. Data Anal.},
  title        = {Management optimization strategy of the closed-loop supply chain by deep learning technology},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heuristics for the two-dimensional cutting stock problem
with usable leftover. <em>IDA</em>, <em>28</em>(2), 591–611. (<a
href="https://doi.org/10.3233/IDA-227447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilization of residue is a challenge in engineering practice, because unreasonable cutting causes excess materials wasted and increases the production cost. This work considers the residual two-dimensional cutting stock problem with usable leftover in which unused parts of cutting patterns can be used for future orders. We propose an algorithm that combines the iterative sequential value correction heuristic with the beam search heuristic, considering both the accumulation and the reusability of leftovers to reduce the material consumption. Cutting plans are constructed iteratively and the best one are chosen as the solution. Cutting patterns in the cutting plan are generated sequentially by recursive techniques, and potentially usable leftover are accumulated by beam search heuristic. Item values are corrected after each pattern to diversify cutting plans. Three sets of simulations under different number of periods, over medium and large instances from the literature, are used to demonstrate the effectiveness of the heuristics. Computational results show that the algorithm provides better solutions, which can save a considerable amount of plate in a long-term production period. The utilization of wastages can save a considerable amount of stock plate and contract the production cost of enterprises in the long-term production period.},
  archive      = {J_IDA},
  author       = {Chen, Qiulian and Chen, Yan},
  doi          = {10.3233/IDA-227447},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {591-611},
  shortjournal = {Intell. Data Anal.},
  title        = {Heuristics for the two-dimensional cutting stock problem with usable leftover},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ep-satty-MCDM based business decision-making model using
behaviour and review data. <em>IDA</em>, <em>28</em>(2), 573–590. (<a
href="https://doi.org/10.3233/IDA-227446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reaching the maximum number of customers at the right time to increase sales and profitability of the business is the primary goal of Electronic Commerce (E-Commerce). However, owing to the low-influenced product, the profitability of e-commerce has been drastically affected in recent years. Theref ore, this work proposes an Empirical Probability assigned Satty’s method integrated Multicriteria Decision-Making model (EP-Satty-MCDM)-based business decision-making model for improving the sales of low-influenced products by advertising with celebrities. Primarily, the authenticated user securely downloaded the encrypted data using Armstrong number private key generated-Trident Curve-Cryptography (Arm-TCC) in the web application. After that, the data is cleansed and the attributes of review and behavior data are extracted. Then, by utilizing the Interval-valued Atanassov intuitionistic fuzzy-based Mann-Whitney U test (IAF-MWU), the correlation between the review and behavior data is evaluated. The correlated features under each user are mapped under the product, and semantic ontology is constructed, where the data is again mapped with the product’s subsections. Afterward, the domains are extracted. Thereafter, to identify whether the product is high-influenced or low-influenced, the obtained ratings from ontology and extracted domains are inputted into the Boosting Regression Tree-Recurrent Neural Network (BRT-RNN). Then, for the decision-making, the positively forecasted celebrities with their garment and low-influenced products are given as the input to EP-Satty-MCDM. The experimental outcomes exhibited that the proposed technique withstands maximum accuracy when contrasted with the existing methodologies.},
  archive      = {J_IDA},
  author       = {Karthick, M. and Satheesh Kumar, S. and Vivek, D. and Viswanathan, A.},
  doi          = {10.3233/IDA-227446},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {573-590},
  shortjournal = {Intell. Data Anal.},
  title        = {Ep-satty-MCDM based business decision-making model using behaviour and review data},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secured finance handling for supply chain integrated
business intelligence using blockchain application scenarios.
<em>IDA</em>, <em>28</em>(2), 553–571. (<a
href="https://doi.org/10.3233/IDA-227445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business intelligence is becoming more essential for supply chain administrators to make good decisions. The globalization of supply chains makes their management and control more challenging. Blockchain is a distributed digital ledger technology that guarantees traceability, transparency, and secu rity and promises to ease global supply chain management issues. This paper proposes the Blockchain-assisted Secure Data Management Framework (BSDMF) for financial data handling for supply chain integrated business intelligence models. Analyzing, collecting, and demonstrating data could be important to a business, its supply chain performance, and sustainability. The blockchain can interrupt supply chain processes for improved finance handling, distributed management, and process automation. The study’s experimental result will help organizations deploy blockchain applications with intelligent business strategies to support supply chain management effectively. The simulation outcome has been implemented, and the recommended method achieves a computation time of fewer than 2 hours, an efficiency ratio of 97.4%, an error ratio of 94.1%, data authentication of 92.1%, and a data management ratio of 98.7%.},
  archive      = {J_IDA},
  author       = {Abd, Sura Khalil and Ali, Mohammed Hasan and Jaber, Mustafa Musa and Abosinnee, Ali S. and Kareem, Z.H. and Wahab, Amelia Natasya Abdul and Hassan, Rosilah and Jassim, Mustafa Mohammed},
  doi          = {10.3233/IDA-227445},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {553-571},
  shortjournal = {Intell. Data Anal.},
  title        = {Secured finance handling for supply chain integrated business intelligence using blockchain application scenarios},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparative analysis of epidemic public opinion and policies
in two regions of china based on big data. <em>IDA</em>, <em>28</em>(2),
533–552. (<a href="https://doi.org/10.3233/IDA-230025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the outbreak of COVID-19 (Corona Virus Disease 2019), the Chinese government has taken strict measures to prevent and control the epidemic. Although the spread of the virus has been controlled, people’s daily life and work have been affected and restricted to varying degrees. Thus people have different sentiments, these may affect people’s implementation and compliance with the policies, thus affecting the effectiveness of epidemic prevention and control. At present, few pieces of literature have analyzed the relationships between people’s feelings, policies, and epidemic trends. The object of this paper is to analyze the text content on social media, to find out the impact of the epidemic blockade policy on the public mood and the concerns expressed by the public about policies changes, and the interaction between policies and epidemic states at different stages of the epidemic. In this paper, we collected the posts of two cities where the epidemic occurred at the same time for analysis and comparative study. On the one hand, we revealed the changes in public attention and attitudes in the two regions during the epidemic, the other hand, it also reflects the differences in public sentiment between the two regions, as well as the correlation between emotions and policies and epidemic trends when different policies are adopted under different circumstances. The obtained results have a certain guiding significance for public health departments to formulate reasonable epidemic prevention policies.},
  archive      = {J_IDA},
  author       = {Qiu, Dong and Huang, Lin},
  doi          = {10.3233/IDA-230025},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {533-552},
  shortjournal = {Intell. Data Anal.},
  title        = {Comparative analysis of epidemic public opinion and policies in two regions of china based on big data},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GeoNLPlify: A spatial data augmentation enhancing text
classification for crisis monitoring. <em>IDA</em>, <em>28</em>(2),
507–531. (<a href="https://doi.org/10.3233/IDA-230040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crises such as natural disasters and public health emergencies generate vast amounts of text data, making it challenging to classify the information into relevant categories. Acquiring expert-labeled data for such scenarios can be difficult, leading to limited training datasets for text classification by fine-tuning BERT-like models. Unfortunately, traditional data augmentation techniques only slightly improve F1-scores. How can data augmentation be used to obtain better results in this applied domain? In this paper, using neural network explicability methods, we aim to highlight that fine-tuned BERT-like models on crisis corpora give too much importance to spatial information to make their predictions. This overfitting of spatial information limits their ability to generalize especially when the event which occurs in a place has evolved and changed since the training dataset has been built. To reduce this bias, we propose GeoNLPlify, 1 a novel data augmentation technique that leverages spatial information to generate new labeled data for text classification related to crises. Our approach aims to address overfitting without necessitating modifications to the underlying model architecture, distinguishing it from other prevalent methods employed to combat overfitting. Our results show that GeoNLPlify significantly improves F1-scores, demonstrating the potential of the spatial information for data augmentation for crisis-related text classification tasks. In order to evaluate the contribution of our method, GeoNLPlify is applied to three public datasets (PADI-web, CrisisNLP and SST2) and compared with classical natural language processing data augmentations.},
  archive      = {J_IDA},
  author       = {Decoupes, Rémy and Roche, Mathieu and Teisseire, Maguelonne},
  doi          = {10.3233/IDA-230040},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {507-531},
  shortjournal = {Intell. Data Anal.},
  title        = {GeoNLPlify: A spatial data augmentation enhancing text classification for crisis monitoring},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-stage multi-modal multi-objective differential
evolution algorithm for vehicle routing problem with time windows.
<em>IDA</em>, <em>28</em>(2), 485–506. (<a
href="https://doi.org/10.3233/IDA-227410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the mathematical model of Vehicle Routing Problem with Time Windows (VRPTW) is established based on the directed graph, and a 3-stage multi-modal multi-objective differential evolution algorithm (3S-MMDEA) is proposed. In the first stage, in order to expand the range of individuals t o be selected, a generalized opposition-based learning (GOBL) strategy is used to generate a reverse population. In the second stage, a search strategy of reachable distribution area is proposed, which divides the population with the selected individual as the center point to improve the convergence of the solution set. In the third stage, an improved individual variation strategy is proposed to legalize the mutant individuals, so that the individual after variation still falls within the range of the population, further improving the diversity of individuals to ensure the diversity of the solution set. Based on the synergy of the above three stages of strategies, the diversity of individuals is ensured, so as to improve the diversity of solution sets, and multiple equivalent optimal paths are obtained to meet the planning needs of different decision-makers. Finally, the performance of the proposed method is evaluated on the standard benchmark datasets of the problem. The experimental results show that the proposed 3S-MMDEA can improve the efficiency of logistics distribution and obtain multiple equivalent optimal paths. The method achieves good performance, superior to the most advanced VRPTW solution methods, and has great potential in practical projects.},
  archive      = {J_IDA},
  author       = {Zhang, Hai-Fei and Ge, Hong-Wei and Li, Ting and Su, ShuZhi and Tong, YuBing},
  doi          = {10.3233/IDA-227410},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {485-506},
  shortjournal = {Intell. Data Anal.},
  title        = {Three-stage multi-modal multi-objective differential evolution algorithm for vehicle routing problem with time windows},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing link prediction efficiency with shortest path and
structural attributes. <em>IDA</em>, <em>28</em>(2), 467–483. (<a
href="https://doi.org/10.3233/IDA-230030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction is one of the most essential and crucial tasks in complex network research since it seeks to forecast missing links in a network based on current ones. This problem has applications in a variety of scientific disciplines, including social network research, recommendation systems, an d biological networks. In previous work, link prediction has been solved through different methods such as path, social theory, topology, and similarity-based. The main issue is that path-based methods ignore topological features, while structure-based methods also fail to combine the path and structured-based features. As a result, a new technique based on the shortest path and topological features’ has been developed. The method uses both local and global similarity indices to measure the similarity. Extensive experiments on real-world datasets from a variety of domains are utilized to empirically test and compare the proposed framework to many state-of-the-art prediction techniques. Over 100 iterations, the collected data showed that the proposed method improved on the other methods in terms of accuracy. SI and AA, among the existing state-of-the-art algorithms, fared best with an AUC value of 82%, while the proposed method has an AUC value of 84%.},
  archive      = {J_IDA},
  author       = {Wasim, Muhammad and Al-Obeidat, Feras and Amin, Adnan and Gul, Haji and Moreira, Fernando},
  doi          = {10.3233/IDA-230030},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {467-483},
  shortjournal = {Intell. Data Anal.},
  title        = {Enhancing link prediction efficiency with shortest path and structural attributes},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning based software effort estimation using
development-centric features for crowdsourcing platform. <em>IDA</em>,
<em>28</em>(2), 451–465. (<a
href="https://doi.org/10.3233/IDA-227358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification is a method for categorizing textual data based on features extracted from the original textual information. When it comes to modelling text structural properties, Graph Convolutional Network (GCN) has demonstrated outstanding performance. However, most existing grap h-based models do not model the structure of a single text unit and do not consider the sequence information in each document (e.g., word order). To resolve these issues and fully utilize the text’s structural and sequential details, a text classification model called Sequential GCN with Multi-Head Attention (SGCN-MHA) is proposed in this paper. For each text, a separate text graph is constructed in which nodes are the words of the text, and the edges between nodes corresponding to the word relations. Then the GCN is used to extract the structural feature. To enable the word nodes in the document graph to hold contextual information, the BiLSTM is also applied to learn the sequential feature for each graph. Finally, the Multi-Head Attention mechanism is adopted to interact with these two features and then aggregate them to get access to critical information in the text. The efficiency of our approach has been tested on two standard datasets, including comparative and ablation experiments.},
  archive      = {J_IDA},
  author       = {Ying, Zuobin and Ling, Min and Zhang, Yiwen},
  doi          = {10.3233/IDA-227358},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {451-465},
  shortjournal = {Intell. Data Anal.},
  title        = {Machine learning based software effort estimation using development-centric features for crowdsourcing platform},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymmetric multilevel interactive attention network
integrating reviews for item recommendation. <em>IDA</em>,
<em>28</em>(2), 433–450. (<a
href="https://doi.org/10.3233/IDA-230128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, most studies in the field have focused on integrating reviews behind ratings to improve recommendation performance. However, two main problems remain (1) Most works use a unified data form and the same processing method to address the user and the item reviews, regardless of their essenti al differences. (2) Most works only adopt simple concatenation operation when constructing user-item interaction, thus ignoring the multilevel relationship between the user and the item, which may lead to suboptimal recommendation performance. In this paper, we propose a novel Asymmetric Multi-Level Interactive Attention Network (AMLIAN) integrating reviews for item recommendation. AMLIAN can predict precise ratings to help the user make better and faster decisions. Specifically, to address the essential difference between the user and the item reviews, AMLIAN uses the asymmetric network to construct user and item features using different data forms (document-level and review-level). To learn more personalized user-item interaction, the user ID and item ID and some processed features of user reviews and item reviews are respectively used for multilevel relationships. Experiments on five real-world datasets show that AMLIAN significantly outperforms state-of-the-art methods.},
  archive      = {J_IDA},
  author       = {Yang, Peilin and Zheng, Wenguang and Xiao, Yingyuan and Jiao, Xu},
  doi          = {10.3233/IDA-230128},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {433-450},
  shortjournal = {Intell. Data Anal.},
  title        = {Asymmetric multilevel interactive attention network integrating reviews for item recommendation},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory outlier detection method based on group division.
<em>IDA</em>, <em>28</em>(2), 415–432. (<a
href="https://doi.org/10.3233/IDA-237384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory-outlier detection can be used to discover the fraudulent behaviour of taxi drivers during operations. Existing detection methods typically consider each trajectory as a whole, resulting in low accuracy and slow speed. In this study, a trajectory outlier detection method based on group di vision is proposed. First, the urban vector region is divided into a series of grids of fixed size, and the grid density is calculated based on the urban road network. Second, according to the grid density, the grids were divided into high- and low-density grids, and the code sequence for each trajectory was obtained using grid coding and density. Third, the trajectory dataset is divided into several groups based on the number of low-density grids through which each trajectory passes. Finally, based on the high-density grid sequences, a regular subtrajectory dataset was obtained within each trajectory group, which was used to calculate the trajectory deviation to detect outlying trajectories. Based on experimental results using real trajectory datasets, it has been found that the proposed method performs better at detecting abnormal trajectories than other similar methods.},
  archive      = {J_IDA},
  author       = {Chen, Chuanming and Xu, Dongsheng and Jin, Qi and Wang, Wenkai and Sun, Liping and Zheng, Xiaoyao and Yu, Qingying},
  doi          = {10.3233/IDA-237384},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {415-432},
  shortjournal = {Intell. Data Anal.},
  title        = {Trajectory outlier detection method based on group division},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Remote sensing image classification method based on improved
ShuffleNet convolutional neural network. <em>IDA</em>, <em>28</em>(2),
397–414. (<a href="https://doi.org/10.3233/IDA-227217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a list of remotely sensed data sources is available, the effective processing of remote sensing images is of great significance in practical applications in various fields. This paper proposes a new lightweight network to solve the problem of remote sensing image processing by using the method o f deep learning. Specifically, the proposed model employs ShuffleNet V2 as the backbone network, appropriately increases part of the convolution kernels to improve the classification accuracy of the network, and uses the maximum overlapping pooling layer to enhance the detailed features of the input images. Finally, Squeeze and Excitation (SE) blocks are introduced as the attention mechanism to improve the architecture of the network. Experimental results based on several multisource data show that our proposed network model has a good classification effect on the test samples and can achieve more excellent classification performance than some existing methods, with an accuracy of 91%, and can be used for the classification of remote sensing images. Our model not only has high accuracy but also has faster training speed compared with large networks and can greatly reduce computation costs. The demo code of our proposed method will be available at https://github.com/li-zi-qi.},
  archive      = {J_IDA},
  author       = {Li, Ziqi and Su, Yuxuan and Zhang, Yonghong and Yin, Hefeng and Sun, Jun and Wu, Xiaojun},
  doi          = {10.3233/IDA-227217},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {397-414},
  shortjournal = {Intell. Data Anal.},
  title        = {Remote sensing image classification method based on improved ShuffleNet convolutional neural network},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). Editorial. <em>IDA</em>, <em>28</em>(2), 393–395. (<a
href="https://doi.org/10.3233/IDA-249001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-249001},
  journal      = {Intelligent Data Analysis},
  month        = {4},
  number       = {2},
  pages        = {393-395},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust partial face recognition using multi-label
attributes. <em>IDA</em>, <em>28</em>(1), 377–392. (<a
href="https://doi.org/10.3233/IDA-227309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial face recognition (PFR) is challenging as the appearance of the face changes significantly with occlusion. In particular, these occlusions can be due to any item and may appear in any position that seriously hinders the extraction of discriminative features. Existing methods deal with PFR ei ther by training a deep model with existing face databases containing limited occlusion types or by extracting un-occluded features directly from face regions without occlusions. Limited training data (i.e., occlusion type and diversity) can not cover the real-occlusion situations, and thus training-based methods can not learn occlusion robust discriminative features. The performance of occlusion region-based method is bounded by occlusion detection. Different from limited training data and occlusion region-based methods, we propose to use multi-label attributes for Partial Face Recognition (Attr4PFR). A novel data augmentation is proposed to solve limited training data and generate occlusion attributes. Apart from occlusion attributes, we also include soft biometric attributes and semantic attributes to explore more rich attributes to combat the loss caused by occlusions. To train our Attr4PFR, we propose an implicit attributes loss combined with a softmax loss to enforce Attr4PFR to learn discriminative features. As multi-label attributes are our auxiliary signal in the training phase, we do not need them in the inference. Extensive experiments on public benchmark AR and IJB-C databases show our method is 3% and 2.3% improvement compared to the state-of-the-art.},
  archive      = {J_IDA},
  author       = {Sang, Gaoli and Zeng, Dan and Yan, Chao and Veldhuis, Raymond and Spreeuwers, Luuk},
  doi          = {10.3233/IDA-227309},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {377-392},
  shortjournal = {Intell. Data Anal.},
  title        = {Robust partial face recognition using multi-label attributes},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing adaboost performance in the presence of
class-label noise: A comparative study on EEG-based classification of
schizophrenic patients and benchmark datasets. <em>IDA</em>,
<em>28</em>(1), 357–376. (<a
href="https://doi.org/10.3233/IDA-227125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of Adaboost is highly sensitive to noisy and outlier samples. This is therefore the weights of these samples are exponentially increased in successive rounds. In this paper, three novel schemes are proposed to hunt the corrupted samples and eliminate them through the training proces s. The methods are: I) a hybrid method based on K-means clustering and K-nearest neighbor, II) a two-layer Adaboost, and III) soft margin support vector machines. All of these solutions are compared to the standard Adaboost on thirteen Gunnar Raetsch’s datasets under three levels of class-label noise. To test the proposed method on a real application, electroencephalography (EEG) signals of 20 schizophrenic patients and 20 age-matched control subjects, are recorded via 20 channels in the idle state. Several features including autoregressive coefficients, band power and fractal dimension are extracted from EEG signals of all participants. Sequential feature subset selection technique is adopted to select the discriminative EEG features. Experimental results imply that exploiting the proposed hunting techniques enhance the Adaboost performance as well as alleviating its robustness against unconfident and noisy samples over Raetsch benchmark and EEG features of the two groups.},
  archive      = {J_IDA},
  author       = {Pouya, Omid Ranjbar and Boostani, Reza and Sabeti, Malihe},
  doi          = {10.3233/IDA-227125},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {357-376},
  shortjournal = {Intell. Data Anal.},
  title        = {Enhancing adaboost performance in the presence of class-label noise: A comparative study on EEG-based classification of schizophrenic patients and benchmark datasets},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building knowledge graphs from technical documents using
named entity recognition and edge weight updating neural network with
triplet loss for entity normalization. <em>IDA</em>, <em>28</em>(1),
331–355. (<a href="https://doi.org/10.3233/IDA-227129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attempts to express information from various documents in graph form are rapidly increasing. The speed and volume in which these documents are being generated call for an automated process, based on machine learning techniques, for cost-effective and timely analysis. Past studies responded to such needs by building knowledge graphs or technology trees from the bibliographic information of documents, or by relying on text mining techniques in order to extract keywords and/or phrases. While these approaches provide an intuitive glance into the technological hotspots or the key features of the select field, there still is room for improvement, especially in terms of recognizing the same entities appearing in different forms so as to interconnect closely related technological concepts properly. In this paper, we propose to build a patent knowledge network using the United States Patent and Trademark Office (USPTO) patent filings for the semiconductor device sector by fine-tuning Huggingface’s named entity recognition (NER) model with our novel edge weight updating neural network. For the named entity normalization, we employ edge weight updating neural network with positive and negative candidates that are chosen by substring matching techniques. Experiment results show that our proposed approach performs very competitively against the conventional keyword extraction models frequently employed in patent analysis, especially for the named entity normalization (NEN) and document retrieval tasks. By grouping entities with named entity normalization model, the resulting knowledge graph achieves higher scores in retrieval tasks. We also show that our model is robust to the out-of-vocabulary problem by employing the fine-tuned BERT NER model.},
  archive      = {J_IDA},
  author       = {Jeon, Sung Hwan and Lee, Hye Jin and Park, Jihye and Cho, Sungzoon},
  doi          = {10.3233/IDA-227129},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {331-355},
  shortjournal = {Intell. Data Anal.},
  title        = {Building knowledge graphs from technical documents using named entity recognition and edge weight updating neural network with triplet loss for entity normalization},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning based software effort estimation using
development-centric features for crowdsourcing platform. <em>IDA</em>,
<em>28</em>(1), 299–329. (<a
href="https://doi.org/10.3233/IDA-237366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd-Sourced software development (CSSD) is getting a good deal of attention from the software and research community in recent times. One of the key challenges faced by CSSD platforms is the task selection mechanism which in practice, contains no intelligent scheme. Rather, rule-of-thumb or intui tion strategies are employed, leading to biasness and subjectivity. Effort considerations on crowdsourced tasks can offer good foundation for task selection criteria but are not much investigated. Software development effort estimation (SDEE) is quite prevalent domain in software engineering but only investigated for in-house development. For open-sourced or crowdsourced platforms, it is rarely explored. Moreover, Machine learning (ML) techniques are overpowering SDEE with a claim to provide more accurate estimation results. This work aims to conjoin ML-based SDEE to analyze development effort measures on CSSD platform. The purpose is to discover development-oriented features for crowdsourced tasks and analyze performance of ML techniques to find best estimation model on CSSD dataset. TopCoder is selected as target CSSD platform for the study. TopCoder’s development tasks data with development-centric features are extracted, leading to statistical, regression and correlation analysis to justify features’ significance. For effort estimation, 10 ML families with 2 respective techniques are applied to get broader aspect of estimation. Five performance metrices (MSE, RMSE, MMRE, MdMRE, Pred (25) and Welch’s statistical test are incorporated to judge the worth of effort estimation model’s performance. Data analysis results show that selected features of TopCoder pertain reasonable model significance, regression, and correlation measures. Findings of ML effort estimation depicted that best results for TopCoder dataset can be acquired by linear, non-linear regression and SVM family models. To conclude, the study identified the most relevant development features for CSSD platform, confirmed by in-depth data analysis. This reflects careful selection of effort estimation features to offer good basis of accurate ML estimate.},
  archive      = {J_IDA},
  author       = {Yasmin, Anum and Haider, Wasi and Daud, Ali and Banjar, Ameen},
  doi          = {10.3233/IDA-237366},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {299-329},
  shortjournal = {Intell. Data Anal.},
  title        = {Machine learning based software effort estimation using development-centric features for crowdsourcing platform},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aggregating knowledge and collaborative information for
sequential recommendation. <em>IDA</em>, <em>28</em>(1), 279–298. (<a
href="https://doi.org/10.3233/IDA-227198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation aims to predict users’ future activities based on their historical interaction sequences. Various neural network architectures, such as Recurrent Neural Networks (RNN), Graph Neural Networks (GNN), and self-attention mechanisms, have been employed in the tasks, exploring m ultiple aspects of user preferences, including general interests, short-term interests, long-term interests, and item co-occurrence patterns. Despite achieving good performance, there are still limitations in capturing complex user preferences. Specifically, the current structures of RNN, GNN, etc., only capture item-level transition relations while neglecting attribute-level transition relations. Additionally, the explicit item relations are studied using item co-occurrence modules, but they cannot capture the implicit item-item relations. To address these issues, we propose a knowledge-augmented Gated Recurrent Unit (GRU) to improve the short-term user interest module and adopt a collaborative item aggregation method to enhance the item co-occurrence module. Additionally, our long-term interest module utilizes a bitwise gating mechanism to select historical item features significant to users’ current preferences. We extensively evaluate our model on three real-world datasets alongside competitive methods, demonstrating its effectiveness in top K sequential recommendation.},
  archive      = {J_IDA},
  author       = {Zhang, Yunqi and Yuan, Jidong and Wei, Chixuan and Xie, Yifei},
  doi          = {10.3233/IDA-227198},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {279-298},
  shortjournal = {Intell. Data Anal.},
  title        = {Aggregating knowledge and collaborative information for sequential recommendation},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MeFiNet: Modeling multi-semantic convolution-based feature
interactions for CTR prediction. <em>IDA</em>, <em>28</em>(1), 261–278.
(<a href="https://doi.org/10.3233/IDA-227113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting more information from feature interactions is essential to improve click-through rate (CTR) prediction accuracy. Although deep learning technology can help capture high-order feature interactions, the combination of features lacks interpretability. In this paper, we propose a multi-seman tic feature interaction learning network (MeFiNet), which utilizes convolution operations to map feature interactions to multi-semantic spaces to improve their expressive ability and uses an improved Squeeze &amp; Excitation method based on SENet to learn the importance of these interactions in different semantic spaces. The Squeeze operation helps to obtain the global importance distribution of semantic spaces, and the Excitation operation helps to dynamically re-assign the weights of semantic features so that both semantic diversity and feature diversity are considered in the model. The generated multi-semantic feature interactions are concatenated with the original feature embeddings and input into a deep learning network. Experiments on three public datasets demonstrate the effectiveness of the proposed model. Compared with state-of-the-art methods, the model achieves excellent performance (+0.18% in AUC and -0.34% in LogLoss VS DeepFM; +0.19% in AUC and -0.33% in LogLoss VS FiBiNet).},
  archive      = {J_IDA},
  author       = {Yan, Cairong and Li, Xiaoke and Tao, Ran and Zhang, Zhaohui and Wan, Yongquan},
  doi          = {10.3233/IDA-227113},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {261-278},
  shortjournal = {Intell. Data Anal.},
  title        = {MeFiNet: Modeling multi-semantic convolution-based feature interactions for CTR prediction},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-aspect multilingual and cross-lingual parliamentary
speech analysis. <em>IDA</em>, <em>28</em>(1), 239–260. (<a
href="https://doi.org/10.3233/IDA-227347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parliamentary and legislative debate transcripts provide an informative insight into elected politicians’ opinions, positions, and policy preferences. They are interesting for political and social sciences as well as linguistics and natural language processing (NLP) research. While exiting research studied individual parliaments, we apply advanced NLP methods to a joint and comparative analysis of six national parliaments (Bulgarian, Czech, French, Slovene, Spanish, and United Kingdom) between 2017 and 2020. We analyze emotions and sentiment in the transcripts from the ParlaMint dataset collection, and assess if the age, gender, and political orientation of speakers can be detected from their speeches. The results show some commonalities and many surprising differences among the analyzed countries.},
  archive      = {J_IDA},
  author       = {Miok, Kristian and Hidalgo Tenorio, Encarnación and Osenova, Petya and Benítez-Castro, Miguel-Ángel and Robnik-Šikonja, Marko},
  doi          = {10.3233/IDA-227347},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {239-260},
  shortjournal = {Intell. Data Anal.},
  title        = {Multi-aspect multilingual and cross-lingual parliamentary speech analysis},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feature-level mask self-supervised assisted learning
approach based on transformer for remaining useful life prediction.
<em>IDA</em>, <em>28</em>(1), 217–237. (<a
href="https://doi.org/10.3233/IDA-227099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the massive industrial data has effectively improved the performance of the data-driven deep learning Remaining Useful Life (RUL) prediction method. However, there are still problems of assigning fixed weights to features and only coarse-grained consideration at the sequence level. This p aper proposes a Transformer-based end-to-end feature-level mask self-supervised learning method for RUL prediction. First, by proposing a fine-grained feature-level mask self-supervised learning method, the data at different time points under all features in a time window is sent to two parallel learning streams with and without random masks. The model can learn more fine-grained degradation information by comparing the information extracted by the two parallel streams. Instead of assigning fixed weights to different features, the abstract information extracted through the above process is invariable correlations between features, which has a good generalization to various situations under different working conditions. Then, the extracted information is encoded and decoded again using an asymmetric structure, and a fully connected network is used to build a mapping between the extracted information and the RUL. We conduct experiments on the public C-MAPSS datasets and show that the proposed method outperforms the other methods, and its advantages are more obvious in complex multi-working conditions.},
  archive      = {J_IDA},
  author       = {Xue, Bing and Gao, Xin and Zhang, Shuwei and Wang, Ning and Fu, Shiyuan and Yu, Jiahao and Zhang, Guangyao and Huang, Zijian},
  doi          = {10.3233/IDA-227099},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {217-237},
  shortjournal = {Intell. Data Anal.},
  title        = {A feature-level mask self-supervised assisted learning approach based on transformer for remaining useful life prediction},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The role of consultative leadership on administrative
development. <em>IDA</em>, <em>28</em>(1), 203–216. (<a
href="https://doi.org/10.3233/IDA-237448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consultative leadership is a democratic style that deliberately incorporates employees into organizational management and decision-making to increase employees’ feelings of ownership and align their objectives with company objectives. As a result, during their everyday tasks, leaders constantly uti lize “consultation management” for their staff. As examples, consider how to coordinate reports, communicate key ideas, and use a variety of flexible promotion strategies. This study investigates the role of consultive leadership on administrative development in developing countries. For this reason, this study has applied a questionnaire to take the respondents’ opinions in the Iraqi ministry of interior affairs. Using the Likert scale has provided quantitative value for the qualitative study. For this reason, questionnaires were provided, and this study’s results showed a positive correlation between consultative leadership and administrative development. As a result, the organization’s leader has more chances to administer the organization successfully than a manager or an unofficial leader who lacks status power.},
  archive      = {J_IDA},
  author       = {Omar, Ayas Mohammed Rasheed and Auso, Khairi Ali},
  doi          = {10.3233/IDA-237448},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {203-216},
  shortjournal = {Intell. Data Anal.},
  title        = {The role of consultative leadership on administrative development},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Economic and financial news hybrid- classification based on
category-associated feature set. <em>IDA</em>, <em>28</em>(1), 185–201.
(<a href="https://doi.org/10.3233/IDA-237373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large amount of economic and financial news is now accessible through various news websites and social media platforms. Categorizing them into appropriate categories can be advantageous for various tasks, such as sentiment analysis and news-based market prediction. Unfortunately, news headlines c ategories may contain ambiguities due to the subjective nature of label assignment by authors or publishers. Consequently, achieving precise classification of news can be time-consuming and still reliant on human expertise. To tackle this challenging task, we proposed a hybrid approach to enhance the performance of economic and financial news classification. This approach combines baseline classifiers with a novel method called the Category Associated Feature Set (CAFS) classifier. CAFS transforms text input from the lexicon-space into the entity-space and discovers associations between entities and classes, akin to association rule learning. Experimental results on three datasets demonstrated that the proposed method is comparable to existing approaches and exhibits a significant improvement in the classification results for out-of-domain datasets. Additionally, employing CAFS in tandem with the existing text classification baselines can provide a general categorizer for distinguishing news categories across various sources without the need for extensive fine-tuning of the parameters associated with those classification baselines. This confirms that utilizing CAFS in a hybrid approach is appropriate and suitable for economic and financial news classification.},
  archive      = {J_IDA},
  author       = {Yathongkhum, Wilawan and Laosiritaworn, Yongyut and Bootkrajang, Jakramate and Treeratpituk, Pucktada and Chaijaruwanich, Jeerayut},
  doi          = {10.3233/IDA-237373},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {185-201},
  shortjournal = {Intell. Data Anal.},
  title        = {Economic and financial news hybrid- classification based on category-associated feature set},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised probabilistic latent semantic analysis with
applications to controversy analysis of legislative bills. <em>IDA</em>,
<em>28</em>(1), 161–183. (<a
href="https://doi.org/10.3233/IDA-227202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic Latent Semantic Analysis (PLSA) is a fundamental text analysis technique that models each word in a document as a sample from a mixture of topics. PLSA is the precursor of probabilistic topic models including Latent Dirichlet Allocation (LDA). PLSA, LDA and their numerous extensions have been successfully applied to many text mining and retrieval tasks. One important extension of LDA is supervised LDA (sLDA), which distinguishes itself from most topic models in that it is supervised. However, to the best of our knowledge, no prior work extends PLSA in a similar manner sLDA extends LDA by jointly modeling the contents and the responses of documents. In this paper, we propose supervised PLSA (sPLSA) which can efficiently infer latent topics and their factorized response values from the contents and the responses of documents. The major challenge lies in estimating a document’s topic distribution which is a constrained probability that is dictated by both the content and the response of the document. To tackle this challenge, we introduce an auxiliary variable to transform the constrained optimization problem to an unconstrained optimization problem. This allows us to derive an efficient Expectation and Maximization (EM) algorithm for parameter estimation. Compared to sLDA, sPLSA converges much faster and requires less hyperparameter tuning, while performing similarly on topic modeling and better in response factorization. This makes sPLSA an appealing choice for latent response analysis such as ranking latent topics by their factorized response values. We apply the proposed sPLSA model to analyze the controversy of bills from the United States Congress. We demonstrate the effectiveness of our model by identifying contentious legislative issues.},
  archive      = {J_IDA},
  author       = {Alemayehu, Eyor and Fang, Yi},
  doi          = {10.3233/IDA-227202},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {161-183},
  shortjournal = {Intell. Data Anal.},
  title        = {Supervised probabilistic latent semantic analysis with applications to controversy analysis of legislative bills},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary feature selection based on hybrid bald eagle
search and particle swarm optimization. <em>IDA</em>, <em>28</em>(1),
121–159. (<a href="https://doi.org/10.3233/IDA-227222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a complicated multi-objective optimization problem with aims at reaching to the best subset of features while remaining a high accuracy in the field of machine learning, which is considered to be a difficult task. In this paper, we design a fitness function to jointly optimize the classification accuracy and the selected features in the linear weighting manner. Then, we propose two hybrid meta-heuristic methods which are the hybrid basic bald eagle search-particle swarm optimization (HBBP) and hybrid chaos-based bald eagle search-particle swarm optimization (HCBP) that alleviate the drawbacks of bald eagle search (BES) by utilizing the advantages of particle swarm optimization (PSO) to efficiently optimize the designed fitness function. Specifically, HBBP is proposed to overcome the disadvantages of the originals (i.e., BES and PSO) and HCBP is proposed to further improve the performance of HBBP. Moreover, a binary optimization is utilized to effectively transfer the solution space from continuous to binary. To evaluate the effectiveness, 17 well-known data sets from the UCI repository are employed as well as a set of well-established algorithms from the literature are adopted to jointly confirm the effectiveness of the proposed methods in terms of fitness value, classification accuracy, computational time and selected features. The results support the superiority of the proposed hybrid methods against the basic optimizers and the comparative algorithms on the most tested data sets.},
  archive      = {J_IDA},
  author       = {Liu, Zhao and Wang, Aimin and Sun, Geng and Li, Jiahui and Bao, Haiming and Liu, Yanheng},
  doi          = {10.3233/IDA-227222},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {121-159},
  shortjournal = {Intell. Data Anal.},
  title        = {Evolutionary feature selection based on hybrid bald eagle search and particle swarm optimization},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conversational recommender based on graph sparsification and
multi-hop attention. <em>IDA</em>, <em>28</em>(1), 99–119. (<a
href="https://doi.org/10.3233/IDA-230148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommender systems provide users with item recommendations via interactive dialogues. Existing methods using graph neural networks have been proven to be an adequate representation of the learning framework for knowledge graphs. However, the knowledge graph involved in the dialogue context is vast and noisy, especially the noise graph nodes, which restrict the primary node’s aggregation to neighbor nodes. In addition, although the recurrent neural network can encode the local structure of word sequences in a dialogue context, it may still be challenging to remember long-term dependencies. To tackle these problems, we propose a sparse multi-hop conversational recommender model named SMCR, which accurately identifies important edges through matching items, thus reducing the computational complexity of sparse graphs. Specifically, we design a multi-hop attention network to encode dialogue context, which can quickly encode the long dialogue sequences to capture the long-term dependencies. Furthermore, we utilize a variational auto-encoder to learn topic information for capturing syntactic dependencies. Extensive experiments on the travel dialogue dataset show significant improvements in our proposed model over the state-of-the-art methods in evaluating recommendation and dialogue generation.},
  archive      = {J_IDA},
  author       = {Zhang, Yihao and Wang, Yuhao and Zhou, Wei and Lan, Pengxiang and Xiang, Haoran and Zhu, Junlin and Yuan, Meng},
  doi          = {10.3233/IDA-230148},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {99-119},
  shortjournal = {Intell. Data Anal.},
  title        = {Conversational recommender based on graph sparsification and multi-hop attention},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSAGNN: Temporal link predict method based on two stream
adaptive graph neural network. <em>IDA</em>, <em>28</em>(1), 77–97. (<a
href="https://doi.org/10.3233/IDA-237367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal link prediction based on graph neural networks has become a hot spot in the field of complex networks. To solve the problems of the existing temporal link prediction methods based on graph neural networks do not consider the future time-domain features and spatial-domain features are limit ed used, this paper proposes a novel temporal link prediction method based on two streams adaptive graph neural networks. Firstly, the network topology features are extracted from the micro, meso, and middle perspectives. Combined with the adaptive mechanism of convolution and self-attention, the preprocessing of the feature extraction is more effective; Secondly, an extended bi-directional long short-term memory network is proposed, which uses graph convolution to process topological features, and recursively learns the state vectors of the target snapshot by using the future time-domain information and the past historical information; Thirdly, the location coding is replaced by the time-coding for the transformer mechanism, so that past information and future information can be learned from each other, and the time-domain information of the network can be further mined; Finally, a novel two-stream network framework is proposed, which combines the processing results of point features and edge features. The experimental results on 9 data sets show that the proposed method has a better prediction effect and better robustness than the classical graph neural network methods.},
  archive      = {J_IDA},
  author       = {Zhu, Yuhang and Guo, Jing and Li, Haitao and Liu, Shuxin and Li, Yingle},
  doi          = {10.3233/IDA-237367},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {77-97},
  shortjournal = {Intell. Data Anal.},
  title        = {TSAGNN: Temporal link predict method based on two stream adaptive graph neural network},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How graph features from message passing affect graph
classification and regression? <em>IDA</em>, <em>28</em>(1), 57–75. (<a
href="https://doi.org/10.3233/IDA-227190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been applied to various graph domains. However, GNNs based on the message passing scheme, which iteratively aggregates information from neighboring nodes, have difficulty learning to represent larger subgraph structures because of the nature of the scheme. We investigate the prediction performance of GNNs when the number of message passing iteration increases to capture larger subgraph structures on classification and regression tasks using various real-world graph datasets. Our empirical results show that the averaged features over nodes obtained by the message passing scheme in GNNs are likely to converge to a certain value, which significantly deteriorates the resulting prediction performance. This is in contrast to the state-of-the-art Weisfeiler–Lehman graph kernel, which has been used actively in machine learning for graphs, as it can comparably learn the large subgraph structures and its performance does not usually drop significantly drop from the first couple of rounds of iterations. Moreover, we report that when we apply node features obtained via GNNs to SVMs, the performance of the Weisfeiler-Lehman kernel can be superior to that of the graph convolutional model, which is a typically employed approach in GNNs.},
  archive      = {J_IDA},
  author       = {Yamada, Masatsugu and Sugiyama, Mahito},
  doi          = {10.3233/IDA-227190},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {57-75},
  shortjournal = {Intell. Data Anal.},
  title        = {How graph features from message passing affect graph classification and regression?},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graph embedding in a uniform space. <em>IDA</em>,
<em>28</em>(1), 33–55. (<a
href="https://doi.org/10.3233/IDA-227123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding (KGE) is typically used for link prediction to automatically predict missing links in knowledge graphs. Current KGE models are mainly based on complicated mathematical associations, which are highly expressive but ignore the uniformity behind the classical bilinear transla tional model TransE, a model that embeds all entities of knowledge graphs in a uniform space, enabling accurate embeddings. This study analyses the uniformity of TransE and proposes a novel KGE model called ConvUs that follows uniformity with expressiveness. Based on the convolution neural network (CNN), ConvUs proposes constraints on convolution filter values and employs a multi-layer, multi-scale CNN architecture with a non-parametric L2 norm-based scoring function for the calculation of triple scores. This addresses potential uniformity-related issues in existing CNN-based KGE models, allowing ConvUs to maintain a uniform embedding space while benefiting from the powerful expressiveness of CNNs. Furthermore, circular convolution is applied to alleviate the potential orderliness contradictions, making ConvUs more suitable for conducting uniform space KGE. Our model outperformed the base model ConvKB and several baselines on the link prediction benchmark WN18RR and FB15k-237, demonstrating strong applicability and generalization and indicating that the uniformity of embedding space with high expressiveness enables more efficient knowledge graph embeddings.},
  archive      = {J_IDA},
  author       = {Tong, Da and Chen, Shudong and Ma, Rong and Qi, Donglin and Yu, Yong},
  doi          = {10.3233/IDA-227123},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {33-55},
  shortjournal = {Intell. Data Anal.},
  title        = {Knowledge graph embedding in a uniform space},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review on network representation learning with
multi-granularity perspective. <em>IDA</em>, <em>28</em>(1), 3–32. (<a
href="https://doi.org/10.3233/IDA-227328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network data is ubiquitous, such as telecommunication, transport systems, online social networks, protein-protein interactions, etc. Since the huge scale and the complexity of network data, former machine learning system tried to understand network data arduously. On the other hand, thought of mult i-granular cognitive computation simulates the problem-solving process of human brains. It simplifies the complex problems and solves problems from the easier to harder. Therefore, the application of multi-granularity problem-solving ideas or methods to deal with network data mining is increasingly adopted by researchers either intentionally or unintentionally. This paper looks into the domain of network representation learning (NRL). It systematically combs the research work in this field in recent years. In this paper, it is discovered that in dealing with the complexity of the network and pursuing the efficiency of computing resources, the multi-granularity solution becomes an excellent path that is hard to go around. Although there are several papers about survey of NRL, to our best knowledge, we are the first to survey the NRL from the perspective of multi-granular computing. This paper proposes the challenges that NRL meets. Furthermore, the feasibility of solving the challenges of NRL with multi-granular computing methodologies is analyzed and discussed. Some potential key scientific problems are sorted out and prospected in applying multi-granular computing for NRL research.},
  archive      = {J_IDA},
  author       = {Fu, Shun and Wang, Lufeng and Yang, Jie},
  doi          = {10.3233/IDA-227328},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {3-32},
  shortjournal = {Intell. Data Anal.},
  title        = {A review on network representation learning with multi-granularity perspective},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024f). Editorial. <em>IDA</em>, <em>28</em>(1), 1–2. (<a
href="https://doi.org/10.3233/IDA-239008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IDA},
  doi          = {10.3233/IDA-239008},
  journal      = {Intelligent Data Analysis},
  month        = {2},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Intell. Data Anal.},
  title        = {Editorial},
  volume       = {28},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
