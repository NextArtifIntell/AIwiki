<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SW_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sw---73">SW - 73</h2>
<ul>
<li><details>
<summary>
(2024). Ontology supported semantic based image retrieval.
<em>SW</em>, <em>15</em>(5), 2125–2137. (<a
href="https://doi.org/10.3233/SW-243660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a two-stage approach for developing a Semantic-Based Image Retrieval system supported by Ontology is proposed. In the initial stage, the Object Detection process is employed to identify objects within the image. Subsequently, a predicate describing the relationship between these two objects is determined using the developed Bi-directional Recurrent Neural Network (Bi-RNN) model. In the second stage, relations defined in the form of are transformed into Ontologies and utilized to search for images that are semantically similar. In addressing the primary challenge of Semantic Gap within the Semantic-Based Image Retrieval approach, the proposed solution involves measuring the number of similar relationships between two images through the utilization of entropy. The Semantic Gap between two images was computed using the Joint Entropy method, leveraging the number of relationships (X) identified in the query image and the total number of relationships (Y) in the image with similar relationships obtained as a query result. The proposed approach exhibits characteristics of a novel method within this field, distinct from other similar methods employed in Semantic-Based Image Retrieval through the utilization of Ontologies. In the performance measurement of the developed model, 91% accuracy was obtained according to the Recall@100 (Top-5 accuracy) result.},
  archive      = {J_SW},
  author       = {Gaşi, Akif and Ensari, Tolga and Dağtekin, Mustafa},
  doi          = {10.3233/SW-243660},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {2125-2137},
  shortjournal = {Semantic Web},
  title        = {Ontology supported semantic based image retrieval},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilingual question answering systems for knowledge
graphs – a survey. <em>SW</em>, <em>15</em>(5), 2089–2124. (<a
href="https://doi.org/10.3233/SW-243633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a survey on multilingual Knowledge Graph Question Answering (mKGQA). We employ a systematic review methodology to collect and analyze the research results in the field of mKGQA by defining scientific literature sources, selecting relevant publications, extracting objective information (e.g., problem, approach, evaluation values, used metrics, etc.), thoroughly analyzing the information, searching for novel insights, and methodically organizing them. Our insights are derived from 46 publications: 26 papers specifically focused on mKGQA systems, 14 papers concerning benchmarks and datasets, and 7 systematic survey articles. Starting its search from 2011, this work presents a comprehensive overview of the research field, encompassing the most recent findings pertaining to mKGQA and Large Language Models. We categorize the acquired information into a well-defined taxonomy, which classifies the methods employed in the development of mKGQA systems. Moreover, we formally define three pivotal characteristics of these methods, namely resource efficiency, multilinguality, and portability. These formal definitions serve as crucial reference points for selecting an appropriate method for mKGQA in a given use case. Lastly, we delve into the challenges of mKGQA, offer a broad outlook on the investigated research field, and outline important directions for future research. Accompanying this paper, we provide all the collected data, scripts, and documentation in an online appendix.},
  archive      = {J_SW},
  author       = {Perevalov, Aleksandr and Both, Andreas and Ngonga Ngomo, Axel-Cyrille},
  doi          = {10.3233/SW-243633},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {2089-2124},
  shortjournal = {Semantic Web},
  title        = {Multilingual question answering systems for knowledge graphs&amp;nbsp;– a survey},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QEndpoint: A novel triple store architecture for large RDF
graphs. <em>SW</em>, <em>15</em>(5), 2069–2087. (<a
href="https://doi.org/10.3233/SW-243616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the relational database realm, there has been a shift towards novel hybrid database architectures combining the properties of transaction processing (OLTP) and analytical processing (OLAP). OLTP workloads are made up by read and write operations on a small number of rows and are typically addressed by indexes such as B+trees. On the other side, OLAP workloads consists of big read operations that scan larger parts of the dataset. To address both workloads some databases introduced an architecture using a buffer or delta partition. Precisely, changes are accumulated in a write-optimized delta partition while the rest of the data is compressed in the read-optimized main partition. Periodically, the delta storage is merged in the main partition. In this paper we investigate for the first time how this architecture can be implemented and behaves for RDF graphs. We describe in detail the indexing-structures one can use for each partition, the merge process as well as the transactional management. We study the performances of our triple store, which we call qEndpoint , over two popular benchmarks, the Berlin SPARQL Benchmark (BSBM) and the recent Wikidata Benchmark (WDBench). We are also studying how it compares against other public Wikidata endpoints. This allows us to study the behavior of the triple store for different workloads, as well as the scalability over large RDF graphs. The results show that, compared to the baselines, our triple store allows for improved indexing times, better response time for some queries, higher insert and delete rates, and low disk and memory footprints, making it ideal to store and serve large Knowledge Graphs.},
  archive      = {J_SW},
  author       = {Willerval, Antoine and Diefenbach, Dennis and Bonifati, Angela},
  doi          = {10.3233/SW-243616},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {2069-2087},
  shortjournal = {Semantic Web},
  title        = {QEndpoint: A novel triple store architecture for large RDF graphs},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic web and its role in facilitating ICT data sharing
for the circular economy: An ontology survey. <em>SW</em>,
<em>15</em>(5), 2035–2067. (<a
href="https://doi.org/10.3233/SW-243586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The environmental pressure, CO 2 emissions (including embodied energy) and delivery risks of our digital infrastructures are increasing. The exponentially growing digitisation of services that drive the transition from industry 4.0 to industry 5.0 has resulted in a rising materials demand for ICT hardware manufacturing. ICT devices such as laptops and data servers are being used on average for 3 and 4–5 years respectively (van Driel ( 2020 )), while research shows that they should last 7 years before replacement ( Journal of Cleaner Production 69 ( 2014 ), 10–16). A solution is to transition from a linear to a circular economy (CE), through which materials that were previously disposed of as waste are re-entered back into product life-cycles through processes such as reuse, recycling, remanufacturing, repurposing. However, the adoption of the CE in the ICT sector is currently limited due to the lack of tools that support knowledge exchange between sustainability, ICT and technology experts in a standardised manner and the limited data availability, accessibility and interoperability needed to build such tools. Further, the already existing knowledge of the domain is fragmented into silos and the lack of a common terminology restricts its interoperability and usability. These also lead to transparency and responsibility issues along the supply chain. For many years now, the Semantic Web has been known to provide solutions to such issues in the form of ontologies. Several ontologies for the ICT, materials and CE domains have been build and successfully utilised to support processes such as predictive maintenance. However, there is a lack of a systematic analysis of the existing ontologies in these domains. Motivated by this, we present a literature survey and analysis of, but not limited to, existing ontologies for ICT devices such as laptops, materials and the CE. In addition, we discuss the need for findable, accessible, interoperable, reusable (FAIR) data in the CE, different factors such as data privacy and security that affect this and the role of ontologies.},
  archive      = {J_SW},
  author       = {Kurteva, Anelia and McMahon, Kathleen and Bozzon, Alessandro and Balkenende, Ruud},
  doi          = {10.3233/SW-243586},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {2035-2067},
  shortjournal = {Semantic Web},
  title        = {Semantic web and its role in facilitating ICT data sharing for the circular economy: An ontology survey},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A holistic view over ontologies for streaming linked data.
<em>SW</em>, <em>15</em>(5), 2005–2033. (<a
href="https://doi.org/10.3233/SW-243570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming Linked Data represents a domain within the Semantic Web dedicated to incorporating Stream Reasoning capabilities into the Semantic Web stack to address dynamic data challenges. Such applied endeavours typically necessitate a robust data modelling process. To this end, RDF Stream Processing (RSP) engines frequently utilize OWL 2 ontologies to facilitate this requirement. Despite the rich body of research on Knowledge Representation (KR), even concerning time-sensitive data, a notable gap exists in the literature regarding a comprehensive survey on KR techniques tailored for Streaming Linked Data. This paper critically overviews the key ontologies employed in RSP applications, evaluating their data modelling and KR abilities specifically for Streaming Linked Data contexts. We analyze these ontologies through three distinct KR perspectives: the conceptualization of streams as Web resources, the structural organization of data streams, and the event modelling within the streams. An analytical framework is introduced for each perspective to ensure a thorough and equitable comparison and deepen the understanding of the surveyed ontologies.},
  archive      = {J_SW},
  author       = {Bonte, Pieter and Ongenae, Femke and Tommasini, Riccardo},
  doi          = {10.3233/SW-243570},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {2005-2033},
  shortjournal = {Semantic Web},
  title        = {A holistic view over ontologies for streaming linked data},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepOnto: A python package for ontology engineering with
deep learning. <em>SW</em>, <em>15</em>(5), 1991–2004. (<a
href="https://doi.org/10.3233/SW-243568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating deep learning techniques, particularly language models (LMs), with knowledge representation techniques like ontologies has raised widespread attention, urging the need of a platform that supports both paradigms. Although packages such as OWL API and Jena offer robust support for basic ontology processing features, they lack the capability to transform various types of information within ontologies into formats suitable for downstream deep learning-based applications. Moreover, widely-used ontology APIs are primarily Java-based while deep learning frameworks like PyTorch and Tensorflow are mainly for Python programming. To address the needs, we present DeepOnto , a Python package designed for ontology engineering with deep learning. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more “Pythonic” manner and extending its capabilities to incorporate other essential components including reasoning, verbalisation, normalisation, taxonomy, projection, and more. Building on this module, DeepOnto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning methods, primarily pre-trained LMs. In this paper, we also demonstrate the practical utility of DeepOnto through two use-cases: the Digital Health Coaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment Evaluation Initiative (OAEI).},
  archive      = {J_SW},
  author       = {He, Yuan and Chen, Jiaoyan and Dong, Hang and Horrocks, Ian and Allocca, Carlo and Kim, Taehun and Sapkota, Brahmananda},
  doi          = {10.3233/SW-243568},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1991-2004},
  shortjournal = {Semantic Web},
  title        = {DeepOnto: A python package for ontology engineering with deep learning},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient management and compliance check of HVAC
information in the building design phase using semantic web
technologies. <em>SW</em>, <em>15</em>(5), 1959–1989. (<a
href="https://doi.org/10.3233/SW-243595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several OWL ontologies have been developed for the AEC industry to manage domain-specific information, yet they often overlook the domain of building services and HVAC components. The Flow Systems Ontology was recently proposed to address this need, but it does not include HVAC components’ size and capacity-related properties. Also, despite their strengths in representing domain-specific knowledge, ontologies cannot efficiently identify poor data quality in BIM models. A four-fold contribution is made in this research paper to define and improve the data quality of HVAC information by (1) extending the existing Flow Systems Ontology, (2) proposing the new Flow Properties Ontology, (3) proposing an HVAC rule set for compliance checking, and (4), moreover, we use Semantic Web technologies to demonstrate the benefits of efficient HVAC data management when sizing components. The demonstration case shows that we can represent the data model in a distributed way, validate it using 36 SHACL shapes and use SPARQL to determine the pressure and flow rate of fans and pumps.},
  archive      = {J_SW},
  author       = {Kücükavci, Ali and Seidenschnur, Mikki and Pauwels, Pieter and Rasmussen, Mads Holten and Hviid, Christian Anker},
  doi          = {10.3233/SW-243595},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1959-1989},
  shortjournal = {Semantic Web},
  title        = {Efficient management and compliance check of HVAC information in the building design phase using semantic web technologies},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilinguality and LLOD: A survey across linguistic
description levels. <em>SW</em>, <em>15</em>(5), 1915–1958. (<a
href="https://doi.org/10.3233/SW-243591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited accessibility to language resources and technologies represents a challenge for the analysis, preservation, and documentation of natural languages other than English. Linguistic Linked (Open) Data (LLOD) holds the promise to ease the creation, linking, and reuse of multilingual linguistic data across distributed and heterogeneous resources. However, individual language resources and technologies accommodate or target different linguistic description levels, e.g., morphology, syntax, phonology, and pragmatics. In this comprehensive survey, the state-of-the-art of multilinguality and LLOD is being represented with a particular focus on linguistic description levels, identifying open challenges and gaps as well as proposing an ideal ecosystem for multilingual LLOD across description levels. This survey seeks to contribute an introductory text for newcomers to the field of multilingual LLOD, uncover gaps and challenges to be tackled by the LLOD community in reference to linguistic description levels, and present a solid basis for a future best practice of multilingual LLOD across description levels.},
  archive      = {J_SW},
  author       = {Gromann, Dagmar and Apostol, Elena-Simona and Chiarcos, Christian and Cremaschi, Marco and Gracia, Jorge and Gkirtzou, Katerina and Liebeskind, Chaya and Mockiene, Liudmila and Rosner, Michael and Schuurman, Ineke and Sérasset, Gilles and Silvano, Purificação and Spahiu, Blerina and Truică, Ciprian-Octavian and Utka, Andrius and Valunaite Oleskeviciene, Giedre},
  doi          = {10.3233/SW-243591},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1915-1958},
  shortjournal = {Semantic Web},
  title        = {Multilinguality and LLOD: A survey across linguistic description levels},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A behaviouristic semantic approach to blockchain-based
e-commerce. <em>SW</em>, <em>15</em>(5), 1863–1914. (<a
href="https://doi.org/10.3233/SW-243543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic commerce and finance are progressively supporting and including decentralized, shared and public ledgers such as the blockchain. This is reshaping traditional commercial activities by advancing them towards Decentralized Finance (DeFi) and Commerce 3.0, thereby supporting the latter’s potential to outpace the hurdles of central authority controllers and lawgivers. The quantity and entropy of the information that must be sought and managed to become active participants in such a relentlessly evolving scenario are increasing at a steady pace. For example, that information comprises asset or service description, general rules of the game, and specific technologies involved for decentralization. Moreover, the relevant information ought to be shared among innumerable and heterogeneous stakeholders, such as producers, buyers, digital identity providers, valuation services, and shipment services, to just name a few. A clear semantic representation of such a complex and multifaceted blockchain-based e-Commerce ecosystem would contribute dramatically to make it more usable, namely more automatically accessible to virtually anyone wanting to play the role of a stakeholder, thereby reducing programmers’ effort. However, we feel that reaching that goal still requires substantial effort in the tailoring of Semantic Web technologies, hence this article sets out on such a route and advances a stack of OWL 2 ontologies for the semantic description of decentralized e-commerce. The stack includes a number of relevant features, ranging from the applicable stakeholders through the supply chain of the offerings for an asset, up to the Ethereum blockchain, its tokens and smart contracts. Ontologies are defined by taking a behaviouristic approach to represent the various participants as agents in terms of their actions, inspired by the Theory of Agents and the related mentalistic notions. The stack is validated through appropriate metrics and SPARQL queries implementing suitable competency questions, then demonstrated through the representation of a real world use case, namely, the iExec marketplace.},
  archive      = {J_SW},
  author       = {Bella, Giampaolo and Cantone, Domenico and Castiglione, Gianpietro and Nicolosi Asmundo, Marianna and Santamaria, Daniele Francesco},
  doi          = {10.3233/SW-243543},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1863-1914},
  shortjournal = {Semantic Web},
  title        = {A behaviouristic semantic approach to blockchain-based e-commerce},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontology of autonomous driving based on the SAE j3016
standard. <em>SW</em>, <em>15</em>(5), 1837–1862. (<a
href="https://doi.org/10.3233/SW-243578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving is a recently developed area in which technology seems to be ahead of its understanding within society. That causes some fears concerning the reliability of autonomous vehicles and controversies over liability in case of accidents. Specifying levels of driving autonomy within the SAE-J3016 standard is widely recognized as a significant step towards comprehending the essence of the achievements. However, the standard provides even more valuable insights into the process of driving automation. In the paper, we develop the ideas using the methods of formal ontology that allow us to make the conceptual system more precise and formalize it. To increase inseparability, we ground our system on a top-level BFO ontology. We present a formal account of several areas covered by the SAE-J3016 standard, including motor vehicles and their systems, driving tasks and subtasks, roles of persons in road communication, and autonomy levels.},
  archive      = {J_SW},
  author       = {Trypuz, Robert and Kulicki, Piotr and Sopek, Mirek},
  doi          = {10.3233/SW-243578},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1837-1862},
  shortjournal = {Semantic Web},
  title        = {Ontology of autonomous driving based on the SAE j3016 standard},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smart-KG: Partition-based linked data fragments for querying
knowledge graphs. <em>SW</em>, <em>15</em>(5), 1791–1835. (<a
href="https://doi.org/10.3233/SW-243571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDF and SPARQL provide a uniform way to publish and query billions of triples in open knowledge graphs (KGs) on the Web. Yet, provisioning of a fast, reliable, and responsive live querying solution for open KGs is still hardly possible through SPARQL endpoints alone: while such endpoints provide a remarkable performance for single queries, they typically can not cope with highly concurrent query workloads by multiple clients. To mitigate this, the Linked Data Fragments (LDF) framework sparked the design of different alternative low-cost interfaces such as Triple Pattern Fragments (TPF), that partially offload the query processing workload to the client side. On the downside, such interfaces still come with the expense of unnecessarily high network load due to the necessary transfer of intermediate results to the client, leading to query performance degradation compared with endpoints. To address this problem, in the present work, we investigate alternative interfaces, refining and extending the original TPF idea, which also aims at reducing server-resource consumption, by shipping query-relevant partitions of KGs from the server to the client. To this end, first, we align formal definitions and notations of the original LDF framework to uniformly present existing LDF implements and such “partition-based” LDF approaches. These novel LDF interfaces retrieve, instead of the exact triples matching a particular query pattern, a subset of pre-materialized, compressed, partitions of the original graph, containing all answers to a query pattern, to be further evaluated on the client side. As a concrete representative of partition-based LDF, we present smart-KG + , extending and refining our prior work (In WWW ’20: The Web Conference 2020 ( 2020 ) 984–994 ACM / IW3C2) in several respects. Our proposed approach is a step forward towards a better-balanced share of the query processing load between clients and servers by shipping graph partitions driven by the structure of RDF graphs to group entities described with the same sets of properties and classes, resulting in significant data transfer reduction. Our experiments demonstrate that the smart-KG + significantly outperforms existing Web SPARQL interfaces on both pre-existing benchmarks for highly concurrent query execution as well as an accustomed query workload inspired by query logs of existing SPARQL endpoints.},
  archive      = {J_SW},
  author       = {Azzam, Amr and Polleres, Axel and D. Fernández, Javier and Acosta, Maribel},
  doi          = {10.3233/SW-243571},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1791-1835},
  shortjournal = {Semantic Web},
  title        = {Smart-KG: Partition-based linked data fragments for querying knowledge graphs},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DIAERESIS: RDF data partitioning and query processing on
SPARK. <em>SW</em>, <em>15</em>(5), 1763–1789. (<a
href="https://doi.org/10.3233/SW-243554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosion of the web and the abundance of linked data demand effective and efficient methods for storage, management, and querying. Apache Spark is one of the most widely used engines for big data processing, with more and more systems adopting it for efficient query answering. Existing approaches exploiting Spark for querying RDF data, adopt partitioning techniques for reducing the data that need to be accessed in order to improve efficiency. However, simplistic data partitioning fails, on one hand, to minimize data access and on the other hand to group data usually queried together. This is translated into limited improvement in terms of efficiency in query answering. In this paper, we present DIAERESIS, a novel platform that accepts as input an RDF dataset and effectively partitions it, minimizing data access and improving query answering efficiency. To achieve this, DIAERESIS first identifies the top-k most important schema nodes, i.e., the most important classes, as centroids and distributes the other schema nodes to the centroid they mostly depend on. Then, it allocates the corresponding instance nodes to the schema nodes they are instantiated under. Our algorithm enables fine-tuning of data distribution, significantly reducing data access for query answering. We experimentally evaluate our approach using both synthetic and real workloads, strictly dominating existing state-of-the-art, showing that we improve query answering in several cases by orders of magnitude.},
  archive      = {J_SW},
  author       = {Troullinou, Georgia and Agathangelos, Giannis and Kondylakis, Haridimos and Stefanidis, Kostas and Plexousakis, Dimitris},
  doi          = {10.3233/SW-243554},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1763-1789},
  shortjournal = {Semantic Web},
  title        = {DIAERESIS: RDF data partitioning and query processing on SPARK},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontology of active and passive environmental exposure.
<em>SW</em>, <em>15</em>(5), 1733–1761. (<a
href="https://doi.org/10.3233/SW-243546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposure is a central concept of the health and behavioural sciences needed to study the influence of the environment on the health and behaviour of people within a spatial context. While an increasing number of studies measure different forms of exposure, including the influence of air quality, noise, and crime, the influence of land cover on physical activity, or of the urban environment on food intake, we lack a common conceptual model of environmental exposure that captures its main structure across all this variety. Against the background of such a model, it becomes possible not only to systematically compare different methodological approaches but also to better link and align the content of the vast amount of scientific publications on this topic in a systematic way. For example, an important methodical distinction is between studies that model exposure as an exclusive outcome of some activity versus ones where the environment acts as a direct independent cause ( active vs. passive exposure ). Here, we propose an information ontology design pattern that can be used to define exposure and to model its variants. It is built around causal relations between concepts including persons, activities, concentrations, exposures, environments and health risks. We formally define environmental stressors and variants of exposure using Description Logic (DL), which allows automatic inference from the RDF-encoded content of a paper. Furthermore, concepts can be linked with data models and modelling methods used in a study. To test the pattern, we translated competency questions into SPARQL queries and ran them over RDF-encoded content. Results show how study characteristics can be classified and summarized in a manner that reflects important methodical differences.},
  archive      = {J_SW},
  author       = {Vámos, Csilla and Scheider, Simon and Sonnenschein, Tabea and Vermeulen, Roel},
  doi          = {10.3233/SW-243546},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1733-1761},
  shortjournal = {Semantic Web},
  title        = {Ontology of active and passive environmental&amp;nbsp;exposure},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Similarity joins and clustering for SPARQL. <em>SW</em>,
<em>15</em>(5), 1701–1732. (<a
href="https://doi.org/10.3233/SW-243540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SPARQL standard provides operators to retrieve exact matches on data, such as graph patterns, filters and grouping. This work proposes and evaluates two new algebraic operators for SPARQL 1.1 that return similarity-based results instead of exact results. First, a similarity join operator is presented, which brings together similar mappings from two sets of solution mappings. Second, a clustering solution modifier is introduced, which instead of grouping solution mappings according to exact values, brings them together by using similarity criteria. For both cases, a variety of algorithms are proposed and analysed, and use-case queries that showcase the relevance and usefulness of the novel operators are presented. For similarity joins, experimental results are provided by comparing different physical operators over a set of real world queries, as well as comparing our implementation to the closest work found in the literature, DBSimJoin, a PostgreSQL extension that supports similarity joins. For clustering, synthetic queries are designed in order to measure the performance of the different algorithms implemented.},
  archive      = {J_SW},
  author       = {Ferrada, Sebastián and Bustos, Benjamin and Hogan, Aidan},
  doi          = {10.3233/SW-243540},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1701-1732},
  shortjournal = {Semantic Web},
  title        = {Similarity joins and clustering for SPARQL},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D2kg: An integrated ontology for knowledge graph-based
representation of government decisions and acts. <em>SW</em>,
<em>15</em>(5), 1677–1699. (<a
href="https://doi.org/10.3233/SW-243535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To implement Open Governance a crucial element is the efficient use of the big amounts of open data produced in the public domain. Public administration is a rich source of data and potentially new knowledge. It is a data intensive sector producing vast amounts of information encoded in government decisions and acts, published nowadays on the World Wide Web. The knowledge shared on the Web is mostly made available via semi-structured documents written in natural language. To exploit this knowledge, technologies such as Natural Language Processing, Information Extraction, Data mining and the Semantic Web could be used, embedding into documents explicit semantics based on formal knowledge representations such as ontologies. Knowledge representation can be made possible by the deployment of Knowledge Graphs, collections of interlinked representations of entities, events or concepts, based on underlying ontologies. This can assist data analysts to achieve a higher level of situational awareness, facilitating automated reasoning towards different objectives, such as for knowledge management, data maintenance, transparency and cybersecurity. This paper presents a new ontology d2kg [d(iavgeia) 2(to) k(nowledge) g(raph)] integrating in a unique way standard EU ontologies, core and controlled vocabularies to enable exploitation of publicly available data from government decisions and acts published on the Greek platform Diavgeia with the aim to facilitate data sharing, re-usability and interoperability. It demonstrates a characteristic example of a Knowledge Graph based representation of government decisions and acts, highlighting its added value to respond to real practical use cases for the promotion of transparency, accountability and public awareness. The developed d2kg ontology in owl is accessible at: http://w3id.org/d2kg , as well as documented at: http://w3id.org/d2kg/documentation .},
  archive      = {J_SW},
  author       = {Serderidis, Konstantinos and Konstantinidis, Ioannis and Meditskos, Georgios and Peristeras, Vassilios and Bassiliades, Nick},
  doi          = {10.3233/SW-243535},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1677-1699},
  shortjournal = {Semantic Web},
  title        = {D2kg: An integrated ontology for knowledge graph-based representation of government decisions and acts},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontology-based GraphQL server generation for data access and
data integration. <em>SW</em>, <em>15</em>(5), 1639–1675. (<a
href="https://doi.org/10.3233/SW-233550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a GraphQL Web API, a so-called GraphQL schema defines the types of data objects that can be queried, and so-called resolver functions are responsible for fetching the relevant data from underlying data sources. Thus, we can expect to use GraphQL not only for data access but also for data integration, if the GraphQL schema reflects the semantics of data from multiple data sources, and the resolver functions can obtain data from these data sources and structure the data according to the schema. However, there does not exist a semantics-aware approach to employ GraphQL for data integration. Furthermore, there are no formal methods for defining a GraphQL API based on an ontology. In this work, we introduce a framework for using GraphQL in which a global domain ontology informs the generation of a GraphQL server that answers requests by querying heterogeneous data sources. The core of this framework consists of an algorithm to generate a GraphQL schema based on an ontology and a generic resolver function based on semantic mappings. We provide a prototype, OBG-gen, of this framework, and we evaluate our approach over a real-world data integration scenario in the materials design domain and two synthetic benchmark scenarios (Linköping GraphQL Benchmark and GTFS-Madrid-Bench). The experimental results of our evaluation indicate that: (i) our approach is feasible to generate GraphQL servers for data access and integration over heterogeneous data sources, thus avoiding a manual construction of GraphQL servers, and (ii) our data access and integration approach is general and applicable to different domains where data is shared or queried via different ways.},
  archive      = {J_SW},
  author       = {Li, Huanyu and Hartig, Olaf and Armiento, Rickard and Lambrix, Patrick},
  doi          = {10.3233/SW-233550},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1639-1675},
  shortjournal = {Semantic Web},
  title        = {Ontology-based GraphQL server generation for data access and data integration},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards counterfactual explanations for ontologies.
<em>SW</em>, <em>15</em>(5), 1611–1636. (<a
href="https://doi.org/10.3233/SW-243566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Debugging and repairing Web Ontology Language (OWL) ontologies has been a key field of research since OWL became a W3C recommendation. One way to understand errors and fix them is through explanations. These explanations are usually extracted from the reasoner and displayed to the ontology authors as is. In the meantime, there has been a recent call in the eXplainable AI (XAI) field to use expert knowledge in the form of knowledge graphs and ontologies. In this paper, a parallel between explanations for machine learning and for ontologies is drawn. This link enables the adaptation of XAI methods to explain ontologies and their entailments. Counterfactual explanations have been identified as a good candidate to solve the explainability problem in machine learning. The CEO (Counterfactual Explanations for Ontologies) method is thus proposed to explain inconsistent ontologies using counterfactual explanations. A preliminary user study is conducted to ensure that using XAI methods for ontologies is relevant and worth pursuing.},
  archive      = {J_SW},
  author       = {Bellucci, Matthieu and Delestre, Nicolas and Malandain, Nicolas and Zanni-Merk, Cecilia},
  doi          = {10.3233/SW-243566},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1611-1636},
  shortjournal = {Semantic Web},
  title        = {Towards counterfactual explanations for ontologies},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What is in your cookie box? Explaining ingredients of web
cookies with knowledge graphs. <em>SW</em>, <em>15</em>(5), 1593–1609.
(<a href="https://doi.org/10.3233/SW-233435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The General Data Protection Regulation (GDPR) has imposed strict requirements for data sharing, one of which is informed consent. A common way to request consent online is via cookies. However, commonly, users accept online cookies being unaware of the meaning of the given consent and the following implications. Once consent is given, the cookie “disappears”, and one forgets that consent was given in the first place. Retrieving cookies and consent logs becomes challenging, as most information is stored in the specific Internet browser’s logs. To make users aware of the data sharing implied by cookie consent and to support transparency and traceability within systems, we present a knowledge graph (KG) based tool for personalised cookie consent information visualisation. The KG is based on the OntoCookie ontology, which models cookies in a machine-readable format and supports data interpretability across domains. Evaluation results confirm that the users’ comprehension of the data shared through cookies is vague and insufficient. Furthermore, our work has resulted in an increase of 47.5% in the users’ willingness to be cautious when viewing cookie banners before giving consent. These and other evaluation results confirm that our cookie data visualisation approach and tool help to increase users’ awareness of cookies and data sharing.},
  archive      = {J_SW},
  author       = {Bushati, Geni and Rasmusen, Sven Carsten and Kurteva, Anelia and Vats, Anurag and Nako, Petraq and Fensel, Anna},
  doi          = {10.3233/SW-233435},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1593-1609},
  shortjournal = {Semantic Web},
  title        = {What is in your cookie box? explaining ingredients of web cookies with knowledge graphs},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LOD4Culture: Easy exploration of cultural heritage linked
open data. <em>SW</em>, <em>15</em>(5), 1563–1592. (<a
href="https://doi.org/10.3233/SW-233358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LOD4Culture is a web application that exploits Cultural Heritage Linked Open Data for tourism and education purposes. Since target users are not fluid on Semantic Web technologies, the user interface is designed to hide the intricacies of RDF or SPARQL. An interactive map is provided for exploring world-wide Cultural Heritage sites that can be filtered by type and that uses cluster markers to adapt the view to different zoom levels. LOD4Culture also includes a Cultural Heritage entity browser that builds comprehensive visualizations of sites, artists, and artworks. All data exchanges are facilitated through the use of a generator of REST APIs over Linked Open Data that translates API calls into SPARQL queries across multiple sources, including Wikidata and DBpedia. Since March 2022, more than 1.7K users have employed LOD4Culture. The application has been mentioned many times in social media and has been featured in the DBpedia Newsletter, in the list of Wikidata tools for visualizing data, and in the open data applications list of datos.gob.es .},
  archive      = {J_SW},
  author       = {Vega-Gorgojo, Guillermo},
  doi          = {10.3233/SW-233358},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1563-1592},
  shortjournal = {Semantic Web},
  title        = {LOD4Culture: Easy exploration of cultural heritage linked open data},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MuHeQA: Zero-shot question answering over multiple and
heterogeneous knowledge bases. <em>SW</em>, <em>15</em>(5), 1547–1561.
(<a href="https://doi.org/10.3233/SW-233379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two main limitations in most of the existing Knowledge Graph Question Answering (KGQA) algorithms. First, the approaches depend heavily on the structure and cannot be easily adapted to other KGs. Second, the availability and amount of additional domain-specific data in structured or unstructured formats has also proven to be critical in many of these systems. Such dependencies limit the applicability of KGQA systems and make their adoption difficult. A novel algorithm is proposed, MuHeQA, that alleviates both limitations by retrieving the answer from textual content automatically generated from KGs instead of queries over them. This new approach (1) works on one or several KGs simultaneously, (2) does not require training data what makes it is domain-independent, (3) enables the combination of knowledge graphs with unstructured information sources to build the answer, and (4) reduces the dependency on the underlying schema since it does not navigate through structured content but only reads property values. MuHeQA extracts answers from textual summaries created by combining information related to the question from multiple knowledge bases, be them structured or not. Experiments over Wikidata and DBpedia show that our approach achieves comparable performance to other approaches in single-fact questions while being domain and KG independent. Results raise important questions for future work about how the textual content that can be created from knowledge graphs enables answer extraction.},
  archive      = {J_SW},
  author       = {Badenes-Olmedo, Carlos and Corcho, Oscar},
  doi          = {10.3233/SW-233379},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1547-1561},
  shortjournal = {Semantic Web},
  title        = {MuHeQA: Zero-shot question answering over multiple and heterogeneous knowledge bases},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the structure of knowledge graphs with ABSTAT
profiles. <em>SW</em>, <em>15</em>(5), 1519–1545. (<a
href="https://doi.org/10.3233/SW-223181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While there has been a trend in the last decades for publishing large-scale and highly-interconnected Knowledge Graphs (KGs), their users often get overwhelmed by the task of understanding their content as a result of their size and complexity. Data profiling approaches have been proposed to summarize large KGs into concise and meaningful representations, so that they can be better explored, processed, and managed. Profiles based on schema patterns represent each triple in a KG with its schema-level counterpart, thus covering the entire KG with profiles of considerable size. In this paper, we provide empirical evidence that profiles based on schema patterns, if explored with suitable mechanisms, can be useful to help users understand the content of big and complex KGs. ABSTAT provides concise pattern-based profiles and comes with faceted interfaces for profile exploration. Using this tool we present a user study based on query completion tasks. We demonstrate that users who look at ABSTAT profiles formulate their queries better and faster than users browsing the ontology of the KGs. The latter is a pretty strong baseline considering that many KGs do not even come with a specific ontology to be explored by the users. To the best of our knowledge, this is the first attempt to investigate the impact of profiling techniques on tasks related to knowledge graph understanding with a user study.},
  archive      = {J_SW},
  author       = {Spahiu, Blerina and Palmonari, Matteo and Alva Principe, Renzo Arturo and Rula, Anisa},
  doi          = {10.3233/SW-223181},
  journal      = {Semantic Web},
  month        = {10},
  number       = {5},
  pages        = {1519-1545},
  shortjournal = {Semantic Web},
  title        = {Understanding the structure of knowledge graphs with ABSTAT profiles},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to create and use a national cross-domain ontology and
data infrastructure on the semantic web. <em>SW</em>, <em>15</em>(4),
1499–1513. (<a href="https://doi.org/10.3233/SW-243468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a model and lessons learned for creating a cross-domain national ontology and Linked (Open) Data (LOD) infrastructure. The idea is to extend the global, domain agnostic “layer cake model” underlying the Semantic Web with domain specific and local features needed in applications. To test and demonstrate the infrastructure, a series of LOD services and portals in use have been created in 2002–2023 that cover a wide range of application domains. They have attracted millions of users in total suggesting feasibility of the proposed model. This line of research and development is unique due to its systematic national level nature and long time span of over twenty years.},
  archive      = {J_SW},
  author       = {Hyvönen, Eero},
  doi          = {10.3233/SW-243468},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1499-1513},
  shortjournal = {Semantic Web},
  title        = {How to create and use a national cross-domain ontology and data infrastructure on the semantic web},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing data use ontology (DUO) for health-data sharing by
extending it with ODRL and DPV. <em>SW</em>, <em>15</em>(4), 1473–1498.
(<a href="https://doi.org/10.3233/SW-243583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Global Alliance for Genomics and Health is an international consortium that is developing the Data Use Ontology (DUO) as a standard providing machine-readable codes for automation in data discovery and responsible sharing of genomics data. DUO concepts, which are encoded using OWL, only contain the textual descriptions of the conditions for data use they represent, and do not specify the intended permissions, prohibitions, and obligations explicitly – which limits their usefulness. We present an exploration of how the Open Digital Rights Language (ODRL) can be used to explicitly represent the information inherent in DUO concepts to create policies that are then used to represent conditions under which datasets are available for use, conditions in requests to use them, and to generate agreements based on a compatibility matching between the two. We also address a current limitation of DUO regarding specifying information relevant to privacy and data protection law by using the Data Privacy Vocabulary (DPV) which supports expressing legal concepts in a jurisdiction-agnostic manner as well as for specific laws like the GDPR. Our work supports the existing socio-technical governance processes involving use of DUO by providing a complementary rather than replacement approach. To support this and improve DUO, we provide a description of how our system can be deployed with a proof of concept demonstration that uses ODRL rules for all DUO concepts, and uses them to generate agreements through matching of requests to data offers. All resources described in this article are available at: https://w3id.org/duodrl/repo .},
  archive      = {J_SW},
  author       = {Pandit, Harshvardhan J. and Esteves, Beatriz},
  doi          = {10.3233/SW-243583},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1473-1498},
  shortjournal = {Semantic Web},
  title        = {Enhancing data use ontology (DUO) for health-data sharing by extending it with ODRL and DPV},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware composition of agent policies by markov
decision process entity embeddings and agent ensembles. <em>SW</em>,
<em>15</em>(4), 1443–1471. (<a
href="https://doi.org/10.3233/SW-233531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational agents support humans in many areas of life and are therefore found in heterogeneous contexts. This means that agents operate in rapidly changing environments and can be confronted with huge state and action spaces. In order to perform services and carry out activities satisfactorily, i.e. in a goal-oriented manner, agents require prior knowledge and therefore have to develop and pursue context-dependent policies. The problem here is that prescribing policies in advance is limited and inflexible, especially in dynamically changing environments. Moreover, the context (i.e. the external and internal state) of an agent determines its choice of actions. Since the environments in which agents operate can be stochastic and complex in terms of the number of states and feasible actions, activities are usually modelled in a simplified way by Markov decision processes so that, for example, agents with reinforcement learning are able to learn policies, i.e. state-action pairs, that help to capture the context and act accordingly to optimally perform activities. However, training policies for all possible contexts using reinforcement learning is time-consuming. A requirement and challenge for agents is to learn strategies quickly and respond immediately in cross-context environments and applications, e.g., the Internet, service robotics, cyber-physical systems. In this work, we propose a novel simulation-based approach that enables a) the representation of heterogeneous contexts through knowledge graphs and entity embeddings and b) the context-aware composition of policies on demand by ensembles of agents running in parallel. The evaluation we conducted with the “Virtual Home” dataset indicates that agents with a need to switch seamlessly between different contexts, e.g. in a home environment, can request on-demand composed policies that lead to the successful completion of context-appropriate activities without having to learn these policies in lengthy training steps and episodes, in contrast to agents that use reinforcement learning. The presented approach enables both context-aware and cross-context applicability of untrained computational agents. Furthermore, the source code of the approach as well as the generated data, i.e. the trained embeddings and the semantic representation of domestic activities, is open source and openly accessible on Github and Figshare.},
  archive      = {J_SW},
  author       = {Merkle, Nicole and Mikut, Ralf},
  doi          = {10.3233/SW-233531},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1443-1471},
  shortjournal = {Semantic Web},
  title        = {Context-aware composition of agent policies by markov decision process entity embeddings and agent ensembles},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ImageSchemaNet: A framester graph for embodied commonsense
knowledge. <em>SW</em>, <em>15</em>(4), 1417–1441. (<a
href="https://doi.org/10.3233/SW-223084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonsense knowledge is a broad and challenging area of research which investigates our understanding of the world as well as human assumptions about reality. Deriving directly from the subjective perception of the external world, it is intrinsically intertwined with embodied cognition. Commonsense reasoning is linked to human sense-making, pattern recognition and knowledge framing abilities. This work presents a new resource that formalizes the cognitive theory of image schemas. Image schemas are dynamic conceptual building blocks originating from our sensorimotor interactions with the physical world, and enable our sense-making cognitive activity to assign coherence and structure to entities, events and situations we experience everyday. ImageSchemaNet is an ontology that aligns pre-existing resources, such as FrameNet, VerbNet, WordNet and MetaNet from the Framester hub, to image schema theory. This article describes an empirical application of ImageSchemaNet, combined with semantic parsers, on the task of annotating natural language sentences with image schemas.},
  archive      = {J_SW},
  author       = {De Giorgis, Stefano and Gangemi, Aldo and Gromann, Dagmar},
  doi          = {10.3233/SW-223084},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1417-1441},
  shortjournal = {Semantic Web},
  title        = {ImageSchemaNet: A framester graph for embodied commonsense knowledge},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeuSyRE: Neuro-symbolic visual understanding and reasoning
framework based on scene graph enrichment. <em>SW</em>, <em>15</em>(4),
1389–1413. (<a href="https://doi.org/10.3233/SW-233510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring the potential of neuro-symbolic hybrid approaches offers promising avenues for seamless high-level understanding and reasoning about visual scenes. Scene Graph Generation (SGG) is a symbolic image representation approach based on deep neural networks (DNN) that involves predicting objects, their attributes, and pairwise visual relationships in images to create scene graphs, which are utilized in downstream visual reasoning. The crowdsourced training datasets used in SGG are highly imbalanced, which results in biased SGG results. The vast number of possible triplets makes it challenging to collect sufficient training samples for every visual concept or relationship. To address these challenges, we propose augmenting the typical data-driven SGG approach with common sense knowledge to enhance the expressiveness and autonomy of visual understanding and reasoning. We present a loosely-coupled neuro-symbolic visual understanding and reasoning framework that employs a DNN-based pipeline for object detection and multi-modal pairwise relationship prediction for scene graph generation and leverages common sense knowledge in heterogenous knowledge graphs to enrich scene graphs for improved downstream reasoning. A comprehensive evaluation is performed on multiple standard datasets, including Visual Genome and Microsoft COCO, in which the proposed approach outperformed the state-of-the-art SGG methods in terms of relationship recall scores, i.e. Recall@K and mean Recall@K, as well as the state-of-the-art scene graph-based image captioning methods in terms of SPICE and CIDEr scores with comparable BLEU, ROGUE and METEOR scores. As a result of enrichment, the qualitative results showed improved expressiveness of scene graphs, resulting in more intuitive and meaningful caption generation using scene graphs. Our results validate the effectiveness of enriching scene graphs with common sense knowledge using heterogeneous knowledge graphs. This work provides a baseline for future research in knowledge-enhanced visual understanding and reasoning. The source code is available at https://github.com/jaleedkhan/neusire .},
  archive      = {J_SW},
  author       = {Khan, M. Jaleed and G. Breslin, John and Curry, Edward},
  doi          = {10.3233/SW-233510},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1389-1413},
  shortjournal = {Semantic Web},
  title        = {NeuSyRE: Neuro-symbolic visual understanding and reasoning framework based on scene graph enrichment},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). INK: Knowledge graph representation for efficient and
performant rule mining. <em>SW</em>, <em>15</em>(4), 1367–1388. (<a
href="https://doi.org/10.3233/SW-233495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic rule mining can be used for both deriving task-agnostic or task-specific information within a Knowledge Graph (KG). Underlying logical inferences to summarise the KG or fully interpretable binary classifiers predicting future events are common results of such a rule mining process. The current methods to perform task-agnostic or task-specific semantic rule mining operate, however, a completely different KG representation, making them less suitable to perform both tasks or incorporate each other’s optimizations. This also results in the need to master multiple techniques for both exploring and mining rules within KGs, as well losing time and resources when converting one KG format into another. In this paper, we use INK, a KG representation based on neighbourhood nodes of interest to mine rules for improved decision support. By selecting one or two sets of nodes of interest, the rule miner created on top of the INK representation will either mine task-agnostic or task-specific rules. In both subfields, the INK miner is competitive to the currently state-of-the-art semantic rule miners on 14 different benchmark datasets within multiple domains.},
  archive      = {J_SW},
  author       = {Steenwinckel, Bram and De Turck, Filip and Ongenae, Femke},
  doi          = {10.3233/SW-233495},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1367-1388},
  shortjournal = {Semantic Web},
  title        = {INK: Knowledge graph representation for efficient and performant rule mining},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reason-able embeddings: Learning concept embeddings with a
transferable neural reasoner. <em>SW</em>, <em>15</em>(4), 1333–1365.
(<a href="https://doi.org/10.3233/SW-233355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for learning embeddings of ALC knowledge base concepts. The embeddings reflect the semantics of the concepts in such a way that it is possible to compute an embedding of a complex concept from the embeddings of its parts by using appropriate neural constructors. Embeddings for different knowledge bases are vectors in a shared vector space, shaped in such a way that approximate subsumption checking for arbitrarily complex concepts can be done by the same neural network, called a reasoner head, for all the knowledge bases. To underline this unique property of enabling reasoning directly on embeddings, we call them reason-able embeddings. We report the results of experimental evaluation showing that the difference in reasoning performance between training a separate reasoner head for each ontology and using a shared reasoner head, is negligible.},
  archive      = {J_SW},
  author       = {Adamski, Dariusz Max and Potoniec, Jędrzej},
  doi          = {10.3233/SW-233355},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1333-1365},
  shortjournal = {Semantic Web},
  title        = {Reason-able embeddings: Learning concept embeddings with a transferable neural reasoner},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A neuro-symbolic system over knowledge graphs for link
prediction. <em>SW</em>, <em>15</em>(4), 1307–1331. (<a
href="https://doi.org/10.3233/SW-233324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuro-Symbolic Artificial Intelligence (AI) focuses on integrating symbolic and sub-symbolic systems to enhance the performance and explainability of predictive models. Symbolic and sub-symbolic approaches differ fundamentally in how they represent data and make use of data features to reach conclusions. Neuro-symbolic systems have recently received significant attention in the scientific community. However, despite efforts in neural-symbolic integration, symbolic processing can still be better exploited, mainly when these hybrid approaches are defined on top of knowledge graphs. This work is built on the statement that knowledge graphs can naturally represent the convergence between data and their contextual meaning (i.e., knowledge). We propose a hybrid system that resorts to symbolic reasoning, expressed as a deductive database, to augment the contextual meaning of entities in a knowledge graph, thus, improving the performance of link prediction implemented using knowledge graph embedding (KGE) models. An entity context is defined as the ego network of the entity in a knowledge graph. Given a link prediction task, the proposed approach deduces new RDF triples in the ego networks of the entities corresponding to the heads and tails of the prediction task on the knowledge graph (KG). Since knowledge graphs may be incomplete and sparse, the facts deduced by the symbolic system not only reduce sparsity but also make explicit meaningful relations among the entities that compose an entity ego network. As a proof of concept, our approach is applied over a KG for lung cancer to predict treatment effectiveness. The empirical results put the deduction power of deductive databases into perspective. They indicate that making explicit deduced relationships in the ego networks empowers all the studied KGE models to generate more accurate links.},
  archive      = {J_SW},
  author       = {Rivas, Ariam and Collarana, Diego and Torrente, Maria and Vidal, Maria-Esther},
  doi          = {10.3233/SW-233324},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1307-1331},
  shortjournal = {Semantic Web},
  title        = {A neuro-symbolic system over knowledge graphs for link prediction},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is neuro-symbolic AI meeting its promises in natural
language processing? A structured review. <em>SW</em>, <em>15</em>(4),
1265–1306. (<a href="https://doi.org/10.3233/SW-223228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis. 1 1},
  archive      = {J_SW},
  author       = {Hamilton, Kyle and Nayak, Aparna and Božić, Bojan and Longo, Luca},
  doi          = {10.3233/SW-223228},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1265-1306},
  shortjournal = {Semantic Web},
  title        = {Is neuro-symbolic AI meeting its promises in natural language processing? a&amp;nbsp;structured review},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuro-symbolic AI and the semantic web. <em>SW</em>,
<em>15</em>(4), 1261–1263. (<a
href="https://doi.org/10.3233/SW-243711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {Hitzler, Pascal and Ebrahimi, Monireh and Sarker, Md Kamruzzaman and Stepanova, Daria},
  doi          = {10.3233/SW-243711},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1261-1263},
  shortjournal = {Semantic Web},
  title        = {Neuro-symbolic AI and the semantic web},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OBO foundry food ontology interconnectivity. <em>SW</em>,
<em>15</em>(4), 1239–1258. (<a
href="https://doi.org/10.3233/SW-233458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its creation in 2016, the FoodOn food ontology has become an interconnected partner in various academic and government projects that span agricultural and public health domains. This paper examines recent data interoperability capabilities arising from food-related ontologies belonging to, or compatible with, the encyclopedic Open Biological and Biomedical Ontology Foundry (OBO) ontology platform, and how research organizations and industry might utilize them for their own projects or for data exchange. Projects are seeking standardized vocabulary across many food supply activities ranging from agricultural production, harvesting, preparation, food processing, marketing, distribution and consumption, as well as more indirect health, economic, food security and sustainability analysis and reporting tools. To satisfy this demand for controlled vocabulary requires establishing domain specific ontologies whose curators coordinate closely to produce recommended patterns for food system vocabulary.},
  archive      = {J_SW},
  author       = {Dooley, Damion and Andrés-Hernández, Liliana and Bordea, Georgeta and Carmody, Leigh and Cavalieri, Duccio and Chan, Lauren and Castellano-Escuder, Pol and Lachat, Carl and Mougin, Fleur and Vitali, Francesco and Yang, Chen and Weber, Magalie and Kucuk McGinty, Hande and Lange, Matthew},
  doi          = {10.3233/SW-233458},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1239-1258},
  shortjournal = {Semantic Web},
  title        = {OBO foundry food ontology interconnectivity},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data sharing in agricultural supply chains: Using semantics
to enable sustainable food systems. <em>SW</em>, <em>15</em>(4),
1207–1237. (<a href="https://doi.org/10.3233/SW-233287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The agrifood system faces a great many economic, social and environmental challenges. One of the biggest practical challenges has been to achieve greater data sharing throughout the agrifood system and the supply chain, both to inform other stakeholders about a product and equally to incentivise greater environmental sustainability. In this paper, a data sharing architecture is described built on three principles (a) reuse of existing semantic standards; (b) integration with legacy systems; and (c) a distributed architecture where stakeholders control access to their own data. The system has been developed based on the requirements of commercial users and is designed to allow queries across a federated network of agrifood stakeholders. The Ploutos semantic model is built on an integration of existing ontologies. The Ploutos architecture is built on a discovery directory and interoperability enablers, which use graph query patterns to traverse the network and collect the requisite data to be shared. The system is exemplified in the context of a pilot involving commercial stakeholders in the processed fruit sector. The data sharing approach is highly extensible with considerable potential for capturing sustainability related data.},
  archive      = {J_SW},
  author       = {Brewster, Christopher and Kalatzis, Nikos and Nouwt, Barry and Kruiger, Han and Verhoosel, Jack},
  doi          = {10.3233/SW-233287},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1207-1237},
  shortjournal = {Semantic Web},
  title        = {Data sharing in agricultural supply chains: Using semantics to enable sustainable food systems},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reuse of the FoodOn ontology in a knowledge base of food
composition data. <em>SW</em>, <em>15</em>(4), 1195–1206. (<a
href="https://doi.org/10.3233/SW-233207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe our work to integrate the FoodOn ontology with our knowledge base of food composition data, WikiFCD. WikiFCD is knowledge base of structured data related to food composition and food items. With a goal to reuse FoodOn identifiers for food items, we imported a subset of the FoodOn ontology into the WikiFCD knowledge base. We aligned the import via a shared use of NCBI taxon identifiers for the taxon names of the plants from which the food items are derived. Reusing FoodOn benefits WikiFCD by allowing us to leverage the food item groupings that FoodOn contains. This integration also has potential future benefits for the FoodOn community due to the fact that WikiFCD provides food composition data at the food item level, and that WikiFCD is mapped to Wikidata and contains a SPARQL endpoint that supports federated queries. Federated queries across WikiFCD and Wikidata allow us to ask questions about food items that benefit from the cross-domain information of Wikidata, greatly increasing the breadth of possible data combinations.},
  archive      = {J_SW},
  author       = {Thornton, Katherine and Seals-Nutt, Kenneth and Matsuzaki, Mika and Dooley, Damion},
  doi          = {10.3233/SW-233207},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1195-1206},
  shortjournal = {Semantic Web},
  title        = {Reuse of the FoodOn ontology in a knowledge base of food composition data},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semantic meta-model for data integration and exploitation
in precision agriculture and livestock farming. <em>SW</em>,
<em>15</em>(4), 1165–1193. (<a
href="https://doi.org/10.3233/SW-233156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the domains of agriculture and livestock farming a large amount of data are produced through numerous heterogeneous sources including sensor data, weather/climate data, statistical and government data, drone/satellite imagery, video, and maps. This plethora of data can be used at precision agriculture and precision livestock farming in order to provide predictive insights in farming operations, drive real-time operational decisions, redesign business processes and support policy-making. The predictive power of the data can be further boosted if data from diverse sources are integrated and processed together, thus providing more unexplored insights. However, the exploitation and integration of data used in precision agriculture is not straightforward since they: i) cannot be easily discovered across the numerous heterogeneous sources and ii) use different structural and naming conventions hindering their interoperability. The aim of this paper is to: i) study the characteristics of data used in precision agriculture &amp; livestock farming and ii) study the user requirements related to data modeling and processing from nine real cases at the agriculture, livestock farming and aquaculture domains and iii) propose a semantic meta-model that is based on W3C standards (DCAT, PROV-O and QB vocabulary) in order to enable the definition of metadata that facilitate the discovery, exploration, integration and accessing of data in the domain.},
  archive      = {J_SW},
  author       = {Zeginis, Dimitris and Kalampokis, Evangelos and Palma, Raul and Atkinson, Rob and Tarabanis, Konstantinos},
  doi          = {10.3233/SW-233156},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1165-1193},
  shortjournal = {Semantic Web},
  title        = {A semantic meta-model for data integration and exploitation in precision agriculture and livestock farming},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Food process ontology requirements. <em>SW</em>,
<em>15</em>(4), 1133–1164. (<a
href="https://doi.org/10.3233/SW-223096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People often value the sensual, celebratory, and health aspects of food, but behind this experience exists many other value-laden agricultural production, distribution, manufacturing, and physiological processes that support or undermine a healthy population and a sustainable future. The complexity of such processes is evident in both every-day food preparation of recipes and in industrial food manufacturing, packaging and storage, each of which depends critically on human or machine agents, chemical or organismal ingredient references, and the explicit instructions and implicit procedures held in formulations or recipes. An integrated ontology landscape does not yet exist to cover all the entities at work in this farm to fork journey. It seems necessary to construct such a vision by reusing expert-curated fit-to-purpose ontology subdomains and their relationship, material, and more abstract organization and role entities. The challenge is to make this merger be, by analogy, one language, rather than nouns and verbs from a dozen or more dialects which cannot be used directly in statements about some aspect of the farm to fork journey without expensive translation or substantial dialect education in order to understand a particular text or domain of knowledge. This work focuses on the ontology components – object and data properties and annotations – needed to model food processes or more general process modelling within the context of the Open Biological and Biomedical Ontology Foundry and congruent ontologies. Ideally these components can be brought together in a general process ontology that can be specialized not only for the food domain but for carrying out other protocols as well. Many operations involved in food identification, preparation, transportation and storage – shaking, boiling, mixing, freezing, labeling, shipping – are actually common to activities from manufacturing and laboratory work to local or home food preparation.},
  archive      = {J_SW},
  author       = {Dooley, Damion and Weber, Magalie and Ibanescu, Liliana and Lange, Matthew and Chan, Lauren and Soldatova, Larisa and Yang, Chen and Warren, Robert and Shimizu, Cogan and McGinty, Hande K. and Hsiao, William},
  doi          = {10.3233/SW-223096},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1133-1164},
  shortjournal = {Semantic Web},
  title        = {Food process ontology requirements},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a global food systems datahub. <em>SW</em>,
<em>15</em>(4), 1129–1132. (<a
href="https://doi.org/10.3233/SW-243688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {McGinty, Hande Küçük and Shimizu, Cogan and Hitzler, Pascal and Sharda, Ajay},
  doi          = {10.3233/SW-243688},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1129-1132},
  shortjournal = {Semantic Web},
  title        = {Towards a global food systems datahub},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Searching for explanations of black-box classifiers in the
space of semantic queries. <em>SW</em>, <em>15</em>(4), 1085–1126. (<a
href="https://doi.org/10.3233/SW-233469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have achieved impressive performance in various tasks, but they are usually opaque with regards to their inner complex operation, obfuscating the reasons for which they make decisions. This opacity raises ethical and legal concerns regarding the real-life use of such models, especially in critical domains such as in medicine, and has led to the emergence of the eXplainable Artificial Intelligence (XAI) field of research, which aims to make the operation of opaque AI systems more comprehensible to humans. The problem of explaining a black-box classifier is often approached by feeding it data and observing its behaviour. In this work, we feed the classifier with data that are part of a knowledge graph, and describe the behaviour with rules that are expressed in the terminology of the knowledge graph, that is understandable by humans. We first theoretically investigate the problem to provide guarantees for the extracted rules and then we investigate the relation of “explanation rules for a specific class” with “semantic queries collecting from the knowledge graph the instances classified by the black-box classifier to this specific class”. Thus we approach the problem of extracting explanation rules as a semantic query reverse engineering problem. We develop algorithms for solving this inverse problem as a heuristic search in the space of semantic queries and we evaluate the proposed algorithms on four simulated use-cases and discuss the results.},
  archive      = {J_SW},
  author       = {Liartis, Jason and Dervakos, Edmund and Menis-Mastromichalakis, Orfeas and Chortaras, Alexandros and Stamou, Giorgos},
  doi          = {10.3233/SW-233469},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1085-1126},
  shortjournal = {Semantic Web},
  title        = {Searching for explanations of black-box classifiers in the space of semantic queries},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data journeys: Explaining AI workflows through abstraction.
<em>SW</em>, <em>15</em>(4), 1057–1083. (<a
href="https://doi.org/10.3233/SW-233407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence systems are not simply built on a single dataset or trained model. Instead, they are made by complex data science workflows involving multiple datasets, models, preparation scripts, and algorithms. Given this complexity, in order to understand these AI systems, we need to provide explanations of their functioning at higher levels of abstraction. To tackle this problem, we focus on the extraction and representation of data journeys from these workflows. A data journey is a multi-layered semantic representation of data processing activity linked to data science code and assets. We propose an ontology to capture the essential elements of a data journey and an approach to extract such data journeys. Using a corpus of Python notebooks from Kaggle, we show that we are able to capture high-level semantic data flow that is more compact than using the code structure itself. Furthermore, we show that introducing an intermediate knowledge graph representation outperforms models that rely only on the code itself. Finally, we report on a user survey to reflect on the challenges and opportunities presented by computational data journeys for explainable AI.},
  archive      = {J_SW},
  author       = {Daga, Enrico and Groth, Paul},
  doi          = {10.3233/SW-233407},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1057-1083},
  shortjournal = {Semantic Web},
  title        = {Data journeys: Explaining AI workflows through abstraction},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Separability and its approximations in ontology-based data
management. <em>SW</em>, <em>15</em>(4), 1021–1056. (<a
href="https://doi.org/10.3233/SW-233391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given two datasets, i.e., two sets of tuples of constants, representing positive and negative examples, logical separability is the reasoning task of finding a formula in a certain target query language that separates them. As already pointed out in previous works, this task turns out to be relevant in several application scenarios such as concept learning and generating referring expressions. Besides, if we think of the input datasets of positive and negative examples as composed of tuples of constants classified, respectively, positively and negatively by a black-box model, then the separating formula can be used to provide global post-hoc explanations of such a model. In this paper, we study the separability task in the context of Ontology-based Data Management (OBDM), in which a domain ontology provides a high-level, logic-based specification of a domain of interest, semantically linked through suitable mapping assertions to the data source layer of an information system. Since a formula that properly separates (proper separation) two input datasets does not always exist, our first contribution is to propose (best) approximations of the proper separation, called (minimally) complete and (maximally) sound separations. We do this by presenting a general framework for separability in OBDM. Then, in a scenario that uses by far the most popular languages for the OBDM paradigm, our second contribution is a comprehensive study of three natural computational problems associated with the framework, namely Verification (check whether a given formula is a proper, complete, or sound separation of two given datasets), Existence (check whether a proper, or best approximated separation of two given datasets exists at all), and Computation (compute any proper, or any best approximated separation of two given datasets).},
  archive      = {J_SW},
  author       = {Cima, Gianluca and Croce, Federico and Lenzerini, Maurizio},
  doi          = {10.3233/SW-233391},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {1021-1056},
  shortjournal = {Semantic Web},
  title        = {Separability and its approximations in ontology-based data management},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Engineering user-centered explanations to query answers in
ontology-driven socio-technical systems. <em>SW</em>, <em>15</em>(4),
991–1020. (<a href="https://doi.org/10.3233/SW-233297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of explanations in intelligent systems has in the last few years entered the spotlight as AI-based solutions appear in an ever-growing set of applications. Though data-driven (or machine learning) techniques are often used as examples of how opaque (also called black box) approaches can lead to problems such as bias and general lack of explainability and interpretability, in reality these features are difficult to tame in general, even for approaches that are based on tools typically considered to be more amenable, like knowledge-based formalisms. In this paper, we continue a line of research and development towards building tools that facilitate the implementation of explainable and interpretable hybrid intelligent socio-technical systems, focusing on features that users can leverage to build explanations to their queries. In particular, we present the implementation of a recently-proposed application framework (and make available its source code) for developing such systems, and explore user-centered mechanisms for building explanations based both on the kinds of explanations required (such as counterfactual, contextual, etc.) and the inputs used for building them (coming from various sources, such as the knowledge base and lower-level data-driven modules). In order to validate our approach, we develop two use cases, one as a running example for detecting hate speech in social platforms and the other as an extension that also contemplates cyberbullying scenarios.},
  archive      = {J_SW},
  author       = {Teze, Juan Carlos L. and Paredes, Jose Nicolas and Martinez, Maria Vanina and Simari, Gerardo Ignacio},
  doi          = {10.3233/SW-233297},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {991-1020},
  shortjournal = {Semantic Web},
  title        = {Engineering user-centered explanations to query answers in ontology-driven socio-technical systems},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explanation ontology: A general-purpose, semantic
representation for supporting user-centered explanations. <em>SW</em>,
<em>15</em>(4), 959–989. (<a
href="https://doi.org/10.3233/SW-233282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, trustworthy Artificial Intelligence (AI) has emerged as a focus for the AI community to ensure better adoption of AI models, and explainable AI is a cornerstone in this area. Over the years, the focus has shifted from building transparent AI methods to making recommendations on how to make black-box or opaque machine learning models and their results more understandable by experts and non-expert users. In our previous work, to address the goal of supporting user-centered explanations that make model recommendations more explainable, we developed an Explanation Ontology (EO). The EO is a general-purpose representation that was designed to help system designers connect explanations to their underlying data and knowledge. This paper addresses the apparent need for improved interoperability to support a wider range of use cases. We expand the EO, mainly in the system attributes contributing to explanations, by introducing new classes and properties to support a broader range of state-of-the-art explainer models. We present the expanded ontology model, highlighting the classes and properties that are important to model a larger set of fifteen literature-backed explanation types that are supported within the expanded EO. We build on these explanation type descriptions to show how to utilize the EO model to represent explanations in five use cases spanning the domains of finance, food, and healthcare. We include competency questions that evaluate the EO’s capabilities to provide guidance for system designers on how to apply our ontology to their own use cases. This guidance includes allowing system designers to query the EO directly and providing them exemplar queries to explore content in the EO represented use cases. We have released this significantly expanded version of the Explanation Ontology at https://purl.org/heals/eo and updated our resource website, https://tetherless-world.github.io/explanation-ontology , with supporting documentation. Overall, through the EO model, we aim to help system designers be better informed about explanations and support these explanations that can be composed, given their systems’ outputs from various AI models, including a mix of machine learning, logical and explainer models, and different types of data and knowledge available to their systems.},
  archive      = {J_SW},
  author       = {Chari, Shruthi and Seneviratne, Oshani and Ghalwash, Mohamed and Shirai, Sola and Gruen, Daniel M. and Meyer, Pablo and Chakraborty, Prithwish and McGuinness, Deborah L.},
  doi          = {10.3233/SW-233282},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {959-989},
  shortjournal = {Semantic Web},
  title        = {Explanation ontology: A general-purpose, semantic representation for supporting user-centered explanations},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable ontology extension in chemistry. <em>SW</em>,
<em>15</em>(4), 937–958. (<a
href="https://doi.org/10.3233/SW-233183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction and annotation enables them to maintain high quality, allowing them to be widely accepted across their community. However, the manual ontology development process does not scale for large domains. We present a new methodology for automatic ontology extension for domains in which the ontology classes have associated graph-structured annotations, and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We train Transformer-based deep learning models on the leaf node structures from the ChEBI ontology and the classes to which they belong. The models are then able to automatically classify previously unseen chemical structures, resulting in automated ontology extension. The proposed models achieved an overall F1 scores of 0.80 and above, improvements of at least 6 percentage points over our previous results on the same dataset. In addition, the models are interpretable: we illustrate that visualizing the model’s attention weights can help to explain the results by providing insight into how the model made its decisions. We also analyse the performance for molecules that have not been part of the ontology and evaluate the logical correctness of the resulting extension.},
  archive      = {J_SW},
  author       = {Glauer, Martin and Memariani, Adel and Neuhaus, Fabian and Mossakowski, Till and Hastings, Janna},
  doi          = {10.3233/SW-233183},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {937-958},
  shortjournal = {Semantic Web},
  title        = {Interpretable ontology extension in chemistry},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The role of ontologies and knowledge in explainable AI.
<em>SW</em>, <em>15</em>(4), 933–936. (<a
href="https://doi.org/10.3233/SW-243529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {Confalonieri, Roberto and Kutz, Oliver and Calvanese, Diego and Alonso-Moral, Jose Maria and Zhou, Shang-Ming},
  doi          = {10.3233/SW-243529},
  journal      = {Semantic Web},
  month        = {10},
  number       = {4},
  pages        = {933-936},
  shortjournal = {Semantic Web},
  title        = {The role of ontologies and knowledge in explainable AI},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CANARD: An approach for generating expressive
correspondences based on competency questions for alignment.
<em>SW</em>, <em>15</em>(3), 897–929. (<a
href="https://doi.org/10.3233/SW-233521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology matching aims at making ontologies interoperable. While the field has fully developed in the last years, most approaches are still limited to the generation of simple correspondences. More expressiveness is, however, required to better address the different kinds of ontology heterogeneities. This paper presents CANARD ( C omplex A lignment N eed and A -box based R elation D iscovery), an approach for generating expressive correspondences that rely on the notion of competency questions for alignment (CQA). A CQA expresses the user knowledge needs in terms of alignment and aims at reducing the alignment space. The approach takes as input a set of CQAs as SPARQL queries over the source ontology. The generation of correspondences is performed by matching the subgraph from the source CQA to the similar surroundings of the instances from the target ontology. Evaluation is carried out on both synthetic and real-world datasets. The impact of several approach parameters is discussed. Experiments have showed that CANARD performs, overall, better on CQA coverage than precision and that using existing same:As links, between the instances of the source and target ontologies, gives better results than exact label matches of their labels. The use of CQA improved also both CQA coverage and precision with respect to using automatically generated queries. The reassessment of the counter-example increased significantly the precision, to the detriment of runtime. Finally, experiments on large datasets showed that CANARD is one of the few systems that can perform on large knowledge bases, but depends on regularly populated knowledge bases and the quality of instance links.},
  archive      = {J_SW},
  author       = {Thiéblin, Elodie and Sousa, Guilherme and Haemmerlé, Ollivier and Trojahn, Cássia},
  doi          = {10.3233/SW-233521},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {897-929},
  shortjournal = {Semantic Web},
  title        = {CANARD: An approach for generating expressive correspondences based on competency questions for alignment},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A study of concept similarity in wikidata. <em>SW</em>,
<em>15</em>(3), 877–896. (<a
href="https://doi.org/10.3233/SW-233520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust estimation of concept similarity is crucial for applications of AI in the commercial, biomedical, and publishing domains, among others. While the related task of word similarity has been extensively studied, resulting in a wide range of methods, estimating concept similarity between nodes in Wikidata has not been considered so far. In light of the adoption of Wikidata for increasingly complex tasks that rely on similarity, and its unique size, breadth, and crowdsourcing nature, we propose that conceptual similarity should be revisited for the case of Wikidata. In this paper, we study a wide range of representative similarity methods for Wikidata, organized into three categories, and leverage background information for knowledge injection via retrofitting. We measure the impact of retrofitting with different weighted subsets from Wikidata and ProBase. Experiments on three benchmarks show that the best performance is achieved by pairing language models with rich information, whereas the impact of injecting knowledge is most positive on methods that originally do not consider comprehensive information. The performance of retrofitting is conditioned on the selection of high-quality similarity knowledge. A key limitation of this study, similar to prior work lies in the limited size and scope of the similarity benchmarks. While Wikidata provides an unprecedented possibility for a representative evaluation of concept similarity, effectively doing so remains a key challenge.},
  archive      = {J_SW},
  author       = {Ilievski, Filip and Shenoy, Kartik and Chalupsky, Hans and Klein, Nicholas and Szekely, Pedro},
  doi          = {10.3233/SW-233520},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {877-896},
  shortjournal = {Semantic Web},
  title        = {A study of concept similarity in wikidata},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The RDF2vec family of knowledge graph embedding methods.
<em>SW</em>, <em>15</em>(3), 845–876. (<a
href="https://doi.org/10.3233/SW-233514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embeddings represent a group of machine learning techniques which project entities and relations of a knowledge graph to continuous vector spaces. RDF2vec is a scalable embedding approach rooted in the combination of random walks with a language model. It has been successfully used in various applications. Recently, multiple variants to the RDF2vec approach have been proposed, introducing variations both on the walk generation and on the language modeling side. The combination of those different approaches has lead to an increasing family of RDF2vec variants. In this paper, we evaluate a total of twelve RDF2vec variants on a comprehensive set of benchmark models, and compare them to seven existing knowledge graph embedding methods from the family of link prediction approaches. Besides the established GEval benchmark introducing various downstream machine learning tasks on the DBpedia knowledge graph, we also use the new DLCC (Description Logic Class Constructors) benchmark consisting of two gold standards, one based on DBpedia, and one based on synthetically generated graphs. The latter allows for analyzing which ontological patterns in a knowledge graph can actually be learned by different embedding. With this evaluation, we observe that certain tailored RDF2vec variants can lead to improved performance on different downstream tasks, given the nature of the underlying problem, and that they, in particular, have a different behavior in modeling similarity and relatedness. The findings can be used to provide guidance in selecting a particular RDF2vec method for a given task.},
  archive      = {J_SW},
  author       = {Portisch, Jan and Paulheim, Heiko},
  doi          = {10.3233/SW-233514},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {845-876},
  shortjournal = {Semantic Web},
  title        = {The RDF2vec family of knowledge graph embedding methods},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LegalNERo: A linked corpus for named entity recognition in
the romanian legal domain. <em>SW</em>, <em>15</em>(3), 831–844. (<a
href="https://doi.org/10.3233/SW-233351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LegalNERo is a manually annotated corpus for named entity recognition in the Romanian legal domain. It provides gold annotations for organizations, locations, persons, time expressions and legal resources mentioned in legal documents. Furthermore, GeoNames identifiers are provided. The resource is available in multiple formats, including span-based, token-based and RDF. The Linked Open Data version is available for both download and querying using SPARQL.},
  archive      = {J_SW},
  author       = {Păis, Vasile and Mitrofan, Maria and Gasan, Carol Luca and Ianov, Alexandru and Ghit,ă, Corvin and Coneschi, Vlad Silviu and Onut, Andrei},
  doi          = {10.3233/SW-233351},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {831-844},
  shortjournal = {Semantic Web},
  title        = {LegalNERo: A linked corpus for named entity recognition in the romanian legal domain},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental schema integration for data wrangling via
knowledge graphs. <em>SW</em>, <em>15</em>(3), 793–830. (<a
href="https://doi.org/10.3233/SW-233347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual data integration is the current approach to go for data wrangling in data-driven decision-making. In this paper, we focus on automating schema integration, which extracts a homogenised representation of the data source schemata and integrates them into a global schema to enable virtual data integration. Schema integration requires a set of well-known constructs: the data source schemata and wrappers, a global integrated schema and the mappings between them. Based on them, virtual data integration systems enable fast and on-demand data exploration via query rewriting. Unfortunately, the generation of such constructs is currently performed in a largely manual manner, hindering its feasibility in real scenarios. This becomes aggravated when dealing with heterogeneous and evolving data sources. To overcome these issues, we propose a fully-fledged semi-automatic and incremental approach grounded on knowledge graphs to generate the required schema integration constructs in four main steps: bootstrapping, schema matching, schema integration, and generation of system-specific constructs. We also present Nextia DI , a tool implementing our approach. Finally, a comprehensive evaluation is presented to scrutinize our approach.},
  archive      = {J_SW},
  author       = {Flores, Javier and Rabbani, Kashif and Nadal, Sergi and Gómez, Cristina and Romero, Oscar and Jamin, Emmanuel and Dasiopoulou, Stamatia},
  doi          = {10.3233/SW-233347},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {793-830},
  shortjournal = {Semantic Web},
  title        = {Incremental schema integration for data wrangling via knowledge graphs},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural axiom network for knowledge graph reasoning.
<em>SW</em>, <em>15</em>(3), 777–792. (<a
href="https://doi.org/10.3233/SW-233276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph reasoning (KGR) aims to infer new knowledge or detect noises, which is essential for improving the quality of knowledge graphs. Recently, various KGR techniques, such as symbolic- and embedding-based methods, have been proposed and shown strong reasoning ability. Symbolic-based reasoning methods infer missing triples according to predefined rules or ontologies. Although rules and axioms have proven effective, it is difficult to obtain them. Embedding-based reasoning methods represent entities and relations as vectors, and complete KGs via vector computation. However, they mainly rely on structural information and ignore implicit axiom information not predefined in KGs but can be reflected in data. That is, each correct triple is also a logically consistent triple and satisfies all axioms. In this paper, we propose a novel NeuR al A xiom N etwork ( NeuRAN ) framework that combines explicit structural and implicit axiom information without introducing additional ontologies. Specifically, the framework consists of a KG embedding module that preserves the semantics of triples and five axiom modules that encode five kinds of implicit axioms. These axioms correspond to five typical object property expression axioms defined in OWL2, including ObjectPropertyDomain , ObjectPropertyRange , DisjointObjectProperties , IrreflexiveObjectProperty and AsymmetricObjectProperty . The KG embedding module and axiom modules compute the scores that the triple conforms to the semantics and the corresponding axioms, respectively. Compared with KG embedding models and CKRL, our method achieves comparable performance on noise detection and triple classification and achieves significant performance on link prediction. Compared with TransE and TransH, our method improves the link prediction performance on the Hits@1 metric by 22.0% and 20.8% on WN18RR-10% dataset, respectively.},
  archive      = {J_SW},
  author       = {Li, Juan and Chen, Xiangnan and Yu, Hongtao and Chen, Jiaoyan and Zhang, Wen},
  doi          = {10.3233/SW-233276},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {777-792},
  shortjournal = {Semantic Web},
  title        = {Neural axiom network for knowledge graph reasoning},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential privacy and SPARQL. <em>SW</em>,
<em>15</em>(3), 745–773. (<a
href="https://doi.org/10.3233/SW-233474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy is a framework that provides formal tools to develop algorithms to access databases and answer statistical queries with quantifiable accuracy and privacy guarantees. The notions of differential privacy are defined independently of the data model and the query language at steak. Most differential privacy results have been obtained on aggregation queries such as counting or finding maximum or average values, and on grouping queries over aggregations such as the creation of histograms. So far, the data model used by the framework research has typically been the relational model and the query language SQL. However, effective realizations of differential privacy for SQL queries that required joins had been limited. This has imposed severe restrictions on applying differential privacy in RDF knowledge graphs and SPARQL queries. By the simple nature of RDF data, most useful queries accessing RDF graphs will require intensive use of joins. Recently, new differential privacy techniques have been developed that can be applied to many types of joins in SQL with reasonable results. This opened the question of whether these new results carry over to RDF and SPARQL. In this paper we provide a positive answer to this question by presenting an algorithm that can answer counting queries over a large class of SPARQL queries that guarantees differential privacy, if the RDF graph is accompanied with semantic information about its structure. We have implemented our algorithm and conducted several experiments, showing the feasibility of our approach for large graph databases. Our aim has been to present an approach that can be used as a stepping stone towards extensions and other realizations of differential privacy for SPARQL and RDF.},
  archive      = {J_SW},
  author       = {Buil-Aranda, Carlos and Lobo, Jorge and Olmedo, Federico},
  doi          = {10.3233/SW-233474},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {745-773},
  shortjournal = {Semantic Web},
  title        = {Differential privacy and SPARQL},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of ontologies and policy languages to represent
information flows in GDPR. <em>SW</em>, <em>15</em>(3), 709–743. (<a
href="https://doi.org/10.3233/SW-223009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article surveys existing vocabularies, ontologies and policy languages that can be used to represent informational items referenced in GDPR rights and obligations, such as the ‘notification of a data breach’, the ‘controller’s identity’ or a ‘DPIA’. Rights and obligations in GDPR are analyzed in terms of information flows between different stakeholders, and a complete collection of 57 different informational items that are mentioned by GDPR is described. 13 privacy-related policy languages and 9 data protection vocabularies and ontologies are studied in relation to this list of informational items. ODRL and LegalRuleML emerge as the languages that can respond positively to a greater number of the defined comparison criteria if complemented with DPV and GDPRtEXT, since 39 out of the 57 informational items can be modelled. Online supplementary material is provided, including a simple search application and a taxonomy of the identified entities.},
  archive      = {J_SW},
  author       = {Esteves, Beatriz and Rodríguez-Doncel, Víctor},
  doi          = {10.3233/SW-223009},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {709-743},
  shortjournal = {Semantic Web},
  title        = {Analysis of ontologies and policy languages to represent information flows in GDPR},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic-enabled architecture for auditable
privacy-preserving data analysis. <em>SW</em>, <em>15</em>(3), 675–708.
(<a href="https://doi.org/10.3233/SW-212883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small and medium-sized organisations face challenges in acquiring, storing and analysing personal data, particularly sensitive data (e.g., data of medical nature), due to data protection regulations, such as the GDPR in the EU, which stipulates high standards in data protection. Consequently, these organisations often refrain from collecting data centrally, which means losing the potential of data analytics and learning from aggregated user data. To enable organisations to leverage the full-potential of the collected personal data, two main technical challenges need to be addressed: (i) organisations must preserve the privacy of individual users and honour their consent, while (ii) being able to provide data and algorithmic governance, e.g., in the form of audit trails, to increase trust in the result and support reproducibility of the data analysis tasks performed on the collected data. Such an auditable, privacy-preserving data analysis is currently challenging to achieve, as existing methods and tools only offer partial solutions to this problem, e.g., data representation of audit trails and user consent, automatic checking of usage policies or data anonymisation. To the best of our knowledge, there exists no approach providing an integrated architecture for auditable, privacy-preserving data analysis. To address these gaps, as the main contribution of this paper, we propose the WellFort approach, a semantic-enabled architecture for auditable, privacy-preserving data analysis which provides secure storage for users’ sensitive data with explicit consent, and delivers a trusted, auditable analysis environment for executing data analytic processes in a privacy-preserving manner. Additional contributions include the adaptation of Semantic Web technologies as an integral part of the WellFort architecture, and the demonstration of the approach through a feasibility study with a prototype supporting use cases from the medical domain. Our evaluation shows that WellFort enables privacy preserving analysis of data, and collects sufficient information in an automated way to support its auditability at the same time.},
  archive      = {J_SW},
  author       = {Ekaputra, Fajar J. and Ekelhart, Andreas and Mayer, Rudolf and Miksa, Tomasz and Šarčević, Tanja and Tsepelakis, Sotirios and Waltersdorfer, Laura},
  doi          = {10.3233/SW-212883},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {675-708},
  shortjournal = {Semantic Web},
  title        = {Semantic-enabled architecture for auditable privacy-preserving data analysis},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consent through the lens of semantics: State of the art
survey and best practices. <em>SW</em>, <em>15</em>(3), 647–673. (<a
href="https://doi.org/10.3233/SW-210438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The acceptance of the GDPR legislation in 2018 started a new technological shift towards achieving transparency. GDPR put focus on the concept of informed consent applicable for data processing, which led to an increase of the responsibilities regarding data sharing for both end users and companies. This paper presents a literature survey of existing solutions that use semantic technology for implementing consent. The main focus is on ontologies, how they are used for consent representation and for consent management in combination with other technologies such as blockchain. We also focus on visualisation solutions aimed at improving individuals’ consent comprehension. Finally, based on the overviewed state of the art we propose best practices for consent implementation.},
  archive      = {J_SW},
  author       = {Kurteva, Anelia and Chhetri, Tek Raj and Pandit, Harshvardhan J. and Fensel, Anna},
  doi          = {10.3233/SW-210438},
  journal      = {Semantic Web},
  month        = {5},
  number       = {3},
  pages        = {647-673},
  shortjournal = {Semantic Web},
  title        = {Consent through the lens of semantics: State&amp;nbsp;of the art survey and best practices},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ontology of 3D environment where a simulated manipulation
task takes place (ENVON). <em>SW</em>, <em>15</em>(2), 613–640. (<a
href="https://doi.org/10.3233/SW-233460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the advent of robotics in shopfloor and warehouse environments, control rooms need to seamlessly exchange information regarding the dynamically changing 3D environment to facilitate tasks and path planning for the robots. Adding to the complexity, this type of environment is heterogeneous as it includes both free space and various types of rigid bodies (equipment, materials, humans etc.). At the same time, 3D environment-related information is also required by the virtual applications (e.g., VR techniques) for the behavioral study of CAD-based product models or simulation of CNC operations. In past research, information models for such heterogeneous 3D environments are often built without ensuring connection among different levels of abstractions required for different applications. For addressing such multiple points of view and modelling requirements for 3D objects and environments, this paper proposes an ontology model that integrates the contextual, topologic, and geometric information of both the rigid bodies and the free space. The ontology provides an evolvable knowledge model that can support simulated task-related information in general. This ontology aims to greatly improve interoperability as a path planning system (e.g., robot) and will be able to deal with different applications by simply updating the contextual semantics related to some targeted application while keeping the geometric and topological models intact by leveraging the semantic link among the models.},
  archive      = {J_SW},
  author       = {Zhao, Yingshen and Sarkar, Arkopaul and Elmhadhbi, Linda and Karray, Mohamed Hedi and Fillatreau, Philippe and Archimède, Bernard},
  doi          = {10.3233/SW-233460},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {613-640},
  shortjournal = {Semantic Web},
  title        = {An ontology of 3D environment where a simulated manipulation task takes place (ENVON)},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semantic framework for condition monitoring in industry
4.0 based on evolving knowledge bases. <em>SW</em>, <em>15</em>(2),
583–611. (<a href="https://doi.org/10.3233/SW-233481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Industry 4.0, factory assets and machines are equipped with sensors that collect data for effective condition monitoring. This is a difficult task since it requires the integration and processing of heterogeneous data from different sources, with different temporal resolutions and underlying meanings. Ontologies have emerged as a pertinent method to deal with data integration and to represent manufacturing knowledge in a machine-interpretable way through the construction of semantic models. Ontologies are used to structure knowledge in knowledge bases, which also contain instances and information about these data. Thus, a knowledge base provides a sort of virtual representation of the different elements involved in a manufacturing process. Moreover, the monitoring of industrial processes depends on the dynamic context of their execution. Under these circumstances, the semantic model must provide a way to represent this evolution in order to represent in which situation(s) a resource is in during the execution of its tasks to support decision making. This paper proposes a semantic framework to address the evolution of knowledge bases for condition monitoring in Industry 4.0. To this end, firstly we propose a semantic model (the COInd4 ontology) for the manufacturing domain that represents the resources and processes that are part of a factory, with special emphasis on the context of these resources and processes. Relevant situations that combine sensor observations with domain knowledge are also represented in the model. Secondly, an approach that uses stream reasoning to detect these situations that lead to potential failures is introduced. This approach enriches data collected from sensors with contextual information using the proposed semantic model. The use of stream reasoning facilitates the integration of data from different data sources, different temporal resolutions as well as the processing of these data in real time. This allows to derive high-level situations from lower-level context and sensor information. Detecting situations can trigger actions to adapt the process behavior, and in turn, this change in behavior can lead to the generation of new contexts leading to new situations. These situations can have different levels of severity, and can be nested in different ways. Dealing with the rich relations among situations requires an efficient approach to organize them. Therefore, we propose a method to build a lattice, ordering those situations depending on the constraints they rely on. This lattice represents a road-map of all the situations that can be reached from a given one, normal or abnormal. This helps in decision support, by allowing the identification of the actions that can be taken to correct the abnormality avoiding in this way the interruption of the manufacturing processes. Finally, an industrial application scenario for the proposed approach is described.},
  archive      = {J_SW},
  author       = {Giustozzi, Franco and Saunier, Julien and Zanni-Merk, Cecilia},
  doi          = {10.3233/SW-233481},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {583-611},
  shortjournal = {Semantic Web},
  title        = {A semantic framework for condition monitoring in industry 4.0 based on evolving&amp;nbsp;knowledge bases},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LinkedDataOps: Quality oriented end-to-end geospatial linked
data production governance. <em>SW</em>, <em>15</em>(2), 555–581. (<a
href="https://doi.org/10.3233/SW-233293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work describes the application of semantic web standards to data quality governance of data production pipelines in the architectural, engineering, and construction (AEC) domain for Ordnance Survey Ireland (OSi). It illustrates a new approach to data quality governance based on establishing a unified knowledge graph for data quality measurements across a complex, heterogeneous, quality-centric data production pipeline. It provides the first comprehensive formal mappings between semantic models of data quality dimensions defined by the four International Organization for Standardization (ISO) and World Wide Web Consortium (W3C) data quality standards applied by different tools and stakeholders. It provides an approach to uplift rule-based data quality reports into quality metrics suitable for aggregation and end-to-end analysis. Current industrial practice tends towards stove-piped, vendor-specific and domain-dependent tools to process data quality observations however there is a lack of open techniques and methodologies for combining quality measurements derived from different data quality standards to provide end-to-end data quality reporting, root cause analysis or visualisation. This work demonstrated that it is effective to use a knowledge graph and semantic web standards to unify distributed data quality monitoring in an organisation and present the results in an end-to-end data dashboard in a data quality standards-agnostic fashion for the Ordnance Survey Ireland data publishing pipeline.},
  archive      = {J_SW},
  author       = {Yaman, Beyza and Thompson, Kevin and Fahey, Fergus and Brennan, Rob},
  doi          = {10.3233/SW-233293},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {555-581},
  shortjournal = {Semantic Web},
  title        = {LinkedDataOps: Quality oriented end-to-end geospatial linked data production governance},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deriving semantic validation rules from industrial
standards: An OPC UA study. <em>SW</em>, <em>15</em>(2), 517–554. (<a
href="https://doi.org/10.3233/SW-233342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial standards provide guidelines for data modeling to ensure interoperability between stakeholders of an industry branch (e.g., robotics). Most frequently, such guidelines are provided in an unstructured format (e.g., pdf documents) which hampers the automated validations of information objects (e.g., data models) that rely on such standards in terms of their compliance with the modeling constraints prescribed by the guidelines. This raises the risk of costly interoperability errors induced by the incorrect use of the standards. There is, therefore, an increased interest in automatic semantic validation of information objects based on industrial standards. In this paper we focus on an approach to semantic validation by formally representing the modeling constraints from unstructured documents as explicit, machine-actionable rules (to be then used for semantic validation) and (semi-)automatically extracting such rules from pdf documents. While our approach aims to be generically applicable, we exemplify an adaptation of the approach in the concrete context of the OPC UA industrial standard, given its large-scale adoption among important industrial stakeholders and the OPC UA internal efforts towards semantic validation. We conclude that (i) it is feasible to represent modeling constraints from the standard specifications as rules, which can be organized in a taxonomy and represented using Semantic Web technologies such as OWL and SPARQL; (ii) we could automatically identify modeling constraints in the specification documents by inspecting the tables ( P = 87 % ) and text of these documents (F1 up to 94%); (iii) the translation of the modeling constraints into formal rules could be fully automated when constraints were extracted from tables and required a Human-in-the-loop approach for constraints extracted from text.},
  archive      = {J_SW},
  author       = {Bareedu, Yashoda Saisree and Frühwirth, Thomas and Niedermeier, Christoph and Sabou, Marta and Steindl, Gernot and Thuluva, Aparna Saisree and Tsaneva, Stefani and Tufek Ozkaya, Nilay},
  doi          = {10.3233/SW-233342},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {517-554},
  shortjournal = {Semantic Web},
  title        = {Deriving semantic validation rules from industrial standards: An OPC UA study},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The materials design ontology. <em>SW</em>, <em>15</em>(2),
481–515. (<a href="https://doi.org/10.3233/SW-233340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the materials design domain, much of the data from materials calculations is stored in different heterogeneous databases with different data and access models. Therefore, accessing and integrating data from different sources is challenging. As ontology-based access and integration alleviates these issues, in this paper we address data access and interoperability for computational materials databases by developing the Materials Design Ontology. This ontology is inspired by and guided by the OPTIMADE effort that aims to make materials databases interoperable and includes many of the data providers in computational materials science. In this paper, first, we describe the development and the content of the Materials Design Ontology. Then, we use a topic model-based approach to propose additional candidate concepts for the ontology. Finally, we show the use of the Materials Design Ontology by a proof-of-concept implementation of a data access and integration system for materials databases based on the ontology. 1 1},
  archive      = {J_SW},
  author       = {Lambrix, Patrick and Armiento, Rickard and Li, Huanyu and Hartig, Olaf and Abd Nikooie Pour, Mina and Li, Ying},
  doi          = {10.3233/SW-233340},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {481-515},
  shortjournal = {Semantic Web},
  title        = {The materials design ontology},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A benchmark dataset with knowledge graph generation for
industry 4.0 production lines. <em>SW</em>, <em>15</em>(2), 461–479. (<a
href="https://doi.org/10.3233/SW-233431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industry 4.0 (I4.0) is a new era in the industrial revolution that emphasizes machine connectivity, automation, and data analytics. The I4.0 pillars such as autonomous robots, cloud computing, horizontal and vertical system integration, and the industrial internet of things have increased the performance and efficiency of production lines in the manufacturing industry. Over the past years, efforts have been made to propose semantic models to represent the manufacturing domain knowledge, one such model is Reference Generalized Ontological Model (RGOM). 1 1 However, its adaptability like other models is not ensured due to the lack of manufacturing data. In this paper, we aim to develop a benchmark dataset for knowledge graph generation in Industry 4.0 production lines and to show the benefits of using ontologies and semantic annotations of data to showcase how the I4.0 industry can benefit from KGs and semantic datasets. This work is the result of collaboration with the production line managers, supervisors, and engineers in the football industry to acquire realistic production line data 2 2 , . 3 3 Knowledge Graphs (KGs) or Knowledge Graph (KG) have emerged as a significant technology to store the semantics of the domain entities. KGs have been used in a variety of industries, including banking, the automobile industry, oil and gas, pharmaceutical and health care, publishing, media, etc. The data is mapped and populated to the RGOM classes and relationships using an automated solution based on JenaAPI, producing an I4.0 KG. It contains more than 2.5 million axioms and about 1 million instances. This KG enables us to demonstrate the adaptability and usefulness of the RGOM. Our research helps the production line staff to take timely decisions by exploiting the information embedded in the KG. In relation to this, the RGOM adaptability is demonstrated with the help of a use case scenario to discover required information such as current temperature at a particular time, the status of the motor, tools deployed on the machine, etc.},
  archive      = {J_SW},
  author       = {Yahya, Muhammad and Ali, Aabid and Mehmood, Qaiser and Yang, Lan and Breslin, John G. and Ali, Muhammad Intizar},
  doi          = {10.3233/SW-233431},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {461-479},
  shortjournal = {Semantic Web},
  title        = {A benchmark dataset with knowledge graph generation for industry 4.0 production lines},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ConSolid: A federated ecosystem for heterogeneous
multi-stakeholder projects. <em>SW</em>, <em>15</em>(2), 429–460. (<a
href="https://doi.org/10.3233/SW-233396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many industries, multiple parties collaborate on a larger project. At the same time, each of those stakeholders participates in multiple independent projects simultaneously. A double patchwork can thus be identified, with a many-to-many relationship between actors and collaborative projects. One key example is the construction industry, where every project is unique, involving specialists for many subdomains, ranging from the architectural design over technical installations to geospatial information, governmental regulation and sometimes even historical research. A digital representation of this process and its outcomes requires semantic interoperability between these subdomains, which however often work with heterogeneous and unstructured data. In this paper we propose to address this double patchwork via a decentralized ecosystem for multi-stakeholder, multi-industry collaborations dealing with heterogeneous information snippets. At its core, this ecosystem, called ConSolid, builds upon the Solid specifications for Web decentralization, but extends these both on a (meta)data pattern level and on microservice level. To increase the robustness of data allocation and filtering, we identify the need to go beyond Solid’s current LDP-inspired interfaces to a Solid Pod and introduce the concept of metadata-generated ‘virtual views’, to be generated using an access-controlled SPARQL interface to a Pod. A recursive, scalable way to discover multi-vault aggregations is proposed, along with data patterns for connecting and aligning heterogeneous (RDF and non-RDF) resources across vaults in a mediatype-agnostic fashion. We demonstrate the use and benefits of the ecosystem using minimal running examples, concluding with the setup of an example use case from the Architecture, Engineering, Construction and Operations (AECO) industry.},
  archive      = {J_SW},
  author       = {Werbrouck, Jeroen and Pauwels, Pieter and Beetz, Jakob and Verborgh, Ruben and Mannens, Erik},
  doi          = {10.3233/SW-233396},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {429-460},
  shortjournal = {Semantic Web},
  title        = {ConSolid: A federated ecosystem for heterogeneous multi-stakeholder projects},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing awareness of industrial robots in collaborative
manufacturing. <em>SW</em>, <em>15</em>(2), 389–428. (<a
href="https://doi.org/10.3233/SW-233394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diffusion of Human-Robot Collaborative cells is prevented by several barriers. Classical control approaches seem not yet fully suitable for facing the variability conveyed by the presence of human operators beside robots. The capabilities of representing heterogeneous knowledge representation and performing abstract reasoning are crucial to enhance the flexibility of control solutions. To this aim, the ontology SOHO (Sharework Ontology for Human-Robot Collaboration) has been specifically designed for representing Human-Robot Collaboration scenarios, following a context-based approach. This work brings several contributions. This paper proposes an extension of SOHO to better characterize behavioral constraints of collaborative tasks. Furthermore, this work shows a knowledge extraction procedure designed to automatize the synthesis of Artificial Intelligence plan-based controllers for realizing flexible coordination of human and robot behaviors in collaborative tasks. The generality of the ontological model and the developed representation capabilities as well as the validity of the synthesized planning domains are evaluated on a number of realistic industrial scenarios where collaborative robots are actually deployed.},
  archive      = {J_SW},
  author       = {Umbrico, Alessandro and Cesta, Amedeo and Orlandini, Andrea},
  doi          = {10.3233/SW-233394},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {389-428},
  shortjournal = {Semantic Web},
  title        = {Enhancing awareness of industrial robots in collaborative manufacturing},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic system models and their simulation in the semantic
web. <em>SW</em>, <em>15</em>(2), 353–388. (<a
href="https://doi.org/10.3233/SW-233359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling and Simulation (M&amp;S) are core tools for designing, analysing and operating today’s industrial systems. They often also represent both a valuable asset and a significant investment. Typically, their use is constrained to a software environment intended to be used by engineers on a single computer. However, the knowledge relevant to a task involving modelling and simulation is in general distributed in nature, even across organizational boundaries, and may be large in volume. Therefore, it is desirable to increase the FAIRness (Findability, Accessibility, Interoperability, and Reuse) of M&amp;S capabilities; to enable their use in loosely coupled systems of systems; and to support their composition and execution by intelligent software agents. In this contribution, the suitability of Semantic Web technologies to achieve these goals is investigated and an open-source proof of concept-implementation based on the Functional Mock-up Interface (FMI) standard is presented. Specifically, models, model instances, and simulation results are exposed through a hypermedia API and an implementation of the Pragmatic Proof Algorithm (PPA) is used to successfully demonstrate the API’s use by a generic software agent. The solution shows an increased degree of FAIRness and fully supports its use in loosely coupled systems. The FAIRness could be further improved by providing more “ rich” (meta)data.},
  archive      = {J_SW},
  author       = {Stüber, Moritz and Frey, Georg},
  doi          = {10.3233/SW-233359},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {353-388},
  shortjournal = {Semantic Web},
  title        = {Dynamic system models and their simulation in the semantic web},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ontology for maintenance activities and its application
to data quality. <em>SW</em>, <em>15</em>(2), 319–352. (<a
href="https://doi.org/10.3233/SW-233299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintenance of assets is a multi-million dollar cost each year for asset intensive organisations in the defence, manufacturing, resource and infrastructure sectors. These costs are tracked though maintenance work order (MWO) records. MWO records contain structured data for dates, costs, and asset identification and unstructured text describing the work required, for example ‘replace leaking pump’. Our focus in this paper is on data quality for maintenance activity terms in MWO records (e.g. replace , repair , adjust and inspect ). We present two contributions in this paper. First, we propose a reference ontology for maintenance activity terms. We use natural language processing to identify seven core maintenance activity terms and their synonyms from 800,000 MWOs. We provide elucidations for these seven terms. Second, we demonstrate use of the reference ontology in an application-level ontology using an industrial use case. The end-to-end NLP-ontology pipeline identifies data quality issues with 55% of the MWO records for a centrifugal pump over 8 years. For the 33% of records where a verb was not provided in the unstructured text, the ontology can infer a relevant activity class. The selection of the maintenance activity terms is informed by the ISO 14224 and ISO 15926-4 standards and conforms to ISO/IEC 21838-2 Basic Formal Ontology (BFO). The reference and application ontologies presented here provide an example for how industrial organisations can augment their maintenance work management processes with ontological workflows to improve data quality.},
  archive      = {J_SW},
  author       = {Woods, Caitlin and Selway, Matt and Bikaun, Tyler and Stumptner, Markus and Hodkiewicz, Melinda},
  doi          = {10.3233/SW-233299},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {319-352},
  shortjournal = {Semantic Web},
  title        = {An ontology for maintenance activities and its application to data quality},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a formal ontology of engineering functions,
behaviours, and capabilities. <em>SW</em>, <em>15</em>(2), 285–318. (<a
href="https://doi.org/10.3233/SW-223188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In both applied ontology and engineering, functionality is a well-researched topic, since it is through teleological causal reasoning that domain experts build mental models of engineering systems, giving birth to functions. These mental models are important throughout the whole lifecycle of any product, being used from the design phase up to diagnosis activities. Though a vast amount of work to model functions has already been carried out, the literature has not settled on a shared and well-defined approach due to the variety of concepts involved and the modeling tasks that functional descriptions should satisfy. The work in this paper posits the basis and makes some crucial steps towards a rich ontological description of functions and related concepts, such as behaviour, capability, and capacity. A conceptual analysis of such notions is carried out using the top-level ontology DOLCE as a framework, and the ensuing logical theory is formally described in first-order logic and OWL, showing how ontological concepts can model major aspects of engineering products in applications. In particular, it is shown how functions can be distinguished from the implementation methods to realize them, how one can differentiate between capabilities and capacities of a product, and how these are related to engineering functions.},
  archive      = {J_SW},
  author       = {Compagno, Francesco and Borgo, Stefano},
  doi          = {10.3233/SW-223188},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {285-318},
  shortjournal = {Semantic Web},
  title        = {Towards a formal ontology of engineering functions, behaviours, and capabilities},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Creating occupant-centered digital twins using the occupant
feedback ontology implemented in a smartwatch app. <em>SW</em>,
<em>15</em>(2), 259–284. (<a
href="https://doi.org/10.3233/SW-223254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occupant feedback enables building managers to improve occupants’ health, comfort, and satisfaction. However, acquiring continuous occupant feedback and integrating this feedback with other building information is challenging. This paper presents a scalable method to acquire continuous occupant feedback and directly integrate this with other building information. Semantic web technologies were applied to solve data interoperability issues. The Occupant Feedback Ontology was developed to describe feedback semantically. Next to this, a smartwatch app – Mintal – was developed to acquire continuous feedback on indoor environmental quality. The app gathers location, medical information, and answers on short micro surveys. Mintal applied the Occupant Feedback Ontology to directly integrate the feedback with linked building data. A case study was performed to evaluate this method. A semantic digital twin was created by integrating linked building data, sensor data, and occupant feedback. Results from SPARQL queries gave more insight into an occupant’s perceived comfort levels in the Open Flat. The case study shows how integrating feedback with building information allows for more occupant-centric decision support tools. The approach presented in this paper can be used in a wide range of use cases, both within and without the architecture, building, and construction domain.},
  archive      = {J_SW},
  author       = {Donkers, Alex and de Vries, Bauke and Yang, Dujuan},
  doi          = {10.3233/SW-223254},
  journal      = {Semantic Web},
  month        = {4},
  number       = {2},
  pages        = {259-284},
  shortjournal = {Semantic Web},
  title        = {Creating occupant-centered digital twins using the occupant feedback ontology implemented in a smartwatch app},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Helio: A framework for implementing the life cycle of
knowledge graphs. <em>SW</em>, <em>15</em>(1), 223–249. (<a
href="https://doi.org/10.3233/SW-233224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building and publishing knowledge graphs (KG) as Linked Data, either on the Web or in private companies, has become a relevant and crucial process in many domains. This process requires that users perform a wide number of tasks conforming to the life cycle of a KG, and these tasks usually involve different unrelated research topics, such as RDF materialisation or link discovery. There is already a large corpus of tools and methods designed to perform these tasks; however, the lack of one tool that gathers them all leads practitioners to develop ad-hoc pipelines that are not generic and, thus, non-reusable. As a result, building and publishing a KG is becoming a complex and resource-consuming process. In this paper, a generic framework called Helio is presented. The framework aims to cover a set of requirements elicited from the KG life cycle and provide a tool capable of performing the different tasks required to build and publish KGs. As a result, Helio aims at providing users with the means for reducing the effort required to perform this process and, also, Helio aims to prevent the development of ad-hoc pipelines. Furthermore, the Helio framework has been applied in many different contexts, from European projects to research work.},
  archive      = {J_SW},
  author       = {Cimmino, Andrea and García-Castro, Raúl},
  doi          = {10.3233/SW-233224},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {223-249},
  shortjournal = {Semantic Web},
  title        = {Helio: A framework for implementing the life cycle of knowledge graphs},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ontological approach for representing declarative mapping
languages. <em>SW</em>, <em>15</em>(1), 191–221. (<a
href="https://doi.org/10.3233/SW-223224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs are currently created using an assortment of techniques and tools: ad hoc code in a programming language, database export scripts, OpenRefine transformations, mapping languages, etc. Focusing on the latter, the wide variety of use cases, data peculiarities, and potential uses has had a substantial impact in how mappings have been created, extended, and applied. As a result, a large number of languages and their associated tools have been created. In this paper, we present the Conceptual Mapping ontology, that is designed to represent the features and characteristics of existing declarative mapping languages to construct Knowledge Graphs. This ontology is built upon the requirements extracted from experts experience, a thorough analysis of the features and capabilities of current mapping languages presented as a comparative framework; and the languages’ limitations discussed by the community and denoted as Mapping Challenges. The ontology is evaluated to ensure that it meets these requirements and has no inconsistencies, pitfalls or modelling errors, and is publicly available online along with its documentation and related resources.},
  archive      = {J_SW},
  author       = {Iglesias-Molina, Ana and Cimmino, Andrea and Ruckhaus, Edna and Chaves-Fraga, David and García-Castro, Raúl and Corcho, Oscar},
  doi          = {10.3233/SW-223224},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {191-221},
  shortjournal = {Semantic Web},
  title        = {An ontological approach for representing declarative mapping languages},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSQ 2.0: A linked dataset of SPARQL query logs. <em>SW</em>,
<em>15</em>(1), 167–189. (<a
href="https://doi.org/10.3233/SW-223015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the Linked SPARQL Queries (LSQ) dataset, which currently describes 43.95 million executions of 11.56 million unique SPARQL queries extracted from the logs of 27 different endpoints. The LSQ dataset provides RDF descriptions of each such query, which are indexed in a public LSQ endpoint, allowing interested parties to find queries with the characteristics they require. We begin by describing the use cases envisaged for the LSQ dataset, which include applications for research on common features of queries, for building custom benchmarks, and for designing user interfaces. We then discuss how LSQ has been used in practice since the release of four initial SPARQL logs in 2015. We discuss the model and vocabulary that we use to represent these queries in RDF. We then provide a brief overview of the 27 endpoints from which we extracted queries in terms of the domain to which they pertain and the data they contain. We provide statistics on the queries included from each log, including the number of query executions, unique queries, as well as distributions of queries for a variety of selected characteristics. We finally discuss how the LSQ dataset is hosted and how it can be accessed and leveraged by interested parties for their use cases.},
  archive      = {J_SW},
  author       = {Stadler, Claus and Saleem, Muhammad and Mehmood, Qaiser and Buil-Aranda, Carlos and Dumontier, Michel and Hogan, Aidan and Ngonga Ngomo, Axel-Cyrille},
  doi          = {10.3233/SW-223015},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {167-189},
  shortjournal = {Semantic Web},
  title        = {LSQ 2.0: A linked dataset of SPARQL query logs},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic overview of data federation systems.
<em>SW</em>, <em>15</em>(1), 107–165. (<a
href="https://doi.org/10.3233/SW-223201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data federation addresses the problem of uniformly accessing multiple, possibly heterogeneous data sources, by mapping them into a unified schema, such as an RDF(S)/OWL ontology or a relational schema, and by supporting the execution of queries, like SPARQL or SQL queries, over that unified schema. Data explosion in volume and variety has made data federation increasingly popular in many application domains. Hence, many data federation systems have been developed in industry and academia, and it has become challenging for users to select suitable systems to achieve their objectives. In order to systematically analyze and compare these systems, we propose an evaluation framework comprising four dimensions: (i) federation capabilities , i.e., query language, data source, and federation techniques; (ii) data security , i.e., authentication, authorization, auditing, encryption, and data masking; (iii) interface , i.e., graphical interface, command line interface, and application programming interface; and (iv) development , i.e., main development language, deployment, commercial support, open source, and release. Using this framework, we thoroughly studied 51 data federation systems from the Semantic Web and Database communities. This paper shares the results of our investigation and aims to provide reference material and insights for users, developers and researchers selecting or further developing data federation systems.},
  archive      = {J_SW},
  author       = {Gu, Zhenzhen and Corcoglioniti, Francesco and Lanti, Davide and Mosca, Alessandro and Xiao, Guohui and Xiong, Jing and Calvanese, Diego},
  doi          = {10.3233/SW-223201},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {107-165},
  shortjournal = {Semantic Web},
  title        = {A systematic overview of data federation systems},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MADLINK: Attentive multihop and entity descriptions for link
prediction in knowledge graphs. <em>SW</em>, <em>15</em>(1), 83–106. (<a
href="https://doi.org/10.3233/SW-222960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs (KGs) comprise of interlinked information in the form of entities and relations between them in a particular domain and provide the backbone for many applications. However, the KGs are often incomplete as the links between the entities are missing. Link Prediction is the task of predicting these missing links in a KG based on the existing links. Recent years have witnessed many studies on link prediction using KG embeddings which is one of the mainstream tasks in KG completion. To do so, most of the existing methods learn the latent representation of the entities and relations whereas only a few of them consider contextual information as well as the textual descriptions of the entities. This paper introduces an attentive encoder-decoder based link prediction approach considering both structural information of the KG and the textual entity descriptions. Random walk based path selection method is used to encapsulate the contextual information of an entity in a KG. The model explores a bidirectional Gated Recurrent Unit (GRU) based encoder-decoder to learn the representation of the paths whereas SBERT is used to generate the representation of the entity descriptions. The proposed approach outperforms most of the state-of-the-art models and achieves comparable results with the rest when evaluated with FB15K, FB15K-237, WN18, WN18RR, and YAGO3-10 datasets.},
  archive      = {J_SW},
  author       = {Biswas, Russa and Sack, Harald and Alam, Mehwish},
  doi          = {10.3233/SW-222960},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {83-106},
  shortjournal = {Semantic Web},
  title        = {MADLINK: Attentive multihop and entity descriptions for link prediction in knowledge graphs},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on knowledge-aware news recommender systems.
<em>SW</em>, <em>15</em>(1), 21–82. (<a
href="https://doi.org/10.3233/SW-222991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {News consumption has shifted over time from traditional media to online platforms, which use recommendation algorithms to help users navigate through the large incoming streams of daily news by suggesting relevant articles based on their preferences and reading behavior. In comparison to domains such as movies or e-commerce, where recommender systems have proved highly successful, the characteristics of the news domain (e.g., high frequency of articles appearing and becoming outdated, greater dynamics of user interest, less explicit relations between articles, and lack of explicit user feedback) pose additional challenges for the recommendation models. While some of these can be overcome by conventional recommendation techniques, injecting external knowledge into news recommender systems has been proposed in order to enhance recommendations by capturing information and patterns not contained in the text and metadata of articles, and hence, tackle shortcomings of traditional models. This survey provides a comprehensive review of knowledge-aware news recommender systems. We propose a taxonomy that divides the models into three categories: neural methods, non-neural entity-centric methods, and non-neural path-based methods. Moreover, the underlying recommendation algorithms, as well as their evaluations are analyzed. Lastly, open issues in the domain of knowledge-aware news recommendations are identified and potential research directions are proposed.},
  archive      = {J_SW},
  author       = {Iana, Andreea and Alam, Mehwish and Paulheim, Heiko},
  doi          = {10.3233/SW-222991},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {21-82},
  shortjournal = {Semantic Web},
  title        = {A survey on knowledge-aware news recommender systems},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Morph-KGC: Scalable knowledge graph materialization with
mapping partitions. <em>SW</em>, <em>15</em>(1), 1–20. (<a
href="https://doi.org/10.3233/SW-223135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs are often constructed from heterogeneous data sources, using declarative rules that map them to a target ontology and materializing them into RDF. When these data sources are large, the materialization of the entire knowledge graph may be computationally expensive and not suitable for those cases where a rapid materialization is required. In this work, we propose an approach to overcome this limitation, based on the novel concept of mapping partitions . Mapping partitions are defined as groups of mapping rules that generate disjoint subsets of the knowledge graph. Each of these groups can be processed separately, reducing the total amount of memory and execution time required by the materialization process. We have included this optimization in our materialization engine Morph-KGC, and we have evaluated it over three different benchmarks. Our experimental results show that, compared with state-of-the-art techniques, the use of mapping partitions in Morph-KGC presents the following advantages: (i) it decreases significantly the time required for materialization, (ii) it reduces the maximum peak of memory used, and (iii) it scales to data sizes that other engines are not capable of processing currently.},
  archive      = {J_SW},
  author       = {Arenas-Guerrero, Julián and Chaves-Fraga, David and Toledo, Jhon and Pérez, María S. and Corcho, Oscar},
  doi          = {10.3233/SW-223135},
  journal      = {Semantic Web},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Semantic Web},
  title        = {Morph-KGC: Scalable knowledge graph materialization with mapping partitions},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
